{"edges": [[0, 6], [0, 5], [0, 1], [0, 2], [0, 7], [0, 3], [0, 8], [0, 4], [1, 10], [1, 11], [1, 9], [1, 12]], "layout": [[0.5882352941176471, 0.0], [0.17647058823529413, 0.5], [0.29411764705882354, 0.5], [0.4117647058823529, 0.5], [0.5294117647058824, 0.5], [0.6470588235294118, 0.5], [0.7647058823529411, 0.5], [0.8823529411764706, 0.5], [1.0, 0.5], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["Our first baseline treats SPR as a plain sequence-classification task: every\nsymbol (\u201cshape+color\u201d token) gets an embedding and sequences are represented by\nthe mean of their token embeddings (implemented with torch.nn.EmbeddingBag).  We\nbuild a vocabulary from the training split, map labels to integers, and train a\nsmall bag-of-embeddings MLP on the provided train set.  No rule engine is used\nyet\u2014this gives us a neural reference point that we can later hybridise with\nsymbolic components for zero-shot generalisation.  During training we track\ncross-entropy loss and, on the dev split after each epoch, compute Shape-\nWeighted Accuracy (SWA), Color-Weighted Accuracy (CWA) and their harmonic mean\n(HWA) as requested.  We store all metrics, losses, predictions and ground-truths\nin the prescribed experiment_data structure and persist it to ./working.  The\nmodel, inputs and loss tensors are always moved to GPU if available, satisfying\nthe device guidelines.  Finally, we evaluate once on the held-out test split and\nprint the three accuracies plus HWA so we have an end-to-end working pipeline\nthat we can iterate on later.", "Hyperparam tuning name: epochs. We grid-search the single hyperparameter\n\u201cepochs\u201d by training independent models for 5, 15, 25 and 40 epochs.   For every\nsetting we re-initialize the model, run the usual train/validation loop, perform\na final test evaluation, and store losses, metrics, predictions and ground-truth\ninside a nested experiment_data[\"epochs\"][f\"{n}_epochs\"] block.   No other\nhyperparameters or code paths are changed, so the experiment isolates the effect\nof longer training.   All collected data are written to\nworking/experiment_data.npy for later plotting/analysis.", "Hyperparam tuning name: learning_rate. We grid-search three learning rates\n(5e-4, 1e-3, 2e-3).   For each rate we (1) build a fresh model, (2) train for a\nsmall number of epochs, (3) record losses and SWA/CWA/HWA on train/validation,\nand (4) finally evaluate on the test split; all results are stored under\nexperiment_data['learning_rate']\u2192'SPR_BENCH' keyed by the learning-rate string.\nThe whole script is self-contained and ends by saving experiment_data to\n\u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: batch_size. We perform a small grid-search over batch\nsizes (32, 64, 128, 256).   For every value we (1) build fresh dataloaders,\nmodel and optimizer, (2) train for five epochs, (3) record train/validation\nlosses and weighted accuracies after each epoch, and (4) finally evaluate on the\ntest split.   All statistics, predictions and ground-truth labels are stored in\nthe required experiment_data structure under the key \u201cbatch_size_tuning\u201d, with\none sub-entry per batch size, and the whole dictionary is saved to\n\u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: embed_dim. We loop over a list of candidate embedding\nsizes (32, 64, 128, 256), train a fresh Bag-of-Embeddings classifier for each\nvalue and record per-epoch losses/metrics plus final test predictions.   Results\nare stored in a nested experiment_data dictionary under the key \u201cembed_dim\u201d \u2192\n\u201cSPR_BENCH\u201d \u2192 embed_size, and the whole structure is saved to\nexperiment_data.npy.   Apart from this hyper-parameter loop, the data loading,\nmodel definition and metric functions remain unchanged.", "Hyperparam tuning name: weight_decay. We add a simple grid-search over four\nweight-decay values (0, 1e-5, 1e-4, 1e-3).   For every value we:   1. rebuild\nthe Bag-of-Embeddings model, Adam optimizer with the specified weight_decay;\n2. train for 5 epochs, logging losses and metrics after each epoch;   3. store\nall run data inside the experiment_data structure under the key \"weight_decay\".\nAfter training we pick the run with the highest dev harmonic weighted accuracy,\nevaluate it on the test split, save its predictions/ground-truth and finally\ndump experiment_data.npy.", "Hyperparam tuning name: dropout_rate. We will introduce a Dropout layer between\nthe pooled embedding and the classifier, then loop over several dropout\nprobabilities (0, 0.1, 0.3, 0.5), training a fresh model for each. After every\nepoch we record train-/val-losses and validation SWA/CWA/HWA; after training we\nevaluate on the test split and store predictions/ground-truth.   All results are\ncollected in a single experiment_data dictionary keyed by the dropout value and\nfinally saved to \u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: label_smoothing. The script trains separate models with\nlabel-smoothing values in {0.00, 0.05, 0.10, 0.15, 0.20}, tracks dev harmonic-\nweighted accuracy for every run, selects the best smoothing, and finally reports\n& stores test-set results. All metrics, losses and predictions are dumped into\nthe required experiment_data.npy structure.", "Hyperparam tuning name: max_grad_norm. We loop over the five clipping values,\nre-initialise the tiny Bag-of-Embeddings model each time, and after every\nbackward pass optionally apply\u2028torch.nn.utils.clip_grad_norm_ to cap the global\nL\u2082 norm of all gradients before the Adam step.   For every run we record per-\nepoch train/validation losses and the three weighted accuracies, and finally\nsave test predictions.   All results are stored under\nexperiment_data['max_grad_norm']['SPR_BENCH'][str(clip)] and written to the\nmandated experiment_data.npy file.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nfrom typing import List, Dict\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------- device -----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------- data loader utilities (given) -----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef harmonic_weighted_accuracy(sequences, y_true, y_pred):\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------- simple vocabulary -----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]) -> List[int]:\n        return [self.stoi[tok] for tok in tokens]\n\n\n# ----------- model -----------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)\n\n\n# ----------- data path -----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------- build vocab and label map -----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ----------- collate function -----------\ndef collate_batch(batch):\n    token_ids = []\n    offsets = [0]\n    label_ids = []\n    for ex in batch:\n        seq, lab = ex[\"sequence\"], ex[\"label\"]\n        ids = vocab(seq.split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[lab])\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    text = torch.tensor(token_ids, dtype=torch.long)\n    labels_t = torch.tensor(label_ids, dtype=torch.long)\n    return text.to(device), offsets.to(device), labels_t.to(device)\n\n\n# ----------- dataloaders -----------\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ----------- experiment tracking -----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ----------- training setup -----------\nembed_dim = 64\nmodel = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nepochs = 5\n\n\n# ----------- helpers -----------\ndef evaluate(data_loader):\n    model.eval()\n    y_true, y_pred, sequences = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, (text, offsets, labels_t) in enumerate(data_loader):\n            outputs = model(text, offsets)\n            loss = criterion(outputs, labels_t)\n            total_loss += loss.item() * labels_t.size(0)\n            preds = outputs.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            # recover sequences for metric weighting\n            start = batch_idx * batch_size\n            end = start + labels_t.size(0)\n            sequences.extend(data_loader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(sequences, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------- training loop -----------\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for text, offsets, labels_t in train_loader:\n        optimizer.zero_grad()\n        outputs = model(text, offsets)\n        loss = criterion(outputs, labels_t)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * labels_t.size(0)\n    train_loss = running_loss / len(spr[\"train\"])\n    val_loss, swa, cwa, hwa, _, _ = evaluate(dev_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n    )\n\n# ----------- final test evaluation -----------\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred_t\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true_t\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, numpy as np, torch\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------- house-keeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ----------------- data utilities -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0 if swa + cwa == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------------- vocab -----------------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\n# ----------------- model -----------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz: int, embed_dim: int, n_cls: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ----------------- dataset path -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# build vocab / label maps\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# collate fn\ndef collate(batch):\n    tok_ids, offs, lab_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        tok_ids.extend(ids)\n        offs.append(offs[-1] + len(ids))\n        lab_ids.append(label2id[ex[\"label\"]])\n    text = torch.tensor(tok_ids, dtype=torch.long)\n    offs = torch.tensor(offs[:-1], dtype=torch.long)\n    labs = torch.tensor(lab_ids, dtype=torch.long)\n    return text.to(device), offs.to(device), labs.to(device)\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n# ----------------- experiment container -----------------\nexperiment_data = {\"epochs\": {}}\n\n# ----------------- helper: evaluation -----------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, seqs, loss_sum = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (txt, off, labs) in enumerate(loader):\n            out = model(txt, off)\n            loss_sum += criterion(out, labs).item() * labs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labs.cpu().tolist()])\n            start = b_idx * batch_size\n            seqs.extend(loader.dataset[\"sequence\"][start : start + labs.size(0)])\n    avg_loss = loss_sum / len(y_true)\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------------- hyperparameter sweep -----------------\nepoch_options = [5, 15, 25, 40]\nembed_dim = 64\nlr = 1e-3\n\nfor n_epochs in epoch_options:\n    key = f\"{n_epochs}_epochs\"\n    print(f\"\\n--- Training model for {n_epochs} epochs ---\")\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # training loop\n    for ep in range(1, n_epochs + 1):\n        model.train()\n        run_loss = 0.0\n        for txt, off, labs in train_loader:\n            optimizer.zero_grad()\n            out = model(txt, off)\n            loss = criterion(out, labs)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * labs.size(0)\n        tr_loss = run_loss / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n\n        exp_rec[\"losses\"][\"train\"].append(tr_loss)\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n        exp_rec[\"metrics\"][\"train\"].append(None)\n        exp_rec[\"metrics\"][\"val\"].append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n\n        print(\n            f\"Epoch {ep}/{n_epochs} | train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} \"\n            f\"| SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n\n    # test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_t, y_p = evaluate(model, test_loader)\n    print(\n        f\"Test @ {n_epochs} epochs | loss={test_loss:.4f} | SWA={swa_t:.4f} | \"\n        f\"CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n    )\n\n    exp_rec[\"predictions\"], exp_rec[\"ground_truth\"] = y_p, y_t\n    exp_rec[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"SWA\": swa_t,\n        \"CWA\": cwa_t,\n        \"HWA\": hwa_t,\n    }\n    experiment_data[\"epochs\"][key] = exp_rec\n\n    torch.cuda.empty_cache()  # free GPU mem between runs\n\n# ----------------- save all -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "import os, pathlib, numpy as np, torch\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- reproducibility ----------\ntorch.manual_seed(42)\n\n# ---------- experiment dictionary ----------\nexperiment_data = {\"learning_rate\": {\"SPR_BENCH\": {}}}  # will be filled per\u2010LR\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- load SPR-BENCH ----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ---------- metric helpers ----------\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_weighted_accuracy(seqs, y_t, y_p):\n    swa = shape_weighted_accuracy(seqs, y_t, y_p)\n    cwa = color_weighted_accuracy(seqs, y_t, y_p)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ---------- vocab ----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {t: i for i, t in enumerate(self.itos)}\n\n    def __call__(self, tokens: List[str]):\n        return [self.stoi[t] for t in tokens]\n\n    def __len__(self):\n        return len(self.itos)\n\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\n\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ---------- collate ----------\ndef collate_batch(batch):\n    token_ids, offsets, label_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[ex[\"label\"]])\n    return (\n        torch.tensor(token_ids, dtype=torch.long).to(device),\n        torch.tensor(offsets[:-1], dtype=torch.long).to(device),\n        torch.tensor(label_ids, dtype=torch.long).to(device),\n    )\n\n\nbatch_size = 128\n\n\ndef make_loader(split, shuffle):\n    return DataLoader(\n        spr[split], batch_size=batch_size, shuffle=shuffle, collate_fn=collate_batch\n    )\n\n\ntrain_loader = make_loader(\"train\", True)\ndev_loader = make_loader(\"dev\", False)\ntest_loader = make_loader(\"test\", False)\n\n\n# ---------- model ----------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz, embed_dim, n_cls):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, txt, offs):\n        return self.fc(self.embedding(txt, offs))\n\n\n# ---------- evaluation ----------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, seqs, total_loss = [], [], [], 0.0\n    with torch.no_grad():\n        for b, (txt, offs, lab) in enumerate(loader):\n            out = model(txt, offs)\n            total_loss += criterion(out, lab).item() * lab.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in lab.cpu().tolist()])\n            start = b * batch_size\n            end = start + lab.size(0)\n            seqs.extend(loader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ---------- training routine ----------\ndef run_experiment(lr, epochs=5, embed_dim=64):\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    train_losses, val_losses, val_metrics = [], [], []\n    for ep in range(1, epochs + 1):\n        model.train()\n        running = 0.0\n        for txt, offs, lab in train_loader:\n            opt.zero_grad()\n            loss = criterion(model(txt, offs), lab)\n            loss.backward()\n            opt.step()\n            running += loss.item() * lab.size(0)\n        tr_loss = running / len(spr[\"train\"])\n        vl_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n        train_losses.append(tr_loss)\n        val_losses.append(vl_loss)\n        val_metrics.append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n        print(\n            f\"LR={lr} | Epoch {ep}: tr_loss={tr_loss:.4f}, val_loss={vl_loss:.4f}, HWA={hwa:.4f}\"\n        )\n    # final test\n    ts_loss, swa_t, cwa_t, hwa_t, y_t, p_t = evaluate(model, test_loader)\n    print(f\"LR={lr} | Test: loss={ts_loss:.4f}, HWA={hwa_t:.4f}\")\n    return {\n        \"metrics\": {\"train\": [], \"val\": val_metrics},\n        \"losses\": {\"train\": train_losses, \"val\": val_losses},\n        \"predictions\": p_t,\n        \"ground_truth\": y_t,\n        \"test_metrics\": {\"SWA\": swa_t, \"CWA\": cwa_t, \"HWA\": hwa_t, \"loss\": ts_loss},\n    }\n\n\n# ---------- grid search ----------\nlrs = [5e-4, 1e-3, 2e-3]\nfor lr in lrs:\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][str(lr)] = run_experiment(lr)\n\n# ---------- save ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, numpy as np, torch\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------- basic setup ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- utilities ------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_weighted_accuracy(seqs, y_t, y_p):\n    swa = shape_weighted_accuracy(seqs, y_t, y_p)\n    cwa = color_weighted_accuracy(seqs, y_t, y_p)\n    return 0.0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\nclass Vocab:\n    def __init__(self, toks: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(toks))\n        self.stoi = {t: i for i, t in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, n_cls: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ---------------- data path ------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab / labels -------------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\ndef collate_batch(batch):\n    token_ids, offsets, lbls = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        lbls.append(label2id[ex[\"label\"]])\n    text = torch.tensor(token_ids, dtype=torch.long).to(device)\n    offs = torch.tensor(offsets[:-1], dtype=torch.long).to(device)\n    labels_t = torch.tensor(lbls, dtype=torch.long).to(device)\n    return text, offs, labels_t\n\n\n# ---------------- experiment dict ------------\nexperiment_data = {\"batch_size_tuning\": {}}\n\n# ---------------- evaluation helper ----------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_t, y_p, seqs, tot_loss = [], [], [], 0.0\n    idx = 0\n    with torch.no_grad():\n        for text, offs, lbl in loader:\n            out = model(text, offs)\n            loss = criterion(out, lbl)\n            tot_loss += loss.item() * lbl.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_p.extend([id2label[p] for p in preds])\n            y_t.extend([id2label[i] for i in lbl.cpu().tolist()])\n            seqs.extend(loader.dataset[\"sequence\"][idx : idx + lbl.size(0)])\n            idx += lbl.size(0)\n    avg_loss = tot_loss / len(y_t)\n    swa = shape_weighted_accuracy(seqs, y_t, y_p)\n    cwa = color_weighted_accuracy(seqs, y_t, y_p)\n    hwa = harmonic_weighted_accuracy(seqs, y_t, y_p)\n    return avg_loss, swa, cwa, hwa, y_t, y_p\n\n\n# ---------------- hyper-parameter sweep -------\nbatch_sizes = [32, 64, 128, 256]\nepochs = 5\nembed_dim = 64\nfor bs in batch_sizes:\n    print(f\"\\n===== Training with batch_size={bs} =====\")\n    # dataloaders\n    train_loader = DataLoader(\n        spr[\"train\"], batch_size=bs, shuffle=True, collate_fn=collate_batch\n    )\n    dev_loader = DataLoader(\n        spr[\"dev\"], batch_size=bs, shuffle=False, collate_fn=collate_batch\n    )\n    test_loader = DataLoader(\n        spr[\"test\"], batch_size=bs, shuffle=False, collate_fn=collate_batch\n    )\n\n    # model / optim\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # storage for this run\n    run_key = f\"bs_{bs}\"\n    experiment_data[\"batch_size_tuning\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training loop\n    for ep in range(1, epochs + 1):\n        model.train()\n        run_loss = 0.0\n        for text, offs, lbl in train_loader:\n            optimizer.zero_grad()\n            out = model(text, offs)\n            loss = criterion(out, lbl)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * lbl.size(0)\n        train_loss = run_loss / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n\n        experiment_data[\"batch_size_tuning\"][run_key][\"losses\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"batch_size_tuning\"][run_key][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"batch_size_tuning\"][run_key][\"metrics\"][\"train\"].append(None)\n        experiment_data[\"batch_size_tuning\"][run_key][\"metrics\"][\"val\"].append(\n            {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n        )\n        print(\n            f\"Epoch {ep}/{epochs} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | HWA {hwa:.4f}\"\n        )\n\n    # final test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_t, y_p = evaluate(model, test_loader)\n    experiment_data[\"batch_size_tuning\"][run_key][\"predictions\"] = y_p\n    experiment_data[\"batch_size_tuning\"][run_key][\"ground_truth\"] = y_t\n    experiment_data[\"batch_size_tuning\"][run_key][\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"SWA\": swa_t,\n        \"CWA\": cwa_t,\n        \"HWA\": hwa_t,\n    }\n    print(f\"Test | loss {test_loss:.4f} | HWA {hwa_t:.4f}\")\n\n# ---------------- save -----------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to working/experiment_data.npy\")\n", "import os, pathlib, numpy as np, torch\nfrom typing import List\nfrom datasets import load_dataset, DatasetDict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------- basic setup / paths ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\n\n# ---------------- data utilities ---------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(1, sum(w))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(1, sum(w))\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    return 0.0 if swa + cwa == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ---------------- vocabulary -------------------------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {t: i for i, t in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]):\n        return [self.stoi[t] for t in tokens]\n\n\n# ---------------- model ------------------------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, emb_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, emb_dim, mode=\"mean\")\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ---------------- data load -------------------------\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\nbatch_size = 128\n\n\ndef collate_fn(batch):\n    token_ids, offsets, label_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[ex[\"label\"]])\n    return (\n        torch.tensor(token_ids, dtype=torch.long).to(device),\n        torch.tensor(offsets[:-1], dtype=torch.long).to(device),\n        torch.tensor(label_ids, dtype=torch.long).to(device),\n    )\n\n\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n)\n\n# ---------------- evaluation helper -----------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, y_true, y_pred, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for bidx, (text, offs, labs) in enumerate(loader):\n            out = model(text, offs)\n            total_loss += criterion(out, labs).item() * labs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labs.cpu().tolist()])\n            start, end = bidx * batch_size, bidx * batch_size + labs.size(0)\n            seqs.extend(loader.dataset[\"sequence\"][start:end])\n    loss = total_loss / len(y_true)\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ---------------- experiment container ---------------\nexperiment_data = {\"embed_dim\": {\"SPR_BENCH\": {}}}\n\n# ---------------- hyper-parameter loop ---------------\nembed_dims = [32, 64, 128, 256]\nepochs = 5\nfor emb_dim in embed_dims:\n    print(f\"\\n====== Training with embed_dim={emb_dim} ======\")\n    model = BagClassifier(len(vocab), emb_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    losses_tr, losses_val, metrics_val = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running = 0.0\n        for text, offs, labs in train_loader:\n            optimizer.zero_grad()\n            loss = criterion(model(text, offs), labs)\n            loss.backward()\n            optimizer.step()\n            running += loss.item() * labs.size(0)\n        tr_loss = running / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n        losses_tr.append(tr_loss)\n        losses_val.append(val_loss)\n        metrics_val.append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n        print(\n            f\"epoch {epoch}: train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} \"\n            f\"| SWA={swa:.3f} CWA={cwa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # final test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(model, test_loader)\n    print(\n        f\"Test | loss={test_loss:.4f} SWA={swa_t:.3f} CWA={cwa_t:.3f} HWA={hwa_t:.3f}\"\n    )\n\n    # store\n    experiment_data[\"embed_dim\"][\"SPR_BENCH\"][str(emb_dim)] = {\n        \"metrics\": {\"train\": None, \"val\": metrics_val},\n        \"losses\": {\"train\": losses_tr, \"val\": losses_val},\n        \"predictions\": y_pred_t,\n        \"ground_truth\": y_true_t,\n        \"test_metrics\": {\"loss\": test_loss, \"SWA\": swa_t, \"CWA\": cwa_t, \"HWA\": hwa_t},\n    }\n\n# ---------------- save -------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, random\nfrom typing import List, Dict\nfrom datasets import load_dataset, DatasetDict\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------- paths / dirs ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- reproducibility -------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data utilities --------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ---------------- vocab -----------------------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]) -> List[int]:\n        return [self.stoi[t] for t in tokens]\n\n\n# ---------------- model -----------------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, n_cls: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ---------------- dataset loading -------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- vocab / label map -----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ---------------- collate ---------------------\ndef collate_batch(batch):\n    token_ids, offsets, label_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[ex[\"label\"]])\n    text = torch.tensor(token_ids, dtype=torch.long)\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    labels_t = torch.tensor(label_ids, dtype=torch.long)\n    sequences = [ex[\"sequence\"] for ex in batch]\n    return text.to(device), offsets.to(device), labels_t.to(device), sequences\n\n\n# ---------------- loaders ---------------------\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ---------------- experiment store ------------\nexperiment_data = {\n    \"weight_decay\": {\n        \"SPR_BENCH\": {\n            \"decay_values\": [],\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"val\": []},\n            \"best_val_hwa\": [],\n            \"test_metrics\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ---------------- evaluation helper ----------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, data_loader):\n    model.eval()\n    y_true, y_pred, sequences = [], [], []\n    tot_loss = 0.0\n    with torch.no_grad():\n        for text, offsets, labels_t, seqs in data_loader:\n            outputs = model(text, offsets)\n            loss = criterion(outputs, labels_t)\n            tot_loss += loss.item() * labels_t.size(0)\n            preds = outputs.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            sequences.extend(seqs)\n    avg_loss = tot_loss / len(y_true)\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(sequences, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ---------------- training loop --------------\ndef run_training(weight_decay: float, epochs: int = 5, embed_dim: int = 64):\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n    tr_losses, val_losses, val_metrics = [], [], []\n\n    for ep in range(1, epochs + 1):\n        model.train()\n        running = 0.0\n        for text, offsets, labels_t, _ in train_loader:\n            optimizer.zero_grad()\n            out = model(text, offsets)\n            loss = criterion(out, labels_t)\n            loss.backward()\n            optimizer.step()\n            running += loss.item() * labels_t.size(0)\n        tr_loss = running / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n        tr_losses.append(tr_loss)\n        val_losses.append(val_loss)\n        val_metrics.append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n        print(\n            f\"decay={weight_decay} | epoch={ep} | tr_loss={tr_loss:.4f} | \"\n            f\"val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n    return model, tr_losses, val_losses, val_metrics\n\n\nweight_decays = [0.0, 1e-5, 1e-4, 1e-3]\nbest_hwa, best_idx = -1, -1\nstored_models: Dict[int, torch.nn.Module] = {}\n\nfor idx, wd in enumerate(weight_decays):\n    model, tr_l, val_l, val_m = run_training(wd)\n    hwa_last = val_m[-1][\"HWA\"]\n    # store experiment data\n    exp = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    exp[\"decay_values\"].append(wd)\n    exp[\"losses\"][\"train\"].append(tr_l)\n    exp[\"losses\"][\"val\"].append(val_l)\n    exp[\"metrics\"][\"val\"].append(val_m)\n    exp[\"best_val_hwa\"].append(hwa_last)\n    # keep model for potential best selection\n    stored_models[idx] = model\n    if hwa_last > best_hwa:\n        best_hwa, best_idx = hwa_last, idx\n\nprint(f\"Best weight_decay={weight_decays[best_idx]} with dev HWA={best_hwa:.4f}\")\n\n# --------------- final test evaluation -------\nbest_model = stored_models[best_idx]\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(best_model, test_loader)\nprint(\n    f\"Test (best model): loss={test_loss:.4f} | SWA={swa_t:.4f} | \"\n    f\"CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexp = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\nexp[\"test_metrics\"].append(\n    {\"loss\": test_loss, \"SWA\": swa_t, \"CWA\": cwa_t, \"HWA\": hwa_t}\n)\nexp[\"predictions\"] = y_pred_t\nexp[\"ground_truth\"] = y_true_t\n\n# --------------- save experiment -------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, numpy as np, torch, random\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- reproducibility ----------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- data loader utilities ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # helper to load a split\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ---------- vocab ----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\n# ---------- model ----------\nclass BagClassifier(nn.Module):\n    def __init__(\n        self, vocab_size: int, embed_dim: int, num_classes: int, dropout_rate: float\n    ):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.dropout = nn.Dropout(p=dropout_rate)\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, text, offsets):\n        pooled = self.embedding(text, offsets)\n        dropped = self.dropout(pooled)\n        return self.fc(dropped)\n\n\n# ---------- data path ----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- build vocab / label maps ----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ---------- collate ----------\ndef collate_batch(batch):\n    token_ids, offsets, label_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[ex[\"label\"]])\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long, device=device)\n    text = torch.tensor(token_ids, dtype=torch.long, device=device)\n    labels_t = torch.tensor(label_ids, dtype=torch.long, device=device)\n    return text, offsets, labels_t\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ---------- experiment container ----------\nexperiment_data = {}\n\n\n# ---------- evaluation helper ----------\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    y_true, y_pred, seqs, total_loss = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (text, offsets, labels_t) in enumerate(data_loader):\n            out = model(text, offsets)\n            loss = criterion(out, labels_t)\n            total_loss += loss.item() * labels_t.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            start = b_idx * batch_size\n            seqs.extend(\n                data_loader.dataset[\"sequence\"][start : start + labels_t.size(0)]\n            )\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ---------- hyperparameter tuning ----------\nembed_dim, epochs, lr = 64, 5, 1e-3\ndropout_rates = [0.0, 0.1, 0.3, 0.5]\n\nfor p in dropout_rates:\n    key = f\"dropout_{p}\"\n    experiment_data[key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"config\": {\"dropout_rate\": p},\n        }\n    }\n    model = BagClassifier(len(vocab), embed_dim, len(labels), p).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for text, offsets, labels_t in train_loader:\n            optimizer.zero_grad()\n            loss = criterion(model(text, offsets), labels_t)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * labels_t.size(0)\n        train_loss = running_loss / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader, criterion)\n\n        experiment_data[key][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[key][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[key][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)\n        experiment_data[key][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n            {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n        )\n\n        print(\n            f\"[p={p}] Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n\n    # final test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(\n        model, test_loader, criterion\n    )\n    print(\n        f\"[p={p}] Test: loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n    )\n\n    experiment_data[key][\"SPR_BENCH\"][\"predictions\"] = y_pred_t\n    experiment_data[key][\"SPR_BENCH\"][\"ground_truth\"] = y_true_t\n\n# ---------- save ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- folders & RNG ---------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\n\n# --------------------- device ----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device {device}\")\n\n\n# --------------------- dataset helpers -------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0.0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# --------------------- vocab -----------------------------\nclass Vocab:\n    def __init__(self, tokens):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {t: i for i, t in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks):\n        return [self.stoi[t] for t in toks]\n\n\n# --------------------- model -----------------------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, n_cls):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, emb_dim, mode=\"mean\")\n        self.fc = nn.Linear(emb_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# --------------------- data ------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# vocab & labels\nall_toks = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_toks)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# collate\ndef collate_fn(batch):\n    token_ids, offsets, label_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[ex[\"label\"]])\n    return (\n        torch.tensor(token_ids, dtype=torch.long).to(device),\n        torch.tensor(offsets[:-1], dtype=torch.long).to(device),\n        torch.tensor(label_ids, dtype=torch.long).to(device),\n    )\n\n\nbatch_size = 128\ntrain_loader = DataLoader(spr[\"train\"], batch_size, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(spr[\"dev\"], batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(spr[\"test\"], batch_size, shuffle=False, collate_fn=collate_fn)\n\n\n# --------------------- evaluation ------------------------\ndef evaluate(model, criterion, dloader):\n    model.eval()\n    y_true, y_pred, seqs, total_loss = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (text, off, labs) in enumerate(dloader):\n            out = model(text, off)\n            loss = criterion(out, labs)\n            total_loss += loss.item() * labs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[t] for t in labs.cpu().tolist()])\n            start, end = b_idx * batch_size, b_idx * batch_size + labs.size(0)\n            seqs.extend(dloader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa, cwa, hwa = (\n        shape_weighted_accuracy(seqs, y_true, y_pred),\n        color_weighted_accuracy(seqs, y_true, y_pred),\n        harmonic_weighted_accuracy(seqs, y_true, y_pred),\n    )\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# --------------------- experiment tracking ---------------\nexperiment_data = {\n    \"label_smoothing\": {\n        \"SPR_BENCH\": {\n            \"smoothing_values\": [],\n            \"losses\": {\"val\": []},\n            \"metrics\": {\"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# --------------------- hyperparameter sweep --------------\nsmooth_vals = [0.00, 0.05, 0.10, 0.15, 0.20]\nepochs = 5\nembed_dim = 64\n\nbest_hwa, best_state, best_smooth = -1.0, None, None\n\nfor sm in smooth_vals:\n    print(f\"\\n===== Training with label_smoothing={sm:.2f} =====\")\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=sm)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for ep in range(1, epochs + 1):\n        model.train()\n        run_loss = 0.0\n        for text, off, labs in train_loader:\n            optimizer.zero_grad()\n            out = model(text, off)\n            loss = criterion(out, labs)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * labs.size(0)\n        train_loss = run_loss / len(spr[\"train\"])\n\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, criterion, dev_loader)\n        print(f\"Epoch {ep}/{epochs}: val_loss={val_loss:.4f} | HWA={hwa:.4f}\")\n\n    # store per-smoothing summary (last epoch metrics)\n    experiment_data[\"label_smoothing\"][\"SPR_BENCH\"][\"smoothing_values\"].append(sm)\n    experiment_data[\"label_smoothing\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"label_smoothing\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n    )\n\n    # keep best on dev\n    if hwa > best_hwa:\n        best_hwa, best_state, best_smooth = hwa, model.state_dict(), sm\n\nprint(f\"\\nBest smoothing={best_smooth:.2f} with dev HWA={best_hwa:.4f}\")\n\n# --------------------- final test evaluation -------------\nbest_model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\nbest_model.load_state_dict(best_state)\nbest_criterion = nn.CrossEntropyLoss(label_smoothing=best_smooth)\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(\n    best_model, best_criterion, test_loader\n)\n\nprint(\n    f\"TEST | loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexp_section = experiment_data[\"label_smoothing\"][\"SPR_BENCH\"]\nexp_section[\"predictions\"] = y_pred_t\nexp_section[\"ground_truth\"] = y_true_t\nexp_section[\"best_smoothing\"] = best_smooth\nexp_section[\"test_metrics\"] = {\"SWA\": swa_t, \"CWA\": cwa_t, \"HWA\": hwa_t}\nexp_section[\"test_loss\"] = test_loss\n\n# --------------------- save ------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, numpy as np, torch\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- paths / device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nassert DATA_PATH.exists(), f\"SPR_BENCH not found at {DATA_PATH}\"\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ---------- helpers ----------\ndef set_seed(s=0):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str):\n    return len({tok[0] for tok in seq.split() if tok})\n\n\ndef count_color_variety(seq: str):\n    return len({tok[1] for tok in seq.split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, yt, yp in zip(w, y_true, y_pred) if yt == yp) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi for wi, yt, yp in zip(w, y_true, y_pred) if yt == yp) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz, embed_dim, num_cls):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, num_cls)\n\n    def forward(self, txt, offs):\n        return self.fc(self.embedding(txt, offs))\n\n\n# ---------- load data / vocab ----------\nspr = load_spr_bench(DATA_PATH)\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\ndef collate(batch):\n    token_ids, offsets, label_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[ex[\"label\"]])\n    return (\n        torch.tensor(token_ids, dtype=torch.long).to(device),\n        torch.tensor(offsets[:-1], dtype=torch.long).to(device),\n        torch.tensor(label_ids, dtype=torch.long).to(device),\n    )\n\n\nbatch_size = 128\ntrain_loader = DataLoader(spr[\"train\"], batch_size, True, collate_fn=collate)\ndev_loader = DataLoader(spr[\"dev\"], batch_size, False, collate_fn=collate)\ntest_loader = DataLoader(spr[\"test\"], batch_size, False, collate_fn=collate)\n\n# ---------- evaluation ----------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_t, y_p, seqs, loss_sum = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (txt, offs, lbs) in enumerate(loader):\n            out = model(txt, offs)\n            loss_sum += criterion(out, lbs).item() * lbs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_p.extend([id2label[p] for p in preds])\n            y_t.extend([id2label[i] for i in lbs.cpu().tolist()])\n            s_start = b_idx * batch_size\n            s_end = s_start + lbs.size(0)\n            seqs.extend(loader.dataset[\"sequence\"][s_start:s_end])\n    n = len(y_t)\n    return (\n        loss_sum / n,\n        shape_weighted_accuracy(seqs, y_t, y_p),\n        color_weighted_accuracy(seqs, y_t, y_p),\n        harmonic_weighted_accuracy(seqs, y_t, y_p),\n        y_t,\n        y_p,\n    )\n\n\n# ---------- experiment dict ----------\nexperiment_data = {\"max_grad_norm\": {\"SPR_BENCH\": {}}}\n\n# ---------- hyperparameter sweep ----------\nclip_values = [0, 0.5, 1, 2, 5]\nepochs = 5\nembed_dim = 64\nlr = 1e-3\n\nfor clip in clip_values:\n    set_seed(42)  # reproducibility per run\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    run_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training\n    for ep in range(1, epochs + 1):\n        model.train()\n        running = 0.0\n        for txt, offs, lbs in train_loader:\n            optimizer.zero_grad()\n            out = model(txt, offs)\n            loss = criterion(out, lbs)\n            loss.backward()\n            if clip > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optimizer.step()\n            running += loss.item() * lbs.size(0)\n        train_loss = running / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n        run_rec[\"losses\"][\"train\"].append(train_loss)\n        run_rec[\"losses\"][\"val\"].append(val_loss)\n        run_rec[\"metrics\"][\"train\"].append(None)\n        run_rec[\"metrics\"][\"val\"].append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n        print(\n            f\"clip={clip} | epoch={ep} | train_loss={train_loss:.4f} | \"\n            f\"val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n    # final test\n    t_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(model, test_loader)\n    print(\n        f\"[clip {clip}] Test | loss={t_loss:.4f} | SWA={swa_t:.4f} | \"\n        f\"CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n    )\n    run_rec[\"predictions\"] = y_pred_t\n    run_rec[\"ground_truth\"] = y_true_t\n    run_rec[\"test_metrics\"] = {\"loss\": t_loss, \"SWA\": swa_t, \"CWA\": cwa_t, \"HWA\": hwa_t}\n    experiment_data[\"max_grad_norm\"][\"SPR_BENCH\"][str(clip)] = run_rec\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------- house-keeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ----------------- data utilities -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0 if swa + cwa == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------------- vocab -----------------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\n# ----------------- model -----------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz: int, embed_dim: int, n_cls: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ----------------- dataset path -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# build vocab / label maps\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# collate fn\ndef collate(batch):\n    tok_ids, offs, lab_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        tok_ids.extend(ids)\n        offs.append(offs[-1] + len(ids))\n        lab_ids.append(label2id[ex[\"label\"]])\n    text = torch.tensor(tok_ids, dtype=torch.long)\n    offs = torch.tensor(offs[:-1], dtype=torch.long)\n    labs = torch.tensor(lab_ids, dtype=torch.long)\n    return text.to(device), offs.to(device), labs.to(device)\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n# ----------------- experiment container -----------------\nexperiment_data = {\"epochs\": {}}\n\n# ----------------- helper: evaluation -----------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, seqs, loss_sum = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (txt, off, labs) in enumerate(loader):\n            out = model(txt, off)\n            loss_sum += criterion(out, labs).item() * labs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labs.cpu().tolist()])\n            start = b_idx * batch_size\n            seqs.extend(loader.dataset[\"sequence\"][start : start + labs.size(0)])\n    avg_loss = loss_sum / len(y_true)\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------------- hyperparameter sweep -----------------\nepoch_options = [5, 15, 25, 40]\nembed_dim = 64\nlr = 1e-3\n\nfor n_epochs in epoch_options:\n    key = f\"{n_epochs}_epochs\"\n    print(f\"\\n--- Training model for {n_epochs} epochs ---\")\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # training loop\n    for ep in range(1, n_epochs + 1):\n        model.train()\n        run_loss = 0.0\n        for txt, off, labs in train_loader:\n            optimizer.zero_grad()\n            out = model(txt, off)\n            loss = criterion(out, labs)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * labs.size(0)\n        tr_loss = run_loss / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n\n        exp_rec[\"losses\"][\"train\"].append(tr_loss)\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n        exp_rec[\"metrics\"][\"train\"].append(None)\n        exp_rec[\"metrics\"][\"val\"].append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n\n        print(\n            f\"Epoch {ep}/{n_epochs} | train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} \"\n            f\"| SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n\n    # test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_t, y_p = evaluate(model, test_loader)\n    print(\n        f\"Test @ {n_epochs} epochs | loss={test_loss:.4f} | SWA={swa_t:.4f} | \"\n        f\"CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n    )\n\n    exp_rec[\"predictions\"], exp_rec[\"ground_truth\"] = y_p, y_t\n    exp_rec[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"SWA\": swa_t,\n        \"CWA\": cwa_t,\n        \"HWA\": hwa_t,\n    }\n    experiment_data[\"epochs\"][key] = exp_rec\n\n    torch.cuda.empty_cache()  # free GPU mem between runs\n\n# ----------------- save all -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------- house-keeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ----------------- data utilities -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0 if swa + cwa == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------------- vocab -----------------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\n# ----------------- model -----------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz: int, embed_dim: int, n_cls: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ----------------- dataset path -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# build vocab / label maps\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# collate fn\ndef collate(batch):\n    tok_ids, offs, lab_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        tok_ids.extend(ids)\n        offs.append(offs[-1] + len(ids))\n        lab_ids.append(label2id[ex[\"label\"]])\n    text = torch.tensor(tok_ids, dtype=torch.long)\n    offs = torch.tensor(offs[:-1], dtype=torch.long)\n    labs = torch.tensor(lab_ids, dtype=torch.long)\n    return text.to(device), offs.to(device), labs.to(device)\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n# ----------------- experiment container -----------------\nexperiment_data = {\"epochs\": {}}\n\n# ----------------- helper: evaluation -----------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, seqs, loss_sum = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (txt, off, labs) in enumerate(loader):\n            out = model(txt, off)\n            loss_sum += criterion(out, labs).item() * labs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labs.cpu().tolist()])\n            start = b_idx * batch_size\n            seqs.extend(loader.dataset[\"sequence\"][start : start + labs.size(0)])\n    avg_loss = loss_sum / len(y_true)\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------------- hyperparameter sweep -----------------\nepoch_options = [5, 15, 25, 40]\nembed_dim = 64\nlr = 1e-3\n\nfor n_epochs in epoch_options:\n    key = f\"{n_epochs}_epochs\"\n    print(f\"\\n--- Training model for {n_epochs} epochs ---\")\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # training loop\n    for ep in range(1, n_epochs + 1):\n        model.train()\n        run_loss = 0.0\n        for txt, off, labs in train_loader:\n            optimizer.zero_grad()\n            out = model(txt, off)\n            loss = criterion(out, labs)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * labs.size(0)\n        tr_loss = run_loss / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n\n        exp_rec[\"losses\"][\"train\"].append(tr_loss)\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n        exp_rec[\"metrics\"][\"train\"].append(None)\n        exp_rec[\"metrics\"][\"val\"].append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n\n        print(\n            f\"Epoch {ep}/{n_epochs} | train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} \"\n            f\"| SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n\n    # test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_t, y_p = evaluate(model, test_loader)\n    print(\n        f\"Test @ {n_epochs} epochs | loss={test_loss:.4f} | SWA={swa_t:.4f} | \"\n        f\"CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n    )\n\n    exp_rec[\"predictions\"], exp_rec[\"ground_truth\"] = y_p, y_t\n    exp_rec[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"SWA\": swa_t,\n        \"CWA\": cwa_t,\n        \"HWA\": hwa_t,\n    }\n    experiment_data[\"epochs\"][key] = exp_rec\n\n    torch.cuda.empty_cache()  # free GPU mem between runs\n\n# ----------------- save all -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch\nfrom typing import List\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------- house-keeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ----------------- data utilities -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(v if t == p else 0 for v, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_weighted_accuracy(seqs, y_true, y_pred):\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    return 0 if swa + cwa == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------------- vocab -----------------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, toks: List[str]):\n        return [self.stoi[t] for t in toks]\n\n\n# ----------------- model -----------------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz: int, embed_dim: int, n_cls: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_sz, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, n_cls)\n\n    def forward(self, text, offsets):\n        return self.fc(self.embedding(text, offsets))\n\n\n# ----------------- dataset path -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# build vocab / label maps\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# collate fn\ndef collate(batch):\n    tok_ids, offs, lab_ids = [], [0], []\n    for ex in batch:\n        ids = vocab(ex[\"sequence\"].split())\n        tok_ids.extend(ids)\n        offs.append(offs[-1] + len(ids))\n        lab_ids.append(label2id[ex[\"label\"]])\n    text = torch.tensor(tok_ids, dtype=torch.long)\n    offs = torch.tensor(offs[:-1], dtype=torch.long)\n    labs = torch.tensor(lab_ids, dtype=torch.long)\n    return text.to(device), offs.to(device), labs.to(device)\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n# ----------------- experiment container -----------------\nexperiment_data = {\"epochs\": {}}\n\n# ----------------- helper: evaluation -----------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    y_true, y_pred, seqs, loss_sum = [], [], [], 0.0\n    with torch.no_grad():\n        for b_idx, (txt, off, labs) in enumerate(loader):\n            out = model(txt, off)\n            loss_sum += criterion(out, labs).item() * labs.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labs.cpu().tolist()])\n            start = b_idx * batch_size\n            seqs.extend(loader.dataset[\"sequence\"][start : start + labs.size(0)])\n    avg_loss = loss_sum / len(y_true)\n    swa, cwa = shape_weighted_accuracy(seqs, y_true, y_pred), color_weighted_accuracy(\n        seqs, y_true, y_pred\n    )\n    hwa = harmonic_weighted_accuracy(seqs, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------------- hyperparameter sweep -----------------\nepoch_options = [5, 15, 25, 40]\nembed_dim = 64\nlr = 1e-3\n\nfor n_epochs in epoch_options:\n    key = f\"{n_epochs}_epochs\"\n    print(f\"\\n--- Training model for {n_epochs} epochs ---\")\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    model = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # training loop\n    for ep in range(1, n_epochs + 1):\n        model.train()\n        run_loss = 0.0\n        for txt, off, labs in train_loader:\n            optimizer.zero_grad()\n            out = model(txt, off)\n            loss = criterion(out, labs)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * labs.size(0)\n        tr_loss = run_loss / len(spr[\"train\"])\n        val_loss, swa, cwa, hwa, _, _ = evaluate(model, dev_loader)\n\n        exp_rec[\"losses\"][\"train\"].append(tr_loss)\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n        exp_rec[\"metrics\"][\"train\"].append(None)\n        exp_rec[\"metrics\"][\"val\"].append({\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa})\n\n        print(\n            f\"Epoch {ep}/{n_epochs} | train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} \"\n            f\"| SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n        )\n\n    # test evaluation\n    test_loss, swa_t, cwa_t, hwa_t, y_t, y_p = evaluate(model, test_loader)\n    print(\n        f\"Test @ {n_epochs} epochs | loss={test_loss:.4f} | SWA={swa_t:.4f} | \"\n        f\"CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n    )\n\n    exp_rec[\"predictions\"], exp_rec[\"ground_truth\"] = y_p, y_t\n    exp_rec[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"SWA\": swa_t,\n        \"CWA\": cwa_t,\n        \"HWA\": hwa_t,\n    }\n    experiment_data[\"epochs\"][key] = exp_rec\n\n    torch.cuda.empty_cache()  # free GPU mem between runs\n\n# ----------------- save all -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 548691.03\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 707684.42\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 539883.90\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Epoch\n1: train_loss=0.5951 | val_loss=0.5392 | SWA=0.7411 | CWA=0.7364 | HWA=0.7387',\n'\\n', 'Epoch 2: train_loss=0.5268 | val_loss=0.5226 | SWA=0.7407 | CWA=0.7358 |\nHWA=0.7382', '\\n', 'Epoch 3: train_loss=0.5205 | val_loss=0.5212 | SWA=0.7458 |\nCWA=0.7405 | HWA=0.7432', '\\n', 'Epoch 4: train_loss=0.5198 | val_loss=0.5213 |\nSWA=0.7481 | CWA=0.7455 | HWA=0.7468', '\\n', 'Epoch 5: train_loss=0.5199 |\nval_loss=0.5210 | SWA=0.7393 | CWA=0.7349 | HWA=0.7371', '\\n', 'Test:\nloss=0.7201 | SWA=0.5950 | CWA=0.6205 | HWA=0.6075', '\\n', 'Execution time: 5\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n347624.98 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 557471.49\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 612835.00\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', '\\n---\nTraining model for 5 epochs ---', '\\n', 'Epoch 1/5 | train_loss=0.5951 |\nval_loss=0.5392 | SWA=0.7411 | CWA=0.7364 | HWA=0.7387', '\\n', 'Epoch 2/5 |\ntrain_loss=0.5268 | val_loss=0.5226 | SWA=0.7407 | CWA=0.7358 | HWA=0.7382',\n'\\n', 'Epoch 3/5 | train_loss=0.5205 | val_loss=0.5212 | SWA=0.7458 | CWA=0.7405\n| HWA=0.7432', '\\n', 'Epoch 4/5 | train_loss=0.5198 | val_loss=0.5213 |\nSWA=0.7481 | CWA=0.7455 | HWA=0.7468', '\\n', 'Epoch 5/5 | train_loss=0.5199 |\nval_loss=0.5210 | SWA=0.7393 | CWA=0.7349 | HWA=0.7371', '\\n', 'Test @ 5 epochs\n| loss=0.7201 | SWA=0.5950 | CWA=0.6205 | HWA=0.6075', '\\n', '\\n--- Training\nmodel for 15 epochs ---', '\\n', 'Epoch 1/15 | train_loss=0.5992 |\nval_loss=0.5428 | SWA=0.7429 | CWA=0.7403 | HWA=0.7416', '\\n', 'Epoch 2/15 |\ntrain_loss=0.5281 | val_loss=0.5230 | SWA=0.7285 | CWA=0.7245 | HWA=0.7265',\n'\\n', 'Epoch 3/15 | train_loss=0.5203 | val_loss=0.5212 | SWA=0.7453 |\nCWA=0.7420 | HWA=0.7436', '\\n', 'Epoch 4/15 | train_loss=0.5200 |\nval_loss=0.5221 | SWA=0.7501 | CWA=0.7449 | HWA=0.7475', '\\n', 'Epoch 5/15 |\ntrain_loss=0.5199 | val_loss=0.5212 | SWA=0.7392 | CWA=0.7339 | HWA=0.7365',\n'\\n', 'Epoch 6/15 | train_loss=0.5198 | val_loss=0.5214 | SWA=0.7540 |\nCWA=0.7469 | HWA=0.7505', '\\n', 'Epoch 7/15 | train_loss=0.5199 |\nval_loss=0.5209 | SWA=0.7392 | CWA=0.7344 | HWA=0.7368', '\\n', 'Epoch 8/15 |\ntrain_loss=0.5200 | val_loss=0.5214 | SWA=0.7425 | CWA=0.7371 | HWA=0.7398',\n'\\n', 'Epoch 9/15 | train_loss=0.5199 | val_loss=0.5215 | SWA=0.7660 |\nCWA=0.7613 | HWA=0.7636', '\\n', 'Epoch 10/15 | train_loss=0.5200 |\nval_loss=0.5216 | SWA=0.7534 | CWA=0.7497 | HWA=0.7515', '\\n', 'Epoch 11/15 |\ntrain_loss=0.5198 | val_loss=0.5209 | SWA=0.7470 | CWA=0.7410 | HWA=0.7440',\n'\\n', 'Epoch 12/15 | train_loss=0.5198 | val_loss=0.5221 | SWA=0.7643 |\nCWA=0.7586 | HWA=0.7615', '\\n', 'Epoch 13/15 | train_loss=0.5198 |\nval_loss=0.5211 | SWA=0.7354 | CWA=0.7311 | HWA=0.7332', '\\n', 'Epoch 14/15 |\ntrain_loss=0.5197 | val_loss=0.5214 | SWA=0.7486 | CWA=0.7440 | HWA=0.7463',\n'\\n', 'Epoch 15/15 | train_loss=0.5198 | val_loss=0.5214 | SWA=0.7492 |\nCWA=0.7438 | HWA=0.7465', '\\n', 'Test @ 15 epochs | loss=0.7247 | SWA=0.5866 |\nCWA=0.6122 | HWA=0.5991', '\\n', '\\n--- Training model for 25 epochs ---', '\\n',\n'Epoch 1/25 | train_loss=0.6017 | val_loss=0.5429 | SWA=0.7346 | CWA=0.7305 |\nHWA=0.7325', '\\n', 'Epoch 2/25 | train_loss=0.5290 | val_loss=0.5228 |\nSWA=0.7367 | CWA=0.7324 | HWA=0.7345', '\\n', 'Epoch 3/25 | train_loss=0.5208 |\nval_loss=0.5218 | SWA=0.7594 | CWA=0.7550 | HWA=0.7572', '\\n', 'Epoch 4/25 |\ntrain_loss=0.5199 | val_loss=0.5216 | SWA=0.7635 | CWA=0.7586 | HWA=0.7610',\n'\\n', 'Epoch 5/25 | train_loss=0.5200 | val_loss=0.5217 | SWA=0.7546 |\nCWA=0.7494 | HWA=0.7520', '\\n', 'Epoch 6/25 | train_loss=0.5199 |\nval_loss=0.5210 | SWA=0.7488 | CWA=0.7435 | HWA=0.7461', '\\n', 'Epoch 7/25 |\ntrain_loss=0.5199 | val_loss=0.5211 | SWA=0.7368 | CWA=0.7323 | HWA=0.7345',\n'\\n', 'Epoch 8/25 | train_loss=0.5199 | val_loss=0.5216 | SWA=0.7352 |\nCWA=0.7309 | HWA=0.7331', '\\n', 'Epoch 9/25 | train_loss=0.5198 |\nval_loss=0.5213 | SWA=0.7560 | CWA=0.7505 | HWA=0.7533', '\\n', 'Epoch 10/25 |\ntrain_loss=0.5201 | val_loss=0.5222 | SWA=0.7537 | CWA=0.7486 | HWA=0.7512',\n'\\n', 'Epoch 11/25 | train_loss=0.5198 | val_loss=0.5218 | SWA=0.7356 |\nCWA=0.7312 | HWA=0.7334', '\\n', 'Epoch 12/25 | train_loss=0.5198 |\nval_loss=0.5218 | SWA=0.7645 | CWA=0.7596 | HWA=0.7621', '\\n', 'Epoch 13/25 |\ntrain_loss=0.5200 | val_loss=0.5213 | SWA=0.7484 | CWA=0.7430 | HWA=0.7457',\n'\\n', 'Epoch 14/25 | train_loss=0.5201 | val_loss=0.5211 | SWA=0.7533 |\nCWA=0.7466 | HWA=0.7499', '\\n', 'Epoch 15/25 | train_loss=0.5199 |\nval_loss=0.5215 | SWA=0.7459 | CWA=0.7407 | HWA=0.7433', '\\n', 'Epoch 16/25 |\ntrain_loss=0.5198 | val_loss=0.5221 | SWA=0.7358 | CWA=0.7316 | HWA=0.7337',\n'\\n', 'Epoch 17/25 | train_loss=0.5199 | val_loss=0.5218 | SWA=0.7421 |\nCWA=0.7380 | HWA=0.7400', '\\n', 'Epoch 18/25 | train_loss=0.5198 |\nval_loss=0.5213 | SWA=0.7596 | CWA=0.7550 | HWA=0.7573', '\\n', 'Epoch 19/25 |\ntrain_loss=0.5197 | val_loss=0.5212 | SWA=0.7374 | CWA=0.7330 | HWA=0.7352',\n'\\n', 'Epoch 20/25 | train_loss=0.5199 | val_loss=0.5206 | SWA=0.7461 |\nCWA=0.7398 | HWA=0.7429', '\\n', 'Epoch 21/25 | train_loss=0.5201 |\nval_loss=0.5213 | SWA=0.7440 | CWA=0.7390 | HWA=0.7415', '\\n', 'Epoch 22/25 |\ntrain_loss=0.5200 | val_loss=0.5213 | SWA=0.7589 | CWA=0.7540 | HWA=0.7565',\n'\\n', 'Epoch 23/25 | train_loss=0.5196 | val_loss=0.5219 | SWA=0.7619 |\nCWA=0.7573 | HWA=0.7596', '\\n', 'Epoch 24/25 | train_loss=0.5200 |\nval_loss=0.5215 | SWA=0.7650 | CWA=0.7595 | HWA=0.7623', '\\n', 'Epoch 25/25 |\ntrain_loss=0.5198 | val_loss=0.5221 | SWA=0.7670 | CWA=0.7618 | HWA=0.7644',\n'\\n', 'Test @ 25 epochs | loss=0.7316 | SWA=0.5898 | CWA=0.6159 | HWA=0.6026',\n'\\n', '\\n--- Training model for 40 epochs ---', '\\n', 'Epoch 1/40 |\ntrain_loss=0.5850 | val_loss=0.5350 | SWA=0.7469 | CWA=0.7432 | HWA=0.7450',\n'\\n', 'Epoch 2/40 | train_loss=0.5257 | val_loss=0.5212 | SWA=0.7403 |\nCWA=0.7366 | HWA=0.7384', '\\n', 'Epoch 3/40 | train_loss=0.5203 |\nval_loss=0.5217 | SWA=0.7410 | CWA=0.7369 | HWA=0.7389', '\\n', 'Epoch 4/40 |\ntrain_loss=0.5199 | val_loss=0.5211 | SWA=0.7560 | CWA=0.7510 | HWA=0.7535',\n'\\n', 'Epoch 5/40 | train_loss=0.5197 | val_loss=0.5211 | SWA=0.7569 |\nCWA=0.7522 | HWA=0.7545', '\\n', 'Epoch 6/40 | train_loss=0.5200 |\nval_loss=0.5213 | SWA=0.7464 | CWA=0.7411 | HWA=0.7438', '\\n', 'Epoch 7/40 |\ntrain_loss=0.5199 | val_loss=0.5219 | SWA=0.7482 | CWA=0.7430 | HWA=0.7456',\n'\\n', 'Epoch 8/40 | train_loss=0.5201 | val_loss=0.5219 | SWA=0.7566 |\nCWA=0.7519 | HWA=0.7542', '\\n', 'Epoch 9/40 | train_loss=0.5199 |\nval_loss=0.5206 | SWA=0.7386 | CWA=0.7335 | HWA=0.7360', '\\n', 'Epoch 10/40 |\ntrain_loss=0.5199 | val_loss=0.5211 | SWA=0.7361 | CWA=0.7322 | HWA=0.7341',\n'\\n', 'Epoch 11/40 | train_loss=0.5197 | val_loss=0.5210 | SWA=0.7339 |\nCWA=0.7305 | HWA=0.7322', '\\n', 'Epoch 12/40 | train_loss=0.5198 |\nval_loss=0.5215 | SWA=0.7506 | CWA=0.7449 | HWA=0.7477', '\\n', 'Epoch 13/40 |\ntrain_loss=0.5200 | val_loss=0.5219 | SWA=0.7428 | CWA=0.7385 | HWA=0.7406',\n'\\n', 'Epoch 14/40 | train_loss=0.5200 | val_loss=0.5213 | SWA=0.7387 |\nCWA=0.7339 | HWA=0.7363', '\\n', 'Epoch 15/40 | train_loss=0.5198 |\nval_loss=0.5210 | SWA=0.7460 | CWA=0.7404 | HWA=0.7432', '\\n', 'Epoch 16/40 |\ntrain_loss=0.5199 | val_loss=0.5213 | SWA=0.7450 | CWA=0.7396 | HWA=0.7423',\n'\\n', 'Epoch 17/40 | train_loss=0.5202 | val_loss=0.5211 | SWA=0.7489 |\nCWA=0.7433 | HWA=0.7461', '\\n', 'Epoch 18/40 | train_loss=0.5197 |\nval_loss=0.5210 | SWA=0.7567 | CWA=0.7522 | HWA=0.7544', '\\n', 'Epoch 19/40 |\ntrain_loss=0.5198 | val_loss=0.5212 | SWA=0.7651 | CWA=0.7614 | HWA=0.7632',\n'\\n', 'Epoch 20/40 | train_loss=0.5201 | val_loss=0.5214 | SWA=0.7486 |\nCWA=0.7428 | HWA=0.7457', '\\n', 'Epoch 21/40 | train_loss=0.5198 |\nval_loss=0.5210 | SWA=0.7371 | CWA=0.7329 | HWA=0.7350', '\\n', 'Epoch 22/40 |\ntrain_loss=0.5201 | val_loss=0.5211 | SWA=0.7514 | CWA=0.7454 | HWA=0.7484',\n'\\n', 'Epoch 23/40 | train_loss=0.5199 | val_loss=0.5213 | SWA=0.7378 |\nCWA=0.7336 | HWA=0.7357', '\\n', 'Epoch 24/40 | train_loss=0.5199 |\nval_loss=0.5208 | SWA=0.7379 | CWA=0.7328 | HWA=0.7354', '\\n', 'Epoch 25/40 |\ntrain_loss=0.5199 | val_loss=0.5213 | SWA=0.7346 | CWA=0.7305 | HWA=0.7325',\n'\\n', 'Epoch 26/40 | train_loss=0.5198 | val_loss=0.5221 | SWA=0.7502 |\nCWA=0.7455 | HWA=0.7479', '\\n', 'Epoch 27/40 | train_loss=0.5199 |\nval_loss=0.5222 | SWA=0.7516 | CWA=0.7467 | HWA=0.7491', '\\n', 'Epoch 28/40 |\ntrain_loss=0.5198 | val_loss=0.5212 | SWA=0.7454 | CWA=0.7388 | HWA=0.7421',\n'\\n', 'Epoch 29/40 | train_loss=0.5197 | val_loss=0.5213 | SWA=0.7438 |\nCWA=0.7392 | HWA=0.7415', '\\n', 'Epoch 30/40 | train_loss=0.5199 |\nval_loss=0.5223 | SWA=0.7438 | CWA=0.7380 | HWA=0.7409', '\\n', 'Epoch 31/40 |\ntrain_loss=0.5196 | val_loss=0.5218 | SWA=0.7514 | CWA=0.7467 | HWA=0.7490',\n'\\n', 'Epoch 32/40 | train_loss=0.5198 | val_loss=0.5210 | SWA=0.7329 |\nCWA=0.7291 | HWA=0.7310', '\\n', 'Epoch 33/40 | train_loss=0.5199 |\nval_loss=0.5218 | SWA=0.7605 | CWA=0.7557 | HWA=0.7581', '\\n', 'Epoch 34/40 |\ntrain_loss=0.5200 | val_loss=0.5214 | SWA=0.7417 | CWA=0.7374 | HWA=0.7395',\n'\\n', 'Epoch 35/40 | train_loss=0.5201 | val_loss=0.5212 | SWA=0.7475 |\nCWA=0.7419 | HWA=0.7447', '\\n', 'Epoch 36/40 | train_loss=0.5199 |\nval_loss=0.5218 | SWA=0.7593 | CWA=0.7536 | HWA=0.7564', '\\n', 'Epoch 37/40 |\ntrain_loss=0.5197 | val_loss=0.5218 | SWA=0.7647 | CWA=0.7607 | HWA=0.7627',\n'\\n', 'Epoch 38/40 | train_loss=0.5200 | val_loss=0.5217 | SWA=0.7486 |\nCWA=0.7433 | HWA=0.7460', '\\n', 'Epoch 39/40 | train_loss=0.5197 |\nval_loss=0.5210 | SWA=0.7597 | CWA=0.7543 | HWA=0.7570', '\\n', 'Epoch 40/40 |\ntrain_loss=0.5200 | val_loss=0.5218 | SWA=0.7400 | CWA=0.7359 | HWA=0.7379',\n'\\n', 'Test @ 40 epochs | loss=0.7208 | SWA=0.5958 | CWA=0.6226 | HWA=0.6089',\n'\\n', 'Saved experiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-6/working', '\\n', 'Execution time: 45 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 389555.40\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 483571.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 497149.83\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n',\n'LR=0.0005 | Epoch 1: tr_loss=0.6435, val_loss=0.5899, HWA=0.7139', '\\n',\n'LR=0.0005 | Epoch 2: tr_loss=0.5610, val_loss=0.5416, HWA=0.7358', '\\n',\n'LR=0.0005 | Epoch 3: tr_loss=0.5327, val_loss=0.5266, HWA=0.7462', '\\n',\n'LR=0.0005 | Epoch 4: tr_loss=0.5236, val_loss=0.5219, HWA=0.7286', '\\n',\n'LR=0.0005 | Epoch 5: tr_loss=0.5207, val_loss=0.5215, HWA=0.7297', '\\n',\n'LR=0.0005 | Test: loss=0.7145, HWA=0.6077', '\\n', 'LR=0.001 | Epoch 1:\ntr_loss=0.6088, val_loss=0.5462, HWA=0.7329', '\\n', 'LR=0.001 | Epoch 2:\ntr_loss=0.5302, val_loss=0.5241, HWA=0.7461', '\\n', 'LR=0.001 | Epoch 3:\ntr_loss=0.5209, val_loss=0.5218, HWA=0.7308', '\\n', 'LR=0.001 | Epoch 4:\ntr_loss=0.5199, val_loss=0.5214, HWA=0.7551', '\\n', 'LR=0.001 | Epoch 5:\ntr_loss=0.5200, val_loss=0.5212, HWA=0.7449', '\\n', 'LR=0.001 | Test:\nloss=0.7215, HWA=0.6040', '\\n', 'LR=0.002 | Epoch 1: tr_loss=0.5811,\nval_loss=0.5234, HWA=0.7557', '\\n', 'LR=0.002 | Epoch 2: tr_loss=0.5210,\nval_loss=0.5215, HWA=0.7352', '\\n', 'LR=0.002 | Epoch 3: tr_loss=0.5204,\nval_loss=0.5213, HWA=0.7443', '\\n', 'LR=0.002 | Epoch 4: tr_loss=0.5205,\nval_loss=0.5225, HWA=0.7426', '\\n', 'LR=0.002 | Epoch 5: tr_loss=0.5211,\nval_loss=0.5212, HWA=0.7572', '\\n', 'LR=0.002 | Test: loss=0.7240, HWA=0.6162',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 390802.19\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 708377.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 816218.89\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n',\n'\\n===== Training with batch_size=32 =====', '\\n', 'Epoch 1/5 | train_loss\n0.5488 | val_loss 0.5223 | HWA 0.7558', '\\n', 'Epoch 2/5 | train_loss 0.5205 |\nval_loss 0.5229 | HWA 0.7415', '\\n', 'Epoch 3/5 | train_loss 0.5207 | val_loss\n0.5213 | HWA 0.7438', '\\n', 'Epoch 4/5 | train_loss 0.5208 | val_loss 0.5219 |\nHWA 0.7503', '\\n', 'Epoch 5/5 | train_loss 0.5210 | val_loss 0.5223 | HWA\n0.7455', '\\n', 'Test | loss 0.7173 | HWA 0.6085', '\\n', '\\n===== Training with\nbatch_size=64 =====', '\\n', 'Epoch 1/5 | train_loss 0.5696 | val_loss 0.5250 |\nHWA 0.7368', '\\n', 'Epoch 2/5 | train_loss 0.5214 | val_loss 0.5215 | HWA\n0.7307', '\\n', 'Epoch 3/5 | train_loss 0.5200 | val_loss 0.5221 | HWA 0.7588',\n'\\n', 'Epoch 4/5 | train_loss 0.5202 | val_loss 0.5228 | HWA 0.7507', '\\n',\n'Epoch 5/5 | train_loss 0.5204 | val_loss 0.5213 | HWA 0.7362', '\\n', 'Test |\nloss 0.7226 | HWA 0.6043', '\\n', '\\n===== Training with batch_size=128 =====',\n'\\n', 'Epoch 1/5 | train_loss 0.5897 | val_loss 0.5376 | HWA 0.7485', '\\n',\n'Epoch 2/5 | train_loss 0.5262 | val_loss 0.5224 | HWA 0.7603', '\\n', 'Epoch 3/5\n| train_loss 0.5203 | val_loss 0.5210 | HWA 0.7314', '\\n', 'Epoch 4/5 |\ntrain_loss 0.5198 | val_loss 0.5212 | HWA 0.7535', '\\n', 'Epoch 5/5 | train_loss\n0.5198 | val_loss 0.5220 | HWA 0.7536', '\\n', 'Test | loss 0.7317 | HWA 0.6018',\n'\\n', '\\n===== Training with batch_size=256 =====', '\\n', 'Epoch 1/5 |\ntrain_loss 0.6287 | val_loss 0.5751 | HWA 0.7294', '\\n', 'Epoch 2/5 | train_loss\n0.5521 | val_loss 0.5344 | HWA 0.7371', '\\n', 'Epoch 3/5 | train_loss 0.5272 |\nval_loss 0.5238 | HWA 0.7374', '\\n', 'Epoch 4/5 | train_loss 0.5210 | val_loss\n0.5211 | HWA 0.7422', '\\n', 'Epoch 5/5 | train_loss 0.5197 | val_loss 0.5213 |\nHWA 0.7341', '\\n', 'Test | loss 0.7173 | HWA 0.6094', '\\n', '\\nSaved results to\nworking/experiment_data.npy', '\\n', 'Execution time: 50 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 304808.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 391969.05\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 492433.70\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n',\n'\\n====== Training with embed_dim=32 ======', '\\n', 'epoch 1: train_loss=0.6386\n| val_loss=0.5844 | SWA=0.707 CWA=0.707 HWA=0.707', '\\n', 'epoch 2:\ntrain_loss=0.5602 | val_loss=0.5406 | SWA=0.724 CWA=0.722 HWA=0.723', '\\n',\n'epoch 3: train_loss=0.5312 | val_loss=0.5257 | SWA=0.740 CWA=0.737 HWA=0.738',\n'\\n', 'epoch 4: train_loss=0.5227 | val_loss=0.5223 | SWA=0.746 CWA=0.741\nHWA=0.743', '\\n', 'epoch 5: train_loss=0.5204 | val_loss=0.5209 | SWA=0.744\nCWA=0.738 HWA=0.741', '\\n', 'Test | loss=0.7195 SWA=0.593 CWA=0.620 HWA=0.606',\n'\\n', '\\n====== Training with embed_dim=64 ======', '\\n', 'epoch 1:\ntrain_loss=0.5840 | val_loss=0.5334 | SWA=0.739 CWA=0.735 HWA=0.737', '\\n',\n'epoch 2: train_loss=0.5254 | val_loss=0.5223 | SWA=0.727 CWA=0.724 HWA=0.726',\n'\\n', 'epoch 3: train_loss=0.5206 | val_loss=0.5210 | SWA=0.735 CWA=0.730\nHWA=0.732', '\\n', 'epoch 4: train_loss=0.5200 | val_loss=0.5207 | SWA=0.753\nCWA=0.747 HWA=0.750', '\\n', 'epoch 5: train_loss=0.5199 | val_loss=0.5217 |\nSWA=0.761 CWA=0.755 HWA=0.758', '\\n', 'Test | loss=0.7303 SWA=0.587 CWA=0.612\nHWA=0.599', '\\n', '\\n====== Training with embed_dim=128 ======', '\\n', 'epoch 1:\ntrain_loss=0.5521 | val_loss=0.5219 | SWA=0.743 CWA=0.738 HWA=0.740', '\\n',\n'epoch 2: train_loss=0.5208 | val_loss=0.5224 | SWA=0.754 CWA=0.751 HWA=0.753',\n'\\n', 'epoch 3: train_loss=0.5205 | val_loss=0.5212 | SWA=0.732 CWA=0.729\nHWA=0.730', '\\n', 'epoch 4: train_loss=0.5203 | val_loss=0.5223 | SWA=0.760\nCWA=0.752 HWA=0.756', '\\n', 'epoch 5: train_loss=0.5203 | val_loss=0.5217 |\nSWA=0.765 CWA=0.757 HWA=0.761', '\\n', 'Test | loss=0.7341 SWA=0.579 CWA=0.605\nHWA=0.592', '\\n', '\\n====== Training with embed_dim=256 ======', '\\n', 'epoch 1:\ntrain_loss=0.5483 | val_loss=0.5222 | SWA=0.760 CWA=0.757 HWA=0.758', '\\n',\n'epoch 2: train_loss=0.5209 | val_loss=0.5223 | SWA=0.737 CWA=0.733 HWA=0.735',\n'\\n', 'epoch 3: train_loss=0.5216 | val_loss=0.5227 | SWA=0.766 CWA=0.761\nHWA=0.764', '\\n', 'epoch 4: train_loss=0.5212 | val_loss=0.5229 | SWA=0.763\nCWA=0.758 HWA=0.761', '\\n', 'epoch 5: train_loss=0.5214 | val_loss=0.5221 |\nSWA=0.744 CWA=0.738 HWA=0.741', '\\n', 'Test | loss=0.7206 SWA=0.593 CWA=0.619\nHWA=0.606', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist\n-v2/experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time:\n28 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'decay=0.0 | epoch=1 | tr_loss=0.6015 | val_loss=0.5423 | SWA=0.7336 |\nCWA=0.7311 | HWA=0.7324', '\\n', 'decay=0.0 | epoch=2 | tr_loss=0.5283 |\nval_loss=0.5217 | SWA=0.7359 | CWA=0.7316 | HWA=0.7338', '\\n', 'decay=0.0 |\nepoch=3 | tr_loss=0.5206 | val_loss=0.5212 | SWA=0.7665 | CWA=0.7607 |\nHWA=0.7636', '\\n', 'decay=0.0 | epoch=4 | tr_loss=0.5199 | val_loss=0.5208 |\nSWA=0.7421 | CWA=0.7366 | HWA=0.7393', '\\n', 'decay=0.0 | epoch=5 |\ntr_loss=0.5197 | val_loss=0.5231 | SWA=0.7407 | CWA=0.7375 | HWA=0.7391', '\\n',\n'decay=1e-05 | epoch=1 | tr_loss=0.6108 | val_loss=0.5445 | SWA=0.7364 |\nCWA=0.7327 | HWA=0.7345', '\\n', 'decay=1e-05 | epoch=2 | tr_loss=0.5290 |\nval_loss=0.5229 | SWA=0.7415 | CWA=0.7367 | HWA=0.7391', '\\n', 'decay=1e-05 |\nepoch=3 | tr_loss=0.5208 | val_loss=0.5211 | SWA=0.7412 | CWA=0.7363 |\nHWA=0.7387', '\\n', 'decay=1e-05 | epoch=4 | tr_loss=0.5197 | val_loss=0.5223 |\nSWA=0.7650 | CWA=0.7601 | HWA=0.7626', '\\n', 'decay=1e-05 | epoch=5 |\ntr_loss=0.5198 | val_loss=0.5210 | SWA=0.7457 | CWA=0.7392 | HWA=0.7424', '\\n',\n'decay=0.0001 | epoch=1 | tr_loss=0.6232 | val_loss=0.5497 | SWA=0.7372 |\nCWA=0.7341 | HWA=0.7356', '\\n', 'decay=0.0001 | epoch=2 | tr_loss=0.5314 |\nval_loss=0.5230 | SWA=0.7407 | CWA=0.7361 | HWA=0.7384', '\\n', 'decay=0.0001 |\nepoch=3 | tr_loss=0.5207 | val_loss=0.5219 | SWA=0.7628 | CWA=0.7576 |\nHWA=0.7602', '\\n', 'decay=0.0001 | epoch=4 | tr_loss=0.5198 | val_loss=0.5207 |\nSWA=0.7456 | CWA=0.7403 | HWA=0.7429', '\\n', 'decay=0.0001 | epoch=5 |\ntr_loss=0.5199 | val_loss=0.5208 | SWA=0.7466 | CWA=0.7406 | HWA=0.7436', '\\n',\n'decay=0.001 | epoch=1 | tr_loss=0.5992 | val_loss=0.5427 | SWA=0.7322 |\nCWA=0.7297 | HWA=0.7310', '\\n', 'decay=0.001 | epoch=2 | tr_loss=0.5308 |\nval_loss=0.5260 | SWA=0.7469 | CWA=0.7419 | HWA=0.7444', '\\n', 'decay=0.001 |\nepoch=3 | tr_loss=0.5228 | val_loss=0.5223 | SWA=0.7408 | CWA=0.7348 |\nHWA=0.7378', '\\n', 'decay=0.001 | epoch=4 | tr_loss=0.5211 | val_loss=0.5216 |\nSWA=0.7456 | CWA=0.7394 | HWA=0.7425', '\\n', 'decay=0.001 | epoch=5 |\ntr_loss=0.5208 | val_loss=0.5211 | SWA=0.7442 | CWA=0.7382 | HWA=0.7412', '\\n',\n'Best weight_decay=0.0001 with dev HWA=0.7436', '\\n', 'Test (best model):\nloss=0.7255 | SWA=0.5945 | CWA=0.6198 | HWA=0.6069', '\\n', 'Execution time: 10\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', '[p=0.0] Epoch 1: train_loss=0.6015 | val_loss=0.5423 | SWA=0.7336 |\nCWA=0.7311 | HWA=0.7324', '\\n', '[p=0.0] Epoch 2: train_loss=0.5283 |\nval_loss=0.5217 | SWA=0.7359 | CWA=0.7316 | HWA=0.7338', '\\n', '[p=0.0] Epoch 3:\ntrain_loss=0.5206 | val_loss=0.5212 | SWA=0.7665 | CWA=0.7607 | HWA=0.7636',\n'\\n', '[p=0.0] Epoch 4: train_loss=0.5199 | val_loss=0.5208 | SWA=0.7421 |\nCWA=0.7366 | HWA=0.7393', '\\n', '[p=0.0] Epoch 5: train_loss=0.5197 |\nval_loss=0.5231 | SWA=0.7407 | CWA=0.7375 | HWA=0.7391', '\\n', '[p=0.0] Test:\nloss=0.7272 | SWA=0.5929 | CWA=0.6193 | HWA=0.6058', '\\n', '[p=0.1] Epoch 1:\ntrain_loss=0.6117 | val_loss=0.5478 | SWA=0.7351 | CWA=0.7300 | HWA=0.7326',\n'\\n', '[p=0.1] Epoch 2: train_loss=0.5358 | val_loss=0.5248 | SWA=0.7461 |\nCWA=0.7411 | HWA=0.7436', '\\n', '[p=0.1] Epoch 3: train_loss=0.5260 |\nval_loss=0.5219 | SWA=0.7312 | CWA=0.7269 | HWA=0.7290', '\\n', '[p=0.1] Epoch 4:\ntrain_loss=0.5262 | val_loss=0.5214 | SWA=0.7578 | CWA=0.7529 | HWA=0.7554',\n'\\n', '[p=0.1] Epoch 5: train_loss=0.5262 | val_loss=0.5213 | SWA=0.7510 |\nCWA=0.7446 | HWA=0.7478', '\\n', '[p=0.1] Test: loss=0.7181 | SWA=0.5904 |\nCWA=0.6164 | HWA=0.6031', '\\n', '[p=0.3] Epoch 1: train_loss=0.6403 |\nval_loss=0.5642 | SWA=0.7333 | CWA=0.7294 | HWA=0.7314', '\\n', '[p=0.3] Epoch 2:\ntrain_loss=0.5561 | val_loss=0.5281 | SWA=0.7480 | CWA=0.7426 | HWA=0.7453',\n'\\n', '[p=0.3] Epoch 3: train_loss=0.5399 | val_loss=0.5230 | SWA=0.7446 |\nCWA=0.7395 | HWA=0.7420', '\\n', '[p=0.3] Epoch 4: train_loss=0.5359 |\nval_loss=0.5220 | SWA=0.7454 | CWA=0.7400 | HWA=0.7427', '\\n', '[p=0.3] Epoch 5:\ntrain_loss=0.5330 | val_loss=0.5210 | SWA=0.7496 | CWA=0.7446 | HWA=0.7471',\n'\\n', '[p=0.3] Test: loss=0.7071 | SWA=0.5977 | CWA=0.6242 | HWA=0.6106', '\\n',\n'[p=0.5] Epoch 1: train_loss=0.6225 | val_loss=0.5496 | SWA=0.7354 | CWA=0.7327\n| HWA=0.7340', '\\n', '[p=0.5] Epoch 2: train_loss=0.5545 | val_loss=0.5279 |\nSWA=0.7370 | CWA=0.7320 | HWA=0.7345', '\\n', '[p=0.5] Epoch 3: train_loss=0.5474\n| val_loss=0.5239 | SWA=0.7357 | CWA=0.7314 | HWA=0.7335', '\\n', '[p=0.5] Epoch\n4: train_loss=0.5426 | val_loss=0.5227 | SWA=0.7290 | CWA=0.7241 | HWA=0.7265',\n'\\n', '[p=0.5] Epoch 5: train_loss=0.5431 | val_loss=0.5230 | SWA=0.7460 |\nCWA=0.7414 | HWA=0.7437', '\\n', '[p=0.5] Test: loss=0.7047 | SWA=0.5945 |\nCWA=0.6202 | HWA=0.6071', '\\n', 'Execution time: 14 seconds seconds (time limit\nis 30 minutes).']", "['Using device cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', '\\n===== Training with label_smoothing=0.00 =====', '\\n', 'Epoch 1/5:\nval_loss=0.5423 | HWA=0.7324', '\\n', 'Epoch 2/5: val_loss=0.5217 | HWA=0.7338',\n'\\n', 'Epoch 3/5: val_loss=0.5212 | HWA=0.7636', '\\n', 'Epoch 4/5:\nval_loss=0.5208 | HWA=0.7393', '\\n', 'Epoch 5/5: val_loss=0.5231 | HWA=0.7391',\n'\\n', '\\n===== Training with label_smoothing=0.05 =====', '\\n', 'Epoch 1/5:\nval_loss=0.5585 | HWA=0.7333', '\\n', 'Epoch 2/5: val_loss=0.5419 | HWA=0.7401',\n'\\n', 'Epoch 3/5: val_loss=0.5409 | HWA=0.7406', '\\n', 'Epoch 4/5:\nval_loss=0.5420 | HWA=0.7621', '\\n', 'Epoch 5/5: val_loss=0.5408 | HWA=0.7411',\n'\\n', '\\n===== Training with label_smoothing=0.10 =====', '\\n', 'Epoch 1/5:\nval_loss=0.5749 | HWA=0.7382', '\\n', 'Epoch 2/5: val_loss=0.5593 | HWA=0.7394',\n'\\n', 'Epoch 3/5: val_loss=0.5598 | HWA=0.7638', '\\n', 'Epoch 4/5:\nval_loss=0.5587 | HWA=0.7539', '\\n', 'Epoch 5/5: val_loss=0.5587 | HWA=0.7419',\n'\\n', '\\n===== Training with label_smoothing=0.15 =====', '\\n', 'Epoch 1/5:\nval_loss=0.5802 | HWA=0.7291', '\\n', 'Epoch 2/5: val_loss=0.5761 | HWA=0.7500',\n'\\n', 'Epoch 3/5: val_loss=0.5751 | HWA=0.7445', '\\n', 'Epoch 4/5:\nval_loss=0.5756 | HWA=0.7533', '\\n', 'Epoch 5/5: val_loss=0.5751 | HWA=0.7443',\n'\\n', '\\n===== Training with label_smoothing=0.20 =====', '\\n', 'Epoch 1/5:\nval_loss=0.6006 | HWA=0.7280', '\\n', 'Epoch 2/5: val_loss=0.5904 | HWA=0.7408',\n'\\n', 'Epoch 3/5: val_loss=0.5912 | HWA=0.7651', '\\n', 'Epoch 4/5:\nval_loss=0.5906 | HWA=0.7586', '\\n', 'Epoch 5/5: val_loss=0.5909 | HWA=0.7487',\n'\\n', '\\nBest smoothing=0.20 with dev HWA=0.7487', '\\n', 'TEST | loss=0.7040 |\nSWA=0.5902 | CWA=0.6159 | HWA=0.6028', '\\n', 'Execution time: 55 seconds seconds\n(time limit is 30 minutes).']", "['clip=0 | epoch=1 | train_loss=0.6015 | val_loss=0.5423 | SWA=0.7336 |\nCWA=0.7311 | HWA=0.7324', '\\n', 'clip=0 | epoch=2 | train_loss=0.5283 |\nval_loss=0.5217 | SWA=0.7359 | CWA=0.7316 | HWA=0.7338', '\\n', 'clip=0 | epoch=3\n| train_loss=0.5206 | val_loss=0.5212 | SWA=0.7665 | CWA=0.7607 | HWA=0.7636',\n'\\n', 'clip=0 | epoch=4 | train_loss=0.5199 | val_loss=0.5208 | SWA=0.7421 |\nCWA=0.7366 | HWA=0.7393', '\\n', 'clip=0 | epoch=5 | train_loss=0.5197 |\nval_loss=0.5231 | SWA=0.7407 | CWA=0.7375 | HWA=0.7391', '\\n', '[clip 0] Test |\nloss=0.7272 | SWA=0.5929 | CWA=0.6193 | HWA=0.6058', '\\n', 'clip=0.5 | epoch=1 |\ntrain_loss=0.6013 | val_loss=0.5421 | SWA=0.7336 | CWA=0.7311 | HWA=0.7324',\n'\\n', 'clip=0.5 | epoch=2 | train_loss=0.5282 | val_loss=0.5217 | SWA=0.7359 |\nCWA=0.7317 | HWA=0.7338', '\\n', 'clip=0.5 | epoch=3 | train_loss=0.5206 |\nval_loss=0.5212 | SWA=0.7678 | CWA=0.7620 | HWA=0.7649', '\\n', 'clip=0.5 |\nepoch=4 | train_loss=0.5198 | val_loss=0.5208 | SWA=0.7423 | CWA=0.7367 |\nHWA=0.7395', '\\n', 'clip=0.5 | epoch=5 | train_loss=0.5197 | val_loss=0.5231 |\nSWA=0.7409 | CWA=0.7377 | HWA=0.7393', '\\n', '[clip 0.5] Test | loss=0.7272 |\nSWA=0.5929 | CWA=0.6192 | HWA=0.6058', '\\n', 'clip=1 | epoch=1 |\ntrain_loss=0.6015 | val_loss=0.5423 | SWA=0.7336 | CWA=0.7311 | HWA=0.7324',\n'\\n', 'clip=1 | epoch=2 | train_loss=0.5283 | val_loss=0.5217 | SWA=0.7359 |\nCWA=0.7316 | HWA=0.7338', '\\n', 'clip=1 | epoch=3 | train_loss=0.5206 |\nval_loss=0.5212 | SWA=0.7665 | CWA=0.7607 | HWA=0.7636', '\\n', 'clip=1 | epoch=4\n| train_loss=0.5199 | val_loss=0.5208 | SWA=0.7421 | CWA=0.7366 | HWA=0.7393',\n'\\n', 'clip=1 | epoch=5 | train_loss=0.5197 | val_loss=0.5231 | SWA=0.7407 |\nCWA=0.7375 | HWA=0.7391', '\\n', '[clip 1] Test | loss=0.7272 | SWA=0.5929 |\nCWA=0.6193 | HWA=0.6058', '\\n', 'clip=2 | epoch=1 | train_loss=0.6015 |\nval_loss=0.5423 | SWA=0.7336 | CWA=0.7311 | HWA=0.7324', '\\n', 'clip=2 | epoch=2\n| train_loss=0.5283 | val_loss=0.5217 | SWA=0.7359 | CWA=0.7316 | HWA=0.7338',\n'\\n', 'clip=2 | epoch=3 | train_loss=0.5206 | val_loss=0.5212 | SWA=0.7665 |\nCWA=0.7607 | HWA=0.7636', '\\n', 'clip=2 | epoch=4 | train_loss=0.5199 |\nval_loss=0.5208 | SWA=0.7421 | CWA=0.7366 | HWA=0.7393', '\\n', 'clip=2 | epoch=5\n| train_loss=0.5197 | val_loss=0.5231 | SWA=0.7407 | CWA=0.7375 | HWA=0.7391',\n'\\n', '[clip 2] Test | loss=0.7272 | SWA=0.5929 | CWA=0.6193 | HWA=0.6058',\n'\\n', 'clip=5 | epoch=1 | train_loss=0.6015 | val_loss=0.5423 | SWA=0.7336 |\nCWA=0.7311 | HWA=0.7324', '\\n', 'clip=5 | epoch=2 | train_loss=0.5283 |\nval_loss=0.5217 | SWA=0.7359 | CWA=0.7316 | HWA=0.7338', '\\n', 'clip=5 | epoch=3\n| train_loss=0.5206 | val_loss=0.5212 | SWA=0.7665 | CWA=0.7607 | HWA=0.7636',\n'\\n', 'clip=5 | epoch=4 | train_loss=0.5199 | val_loss=0.5208 | SWA=0.7421 |\nCWA=0.7366 | HWA=0.7393', '\\n', 'clip=5 | epoch=5 | train_loss=0.5197 |\nval_loss=0.5231 | SWA=0.7407 | CWA=0.7375 | HWA=0.7391', '\\n', '[clip 5] Test |\nloss=0.7272 | SWA=0.5929 | CWA=0.6193 | HWA=0.6058', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 55 seconds seconds (time limit is\n30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test':\n10000}\", '\\n', '\\n--- Training model for 5 epochs ---', '\\n', 'Epoch 1/5 |\ntrain_loss=0.5788 | val_loss=0.5327 | SWA=0.7301 | CWA=0.7259 | HWA=0.7280',\n'\\n', 'Epoch 2/5 | train_loss=0.5238 | val_loss=0.5220 | SWA=0.7352 | CWA=0.7311\n| HWA=0.7331', '\\n', 'Epoch 3/5 | train_loss=0.5201 | val_loss=0.5211 |\nSWA=0.7470 | CWA=0.7414 | HWA=0.7442', '\\n', 'Epoch 4/5 | train_loss=0.5200 |\nval_loss=0.5211 | SWA=0.7500 | CWA=0.7441 | HWA=0.7471', '\\n', 'Epoch 5/5 |\ntrain_loss=0.5199 | val_loss=0.5209 | SWA=0.7451 | CWA=0.7398 | HWA=0.7425',\n'\\n', 'Test @ 5 epochs | loss=0.7254 | SWA=0.5928 | CWA=0.6193 | HWA=0.6058',\n'\\n', '\\n--- Training model for 15 epochs ---', '\\n', 'Epoch 1/15 |\ntrain_loss=0.6021 | val_loss=0.5374 | SWA=0.7445 | CWA=0.7403 | HWA=0.7424',\n'\\n', 'Epoch 2/15 | train_loss=0.5260 | val_loss=0.5217 | SWA=0.7326 |\nCWA=0.7291 | HWA=0.7309', '\\n', 'Epoch 3/15 | train_loss=0.5205 |\nval_loss=0.5217 | SWA=0.7403 | CWA=0.7358 | HWA=0.7380', '\\n', 'Epoch 4/15 |\ntrain_loss=0.5198 | val_loss=0.5219 | SWA=0.7314 | CWA=0.7281 | HWA=0.7297',\n'\\n', 'Epoch 5/15 | train_loss=0.5198 | val_loss=0.5212 | SWA=0.7462 |\nCWA=0.7409 | HWA=0.7435', '\\n', 'Epoch 6/15 | train_loss=0.5199 |\nval_loss=0.5212 | SWA=0.7421 | CWA=0.7377 | HWA=0.7399', '\\n', 'Epoch 7/15 |\ntrain_loss=0.5199 | val_loss=0.5220 | SWA=0.7674 | CWA=0.7612 | HWA=0.7643',\n'\\n', 'Epoch 8/15 | train_loss=0.5198 | val_loss=0.5211 | SWA=0.7434 |\nCWA=0.7378 | HWA=0.7406', '\\n', 'Epoch 9/15 | train_loss=0.5198 |\nval_loss=0.5232 | SWA=0.7756 | CWA=0.7723 | HWA=0.7739', '\\n', 'Epoch 10/15 |\ntrain_loss=0.5201 | val_loss=0.5213 | SWA=0.7415 | CWA=0.7367 | HWA=0.7391',\n'\\n', 'Epoch 11/15 | train_loss=0.5199 | val_loss=0.5214 | SWA=0.7500 |\nCWA=0.7449 | HWA=0.7474', '\\n', 'Epoch 12/15 | train_loss=0.5199 |\nval_loss=0.5212 | SWA=0.7577 | CWA=0.7516 | HWA=0.7546', '\\n', 'Epoch 13/15 |\ntrain_loss=0.5200 | val_loss=0.5211 | SWA=0.7556 | CWA=0.7491 | HWA=0.7523',\n'\\n', 'Epoch 14/15 | train_loss=0.5199 | val_loss=0.5212 | SWA=0.7428 |\nCWA=0.7378 | HWA=0.7403', '\\n', 'Epoch 15/15 | train_loss=0.5197 |\nval_loss=0.5212 | SWA=0.7613 | CWA=0.7552 | HWA=0.7583', '\\n', 'Test @ 15 epochs\n| loss=0.7334 | SWA=0.5861 | CWA=0.6116 | HWA=0.5986', '\\n', '\\n--- Training\nmodel for 25 epochs ---', '\\n', 'Epoch 1/25 | train_loss=0.5913 |\nval_loss=0.5379 | SWA=0.7457 | CWA=0.7400 | HWA=0.7428', '\\n', 'Epoch 2/25 |\ntrain_loss=0.5269 | val_loss=0.5222 | SWA=0.7475 | CWA=0.7411 | HWA=0.7443',\n'\\n', 'Epoch 3/25 | train_loss=0.5204 | val_loss=0.5206 | SWA=0.7410 |\nCWA=0.7371 | HWA=0.7390', '\\n', 'Epoch 4/25 | train_loss=0.5198 |\nval_loss=0.5216 | SWA=0.7528 | CWA=0.7463 | HWA=0.7495', '\\n', 'Epoch 5/25 |\ntrain_loss=0.5200 | val_loss=0.5210 | SWA=0.7506 | CWA=0.7439 | HWA=0.7473',\n'\\n', 'Epoch 6/25 | train_loss=0.5199 | val_loss=0.5216 | SWA=0.7521 |\nCWA=0.7470 | HWA=0.7495', '\\n', 'Epoch 7/25 | train_loss=0.5198 |\nval_loss=0.5211 | SWA=0.7483 | CWA=0.7417 | HWA=0.7450', '\\n', 'Epoch 8/25 |\ntrain_loss=0.5195 | val_loss=0.5209 | SWA=0.7537 | CWA=0.7487 | HWA=0.7512',\n'\\n', 'Epoch 9/25 | train_loss=0.5200 | val_loss=0.5214 | SWA=0.7548 |\nCWA=0.7492 | HWA=0.7520', '\\n', 'Epoch 10/25 | train_loss=0.5200 |\nval_loss=0.5213 | SWA=0.7509 | CWA=0.7441 | HWA=0.7475', '\\n', 'Epoch 11/25 |\ntrain_loss=0.5197 | val_loss=0.5228 | SWA=0.7732 | CWA=0.7694 | HWA=0.7713',\n'\\n', 'Epoch 12/25 | train_loss=0.5198 | val_loss=0.5213 | SWA=0.7558 |\nCWA=0.7485 | HWA=0.7521', '\\n', 'Epoch 13/25 | train_loss=0.5198 |\nval_loss=0.5216 | SWA=0.7521 | CWA=0.7466 | HWA=0.7493', '\\n', 'Epoch 14/25 |\ntrain_loss=0.5199 | val_loss=0.5220 | SWA=0.7625 | CWA=0.7575 | HWA=0.7600',\n'\\n', 'Epoch 15/25 | train_loss=0.5199 | val_loss=0.5206 | SWA=0.7393 |\nCWA=0.7344 | HWA=0.7368', '\\n', 'Epoch 16/25 | train_loss=0.5198 |\nval_loss=0.5211 | SWA=0.7472 | CWA=0.7425 | HWA=0.7449', '\\n', 'Epoch 17/25 |\ntrain_loss=0.5197 | val_loss=0.5209 | SWA=0.7499 | CWA=0.7447 | HWA=0.7473',\n'\\n', 'Epoch 18/25 | train_loss=0.5198 | val_loss=0.5230 | SWA=0.7725 |\nCWA=0.7692 | HWA=0.7709', '\\n', 'Epoch 19/25 | train_loss=0.5199 |\nval_loss=0.5217 | SWA=0.7600 | CWA=0.7545 | HWA=0.7573', '\\n', 'Epoch 20/25 |\ntrain_loss=0.5196 | val_loss=0.5219 | SWA=0.7372 | CWA=0.7322 | HWA=0.7347',\n'\\n', 'Epoch 21/25 | train_loss=0.5198 | val_loss=0.5211 | SWA=0.7401 |\nCWA=0.7347 | HWA=0.7374', '\\n', 'Epoch 22/25 | train_loss=0.5197 |\nval_loss=0.5214 | SWA=0.7461 | CWA=0.7422 | HWA=0.7442', '\\n', 'Epoch 23/25 |\ntrain_loss=0.5197 | val_loss=0.5217 | SWA=0.7637 | CWA=0.7586 | HWA=0.7612',\n'\\n', 'Epoch 24/25 | train_loss=0.5199 | val_loss=0.5210 | SWA=0.7405 |\nCWA=0.7357 | HWA=0.7381', '\\n', 'Epoch 25/25 | train_loss=0.5196 |\nval_loss=0.5215 | SWA=0.7413 | CWA=0.7356 | HWA=0.7385', '\\n', 'Test @ 25 epochs\n| loss=0.7261 | SWA=0.5904 | CWA=0.6162 | HWA=0.6030', '\\n', '\\n--- Training\nmodel for 40 epochs ---', '\\n', 'Epoch 1/40 | train_loss=0.6075 |\nval_loss=0.5443 | SWA=0.7347 | CWA=0.7337 | HWA=0.7342', '\\n', 'Epoch 2/40 |\ntrain_loss=0.5282 | val_loss=0.5225 | SWA=0.7371 | CWA=0.7329 | HWA=0.7350',\n'\\n', 'Epoch 3/40 | train_loss=0.5206 | val_loss=0.5212 | SWA=0.7374 |\nCWA=0.7328 | HWA=0.7351', '\\n', 'Epoch 4/40 | train_loss=0.5197 |\nval_loss=0.5211 | SWA=0.7554 | CWA=0.7499 | HWA=0.7526', '\\n', 'Epoch 5/40 |\ntrain_loss=0.5198 | val_loss=0.5217 | SWA=0.7589 | CWA=0.7549 | HWA=0.7569',\n'\\n', 'Epoch 6/40 | train_loss=0.5197 | val_loss=0.5212 | SWA=0.7362 |\nCWA=0.7312 | HWA=0.7337', '\\n', 'Epoch 7/40 | train_loss=0.5198 |\nval_loss=0.5212 | SWA=0.7585 | CWA=0.7527 | HWA=0.7556', '\\n', 'Epoch 8/40 |\ntrain_loss=0.5202 | val_loss=0.5210 | SWA=0.7551 | CWA=0.7497 | HWA=0.7524',\n'\\n', 'Epoch 9/40 | train_loss=0.5198 | val_loss=0.5213 | SWA=0.7347 |\nCWA=0.7303 | HWA=0.7325', '\\n', 'Epoch 10/40 | train_loss=0.5199 |\nval_loss=0.5213 | SWA=0.7351 | CWA=0.7310 | HWA=0.7330', '\\n', 'Epoch 11/40 |\ntrain_loss=0.5199 | val_loss=0.5216 | SWA=0.7495 | CWA=0.7440 | HWA=0.7467',\n'\\n', 'Epoch 12/40 | train_loss=0.5198 | val_loss=0.5222 | SWA=0.7667 |\nCWA=0.7619 | HWA=0.7643', '\\n', 'Epoch 13/40 | train_loss=0.5199 |\nval_loss=0.5218 | SWA=0.7377 | CWA=0.7337 | HWA=0.7357', '\\n', 'Epoch 14/40 |\ntrain_loss=0.5201 | val_loss=0.5214 | SWA=0.7395 | CWA=0.7347 | HWA=0.7371',\n'\\n', 'Epoch 15/40 | train_loss=0.5198 | val_loss=0.5213 | SWA=0.7447 |\nCWA=0.7394 | HWA=0.7420', '\\n', 'Epoch 16/40 | train_loss=0.5198 |\nval_loss=0.5213 | SWA=0.7451 | CWA=0.7392 | HWA=0.7422', '\\n', 'Epoch 17/40 |\ntrain_loss=0.5199 | val_loss=0.5216 | SWA=0.7333 | CWA=0.7295 | HWA=0.7314',\n'\\n', 'Epoch 18/40 | train_loss=0.5199 | val_loss=0.5216 | SWA=0.7437 |\nCWA=0.7378 | HWA=0.7407', '\\n', 'Epoch 19/40 | train_loss=0.5196 |\nval_loss=0.5222 | SWA=0.7381 | CWA=0.7336 | HWA=0.7358', '\\n', 'Epoch 20/40 |\ntrain_loss=0.5200 | val_loss=0.5215 | SWA=0.7551 | CWA=0.7494 | HWA=0.7522',\n'\\n', 'Epoch 21/40 | train_loss=0.5197 | val_loss=0.5213 | SWA=0.7469 |\nCWA=0.7414 | HWA=0.7442', '\\n', 'Epoch 22/40 | train_loss=0.5199 |\nval_loss=0.5220 | SWA=0.7440 | CWA=0.7402 | HWA=0.7421', '\\n', 'Epoch 23/40 |\ntrain_loss=0.5198 | val_loss=0.5218 | SWA=0.7407 | CWA=0.7360 | HWA=0.7383',\n'\\n', 'Epoch 24/40 | train_loss=0.5198 | val_loss=0.5209 | SWA=0.7432 |\nCWA=0.7373 | HWA=0.7402', '\\n', 'Epoch 25/40 | train_loss=0.5198 |\nval_loss=0.5216 | SWA=0.7425 | CWA=0.7386 | HWA=0.7406', '\\n', 'Epoch 26/40 |\ntrain_loss=0.5199 | val_loss=0.5215 | SWA=0.7496 | CWA=0.7446 | HWA=0.7471',\n'\\n', 'Epoch 27/40 | train_loss=0.5201 | val_loss=0.5215 | SWA=0.7566 |\nCWA=0.7514 | HWA=0.7540', '\\n', 'Epoch 28/40 | train_loss=0.5199 |\nval_loss=0.5211 | SWA=0.7536 | CWA=0.7488 | HWA=0.7512', '\\n', 'Epoch 29/40 |\ntrain_loss=0.5200 | val_loss=0.5215 | SWA=0.7532 | CWA=0.7472 | HWA=0.7502',\n'\\n', 'Epoch 30/40 | train_loss=0.5198 | val_loss=0.5208 | SWA=0.7395 |\nCWA=0.7336 | HWA=0.7366', '\\n', 'Epoch 31/40 | train_loss=0.5198 |\nval_loss=0.5211 | SWA=0.7364 | CWA=0.7318 | HWA=0.7341', '\\n', 'Epoch 32/40 |\ntrain_loss=0.5198 | val_loss=0.5218 | SWA=0.7520 | CWA=0.7482 | HWA=0.7501',\n'\\n', 'Epoch 33/40 | train_loss=0.5199 | val_loss=0.5219 | SWA=0.7643 |\nCWA=0.7582 | HWA=0.7613', '\\n', 'Epoch 34/40 | train_loss=0.5201 |\nval_loss=0.5217 | SWA=0.7639 | CWA=0.7582 | HWA=0.7610', '\\n', 'Epoch 35/40 |\ntrain_loss=0.5199 | val_loss=0.5214 | SWA=0.7399 | CWA=0.7352 | HWA=0.7375',\n'\\n', 'Epoch 36/40 | train_loss=0.5199 | val_loss=0.5212 | SWA=0.7610 |\nCWA=0.7555 | HWA=0.7583', '\\n', 'Epoch 37/40 | train_loss=0.5199 |\nval_loss=0.5217 | SWA=0.7524 | CWA=0.7468 | HWA=0.7496', '\\n', 'Epoch 38/40 |\ntrain_loss=0.5200 | val_loss=0.5215 | SWA=0.7364 | CWA=0.7317 | HWA=0.7340',\n'\\n', 'Epoch 39/40 | train_loss=0.5199 | val_loss=0.5216 | SWA=0.7423 |\nCWA=0.7380 | HWA=0.7402', '\\n', 'Epoch 40/40 | train_loss=0.5198 |\nval_loss=0.5207 | SWA=0.7488 | CWA=0.7436 | HWA=0.7462', '\\n', 'Test @ 40 epochs\n| loss=0.7227 | SWA=0.5939 | CWA=0.6197 | HWA=0.6065', '\\n', 'Saved\nexperiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-9/working', '\\n', 'Execution time: 45 seconds seconds\n(time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test':\n10000}\", '\\n', '\\n--- Training model for 5 epochs ---', '\\n', 'Epoch 1/5 |\ntrain_loss=0.6176 | val_loss=0.5480 | SWA=0.7468 | CWA=0.7449 | HWA=0.7458',\n'\\n', 'Epoch 2/5 | train_loss=0.5308 | val_loss=0.5236 | SWA=0.7537 | CWA=0.7503\n| HWA=0.7520', '\\n', 'Epoch 3/5 | train_loss=0.5207 | val_loss=0.5210 |\nSWA=0.7450 | CWA=0.7409 | HWA=0.7429', '\\n', 'Epoch 4/5 | train_loss=0.5200 |\nval_loss=0.5212 | SWA=0.7408 | CWA=0.7374 | HWA=0.7391', '\\n', 'Epoch 5/5 |\ntrain_loss=0.5198 | val_loss=0.5220 | SWA=0.7454 | CWA=0.7410 | HWA=0.7432',\n'\\n', 'Test @ 5 epochs | loss=0.7211 | SWA=0.5913 | CWA=0.6181 | HWA=0.6044',\n'\\n', '\\n--- Training model for 15 epochs ---', '\\n', 'Epoch 1/15 |\ntrain_loss=0.5950 | val_loss=0.5409 | SWA=0.7329 | CWA=0.7303 | HWA=0.7316',\n'\\n', 'Epoch 2/15 | train_loss=0.5288 | val_loss=0.5227 | SWA=0.7406 |\nCWA=0.7352 | HWA=0.7379', '\\n', 'Epoch 3/15 | train_loss=0.5209 |\nval_loss=0.5214 | SWA=0.7410 | CWA=0.7355 | HWA=0.7382', '\\n', 'Epoch 4/15 |\ntrain_loss=0.5203 | val_loss=0.5210 | SWA=0.7393 | CWA=0.7351 | HWA=0.7372',\n'\\n', 'Epoch 5/15 | train_loss=0.5199 | val_loss=0.5214 | SWA=0.7526 |\nCWA=0.7477 | HWA=0.7502', '\\n', 'Epoch 6/15 | train_loss=0.5200 |\nval_loss=0.5212 | SWA=0.7610 | CWA=0.7557 | HWA=0.7584', '\\n', 'Epoch 7/15 |\ntrain_loss=0.5200 | val_loss=0.5212 | SWA=0.7521 | CWA=0.7458 | HWA=0.7489',\n'\\n', 'Epoch 8/15 | train_loss=0.5199 | val_loss=0.5213 | SWA=0.7518 |\nCWA=0.7469 | HWA=0.7493', '\\n', 'Epoch 9/15 | train_loss=0.5199 |\nval_loss=0.5225 | SWA=0.7613 | CWA=0.7565 | HWA=0.7589', '\\n', 'Epoch 10/15 |\ntrain_loss=0.5199 | val_loss=0.5211 | SWA=0.7554 | CWA=0.7491 | HWA=0.7522',\n'\\n', 'Epoch 11/15 | train_loss=0.5199 | val_loss=0.5213 | SWA=0.7494 |\nCWA=0.7455 | HWA=0.7475', '\\n', 'Epoch 12/15 | train_loss=0.5198 |\nval_loss=0.5210 | SWA=0.7610 | CWA=0.7555 | HWA=0.7582', '\\n', 'Epoch 13/15 |\ntrain_loss=0.5198 | val_loss=0.5229 | SWA=0.7574 | CWA=0.7532 | HWA=0.7553',\n'\\n', 'Epoch 14/15 | train_loss=0.5198 | val_loss=0.5215 | SWA=0.7445 |\nCWA=0.7396 | HWA=0.7420', '\\n', 'Epoch 15/15 | train_loss=0.5199 |\nval_loss=0.5226 | SWA=0.7604 | CWA=0.7555 | HWA=0.7579', '\\n', 'Test @ 15 epochs\n| loss=0.7336 | SWA=0.5870 | CWA=0.6131 | HWA=0.5997', '\\n', '\\n--- Training\nmodel for 25 epochs ---', '\\n', 'Epoch 1/25 | train_loss=0.5998 |\nval_loss=0.5379 | SWA=0.7390 | CWA=0.7355 | HWA=0.7373', '\\n', 'Epoch 2/25 |\ntrain_loss=0.5260 | val_loss=0.5228 | SWA=0.7391 | CWA=0.7348 | HWA=0.7369',\n'\\n', 'Epoch 3/25 | train_loss=0.5202 | val_loss=0.5214 | SWA=0.7506 |\nCWA=0.7453 | HWA=0.7480', '\\n', 'Epoch 4/25 | train_loss=0.5198 |\nval_loss=0.5210 | SWA=0.7465 | CWA=0.7413 | HWA=0.7439', '\\n', 'Epoch 5/25 |\ntrain_loss=0.5198 | val_loss=0.5206 | SWA=0.7501 | CWA=0.7433 | HWA=0.7467',\n'\\n', 'Epoch 6/25 | train_loss=0.5198 | val_loss=0.5219 | SWA=0.7452 |\nCWA=0.7410 | HWA=0.7431', '\\n', 'Epoch 7/25 | train_loss=0.5199 |\nval_loss=0.5221 | SWA=0.7624 | CWA=0.7579 | HWA=0.7601', '\\n', 'Epoch 8/25 |\ntrain_loss=0.5197 | val_loss=0.5215 | SWA=0.7528 | CWA=0.7482 | HWA=0.7505',\n'\\n', 'Epoch 9/25 | train_loss=0.5199 | val_loss=0.5211 | SWA=0.7434 |\nCWA=0.7389 | HWA=0.7411', '\\n', 'Epoch 10/25 | train_loss=0.5200 |\nval_loss=0.5214 | SWA=0.7474 | CWA=0.7418 | HWA=0.7446', '\\n', 'Epoch 11/25 |\ntrain_loss=0.5200 | val_loss=0.5214 | SWA=0.7442 | CWA=0.7392 | HWA=0.7417',\n'\\n', 'Epoch 12/25 | train_loss=0.5198 | val_loss=0.5211 | SWA=0.7539 |\nCWA=0.7491 | HWA=0.7515', '\\n', 'Epoch 13/25 | train_loss=0.5198 |\nval_loss=0.5212 | SWA=0.7508 | CWA=0.7450 | HWA=0.7479', '\\n', 'Epoch 14/25 |\ntrain_loss=0.5198 | val_loss=0.5212 | SWA=0.7435 | CWA=0.7375 | HWA=0.7405',\n'\\n', 'Epoch 15/25 | train_loss=0.5199 | val_loss=0.5219 | SWA=0.7314 |\nCWA=0.7278 | HWA=0.7296', '\\n', 'Epoch 16/25 | train_loss=0.5197 |\nval_loss=0.5212 | SWA=0.7441 | CWA=0.7394 | HWA=0.7417', '\\n', 'Epoch 17/25 |\ntrain_loss=0.5198 | val_loss=0.5214 | SWA=0.7517 | CWA=0.7462 | HWA=0.7489',\n'\\n', 'Epoch 18/25 | train_loss=0.5199 | val_loss=0.5220 | SWA=0.7326 |\nCWA=0.7293 | HWA=0.7310', '\\n', 'Epoch 19/25 | train_loss=0.5200 |\nval_loss=0.5219 | SWA=0.7496 | CWA=0.7452 | HWA=0.7474', '\\n', 'Epoch 20/25 |\ntrain_loss=0.5197 | val_loss=0.5215 | SWA=0.7324 | CWA=0.7279 | HWA=0.7301',\n'\\n', 'Epoch 21/25 | train_loss=0.5200 | val_loss=0.5210 | SWA=0.7441 |\nCWA=0.7391 | HWA=0.7416', '\\n', 'Epoch 22/25 | train_loss=0.5200 |\nval_loss=0.5217 | SWA=0.7551 | CWA=0.7514 | HWA=0.7533', '\\n', 'Epoch 23/25 |\ntrain_loss=0.5199 | val_loss=0.5211 | SWA=0.7513 | CWA=0.7463 | HWA=0.7488',\n'\\n', 'Epoch 24/25 | train_loss=0.5197 | val_loss=0.5218 | SWA=0.7609 |\nCWA=0.7561 | HWA=0.7585', '\\n', 'Epoch 25/25 | train_loss=0.5199 |\nval_loss=0.5212 | SWA=0.7600 | CWA=0.7546 | HWA=0.7573', '\\n', 'Test @ 25 epochs\n| loss=0.7270 | SWA=0.5874 | CWA=0.6132 | HWA=0.6000', '\\n', '\\n--- Training\nmodel for 40 epochs ---', '\\n', 'Epoch 1/40 | train_loss=0.5770 |\nval_loss=0.5331 | SWA=0.7411 | CWA=0.7394 | HWA=0.7402', '\\n', 'Epoch 2/40 |\ntrain_loss=0.5249 | val_loss=0.5221 | SWA=0.7362 | CWA=0.7321 | HWA=0.7341',\n'\\n', 'Epoch 3/40 | train_loss=0.5203 | val_loss=0.5216 | SWA=0.7442 |\nCWA=0.7404 | HWA=0.7423', '\\n', 'Epoch 4/40 | train_loss=0.5201 |\nval_loss=0.5217 | SWA=0.7582 | CWA=0.7535 | HWA=0.7558', '\\n', 'Epoch 5/40 |\ntrain_loss=0.5200 | val_loss=0.5214 | SWA=0.7492 | CWA=0.7442 | HWA=0.7467',\n'\\n', 'Epoch 6/40 | train_loss=0.5196 | val_loss=0.5219 | SWA=0.7575 |\nCWA=0.7521 | HWA=0.7548', '\\n', 'Epoch 7/40 | train_loss=0.5197 |\nval_loss=0.5227 | SWA=0.7628 | CWA=0.7589 | HWA=0.7609', '\\n', 'Epoch 8/40 |\ntrain_loss=0.5198 | val_loss=0.5212 | SWA=0.7433 | CWA=0.7387 | HWA=0.7410',\n'\\n', 'Epoch 9/40 | train_loss=0.5199 | val_loss=0.5213 | SWA=0.7529 |\nCWA=0.7468 | HWA=0.7499', '\\n', 'Epoch 10/40 | train_loss=0.5200 |\nval_loss=0.5216 | SWA=0.7609 | CWA=0.7553 | HWA=0.7581', '\\n', 'Epoch 11/40 |\ntrain_loss=0.5199 | val_loss=0.5212 | SWA=0.7546 | CWA=0.7488 | HWA=0.7517',\n'\\n', 'Epoch 12/40 | train_loss=0.5197 | val_loss=0.5225 | SWA=0.7715 |\nCWA=0.7679 | HWA=0.7697', '\\n', 'Epoch 13/40 | train_loss=0.5198 |\nval_loss=0.5211 | SWA=0.7431 | CWA=0.7377 | HWA=0.7404', '\\n', 'Epoch 14/40 |\ntrain_loss=0.5199 | val_loss=0.5221 | SWA=0.7651 | CWA=0.7605 | HWA=0.7628',\n'\\n', 'Epoch 15/40 | train_loss=0.5199 | val_loss=0.5211 | SWA=0.7513 |\nCWA=0.7456 | HWA=0.7484', '\\n', 'Epoch 16/40 | train_loss=0.5197 |\nval_loss=0.5222 | SWA=0.7668 | CWA=0.7614 | HWA=0.7641', '\\n', 'Epoch 17/40 |\ntrain_loss=0.5198 | val_loss=0.5212 | SWA=0.7525 | CWA=0.7474 | HWA=0.7500',\n'\\n', 'Epoch 18/40 | train_loss=0.5199 | val_loss=0.5209 | SWA=0.7482 |\nCWA=0.7424 | HWA=0.7453', '\\n', 'Epoch 19/40 | train_loss=0.5198 |\nval_loss=0.5208 | SWA=0.7523 | CWA=0.7476 | HWA=0.7499', '\\n', 'Epoch 20/40 |\ntrain_loss=0.5199 | val_loss=0.5210 | SWA=0.7370 | CWA=0.7325 | HWA=0.7347',\n'\\n', 'Epoch 21/40 | train_loss=0.5198 | val_loss=0.5213 | SWA=0.7441 |\nCWA=0.7388 | HWA=0.7415', '\\n', 'Epoch 22/40 | train_loss=0.5197 |\nval_loss=0.5213 | SWA=0.7382 | CWA=0.7335 | HWA=0.7358', '\\n', 'Epoch 23/40 |\ntrain_loss=0.5199 | val_loss=0.5209 | SWA=0.7390 | CWA=0.7342 | HWA=0.7366',\n'\\n', 'Epoch 24/40 | train_loss=0.5198 | val_loss=0.5214 | SWA=0.7493 |\nCWA=0.7447 | HWA=0.7470', '\\n', 'Epoch 25/40 | train_loss=0.5200 |\nval_loss=0.5215 | SWA=0.7401 | CWA=0.7355 | HWA=0.7378', '\\n', 'Epoch 26/40 |\ntrain_loss=0.5200 | val_loss=0.5222 | SWA=0.7658 | CWA=0.7609 | HWA=0.7633',\n'\\n', 'Epoch 27/40 | train_loss=0.5198 | val_loss=0.5216 | SWA=0.7355 |\nCWA=0.7311 | HWA=0.7333', '\\n', 'Epoch 28/40 | train_loss=0.5198 |\nval_loss=0.5216 | SWA=0.7341 | CWA=0.7299 | HWA=0.7320', '\\n', 'Epoch 29/40 |\ntrain_loss=0.5199 | val_loss=0.5220 | SWA=0.7641 | CWA=0.7583 | HWA=0.7612',\n'\\n', 'Epoch 30/40 | train_loss=0.5198 | val_loss=0.5222 | SWA=0.7338 |\nCWA=0.7305 | HWA=0.7321', '\\n', 'Epoch 31/40 | train_loss=0.5198 |\nval_loss=0.5216 | SWA=0.7445 | CWA=0.7400 | HWA=0.7422', '\\n', 'Epoch 32/40 |\ntrain_loss=0.5199 | val_loss=0.5217 | SWA=0.7540 | CWA=0.7498 | HWA=0.7519',\n'\\n', 'Epoch 33/40 | train_loss=0.5197 | val_loss=0.5219 | SWA=0.7414 |\nCWA=0.7364 | HWA=0.7389', '\\n', 'Epoch 34/40 | train_loss=0.5200 |\nval_loss=0.5216 | SWA=0.7653 | CWA=0.7600 | HWA=0.7626', '\\n', 'Epoch 35/40 |\ntrain_loss=0.5199 | val_loss=0.5211 | SWA=0.7492 | CWA=0.7424 | HWA=0.7458',\n'\\n', 'Epoch 36/40 | train_loss=0.5198 | val_loss=0.5211 | SWA=0.7407 |\nCWA=0.7360 | HWA=0.7383', '\\n', 'Epoch 37/40 | train_loss=0.5197 |\nval_loss=0.5217 | SWA=0.7368 | CWA=0.7319 | HWA=0.7343', '\\n', 'Epoch 38/40 |\ntrain_loss=0.5200 | val_loss=0.5220 | SWA=0.7629 | CWA=0.7596 | HWA=0.7612',\n'\\n', 'Epoch 39/40 | train_loss=0.5199 | val_loss=0.5211 | SWA=0.7546 |\nCWA=0.7482 | HWA=0.7514', '\\n', 'Epoch 40/40 | train_loss=0.5196 |\nval_loss=0.5216 | SWA=0.7619 | CWA=0.7555 | HWA=0.7587', '\\n', 'Test @ 40 epochs\n| loss=0.7324 | SWA=0.5845 | CWA=0.6095 | HWA=0.5967', '\\n', 'Saved\nexperiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-7/working', '\\n', 'Execution time: 45 seconds seconds\n(time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test':\n10000}\", '\\n', '\\n--- Training model for 5 epochs ---', '\\n', 'Epoch 1/5 |\ntrain_loss=0.6049 | val_loss=0.5409 | SWA=0.7422 | CWA=0.7388 | HWA=0.7405',\n'\\n', 'Epoch 2/5 | train_loss=0.5287 | val_loss=0.5230 | SWA=0.7318 | CWA=0.7296\n| HWA=0.7307', '\\n', 'Epoch 3/5 | train_loss=0.5208 | val_loss=0.5207 |\nSWA=0.7335 | CWA=0.7292 | HWA=0.7313', '\\n', 'Epoch 4/5 | train_loss=0.5203 |\nval_loss=0.5212 | SWA=0.7474 | CWA=0.7420 | HWA=0.7447', '\\n', 'Epoch 5/5 |\ntrain_loss=0.5197 | val_loss=0.5214 | SWA=0.7448 | CWA=0.7404 | HWA=0.7426',\n'\\n', 'Test @ 5 epochs | loss=0.7185 | SWA=0.5911 | CWA=0.6177 | HWA=0.6041',\n'\\n', '\\n--- Training model for 15 epochs ---', '\\n', 'Epoch 1/15 |\ntrain_loss=0.6067 | val_loss=0.5466 | SWA=0.7331 | CWA=0.7305 | HWA=0.7318',\n'\\n', 'Epoch 2/15 | train_loss=0.5316 | val_loss=0.5236 | SWA=0.7293 |\nCWA=0.7250 | HWA=0.7271', '\\n', 'Epoch 3/15 | train_loss=0.5213 |\nval_loss=0.5212 | SWA=0.7325 | CWA=0.7287 | HWA=0.7306', '\\n', 'Epoch 4/15 |\ntrain_loss=0.5201 | val_loss=0.5216 | SWA=0.7600 | CWA=0.7557 | HWA=0.7578',\n'\\n', 'Epoch 5/15 | train_loss=0.5200 | val_loss=0.5213 | SWA=0.7424 |\nCWA=0.7373 | HWA=0.7398', '\\n', 'Epoch 6/15 | train_loss=0.5199 |\nval_loss=0.5221 | SWA=0.7417 | CWA=0.7369 | HWA=0.7393', '\\n', 'Epoch 7/15 |\ntrain_loss=0.5201 | val_loss=0.5213 | SWA=0.7528 | CWA=0.7467 | HWA=0.7497',\n'\\n', 'Epoch 8/15 | train_loss=0.5201 | val_loss=0.5214 | SWA=0.7397 |\nCWA=0.7348 | HWA=0.7373', '\\n', 'Epoch 9/15 | train_loss=0.5199 |\nval_loss=0.5214 | SWA=0.7323 | CWA=0.7286 | HWA=0.7304', '\\n', 'Epoch 10/15 |\ntrain_loss=0.5199 | val_loss=0.5219 | SWA=0.7324 | CWA=0.7287 | HWA=0.7305',\n'\\n', 'Epoch 11/15 | train_loss=0.5202 | val_loss=0.5224 | SWA=0.7640 |\nCWA=0.7609 | HWA=0.7625', '\\n', 'Epoch 12/15 | train_loss=0.5197 |\nval_loss=0.5219 | SWA=0.7494 | CWA=0.7438 | HWA=0.7466', '\\n', 'Epoch 13/15 |\ntrain_loss=0.5198 | val_loss=0.5211 | SWA=0.7407 | CWA=0.7355 | HWA=0.7381',\n'\\n', 'Epoch 14/15 | train_loss=0.5199 | val_loss=0.5211 | SWA=0.7479 |\nCWA=0.7424 | HWA=0.7451', '\\n', 'Epoch 15/15 | train_loss=0.5198 |\nval_loss=0.5209 | SWA=0.7429 | CWA=0.7374 | HWA=0.7401', '\\n', 'Test @ 15 epochs\n| loss=0.7240 | SWA=0.5899 | CWA=0.6157 | HWA=0.6025', '\\n', '\\n--- Training\nmodel for 25 epochs ---', '\\n', 'Epoch 1/25 | train_loss=0.5948 |\nval_loss=0.5356 | SWA=0.7358 | CWA=0.7324 | HWA=0.7341', '\\n', 'Epoch 2/25 |\ntrain_loss=0.5263 | val_loss=0.5236 | SWA=0.7610 | CWA=0.7572 | HWA=0.7591',\n'\\n', 'Epoch 3/25 | train_loss=0.5204 | val_loss=0.5213 | SWA=0.7464 |\nCWA=0.7406 | HWA=0.7435', '\\n', 'Epoch 4/25 | train_loss=0.5200 |\nval_loss=0.5213 | SWA=0.7489 | CWA=0.7445 | HWA=0.7467', '\\n', 'Epoch 5/25 |\ntrain_loss=0.5198 | val_loss=0.5218 | SWA=0.7617 | CWA=0.7564 | HWA=0.7590',\n'\\n', 'Epoch 6/25 | train_loss=0.5198 | val_loss=0.5219 | SWA=0.7603 |\nCWA=0.7552 | HWA=0.7577', '\\n', 'Epoch 7/25 | train_loss=0.5198 |\nval_loss=0.5212 | SWA=0.7361 | CWA=0.7315 | HWA=0.7338', '\\n', 'Epoch 8/25 |\ntrain_loss=0.5199 | val_loss=0.5215 | SWA=0.7422 | CWA=0.7372 | HWA=0.7397',\n'\\n', 'Epoch 9/25 | train_loss=0.5199 | val_loss=0.5215 | SWA=0.7372 |\nCWA=0.7324 | HWA=0.7348', '\\n', 'Epoch 10/25 | train_loss=0.5200 |\nval_loss=0.5208 | SWA=0.7501 | CWA=0.7435 | HWA=0.7468', '\\n', 'Epoch 11/25 |\ntrain_loss=0.5199 | val_loss=0.5209 | SWA=0.7417 | CWA=0.7366 | HWA=0.7391',\n'\\n', 'Epoch 12/25 | train_loss=0.5200 | val_loss=0.5219 | SWA=0.7565 |\nCWA=0.7510 | HWA=0.7537', '\\n', 'Epoch 13/25 | train_loss=0.5197 |\nval_loss=0.5217 | SWA=0.7603 | CWA=0.7568 | HWA=0.7585', '\\n', 'Epoch 14/25 |\ntrain_loss=0.5199 | val_loss=0.5220 | SWA=0.7658 | CWA=0.7624 | HWA=0.7641',\n'\\n', 'Epoch 15/25 | train_loss=0.5198 | val_loss=0.5225 | SWA=0.7510 |\nCWA=0.7457 | HWA=0.7484', '\\n', 'Epoch 16/25 | train_loss=0.5198 |\nval_loss=0.5212 | SWA=0.7389 | CWA=0.7341 | HWA=0.7365', '\\n', 'Epoch 17/25 |\ntrain_loss=0.5198 | val_loss=0.5217 | SWA=0.7363 | CWA=0.7316 | HWA=0.7339',\n'\\n', 'Epoch 18/25 | train_loss=0.5197 | val_loss=0.5213 | SWA=0.7453 |\nCWA=0.7407 | HWA=0.7430', '\\n', 'Epoch 19/25 | train_loss=0.5199 |\nval_loss=0.5213 | SWA=0.7422 | CWA=0.7375 | HWA=0.7399', '\\n', 'Epoch 20/25 |\ntrain_loss=0.5199 | val_loss=0.5217 | SWA=0.7616 | CWA=0.7574 | HWA=0.7595',\n'\\n', 'Epoch 21/25 | train_loss=0.5199 | val_loss=0.5210 | SWA=0.7417 |\nCWA=0.7358 | HWA=0.7387', '\\n', 'Epoch 22/25 | train_loss=0.5197 |\nval_loss=0.5209 | SWA=0.7421 | CWA=0.7380 | HWA=0.7401', '\\n', 'Epoch 23/25 |\ntrain_loss=0.5202 | val_loss=0.5220 | SWA=0.7545 | CWA=0.7482 | HWA=0.7513',\n'\\n', 'Epoch 24/25 | train_loss=0.5200 | val_loss=0.5211 | SWA=0.7385 |\nCWA=0.7344 | HWA=0.7364', '\\n', 'Epoch 25/25 | train_loss=0.5199 |\nval_loss=0.5217 | SWA=0.7594 | CWA=0.7535 | HWA=0.7565', '\\n', 'Test @ 25 epochs\n| loss=0.7295 | SWA=0.5908 | CWA=0.6160 | HWA=0.6032', '\\n', '\\n--- Training\nmodel for 40 epochs ---', '\\n', 'Epoch 1/40 | train_loss=0.5710 |\nval_loss=0.5275 | SWA=0.7420 | CWA=0.7366 | HWA=0.7393', '\\n', 'Epoch 2/40 |\ntrain_loss=0.5220 | val_loss=0.5213 | SWA=0.7439 | CWA=0.7375 | HWA=0.7407',\n'\\n', 'Epoch 3/40 | train_loss=0.5197 | val_loss=0.5211 | SWA=0.7563 |\nCWA=0.7499 | HWA=0.7531', '\\n', 'Epoch 4/40 | train_loss=0.5199 |\nval_loss=0.5216 | SWA=0.7618 | CWA=0.7564 | HWA=0.7591', '\\n', 'Epoch 5/40 |\ntrain_loss=0.5199 | val_loss=0.5216 | SWA=0.7640 | CWA=0.7587 | HWA=0.7614',\n'\\n', 'Epoch 6/40 | train_loss=0.5198 | val_loss=0.5212 | SWA=0.7442 |\nCWA=0.7404 | HWA=0.7423', '\\n', 'Epoch 7/40 | train_loss=0.5198 |\nval_loss=0.5212 | SWA=0.7457 | CWA=0.7414 | HWA=0.7436', '\\n', 'Epoch 8/40 |\ntrain_loss=0.5197 | val_loss=0.5212 | SWA=0.7388 | CWA=0.7342 | HWA=0.7365',\n'\\n', 'Epoch 9/40 | train_loss=0.5198 | val_loss=0.5210 | SWA=0.7339 |\nCWA=0.7295 | HWA=0.7317', '\\n', 'Epoch 10/40 | train_loss=0.5200 |\nval_loss=0.5216 | SWA=0.7515 | CWA=0.7452 | HWA=0.7483', '\\n', 'Epoch 11/40 |\ntrain_loss=0.5197 | val_loss=0.5207 | SWA=0.7515 | CWA=0.7447 | HWA=0.7481',\n'\\n', 'Epoch 12/40 | train_loss=0.5198 | val_loss=0.5224 | SWA=0.7631 |\nCWA=0.7580 | HWA=0.7605', '\\n', 'Epoch 13/40 | train_loss=0.5200 |\nval_loss=0.5212 | SWA=0.7534 | CWA=0.7491 | HWA=0.7513', '\\n', 'Epoch 14/40 |\ntrain_loss=0.5197 | val_loss=0.5217 | SWA=0.7503 | CWA=0.7446 | HWA=0.7474',\n'\\n', 'Epoch 15/40 | train_loss=0.5198 | val_loss=0.5220 | SWA=0.7704 |\nCWA=0.7648 | HWA=0.7676', '\\n', 'Epoch 16/40 | train_loss=0.5199 |\nval_loss=0.5212 | SWA=0.7543 | CWA=0.7477 | HWA=0.7510', '\\n', 'Epoch 17/40 |\ntrain_loss=0.5199 | val_loss=0.5219 | SWA=0.7420 | CWA=0.7378 | HWA=0.7399',\n'\\n', 'Epoch 18/40 | train_loss=0.5200 | val_loss=0.5218 | SWA=0.7469 |\nCWA=0.7430 | HWA=0.7450', '\\n', 'Epoch 19/40 | train_loss=0.5196 |\nval_loss=0.5222 | SWA=0.7331 | CWA=0.7297 | HWA=0.7314', '\\n', 'Epoch 20/40 |\ntrain_loss=0.5197 | val_loss=0.5212 | SWA=0.7568 | CWA=0.7514 | HWA=0.7541',\n'\\n', 'Epoch 21/40 | train_loss=0.5200 | val_loss=0.5212 | SWA=0.7451 |\nCWA=0.7396 | HWA=0.7423', '\\n', 'Epoch 22/40 | train_loss=0.5202 |\nval_loss=0.5213 | SWA=0.7421 | CWA=0.7363 | HWA=0.7392', '\\n', 'Epoch 23/40 |\ntrain_loss=0.5198 | val_loss=0.5210 | SWA=0.7505 | CWA=0.7454 | HWA=0.7479',\n'\\n', 'Epoch 24/40 | train_loss=0.5200 | val_loss=0.5214 | SWA=0.7512 |\nCWA=0.7464 | HWA=0.7488', '\\n', 'Epoch 25/40 | train_loss=0.5199 |\nval_loss=0.5214 | SWA=0.7580 | CWA=0.7530 | HWA=0.7555', '\\n', 'Epoch 26/40 |\ntrain_loss=0.5200 | val_loss=0.5217 | SWA=0.7574 | CWA=0.7502 | HWA=0.7538',\n'\\n', 'Epoch 27/40 | train_loss=0.5198 | val_loss=0.5212 | SWA=0.7465 |\nCWA=0.7407 | HWA=0.7436', '\\n', 'Epoch 28/40 | train_loss=0.5198 |\nval_loss=0.5215 | SWA=0.7564 | CWA=0.7506 | HWA=0.7535', '\\n', 'Epoch 29/40 |\ntrain_loss=0.5199 | val_loss=0.5210 | SWA=0.7404 | CWA=0.7352 | HWA=0.7378',\n'\\n', 'Epoch 30/40 | train_loss=0.5198 | val_loss=0.5214 | SWA=0.7626 |\nCWA=0.7579 | HWA=0.7602', '\\n', 'Epoch 31/40 | train_loss=0.5199 |\nval_loss=0.5229 | SWA=0.7672 | CWA=0.7627 | HWA=0.7650', '\\n', 'Epoch 32/40 |\ntrain_loss=0.5199 | val_loss=0.5209 | SWA=0.7523 | CWA=0.7478 | HWA=0.7500',\n'\\n', 'Epoch 33/40 | train_loss=0.5199 | val_loss=0.5211 | SWA=0.7361 |\nCWA=0.7316 | HWA=0.7338', '\\n', 'Epoch 34/40 | train_loss=0.5200 |\nval_loss=0.5213 | SWA=0.7511 | CWA=0.7449 | HWA=0.7480', '\\n', 'Epoch 35/40 |\ntrain_loss=0.5198 | val_loss=0.5221 | SWA=0.7649 | CWA=0.7593 | HWA=0.7621',\n'\\n', 'Epoch 36/40 | train_loss=0.5201 | val_loss=0.5209 | SWA=0.7544 |\nCWA=0.7499 | HWA=0.7521', '\\n', 'Epoch 37/40 | train_loss=0.5200 |\nval_loss=0.5216 | SWA=0.7474 | CWA=0.7433 | HWA=0.7454', '\\n', 'Epoch 38/40 |\ntrain_loss=0.5199 | val_loss=0.5209 | SWA=0.7421 | CWA=0.7364 | HWA=0.7392',\n'\\n', 'Epoch 39/40 | train_loss=0.5199 | val_loss=0.5217 | SWA=0.7345 |\nCWA=0.7309 | HWA=0.7327', '\\n', 'Epoch 40/40 | train_loss=0.5199 |\nval_loss=0.5215 | SWA=0.7545 | CWA=0.7491 | HWA=0.7518', '\\n', 'Test @ 40 epochs\n| loss=0.7252 | SWA=0.5923 | CWA=0.6188 | HWA=0.6053', '\\n', 'Saved\nexperiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-6/working', '\\n', 'Execution time: a minute seconds\n(time limit is 30 minutes).']", ""], "analysis": ["", "", "", "The code executed successfully without any errors or bugs. The training and\nevaluation process for different batch sizes was completed, and the results were\nsaved to a file. The harmonic weighted accuracy (HWA) metric was used as a key\nevaluation metric, and the performance was consistent across different batch\nsizes. No issues were identified in the execution.", "", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5199, "best_value": 0.5199}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521, "best_value": 0.521}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Best shape-weighted accuracy on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7481, "best_value": 0.7481}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Best color-weighted accuracy on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7455, "best_value": 0.7455}]}, {"metric_name": "harmonic-weighted accuracy", "lower_is_better": false, "description": "Best harmonic-weighted accuracy on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7468, "best_value": 0.7468}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value on the training dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.5198, "best_value": 0.5198}, {"dataset_name": "15_epochs", "final_value": 0.5197, "best_value": 0.5197}, {"dataset_name": "25_epochs", "final_value": 0.5196, "best_value": 0.5196}, {"dataset_name": "40_epochs", "final_value": 0.5196, "best_value": 0.5196}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.521, "best_value": 0.521}, {"dataset_name": "15_epochs", "final_value": 0.5209, "best_value": 0.5209}, {"dataset_name": "25_epochs", "final_value": 0.5206, "best_value": 0.5206}, {"dataset_name": "40_epochs", "final_value": 0.5206, "best_value": 0.5206}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7481, "best_value": 0.7481}, {"dataset_name": "15_epochs", "final_value": 0.766, "best_value": 0.766}, {"dataset_name": "25_epochs", "final_value": 0.767, "best_value": 0.767}, {"dataset_name": "40_epochs", "final_value": 0.7651, "best_value": 0.7651}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7455, "best_value": 0.7455}, {"dataset_name": "15_epochs", "final_value": 0.7613, "best_value": 0.7613}, {"dataset_name": "25_epochs", "final_value": 0.7618, "best_value": 0.7618}, {"dataset_name": "40_epochs", "final_value": 0.7614, "best_value": 0.7614}]}, {"metric_name": "validation harmonic-weighted accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7468, "best_value": 0.7468}, {"dataset_name": "15_epochs", "final_value": 0.7636, "best_value": 0.7636}, {"dataset_name": "25_epochs", "final_value": 0.7644, "best_value": 0.7644}, {"dataset_name": "40_epochs", "final_value": 0.7632, "best_value": 0.7632}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7201, "best_value": 0.7201}, {"dataset_name": "15_epochs", "final_value": 0.7247, "best_value": 0.7247}, {"dataset_name": "25_epochs", "final_value": 0.7316, "best_value": 0.7316}, {"dataset_name": "40_epochs", "final_value": 0.7208, "best_value": 0.7208}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.595, "best_value": 0.595}, {"dataset_name": "15_epochs", "final_value": 0.5866, "best_value": 0.5866}, {"dataset_name": "25_epochs", "final_value": 0.5898, "best_value": 0.5898}, {"dataset_name": "40_epochs", "final_value": 0.5958, "best_value": 0.5958}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.6205, "best_value": 0.6205}, {"dataset_name": "15_epochs", "final_value": 0.6122, "best_value": 0.6122}, {"dataset_name": "25_epochs", "final_value": 0.6159, "best_value": 0.6159}, {"dataset_name": "40_epochs", "final_value": 0.6226, "best_value": 0.6226}]}, {"metric_name": "test harmonic-weighted accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.6075, "best_value": 0.6075}, {"dataset_name": "15_epochs", "final_value": 0.5991, "best_value": 0.5991}, {"dataset_name": "25_epochs", "final_value": 0.6026, "best_value": 0.6026}, {"dataset_name": "40_epochs", "final_value": 0.6089, "best_value": 0.6089}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521086, "best_value": 0.520042}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521241, "best_value": 0.521221}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation metric SWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.758691, "best_value": 0.758691}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation metric CWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.755659, "best_value": 0.755659}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "Validation metric HWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.757172, "best_value": 0.757172}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "Test metric SWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.602666, "best_value": 0.602666}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "Test metric CWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.630268, "best_value": 0.630268}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "Test metric HWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.616158, "best_value": 0.616158}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.723998, "best_value": 0.714542}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss computed during training.", "data": [{"dataset_name": "bs_32", "final_value": 0.521, "best_value": 0.521}, {"dataset_name": "bs_64", "final_value": 0.5204, "best_value": 0.5204}, {"dataset_name": "bs_128", "final_value": 0.5198, "best_value": 0.5198}, {"dataset_name": "bs_256", "final_value": 0.5197, "best_value": 0.5197}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss computed on the validation dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.5213, "best_value": 0.5213}, {"dataset_name": "bs_64", "final_value": 0.5213, "best_value": 0.5213}, {"dataset_name": "bs_128", "final_value": 0.521, "best_value": 0.521}, {"dataset_name": "bs_256", "final_value": 0.5211, "best_value": 0.5211}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "Harmonic weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.7558, "best_value": 0.7558}, {"dataset_name": "bs_64", "final_value": 0.7588, "best_value": 0.7588}, {"dataset_name": "bs_128", "final_value": 0.7603, "best_value": 0.7603}, {"dataset_name": "bs_256", "final_value": 0.7422, "best_value": 0.7422}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss computed on the test dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.7173, "best_value": 0.7173}, {"dataset_name": "bs_64", "final_value": 0.7226, "best_value": 0.7226}, {"dataset_name": "bs_128", "final_value": 0.7317, "best_value": 0.7317}, {"dataset_name": "bs_256", "final_value": 0.7173, "best_value": 0.7173}]}, {"metric_name": "test harmonic weighted accuracy", "lower_is_better": false, "description": "Harmonic weighted accuracy computed on the test dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.6085, "best_value": 0.6085}, {"dataset_name": "bs_64", "final_value": 0.6043, "best_value": 0.6043}, {"dataset_name": "bs_128", "final_value": 0.6018, "best_value": 0.6018}, {"dataset_name": "bs_256", "final_value": 0.6094, "best_value": 0.6094}]}]}, {"metric_names": [{"metric_name": "Training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5214, "best_value": 0.5199}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5221, "best_value": 0.5209}]}, {"metric_name": "Validation Shape-Weighted Accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.744, "best_value": 0.765}]}, {"metric_name": "Validation Color-Weighted Accuracy", "lower_is_better": false, "description": "The accuracy weighted by color during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.738, "best_value": 0.757}]}, {"metric_name": "Validation Harmonic-Weighted Accuracy", "lower_is_better": false, "description": "The harmonic mean of shape and color weighted accuracies during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.741, "best_value": 0.761}]}, {"metric_name": "Test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7341, "best_value": 0.7195}]}, {"metric_name": "Test Shape-Weighted Accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.579, "best_value": 0.593}]}, {"metric_name": "Test Color-Weighted Accuracy", "lower_is_better": false, "description": "The accuracy weighted by color during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.605, "best_value": 0.62}]}, {"metric_name": "Test Harmonic-Weighted Accuracy", "lower_is_better": false, "description": "The harmonic mean of shape and color weighted accuracies during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.592, "best_value": 0.606}]}]}, {"metric_names": [{"metric_name": "Training loss", "lower_is_better": true, "description": "Loss during training phase, lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5199, "best_value": 0.5199}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Loss during validation phase, lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5208, "best_value": 0.5208}]}, {"metric_name": "Validation shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by shape during validation phase, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7466, "best_value": 0.7466}]}, {"metric_name": "Validation color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by color during validation phase, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7406, "best_value": 0.7406}]}, {"metric_name": "Validation harmonic-weighted accuracy", "lower_is_better": false, "description": "Harmonic mean of shape and color-weighted accuracy during validation phase, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7436, "best_value": 0.7436}]}, {"metric_name": "Test loss", "lower_is_better": true, "description": "Loss during testing phase, lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7255, "best_value": 0.7255}]}, {"metric_name": "Test shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by shape during testing phase, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5945, "best_value": 0.5945}]}, {"metric_name": "Test color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by color during testing phase, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6198, "best_value": 0.6198}]}, {"metric_name": "Test harmonic-weighted accuracy", "lower_is_better": false, "description": "Harmonic mean of shape and color-weighted accuracy during testing phase, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6069, "best_value": 0.6069}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model is performing on the validation set. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.523, "best_value": 0.521}]}, {"metric_name": "validation shape weighted accuracy (SWA)", "lower_is_better": false, "description": "Measures the weighted accuracy for shapes in the validation set. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.746, "best_value": 0.7665}]}, {"metric_name": "validation color weighted accuracy (CWA)", "lower_is_better": false, "description": "Measures the weighted accuracy for colors in the validation set. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7414, "best_value": 0.7607}]}, {"metric_name": "validation harmonic weighted accuracy (HWA)", "lower_is_better": false, "description": "Measures the harmonic mean of accuracies in the validation set. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7437, "best_value": 0.7636}]}]}, {"metric_names": [{"metric_name": "Validation Loss", "lower_is_better": true, "description": "The loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5909, "best_value": 0.5909}]}, {"metric_name": "Validation Shape-Weighted Accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7514, "best_value": 0.7514}]}, {"metric_name": "Validation Color-Weighted Accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7461, "best_value": 0.7461}]}, {"metric_name": "Validation Harmonic-Weighted Accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7487, "best_value": 0.7487}]}, {"metric_name": "Test Loss", "lower_is_better": true, "description": "The loss on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.704, "best_value": 0.704}]}, {"metric_name": "Test Shape-Weighted Accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5902, "best_value": 0.5902}]}, {"metric_name": "Test Color-Weighted Accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6159, "best_value": 0.6159}]}, {"metric_name": "Test Harmonic-Weighted Accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6028, "best_value": 0.6028}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.519705, "best_value": 0.519705}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.523079, "best_value": 0.523079}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The accuracy for shape classification during validation, weighted by shape frequency.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.740902, "best_value": 0.740902}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The accuracy for color classification during validation, weighted by color frequency.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.737661, "best_value": 0.737661}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic mean of shape and color weighted accuracies during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.739278, "best_value": 0.739278}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.72719, "best_value": 0.72719}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The accuracy for shape classification during testing, weighted by shape frequency.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.592871, "best_value": 0.592871}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The accuracy for color classification during testing, weighted by color frequency.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.619239, "best_value": 0.619239}]}, {"metric_name": "test harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic mean of shape and color weighted accuracies during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.605768, "best_value": 0.605768}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value on the training dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.5199, "best_value": 0.5199}, {"dataset_name": "15_epochs", "final_value": 0.5197, "best_value": 0.5197}, {"dataset_name": "25_epochs", "final_value": 0.5195, "best_value": 0.5195}, {"dataset_name": "40_epochs", "final_value": 0.5196, "best_value": 0.5196}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.5209, "best_value": 0.5209}, {"dataset_name": "15_epochs", "final_value": 0.5211, "best_value": 0.5211}, {"dataset_name": "25_epochs", "final_value": 0.5206, "best_value": 0.5206}, {"dataset_name": "40_epochs", "final_value": 0.5207, "best_value": 0.5207}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "15_epochs", "final_value": 0.7756, "best_value": 0.7756}, {"dataset_name": "25_epochs", "final_value": 0.7732, "best_value": 0.7732}, {"dataset_name": "40_epochs", "final_value": 0.7667, "best_value": 0.7667}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7441, "best_value": 0.7441}, {"dataset_name": "15_epochs", "final_value": 0.7723, "best_value": 0.7723}, {"dataset_name": "25_epochs", "final_value": 0.7694, "best_value": 0.7694}, {"dataset_name": "40_epochs", "final_value": 0.7619, "best_value": 0.7619}]}, {"metric_name": "validation harmonic-weighted accuracy", "lower_is_better": false, "description": "Harmonic-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7471, "best_value": 0.7471}, {"dataset_name": "15_epochs", "final_value": 0.7739, "best_value": 0.7739}, {"dataset_name": "25_epochs", "final_value": 0.7713, "best_value": 0.7713}, {"dataset_name": "40_epochs", "final_value": 0.7643, "best_value": 0.7643}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7254, "best_value": 0.7254}, {"dataset_name": "15_epochs", "final_value": 0.7334, "best_value": 0.7334}, {"dataset_name": "25_epochs", "final_value": 0.7261, "best_value": 0.7261}, {"dataset_name": "40_epochs", "final_value": 0.7227, "best_value": 0.7227}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.5928, "best_value": 0.5928}, {"dataset_name": "15_epochs", "final_value": 0.5861, "best_value": 0.5861}, {"dataset_name": "25_epochs", "final_value": 0.5904, "best_value": 0.5904}, {"dataset_name": "40_epochs", "final_value": 0.5939, "best_value": 0.5939}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.6193, "best_value": 0.6193}, {"dataset_name": "15_epochs", "final_value": 0.6116, "best_value": 0.6116}, {"dataset_name": "25_epochs", "final_value": 0.6162, "best_value": 0.6162}, {"dataset_name": "40_epochs", "final_value": 0.6197, "best_value": 0.6197}]}, {"metric_name": "test harmonic-weighted accuracy", "lower_is_better": false, "description": "Harmonic-weighted accuracy on the test dataset.", "data": [{"dataset_name": "5_epochs", "final_value": 0.6058, "best_value": 0.6058}, {"dataset_name": "15_epochs", "final_value": 0.5986, "best_value": 0.5986}, {"dataset_name": "25_epochs", "final_value": 0.603, "best_value": 0.603}, {"dataset_name": "40_epochs", "final_value": 0.6065, "best_value": 0.6065}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Tracks the loss during training, aiming to minimize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.5198, "best_value": 0.5198}, {"dataset_name": "15_epochs", "final_value": 0.5198, "best_value": 0.5198}, {"dataset_name": "25_epochs", "final_value": 0.5197, "best_value": 0.5197}, {"dataset_name": "40_epochs", "final_value": 0.5196, "best_value": 0.5196}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Tracks the loss during validation, aiming to minimize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.521, "best_value": 0.521}, {"dataset_name": "15_epochs", "final_value": 0.521, "best_value": 0.521}, {"dataset_name": "25_epochs", "final_value": 0.5206, "best_value": 0.5206}, {"dataset_name": "40_epochs", "final_value": 0.5208, "best_value": 0.5208}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Measures the shape-weighted accuracy on the validation set, aiming to maximize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7537, "best_value": 0.7537}, {"dataset_name": "15_epochs", "final_value": 0.7613, "best_value": 0.7613}, {"dataset_name": "25_epochs", "final_value": 0.7624, "best_value": 0.7624}, {"dataset_name": "40_epochs", "final_value": 0.7715, "best_value": 0.7715}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "Measures the color-weighted accuracy on the validation set, aiming to maximize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7503, "best_value": 0.7503}, {"dataset_name": "15_epochs", "final_value": 0.7565, "best_value": 0.7565}, {"dataset_name": "25_epochs", "final_value": 0.7579, "best_value": 0.7579}, {"dataset_name": "40_epochs", "final_value": 0.7679, "best_value": 0.7679}]}, {"metric_name": "validation harmonic-weighted accuracy", "lower_is_better": false, "description": "Measures the harmonic-weighted accuracy on the validation set, aiming to maximize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.752, "best_value": 0.752}, {"dataset_name": "15_epochs", "final_value": 0.7589, "best_value": 0.7589}, {"dataset_name": "25_epochs", "final_value": 0.7601, "best_value": 0.7601}, {"dataset_name": "40_epochs", "final_value": 0.7697, "best_value": 0.7697}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Tracks the loss on the test set, aiming to minimize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.7211, "best_value": 0.7211}, {"dataset_name": "15_epochs", "final_value": 0.7336, "best_value": 0.7336}, {"dataset_name": "25_epochs", "final_value": 0.727, "best_value": 0.727}, {"dataset_name": "40_epochs", "final_value": 0.7324, "best_value": 0.7324}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Measures the shape-weighted accuracy on the test set, aiming to maximize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.5913, "best_value": 0.5913}, {"dataset_name": "15_epochs", "final_value": 0.587, "best_value": 0.587}, {"dataset_name": "25_epochs", "final_value": 0.5874, "best_value": 0.5874}, {"dataset_name": "40_epochs", "final_value": 0.5845, "best_value": 0.5845}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Measures the color-weighted accuracy on the test set, aiming to maximize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.6181, "best_value": 0.6181}, {"dataset_name": "15_epochs", "final_value": 0.6131, "best_value": 0.6131}, {"dataset_name": "25_epochs", "final_value": 0.6132, "best_value": 0.6132}, {"dataset_name": "40_epochs", "final_value": 0.6095, "best_value": 0.6095}]}, {"metric_name": "test harmonic-weighted accuracy", "lower_is_better": false, "description": "Measures the harmonic-weighted accuracy on the test set, aiming to maximize this value.", "data": [{"dataset_name": "5_epochs", "final_value": 0.6044, "best_value": 0.6044}, {"dataset_name": "15_epochs", "final_value": 0.5997, "best_value": 0.5997}, {"dataset_name": "25_epochs", "final_value": 0.6, "best_value": 0.6}, {"dataset_name": "40_epochs", "final_value": 0.5967, "best_value": 0.5967}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "Training Set", "final_value": 0.5196, "best_value": 0.5196}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "Validation Set", "final_value": 0.5207, "best_value": 0.5207}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation.", "data": [{"dataset_name": "Validation Set", "final_value": 0.7704, "best_value": 0.7704}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during validation.", "data": [{"dataset_name": "Validation Set", "final_value": 0.7648, "best_value": 0.7648}]}, {"metric_name": "validation harmonic-weighted accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy during validation.", "data": [{"dataset_name": "Validation Set", "final_value": 0.7676, "best_value": 0.7676}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "Test Set", "final_value": 0.7252, "best_value": 0.7252}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during testing.", "data": [{"dataset_name": "Test Set", "final_value": 0.5923, "best_value": 0.5923}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during testing.", "data": [{"dataset_name": "Test Set", "final_value": 0.6188, "best_value": 0.6188}]}, {"metric_name": "test harmonic-weighted accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy during testing.", "data": [{"dataset_name": "Test Set", "final_value": 0.6053, "best_value": 0.6053}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_val_HWA.png", "../../logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_test_metric_bars.png", "../../logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_confusion_40_epochs.png"], ["../../logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_val_HWA.png", "../../logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_test_HWA_bar.png"], ["../../logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_batch_size_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_validation_HWA_curves.png", "../../logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_test_HWA_bar.png"], ["../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed32.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed32.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed64.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed64.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed128.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed128.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed256.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed256.png", "../../logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_test_HWA_vs_embedding.png"], ["../../logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_loss_curves_weight_decay.png", "../../logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_val_HWA_curves.png", "../../logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_final_dev_HWA_bar.png", "../../logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_test_metrics_bar.png"], ["../../logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_SWA_curves.png", "../../logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_CWA_curves.png", "../../logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_HWA_curves.png", "../../logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_final_HWA_bar.png"], ["../../logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_val_loss_vs_smoothing.png", "../../logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_val_weighted_accuracies_vs_smoothing.png", "../../logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_test_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_test_weighted_accuracies_bar.png"], ["../../logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_test_weighted_accuracies.png", "../../logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_val_HWA_over_epochs.png"], ["../../logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_val_HWA.png", "../../logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_test_metric_bars.png", "../../logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_confusion_40_epochs.png"], ["../../logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_val_HWA.png", "../../logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_test_metric_bars.png", "../../logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_confusion_5_epochs.png"], ["../../logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_val_HWA.png", "../../logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_test_metric_bars.png", "../../logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_confusion_40_epochs.png"], ["../../logs/0-run/experiment_results/seed_aggregation_4c56b0ea04a647b193451029b948342a/SPR_BENCH_agg_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_4c56b0ea04a647b193451029b948342a/SPR_BENCH_agg_val_HWA.png", "../../logs/0-run/experiment_results/seed_aggregation_4c56b0ea04a647b193451029b948342a/SPR_BENCH_agg_test_metrics.png"]], "plot_paths": [["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_val_metrics.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_final_metrics_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_val_HWA.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_test_metric_bars.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_confusion_40_epochs.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_val_HWA.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_test_HWA_bar.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_batch_size_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_validation_HWA_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_test_HWA_bar.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed32.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed32.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed64.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed64.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed128.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed128.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed256.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed256.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_test_HWA_vs_embedding.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_loss_curves_weight_decay.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_val_HWA_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_final_dev_HWA_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_test_metrics_bar.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_SWA_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_CWA_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_HWA_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_final_HWA_bar.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_val_loss_vs_smoothing.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_val_weighted_accuracies_vs_smoothing.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_test_confusion_matrix.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_test_weighted_accuracies_bar.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_test_weighted_accuracies.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_val_HWA_over_epochs.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_val_HWA.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_test_metric_bars.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_confusion_40_epochs.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_val_HWA.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_test_metric_bars.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_confusion_5_epochs.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_val_HWA.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_test_metric_bars.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_confusion_40_epochs.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_4c56b0ea04a647b193451029b948342a/SPR_BENCH_agg_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_4c56b0ea04a647b193451029b948342a/SPR_BENCH_agg_val_HWA.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_4c56b0ea04a647b193451029b948342a/SPR_BENCH_agg_test_metrics.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation loss over epochs. Both losses decrease steadily, with the validation loss closely following the training loss. By the fifth epoch, the losses converge, indicating that the model has achieved a good fit without overfitting. The consistent reduction in both losses suggests that the model is learning effectively from the data.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot displays the progression of Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and an additional metric (HWA) over epochs. All metrics improve steadily until the fourth epoch, where they peak, followed by a slight decline in the fifth epoch. This suggests that the model performs best at epoch 4, and further training might lead to overfitting.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart summarizes the final validation results for the three metrics: SWA, CWA, and HWA. SWA and HWA achieve the highest scores (0.74), while CWA is slightly lower (0.73). The close performance across metrics indicates that the model generalizes well across the different weighted accuracies.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix for the test set shows the distribution of true and predicted labels. The majority of predictions are correct, as evidenced by the darker diagonal, indicating strong model performance. However, there are non-negligible misclassifications, particularly in the upper right and lower left quadrants, suggesting areas for improvement in distinguishing between classes.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training and validation loss curves across different epoch configurations (5, 15, 25, and 40 epochs) show a rapid convergence within the first few epochs, after which the loss stabilizes. This indicates that the model learns quickly and reaches a plateau. There is no significant overfitting observed since the training and validation losses remain close across all configurations.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_loss_curves.png"}, {"analysis": "The Harmonic Weighted Accuracy (HWA) over validation data fluctuates significantly across epochs for all configurations. While there are occasional peaks, the lack of a clear upward trend suggests that the model's performance is inconsistent and possibly sensitive to the training dynamics or data complexities. Longer training (e.g., 40 epochs) does not necessarily lead to better or more stable HWA.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_val_HWA.png"}, {"analysis": "The final test metrics (SWA, CWA, and HWA) are relatively consistent across different epoch configurations. This suggests that the model's performance is robust to the number of training epochs, with no significant improvement observed as epochs increase. The scores hover around a similar range, implying that the model achieves its best performance early in training.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_test_metric_bars.png"}, {"analysis": "The confusion matrix for the 40-epoch configuration shows a moderate level of misclassification, with more false negatives than false positives. This imbalance indicates that the model might struggle more with recognizing positive labels, which could be a point of improvement in future iterations.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f2fd50b2bb004f81b81078e3163646c5_proc_457453/SPR_BENCH_confusion_40_epochs.png"}], [{"analysis": "The loss curves indicate that all learning rates (0.0005, 0.001, 0.002) lead to a rapid decrease in training and validation loss, with convergence occurring around 3 epochs. The learning rate of 0.002 achieves the fastest convergence, followed by 0.001 and then 0.0005. However, the validation loss for all learning rates stabilizes at approximately the same value after convergence, suggesting that all configurations are effective in minimizing loss but differ in training speed.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation HWA curves depict fluctuating trends across epochs for all learning rates. The learning rate of 0.002 demonstrates the highest final HWA, indicating better generalization on the validation set. The learning rate of 0.001 shows competitive but slightly lower performance, while 0.0005 performs the worst in terms of HWA. The fluctuations suggest sensitivity to learning rate and possibly the need for more epochs to stabilize the metric.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_val_HWA.png"}, {"analysis": "The final test HWA per learning rate shows comparable performance across all learning rates, with marginal differences. The learning rate of 0.002 slightly outperforms the others, but the differences are not substantial. This suggests that while higher learning rates may accelerate training, the ultimate test performance does not vary significantly across the tested learning rates.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_fd9c6136b93f4ffebfa2d98710bc139f_proc_457454/SPR_BENCH_test_HWA_bar.png"}], [{"analysis": "The train and validation loss curves show that smaller batch sizes (e.g., 32 and 64) lead to faster convergence compared to larger batch sizes (e.g., 128 and 256). The red curve (batch size 256) exhibits slower convergence and higher initial loss, suggesting that larger batch sizes might hinder the model's ability to generalize effectively. However, all batch sizes eventually converge to a similar loss value, indicating that the model is capable of learning effectively across different batch sizes, albeit at different rates.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_batch_size_loss_curves.png"}, {"analysis": "The plot for validation HWA (Harmonic Weighted Accuracy) across epochs exhibits significant fluctuations for batch sizes 32, 64, and 128, while batch size 256 shows a more stable but lower performance trend. This suggests that smaller batch sizes may enable the model to capture finer details but at the cost of stability. Batch size 128 demonstrates the highest peak in HWA, indicating its potential to achieve the best performance under certain conditions, though it also exhibits instability.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_validation_HWA_curves.png"}, {"analysis": "The test HWA plot reveals that all batch sizes achieve nearly identical performance, with HWA values ranging around 0.60-0.61. This indicates that while training dynamics and validation performance may vary across batch sizes, the final test performance is relatively insensitive to batch size. This suggests that the model's generalization capabilities are robust to changes in batch size during training.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1a506fbcbf3042928da4aea138689b18_proc_457455/SPR_BENCH_test_HWA_bar.png"}], [{"analysis": "The training and validation loss curves show a steady decline over the epochs, indicating that the model is learning effectively. There is no significant overfitting as the validation loss closely follows the training loss. The embedding dimension of 32 appears to provide a stable training process.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed32.png"}, {"analysis": "The accuracy metrics (SWA, CWA, HWA) improve consistently over the epochs, demonstrating that the model generalizes well to the validation set. The metrics plateau slightly towards the end, suggesting the model is nearing convergence. The embedding dimension of 32 supports a good balance between performance and complexity.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed32.png"}, {"analysis": "The training and validation loss curves show a rapid initial decline, followed by stabilization. This indicates effective learning with minimal overfitting. The embedding dimension of 64 allows for better representation of the data, leading to slightly improved training dynamics.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed64.png"}, {"analysis": "The accuracy metrics (SWA, CWA, HWA) show a steady increase, with higher values compared to the smaller embedding dimension. This suggests that the embedding dimension of 64 provides a better feature representation, leading to improved generalization on the validation set.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed64.png"}, {"analysis": "The training and validation loss curves show a sharp initial decline, followed by stabilization at a low loss value. The embedding dimension of 128 supports effective learning but does not significantly improve over smaller dimensions, indicating diminishing returns in loss reduction.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed128.png"}, {"analysis": "The accuracy metrics (SWA, CWA, HWA) show an initial increase, followed by a slight dip and then recovery. This could indicate overparameterization or instability due to the higher embedding dimension of 128. The final accuracy values are slightly higher, but the gains are marginal.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed128.png"}, {"analysis": "The training and validation loss curves show a similar trend to the smaller embedding dimensions, but the loss reduction stabilizes earlier. This suggests that the embedding dimension of 256 might be too large, leading to overparameterization without significant performance gains.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_loss_curves_embed256.png"}, {"analysis": "The accuracy metrics (SWA, CWA, HWA) show a similar pattern to the embedding dimension of 128, with an initial increase, a dip, and recovery. The final accuracy values are comparable to those with smaller dimensions, indicating no substantial benefit from the larger embedding dimension.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_metrics_embed256.png"}, {"analysis": "The test HWA metric shows a U-shaped relationship with embedding dimension. Smaller and larger embedding dimensions (e.g., 32 and 256) perform better, while intermediate dimensions (e.g., 128) perform worse. This suggests that both underparameterization and overparameterization can negatively impact the model's ability to generalize.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a8387da82b2f47ab83bd029a42eab5d3_proc_457456/SPR_BENCH_test_HWA_vs_embedding.png"}], [{"analysis": "This plot shows the training and validation cross-entropy loss for different weight decay values over 5 epochs. The training loss decreases steadily for all configurations, with smaller weight decay values (e.g., 0.0 and 1e-05) leading to faster initial convergence. However, the validation loss curves suggest overfitting for lower weight decays, as the gap between training and validation loss increases. The weight decay of 0.001 demonstrates the most consistent performance across training and validation, indicating better generalization.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_loss_curves_weight_decay.png"}, {"analysis": "This plot visualizes the harmonic weighted accuracy (HWA) on the development set over epochs for different weight decay values. The weight decay of 1e-05 achieves the highest peak HWA at epoch 3 but experiences a sharp decline afterward, indicating instability. The weight decay of 0.0001 shows a more stable trend, achieving competitive peak performance and maintaining it better across epochs. The weight decay of 0.001 exhibits the most consistent but slightly lower performance, suggesting a trade-off between stability and peak accuracy.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_val_HWA_curves.png"}, {"analysis": "This bar chart compares the final harmonic weighted accuracy (HWA) on the development set for different weight decay values. The results indicate minimal variation in the final HWA across weight decay values, suggesting that the choice of weight decay has a limited impact on the final model performance when evaluated at the last epoch. This aligns with the earlier observation that peak performance and stability during training vary more significantly than the final HWA.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_final_dev_HWA_bar.png"}, {"analysis": "This bar chart presents the test metrics for the best-performing model, including shape-weighted accuracy (SWA), color-weighted accuracy (CWA), and harmonic weighted accuracy (HWA). The CWA achieves the highest score, followed by HWA and SWA. This indicates that the model performs better when considering color variations compared to shape variations, suggesting that the model's reasoning capabilities are more aligned with color-related patterns.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_8502eb62ab9a42e980161ffb6c49c2f2_proc_457454/SPR_BENCH_test_metrics_bar.png"}], [{"analysis": "The plot compares training and validation loss for different dropout rates. Across all dropout settings, both training and validation loss decrease significantly during the first two epochs, and then plateau. Lower dropout rates (0.0 and 0.1) show slightly better convergence for validation loss, indicating better generalization. Higher dropout rates (0.3 and 0.5) result in slightly higher validation loss, suggesting potential underfitting due to excessive regularization.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the Shape-Weighted Accuracy (SWA) on the validation set across epochs for different dropout rates. Dropout rates of 0.1 and 0.3 exhibit better performance trends, with dropout 0.1 achieving the highest peak SWA at epoch 3. Dropout 0.0 and 0.5 show lower SWA values, indicating that some regularization (dropout 0.1 or 0.3) is beneficial for improving generalization in terms of SWA.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_SWA_curves.png"}, {"analysis": "The plot tracks the Color-Weighted Accuracy (CWA) on the validation set for different dropout rates. Similar to the SWA plot, dropout rates of 0.1 and 0.3 generally perform better, with dropout 0.1 achieving a peak at epoch 3. Dropout 0.0 and 0.5 show relatively lower CWA values, again indicating that moderate dropout helps in improving performance on the CWA metric.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_CWA_curves.png"}, {"analysis": "This plot illustrates the Heterogeneous Weighted Accuracy (HWA) on the validation set across epochs for different dropout rates. Dropout rates of 0.1 and 0.3 demonstrate better performance, with dropout 0.1 achieving the highest HWA at epoch 3. Dropout 0.5 shows the lowest performance, indicating that excessive regularization harms generalization for HWA.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_HWA_curves.png"}, {"analysis": "This bar chart summarizes the final HWA values for different dropout rates after the last epoch. All dropout rates achieve similar final HWA values, but dropout rates of 0.1 and 0.3 slightly outperform others. This suggests that while dropout has a marginal impact on the final performance, moderate dropout rates (0.1 and 0.3) are preferable for achieving better results.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d979c742ecc641efa4c283e570ea52a7_proc_457456/SPR_BENCH_final_HWA_bar.png"}], [{"analysis": "The first plot illustrates the relationship between the label smoothing coefficient and validation loss. As the label smoothing coefficient increases, the validation loss also increases linearly. This suggests that while label smoothing may contribute to regularization, it also introduces a trade-off by increasing the loss. The linear trend indicates a consistent impact of label smoothing on the model's loss across the tested range.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_val_loss_vs_smoothing.png"}, {"analysis": "The second plot shows the impact of label smoothing on three accuracy metrics: Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and HWA. As the label smoothing coefficient increases, all three metrics improve, with SWA showing the most significant improvement, followed by HWA and CWA. This suggests that label smoothing positively impacts the model's ability to generalize across these metrics, particularly for SWA, which may benefit from the regularization effect of label smoothing.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_val_weighted_accuracies_vs_smoothing.png"}, {"analysis": "The third plot presents the confusion matrix for the test set. The matrix shows the distribution of true and predicted labels. The darker diagonal elements indicate a higher number of correct predictions, while the off-diagonal elements represent misclassifications. The distribution suggests that the model performs reasonably well, though there is room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_test_confusion_matrix.png"}, {"analysis": "The fourth plot compares the test weighted accuracies for SWA, CWA, and HWA using the best label smoothing coefficient (0.20). CWA achieves the highest accuracy, followed by HWA and SWA. This indicates that the model performs best on the CWA metric when label smoothing is optimized, suggesting that the model's ability to generalize across color variations is stronger than for shape or overall variations.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_35c588c9695e408e91d45800cf45399c_proc_457453/SPR_BENCH_test_weighted_accuracies_bar.png"}], [{"analysis": "The loss curves show a steady decline in cross-entropy loss for both training and validation sets across epochs. The training loss decreases more significantly than the validation loss, which indicates effective learning but raises a slight concern about potential overfitting. Different gradient clipping values (clip=0, 0.5, 1, 2, 5) appear to have a minimal impact on the overall loss trends, suggesting that gradient clipping does not drastically affect model convergence in this setup.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_loss_curves.png"}, {"analysis": "The weighted accuracy metrics (SWA, CWA, HWA) remain consistent across different max_grad_norm values (0, 0.5, 1, 2, 5). This consistency suggests that the choice of max_grad_norm does not significantly influence the model's ability to generalize in terms of weighted accuracies. The metrics are all approximately equal, indicating balanced performance across shape, color, and hybrid weighted accuracies.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_test_weighted_accuracies.png"}, {"analysis": "The validation HWA over epochs for extreme clipping values (clip=0 and clip=5) shows that clip=5 achieves a peak HWA at epoch 3 but declines afterward, while clip=0 maintains a relatively stable HWA. This suggests that while higher clipping values may initially boost performance, they might introduce instability or overfitting in later epochs.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dfa7010a19f841049cbb03081f2ddf27_proc_457455/SPR_BENCH_val_HWA_over_epochs.png"}], [{"analysis": "The training and validation loss curves show that the model converges quickly within the first few epochs. However, after initial convergence, the loss values remain relatively stable with negligible differences between training and validation losses. This indicates that the model is not overfitting, but there is limited improvement in reducing the loss beyond the initial epochs.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_loss_curves.png"}, {"analysis": "The harmonic weighted accuracy (HWA) plot for validation shows significant fluctuations across all epoch configurations. This suggests that the model\u2019s performance on the validation set is inconsistent and may not be stable. The lack of a clear upward trend indicates that the current hyperparameter settings might not be optimal for achieving consistent improvements.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_val_HWA.png"}, {"analysis": "The final test metrics for SWA, CWA, and HWA show minimal variation across different epoch configurations, with scores hovering around the same range. This implies that increasing the number of training epochs does not significantly enhance performance on the test set. The current results suggest that the model might have reached its capacity under the current setup.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_test_metric_bars.png"}, {"analysis": "The confusion matrix for the 40-epoch configuration reveals that the model struggles with false positives and false negatives, as evidenced by the relatively high off-diagonal values. While the true positive and true negative counts are higher, the misclassification rates indicate room for improvement in the model\u2019s discriminative ability.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/SPR_BENCH_confusion_40_epochs.png"}], [{"analysis": "The first plot shows the training and validation loss curves across different epochs (5, 15, 25, and 40 epochs). The loss decreases sharply in the initial epochs and stabilizes quickly, indicating that the model converges early during training. However, the gap between training and validation loss is minimal, suggesting low overfitting. This behavior is consistent across all epoch configurations, implying that increasing the number of epochs further offers no significant advantage in terms of loss reduction.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot illustrates the validation harmonic weighted accuracy (HWA) for different epoch configurations. The HWA shows significant fluctuations across epochs, especially for higher epoch configurations like 25 and 40 epochs. This variability may indicate instability in the model's generalization performance. However, the overall trend suggests that higher epochs (e.g., 40 epochs) tend to achieve slightly better peak accuracy compared to lower epochs, despite the fluctuations.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_val_HWA.png"}, {"analysis": "The third plot compares the final test metrics (SWA, CWA, and HWA) across different epoch configurations. The scores are relatively stable across all metrics and epochs, with only slight variations. This indicates that the model's performance does not significantly improve beyond a certain number of epochs, and the choice of 15 or 25 epochs might be optimal to balance training time and performance.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_test_metric_bars.png"}, {"analysis": "The fourth plot displays the confusion matrix for the 5-epoch configuration. The true positives and true negatives are reasonably high, but there is a notable number of false positives and false negatives. This suggests that the model struggles with certain classes, which could be addressed through better data balancing or additional feature engineering. The confusion matrix provides insights into specific areas where the model's predictions are misaligned with the ground truth.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/SPR_BENCH_confusion_5_epochs.png"}], [{"analysis": "This plot shows the training and validation loss over epochs for different training durations. The loss drops sharply initially and stabilizes after a few epochs, indicating that the model converges quickly. However, there is a slight gap between training and validation loss, which might suggest some degree of overfitting. Increasing the number of epochs beyond 15 does not significantly reduce the loss, implying that the model has reached its optimal learning capacity early on.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the Harmonic Weighted Accuracy (HWA) on the validation set across training epochs for different training durations. The HWA fluctuates significantly across epochs, particularly for longer training durations such as 40 epochs. This variability might indicate instability in model performance or sensitivity to hyperparameters. Overall, while the trends are noisy, there is no clear improvement in HWA with an increase in the number of epochs.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_val_HWA.png"}, {"analysis": "This bar chart compares the final test set metrics\u2014Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and Harmonic Weighted Accuracy (HWA)\u2014for different training durations. The scores are relatively consistent across all training durations, suggesting that extending training beyond 5 epochs does not yield significant performance gains. The similarity in metrics indicates that the model generalizes well to the test set regardless of training duration.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_test_metric_bars.png"}, {"analysis": "This confusion matrix for the model trained for 40 epochs highlights the distribution of true and predicted labels. The model achieves a reasonable balance between true positives and true negatives. However, the number of false positives and false negatives is non-negligible, suggesting room for improvement in classification accuracy. Adjusting the decision threshold or exploring alternative loss functions might help reduce these errors.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/SPR_BENCH_confusion_40_epochs.png"}], []], "vlm_feedback_summary": ["The plots demonstrate effective model training and validation, with the model\nachieving its best performance at epoch 4. The final validation metrics are\nconsistent and robust, with minimal overfitting observed. The confusion matrix\nhighlights good performance on the test set, though improvements in class\ndifferentiation could further enhance results.", "The plots indicate that the model converges quickly, and longer training does\nnot significantly improve the metrics. Validation performance is inconsistent,\nand the confusion matrix highlights a bias toward false negatives. Further\noptimization may focus on stabilizing validation performance and addressing\nlabel imbalance.", "The results suggest that a learning rate of 0.002 provides the fastest\nconvergence and slightly better generalization performance. However, the final\ntest HWA is comparable across all learning rates, indicating that the choice of\nlearning rate has limited impact on the ultimate model performance in this\nsetup.", "The analysis highlights that smaller batch sizes lead to faster convergence but\nmay introduce instability in validation performance. Larger batch sizes exhibit\nslower convergence but more stable trends. Despite these differences in training\nand validation dynamics, the test performance remains consistent across batch\nsizes, indicating robust generalization.", "The plots provide valuable insights into the impact of embedding dimensions on\nmodel performance. Smaller embedding dimensions (32, 64) show consistent\nimprovements in loss and accuracy metrics, while larger dimensions (128, 256)\nexhibit diminishing returns or instability. The U-shaped trend in the test HWA\nmetric highlights the importance of selecting an optimal embedding dimension to\nbalance model complexity and generalization.", "The plots provide insights into the impact of weight decay on training\nstability, peak performance, and generalization. Lower weight decay values lead\nto faster convergence but risk overfitting and instability, while higher weight\ndecay values offer more consistent generalization. The test metrics reveal that\nthe model performs better on color-weighted tasks compared to shape-weighted\ntasks, indicating areas for further improvement in reasoning capabilities.", "The plots highlight the impact of dropout rates on training and validation\nperformance across multiple metrics (SWA, CWA, HWA). Moderate dropout rates (0.1\nand 0.3) consistently yield better generalization across epochs and metrics.\nExcessive dropout (0.5) leads to underfitting, while no dropout (0.0) results in\nsuboptimal generalization. The results suggest that fine-tuning dropout rates is\ncrucial for optimizing model performance on SPR tasks.", "The plots collectively demonstrate the impact of label smoothing on model\nperformance. Increasing the label smoothing coefficient improves weighted\naccuracy metrics (SWA, CWA, HWA) while linearly increasing validation loss. The\nconfusion matrix highlights reasonable performance with room for reducing\nmisclassifications. Optimized label smoothing (0.20) achieves the highest\naccuracy on the CWA metric, indicating strong generalization to color\nvariations.", "The experimental results reveal that gradient clipping has minimal impact on\nloss trends and weighted accuracies, but extreme clipping values can cause\ninstability in validation HWA over epochs. The model shows balanced performance\nacross SWA, CWA, and HWA metrics, with no significant sensitivity to\nmax_grad_norm.", "The plots reveal that while the model avoids overfitting and achieves stable\nloss values, its performance metrics show inconsistency and limited improvement\nacross different epochs. The harmonic weighted accuracy fluctuates\nsignificantly, and the test metrics do not show substantial gains with increased\ntraining. The confusion matrix highlights challenges in reducing\nmisclassifications, suggesting the need for further optimization of\nhyperparameters or model adjustments.", "The provided plots offer valuable insights into model performance. The training\nand validation loss curves suggest early convergence with minimal overfitting.\nThe validation HWA plot indicates some instability in generalization, especially\nat higher epochs, though peak performance improves slightly. Final test metrics\nacross different epochs show stable results, suggesting diminishing returns with\nincreased epochs. The confusion matrix highlights areas for improvement in\nclass-specific predictions.", "The plots reveal that the model converges quickly, with minimal improvement in\nmetrics beyond 15 epochs. Validation performance is unstable, and the final\nmetrics suggest diminishing returns from longer training durations. The\nconfusion matrix indicates reasonable classification performance but highlights\nareas for potential improvement.", "[]"], "exec_time": [5.370668172836304, 45.01773190498352, 11.360636472702026, 50.09240984916687, 28.99705982208252, 10.108207941055298, 14.068934917449951, 55.17491936683655, 55.15344977378845, 45.317347049713135, 45.24588060379028, 115.25728559494019, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['0.1'", "'0.3']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- paths -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------- load data -----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch safely\ndef get(data, *keys, default=None):\n    for k in keys:\n        data = data.get(k, {})\n    return data if data != {} else default\n\n\nds_name = \"SPR_BENCH\"\nexp = experiment_data.get(ds_name, {})\n\nloss_train = get(exp, \"losses\", \"train\", default=[])\nloss_val = get(exp, \"losses\", \"val\", default=[])\nmetrics_val = get(exp, \"metrics\", \"val\", default=[])\ny_pred_t = exp.get(\"predictions\", [])\ny_true_t = exp.get(\"ground_truth\", [])\n\nepochs = np.arange(1, len(loss_train) + 1)\n\n# unpack validation metrics list-of-dicts into arrays\nswa_vals, cwa_vals, hwa_vals = [], [], []\nfor d in metrics_val:\n    if isinstance(d, dict):\n        swa_vals.append(d.get(\"SWA\", np.nan))\n        cwa_vals.append(d.get(\"CWA\", np.nan))\n        hwa_vals.append(d.get(\"HWA\", np.nan))\n\n# ----------- Plot 1: loss curves -----------\ntry:\n    plt.figure()\n    if len(loss_train) > 0:\n        plt.plot(epochs, loss_train, label=\"Train Loss\")\n    if len(loss_val) > 0:\n        plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{ds_name}: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------- Plot 2: validation metrics -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        plt.figure()\n        plt.plot(epochs, swa_vals, label=\"SWA\")\n        plt.plot(epochs, cwa_vals, label=\"CWA\")\n        plt.plot(epochs, hwa_vals, label=\"HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{ds_name}: Validation Metrics Over Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric plot: {e}\")\n    plt.close()\n\n# ----------- Plot 3: final test metrics bar chart -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        swa_t, cwa_t, hwa_t = (\n            swa_vals[-1],\n            cwa_vals[-1],\n            hwa_vals[-1],\n        )  # already computed during save\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        values = [swa_t, cwa_t, hwa_t]\n        plt.figure()\n        plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n        plt.ylim(0, 1)\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(f\"{ds_name}: Final Validation Metric Values\")\n        fname = os.path.join(working_dir, f\"{ds_name}_final_metrics_bar.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ----------- Plot 4: confusion matrix -----------\ntry:\n    if y_true_t and y_pred_t:\n        labels = sorted(list(set(y_true_t) | set(y_pred_t)))\n        label_to_idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for yt, yp in zip(y_true_t, y_pred_t):\n            cm[label_to_idx[yt], label_to_idx[yp]] += 1\n        plt.figure(figsize=(5, 4))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name}: Confusion Matrix (Test Set)\")\n        fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ----------- print final metrics -----------\nif swa_vals and cwa_vals and hwa_vals:\n    print(\n        f\"Final Validation Metrics -> SWA: {swa_vals[-1]:.4f}, CWA: {cwa_vals[-1]:.4f}, HWA: {hwa_vals[-1]:.4f}\"\n    )\nif y_true_t and y_pred_t:\n    # compute test metrics quickly\n    def c_variety(seq):\n        return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n    def s_variety(seq):\n        return len(set(tok[0] for tok in seq.split()))\n\n    sequences = y_true_t  # placeholder since sequences not stored for test; metrics already saved elsewhere\n    print(f\"Test predictions available: {len(y_pred_t)} samples\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- paths & loading -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data.get(\"epochs\", {})\n    # Collect final test metrics and identify best run (by HWA)\n    final_scores = {}\n    best_run, best_hwa = None, -1\n    for k, v in runs.items():\n        tm = v[\"test_metrics\"]\n        final_scores[k] = tm\n        if tm[\"HWA\"] > best_hwa:\n            best_hwa, best_run = tm[\"HWA\"], k\n        print(\n            f\"{k}: loss={tm['loss']:.4f}, SWA={tm['SWA']:.4f}, \"\n            f\"CWA={tm['CWA']:.4f}, HWA={tm['HWA']:.4f}\"\n        )\n\n    # ---------- Figure 1: loss curves ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            plt.plot(v[\"losses\"][\"train\"], \"--\", label=f\"{k} train\")\n            plt.plot(v[\"losses\"][\"val\"], \"-\", label=f\"{k} val\")\n        plt.title(\"SPR_BENCH Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 2: validation HWA ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            hwa = [m[\"HWA\"] for m in v[\"metrics\"][\"val\"]]\n            plt.plot(hwa, label=k)\n        plt.title(\"SPR_BENCH Validation Harmonic Weighted Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_val_HWA.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 3: bar chart of final test metrics ----------\n    try:\n        plt.figure()\n        labels = list(final_scores.keys())\n        x = np.arange(len(labels))\n        width = 0.25\n        swa_vals = [final_scores[k][\"SWA\"] for k in labels]\n        cwa_vals = [final_scores[k][\"CWA\"] for k in labels]\n        hwa_vals = [final_scores[k][\"HWA\"] for k in labels]\n        plt.bar(x - width, swa_vals, width, label=\"SWA\")\n        plt.bar(x, cwa_vals, width, label=\"CWA\")\n        plt.bar(x + width, hwa_vals, width, label=\"HWA\")\n        plt.title(\"SPR_BENCH Final Test Metrics\")\n        plt.xticks(x, labels, rotation=45, ha=\"right\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_test_metric_bars.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 4: confusion matrix of best run ----------\n    try:\n        import itertools\n        from collections import Counter\n\n        best_pred = runs[best_run][\"predictions\"]\n        best_gt = runs[best_run][\"ground_truth\"]\n        labels_set = sorted(set(best_gt))\n        idx = {l: i for i, l in enumerate(labels_set)}\n        cm = np.zeros((len(labels_set), len(labels_set)), dtype=int)\n        for t, p in zip(best_gt, best_pred):\n            cm[idx[t], idx[p]] += 1\n\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH Confusion Matrix ({best_run})\")\n        plt.xticks(range(len(labels_set)), labels_set, rotation=90)\n        plt.yticks(range(len(labels_set)), labels_set)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        # annotate cells\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                fontsize=6,\n            )\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_run}.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----- paths -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load -----\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    lr_dict = experiment_data.get(\"learning_rate\", {}).get(\"SPR_BENCH\", {})\n    lrs = list(lr_dict.keys())\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        for lr in lrs:\n            losses = lr_dict[lr][\"losses\"]\n            epochs = np.arange(len(losses[\"train\"]))\n            # subsample to at most 5 points\n            idx = np.linspace(0, len(epochs) - 1, min(5, len(epochs))).astype(int)\n            plt.plot(\n                epochs[idx],\n                np.array(losses[\"train\"])[idx],\n                marker=\"o\",\n                label=f\"train lr={lr}\",\n            )\n            plt.plot(\n                epochs[idx],\n                np.array(losses[\"val\"])[idx],\n                marker=\"x\",\n                linestyle=\"--\",\n                label=f\"val lr={lr}\",\n            )\n        plt.title(\"SPR_BENCH Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- 2. validation HWA ----------\n    try:\n        plt.figure()\n        for lr in lrs:\n            hwa_vals = [m[\"HWA\"] for m in lr_dict[lr][\"metrics\"][\"val\"]]\n            epochs = np.arange(len(hwa_vals))\n            idx = np.linspace(0, len(epochs) - 1, min(5, len(epochs))).astype(int)\n            plt.plot(\n                epochs[idx],\n                np.array(hwa_vals)[idx],\n                marker=\"s\",\n                label=f\"val HWA lr={lr}\",\n            )\n        plt.title(\"SPR_BENCH Validation HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_HWA.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # ---------- 3. test HWA bar chart ----------\n    try:\n        plt.figure()\n        test_hwa = [lr_dict[lr][\"test_metrics\"][\"HWA\"] for lr in lrs]\n        plt.bar(np.arange(len(lrs)), test_hwa, tick_label=lrs)\n        plt.title(\"SPR_BENCH Final Test HWA per Learning Rate\")\n        plt.ylabel(\"HWA\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_HWA_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test HWA bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------- setup --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data.get(\"batch_size_tuning\", {})\n    bss = sorted(int(k.split(\"_\")[1]) for k in runs.keys())\n    run_keys = [f\"bs_{bs}\" for bs in bss]\n\n    # Extract needed arrays\n    train_losses = {rk: runs[rk][\"losses\"][\"train\"] for rk in run_keys}\n    val_losses = {rk: runs[rk][\"losses\"][\"val\"] for rk in run_keys}\n    val_hwa = {rk: [m[\"HWA\"] for m in runs[rk][\"metrics\"][\"val\"]] for rk in run_keys}\n    test_hwa = {rk: runs[rk][\"test_metrics\"][\"HWA\"] for rk in run_keys}\n\n    # ---- Figure 1 : Loss curves ----\n    try:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        for rk in run_keys:\n            axs[0].plot(train_losses[rk], label=rk)\n            axs[1].plot(val_losses[rk], label=rk)\n        axs[0].set_title(\"Train Loss\")\n        axs[1].set_title(\"Validation Loss\")\n        for ax in axs:\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Loss\")\n            ax.legend()\n        fig.suptitle(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.tight_layout()\n        path = os.path.join(working_dir, \"SPR_BENCH_batch_size_loss_curves.png\")\n        plt.savefig(path)\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss curve figure: {e}\")\n        plt.close()\n\n    # ---- Figure 2 : Validation HWA ----\n    try:\n        plt.figure(figsize=(6, 4))\n        for rk in run_keys:\n            plt.plot(val_hwa[rk], label=rk)\n        plt.title(\"SPR_BENCH Validation HWA across Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend()\n        plt.tight_layout()\n        path = os.path.join(working_dir, \"SPR_BENCH_validation_HWA_curves.png\")\n        plt.savefig(path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation HWA figure: {e}\")\n        plt.close()\n\n    # ---- Figure 3 : Test HWA bar chart ----\n    try:\n        plt.figure(figsize=(6, 4))\n        bs_labels = [str(bs) for bs in bss]\n        hwa_vals = [test_hwa[f\"bs_{bs}\"] for bs in bss]\n        plt.bar(bs_labels, hwa_vals, color=\"skyblue\")\n        plt.title(\"SPR_BENCH Test HWA vs Batch Size\")\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"HWA\")\n        for i, val in enumerate(hwa_vals):\n            plt.text(i, val + 0.005, f\"{val:.2f}\", ha=\"center\")\n        plt.tight_layout()\n        path = os.path.join(working_dir, \"SPR_BENCH_test_HWA_bar.png\")\n        plt.savefig(path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test HWA bar figure: {e}\")\n        plt.close()\n\nprint(\"Done\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    data_tree = experiment_data.get(\"embed_dim\", {}).get(\"SPR_BENCH\", {})\n    embed_dims = sorted(data_tree.keys(), key=lambda x: int(x))\n\n    # per-dimension plots\n    for emb in embed_dims:\n        dat = data_tree[emb]\n        losses_tr = dat[\"losses\"][\"train\"]\n        losses_val = dat[\"losses\"][\"val\"]\n        metrics_val = dat[\"metrics\"][\"val\"]  # list of dicts per epoch\n        epochs = np.arange(1, len(losses_tr) + 1)\n\n        # -------- loss curve --------\n        try:\n            plt.figure()\n            plt.plot(epochs, losses_tr, label=\"Train Loss\")\n            plt.plot(epochs, losses_val, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"Dataset: SPR_BENCH | Train vs Val Loss (embed_dim={emb})\")\n            plt.legend()\n            fname = f\"SPR_BENCH_loss_curves_embed{emb}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for emb={emb}: {e}\")\n            plt.close()\n\n        # -------- metric curve --------\n        try:\n            swa = [m[\"SWA\"] for m in metrics_val]\n            cwa = [m[\"CWA\"] for m in metrics_val]\n            hwa = [m[\"HWA\"] for m in metrics_val]\n\n            plt.figure()\n            plt.plot(epochs, swa, label=\"SWA\")\n            plt.plot(epochs, cwa, label=\"CWA\")\n            plt.plot(epochs, hwa, label=\"HWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"Dataset: SPR_BENCH | Val Metrics (embed_dim={emb})\")\n            plt.legend()\n            fname = f\"SPR_BENCH_metrics_embed{emb}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating metric plot for emb={emb}: {e}\")\n            plt.close()\n\n    # -------- summary plot across embedding sizes --------\n    try:\n        test_hwa = [data_tree[emb][\"test_metrics\"][\"HWA\"] for emb in embed_dims]\n        plt.figure()\n        plt.plot([int(e) for e in embed_dims], test_hwa, marker=\"o\")\n        plt.xlabel(\"Embedding Dimension\")\n        plt.ylabel(\"Test HWA\")\n        plt.title(\"Dataset: SPR_BENCH | Test HWA vs Embedding Dimension\")\n        fname = \"SPR_BENCH_test_HWA_vs_embedding.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating summary plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None:\n    decays = exp[\"decay_values\"]\n    tr_losses = exp[\"losses\"][\"train\"]  # list[len(decays)][epochs]\n    val_losses = exp[\"losses\"][\"val\"]\n    val_metrics = exp[\"metrics\"][\"val\"]  # list of list of dicts\n    best_val_hwa = exp[\"best_val_hwa\"]\n    test_metrics = exp[\"test_metrics\"][0] if exp[\"test_metrics\"] else {}\n    epochs = range(1, len(tr_losses[0]) + 1)\n\n    # ----------- 1. loss curves ----------\n    try:\n        plt.figure(figsize=(6, 4))\n        for i, wd in enumerate(decays):\n            plt.plot(epochs, tr_losses[i], label=f\"train wd={wd}\")\n            plt.plot(epochs, val_losses[i], linestyle=\"--\", label=f\"val wd={wd}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\n            \"SPR_BENCH: Training vs Validation Loss\\nLeft: Train, Right: Val (per weight decay)\"\n        )\n        plt.legend(fontsize=7)\n        file_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves_weight_decay.png\")\n        plt.savefig(file_path, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ----------- 2. HWA curves -----------\n    try:\n        plt.figure(figsize=(6, 4))\n        for i, wd in enumerate(decays):\n            hwa = [m[\"HWA\"] for m in val_metrics[i]]\n            plt.plot(epochs, hwa, label=f\"wd={wd}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Acc\")\n        plt.title(\"SPR_BENCH: Dev HWA over Epochs\\nLines: Different weight decays\")\n        plt.legend(fontsize=7)\n        file_path = os.path.join(working_dir, \"SPR_BENCH_val_HWA_curves.png\")\n        plt.savefig(file_path, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA curve plot: {e}\")\n        plt.close()\n\n    # ----------- 3. final dev HWA bar ----\n    try:\n        plt.figure(figsize=(5, 3))\n        plt.bar(range(len(decays)), best_val_hwa, tick_label=[str(d) for d in decays])\n        plt.ylabel(\"Final Dev HWA\")\n        plt.xlabel(\"Weight Decay\")\n        plt.title(\n            \"SPR_BENCH: Final Dev HWA per Weight Decay\\nBar height = last-epoch HWA\"\n        )\n        file_path = os.path.join(working_dir, \"SPR_BENCH_final_dev_HWA_bar.png\")\n        plt.savefig(file_path, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating dev HWA bar plot: {e}\")\n        plt.close()\n\n    # ----------- 4. test metrics bar -----\n    try:\n        if test_metrics:\n            names = [\"SWA\", \"CWA\", \"HWA\"]\n            vals = [test_metrics[k] for k in names]\n            plt.figure(figsize=(4, 3))\n            plt.bar(names, vals, color=[\"tab:blue\", \"tab:orange\", \"tab:green\"])\n            plt.ylim(0, 1)\n            plt.title(\"SPR_BENCH: Test Metrics (Best Model)\\nLeft\u2192Right: SWA, CWA, HWA\")\n            file_path = os.path.join(working_dir, \"SPR_BENCH_test_metrics_bar.png\")\n            plt.savefig(file_path, dpi=150, bbox_inches=\"tight\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\n\n    # ----------- console summary ----------\n    try:\n        best_idx = int(np.argmax(best_val_hwa))\n        print(f\"Best decay={decays[best_idx]} | Dev HWA={best_val_hwa[best_idx]:.4f}\")\n        if test_metrics:\n            print(\"Test metrics:\", {k: round(v, 4) for k, v in test_metrics.items()})\n    except Exception as e:\n        print(f\"Error printing summary: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef get_metric_list(exp_key, metric_name):\n    vals = experiment_data[exp_key][\"SPR_BENCH\"][\"metrics\"][\"val\"]\n    return [m.get(metric_name) if isinstance(m, dict) else None for m in vals]\n\n\n# ---------- figure 1: loss curves ----------\ntry:\n    plt.figure()\n    for k in experiment_data:\n        tr = experiment_data[k][\"SPR_BENCH\"][\"losses\"][\"train\"]\n        va = experiment_data[k][\"SPR_BENCH\"][\"losses\"][\"val\"]\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, \"--\", label=f\"{k} train\")\n        plt.plot(epochs, va, \"-\", label=f\"{k} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\\n(dropout comparison)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ---------- figure 2/3/4: metric curves ----------\nfor metric in [\"SWA\", \"CWA\", \"HWA\"]:\n    try:\n        plt.figure()\n        for k in experiment_data:\n            vals = get_metric_list(k, metric)\n            epochs = range(1, len(vals) + 1)\n            plt.plot(epochs, vals, label=k)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(metric)\n        plt.title(f\"SPR_BENCH: Validation {metric} vs Epoch\\n(dropout comparison)\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_{metric}_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {metric} curves: {e}\")\n        plt.close()\n\n# ---------- figure 5: final HWA bar chart ----------\ntry:\n    plt.figure()\n    names, final_hwa = [], []\n    for k in experiment_data:\n        hwa_vals = get_metric_list(k, \"HWA\")\n        if hwa_vals and hwa_vals[-1] is not None:\n            names.append(k)\n            final_hwa.append(hwa_vals[-1])\n    plt.bar(names, final_hwa, color=\"skyblue\")\n    plt.ylabel(\"Final Epoch HWA\")\n    plt.title(\"SPR_BENCH: Final Validation HWA per Dropout\\n(bar chart)\")\n    plt.xticks(rotation=45, ha=\"right\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_HWA_bar.png\")\n    plt.tight_layout()\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final HWA bar chart: {e}\")\n    plt.close()\n\nprint(\"Generated figures:\", [f for f in os.listdir(working_dir) if f.endswith(\".png\")])\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"label_smoothing\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None:\n    sm_vals = exp[\"smoothing_values\"]\n    val_loss = exp[\"losses\"][\"val\"]\n    val_mets = exp[\"metrics\"][\"val\"]\n    swa_vals = [d[\"SWA\"] for d in val_mets]\n    cwa_vals = [d[\"CWA\"] for d in val_mets]\n    hwa_vals = [d[\"HWA\"] for d in val_mets]\n    preds = exp[\"predictions\"]\n    gts = exp[\"ground_truth\"]\n    best_sm = exp[\"best_smoothing\"]\n    tst_mets = exp[\"test_metrics\"]\n    tst_loss = exp[\"test_loss\"]\n\n    # ---------- 1) Validation loss vs smoothing ----------\n    try:\n        plt.figure()\n        plt.plot(sm_vals, val_loss, marker=\"o\")\n        plt.title(\n            \"SPR_BENCH: Validation Loss vs Label Smoothing\\nLeft: coef, Right: loss\"\n        )\n        plt.xlabel(\"Label Smoothing Coefficient\")\n        plt.ylabel(\"Validation Loss\")\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_loss_vs_smoothing.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val-loss plot: {e}\")\n        plt.close()\n\n    # ---------- 2) Validation weighted accuracies ----------\n    try:\n        plt.figure()\n        plt.plot(sm_vals, swa_vals, marker=\"o\", label=\"SWA\")\n        plt.plot(sm_vals, cwa_vals, marker=\"s\", label=\"CWA\")\n        plt.plot(sm_vals, hwa_vals, marker=\"^\", label=\"HWA\")\n        plt.title(\n            \"SPR_BENCH: Weighted Accuracies vs Label Smoothing\\nLeft: coef, Right: accuracy\"\n        )\n        plt.xlabel(\"Label Smoothing Coefficient\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.savefig(\n            os.path.join(\n                working_dir, \"SPR_BENCH_val_weighted_accuracies_vs_smoothing.png\"\n            )\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val-acc plot: {e}\")\n        plt.close()\n\n    # ---------- 3) Confusion matrix on test ----------\n    try:\n        labels = sorted(set(gts) | set(preds))\n        lbl2idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[lbl2idx[t], lbl2idx[p]] += 1\n\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.title(\"SPR_BENCH: Test Confusion Matrix\\nRows: True, Cols: Pred\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion-matrix plot: {e}\")\n        plt.close()\n\n    # ---------- 4) Test weighted accuracies bar ----------\n    try:\n        plt.figure()\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        values = [tst_mets[m] for m in metrics]\n        plt.bar(metrics, values, color=[\"tab:orange\", \"tab:green\", \"tab:purple\"])\n        plt.ylim(0, 1)\n        plt.title(f\"SPR_BENCH: Test Weighted Accuracies (best smoothing={best_sm:.2f})\")\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_test_weighted_accuracies_bar.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test-acc bar plot: {e}\")\n        plt.close()\n\n    # ---------- print summary ----------\n    print(f\"Best smoothing on dev: {best_sm:.2f} | dev HWA={max(hwa_vals):.4f}\")\n    print(\n        f'TEST | loss={tst_loss:.4f} | SWA={tst_mets[\"SWA\"]:.4f} | CWA={tst_mets[\"CWA\"]:.4f} | HWA={tst_mets[\"HWA\"]:.4f}'\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"max_grad_norm\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\nif runs:\n    clips = sorted(runs.keys(), key=float)\n    epochs = len(runs[clips[0]][\"losses\"][\"train\"])\n\n    # ------------- Figure 1 : train / val loss curves -------------\n    try:\n        plt.figure()\n        for c in clips:\n            plt.plot(\n                range(1, epochs + 1),\n                runs[c][\"losses\"][\"train\"],\n                label=f\"train (clip={c})\",\n                linestyle=\"-\",\n            )\n            plt.plot(\n                range(1, epochs + 1),\n                runs[c][\"losses\"][\"val\"],\n                label=f\"val   (clip={c})\",\n                linestyle=\"--\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\\nSolid: Train, Dashed: Validation\")\n        plt.legend(fontsize=6, ncol=2)\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ------------- Figure 2 : test weighted accuracies -------------\n    try:\n        ind = np.arange(len(clips))\n        width = 0.25\n        swa = [runs[c][\"test_metrics\"][\"SWA\"] for c in clips]\n        cwa = [runs[c][\"test_metrics\"][\"CWA\"] for c in clips]\n        hwa = [runs[c][\"test_metrics\"][\"HWA\"] for c in clips]\n\n        plt.figure(figsize=(8, 4))\n        plt.bar(ind - width, swa, width, label=\"SWA\")\n        plt.bar(ind, cwa, width, label=\"CWA\")\n        plt.bar(ind + width, hwa, width, label=\"HWA\")\n        plt.xticks(ind, clips)\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(\"SPR_BENCH Test Weighted Accuracies\\nEffect of max_grad_norm\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_weighted_accuracies.png\")\n        plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating weighted accuracy plot: {e}\")\n        plt.close()\n\n    # ------------- Figure 3 : val HWA over epochs (clip 0 vs 5) -------------\n    try:\n        focus = [clips[0], clips[-1]]  # smallest and largest clipping norms\n        plt.figure()\n        for c in focus:\n            hwas = [m[\"HWA\"] for m in runs[c][\"metrics\"][\"val\"]]\n            plt.plot(range(1, epochs + 1), hwas, label=f\"clip={c}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation HWA\")\n        plt.title(\n            \"SPR_BENCH Validation HWA Over Epochs\\nComparing Extreme Clipping Values\"\n        )\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_HWA_over_epochs.png\")\n        plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val HWA plot: {e}\")\n        plt.close()\n\n    # ---------- print best metrics ----------\n    best_clip = max(clips, key=lambda c: runs[c][\"test_metrics\"][\"HWA\"])\n    best = runs[best_clip][\"test_metrics\"]\n    print(f\"Best clip value: {best_clip}\")\n    print(\n        f\"Test metrics -> Loss: {best['loss']:.4f}, SWA: {best['SWA']:.4f}, \"\n        f\"CWA: {best['CWA']:.4f}, HWA: {best['HWA']:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- paths & loading -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data.get(\"epochs\", {})\n    # Collect final test metrics and identify best run (by HWA)\n    final_scores = {}\n    best_run, best_hwa = None, -1\n    for k, v in runs.items():\n        tm = v[\"test_metrics\"]\n        final_scores[k] = tm\n        if tm[\"HWA\"] > best_hwa:\n            best_hwa, best_run = tm[\"HWA\"], k\n        print(\n            f\"{k}: loss={tm['loss']:.4f}, SWA={tm['SWA']:.4f}, \"\n            f\"CWA={tm['CWA']:.4f}, HWA={tm['HWA']:.4f}\"\n        )\n\n    # ---------- Figure 1: loss curves ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            plt.plot(v[\"losses\"][\"train\"], \"--\", label=f\"{k} train\")\n            plt.plot(v[\"losses\"][\"val\"], \"-\", label=f\"{k} val\")\n        plt.title(\"SPR_BENCH Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 2: validation HWA ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            hwa = [m[\"HWA\"] for m in v[\"metrics\"][\"val\"]]\n            plt.plot(hwa, label=k)\n        plt.title(\"SPR_BENCH Validation Harmonic Weighted Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_val_HWA.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 3: bar chart of final test metrics ----------\n    try:\n        plt.figure()\n        labels = list(final_scores.keys())\n        x = np.arange(len(labels))\n        width = 0.25\n        swa_vals = [final_scores[k][\"SWA\"] for k in labels]\n        cwa_vals = [final_scores[k][\"CWA\"] for k in labels]\n        hwa_vals = [final_scores[k][\"HWA\"] for k in labels]\n        plt.bar(x - width, swa_vals, width, label=\"SWA\")\n        plt.bar(x, cwa_vals, width, label=\"CWA\")\n        plt.bar(x + width, hwa_vals, width, label=\"HWA\")\n        plt.title(\"SPR_BENCH Final Test Metrics\")\n        plt.xticks(x, labels, rotation=45, ha=\"right\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_test_metric_bars.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 4: confusion matrix of best run ----------\n    try:\n        import itertools\n        from collections import Counter\n\n        best_pred = runs[best_run][\"predictions\"]\n        best_gt = runs[best_run][\"ground_truth\"]\n        labels_set = sorted(set(best_gt))\n        idx = {l: i for i, l in enumerate(labels_set)}\n        cm = np.zeros((len(labels_set), len(labels_set)), dtype=int)\n        for t, p in zip(best_gt, best_pred):\n            cm[idx[t], idx[p]] += 1\n\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH Confusion Matrix ({best_run})\")\n        plt.xticks(range(len(labels_set)), labels_set, rotation=90)\n        plt.yticks(range(len(labels_set)), labels_set)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        # annotate cells\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                fontsize=6,\n            )\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_run}.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- paths & loading -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data.get(\"epochs\", {})\n    # Collect final test metrics and identify best run (by HWA)\n    final_scores = {}\n    best_run, best_hwa = None, -1\n    for k, v in runs.items():\n        tm = v[\"test_metrics\"]\n        final_scores[k] = tm\n        if tm[\"HWA\"] > best_hwa:\n            best_hwa, best_run = tm[\"HWA\"], k\n        print(\n            f\"{k}: loss={tm['loss']:.4f}, SWA={tm['SWA']:.4f}, \"\n            f\"CWA={tm['CWA']:.4f}, HWA={tm['HWA']:.4f}\"\n        )\n\n    # ---------- Figure 1: loss curves ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            plt.plot(v[\"losses\"][\"train\"], \"--\", label=f\"{k} train\")\n            plt.plot(v[\"losses\"][\"val\"], \"-\", label=f\"{k} val\")\n        plt.title(\"SPR_BENCH Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 2: validation HWA ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            hwa = [m[\"HWA\"] for m in v[\"metrics\"][\"val\"]]\n            plt.plot(hwa, label=k)\n        plt.title(\"SPR_BENCH Validation Harmonic Weighted Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_val_HWA.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 3: bar chart of final test metrics ----------\n    try:\n        plt.figure()\n        labels = list(final_scores.keys())\n        x = np.arange(len(labels))\n        width = 0.25\n        swa_vals = [final_scores[k][\"SWA\"] for k in labels]\n        cwa_vals = [final_scores[k][\"CWA\"] for k in labels]\n        hwa_vals = [final_scores[k][\"HWA\"] for k in labels]\n        plt.bar(x - width, swa_vals, width, label=\"SWA\")\n        plt.bar(x, cwa_vals, width, label=\"CWA\")\n        plt.bar(x + width, hwa_vals, width, label=\"HWA\")\n        plt.title(\"SPR_BENCH Final Test Metrics\")\n        plt.xticks(x, labels, rotation=45, ha=\"right\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_test_metric_bars.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 4: confusion matrix of best run ----------\n    try:\n        import itertools\n        from collections import Counter\n\n        best_pred = runs[best_run][\"predictions\"]\n        best_gt = runs[best_run][\"ground_truth\"]\n        labels_set = sorted(set(best_gt))\n        idx = {l: i for i, l in enumerate(labels_set)}\n        cm = np.zeros((len(labels_set), len(labels_set)), dtype=int)\n        for t, p in zip(best_gt, best_pred):\n            cm[idx[t], idx[p]] += 1\n\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH Confusion Matrix ({best_run})\")\n        plt.xticks(range(len(labels_set)), labels_set, rotation=90)\n        plt.yticks(range(len(labels_set)), labels_set)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        # annotate cells\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                fontsize=6,\n            )\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_run}.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- paths & loading -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data.get(\"epochs\", {})\n    # Collect final test metrics and identify best run (by HWA)\n    final_scores = {}\n    best_run, best_hwa = None, -1\n    for k, v in runs.items():\n        tm = v[\"test_metrics\"]\n        final_scores[k] = tm\n        if tm[\"HWA\"] > best_hwa:\n            best_hwa, best_run = tm[\"HWA\"], k\n        print(\n            f\"{k}: loss={tm['loss']:.4f}, SWA={tm['SWA']:.4f}, \"\n            f\"CWA={tm['CWA']:.4f}, HWA={tm['HWA']:.4f}\"\n        )\n\n    # ---------- Figure 1: loss curves ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            plt.plot(v[\"losses\"][\"train\"], \"--\", label=f\"{k} train\")\n            plt.plot(v[\"losses\"][\"val\"], \"-\", label=f\"{k} val\")\n        plt.title(\"SPR_BENCH Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 2: validation HWA ----------\n    try:\n        plt.figure()\n        for k, v in runs.items():\n            hwa = [m[\"HWA\"] for m in v[\"metrics\"][\"val\"]]\n            plt.plot(hwa, label=k)\n        plt.title(\"SPR_BENCH Validation Harmonic Weighted Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_val_HWA.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 3: bar chart of final test metrics ----------\n    try:\n        plt.figure()\n        labels = list(final_scores.keys())\n        x = np.arange(len(labels))\n        width = 0.25\n        swa_vals = [final_scores[k][\"SWA\"] for k in labels]\n        cwa_vals = [final_scores[k][\"CWA\"] for k in labels]\n        hwa_vals = [final_scores[k][\"HWA\"] for k in labels]\n        plt.bar(x - width, swa_vals, width, label=\"SWA\")\n        plt.bar(x, cwa_vals, width, label=\"CWA\")\n        plt.bar(x + width, hwa_vals, width, label=\"HWA\")\n        plt.title(\"SPR_BENCH Final Test Metrics\")\n        plt.xticks(x, labels, rotation=45, ha=\"right\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_test_metric_bars.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n    # ---------- Figure 4: confusion matrix of best run ----------\n    try:\n        import itertools\n        from collections import Counter\n\n        best_pred = runs[best_run][\"predictions\"]\n        best_gt = runs[best_run][\"ground_truth\"]\n        labels_set = sorted(set(best_gt))\n        idx = {l: i for i, l in enumerate(labels_set)}\n        cm = np.zeros((len(labels_set), len(labels_set)), dtype=int)\n        for t, p in zip(best_gt, best_pred):\n            cm[idx[t], idx[p]] += 1\n\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.title(f\"SPR_BENCH Confusion Matrix ({best_run})\")\n        plt.xticks(range(len(labels_set)), labels_set, rotation=90)\n        plt.yticks(range(len(labels_set)), labels_set)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        # annotate cells\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                fontsize=6,\n            )\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_run}.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport math\n\n# ----------------- paths & loading -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# List supplied in the \"Experiment Data Path\" section\nexperiment_data_path_list = [\n    \"experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_aef9cbfec21144c0940d0ce40a6ce7df_proc_457456/experiment_data.npy\",\n    \"experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_dd17ad7dfe66429bae7a28c8530af256_proc_457454/experiment_data.npy\",\n    \"experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f633eb88b3df4d589b8dc0e84c527352_proc_457453/experiment_data.npy\",\n]\n\nall_best_runs = []  # containers for best-run dicts (one per seed/file)\nall_test_metrics = []  # list of dicts: {'SWA':..,'CWA':..,'HWA':..}\n\n# ----------------- read & collect -----------------\nfor p in experiment_data_path_list:\n    try:\n        full_p = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        edata = np.load(full_p, allow_pickle=True).item()\n    except Exception as e:\n        print(f\"Error loading experiment data from {p}: {e}\")\n        continue\n\n    runs = edata.get(\"epochs\", {})\n    # choose best run inside this file\n    best_run_name, best_hwa = None, -math.inf\n    for k, v in runs.items():\n        hwa_val = v[\"test_metrics\"][\"HWA\"]\n        if hwa_val > best_hwa:\n            best_hwa, best_run_name = hwa_val, k\n    if best_run_name is None:\n        continue\n    best_run = runs[best_run_name]\n    all_best_runs.append(best_run)\n    all_test_metrics.append(best_run[\"test_metrics\"])\n\n# Stop if nothing loaded\nif len(all_best_runs) == 0:\n    print(\"No experiment data successfully loaded \u2013 nothing to plot.\")\nelse:\n    # -------- align epoch lengths --------\n    min_epochs = min(len(r[\"losses\"][\"train\"]) for r in all_best_runs)\n\n    train_loss_mat = np.vstack(\n        [r[\"losses\"][\"train\"][:min_epochs] for r in all_best_runs]\n    )\n    val_loss_mat = np.vstack([r[\"losses\"][\"val\"][:min_epochs] for r in all_best_runs])\n    val_hwa_mat = np.vstack(\n        [[m[\"HWA\"] for m in r[\"metrics\"][\"val\"][:min_epochs]] for r in all_best_runs]\n    )\n\n    # -------- compute aggregates --------\n    def mean_and_se(mat):\n        mean = mat.mean(axis=0)\n        se = mat.std(axis=0, ddof=1) / np.sqrt(mat.shape[0])\n        return mean, se\n\n    train_mean, train_se = mean_and_se(train_loss_mat)\n    val_mean, val_se = mean_and_se(val_loss_mat)\n    hwa_mean, hwa_se = mean_and_se(val_hwa_mat)\n\n    # aggregate test metrics\n    test_metrics_arr = {\n        k: np.array([m[k] for m in all_test_metrics]) for k in [\"SWA\", \"CWA\", \"HWA\"]\n    }\n    test_mean = {k: float(v.mean()) for k, v in test_metrics_arr.items()}\n    test_se = {\n        k: float(v.std(ddof=1) / np.sqrt(v.size)) for k, v in test_metrics_arr.items()\n    }\n\n    # ------------ Figure 1: aggregated loss curves -------------\n    try:\n        plt.figure()\n        epochs = np.arange(min_epochs)\n        plt.fill_between(\n            epochs,\n            train_mean - train_se,\n            train_mean + train_se,\n            alpha=0.3,\n            label=\"Train Loss \u00b1 SE\",\n        )\n        plt.plot(epochs, train_mean, \"--\", color=\"C0\")\n        plt.fill_between(\n            epochs,\n            val_mean - val_se,\n            val_mean + val_se,\n            alpha=0.3,\n            label=\"Val Loss \u00b1 SE\",\n        )\n        plt.plot(epochs, val_mean, \"-\", color=\"C1\")\n        plt.title(\n            \"SPR_BENCH (Aggregated) Training vs Validation Loss\\nMean over Seeds with Standard Error\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend(fontsize=7)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot: {e}\")\n        plt.close()\n\n    # ------------ Figure 2: aggregated validation HWA ----------\n    try:\n        plt.figure()\n        epochs = np.arange(min_epochs)\n        plt.fill_between(\n            epochs,\n            hwa_mean - hwa_se,\n            hwa_mean + hwa_se,\n            alpha=0.3,\n            color=\"C2\",\n            label=\"HWA \u00b1 SE\",\n        )\n        plt.plot(epochs, hwa_mean, color=\"C2\")\n        plt.title(\n            \"SPR_BENCH (Aggregated) Validation Harmonic Weighted Accuracy\\nMean over Seeds with Standard Error\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend(fontsize=7)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_val_HWA.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated HWA plot: {e}\")\n        plt.close()\n\n    # ------------ Figure 3: final test metrics -----------------\n    try:\n        plt.figure()\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        x = np.arange(len(metrics))\n        means = [test_mean[m] for m in metrics]\n        ses = [test_se[m] for m in metrics]\n        plt.bar(x, means, yerr=ses, capsize=5, color=[\"C0\", \"C1\", \"C2\"])\n        plt.xticks(x, metrics)\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH Final Test Metrics\\nMean over Seeds with Standard Error\")\n        for xi, m, se in zip(x, means, ses):\n            plt.text(xi, m + se + 0.01, f\"{m:.3f}\u00b1{se:.3f}\", ha=\"center\", fontsize=7)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_agg_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated test-metric bar plot: {e}\")\n        plt.close()\n\n    # ------------- console printout -----------------\n    print(\"=== Aggregated Final Test Metrics (mean \u00b1 SE) ===\")\n    for k in [\"SWA\", \"CWA\", \"HWA\"]:\n        print(f\"{k}: {test_mean[k]:.4f} \u00b1 {test_se[k]:.4f}\")\n"], "plot_plan": ["We first load the saved experiment_data.npy dictionary, extract the losses recorded per epoch, the validation metrics (SWA, CWA, HWA), and the final test-set predictions/ground-truth. We then create up to four standard plots inside separate try-except blocks: (1) a line plot of training vs. validation loss over epochs, (2) a line plot of the three validation metrics over epochs, (3) a bar chart comparing the final test-set SWA, CWA and HWA values, and (4) a confusion-matrix heat-map built from the test predictions. Each plot is titled, saved to working_dir with descriptive filenames that include the dataset name, then closed. Figures are limited to exactly those four to respect the \u201c\u2264 5\u201d rule. All code uses only the data present in experiment_data.npy and basic matplotlib; no synthetic data are generated. Finally, the script prints the final test metrics so they appear in the notebook / console output.", "We first load the serialized dictionary from experiment_data.npy and extract the loss trajectories, validation metric trajectories, and final test metrics for every hyper-parameter setting (e.g. \u201c5_epochs\u201d, \u201c15_epochs\u201d\u2026).  Using this information we create at most four figures: (1) an overlay of training/validation loss curves for every run, (2) an overlay of validation HWA curves, (3) a grouped bar chart comparing the final test SWA, CWA and HWA across runs, and (4) a confusion-matrix-style heat-map for the run that achieved the best test HWA (computed directly from its stored prediction/ground-truth lists).  Each figure is wrapped in an independent try-except block so that failures do not halt the rest of the script; every figure is saved to working_dir with clear, descriptive filenames that include the dataset name and plot type, is properly titled and then closed.  We also print the final test metrics for every run so the user can see numeric values in the console.  All plotting strictly uses data found in experiment_data.npy\u2014no fabricated points.  The script begins with the required imports and working_dir definition, guarantees working_dir exists, and respects the \u201cat most five figures\u201d rule by producing four.  Confusion matrices are computed only for the best run to avoid excessive plots.  The entire procedure is kept concise and self-contained inside one executable block.", "We will start by loading the saved experiment_data.npy into a dictionary, then iterate over the \u201clearning_rate \u2192 SPR_BENCH \u2192 {lr}\u201d hierarchy to collect per-epoch training/validation losses and validation HWA scores.  \nThree figures will be generated:  \n1) A single plot that overlays the training and validation loss curves for each learning rate.  \n2) A plot that overlays the validation HWA curves for each learning rate.  \n3) A bar chart that summarizes the final test HWA for each learning rate.  \nEach figure is wrapped in its own try-except block, saved with descriptive names, and the figure is closed afterward.  \nThe code follows the required imports/header, uses only values found in experiment_data.npy and limits the number of plotted epochs to at most five points per run (by subsampling if needed).  \nAny loading or plotting error is caught and printed while still guaranteeing figures are closed.", "We first load the saved experiment_data.npy, gather the per-epoch train/val losses and validation HWA for every batch-size run and also collect the final test HWA. Three visualizations are created: (1) a two-subplot figure showing the train and validation loss curves for all batch sizes, (2) a single-subplot figure with the validation HWA curves, and (3) a bar chart summarizing final test HWA versus batch size. Each plot is wrapped in its own try-except block, saved to working_dir with descriptive names that include the dataset \u201cSPR_BENCH\u201d, then the figure is closed. We limit ourselves to these three figures (\u22645) to stay concise, use only data present in experiment_data, and rely solely on basic matplotlib. The code begins with the mandated imports and directory setup and finishes by printing a short \u201cDone\u201d confirmation.", "We load the saved experiment_data.npy, locate the \u201cembed_dim \u2192 SPR_BENCH\u201d subtree, and iterate over each embedding dimension.  For every dimension we extract the per-epoch training/validation losses and validation metrics (SWA, CWA, HWA) and create two separate plots: a loss curve and a metric curve.  Each plotting routine is wrapped in its own try-except block to prevent one failure from stopping the rest, and every figure is saved into working_dir with an informative, unique filename (e.g. \u201cSPR_BENCH_loss_curves_embed32.png\u201d).  After closing those per-dimension plots, we make a global summary figure that compares the final test HWA achieved by each embedding size so we can see the overall trend.  All figures include clear titles that mention the dataset and what\u2019s being shown, and axes are labeled; subtitles clarify whether curves are train vs. validation or different metric types.  We strictly visualize only the data stored in experiment_data.npy\u2014no synthetic or missing values are fabricated.  At most nine figures are produced (4 loss curves + 4 metric curves + 1 summary), well within the five-figure limit for any single repeated plot type.  Finally, every pyplot figure is closed after saving to keep memory usage low.", "We will load the saved experiment_data.npy, extract the \u201cweight_decay \u2192 SPR_BENCH\u201d branch and unpack train / val losses, per-epoch validation metrics, the list of weight-decay values, and the final test metrics for the best model. Four concise visualizations will be produced: (1) a combined line plot of training vs. validation loss across epochs for every weight-decay setting; (2) a combined line plot of harmonic-weighted accuracy (HWA) on the dev set across epochs for every decay; (3) a bar chart comparing the final dev-set HWA achieved by each decay value; and (4) a small bar chart showing the best model\u2019s SWA, CWA and HWA on the test split. Each plot is wrapped in its own try-except block, saved with descriptive file names that include \u201cSPR_BENCH\u201d and the plot type, given informative titles and subtitles, and the figure is always closed afterward. We limit ourselves to these four figures (\u22645) to respect the guideline on the number of similar plots. After plotting, the script prints the best dev HWA, the corresponding decay, and the test metrics so users can verify the numbers in the console without opening the images. Nothing is simulated; all values come directly from the saved experiment array. The code is kept short and relies only on matplotlib, numpy, and os as required. Finally, we ensure working_dir exists and store all output there.", "We first load experiment_data.npy and iterate over each dropout experiment, collecting per-epoch training/validation loss and validation metrics (SWA, CWA, HWA).  We then produce at most five simple matplotlib figures: (1) combined training/validation loss curves, (2)-(4) validation SWA, CWA, and HWA curves, and (5) a bar chart of the final-epoch HWA for all dropout rates.  Each figure is built inside its own try-except to avoid crashing and is saved into working_dir with descriptive names that include the dataset (SPR_BENCH) and plot type.  We always call plt.close() regardless of success.  All plotting is purely driven by the contents of experiment_data.npy\u2014no fabricated data.  The code begins with the required imports and ensures working_dir exists.  Finally, the script prints a short confirmation message with the list of generated files.", "We load experiment_data.npy, dig into the label_smoothing/SPR_BENCH section, and pull out the smoothing values, validation losses, and per-metric validation results.  \nA first plot shows validation loss versus the smoothing coefficient so we can visually pick the sweet-spot.  \nA second figure overlays SWA, CWA and HWA against smoothing to compare how each weighted accuracy reacts to regularisation.  \nTo inspect final model quality, we compute a simple confusion matrix from ground-truth and prediction lists and visualise it as a heat-map.  \nFinally, we summarise the best model\u2019s test SWA/CWA/HWA in a single bar chart for a compact snapshot.  \nEach figure is wrapped in its own try-except block, saved under descriptive names (including the dataset and plot kind), and closed afterwards.  \nAfter plotting we print best smoothing, dev HWA, and full test metrics so the notebook/CI log captures key numbers too.  \nThe script respects all directory and data constraints, never invents values, and always titles/labels figures clearly.", "We first read experiment_data.npy and fetch the SPR_BENCH section under the max_grad_norm sweep. From each clip\u2010value run we gather: (a) the per-epoch training/validation losses, and (b) the final test metrics (SWA, CWA, HWA).  \nFigure 1 overlays the five training-loss curves and the five validation-loss curves across epochs, helping us see how gradient clipping influences convergence.  \nFigure 2 is a grouped bar chart that compares SWA, CWA and HWA at test time for every clip value, clearly exposing which clipping norm yields the best harmonic score.  \nFigure 3 tracks the validation HWA over epochs for the two most extreme settings (clip = 0 and clip = 5) so we can inspect learning dynamics without exceeding five figures overall.  \nEach plot is wrapped in its own try-except, saved to working_dir with descriptive names (including dataset and plot type), given a title and closed afterwards.  \nFinally, the script prints the clip value that achieves the highest test HWA along with the corresponding metrics, fulfilling the \u201cevaluation metric(s)\u201d requirement.  \nNo synthetic data are created; every value comes directly from experiment_data.npy, and we cap plotted epochs to all five available so we remain within the limit.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "epochs", "learning_rate", "batch_size", "embed_dim", "weight_decay", "dropout_rate", "label_smoothing", "max_grad_norm", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate the working directory, load the serialized dictionary\nfrom experiment_data.npy, and iterate over every dataset stored inside.   For\neach dataset, it will pull out the training-loss history, validation-loss\nhistory, and the list of per-epoch validation metrics.   It will then compute\n(i) the final training loss, (ii) the final validation loss, and (iii) the best\nvalidation SWA, CWA, and HWA obtained across epochs.   Finally, it prints the\ndataset name followed by each metric\u2019s descriptive label and its value, ensuring\nthe output is clean and unambiguous.", "The script below loads the serialized dictionary from\nworking/experiment_data.npy, walks through every hyper-parameter run (e.g.,\n\u201c5_epochs\u201d), finds the best value for each stored metric (minimum for losses,\nmaximum for accuracy-based scores), and prints those values. Each block starts\nwith the dataset name so the output is unambiguous, and every line names the\nmetric explicitly. No plots are produced and the code executes immediately when\nrun.", "The script will load the saved NumPy dictionary, navigate through the nested\nstructure (learning_rate \u2794 dataset \u2794 individual learning\u2010rate runs) and pull out\nthe last recorded value for each loss series as well as the last element in the\nvalidation metric list.  It will then print the dataset name once, followed by a\nblock of clearly-labelled final metrics for every learning-rate configuration\nthat was run.  All code is kept at top level so it executes immediately when the\nfile is run; no plots are created.  Only the explicitly requested metrics\u2014final\ntraining loss, final validation loss, final validation SWA/CWA/HWA, and test\nSWA/CWA/HWA/loss\u2014are printed with unambiguous names.", "We will load the results dictionary from working/experiment_data.npy, iterate\nover every run inside the \u201cbatch_size_tuning\u201d section, and extract the final\ntraining loss, the best (minimum) validation loss, the best (maximum) validation\nharmonic-weighted accuracy, the test loss, and the test harmonic-weighted\naccuracy. For each batch-size run we will print the run name first, followed by\neach metric label and its value. All code is at the top level so it executes\nimmediately when the script is run.", "Below is a small utility that immediately loads the saved NumPy dictionary,\nwalks through every recorded experiment for the SPR _BENCH dataset, and prints\nthe final-epoch training loss, final-epoch validation loss, final-epoch\nvalidation Shape-Weighted Accuracy, Color-Weighted Accuracy, Harmonic-Weighted\nAccuracy, and the test-set counterparts (including test loss). Each metric is\nclearly labeled, and the dataset name (SPR _BENCH) is displayed before the\nmetrics of each embedding-dimension configuration.", "The solution will load the saved experiment data from the working directory,\nidentify the run that obtained the best validation harmonic-weighted accuracy\n(HWA), and then print the final training/validation losses and accuracies for\nthat run together with the corresponding test results. All metric names are\nprinted explicitly and the script executes immediately on import.", "The script will locate and load the experiment_data.npy file, iterate through\nevery hyper-parameter setting saved under the common dataset key \u2018SPR_BENCH,\u2019\nand, for each setting, identify the epoch that achieved the highest validation\nharmonic weighted accuracy (HWA). It will then print the dataset name once,\nfollowed by a block of clearly labelled best-epoch metrics (validation loss,\nSWA, CWA, HWA) for every dropout configuration. All code executes immediately at\nimport time, adhering to the specified structure.", "The script will first determine the working directory created by the training\ncode, load the stored NumPy dictionary, and then iterate over every dataset\nrecorded under the \u201clabel_smoothing\u201d section.   For each dataset it prints: the\nbest smoothing value chosen during hyper-parameter search, the final validation\nloss and accuracies (taken from the last training run stored), and the final\ntest loss together with the shape-, color-, and harmonic-weighted accuracies.\nAll metric names are explicitly spelled out so there is no ambiguity, and the\nprogram runs immediately without any special entry point.", "The script will load the saved experiment dictionary from the working directory,\niterate through the recorded runs for each dataset, and select the run whose\nfinal-epoch validation Harmonic Weighted Accuracy (HWA) is highest. For that\nbest run it will extract the last-epoch training loss, last-epoch validation\nloss and accuracies, and the stored test metrics. It then prints the dataset\nname followed by each metric name and its corresponding value, ensuring clear,\nexplicit labels. No plotting or special entry point is used; everything runs\nimmediately at the global scope.", "The script below loads the serialized dictionary from\nworking/experiment_data.npy, walks through every hyper-parameter run (e.g.,\n\u201c5_epochs\u201d), finds the best value for each stored metric (minimum for losses,\nmaximum for accuracy-based scores), and prints those values. Each block starts\nwith the dataset name so the output is unambiguous, and every line names the\nmetric explicitly. No plots are produced and the code executes immediately when\nrun.", "The script below loads the serialized dictionary from\nworking/experiment_data.npy, walks through every hyper-parameter run (e.g.,\n\u201c5_epochs\u201d), finds the best value for each stored metric (minimum for losses,\nmaximum for accuracy-based scores), and prints those values. Each block starts\nwith the dataset name so the output is unambiguous, and every line names the\nmetric explicitly. No plots are produced and the code executes immediately when\nrun.", "The script below loads the serialized dictionary from\nworking/experiment_data.npy, walks through every hyper-parameter run (e.g.,\n\u201c5_epochs\u201d), finds the best value for each stored metric (minimum for losses,\nmaximum for accuracy-based scores), and prints those values. Each block starts\nwith the dataset name so the output is unambiguous, and every line names the\nmetric explicitly. No plots are produced and the code executes immediately when\nrun.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ----------- locate and load experiment data -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------- utility to print nicely -----------\ndef print_metric(label: str, value: float):\n    print(f\"{label}: {value:.4f}\")\n\n\n# ----------- analysis and output -----------\nfor dataset_name, ds_data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- losses ---\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print_metric(\"training loss (final epoch)\", train_losses[-1])\n    if val_losses:\n        print_metric(\"validation loss (final epoch)\", val_losses[-1])\n\n    # --- validation metrics across epochs ---\n    val_metrics_epochs = ds_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Filter out any epochs that may have placeholder None values\n    val_metrics_epochs = [m for m in val_metrics_epochs if isinstance(m, dict)]\n\n    if val_metrics_epochs:\n        # Collect lists for each metric\n        swa_list = [m.get(\"SWA\", 0.0) for m in val_metrics_epochs]\n        cwa_list = [m.get(\"CWA\", 0.0) for m in val_metrics_epochs]\n        hwa_list = [m.get(\"HWA\", 0.0) for m in val_metrics_epochs]\n\n        print_metric(\"shape-weighted accuracy (best validation)\", max(swa_list))\n        print_metric(\"color-weighted accuracy (best validation)\", max(cwa_list))\n        print_metric(\"harmonic-weighted accuracy (best validation)\", max(hwa_list))\n", "import os\nimport numpy as np\n\n# ----------------- locate and load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions -----------------\ndef best(values, mode=\"min\"):\n    \"\"\"Return best element according to mode.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\n# ----------------- iterate over runs -----------------\nfor run_name, run_rec in experiment_data.get(\"epochs\", {}).items():\n    print(f\"\\n========== Results for hyper-parameter setting: {run_name} ==========\")\n\n    # -------- Training metrics --------\n    train_losses = run_rec[\"losses\"][\"train\"]\n    best_train_loss = best(train_losses, mode=\"min\")\n\n    print(\"Training Set:\")\n    print(f\"training best loss: {best_train_loss:.4f}\")\n\n    # -------- Validation metrics --------\n    val_losses = run_rec[\"losses\"][\"val\"]\n    val_metrics = run_rec[\"metrics\"][\"val\"]\n\n    best_val_loss = best(val_losses, mode=\"min\")\n    best_val_swa = best([m[\"SWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_cwa = best([m[\"CWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_hwa = best([m[\"HWA\"] for m in val_metrics if m is not None], mode=\"max\")\n\n    print(\"Validation Set:\")\n    print(f\"validation best loss: {best_val_loss:.4f}\")\n    print(f\"validation best shape-weighted accuracy: {best_val_swa:.4f}\")\n    print(f\"validation best color-weighted accuracy: {best_val_cwa:.4f}\")\n    print(f\"validation best harmonic-weighted accuracy: {best_val_hwa:.4f}\")\n\n    # -------- Test metrics --------\n    test_metrics = run_rec[\"test_metrics\"]\n\n    print(\"Test Set:\")\n    print(f\"test loss: {test_metrics['loss']:.4f}\")\n    print(f\"test shape-weighted accuracy: {test_metrics['SWA']:.4f}\")\n    print(f\"test color-weighted accuracy: {test_metrics['CWA']:.4f}\")\n    print(f\"test harmonic-weighted accuracy: {test_metrics['HWA']:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- traverse and report ----------\nlr_root = experiment_data.get(\"learning_rate\", {})\nfor dataset_name, lr_dict in lr_root.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for lr_str, run_dict in lr_dict.items():\n        losses = run_dict[\"losses\"]\n        val_metrics = run_dict[\"metrics\"][\"val\"]\n        test_metrics = run_dict[\"test_metrics\"]\n\n        final_train_loss = losses[\"train\"][-1] if losses[\"train\"] else None\n        final_val_loss = losses[\"val\"][-1] if losses[\"val\"] else None\n        final_val_swa = val_metrics[-1][\"SWA\"]\n        final_val_cwa = val_metrics[-1][\"CWA\"]\n        final_val_hwa = val_metrics[-1][\"HWA\"]\n\n        print(f\"\\n  Learning rate: {lr_str}\")\n        print(f\"    train loss: {final_train_loss:.6f}\")\n        print(f\"    validation loss: {final_val_loss:.6f}\")\n        print(f\"    validation SWA: {final_val_swa:.6f}\")\n        print(f\"    validation CWA: {final_val_cwa:.6f}\")\n        print(f\"    validation HWA: {final_val_hwa:.6f}\")\n        print(f\"    test SWA: {test_metrics['SWA']:.6f}\")\n        print(f\"    test CWA: {test_metrics['CWA']:.6f}\")\n        print(f\"    test HWA: {test_metrics['HWA']:.6f}\")\n        print(f\"    test loss: {test_metrics['loss']:.6f}\")\n", "import os\nimport numpy as np\n\n# -------- load --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------- extract & print --------\ntuning_results = experiment_data.get(\"batch_size_tuning\", {})\nfor run_name, run_data in tuning_results.items():\n    print(f\"\\nDataset: {run_name}\")\n\n    # training loss (final epoch)\n    final_train_loss = run_data[\"losses\"][\"train\"][-1]\n    print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # validation loss (best / minimum)\n    val_losses = run_data[\"losses\"][\"val\"]\n    best_val_loss = min(val_losses) if val_losses else float(\"nan\")\n    print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # validation HWA (best / maximum)\n    val_hw_as = [m[\"HWA\"] for m in run_data[\"metrics\"][\"val\"] if m]\n    best_val_hwa = max(val_hw_as) if val_hw_as else float(\"nan\")\n    print(f\"best validation harmonic weighted accuracy: {best_val_hwa:.4f}\")\n\n    # test metrics\n    test_metrics = run_data.get(\"test_metrics\", {})\n    test_loss = test_metrics.get(\"loss\", float(\"nan\"))\n    test_hwa = test_metrics.get(\"HWA\", float(\"nan\"))\n    print(f\"test loss: {test_loss:.4f}\")\n    print(f\"test harmonic weighted accuracy: {test_hwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------- locate and load -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------- parse & display -------------------\nfor dataset_name, cfg_dict in experiment_data.get(\"embed_dim\", {}).items():\n    # dataset_name here is 'SPR_BENCH'\n    print(f\"\\nDataset: {dataset_name}\")\n    for emb_dim, results in sorted(cfg_dict.items(), key=lambda x: int(x[0])):\n        print(f\"\\n  Embedding dimension = {emb_dim}\")\n        # final epoch indices\n        final_train_loss = results[\"losses\"][\"train\"][-1]\n        final_val_loss = results[\"losses\"][\"val\"][-1]\n\n        final_val_metrics = results[\"metrics\"][\"val\"][-1]  # dict with SWA, CWA, HWA\n        test_metrics = results[\"test_metrics\"]\n\n        # ----- Training -----\n        print(f\"    Training loss: {final_train_loss:.4f}\")\n\n        # ----- Validation -----\n        print(f\"    Validation loss: {final_val_loss:.4f}\")\n        print(f\"    Validation Shape-Weighted Accuracy: {final_val_metrics['SWA']:.3f}\")\n        print(f\"    Validation Color-Weighted Accuracy: {final_val_metrics['CWA']:.3f}\")\n        print(\n            f\"    Validation Harmonic-Weighted Accuracy: {final_val_metrics['HWA']:.3f}\"\n        )\n\n        # ----- Test -----\n        print(f\"    Test loss: {test_metrics['loss']:.4f}\")\n        print(f\"    Test Shape-Weighted Accuracy: {test_metrics['SWA']:.3f}\")\n        print(f\"    Test Color-Weighted Accuracy: {test_metrics['CWA']:.3f}\")\n        print(f\"    Test Harmonic-Weighted Accuracy: {test_metrics['HWA']:.3f}\")\n", "import os\nimport numpy as np\n\n# ------------ locate and load experiment data -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------ helper to get best-run index -----------------\ndef get_best_run(exp_dict):\n    \"\"\"Return index of the run with the highest recorded dev HWA.\"\"\"\n    hwas = exp_dict[\"best_val_hwa\"]  # list of floats, one per run\n    return int(np.argmax(hwas))  # index of best run\n\n\ndef print_metrics_for_dataset(name, exp_dict):\n    \"\"\"Print the final / best metrics for a single dataset.\"\"\"\n    best_idx = get_best_run(exp_dict)\n\n    # Training & validation losses are lists (one list per run \u2192 epochs).\n    final_train_loss = exp_dict[\"losses\"][\"train\"][best_idx][-1]\n    final_val_loss = exp_dict[\"losses\"][\"val\"][best_idx][-1]\n\n    # Validation accuracies (list per run \u2192 epochs, each epoch a dict)\n    final_val_metrics = exp_dict[\"metrics\"][\"val\"][best_idx][-1]\n    val_swa = final_val_metrics[\"SWA\"]\n    val_cwa = final_val_metrics[\"CWA\"]\n    val_hwa = final_val_metrics[\"HWA\"]\n\n    # Test metrics (stored once for the best run)\n    test_metrics = exp_dict[\"test_metrics\"][-1]  # only one dict stored\n    test_loss = test_metrics[\"loss\"]\n    test_swa = test_metrics[\"SWA\"]\n    test_cwa = test_metrics[\"CWA\"]\n    test_hwa = test_metrics[\"HWA\"]\n\n    # --------------- print section -------------------------\n    print(f\"\\nDataset: {name}\")\n    print(f\"Training loss (final epoch, best run): {final_train_loss:.4f}\")\n    print(f\"Validation loss (final epoch, best run): {final_val_loss:.4f}\")\n    print(f\"Validation shape-weighted accuracy (final epoch): {val_swa:.4f}\")\n    print(f\"Validation color-weighted accuracy (final epoch): {val_cwa:.4f}\")\n    print(f\"Validation harmonic-weighted accuracy (final epoch): {val_hwa:.4f}\")\n    print(f\"Test loss (best model): {test_loss:.4f}\")\n    print(f\"Test shape-weighted accuracy: {test_swa:.4f}\")\n    print(f\"Test color-weighted accuracy: {test_cwa:.4f}\")\n    print(f\"Test harmonic-weighted accuracy: {test_hwa:.4f}\")\n\n\n# ------------ iterate through datasets --------------------\nfor top_key in experiment_data:  # e.g., \"weight_decay\"\n    for dataset_name, exp_dict in experiment_data[top_key].items():\n        print_metrics_for_dataset(dataset_name, exp_dict)\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- print ----------\ndataset_name = \"SPR_BENCH\"\nprint(f\"Dataset: {dataset_name}\")\n\nfor cfg_name, cfg_data in experiment_data.items():\n    run = cfg_data[dataset_name]  # shortcut\n    val_metrics = run[\"metrics\"][\"val\"]  # list of dicts per epoch\n    val_losses = run[\"losses\"][\"val\"]  # list of floats per epoch\n\n    # find epoch with best validation HWA\n    best_idx, best_hwa = max(enumerate(val_metrics), key=lambda x: x[1][\"HWA\"])\n\n    best_metrics = val_metrics[best_idx]\n    best_val_loss = val_losses[best_idx]\n\n    # prefix block identifying the hyper-parameter setting\n    print(f\"\\nConfiguration: {cfg_name}\")\n    print(f\"best validation loss: {best_val_loss:.4f}\")\n    print(f\"best validation shape weighted accuracy (SWA): {best_metrics['SWA']:.4f}\")\n    print(f\"best validation color weighted accuracy (CWA): {best_metrics['CWA']:.4f}\")\n    print(\n        f\"best validation harmonic weighted accuracy (HWA): {best_metrics['HWA']:.4f}\"\n    )\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Could not find experiment data at {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Parse and print metrics\n# ------------------------------------------------------------------\nsection = experiment_data.get(\"label_smoothing\", {})\nif not section:\n    raise KeyError(\"'label_smoothing' section not found in experiment data\")\n\nfor dataset_name, data in section.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Best smoothing hyper-parameter\n    best_smoothing = data.get(\"best_smoothing\", None)\n    if best_smoothing is not None:\n        print(f\"Best label smoothing value: {best_smoothing:.2f}\")\n\n    # Final validation metrics (last element in each list)\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n\n    if val_losses:\n        final_val_loss = val_losses[-1]\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n    if val_metrics:\n        final_val_swa = val_metrics[-1].get(\"SWA\", float(\"nan\"))\n        final_val_cwa = val_metrics[-1].get(\"CWA\", float(\"nan\"))\n        final_val_hwa = val_metrics[-1].get(\"HWA\", float(\"nan\"))\n        print(f\"Final validation shape-weighted accuracy: {final_val_swa:.4f}\")\n        print(f\"Final validation color-weighted accuracy: {final_val_cwa:.4f}\")\n        print(f\"Final validation harmonic-weighted accuracy: {final_val_hwa:.4f}\")\n\n    # Test metrics\n    test_loss = data.get(\"test_loss\", None)\n    if test_loss is not None:\n        print(f\"Test loss: {test_loss:.4f}\")\n\n    test_metrics = data.get(\"test_metrics\", {})\n    if test_metrics:\n        print(\n            f\"Test shape-weighted accuracy: {test_metrics.get('SWA', float('nan')):.4f}\"\n        )\n        print(\n            f\"Test color-weighted accuracy: {test_metrics.get('CWA', float('nan')):.4f}\"\n        )\n        print(\n            f\"Test harmonic-weighted accuracy: {test_metrics.get('HWA', float('nan')):.4f}\"\n        )\n\n    # Blank line for readability between datasets\n    print()\n", "import os\nimport numpy as np\n\n# ---------- load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- iterate and report ----------\nfor exp_name, datasets in experiment_data.items():  # exp_name -> 'max_grad_norm'\n    for dataset_name, runs in datasets.items():  # dataset_name -> 'SPR_BENCH'\n        # pick the run (clip value) with the best final-epoch validation HWA\n        best_clip, best_run, best_hwa = None, None, -1.0\n        for clip_val, rec in runs.items():  # clip_val is a string like '0.5'\n            val_metrics_last = rec[\"metrics\"][\"val\"][-1]  # dict with SWA, CWA, HWA\n            hwa = val_metrics_last[\"HWA\"]\n            if hwa > best_hwa:\n                best_hwa = hwa\n                best_clip = clip_val\n                best_run = rec\n\n        # safety check (should always pass)\n        if best_run is None:\n            continue\n\n        # extract metrics\n        train_loss_final = best_run[\"losses\"][\"train\"][-1]\n        val_loss_final = best_run[\"losses\"][\"val\"][-1]\n        val_metrics = best_run[\"metrics\"][\"val\"][-1]  # SWA, CWA, HWA\n        test_metrics = best_run[\"test_metrics\"]  # dict with loss, SWA, CWA, HWA\n\n        # ---------- print ----------\n        print(dataset_name)\n        print(f\"selected clip value: {best_clip}\")\n        print(f\"train loss: {train_loss_final:.6f}\")\n        print(f\"validation loss: {val_loss_final:.6f}\")\n        print(f\"validation shape weighted accuracy: {val_metrics['SWA']:.6f}\")\n        print(f\"validation color weighted accuracy: {val_metrics['CWA']:.6f}\")\n        print(f\"validation harmonic weighted accuracy: {val_metrics['HWA']:.6f}\")\n        print(f\"test loss: {test_metrics['loss']:.6f}\")\n        print(f\"test shape weighted accuracy: {test_metrics['SWA']:.6f}\")\n        print(f\"test color weighted accuracy: {test_metrics['CWA']:.6f}\")\n        print(f\"test harmonic weighted accuracy: {test_metrics['HWA']:.6f}\")\n        print(\"-\" * 40)\n", "import os\nimport numpy as np\n\n# ----------------- locate and load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions -----------------\ndef best(values, mode=\"min\"):\n    \"\"\"Return best element according to mode.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\n# ----------------- iterate over runs -----------------\nfor run_name, run_rec in experiment_data.get(\"epochs\", {}).items():\n    print(f\"\\n========== Results for hyper-parameter setting: {run_name} ==========\")\n\n    # -------- Training metrics --------\n    train_losses = run_rec[\"losses\"][\"train\"]\n    best_train_loss = best(train_losses, mode=\"min\")\n\n    print(\"Training Set:\")\n    print(f\"training best loss: {best_train_loss:.4f}\")\n\n    # -------- Validation metrics --------\n    val_losses = run_rec[\"losses\"][\"val\"]\n    val_metrics = run_rec[\"metrics\"][\"val\"]\n\n    best_val_loss = best(val_losses, mode=\"min\")\n    best_val_swa = best([m[\"SWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_cwa = best([m[\"CWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_hwa = best([m[\"HWA\"] for m in val_metrics if m is not None], mode=\"max\")\n\n    print(\"Validation Set:\")\n    print(f\"validation best loss: {best_val_loss:.4f}\")\n    print(f\"validation best shape-weighted accuracy: {best_val_swa:.4f}\")\n    print(f\"validation best color-weighted accuracy: {best_val_cwa:.4f}\")\n    print(f\"validation best harmonic-weighted accuracy: {best_val_hwa:.4f}\")\n\n    # -------- Test metrics --------\n    test_metrics = run_rec[\"test_metrics\"]\n\n    print(\"Test Set:\")\n    print(f\"test loss: {test_metrics['loss']:.4f}\")\n    print(f\"test shape-weighted accuracy: {test_metrics['SWA']:.4f}\")\n    print(f\"test color-weighted accuracy: {test_metrics['CWA']:.4f}\")\n    print(f\"test harmonic-weighted accuracy: {test_metrics['HWA']:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------- locate and load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions -----------------\ndef best(values, mode=\"min\"):\n    \"\"\"Return best element according to mode.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\n# ----------------- iterate over runs -----------------\nfor run_name, run_rec in experiment_data.get(\"epochs\", {}).items():\n    print(f\"\\n========== Results for hyper-parameter setting: {run_name} ==========\")\n\n    # -------- Training metrics --------\n    train_losses = run_rec[\"losses\"][\"train\"]\n    best_train_loss = best(train_losses, mode=\"min\")\n\n    print(\"Training Set:\")\n    print(f\"training best loss: {best_train_loss:.4f}\")\n\n    # -------- Validation metrics --------\n    val_losses = run_rec[\"losses\"][\"val\"]\n    val_metrics = run_rec[\"metrics\"][\"val\"]\n\n    best_val_loss = best(val_losses, mode=\"min\")\n    best_val_swa = best([m[\"SWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_cwa = best([m[\"CWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_hwa = best([m[\"HWA\"] for m in val_metrics if m is not None], mode=\"max\")\n\n    print(\"Validation Set:\")\n    print(f\"validation best loss: {best_val_loss:.4f}\")\n    print(f\"validation best shape-weighted accuracy: {best_val_swa:.4f}\")\n    print(f\"validation best color-weighted accuracy: {best_val_cwa:.4f}\")\n    print(f\"validation best harmonic-weighted accuracy: {best_val_hwa:.4f}\")\n\n    # -------- Test metrics --------\n    test_metrics = run_rec[\"test_metrics\"]\n\n    print(\"Test Set:\")\n    print(f\"test loss: {test_metrics['loss']:.4f}\")\n    print(f\"test shape-weighted accuracy: {test_metrics['SWA']:.4f}\")\n    print(f\"test color-weighted accuracy: {test_metrics['CWA']:.4f}\")\n    print(f\"test harmonic-weighted accuracy: {test_metrics['HWA']:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------- locate and load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper functions -----------------\ndef best(values, mode=\"min\"):\n    \"\"\"Return best element according to mode.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\n# ----------------- iterate over runs -----------------\nfor run_name, run_rec in experiment_data.get(\"epochs\", {}).items():\n    print(f\"\\n========== Results for hyper-parameter setting: {run_name} ==========\")\n\n    # -------- Training metrics --------\n    train_losses = run_rec[\"losses\"][\"train\"]\n    best_train_loss = best(train_losses, mode=\"min\")\n\n    print(\"Training Set:\")\n    print(f\"training best loss: {best_train_loss:.4f}\")\n\n    # -------- Validation metrics --------\n    val_losses = run_rec[\"losses\"][\"val\"]\n    val_metrics = run_rec[\"metrics\"][\"val\"]\n\n    best_val_loss = best(val_losses, mode=\"min\")\n    best_val_swa = best([m[\"SWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_cwa = best([m[\"CWA\"] for m in val_metrics if m is not None], mode=\"max\")\n    best_val_hwa = best([m[\"HWA\"] for m in val_metrics if m is not None], mode=\"max\")\n\n    print(\"Validation Set:\")\n    print(f\"validation best loss: {best_val_loss:.4f}\")\n    print(f\"validation best shape-weighted accuracy: {best_val_swa:.4f}\")\n    print(f\"validation best color-weighted accuracy: {best_val_cwa:.4f}\")\n    print(f\"validation best harmonic-weighted accuracy: {best_val_hwa:.4f}\")\n\n    # -------- Test metrics --------\n    test_metrics = run_rec[\"test_metrics\"]\n\n    print(\"Test Set:\")\n    print(f\"test loss: {test_metrics['loss']:.4f}\")\n    print(f\"test shape-weighted accuracy: {test_metrics['SWA']:.4f}\")\n    print(f\"test color-weighted accuracy: {test_metrics['CWA']:.4f}\")\n    print(f\"test harmonic-weighted accuracy: {test_metrics['HWA']:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'training loss (final epoch): 0.5199', '\\n', 'validation\nloss (final epoch): 0.5210', '\\n', 'shape-weighted accuracy (best validation):\n0.7481', '\\n', 'color-weighted accuracy (best validation): 0.7455', '\\n',\n'harmonic-weighted accuracy (best validation): 0.7468', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\n========== Results for hyper-parameter setting: 5_epochs ==========', '\\n',\n'Training Set:', '\\n', 'training best loss: 0.5198', '\\n', 'Validation Set:',\n'\\n', 'validation best loss: 0.5210', '\\n', 'validation best shape-weighted\naccuracy: 0.7481', '\\n', 'validation best color-weighted accuracy: 0.7455',\n'\\n', 'validation best harmonic-weighted accuracy: 0.7468', '\\n', 'Test Set:',\n'\\n', 'test loss: 0.7201', '\\n', 'test shape-weighted accuracy: 0.5950', '\\n',\n'test color-weighted accuracy: 0.6205', '\\n', 'test harmonic-weighted accuracy:\n0.6075', '\\n', '\\n========== Results for hyper-parameter setting: 15_epochs\n==========', '\\n', 'Training Set:', '\\n', 'training best loss: 0.5197', '\\n',\n'Validation Set:', '\\n', 'validation best loss: 0.5209', '\\n', 'validation best\nshape-weighted accuracy: 0.7660', '\\n', 'validation best color-weighted\naccuracy: 0.7613', '\\n', 'validation best harmonic-weighted accuracy: 0.7636',\n'\\n', 'Test Set:', '\\n', 'test loss: 0.7247', '\\n', 'test shape-weighted\naccuracy: 0.5866', '\\n', 'test color-weighted accuracy: 0.6122', '\\n', 'test\nharmonic-weighted accuracy: 0.5991', '\\n', '\\n========== Results for hyper-\nparameter setting: 25_epochs ==========', '\\n', 'Training Set:', '\\n', 'training\nbest loss: 0.5196', '\\n', 'Validation Set:', '\\n', 'validation best loss:\n0.5206', '\\n', 'validation best shape-weighted accuracy: 0.7670', '\\n',\n'validation best color-weighted accuracy: 0.7618', '\\n', 'validation best\nharmonic-weighted accuracy: 0.7644', '\\n', 'Test Set:', '\\n', 'test loss:\n0.7316', '\\n', 'test shape-weighted accuracy: 0.5898', '\\n', 'test color-\nweighted accuracy: 0.6159', '\\n', 'test harmonic-weighted accuracy: 0.6026',\n'\\n', '\\n========== Results for hyper-parameter setting: 40_epochs ==========',\n'\\n', 'Training Set:', '\\n', 'training best loss: 0.5196', '\\n', 'Validation\nSet:', '\\n', 'validation best loss: 0.5206', '\\n', 'validation best shape-\nweighted accuracy: 0.7651', '\\n', 'validation best color-weighted accuracy:\n0.7614', '\\n', 'validation best harmonic-weighted accuracy: 0.7632', '\\n', 'Test\nSet:', '\\n', 'test loss: 0.7208', '\\n', 'test shape-weighted accuracy: 0.5958',\n'\\n', 'test color-weighted accuracy: 0.6226', '\\n', 'test harmonic-weighted\naccuracy: 0.6089', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '\\n  Learning rate: 0.0005', '\\n', '    train\nloss: 0.520706', '\\n', '    validation loss: 0.521506', '\\n', '    validation\nSWA: 0.731485', '\\n', '    validation CWA: 0.728021', '\\n', '    validation HWA:\n0.729749', '\\n', '    test SWA: 0.594812', '\\n', '    test CWA: 0.621214', '\\n',\n'    test HWA: 0.607726', '\\n', '    test loss: 0.714542', '\\n', '\\n  Learning\nrate: 0.001', '\\n', '    train loss: 0.520042', '\\n', '    validation loss:\n0.521221', '\\n', '    validation SWA: 0.747994', '\\n', '    validation CWA:\n0.741871', '\\n', '    validation HWA: 0.744920', '\\n', '    test SWA: 0.591393',\n'\\n', '    test CWA: 0.617142', '\\n', '    test HWA: 0.603993', '\\n', '    test\nloss: 0.721539', '\\n', '\\n  Learning rate: 0.002', '\\n', '    train loss:\n0.521086', '\\n', '    validation loss: 0.521241', '\\n', '    validation SWA:\n0.758691', '\\n', '    validation CWA: 0.755659', '\\n', '    validation HWA:\n0.757172', '\\n', '    test SWA: 0.602666', '\\n', '    test CWA: 0.630268', '\\n',\n'    test HWA: 0.616158', '\\n', '    test loss: 0.723998', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: bs_32', '\\n', 'final training loss: 0.5210', '\\n', 'best validation\nloss: 0.5213', '\\n', 'best validation harmonic weighted accuracy: 0.7558', '\\n',\n'test loss: 0.7173', '\\n', 'test harmonic weighted accuracy: 0.6085', '\\n',\n'\\nDataset: bs_64', '\\n', 'final training loss: 0.5204', '\\n', 'best validation\nloss: 0.5213', '\\n', 'best validation harmonic weighted accuracy: 0.7588', '\\n',\n'test loss: 0.7226', '\\n', 'test harmonic weighted accuracy: 0.6043', '\\n',\n'\\nDataset: bs_128', '\\n', 'final training loss: 0.5198', '\\n', 'best validation\nloss: 0.5210', '\\n', 'best validation harmonic weighted accuracy: 0.7603', '\\n',\n'test loss: 0.7317', '\\n', 'test harmonic weighted accuracy: 0.6018', '\\n',\n'\\nDataset: bs_256', '\\n', 'final training loss: 0.5197', '\\n', 'best validation\nloss: 0.5211', '\\n', 'best validation harmonic weighted accuracy: 0.7422', '\\n',\n'test loss: 0.7173', '\\n', 'test harmonic weighted accuracy: 0.6094', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '\\n  Embedding dimension = 32', '\\n', '\nTraining loss: 0.5204', '\\n', '    Validation loss: 0.5209', '\\n', '\nValidation Shape-Weighted Accuracy: 0.744', '\\n', '    Validation Color-Weighted\nAccuracy: 0.738', '\\n', '    Validation Harmonic-Weighted Accuracy: 0.741',\n'\\n', '    Test loss: 0.7195', '\\n', '    Test Shape-Weighted Accuracy: 0.593',\n'\\n', '    Test Color-Weighted Accuracy: 0.620', '\\n', '    Test Harmonic-\nWeighted Accuracy: 0.606', '\\n', '\\n  Embedding dimension = 64', '\\n', '\nTraining loss: 0.5199', '\\n', '    Validation loss: 0.5217', '\\n', '\nValidation Shape-Weighted Accuracy: 0.761', '\\n', '    Validation Color-Weighted\nAccuracy: 0.755', '\\n', '    Validation Harmonic-Weighted Accuracy: 0.758',\n'\\n', '    Test loss: 0.7303', '\\n', '    Test Shape-Weighted Accuracy: 0.587',\n'\\n', '    Test Color-Weighted Accuracy: 0.612', '\\n', '    Test Harmonic-\nWeighted Accuracy: 0.599', '\\n', '\\n  Embedding dimension = 128', '\\n', '\nTraining loss: 0.5203', '\\n', '    Validation loss: 0.5217', '\\n', '\nValidation Shape-Weighted Accuracy: 0.765', '\\n', '    Validation Color-Weighted\nAccuracy: 0.757', '\\n', '    Validation Harmonic-Weighted Accuracy: 0.761',\n'\\n', '    Test loss: 0.7341', '\\n', '    Test Shape-Weighted Accuracy: 0.579',\n'\\n', '    Test Color-Weighted Accuracy: 0.605', '\\n', '    Test Harmonic-\nWeighted Accuracy: 0.592', '\\n', '\\n  Embedding dimension = 256', '\\n', '\nTraining loss: 0.5214', '\\n', '    Validation loss: 0.5221', '\\n', '\nValidation Shape-Weighted Accuracy: 0.744', '\\n', '    Validation Color-Weighted\nAccuracy: 0.738', '\\n', '    Validation Harmonic-Weighted Accuracy: 0.741',\n'\\n', '    Test loss: 0.7206', '\\n', '    Test Shape-Weighted Accuracy: 0.593',\n'\\n', '    Test Color-Weighted Accuracy: 0.619', '\\n', '    Test Harmonic-\nWeighted Accuracy: 0.606', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Training loss (final epoch, best run): 0.5199',\n'\\n', 'Validation loss (final epoch, best run): 0.5208', '\\n', 'Validation\nshape-weighted accuracy (final epoch): 0.7466', '\\n', 'Validation color-weighted\naccuracy (final epoch): 0.7406', '\\n', 'Validation harmonic-weighted accuracy\n(final epoch): 0.7436', '\\n', 'Test loss (best model): 0.7255', '\\n', 'Test\nshape-weighted accuracy: 0.5945', '\\n', 'Test color-weighted accuracy: 0.6198',\n'\\n', 'Test harmonic-weighted accuracy: 0.6069', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '\\nConfiguration: dropout_0.0', '\\n', 'best\nvalidation loss: 0.5212', '\\n', 'best validation shape weighted accuracy (SWA):\n0.7665', '\\n', 'best validation color weighted accuracy (CWA): 0.7607', '\\n',\n'best validation harmonic weighted accuracy (HWA): 0.7636', '\\n',\n'\\nConfiguration: dropout_0.1', '\\n', 'best validation loss: 0.5214', '\\n',\n'best validation shape weighted accuracy (SWA): 0.7578', '\\n', 'best validation\ncolor weighted accuracy (CWA): 0.7529', '\\n', 'best validation harmonic weighted\naccuracy (HWA): 0.7554', '\\n', '\\nConfiguration: dropout_0.3', '\\n', 'best\nvalidation loss: 0.5210', '\\n', 'best validation shape weighted accuracy (SWA):\n0.7496', '\\n', 'best validation color weighted accuracy (CWA): 0.7446', '\\n',\n'best validation harmonic weighted accuracy (HWA): 0.7471', '\\n',\n'\\nConfiguration: dropout_0.5', '\\n', 'best validation loss: 0.5230', '\\n',\n'best validation shape weighted accuracy (SWA): 0.7460', '\\n', 'best validation\ncolor weighted accuracy (CWA): 0.7414', '\\n', 'best validation harmonic weighted\naccuracy (HWA): 0.7437', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best label smoothing value: 0.20', '\\n', 'Final\nvalidation loss: 0.5909', '\\n', 'Final validation shape-weighted accuracy:\n0.7514', '\\n', 'Final validation color-weighted accuracy: 0.7461', '\\n', 'Final\nvalidation harmonic-weighted accuracy: 0.7487', '\\n', 'Test loss: 0.7040', '\\n',\n'Test shape-weighted accuracy: 0.5902', '\\n', 'Test color-weighted accuracy:\n0.6159', '\\n', 'Test harmonic-weighted accuracy: 0.6028', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'selected clip value: 0.5', '\\n', 'train loss: 0.519705',\n'\\n', 'validation loss: 0.523079', '\\n', 'validation shape weighted accuracy:\n0.740902', '\\n', 'validation color weighted accuracy: 0.737661', '\\n',\n'validation harmonic weighted accuracy: 0.739278', '\\n', 'test loss: 0.727190',\n'\\n', 'test shape weighted accuracy: 0.592871', '\\n', 'test color weighted\naccuracy: 0.619239', '\\n', 'test harmonic weighted accuracy: 0.605768', '\\n',\n'----------------------------------------', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\n========== Results for hyper-parameter setting: 5_epochs ==========', '\\n',\n'Training Set:', '\\n', 'training best loss: 0.5199', '\\n', 'Validation Set:',\n'\\n', 'validation best loss: 0.5209', '\\n', 'validation best shape-weighted\naccuracy: 0.7500', '\\n', 'validation best color-weighted accuracy: 0.7441',\n'\\n', 'validation best harmonic-weighted accuracy: 0.7471', '\\n', 'Test Set:',\n'\\n', 'test loss: 0.7254', '\\n', 'test shape-weighted accuracy: 0.5928', '\\n',\n'test color-weighted accuracy: 0.6193', '\\n', 'test harmonic-weighted accuracy:\n0.6058', '\\n', '\\n========== Results for hyper-parameter setting: 15_epochs\n==========', '\\n', 'Training Set:', '\\n', 'training best loss: 0.5197', '\\n',\n'Validation Set:', '\\n', 'validation best loss: 0.5211', '\\n', 'validation best\nshape-weighted accuracy: 0.7756', '\\n', 'validation best color-weighted\naccuracy: 0.7723', '\\n', 'validation best harmonic-weighted accuracy: 0.7739',\n'\\n', 'Test Set:', '\\n', 'test loss: 0.7334', '\\n', 'test shape-weighted\naccuracy: 0.5861', '\\n', 'test color-weighted accuracy: 0.6116', '\\n', 'test\nharmonic-weighted accuracy: 0.5986', '\\n', '\\n========== Results for hyper-\nparameter setting: 25_epochs ==========', '\\n', 'Training Set:', '\\n', 'training\nbest loss: 0.5195', '\\n', 'Validation Set:', '\\n', 'validation best loss:\n0.5206', '\\n', 'validation best shape-weighted accuracy: 0.7732', '\\n',\n'validation best color-weighted accuracy: 0.7694', '\\n', 'validation best\nharmonic-weighted accuracy: 0.7713', '\\n', 'Test Set:', '\\n', 'test loss:\n0.7261', '\\n', 'test shape-weighted accuracy: 0.5904', '\\n', 'test color-\nweighted accuracy: 0.6162', '\\n', 'test harmonic-weighted accuracy: 0.6030',\n'\\n', '\\n========== Results for hyper-parameter setting: 40_epochs ==========',\n'\\n', 'Training Set:', '\\n', 'training best loss: 0.5196', '\\n', 'Validation\nSet:', '\\n', 'validation best loss: 0.5207', '\\n', 'validation best shape-\nweighted accuracy: 0.7667', '\\n', 'validation best color-weighted accuracy:\n0.7619', '\\n', 'validation best harmonic-weighted accuracy: 0.7643', '\\n', 'Test\nSet:', '\\n', 'test loss: 0.7227', '\\n', 'test shape-weighted accuracy: 0.5939',\n'\\n', 'test color-weighted accuracy: 0.6197', '\\n', 'test harmonic-weighted\naccuracy: 0.6065', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\n========== Results for hyper-parameter setting: 5_epochs ==========', '\\n',\n'Training Set:', '\\n', 'training best loss: 0.5198', '\\n', 'Validation Set:',\n'\\n', 'validation best loss: 0.5210', '\\n', 'validation best shape-weighted\naccuracy: 0.7537', '\\n', 'validation best color-weighted accuracy: 0.7503',\n'\\n', 'validation best harmonic-weighted accuracy: 0.7520', '\\n', 'Test Set:',\n'\\n', 'test loss: 0.7211', '\\n', 'test shape-weighted accuracy: 0.5913', '\\n',\n'test color-weighted accuracy: 0.6181', '\\n', 'test harmonic-weighted accuracy:\n0.6044', '\\n', '\\n========== Results for hyper-parameter setting: 15_epochs\n==========', '\\n', 'Training Set:', '\\n', 'training best loss: 0.5198', '\\n',\n'Validation Set:', '\\n', 'validation best loss: 0.5210', '\\n', 'validation best\nshape-weighted accuracy: 0.7613', '\\n', 'validation best color-weighted\naccuracy: 0.7565', '\\n', 'validation best harmonic-weighted accuracy: 0.7589',\n'\\n', 'Test Set:', '\\n', 'test loss: 0.7336', '\\n', 'test shape-weighted\naccuracy: 0.5870', '\\n', 'test color-weighted accuracy: 0.6131', '\\n', 'test\nharmonic-weighted accuracy: 0.5997', '\\n', '\\n========== Results for hyper-\nparameter setting: 25_epochs ==========', '\\n', 'Training Set:', '\\n', 'training\nbest loss: 0.5197', '\\n', 'Validation Set:', '\\n', 'validation best loss:\n0.5206', '\\n', 'validation best shape-weighted accuracy: 0.7624', '\\n',\n'validation best color-weighted accuracy: 0.7579', '\\n', 'validation best\nharmonic-weighted accuracy: 0.7601', '\\n', 'Test Set:', '\\n', 'test loss:\n0.7270', '\\n', 'test shape-weighted accuracy: 0.5874', '\\n', 'test color-\nweighted accuracy: 0.6132', '\\n', 'test harmonic-weighted accuracy: 0.6000',\n'\\n', '\\n========== Results for hyper-parameter setting: 40_epochs ==========',\n'\\n', 'Training Set:', '\\n', 'training best loss: 0.5196', '\\n', 'Validation\nSet:', '\\n', 'validation best loss: 0.5208', '\\n', 'validation best shape-\nweighted accuracy: 0.7715', '\\n', 'validation best color-weighted accuracy:\n0.7679', '\\n', 'validation best harmonic-weighted accuracy: 0.7697', '\\n', 'Test\nSet:', '\\n', 'test loss: 0.7324', '\\n', 'test shape-weighted accuracy: 0.5845',\n'\\n', 'test color-weighted accuracy: 0.6095', '\\n', 'test harmonic-weighted\naccuracy: 0.5967', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\n========== Results for hyper-parameter setting: 5_epochs ==========', '\\n',\n'Training Set:', '\\n', 'training best loss: 0.5197', '\\n', 'Validation Set:',\n'\\n', 'validation best loss: 0.5207', '\\n', 'validation best shape-weighted\naccuracy: 0.7474', '\\n', 'validation best color-weighted accuracy: 0.7420',\n'\\n', 'validation best harmonic-weighted accuracy: 0.7447', '\\n', 'Test Set:',\n'\\n', 'test loss: 0.7185', '\\n', 'test shape-weighted accuracy: 0.5911', '\\n',\n'test color-weighted accuracy: 0.6177', '\\n', 'test harmonic-weighted accuracy:\n0.6041', '\\n', '\\n========== Results for hyper-parameter setting: 15_epochs\n==========', '\\n', 'Training Set:', '\\n', 'training best loss: 0.5197', '\\n',\n'Validation Set:', '\\n', 'validation best loss: 0.5209', '\\n', 'validation best\nshape-weighted accuracy: 0.7640', '\\n', 'validation best color-weighted\naccuracy: 0.7609', '\\n', 'validation best harmonic-weighted accuracy: 0.7625',\n'\\n', 'Test Set:', '\\n', 'test loss: 0.7240', '\\n', 'test shape-weighted\naccuracy: 0.5899', '\\n', 'test color-weighted accuracy: 0.6157', '\\n', 'test\nharmonic-weighted accuracy: 0.6025', '\\n', '\\n========== Results for hyper-\nparameter setting: 25_epochs ==========', '\\n', 'Training Set:', '\\n', 'training\nbest loss: 0.5197', '\\n', 'Validation Set:', '\\n', 'validation best loss:\n0.5208', '\\n', 'validation best shape-weighted accuracy: 0.7658', '\\n',\n'validation best color-weighted accuracy: 0.7624', '\\n', 'validation best\nharmonic-weighted accuracy: 0.7641', '\\n', 'Test Set:', '\\n', 'test loss:\n0.7295', '\\n', 'test shape-weighted accuracy: 0.5908', '\\n', 'test color-\nweighted accuracy: 0.6160', '\\n', 'test harmonic-weighted accuracy: 0.6032',\n'\\n', '\\n========== Results for hyper-parameter setting: 40_epochs ==========',\n'\\n', 'Training Set:', '\\n', 'training best loss: 0.5196', '\\n', 'Validation\nSet:', '\\n', 'validation best loss: 0.5207', '\\n', 'validation best shape-\nweighted accuracy: 0.7704', '\\n', 'validation best color-weighted accuracy:\n0.7648', '\\n', 'validation best harmonic-weighted accuracy: 0.7676', '\\n', 'Test\nSet:', '\\n', 'test loss: 0.7252', '\\n', 'test shape-weighted accuracy: 0.5923',\n'\\n', 'test color-weighted accuracy: 0.6188', '\\n', 'test harmonic-weighted\naccuracy: 0.6053', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}