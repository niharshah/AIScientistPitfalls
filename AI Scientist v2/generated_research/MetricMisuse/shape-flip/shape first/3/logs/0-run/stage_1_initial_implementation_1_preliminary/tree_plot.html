<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[1, 7], [1, 6], [1, 5], [1, 4]], "layout": [[0.14285714285714285, 0.0], [0.42857142857142855, 0.0], [0.7142857142857143, 0.0], [1.0, 0.0], [0.0, 1.0], [0.2857142857142857, 1.0], [0.5714285714285714, 1.0], [0.8571428571428571, 1.0]], "plan": ["The first baseline treats each SPR sequence as a bag of discrete tokens. We\nbuild a vocabulary from the training set, encode every token to an id, and pad\nbatches. A small neural-symbolic model embeds each token, averages non-pad\nembeddings, concatenates two symbolic statistics (shape-variety and color-\nvariety) and predicts the class with a linear layer. Training uses cross-\nentropy, Adam and 5 epochs; we monitor loss plus SWA, CWA and their harmonic\nmean (HWA) on dev at every epoch. All training curves, predictions and metrics\nare stored in `experiment_data` and saved to `./working/experiment_data.npy`.\nThe implementation follows the required GPU/CPU handling, saving rules and runs\nimmediately when executed.", "Our first baseline treats SPR as a plain sequence-classification task: every\nsymbol (\u201cshape+color\u201d token) gets an embedding and sequences are represented by\nthe mean of their token embeddings (implemented with torch.nn.EmbeddingBag).  We\nbuild a vocabulary from the training split, map labels to integers, and train a\nsmall bag-of-embeddings MLP on the provided train set.  No rule engine is used\nyet\u2014this gives us a neural reference point that we can later hybridise with\nsymbolic components for zero-shot generalisation.  During training we track\ncross-entropy loss and, on the dev split after each epoch, compute Shape-\nWeighted Accuracy (SWA), Color-Weighted Accuracy (CWA) and their harmonic mean\n(HWA) as requested.  We store all metrics, losses, predictions and ground-truths\nin the prescribed experiment_data structure and persist it to ./working.  The\nmodel, inputs and loss tensors are always moved to GPU if available, satisfying\nthe device guidelines.  Finally, we evaluate once on the held-out test split and\nprint the three accuracies plus HWA so we have an end-to-end working pipeline\nthat we can iterate on later.", "A simple but informative baseline is to combine symbolic counts with a\nlightweight neural bag-of-tokens encoder.   Tokens (two-character \u201cshape-color\u201d\ncodes) are embedded, mean-pooled and concatenated with two symbolic features:\nnumber of distinct shapes and colors in the sequence.   The resulting\n34-dimensional vector is passed through a small MLP and trained with cross-\nentropy on the provided train split; no hand-crafted rules are required, so the\nmodel can be evaluated zero-shot on dev and test.   We measure Shape-Weighted\nAccuracy (SWA) and Color-Weighted Accuracy (CWA) with the utilities from SPR.py,\nand report their harmonic mean (HWA) at each epoch.   All tensors, the model and\noptimiser are moved to GPU when available, with losses and metrics stored in a\nstructured experiment_data dictionary and saved as NumPy arrays.   A loss/metric\ncurve is plotted and saved to the ./working directory.   This gives a\nreproducible end-to-end pipeline from data loading to evaluation that we can\nlater enrich with more expressive neural-symbolic reasoning.", "We start with a lightweight neural-symbolic baseline: every sequence is\ntokenised at the (shape-color) symbol level, converted to IDs, embedded and\nmean-pooled.  Two symbolic features\u2014the number of distinct shapes and distinct\ncolours\u2014are concatenated to the pooled embedding, and a tiny MLP predicts the\nrule label.  This gives the network a direct \u201chint\u201d about symbolic structure\nwhile keeping the model simple and fast.  We build a vocabulary from the\ntraining split, pad sequences in a DataLoader, train for a handful of epochs\nwith cross-entropy, and at every epoch compute SWA, CWA and their harmonic mean\n(HWA) on the dev set.  All tensors and the model are moved to GPU when\navailable, metrics/losses are tracked in the required experiment_data structure,\nsaved to ./working, and a quick HWA curve is plotted.  Finally we evaluate on\nthe hidden test split and print the scores.  This delivers a fully working end-\nto-end baseline that can later be replaced by more sophisticated neural-symbolic\narchitectures.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, math, datetime, random, json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nimport matplotlib.pyplot as plt\n\n# ------------------- working directory & device ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------- SPR benchmark loader ----------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_file: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_weighted_accuracy(swa, cwa):\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ------------------- Dataset & Vocab ---------------------------------------\nclass SPRVocab:\n    PAD = \"<pad>\"\n    UNK = \"<unk>\"\n\n    def __init__(self, sequences):\n        tokens = set()\n        for seq in sequences:\n            tokens.update(seq.split())\n        self.itos = [self.PAD, self.UNK] + sorted(tokens)\n        self.stoi = {tok: idx for idx, tok in enumerate(self.itos)}\n\n    def encode(self, seq):\n        return [self.stoi.get(tok, 1) for tok in seq.split()]  # 1 -> UNK\n\n    @property\n    def pad_id(self):\n        return 0\n\n    @property\n    def size(self):\n        return len(self.itos)\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split, vocab, label_map):\n        self.data = hf_split\n        self.vocab = vocab\n        self.label_map = label_map\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        example = self.data[idx]\n        seq_ids = torch.tensor(self.vocab.encode(example[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(self.label_map[example[\"label\"]], dtype=torch.long)\n        shape_var = torch.tensor(\n            count_shape_variety(example[\"sequence\"]), dtype=torch.float\n        )\n        color_var = torch.tensor(\n            count_color_variety(example[\"sequence\"]), dtype=torch.float\n        )\n        return {\n            \"ids\": seq_ids,\n            \"label\": label,\n            \"shape_var\": shape_var,\n            \"color_var\": color_var,\n            \"raw_sequence\": example[\"sequence\"],\n        }\n\n\ndef collate_fn(batch):\n    max_len = max(len(x[\"ids\"]) for x in batch)\n    ids_pad = torch.full((len(batch), max_len), vocab.pad_id, dtype=torch.long)\n    for i, x in enumerate(batch):\n        ids_pad[i, : len(x[\"ids\"])] = x[\"ids\"]\n    labels = torch.stack([x[\"label\"] for x in batch])\n    shape_v = torch.stack([x[\"shape_var\"] for x in batch])\n    color_v = torch.stack([x[\"color_var\"] for x in batch])\n    raw_seq = [x[\"raw_sequence\"] for x in batch]\n    return {\n        \"ids\": ids_pad,\n        \"label\": labels,\n        \"shape_var\": shape_v,\n        \"color_var\": color_v,\n        \"raw_sequence\": raw_seq,\n    }\n\n\n# ------------------- Model --------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_labels):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.fc = nn.Linear(embed_dim + 2, num_labels)  # concat shape & color stats\n\n    def forward(self, ids, shape_var, color_var):\n        mask = (ids != 0).unsqueeze(-1)\n        summed = (self.embed(ids) * mask).sum(1)\n        counts = mask.sum(1).clamp(min=1)\n        mean = summed / counts\n        features = torch.cat(\n            [mean, shape_var.unsqueeze(1), color_var.unsqueeze(1)], dim=1\n        )\n        return self.fc(features)\n\n\n# ------------------- Load data ---------------------------------------------\nroot_path = pathlib.Path(os.path.join(os.getcwd(), \"SPR_BENCH\"))\nassert root_path.exists(), f\"Expected SPR_BENCH directory at {root_path}\"\nspr = load_spr_bench(root_path)\nprint({k: len(v) for k, v in spr.items()})\n\n# build vocab & label map\nvocab = SPRVocab(spr[\"train\"][\"sequence\"])\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel_map = {lbl: i for i, lbl in enumerate(labels)}\nid2label = {i: lbl for lbl, i in label_map.items()}\n\n# datasets & loaders\nbatch_size = 128\ntrain_ds = SPRDataset(spr[\"train\"], vocab, label_map)\ndev_ds = SPRDataset(spr[\"dev\"], vocab, label_map)\ntest_ds = SPRDataset(spr[\"test\"], vocab, label_map)\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size, shuffle=False, collate_fn=collate_fn)\n\n# ------------------- Init model --------------------------------------------\nembed_dim = 64\nmodel = MeanPoolClassifier(vocab.size, embed_dim, len(labels)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ------------------- Experiment data dict ----------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_HWA\": [], \"val_HWA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ------------------- Training loop -----------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        ids = batch[\"ids\"].to(device)\n        labels_t = batch[\"label\"].to(device)\n        shape_v = batch[\"shape_var\"].to(device)\n        color_v = batch[\"color_var\"].to(device)\n\n        optimizer.zero_grad()\n        logits = model(ids, shape_v, color_v)\n        loss = criterion(logits, labels_t)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * ids.size(0)\n    train_loss = running_loss / len(train_ds)\n\n    # --- validation ---\n    model.eval()\n    val_loss, all_preds, all_trues, val_sequences = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            ids = batch[\"ids\"].to(device)\n            labels_t = batch[\"label\"].to(device)\n            shape_v = batch[\"shape_var\"].to(device)\n            color_v = batch[\"color_var\"].to(device)\n\n            logits = model(ids, shape_v, color_v)\n            loss = criterion(logits, labels_t)\n            val_loss += loss.item() * ids.size(0)\n\n            preds = logits.argmax(1).cpu().tolist()\n            trues = labels_t.cpu().tolist()\n            all_preds.extend(preds)\n            all_trues.extend(trues)\n            val_sequences.extend(batch[\"raw_sequence\"])\n    val_loss /= len(dev_ds)\n\n    # metrics\n    swa = shape_weighted_accuracy(val_sequences, all_trues, all_preds)\n    cwa = color_weighted_accuracy(val_sequences, all_trues, all_preds)\n    hwa = harmonic_weighted_accuracy(swa, cwa)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_HWA\"].append(None)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_HWA\"].append(hwa)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(\n        datetime.datetime.utcnow().isoformat()\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n        f\"SWA={swa:.3f} CWA={cwa:.3f} HWA={hwa:.3f}\"\n    )\n\n# ------------------- Test evaluation ---------------------------------------\nmodel.eval()\ntest_preds, test_trues, test_seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        ids = batch[\"ids\"].to(device)\n        shape_v = batch[\"shape_var\"].to(device)\n        color_v = batch[\"color_var\"].to(device)\n        logits = model(ids, shape_v, color_v)\n        preds = logits.argmax(1).cpu().tolist()\n        test_preds.extend(preds)\n        test_trues.extend(batch[\"label\"].tolist())\n        test_seqs.extend(batch[\"raw_sequence\"])\n\nswa_test = shape_weighted_accuracy(test_seqs, test_trues, test_preds)\ncwa_test = color_weighted_accuracy(test_seqs, test_trues, test_preds)\nhwa_test = harmonic_weighted_accuracy(swa_test, cwa_test)\nprint(f\"\\nTEST  | SWA={swa_test:.3f} CWA={cwa_test:.3f} HWA={hwa_test:.3f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_trues\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_HWA\"] = hwa_test\n\n# ------------------- Save results ------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# Quick loss plot\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.title(\"Loss Curve SPR_BENCH\")\nplt.savefig(os.path.join(working_dir, \"loss_curve_spr.png\"), dpi=120)\nplt.close()\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nfrom typing import List, Dict\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------- device -----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------- data loader utilities (given) -----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef harmonic_weighted_accuracy(sequences, y_true, y_pred):\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------- simple vocabulary -----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]) -> List[int]:\n        return [self.stoi[tok] for tok in tokens]\n\n\n# ----------- model -----------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)\n\n\n# ----------- data path -----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------- build vocab and label map -----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ----------- collate function -----------\ndef collate_batch(batch):\n    token_ids = []\n    offsets = [0]\n    label_ids = []\n    for ex in batch:\n        seq, lab = ex[\"sequence\"], ex[\"label\"]\n        ids = vocab(seq.split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[lab])\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    text = torch.tensor(token_ids, dtype=torch.long)\n    labels_t = torch.tensor(label_ids, dtype=torch.long)\n    return text.to(device), offsets.to(device), labels_t.to(device)\n\n\n# ----------- dataloaders -----------\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ----------- experiment tracking -----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ----------- training setup -----------\nembed_dim = 64\nmodel = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nepochs = 5\n\n\n# ----------- helpers -----------\ndef evaluate(data_loader):\n    model.eval()\n    y_true, y_pred, sequences = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, (text, offsets, labels_t) in enumerate(data_loader):\n            outputs = model(text, offsets)\n            loss = criterion(outputs, labels_t)\n            total_loss += loss.item() * labels_t.size(0)\n            preds = outputs.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            # recover sequences for metric weighting\n            start = batch_idx * batch_size\n            end = start + labels_t.size(0)\n            sequences.extend(data_loader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(sequences, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------- training loop -----------\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for text, offsets, labels_t in train_loader:\n        optimizer.zero_grad()\n        outputs = model(text, offsets)\n        loss = criterion(outputs, labels_t)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * labels_t.size(0)\n    train_loss = running_loss / len(spr[\"train\"])\n    val_loss, swa, cwa, hwa, _, _ = evaluate(dev_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n    )\n\n# ----------- final test evaluation -----------\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred_t\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true_t\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, math, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- load data ----------\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"./SPR_BENCH\"))\ndsets = load_spr_bench(DATA_PATH)  # HuggingFace DatasetDict\n\n\n# ---------- build vocabulary ----------\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = {\"<pad>\": 0}\nfor split in dsets.values():\n    for seq in split[\"sequence\"]:\n        for tok in tokenize(seq):\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n# ---------- label mapping ----------\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Labels ({num_classes}): {labels}\")\n\n\n# ---------- torch Dataset ----------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.label = [label2id[l] for l in hf_ds[\"label\"]]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        tokens = [vocab[t] for t in tokenize(self.seq[idx])]\n        shapes = len(set(t[0] for t in tokenize(self.seq[idx])))\n        colors = len(set(t[1] for t in tokenize(self.seq[idx]) if len(t) > 1))\n        return {\n            \"tokens\": torch.LongTensor(tokens),\n            \"shape_var\": torch.tensor(shapes, dtype=torch.float),\n            \"color_var\": torch.tensor(colors, dtype=torch.float),\n            \"label\": torch.tensor(self.label[idx], dtype=torch.long),\n            \"raw_seq\": self.seq[idx],\n        }\n\n\ndef collate(batch):\n    max_len = max(len(x[\"tokens\"]) for x in batch)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    mask = torch.zeros(len(batch), max_len, dtype=torch.bool)\n    for i, x in enumerate(batch):\n        l = len(x[\"tokens\"])\n        padded[i, :l] = x[\"tokens\"]\n        mask[i, :l] = 1\n    out = {\n        \"tokens\": padded,\n        \"mask\": mask,\n        \"shape_var\": torch.stack([x[\"shape_var\"] for x in batch]),\n        \"color_var\": torch.stack([x[\"color_var\"] for x in batch]),\n        \"label\": torch.stack([x[\"label\"] for x in batch]),\n        \"raw_seq\": [x[\"raw_seq\"] for x in batch],\n    }\n    return out\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(dsets[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(dsets[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(dsets[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------- model ----------\nclass SPRBaseline(nn.Module):\n    def __init__(self, vocab_sz, embed_dim, num_cls):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, embed_dim, padding_idx=0)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim + 2, 64), nn.ReLU(), nn.Linear(64, num_cls)\n        )\n\n    def forward(self, tokens, mask, shape_var, color_var):\n        emb = self.embed(tokens)  # B x T x E\n        mask = mask.unsqueeze(-1)\n        sum_emb = (emb * mask).sum(1)\n        len_emb = mask.sum(1).clamp(min=1)\n        mean_emb = sum_emb / len_emb  # B x E\n        feats = torch.cat(\n            [mean_emb, shape_var.unsqueeze(1), color_var.unsqueeze(1)], dim=1\n        )  # B x (E+2)\n        return self.ff(feats)\n\n\nmodel = SPRBaseline(vocab_size, 32, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------- experiment tracking ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_HWA\": [], \"val_HWA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\ndef evaluate(loader):\n    model.eval()\n    all_pred, all_true, all_seq = [], [], []\n    total_loss, n = 0.0, 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(\n                batch[\"tokens\"], batch[\"mask\"], batch[\"shape_var\"], batch[\"color_var\"]\n            )\n            loss = criterion(logits, batch[\"label\"])\n            total_loss += loss.item() * len(batch[\"label\"])\n            n += len(batch[\"label\"])\n            preds = logits.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(batch[\"label\"].cpu().tolist())\n            all_seq.extend(batch[\"raw_seq\"])\n    # metrics\n    y_true_lbl = [labels[i] for i in all_true]\n    y_pred_lbl = [labels[i] for i in all_pred]\n    swa = shape_weighted_accuracy(all_seq, y_true_lbl, y_pred_lbl)\n    cwa = color_weighted_accuracy(all_seq, y_true_lbl, y_pred_lbl)\n    hwa = 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n    return total_loss / n, hwa, swa, cwa, y_pred_lbl, y_true_lbl\n\n\n# ---------- training ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(\n            batch[\"tokens\"], batch[\"mask\"], batch[\"shape_var\"], batch[\"color_var\"]\n        )\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * len(batch[\"label\"])\n    train_loss = running_loss / len(train_loader.dataset)\n    val_loss, val_hwa, val_swa, val_cwa, _, _ = evaluate(dev_loader)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n        f\"SWA={val_swa:.3f}  CWA={val_cwa:.3f}  HWA={val_hwa:.3f}\"\n    )\n    # log\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_HWA\"].append(\n        float(\"nan\")\n    )  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_HWA\"].append(val_hwa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n# ---------- final test evaluation ----------\ntest_loss, test_hwa, test_swa, test_cwa, y_pred, y_true = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f}  SWA={test_swa:.3f}  CWA={test_cwa:.3f}  HWA={test_hwa:.3f}\"\n)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true\n\n# ---------- save metrics ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- plot ----------\nepochs_axis = np.arange(1, epochs + 1)\nplt.figure()\nplt.plot(\n    epochs_axis, experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train_loss\"\n)\nplt.plot(epochs_axis, experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nplt.close()\n\nplt.figure()\nplt.plot(\n    epochs_axis, experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_HWA\"], label=\"val_HWA\"\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"HWA\")\nplt.legend()\nplt.title(\"Validation HWA\")\nplt.savefig(os.path.join(working_dir, \"val_hwa_curve.png\"))\nplt.close()\n", "import os, pathlib, time, math, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy\n\n# --------------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# experiment data container\nexperiment_data = {\n    \"spr_bench\": {\n        \"metrics\": {\"train_HWA\": [], \"val_HWA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": {\"val\": [], \"test\": []},\n        \"ground_truth\": {\"val\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# --------------------------------------------------------------------------\n# device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------------------------------------------------------------\n# load dataset\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\n\n\n# --------------------------------------------------------------------------\n# build vocabulary from train split\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(dsets[\"train\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(seq, vocab):\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\n# --------------------------------------------------------------------------\n# PyTorch Dataset wrappers\nclass SPRDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_split, vocab):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        tok_ids = torch.tensor(encode_sequence(seq, self.vocab), dtype=torch.long)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        # symbolic features\n        s_var = len(set(tok[0] for tok in seq.strip().split() if tok))\n        c_var = len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n        sym_feat = torch.tensor([s_var, c_var], dtype=torch.float)\n        return {\"input_ids\": tok_ids, \"sym\": sym_feat, \"label\": label, \"sequence\": seq}\n\n\ndef collate(batch):\n    lengths = [len(ex[\"input_ids\"]) for ex in batch]\n    maxlen = max(lengths)\n    input_ids = torch.zeros(len(batch), maxlen, dtype=torch.long)\n    for i, ex in enumerate(batch):\n        input_ids[i, : lengths[i]] = ex[\"input_ids\"]\n    sym = torch.stack([ex[\"sym\"] for ex in batch])\n    labels = torch.stack([ex[\"label\"] for ex in batch])\n    sequences = [ex[\"sequence\"] for ex in batch]\n    return {\n        \"input_ids\": input_ids,\n        \"sym\": sym,\n        \"label\": labels,\n        \"sequence\": sequences,\n        \"lengths\": torch.tensor(lengths, dtype=torch.long),\n    }\n\n\ntrain_ds = SPRDataset(dsets[\"train\"], vocab)\nval_ds = SPRDataset(dsets[\"dev\"], vocab)\ntest_ds = SPRDataset(dsets[\"test\"], vocab)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------------------------------------------------------------------\n# Model\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, sym_dim, n_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.lin = nn.Sequential(\n            nn.Linear(emb_dim + sym_dim, 128), nn.ReLU(), nn.Linear(128, n_labels)\n        )\n\n    def forward(self, input_ids, sym_feats):\n        mask = (input_ids != 0).unsqueeze(-1)\n        emb = self.emb(input_ids)\n        emb = emb * mask\n        sum_emb = emb.sum(1)\n        lengths = mask.sum(1).clamp(min=1)\n        mean_emb = sum_emb / lengths  # shape [B, emb_dim]\n        x = torch.cat([mean_emb, sym_feats], dim=1)\n        return self.lin(x)\n\n\nn_labels = int(max(dsets[\"train\"][\"label\"])) + 1\nmodel = MeanPoolClassifier(vocab_size, emb_dim=32, sym_dim=2, n_labels=n_labels).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# --------------------------------------------------------------------------\ndef evaluate(loader):\n    model.eval()\n    total_loss, total, correct = 0, 0, 0\n    all_seq, all_true, all_pred = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            sym = batch[\"sym\"].to(device)\n            labels = batch[\"label\"].to(device)\n            logits = model(input_ids, sym)\n            loss = criterion(logits, labels)\n            total_loss += loss.item() * labels.size(0)\n            preds = logits.argmax(-1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n            all_seq.extend(batch[\"sequence\"])\n            all_true.extend(labels.cpu().tolist())\n            all_pred.extend(preds.cpu().tolist())\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    hwa = 2 * swa * cwa / (swa + cwa + 1e-8)\n    return total_loss / total, hwa, swa, cwa, all_pred, all_true\n\n\n# --------------------------------------------------------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    epoch_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        sym = batch[\"sym\"].to(device)\n        labels = batch[\"label\"].to(device)\n        logits = model(input_ids, sym)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * labels.size(0)\n    train_loss = epoch_loss / len(train_ds)\n    val_loss, val_hwa, val_swa, val_cwa, val_preds, val_gts = evaluate(val_loader)\n    # training set HWA for reference (quick estimate via last mini-batch)\n    tr_loss_tmp, tr_hwa, _, _, _, _ = evaluate(train_loader)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} | HWA {val_hwa:.4f} (S:{val_swa:.4f} C:{val_cwa:.4f})\"\n    )\n    # log\n    experiment_data[\"spr_bench\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"spr_bench\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"spr_bench\"][\"metrics\"][\"train_HWA\"].append(tr_hwa)\n    experiment_data[\"spr_bench\"][\"metrics\"][\"val_HWA\"].append(val_hwa)\n    experiment_data[\"spr_bench\"][\"predictions\"][\"val\"].append(val_preds)\n    experiment_data[\"spr_bench\"][\"ground_truth\"][\"val\"].append(val_gts)\n    experiment_data[\"spr_bench\"][\"timestamps\"].append(time.time())\n\n# --------------------------------------------------------------------------\n# final test evaluation\ntest_loss, test_hwa, test_swa, test_cwa, test_preds, test_gts = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f} HWA={test_hwa:.4f} (SWA={test_swa:.4f} CWA={test_cwa:.4f})\"\n)\nexperiment_data[\"spr_bench\"][\"predictions\"][\"test\"] = test_preds\nexperiment_data[\"spr_bench\"][\"ground_truth\"][\"test\"] = test_gts\nexperiment_data[\"spr_bench\"][\"test_metrics\"] = {\n    \"HWA\": test_hwa,\n    \"SWA\": test_swa,\n    \"CWA\": test_cwa,\n}\n\n# --------------------------------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------------------------------------------------------------------\n# simple plot\nepochs = range(1, EPOCHS + 1)\nplt.figure()\nplt.plot(\n    epochs, experiment_data[\"spr_bench\"][\"metrics\"][\"train_HWA\"], label=\"Train HWA\"\n)\nplt.plot(epochs, experiment_data[\"spr_bench\"][\"metrics\"][\"val_HWA\"], label=\"Val HWA\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"HWA\")\nplt.legend()\nplt.title(\"HWA over epochs\")\nplt.savefig(os.path.join(working_dir, \"spr_bench_hwa_curve.png\"))\nplt.close()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nfrom typing import List, Dict\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------- device -----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------- data loader utilities (given) -----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef harmonic_weighted_accuracy(sequences, y_true, y_pred):\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------- simple vocabulary -----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]) -> List[int]:\n        return [self.stoi[tok] for tok in tokens]\n\n\n# ----------- model -----------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)\n\n\n# ----------- data path -----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------- build vocab and label map -----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ----------- collate function -----------\ndef collate_batch(batch):\n    token_ids = []\n    offsets = [0]\n    label_ids = []\n    for ex in batch:\n        seq, lab = ex[\"sequence\"], ex[\"label\"]\n        ids = vocab(seq.split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[lab])\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    text = torch.tensor(token_ids, dtype=torch.long)\n    labels_t = torch.tensor(label_ids, dtype=torch.long)\n    return text.to(device), offsets.to(device), labels_t.to(device)\n\n\n# ----------- dataloaders -----------\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ----------- experiment tracking -----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ----------- training setup -----------\nembed_dim = 64\nmodel = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nepochs = 5\n\n\n# ----------- helpers -----------\ndef evaluate(data_loader):\n    model.eval()\n    y_true, y_pred, sequences = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, (text, offsets, labels_t) in enumerate(data_loader):\n            outputs = model(text, offsets)\n            loss = criterion(outputs, labels_t)\n            total_loss += loss.item() * labels_t.size(0)\n            preds = outputs.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            # recover sequences for metric weighting\n            start = batch_idx * batch_size\n            end = start + labels_t.size(0)\n            sequences.extend(data_loader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(sequences, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------- training loop -----------\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for text, offsets, labels_t in train_loader:\n        optimizer.zero_grad()\n        outputs = model(text, offsets)\n        loss = criterion(outputs, labels_t)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * labels_t.size(0)\n    train_loss = running_loss / len(spr[\"train\"])\n    val_loss, swa, cwa, hwa, _, _ = evaluate(dev_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n    )\n\n# ----------- final test evaluation -----------\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred_t\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true_t\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nfrom typing import List, Dict\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------- device -----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------- data loader utilities (given) -----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef harmonic_weighted_accuracy(sequences, y_true, y_pred):\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------- simple vocabulary -----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]) -> List[int]:\n        return [self.stoi[tok] for tok in tokens]\n\n\n# ----------- model -----------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)\n\n\n# ----------- data path -----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------- build vocab and label map -----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ----------- collate function -----------\ndef collate_batch(batch):\n    token_ids = []\n    offsets = [0]\n    label_ids = []\n    for ex in batch:\n        seq, lab = ex[\"sequence\"], ex[\"label\"]\n        ids = vocab(seq.split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[lab])\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    text = torch.tensor(token_ids, dtype=torch.long)\n    labels_t = torch.tensor(label_ids, dtype=torch.long)\n    return text.to(device), offsets.to(device), labels_t.to(device)\n\n\n# ----------- dataloaders -----------\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ----------- experiment tracking -----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ----------- training setup -----------\nembed_dim = 64\nmodel = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nepochs = 5\n\n\n# ----------- helpers -----------\ndef evaluate(data_loader):\n    model.eval()\n    y_true, y_pred, sequences = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, (text, offsets, labels_t) in enumerate(data_loader):\n            outputs = model(text, offsets)\n            loss = criterion(outputs, labels_t)\n            total_loss += loss.item() * labels_t.size(0)\n            preds = outputs.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            # recover sequences for metric weighting\n            start = batch_idx * batch_size\n            end = start + labels_t.size(0)\n            sequences.extend(data_loader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(sequences, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------- training loop -----------\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for text, offsets, labels_t in train_loader:\n        optimizer.zero_grad()\n        outputs = model(text, offsets)\n        loss = criterion(outputs, labels_t)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * labels_t.size(0)\n    train_loss = running_loss / len(spr[\"train\"])\n    val_loss, swa, cwa, hwa, _, _ = evaluate(dev_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n    )\n\n# ----------- final test evaluation -----------\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred_t\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true_t\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nfrom typing import List, Dict\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ----------- device -----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------- data loader utilities (given) -----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef harmonic_weighted_accuracy(sequences, y_true, y_pred):\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    return 0 if (swa + cwa) == 0 else 2 * swa * cwa / (swa + cwa)\n\n\n# ----------- simple vocabulary -----------\nclass Vocab:\n    def __init__(self, tokens: List[str]):\n        self.itos = [\"<pad>\"] + sorted(set(tokens))\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def __call__(self, tokens: List[str]) -> List[int]:\n        return [self.stoi[tok] for tok in tokens]\n\n\n# ----------- model -----------\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)\n\n\n# ----------- data path -----------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"SPR_BENCH not found at {DATA_PATH}\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------- build vocab and label map -----------\nall_tokens = [tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split()]\nvocab = Vocab(all_tokens)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\n\n\n# ----------- collate function -----------\ndef collate_batch(batch):\n    token_ids = []\n    offsets = [0]\n    label_ids = []\n    for ex in batch:\n        seq, lab = ex[\"sequence\"], ex[\"label\"]\n        ids = vocab(seq.split())\n        token_ids.extend(ids)\n        offsets.append(offsets[-1] + len(ids))\n        label_ids.append(label2id[lab])\n    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n    text = torch.tensor(token_ids, dtype=torch.long)\n    labels_t = torch.tensor(label_ids, dtype=torch.long)\n    return text.to(device), offsets.to(device), labels_t.to(device)\n\n\n# ----------- dataloaders -----------\nbatch_size = 128\ntrain_loader = DataLoader(\n    spr[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\ndev_loader = DataLoader(\n    spr[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_loader = DataLoader(\n    spr[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\n\n# ----------- experiment tracking -----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ----------- training setup -----------\nembed_dim = 64\nmodel = BagClassifier(len(vocab), embed_dim, len(labels)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nepochs = 5\n\n\n# ----------- helpers -----------\ndef evaluate(data_loader):\n    model.eval()\n    y_true, y_pred, sequences = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, (text, offsets, labels_t) in enumerate(data_loader):\n            outputs = model(text, offsets)\n            loss = criterion(outputs, labels_t)\n            total_loss += loss.item() * labels_t.size(0)\n            preds = outputs.argmax(1).cpu().tolist()\n            y_pred.extend([id2label[p] for p in preds])\n            y_true.extend([id2label[i] for i in labels_t.cpu().tolist()])\n            # recover sequences for metric weighting\n            start = batch_idx * batch_size\n            end = start + labels_t.size(0)\n            sequences.extend(data_loader.dataset[\"sequence\"][start:end])\n    avg_loss = total_loss / len(y_true)\n    swa = shape_weighted_accuracy(sequences, y_true, y_pred)\n    cwa = color_weighted_accuracy(sequences, y_true, y_pred)\n    hwa = harmonic_weighted_accuracy(sequences, y_true, y_pred)\n    return avg_loss, swa, cwa, hwa, y_true, y_pred\n\n\n# ----------- training loop -----------\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for text, offsets, labels_t in train_loader:\n        optimizer.zero_grad()\n        outputs = model(text, offsets)\n        loss = criterion(outputs, labels_t)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * labels_t.size(0)\n    train_loss = running_loss / len(spr[\"train\"])\n    val_loss, swa, cwa, hwa, _, _ = evaluate(dev_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)  # placeholder\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"HWA\": hwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | HWA={hwa:.4f}\"\n    )\n\n# ----------- final test evaluation -----------\ntest_loss, swa_t, cwa_t, hwa_t, y_true_t, y_pred_t = evaluate(test_loader)\nprint(\n    f\"Test: loss={test_loss:.4f} | SWA={swa_t:.4f} | CWA={cwa_t:.4f} | HWA={hwa_t:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred_t\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true_t\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 148, in <module>\\n    assert root_path.exists(), f\"Expected\nSPR_BENCH directory at {root_path}\"\\n\n^^^^^^^^^^^^^^^^^^\\nAssertionError: Expected SPR_BENCH directory at\n/home/zxl240011/AI-Scientist-v2/experiments/2025-07-29_02-18-\n25_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 548691.03\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 707684.42\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 539883.90\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Epoch\n1: train_loss=0.5951 | val_loss=0.5392 | SWA=0.7411 | CWA=0.7364 | HWA=0.7387',\n'\\n', 'Epoch 2: train_loss=0.5268 | val_loss=0.5226 | SWA=0.7407 | CWA=0.7358 |\nHWA=0.7382', '\\n', 'Epoch 3: train_loss=0.5205 | val_loss=0.5212 | SWA=0.7458 |\nCWA=0.7405 | HWA=0.7432', '\\n', 'Epoch 4: train_loss=0.5198 | val_loss=0.5213 |\nSWA=0.7481 | CWA=0.7455 | HWA=0.7468', '\\n', 'Epoch 5: train_loss=0.5199 |\nval_loss=0.5210 | SWA=0.7393 | CWA=0.7349 | HWA=0.7371', '\\n', 'Test:\nloss=0.7201 | SWA=0.5950 | CWA=0.6205 | HWA=0.6075', '\\n', 'Execution time: 5\nseconds seconds (time limit is 30 minutes).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 4, in <module>\\n\nfrom SPR import load_spr_bench, shape_weighted_accuracy,\ncolor_weighted_accuracy\\nModuleNotFoundError: No module named \\'SPR\\'\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 5, in <module>\\n\nfrom SPR import load_spr_bench, shape_weighted_accuracy,\ncolor_weighted_accuracy\\nModuleNotFoundError: No module named \\'SPR\\'\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 511734.51\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 622041.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 661885.78\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Epoch\n1: train_loss=0.5788 | val_loss=0.5327 | SWA=0.7301 | CWA=0.7259 | HWA=0.7280',\n'\\n', 'Epoch 2: train_loss=0.5238 | val_loss=0.5220 | SWA=0.7352 | CWA=0.7311 |\nHWA=0.7331', '\\n', 'Epoch 3: train_loss=0.5201 | val_loss=0.5211 | SWA=0.7470 |\nCWA=0.7414 | HWA=0.7442', '\\n', 'Epoch 4: train_loss=0.5200 | val_loss=0.5211 |\nSWA=0.7500 | CWA=0.7441 | HWA=0.7471', '\\n', 'Epoch 5: train_loss=0.5199 |\nval_loss=0.5209 | SWA=0.7451 | CWA=0.7398 | HWA=0.7425', '\\n', 'Test:\nloss=0.7254 | SWA=0.5928 | CWA=0.6193 | HWA=0.6058', '\\n', 'Execution time: 6\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 433224.26\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 686532.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 817826.31\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Epoch\n1: train_loss=0.6176 | val_loss=0.5480 | SWA=0.7468 | CWA=0.7449 | HWA=0.7458',\n'\\n', 'Epoch 2: train_loss=0.5308 | val_loss=0.5236 | SWA=0.7537 | CWA=0.7503 |\nHWA=0.7520', '\\n', 'Epoch 3: train_loss=0.5207 | val_loss=0.5210 | SWA=0.7450 |\nCWA=0.7409 | HWA=0.7429', '\\n', 'Epoch 4: train_loss=0.5200 | val_loss=0.5212 |\nSWA=0.7408 | CWA=0.7374 | HWA=0.7391', '\\n', 'Epoch 5: train_loss=0.5198 |\nval_loss=0.5220 | SWA=0.7454 | CWA=0.7410 | HWA=0.7432', '\\n', 'Test:\nloss=0.7211 | SWA=0.5913 | CWA=0.6181 | HWA=0.6044', '\\n', 'Execution time: 6\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 387301.78\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 547273.49\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 511731.39\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Epoch\n1: train_loss=0.6049 | val_loss=0.5409 | SWA=0.7422 | CWA=0.7388 | HWA=0.7405',\n'\\n', 'Epoch 2: train_loss=0.5287 | val_loss=0.5230 | SWA=0.7318 | CWA=0.7296 |\nHWA=0.7307', '\\n', 'Epoch 3: train_loss=0.5208 | val_loss=0.5207 | SWA=0.7335 |\nCWA=0.7292 | HWA=0.7313', '\\n', 'Epoch 4: train_loss=0.5203 | val_loss=0.5212 |\nSWA=0.7474 | CWA=0.7420 | HWA=0.7447', '\\n', 'Epoch 5: train_loss=0.5197 |\nval_loss=0.5214 | SWA=0.7448 | CWA=0.7404 | HWA=0.7426', '\\n', 'Test:\nloss=0.7185 | SWA=0.5911 | CWA=0.6177 | HWA=0.6041', '\\n', 'Execution time: 12\nseconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution failed due to a missing SPR_BENCH directory at the specified path.\nThis directory is expected to contain the benchmark datasets, but it was not\nfound. To fix this, ensure that the SPR_BENCH directory is correctly placed at\nthe specified path (/home/zxl240011/AI-Scientist-v2/experiments/2025-07-29_02-\n18-25_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH) before running the script. Verify the directory structure and the\npresence of the required CSV files (train.csv, dev.csv, test.csv) within it.", "", "The execution failed due to a missing module error: 'No module named SPR'. This\nissue occurs because the script attempts to import the 'SPR' module, but the\nmodule is not available in the environment. To fix this, ensure that the\n'SPR.py' file is located in the same directory as the script being executed or\nin a directory included in the Python module search path. Alternatively, the\nscript can be updated to include the correct path to 'SPR.py' before importing\nit, using `sys.path.append` or similar methods.", "The code execution failed due to a missing module error: 'No module named\n'SPR''. This indicates that the script could not find the 'SPR' module, which is\nlikely expected to contain the necessary functions and classes for the\nexperiment. To fix this issue, ensure that the 'SPR.py' file is in the same\ndirectory as the script being executed or in a directory included in the Python\npath. Alternatively, the module can be installed if it is part of an external\npackage. Verify the module's presence and correct any import path issues.", "", "", "", ""], "exc_type": ["AssertionError", null, "ModuleNotFoundError", "ModuleNotFoundError", null, null, null, null], "exc_info": [{"args": ["Expected SPR_BENCH directory at /home/zxl240011/AI-Scientist-v2/experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-1/SPR_BENCH"]}, null, {"args": ["No module named 'SPR'"], "name": "SPR", "msg": "No module named 'SPR'"}, {"args": ["No module named 'SPR'"], "name": "SPR", "msg": "No module named 'SPR'"}, null, null, null, null], "exc_stack": [[["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 148, "<module>", "assert root_path.exists(), f\"Expected SPR_BENCH directory at {root_path}\""]], null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 4, "<module>", "from SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 5, "<module>", "from SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5199, "best_value": 0.5199}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss at the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.521, "best_value": 0.521}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Best shape-weighted accuracy on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7481, "best_value": 0.7481}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Best color-weighted accuracy on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7455, "best_value": 0.7455}]}, {"metric_name": "harmonic-weighted accuracy", "lower_is_better": false, "description": "Best harmonic-weighted accuracy on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7468, "best_value": 0.7468}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, typically minimized.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5199, "best_value": 0.5199}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, typically minimized.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5209, "best_value": 0.5209}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy score based on shape-weighted evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "The accuracy score based on color-weighted evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7441, "best_value": 0.7441}]}, {"metric_name": "harmonic-weighted accuracy", "lower_is_better": false, "description": "The accuracy score based on harmonic-weighted evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7471, "best_value": 0.7471}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during the final epoch of training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5198, "best_value": 0.5198}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during the final epoch of validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.522, "best_value": 0.522}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape features during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7537, "best_value": 0.7537}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by color features during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7503, "best_value": 0.7503}]}, {"metric_name": "harmonic-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted harmonically during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.752, "best_value": 0.752}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss observed during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5197, "best_value": 0.5197}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss observed during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5214, "best_value": 0.5214}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by shape during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7474, "best_value": 0.7474}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by color during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.742, "best_value": 0.742}]}, {"metric_name": "harmonic-weighted accuracy", "lower_is_better": false, "description": "Harmonic mean of accuracies during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7447, "best_value": 0.7447}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_confusion_matrix.png"], [], [], ["../../logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_final_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_agg_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_agg_val_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_agg_final_metrics_bar.png", "../../logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_confusion_matrix_example.png"]], "plot_paths": [[], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_val_metrics.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_final_metrics_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_confusion_matrix.png"], [], [], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_val_metrics.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_final_metrics_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_val_metrics.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_final_metrics_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_val_metrics.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_final_metrics_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_agg_loss_curves.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_agg_val_metrics.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_agg_final_metrics_bar.png", "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_5fc974673d344d8484b123636cb04bc2/SPR_BENCH_confusion_matrix_example.png"]], "plot_analyses": [[], [{"analysis": "This plot shows the training and validation loss over epochs. Both losses decrease steadily, with the validation loss closely following the training loss. By the fifth epoch, the losses converge, indicating that the model has achieved a good fit without overfitting. The consistent reduction in both losses suggests that the model is learning effectively from the data.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot displays the progression of Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and an additional metric (HWA) over epochs. All metrics improve steadily until the fourth epoch, where they peak, followed by a slight decline in the fifth epoch. This suggests that the model performs best at epoch 4, and further training might lead to overfitting.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart summarizes the final validation results for the three metrics: SWA, CWA, and HWA. SWA and HWA achieve the highest scores (0.74), while CWA is slightly lower (0.73). The close performance across metrics indicates that the model generalizes well across the different weighted accuracies.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix for the test set shows the distribution of true and predicted labels. The majority of predictions are correct, as evidenced by the darker diagonal, indicating strong model performance. However, there are non-negligible misclassifications, particularly in the upper right and lower left quadrants, suggesting areas for improvement in distinguishing between classes.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_ca076407e671466abbcc4243d0c66723_proc_456838/SPR_BENCH_confusion_matrix.png"}], [], [], [{"analysis": "The plot indicates that both training and validation loss decrease rapidly in the first two epochs and then stabilize at a similar value. This suggests that the model converges quickly and does not overfit, as the training and validation losses remain close throughout the training process. This is a positive sign of effective learning and generalization.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_loss_curves.png"}, {"analysis": "The plot shows the progression of three metrics (SWA, CWA, and HWA) over epochs. All metrics improve steadily until epoch 3, after which they plateau or slightly decline. This trend suggests that the model achieves optimal performance around epoch 3. SWA consistently outperforms CWA and HWA, indicating that the model is better at accounting for shape variety than color or hybrid attributes.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_val_metrics.png"}, {"analysis": "The bar chart presents the final validation metric values for SWA, CWA, and HWA. SWA achieves the highest value (0.75), followed closely by CWA and HWA (both at 0.74). This reinforces the observation that the model performs slightly better on shape-weighted accuracy compared to color-weighted or hybrid-weighted accuracy. The close values indicate balanced performance across metrics.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix for the test set shows that the model has a reasonably balanced performance in predicting both classes. However, there is a noticeable number of misclassifications in both directions. Further analysis could focus on whether these errors are concentrated in specific types of sequences or occur uniformly across the dataset.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over five epochs. The training loss decreases significantly in the first two epochs and stabilizes by the third epoch, indicating that the model is learning effectively. However, the validation loss decreases initially and then starts to increase slightly after the third epoch, suggesting potential overfitting. The gap between training and validation loss is minimal, which is a good sign of generalization, but the slight increase in validation loss warrants further regularization or early stopping.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot tracks the progression of three metrics\u2014Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and an additional metric (HWA)\u2014over the epochs. All metrics peak at the second epoch and then decline, reaching their lowest values at the fourth epoch, before recovering slightly in the fifth epoch. This pattern aligns with the overfitting observed in the loss plot, as the model performs best early on but struggles to maintain performance as training progresses. The initial peak suggests that the model captures the patterns effectively, but the subsequent decline indicates a need to address overfitting or improve robustness.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart provides a summary of the final validation metric values for SWA, CWA, and HWA. The SWA achieves the highest value at 0.75, while CWA and HWA are slightly lower at 0.74. These values suggest consistent performance across the metrics, but the marginal differences indicate that the model performs slightly better in shape-weighted tasks compared to color-weighted and hybrid tasks. Overall, the results are promising but leave room for improvement to achieve higher accuracy.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix for the test set reveals the distribution of true and predicted labels for a binary classification task. The model shows a strong performance for both classes, but there is a notable number of misclassifications, as indicated by the off-diagonal elements. The imbalance between true positives and true negatives suggests that the model might favor one class over the other, which could be addressed by adjusting the class weights or using a more balanced dataset.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over epochs. The training loss decreases rapidly and stabilizes by epoch 3, indicating that the model is learning effectively. The validation loss follows a similar trend, with a slight plateau after epoch 3. The close alignment between training and validation loss implies that the model is not overfitting and has good generalization performance.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot tracks the evolution of three metrics (SWA, CWA, HWA) over epochs. Initially, all metrics decrease slightly, suggesting some instability in the learning process. However, from epoch 3 onward, there is a significant improvement in all metrics, peaking at epoch 4, followed by a slight decline. This indicates that the model achieves its best performance around epoch 4, demonstrating its ability to generalize well on the validation set.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart summarizes the final validation metric values for SWA, CWA, and HWA. All metrics converge to approximately 0.74, suggesting consistent performance across different evaluation criteria. This uniformity indicates that the model performs well in balancing shape and color-weighted reasoning tasks.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_final_metrics_bar.png"}, {"analysis": "The confusion matrix for the test set shows the distribution of true versus predicted labels. The model performs reasonably well, with a noticeable number of correct predictions for both classes. However, there is a slight imbalance, as the off-diagonal elements are non-zero, indicating some misclassifications. This suggests room for improvement in reducing false positives and false negatives.", "plot_path": "experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["[]", "The plots demonstrate effective model training and validation, with the model\nachieving its best performance at epoch 4. The final validation metrics are\nconsistent and robust, with minimal overfitting observed. The confusion matrix\nhighlights good performance on the test set, though improvements in class\ndifferentiation could further enhance results.", "[]", "[]", "The plots collectively indicate that the model converges quickly, achieves\nbalanced performance across metrics, and performs well on the SPR_BENCH\nbenchmark. SWA is slightly better than CWA and HWA, and the confusion matrix\nsuggests room for improvement in classification accuracy.", "The plots indicate that the model is learning effectively but shows signs of\noverfitting after the second epoch. Validation metrics peak early and then\ndecline, suggesting that further regularization or early stopping might be\nbeneficial. The final validation metrics are consistent but leave room for\nimprovement, and the confusion matrix highlights potential class imbalance\nissues.", "The plots indicate that the model achieves good generalization, with validation\nmetrics peaking at epoch 4. The final validation metrics are consistent across\ndifferent criteria, and the confusion matrix highlights reasonable performance\nwith some room for improvement in misclassification rates.", "[]"], "exec_time": [0.4670422077178955, 5.370668172836304, 0.0022194385528564453, 0.002040386199951172, 6.067805528640747, 6.05088472366333, 12.460425853729248, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], ["[\"SPR_BENCH\"]"], [], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- paths -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------- load data -----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch safely\ndef get(data, *keys, default=None):\n    for k in keys:\n        data = data.get(k, {})\n    return data if data != {} else default\n\n\nds_name = \"SPR_BENCH\"\nexp = experiment_data.get(ds_name, {})\n\nloss_train = get(exp, \"losses\", \"train\", default=[])\nloss_val = get(exp, \"losses\", \"val\", default=[])\nmetrics_val = get(exp, \"metrics\", \"val\", default=[])\ny_pred_t = exp.get(\"predictions\", [])\ny_true_t = exp.get(\"ground_truth\", [])\n\nepochs = np.arange(1, len(loss_train) + 1)\n\n# unpack validation metrics list-of-dicts into arrays\nswa_vals, cwa_vals, hwa_vals = [], [], []\nfor d in metrics_val:\n    if isinstance(d, dict):\n        swa_vals.append(d.get(\"SWA\", np.nan))\n        cwa_vals.append(d.get(\"CWA\", np.nan))\n        hwa_vals.append(d.get(\"HWA\", np.nan))\n\n# ----------- Plot 1: loss curves -----------\ntry:\n    plt.figure()\n    if len(loss_train) > 0:\n        plt.plot(epochs, loss_train, label=\"Train Loss\")\n    if len(loss_val) > 0:\n        plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{ds_name}: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------- Plot 2: validation metrics -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        plt.figure()\n        plt.plot(epochs, swa_vals, label=\"SWA\")\n        plt.plot(epochs, cwa_vals, label=\"CWA\")\n        plt.plot(epochs, hwa_vals, label=\"HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{ds_name}: Validation Metrics Over Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric plot: {e}\")\n    plt.close()\n\n# ----------- Plot 3: final test metrics bar chart -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        swa_t, cwa_t, hwa_t = (\n            swa_vals[-1],\n            cwa_vals[-1],\n            hwa_vals[-1],\n        )  # already computed during save\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        values = [swa_t, cwa_t, hwa_t]\n        plt.figure()\n        plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n        plt.ylim(0, 1)\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(f\"{ds_name}: Final Validation Metric Values\")\n        fname = os.path.join(working_dir, f\"{ds_name}_final_metrics_bar.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ----------- Plot 4: confusion matrix -----------\ntry:\n    if y_true_t and y_pred_t:\n        labels = sorted(list(set(y_true_t) | set(y_pred_t)))\n        label_to_idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for yt, yp in zip(y_true_t, y_pred_t):\n            cm[label_to_idx[yt], label_to_idx[yp]] += 1\n        plt.figure(figsize=(5, 4))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name}: Confusion Matrix (Test Set)\")\n        fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ----------- print final metrics -----------\nif swa_vals and cwa_vals and hwa_vals:\n    print(\n        f\"Final Validation Metrics -> SWA: {swa_vals[-1]:.4f}, CWA: {cwa_vals[-1]:.4f}, HWA: {hwa_vals[-1]:.4f}\"\n    )\nif y_true_t and y_pred_t:\n    # compute test metrics quickly\n    def c_variety(seq):\n        return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n    def s_variety(seq):\n        return len(set(tok[0] for tok in seq.split()))\n\n    sequences = y_true_t  # placeholder since sequences not stored for test; metrics already saved elsewhere\n    print(f\"Test predictions available: {len(y_pred_t)} samples\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- paths -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------- load data -----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch safely\ndef get(data, *keys, default=None):\n    for k in keys:\n        data = data.get(k, {})\n    return data if data != {} else default\n\n\nds_name = \"SPR_BENCH\"\nexp = experiment_data.get(ds_name, {})\n\nloss_train = get(exp, \"losses\", \"train\", default=[])\nloss_val = get(exp, \"losses\", \"val\", default=[])\nmetrics_val = get(exp, \"metrics\", \"val\", default=[])\ny_pred_t = exp.get(\"predictions\", [])\ny_true_t = exp.get(\"ground_truth\", [])\n\nepochs = np.arange(1, len(loss_train) + 1)\n\n# unpack validation metrics list-of-dicts into arrays\nswa_vals, cwa_vals, hwa_vals = [], [], []\nfor d in metrics_val:\n    if isinstance(d, dict):\n        swa_vals.append(d.get(\"SWA\", np.nan))\n        cwa_vals.append(d.get(\"CWA\", np.nan))\n        hwa_vals.append(d.get(\"HWA\", np.nan))\n\n# ----------- Plot 1: loss curves -----------\ntry:\n    plt.figure()\n    if len(loss_train) > 0:\n        plt.plot(epochs, loss_train, label=\"Train Loss\")\n    if len(loss_val) > 0:\n        plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{ds_name}: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------- Plot 2: validation metrics -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        plt.figure()\n        plt.plot(epochs, swa_vals, label=\"SWA\")\n        plt.plot(epochs, cwa_vals, label=\"CWA\")\n        plt.plot(epochs, hwa_vals, label=\"HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{ds_name}: Validation Metrics Over Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric plot: {e}\")\n    plt.close()\n\n# ----------- Plot 3: final test metrics bar chart -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        swa_t, cwa_t, hwa_t = (\n            swa_vals[-1],\n            cwa_vals[-1],\n            hwa_vals[-1],\n        )  # already computed during save\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        values = [swa_t, cwa_t, hwa_t]\n        plt.figure()\n        plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n        plt.ylim(0, 1)\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(f\"{ds_name}: Final Validation Metric Values\")\n        fname = os.path.join(working_dir, f\"{ds_name}_final_metrics_bar.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ----------- Plot 4: confusion matrix -----------\ntry:\n    if y_true_t and y_pred_t:\n        labels = sorted(list(set(y_true_t) | set(y_pred_t)))\n        label_to_idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for yt, yp in zip(y_true_t, y_pred_t):\n            cm[label_to_idx[yt], label_to_idx[yp]] += 1\n        plt.figure(figsize=(5, 4))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name}: Confusion Matrix (Test Set)\")\n        fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ----------- print final metrics -----------\nif swa_vals and cwa_vals and hwa_vals:\n    print(\n        f\"Final Validation Metrics -> SWA: {swa_vals[-1]:.4f}, CWA: {cwa_vals[-1]:.4f}, HWA: {hwa_vals[-1]:.4f}\"\n    )\nif y_true_t and y_pred_t:\n    # compute test metrics quickly\n    def c_variety(seq):\n        return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n    def s_variety(seq):\n        return len(set(tok[0] for tok in seq.split()))\n\n    sequences = y_true_t  # placeholder since sequences not stored for test; metrics already saved elsewhere\n    print(f\"Test predictions available: {len(y_pred_t)} samples\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- paths -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------- load data -----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch safely\ndef get(data, *keys, default=None):\n    for k in keys:\n        data = data.get(k, {})\n    return data if data != {} else default\n\n\nds_name = \"SPR_BENCH\"\nexp = experiment_data.get(ds_name, {})\n\nloss_train = get(exp, \"losses\", \"train\", default=[])\nloss_val = get(exp, \"losses\", \"val\", default=[])\nmetrics_val = get(exp, \"metrics\", \"val\", default=[])\ny_pred_t = exp.get(\"predictions\", [])\ny_true_t = exp.get(\"ground_truth\", [])\n\nepochs = np.arange(1, len(loss_train) + 1)\n\n# unpack validation metrics list-of-dicts into arrays\nswa_vals, cwa_vals, hwa_vals = [], [], []\nfor d in metrics_val:\n    if isinstance(d, dict):\n        swa_vals.append(d.get(\"SWA\", np.nan))\n        cwa_vals.append(d.get(\"CWA\", np.nan))\n        hwa_vals.append(d.get(\"HWA\", np.nan))\n\n# ----------- Plot 1: loss curves -----------\ntry:\n    plt.figure()\n    if len(loss_train) > 0:\n        plt.plot(epochs, loss_train, label=\"Train Loss\")\n    if len(loss_val) > 0:\n        plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{ds_name}: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------- Plot 2: validation metrics -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        plt.figure()\n        plt.plot(epochs, swa_vals, label=\"SWA\")\n        plt.plot(epochs, cwa_vals, label=\"CWA\")\n        plt.plot(epochs, hwa_vals, label=\"HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{ds_name}: Validation Metrics Over Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric plot: {e}\")\n    plt.close()\n\n# ----------- Plot 3: final test metrics bar chart -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        swa_t, cwa_t, hwa_t = (\n            swa_vals[-1],\n            cwa_vals[-1],\n            hwa_vals[-1],\n        )  # already computed during save\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        values = [swa_t, cwa_t, hwa_t]\n        plt.figure()\n        plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n        plt.ylim(0, 1)\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(f\"{ds_name}: Final Validation Metric Values\")\n        fname = os.path.join(working_dir, f\"{ds_name}_final_metrics_bar.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ----------- Plot 4: confusion matrix -----------\ntry:\n    if y_true_t and y_pred_t:\n        labels = sorted(list(set(y_true_t) | set(y_pred_t)))\n        label_to_idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for yt, yp in zip(y_true_t, y_pred_t):\n            cm[label_to_idx[yt], label_to_idx[yp]] += 1\n        plt.figure(figsize=(5, 4))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name}: Confusion Matrix (Test Set)\")\n        fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ----------- print final metrics -----------\nif swa_vals and cwa_vals and hwa_vals:\n    print(\n        f\"Final Validation Metrics -> SWA: {swa_vals[-1]:.4f}, CWA: {cwa_vals[-1]:.4f}, HWA: {hwa_vals[-1]:.4f}\"\n    )\nif y_true_t and y_pred_t:\n    # compute test metrics quickly\n    def c_variety(seq):\n        return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n    def s_variety(seq):\n        return len(set(tok[0] for tok in seq.split()))\n\n    sequences = y_true_t  # placeholder since sequences not stored for test; metrics already saved elsewhere\n    print(f\"Test predictions available: {len(y_pred_t)} samples\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- paths -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------- load data -----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch safely\ndef get(data, *keys, default=None):\n    for k in keys:\n        data = data.get(k, {})\n    return data if data != {} else default\n\n\nds_name = \"SPR_BENCH\"\nexp = experiment_data.get(ds_name, {})\n\nloss_train = get(exp, \"losses\", \"train\", default=[])\nloss_val = get(exp, \"losses\", \"val\", default=[])\nmetrics_val = get(exp, \"metrics\", \"val\", default=[])\ny_pred_t = exp.get(\"predictions\", [])\ny_true_t = exp.get(\"ground_truth\", [])\n\nepochs = np.arange(1, len(loss_train) + 1)\n\n# unpack validation metrics list-of-dicts into arrays\nswa_vals, cwa_vals, hwa_vals = [], [], []\nfor d in metrics_val:\n    if isinstance(d, dict):\n        swa_vals.append(d.get(\"SWA\", np.nan))\n        cwa_vals.append(d.get(\"CWA\", np.nan))\n        hwa_vals.append(d.get(\"HWA\", np.nan))\n\n# ----------- Plot 1: loss curves -----------\ntry:\n    plt.figure()\n    if len(loss_train) > 0:\n        plt.plot(epochs, loss_train, label=\"Train Loss\")\n    if len(loss_val) > 0:\n        plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{ds_name}: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------- Plot 2: validation metrics -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        plt.figure()\n        plt.plot(epochs, swa_vals, label=\"SWA\")\n        plt.plot(epochs, cwa_vals, label=\"CWA\")\n        plt.plot(epochs, hwa_vals, label=\"HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{ds_name}: Validation Metrics Over Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric plot: {e}\")\n    plt.close()\n\n# ----------- Plot 3: final test metrics bar chart -----------\ntry:\n    if swa_vals and cwa_vals and hwa_vals:\n        swa_t, cwa_t, hwa_t = (\n            swa_vals[-1],\n            cwa_vals[-1],\n            hwa_vals[-1],\n        )  # already computed during save\n        metrics = [\"SWA\", \"CWA\", \"HWA\"]\n        values = [swa_t, cwa_t, hwa_t]\n        plt.figure()\n        plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n        plt.ylim(0, 1)\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(f\"{ds_name}: Final Validation Metric Values\")\n        fname = os.path.join(working_dir, f\"{ds_name}_final_metrics_bar.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ----------- Plot 4: confusion matrix -----------\ntry:\n    if y_true_t and y_pred_t:\n        labels = sorted(list(set(y_true_t) | set(y_pred_t)))\n        label_to_idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for yt, yp in zip(y_true_t, y_pred_t):\n            cm[label_to_idx[yt], label_to_idx[yp]] += 1\n        plt.figure(figsize=(5, 4))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name}: Confusion Matrix (Test Set)\")\n        fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ----------- print final metrics -----------\nif swa_vals and cwa_vals and hwa_vals:\n    print(\n        f\"Final Validation Metrics -> SWA: {swa_vals[-1]:.4f}, CWA: {cwa_vals[-1]:.4f}, HWA: {hwa_vals[-1]:.4f}\"\n    )\nif y_true_t and y_pred_t:\n    # compute test metrics quickly\n    def c_variety(seq):\n        return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n    def s_variety(seq):\n        return len(set(tok[0] for tok in seq.split()))\n\n    sequences = y_true_t  # placeholder since sequences not stored for test; metrics already saved elsewhere\n    print(f\"Test predictions available: {len(y_pred_t)} samples\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load multiple experiment files ----------\nexperiment_data_path_list = [\n    \"experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_48fe5d59d19b4eec8de94f1da6aff3ea_proc_456840/experiment_data.npy\",\n    \"experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_d16d35eac9af417b87d9385a75f5ace7_proc_456839/experiment_data.npy\",\n    \"experiments/2025-07-29_02-18-25_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_6352f3a1515f43c88021df6a6828adf5_proc_456837/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        exp = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p), allow_pickle=True\n        ).item()\n        all_experiment_data.append(exp)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\n\n# helper\ndef get(data, *keys, default=None):\n    for k in keys:\n        data = data.get(k, {})\n    return data if data != {} else default\n\n\nds_name = \"SPR_BENCH\"\nruns = [exp.get(ds_name, {}) for exp in all_experiment_data if ds_name in exp]\n\nif not runs:\n    print(f\"No runs found for dataset {ds_name}\")\n    exit()\n\n\n# ---------- aggregate per-epoch arrays ----------\ndef stack_with_nan(list_of_lists):\n    max_len = max(len(lst) for lst in list_of_lists)\n    out = []\n    for lst in list_of_lists:\n        padded = np.array(lst + [np.nan] * (max_len - len(lst)), dtype=float)\n        out.append(padded)\n    return np.vstack(out)\n\n\nloss_train_mat = stack_with_nan([get(r, \"losses\", \"train\", default=[]) for r in runs])\nloss_val_mat = stack_with_nan([get(r, \"losses\", \"val\", default=[]) for r in runs])\n\nmetrics_keys = [\"SWA\", \"CWA\", \"HWA\"]\nmetric_mats = {\n    k: stack_with_nan(\n        [[d.get(k, np.nan) for d in get(r, \"metrics\", \"val\", default=[])] for r in runs]\n    )\n    for k in metrics_keys\n}\n\nepochs = np.arange(1, loss_train_mat.shape[1] + 1)\n\n\n# convenience for mean & sem\ndef mean_sem(mat):\n    mean = np.nanmean(mat, axis=0)\n    sem = np.nanstd(mat, axis=0) / np.sqrt(np.sum(~np.isnan(mat), axis=0))\n    return mean, sem\n\n\n# ---------- Plot 1: aggregated loss curves ----------\ntry:\n    plt.figure()\n    for mat, lbl, col in [\n        (loss_train_mat, \"Train Loss\", \"tab:blue\"),\n        (loss_val_mat, \"Val Loss\", \"tab:orange\"),\n    ]:\n        mean, sem = mean_sem(mat)\n        plt.plot(epochs, mean, label=lbl, color=col)\n        plt.fill_between(epochs, mean - sem, mean + sem, color=col, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{ds_name}: Mean \u00b1 SEM Loss over {len(runs)} runs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"{ds_name}_agg_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: aggregated validation metrics ----------\ntry:\n    plt.figure()\n    colors = {\"SWA\": \"tab:green\", \"CWA\": \"tab:red\", \"HWA\": \"tab:pink\"}\n    for k in metrics_keys:\n        mean, sem = mean_sem(metric_mats[k])\n        plt.plot(epochs, mean, label=f\"{k} mean\", color=colors[k])\n        plt.fill_between(epochs, mean - sem, mean + sem, color=colors[k], alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric value\")\n    plt.title(f\"{ds_name}: Validation metrics (mean \u00b1 SEM, {len(runs)} runs)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"{ds_name}_agg_val_metrics.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated metric plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: final epoch bar chart with error bars ----------\ntry:\n    final_vals = {k: metric_mats[k][:, -1] for k in metrics_keys}\n    means = [np.nanmean(final_vals[k]) for k in metrics_keys]\n    sems = [\n        np.nanstd(final_vals[k]) / np.sqrt(np.sum(~np.isnan(final_vals[k])))\n        for k in metrics_keys\n    ]\n    plt.figure()\n    x = np.arange(len(metrics_keys))\n    plt.bar(x, means, yerr=sems, capsize=5, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n    plt.xticks(x, metrics_keys)\n    plt.ylim(0, 1)\n    for i, (m, s) in enumerate(zip(means, sems)):\n        plt.text(i, m + 0.02, f\"{m:.2f}\u00b1{s:.2f}\", ha=\"center\")\n    plt.title(f\"{ds_name}: Final validation metrics (mean \u00b1 SEM, {len(runs)} runs)\")\n    plt.savefig(os.path.join(working_dir, f\"{ds_name}_agg_final_metrics_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final metric bar plot: {e}\")\n    plt.close()\n\n# ---------- Plot 4: confusion matrix of first run (illustrative) ----------\ntry:\n    y_true_t = runs[0].get(\"ground_truth\", [])\n    y_pred_t = runs[0].get(\"predictions\", [])\n    if y_true_t and y_pred_t:\n        labels = sorted(list(set(y_true_t) | set(y_pred_t)))\n        idx = {l: i for i, l in enumerate(labels)}\n        cm = np.zeros((len(labels), len(labels)), int)\n        for yt, yp in zip(y_true_t, y_pred_t):\n            cm[idx[yt], idx[yp]] += 1\n        plt.figure(figsize=(5, 4))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(labels)), labels, rotation=90)\n        plt.yticks(range(len(labels)), labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name}: Confusion Matrix (example run)\")\n        plt.savefig(\n            os.path.join(working_dir, f\"{ds_name}_confusion_matrix_example.png\"),\n            bbox_inches=\"tight\",\n        )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print final averaged metrics ----------\nprint(f\"{ds_name} final epoch mean metrics across {len(runs)} runs:\")\nfor k, m, s in zip(metrics_keys, means, sems):\n    print(f\"  {k}: {m:.4f} \u00b1 {s:.4f}\")\n"], "plot_plan": [null, "We first load the saved experiment_data.npy dictionary, extract the losses recorded per epoch, the validation metrics (SWA, CWA, HWA), and the final test-set predictions/ground-truth. We then create up to four standard plots inside separate try-except blocks: (1) a line plot of training vs. validation loss over epochs, (2) a line plot of the three validation metrics over epochs, (3) a bar chart comparing the final test-set SWA, CWA and HWA values, and (4) a confusion-matrix heat-map built from the test predictions. Each plot is titled, saved to working_dir with descriptive filenames that include the dataset name, then closed. Figures are limited to exactly those four to respect the \u201c\u2264 5\u201d rule. All code uses only the data present in experiment_data.npy and basic matplotlib; no synthetic data are generated. Finally, the script prints the final test metrics so they appear in the notebook / console output.", null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["", "The script will locate the working directory, load the serialized dictionary\nfrom experiment_data.npy, and iterate over every dataset stored inside.   For\neach dataset, it will pull out the training-loss history, validation-loss\nhistory, and the list of per-epoch validation metrics.   It will then compute\n(i) the final training loss, (ii) the final validation loss, and (iii) the best\nvalidation SWA, CWA, and HWA obtained across epochs.   Finally, it prints the\ndataset name followed by each metric\u2019s descriptive label and its value, ensuring\nthe output is clean and unambiguous.", "", "", "The script will locate the working directory, load the serialized dictionary\nfrom experiment_data.npy, and iterate over every dataset stored inside.   For\neach dataset, it will pull out the training-loss history, validation-loss\nhistory, and the list of per-epoch validation metrics.   It will then compute\n(i) the final training loss, (ii) the final validation loss, and (iii) the best\nvalidation SWA, CWA, and HWA obtained across epochs.   Finally, it prints the\ndataset name followed by each metric\u2019s descriptive label and its value, ensuring\nthe output is clean and unambiguous.", "The script will locate the working directory, load the serialized dictionary\nfrom experiment_data.npy, and iterate over every dataset stored inside.   For\neach dataset, it will pull out the training-loss history, validation-loss\nhistory, and the list of per-epoch validation metrics.   It will then compute\n(i) the final training loss, (ii) the final validation loss, and (iii) the best\nvalidation SWA, CWA, and HWA obtained across epochs.   Finally, it prints the\ndataset name followed by each metric\u2019s descriptive label and its value, ensuring\nthe output is clean and unambiguous.", "The script will locate the working directory, load the serialized dictionary\nfrom experiment_data.npy, and iterate over every dataset stored inside.   For\neach dataset, it will pull out the training-loss history, validation-loss\nhistory, and the list of per-epoch validation metrics.   It will then compute\n(i) the final training loss, (ii) the final validation loss, and (iii) the best\nvalidation SWA, CWA, and HWA obtained across epochs.   Finally, it prints the\ndataset name followed by each metric\u2019s descriptive label and its value, ensuring\nthe output is clean and unambiguous.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\n\n# ----------- locate and load experiment data -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------- utility to print nicely -----------\ndef print_metric(label: str, value: float):\n    print(f\"{label}: {value:.4f}\")\n\n\n# ----------- analysis and output -----------\nfor dataset_name, ds_data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- losses ---\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print_metric(\"training loss (final epoch)\", train_losses[-1])\n    if val_losses:\n        print_metric(\"validation loss (final epoch)\", val_losses[-1])\n\n    # --- validation metrics across epochs ---\n    val_metrics_epochs = ds_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Filter out any epochs that may have placeholder None values\n    val_metrics_epochs = [m for m in val_metrics_epochs if isinstance(m, dict)]\n\n    if val_metrics_epochs:\n        # Collect lists for each metric\n        swa_list = [m.get(\"SWA\", 0.0) for m in val_metrics_epochs]\n        cwa_list = [m.get(\"CWA\", 0.0) for m in val_metrics_epochs]\n        hwa_list = [m.get(\"HWA\", 0.0) for m in val_metrics_epochs]\n\n        print_metric(\"shape-weighted accuracy (best validation)\", max(swa_list))\n        print_metric(\"color-weighted accuracy (best validation)\", max(cwa_list))\n        print_metric(\"harmonic-weighted accuracy (best validation)\", max(hwa_list))\n", "", "", "import os\nimport numpy as np\n\n# ----------- locate and load experiment data -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------- utility to print nicely -----------\ndef print_metric(label: str, value: float):\n    print(f\"{label}: {value:.4f}\")\n\n\n# ----------- analysis and output -----------\nfor dataset_name, ds_data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- losses ---\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print_metric(\"training loss (final epoch)\", train_losses[-1])\n    if val_losses:\n        print_metric(\"validation loss (final epoch)\", val_losses[-1])\n\n    # --- validation metrics across epochs ---\n    val_metrics_epochs = ds_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Filter out any epochs that may have placeholder None values\n    val_metrics_epochs = [m for m in val_metrics_epochs if isinstance(m, dict)]\n\n    if val_metrics_epochs:\n        # Collect lists for each metric\n        swa_list = [m.get(\"SWA\", 0.0) for m in val_metrics_epochs]\n        cwa_list = [m.get(\"CWA\", 0.0) for m in val_metrics_epochs]\n        hwa_list = [m.get(\"HWA\", 0.0) for m in val_metrics_epochs]\n\n        print_metric(\"shape-weighted accuracy (best validation)\", max(swa_list))\n        print_metric(\"color-weighted accuracy (best validation)\", max(cwa_list))\n        print_metric(\"harmonic-weighted accuracy (best validation)\", max(hwa_list))\n", "import os\nimport numpy as np\n\n# ----------- locate and load experiment data -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------- utility to print nicely -----------\ndef print_metric(label: str, value: float):\n    print(f\"{label}: {value:.4f}\")\n\n\n# ----------- analysis and output -----------\nfor dataset_name, ds_data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- losses ---\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print_metric(\"training loss (final epoch)\", train_losses[-1])\n    if val_losses:\n        print_metric(\"validation loss (final epoch)\", val_losses[-1])\n\n    # --- validation metrics across epochs ---\n    val_metrics_epochs = ds_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Filter out any epochs that may have placeholder None values\n    val_metrics_epochs = [m for m in val_metrics_epochs if isinstance(m, dict)]\n\n    if val_metrics_epochs:\n        # Collect lists for each metric\n        swa_list = [m.get(\"SWA\", 0.0) for m in val_metrics_epochs]\n        cwa_list = [m.get(\"CWA\", 0.0) for m in val_metrics_epochs]\n        hwa_list = [m.get(\"HWA\", 0.0) for m in val_metrics_epochs]\n\n        print_metric(\"shape-weighted accuracy (best validation)\", max(swa_list))\n        print_metric(\"color-weighted accuracy (best validation)\", max(cwa_list))\n        print_metric(\"harmonic-weighted accuracy (best validation)\", max(hwa_list))\n", "import os\nimport numpy as np\n\n# ----------- locate and load experiment data -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------- utility to print nicely -----------\ndef print_metric(label: str, value: float):\n    print(f\"{label}: {value:.4f}\")\n\n\n# ----------- analysis and output -----------\nfor dataset_name, ds_data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- losses ---\n    train_losses = ds_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        print_metric(\"training loss (final epoch)\", train_losses[-1])\n    if val_losses:\n        print_metric(\"validation loss (final epoch)\", val_losses[-1])\n\n    # --- validation metrics across epochs ---\n    val_metrics_epochs = ds_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Filter out any epochs that may have placeholder None values\n    val_metrics_epochs = [m for m in val_metrics_epochs if isinstance(m, dict)]\n\n    if val_metrics_epochs:\n        # Collect lists for each metric\n        swa_list = [m.get(\"SWA\", 0.0) for m in val_metrics_epochs]\n        cwa_list = [m.get(\"CWA\", 0.0) for m in val_metrics_epochs]\n        hwa_list = [m.get(\"HWA\", 0.0) for m in val_metrics_epochs]\n\n        print_metric(\"shape-weighted accuracy (best validation)\", max(swa_list))\n        print_metric(\"color-weighted accuracy (best validation)\", max(cwa_list))\n        print_metric(\"harmonic-weighted accuracy (best validation)\", max(hwa_list))\n", ""], "parse_term_out": ["", "['SPR_BENCH', '\\n', 'training loss (final epoch): 0.5199', '\\n', 'validation\nloss (final epoch): 0.5210', '\\n', 'shape-weighted accuracy (best validation):\n0.7481', '\\n', 'color-weighted accuracy (best validation): 0.7455', '\\n',\n'harmonic-weighted accuracy (best validation): 0.7468', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "", "", "['SPR_BENCH', '\\n', 'training loss (final epoch): 0.5199', '\\n', 'validation\nloss (final epoch): 0.5209', '\\n', 'shape-weighted accuracy (best validation):\n0.7500', '\\n', 'color-weighted accuracy (best validation): 0.7441', '\\n',\n'harmonic-weighted accuracy (best validation): 0.7471', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training loss (final epoch): 0.5198', '\\n', 'validation\nloss (final epoch): 0.5220', '\\n', 'shape-weighted accuracy (best validation):\n0.7537', '\\n', 'color-weighted accuracy (best validation): 0.7503', '\\n',\n'harmonic-weighted accuracy (best validation): 0.7520', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training loss (final epoch): 0.5197', '\\n', 'validation\nloss (final epoch): 0.5214', '\\n', 'shape-weighted accuracy (best validation):\n0.7474', '\\n', 'color-weighted accuracy (best validation): 0.7420', '\\n',\n'harmonic-weighted accuracy (best validation): 0.7447', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
