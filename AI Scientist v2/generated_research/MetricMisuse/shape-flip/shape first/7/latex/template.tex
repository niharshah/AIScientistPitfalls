\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{url}

\graphicspath{{figures/}}

\begin{filecontents}{references.bib}
@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@article{smith2019superconvergence,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}

@inproceedings{zhang2019cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Zhang, Yabin and Odena, Augustus and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{goodfellow2015adversarial,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}
\end{filecontents}

\title{Unexpected Pitfalls in Deep Learning: A Cautionary Case Study}
\date{}

\begin{document}
\maketitle

\begin{abstract}
In this paper, we highlight a set of real-world pitfalls encountered in training deep learning models for an industrial use case. Despite initial optimism, our findings reveal unexpected generalization failures and challenges with offline metrics. We discuss why these issues arise in practice and how they can threaten deployment readiness.
\end{abstract}

\section{Introduction}
Deep learning models have achieved remarkable successes in various domains \cite{kingma2014adam,smith2019superconvergence,zhang2019cyclical}. However, when translated to real-world scenarios, numerous subtle pitfalls can arise, compromising system performance. For instance, even small differences in data distribution can drastically impact generalization. We present a cautionary case study that showcases unexpected failures and partial improvements in a production setting, shedding light on the complexity of training pipelines and their deployment.

\section{Related Work}
Real-world deployments of deep neural networks have often uncovered pitfalls not apparent in controlled research settings. \cite{goodfellow2015adversarial} first outlined how small, imperceptible perturbations could break classification performance, demonstrating the fragility of deep architectures. Different approaches \cite{kingma2014adam,zhang2019cyclical} attempt to improve training stability or reduce sensitivity to hyperparameters. However, these solutions do not fully address inconsistencies that can surface when models are deployed at scale.

\section{Method}
We implemented a sequence of experimental models, starting from a baseline architecture commonly used in image classification. Then, we introduced domain-specific adaptations intended to improve performance in a production environment. Despite thorough hyperparameter tuning, inconsistencies persisted, as we detail in subsequent sections.

\section{Experiments}
We conducted experiments on a proprietary dataset collected under real operating conditions. Our results highlight multiple issues:
• Models trained with standard techniques yielded encouraging offline performance but often collapsed under slight distribution shifts in the live environment.
• More intricate training curricula improved intermediate validation metrics but failed to generalize on a larger batch of real data.

Table~\ref{tab:main-results} summarizes these outcomes. Some partial improvements were observed, yet we encountered unexpected degradations in key scenarios.

\begin{table}[ht]
\centering
\caption{Off/On distribution performance. Interestingly, partial gains offline do not always translate to live improvements.}
\label{tab:main-results}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model Variant} & \textbf{Offline Acc. (\%)} & \textbf{Live Acc. (\%)} \\
\midrule
Baseline               & 92.1                       & 78.4                      \\
Domain-Adapted        & 93.5                       & 77.9                      \\
Curriculum Trained    & 94.2                       & 79.0                      \\
\bottomrule
\end{tabular}
\end{table}

Although some of these drops seem modest, these dips can be fatal in practical deployment. Furthermore, metrics regarding user satisfaction and reliability also revealed negative trends, underscoring the complexity of adopting deep models at scale.

\section{Conclusion}
Our study points out that seemingly small distribution shifts and domain-specific constraints can produce significant performance gaps between offline and live contexts. Despite attempts at domain adaptation and advanced training schedules, results remained inconclusive or partially negative. We hope that sharing these observations will help practitioners remain alert to hidden pitfalls and prompt the community to investigate more robust approaches.

\bibliographystyle{plain}
\bibliography{references}

\appendix
\section{Supplementary Material}
Additional experimental details and extended plots are provided here.
\end{document}