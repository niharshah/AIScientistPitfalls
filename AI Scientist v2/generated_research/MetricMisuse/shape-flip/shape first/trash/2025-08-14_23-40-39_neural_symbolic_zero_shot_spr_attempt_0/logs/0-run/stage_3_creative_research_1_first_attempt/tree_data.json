{"edges": [[0, 3], [0, 1], [0, 2], [0, 4], [2, 8], [2, 9], [2, 11], [2, 6], [2, 7], [2, 12], [2, 10], [4, 5]], "layout": [[0.5714285714285714, 0.0], [0.2857142857142857, 0.5], [0.42857142857142855, 0.5], [0.5714285714285714, 0.5], [1.0, 0.5], [1.0, 1.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0]], "plan": ["Hyperparam tuning name: EPOCHS. The updated solution simply trains the baseline\nnetwork longer (20 epochs) while introducing a small early-stopping routine that\nhalts when the best validation BPS hasn\u2019t improved for three consecutive epochs.\nNo other hyper-parameters or model architecture are changed, so any improvement\nis purely due to letting the optimiser converge. All metrics, losses and\npredictions are stored in the required experiment_data structure and saved to\nexperiment_data.npy.", "We extend the baseline with a lightweight neural-symbolic classifier.  Each\ntoken is embedded and processed by a two-layer Transformer encoder so that the\nnetwork can discover compositional patterns.  In parallel, two symbolic\nfeatures\u2014the numbers of distinct shapes and colors in the sequence\u2014are extracted\non-the-fly.  The CLS representation from the Transformer is concatenated with\nthese symbolic counts (normalized) and fed to an MLP for prediction, enabling\nexplicit rule cues to influence the neural decision.  Training uses Adam, early\nstopping on the validation Shape-Weighted Accuracy (SWA), and the best\ncheckpoint is evaluated on dev and test, which we regard as zero-shot splits.\nAll metrics and losses are logged into the required experiment_data structure\nand saved.  This simple hybrid already gives the model explicit awareness of\nrule complexity while retaining expressive sequence modelling capacity.", "We enhance the baseline with a lightweight neural-symbolic hybrid: a small\nTransformer encoder produces a dense embedding of each SPR sequence, while an\nexplicit symbolic feature vector (sequence length, shape-variety, colour-\nvariety) is extracted deterministically.  The symbolic vector is projected and\nconcatenated to the Transformer embedding, allowing the classifier to ground\nneural predictions in rule-oriented statistics and therefore generalise zero-\nshot to unseen shape rules.  We train with cross-entropy, early-stopping on dev\nShape-Weighted Accuracy (SWA), and log all metrics.  Only SWA is used for model\nselection and final reporting.  All computation follows the mandatory GPU/CPU\nhandling instructions, stores artefacts in ./working, and finishes within the\ntime limits.", "To inject symbolic reasoning signals without heavy engineering, we enrich the\nsimple neural baseline with explicit symbolic features that any rule-reasoning\nalgorithm would exploit: per-sequence counts of (i) distinct shapes and (ii)\ndistinct colours.  A token is decomposed into its shape-char (index 0) and\ncolour-char (index 1), the network learns regular token embeddings exactly as\nbefore, then concatenates the averaged token embedding with the two symbolic\nscalars.  This tiny neural-symbolic fusion keeps training fast while giving the\nclassifier direct access to rule-critical statistics, enabling much stronger\nzero-shot generalisation.  We monitor Shape-Weighted Accuracy (SWA) only, apply\nearly stopping on the dev SWA, and persist all metrics for later analysis.  The\ncode below is self-contained, follows the required GPU/CPU handling, and prints\ntrain/validation loss plus SWA each epoch.", "We enrich the baseline by integrating lightweight symbolic features (shape\nvariety, color variety, sequence length, first/last shape) with a neural\nencoder.   Each token is embedded and processed by a 1-layer Transformer\nencoder; the [CLS]\u2010style mean-pooling vector is concatenated with the symbolic\nvector and passed to an MLP classifier.   This hybrid design lets the symbolic\nbranch capture rule-level regularities while the neural branch captures\ncontextual patterns, boosting zero-shot generalisation.   Training uses Adam,\nearly stopping on validation Shape-Weighted Accuracy (SWA), and all\nmetrics/losses are recorded to experiment_data.npy.   We keep the implementation\ncompact (\u2248300 lines) and GPU-friendly; it should finish in <30 min on a single\nGPU or CPU fallback.   Only SWA is reported for comparisons, complying with the\n\u201cchoose one metric\u201d requirement, while losses are also tracked.   The code auto-\nlocates SPR_BENCH (or uses $SPR_PATH), builds vocabulary, trains, evaluates,\nprints epoch-wise logs and saves numpy artefacts.   No explicit entry-point\nguard is used so the script executes immediately when run.", "The original implementation under-counted shape types because it blindly used\nthe first character of every token (e.g. token \u201cAB2\u201d was always mapped to shape\n\u201cA\u201d).   We correct this by extracting the leading CAPITAL-letter substring with\na tiny regex helper.   Because these shape counts are used in the symbolic\nfeatures and (more importantly) to weight the final accuracy, fixing them\nremoves a systematic bias that made validation scores look good while depressing\nreal-test performance.   The rest of the script is kept intact, with small\nhygiene tweaks (seed fixing, dropout for robustness) and full compliance with\nthe required GPU/metrics/serialization rules.", "To improve zero-shot generalization we add an auxiliary, self-supervised\nobjective that forces the encoder to infer symbolic statistics (sequence length,\nshape variety, color variety) while it is learning the main rule label.  A\nTransformer encodes the token sequence; its pooled embedding is passed to: (1)\nthe usual classifier head and (2) a small regression head that predicts the\nthree symbolic counts.  The total loss is CE(label) + \u03bb\u00b7MSE(aux), with \u03bb=0.2.\nPredicting those statistics encourages the network to internalise rule-relevant\nstructure and reduces over-fitting.  We keep the explicit symbolic features,\napply dropout, gradient clipping and a cosine scheduler with warm-up.  Early\nstopping on validation SWA is retained.  All metrics and predictions are logged\n& saved.  The chosen metric for comparison remains Shape-Weighted Accuracy.", "To push zero-shot reasoning further, we inject an auxiliary self-supervised\nobjective that explicitly teaches the network to estimate two symbolic\nproperties of every sequence\u2014the number of distinct shapes and distinct\ncolours\u2014while still predicting the final rule label.  A single Transformer\nencoder still embeds the token sequence, but its pooled representation is now\npassed to three parallel heads: (1) a classifier, (2) a regressor for shape\nvariety, and (3) a regressor for colour variety.  The ground-truth counts are\ntrivially available from every sequence, so no extra annotation is required, and\nthe extra tasks encourage the model to internalise \u201csymbolic\u201d statistics useful\nfor zero-shot generalisation.  We keep the original symbolic features (length,\nshape variety, colour variety) as inputs, fuse them with the neural embedding,\nand regularise with dropout plus weight-decay.  Training minimises CE(label) +\n\u03bb\u00b7MSE(shape) + \u03bb\u00b7MSE(colour) (\u03bb = 0.1).  Early stopping monitors validation\nShape-Weighted Accuracy (SWA).  All metrics, losses and predictions are logged\nand saved, meeting the specified experiment-data protocol.  The entire script is\nself-contained, honours GPU handling rules, and completes in well under 30\nminutes on a single GPU/CPU.", "We extend the neural-symbolic baseline with a lightweight multi-task objective\nthat forces the model to internalise explicit rule signals.   Beside the main\nclassifier, a secondary head learns to regress two symbolic properties \u2013 shape-\nvariety and color-variety \u2013 computed from every sequence.   During training we\nminimise CE(label) + \u03b1\u00b7MSE(sym_pred, sym_target) (\u03b1 = 0.1).   This extra\nsupervision steers latent representations toward rule-aware features, improving\nzero-shot generalisation while adding negligible computation.   We further add\ndropout, weight-decay and gradient clipping to reduce overfitting, and we\nstandardise the symbolic targets so both dimensions are on a similar numerical\nscale.   Validation still uses only the classifier head and Shape-Weighted\nAccuracy (SWA).   The code below loads SPR_BENCH, trains the new model with\nearly stopping, prints per-epoch losses & SWA, saves metrics, and outputs final\nDEV / TEST scores.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, math, pathlib, numpy as np, torch\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict, disable_caching\n\n# ----------------- Paths / working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- Device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ----------------- Disable HF cache -----------------\ndisable_caching()\n\n# ----------------- Experiment data container -----------------\nexperiment_data = {\n    \"epochs_tuning\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\n                \"train_loss\": [],\n                \"val_loss\": [],\n                \"val_swa\": [],\n                \"val_cwa\": [],\n                \"val_bps\": [],\n            },\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n            \"timestamps\": [],\n        }\n    }\n}\n\n# ----------------- Hyper-parameters -----------------\nEMB_DIM = 64\nHIDDEN_DIM = 128\nBATCH_SIZE = 128\nEPOCHS = 20  # extended training\nLR = 1e-3\nPATIENCE = 3  # early-stopping patience (epochs w/o BPS improvement)\nPAD_TOKEN, UNK_TOKEN = \"<pad>\", \"<unk>\"\n\n\n# ----------------- Dataset locating helpers -----------------\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        print(\"[Data] Using SPR_BENCH from SPR_PATH=\", env)\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            print(\"[Data] Found SPR_BENCH at\", p / \"SPR_BENCH\")\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        print(\"[Data] Using fallback path\", fb)\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found; set SPR_PATH.\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# ----------------- Load data & build vocab -----------------\nDATA_PATH = resolve_spr_path()\nspr = load_spr_bench(DATA_PATH)\n\ntrain_sequences = spr[\"train\"][\"sequence\"]\ntoken_counter = Counter(tok for seq in train_sequences for tok in seq.split())\nvocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\nfor tok in token_counter:\n    vocab[tok] = len(vocab)\ninv_vocab = {i: t for t, i in vocab.items()}\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nid2label = {i: l for l, i in label2id.items()}\nNUM_CLASSES = len(label2id)\nprint(f\"Vocab size {len(vocab)} | Classes {NUM_CLASSES}\")\n\n\ndef encode_sequence(seq: str):\n    return [vocab.get(tok, vocab[UNK_TOKEN]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs, self.labels = split[\"sequence\"], split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return dict(\n            input_ids=torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            labels=torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n            seq_str=self.seqs[idx],\n        )\n\n\ndef collate_fn(batch):\n    lens = [len(b[\"input_ids\"]) for b in batch]\n    max_len = max(lens)\n    inputs = torch.full((len(batch), max_len), vocab[PAD_TOKEN], dtype=torch.long)\n    for i, b in enumerate(batch):\n        inputs[i, : lens[i]] = b[\"input_ids\"]\n    return dict(\n        input_ids=inputs,\n        labels=torch.stack([b[\"labels\"] for b in batch]),\n        seq_strs=[b[\"seq_str\"] for b in batch],\n    )\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ----------------- Model -----------------\nclass SPRClassifier(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, out_dim):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.fc1, self.relu = nn.Linear(emb_dim, HIDDEN_DIM), nn.ReLU()\n        self.fc2 = nn.Linear(HIDDEN_DIM, out_dim)\n\n    def forward(self, x):\n        mask = (x != 0).float().unsqueeze(-1)\n        avg = (self.emb(x) * mask).sum(1) / (mask.sum(1).clamp(min=1e-6))\n        return self.fc2(self.relu(self.fc1(avg)))\n\n\nmodel = SPRClassifier(len(vocab), EMB_DIM, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=LR)\n\n\n# ----------------- Evaluation helper -----------------\ndef evaluate(loader):\n    model.eval()\n    tot_loss = 0\n    n = 0\n    preds, labels, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot_loss += loss.item() * bs\n            n += bs\n            p = logits.argmax(1).cpu().tolist()\n            l = batch[\"labels\"].cpu().tolist()\n            preds.extend(p)\n            labels.extend(l)\n            seqs.extend(batch[\"seq_strs\"])\n    loss = tot_loss / max(n, 1)\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    cwa = color_weighted_accuracy(seqs, labels, preds)\n    bps = math.sqrt(swa * cwa) if swa >= 0 and cwa >= 0 else 0.0\n    return loss, swa, cwa, bps, preds, labels\n\n\n# ----------------- Training w/ early stopping -----------------\nbest_bps = -1\npatience_ctr = 0\nbest_state = None\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss = 0\n    seen = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        optim.zero_grad()\n        loss = criterion(model(batch[\"input_ids\"]), batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        run_loss += loss.item() * batch[\"labels\"].size(0)\n        seen += batch[\"labels\"].size(0)\n    train_loss = run_loss / seen\n\n    val_loss, swa, cwa, bps, _, _ = evaluate(dev_loader)\n    print(\n        f\"Epoch {epoch:02d} | train {train_loss:.4f} | val {val_loss:.4f} | SWA {swa:.4f} | CWA {cwa:.4f} | BPS {bps:.4f}\"\n    )\n\n    # log\n    m = experiment_data[\"epochs_tuning\"][\"SPR_BENCH\"][\"metrics\"]\n    m[\"train_loss\"].append(train_loss)\n    m[\"val_loss\"].append(val_loss)\n    m[\"val_swa\"].append(swa)\n    m[\"val_cwa\"].append(cwa)\n    m[\"val_bps\"].append(bps)\n    experiment_data[\"epochs_tuning\"][\"SPR_BENCH\"][\"timestamps\"].append(\n        datetime.utcnow().isoformat()\n    )\n\n    # early stopping check\n    if bps > best_bps:\n        best_bps = bps\n        best_state = model.state_dict()\n        patience_ctr = 0\n    else:\n        patience_ctr += 1\n    if patience_ctr >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n\n# ----------------- Restore best model -----------------\nif best_state is not None:\n    model.load_state_dict(best_state)\n\n# ----------------- Final evaluation -----------------\ndev_loss, dev_swa, dev_cwa, dev_bps, dev_preds, dev_labels = evaluate(dev_loader)\ntest_loss, test_swa, test_cwa, test_bps, test_preds, test_labels = evaluate(test_loader)\n\nprint(\n    f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f} | CWA {dev_cwa:.4f} | BPS {dev_bps:.4f}\"\n)\nprint(\n    f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f} | CWA {test_cwa:.4f} | BPS {test_bps:.4f}\"\n)\n\nd = experiment_data[\"epochs_tuning\"][\"SPR_BENCH\"]\nd[\"predictions\"][\"dev\"] = dev_preds\nd[\"ground_truth\"][\"dev\"] = dev_labels\nd[\"predictions\"][\"test\"] = test_preds\nd[\"ground_truth\"][\"test\"] = test_labels\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, math, numpy as np, torch\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict, disable_caching\n\n# ============== working dir ==========================\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ============== device ===============================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ============== experiment data container ============\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# ============== Utilities ============================\ndisable_caching()\nPAD, UNK, CLS = \"<pad>\", \"<unk>\", \"<cls>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"Could not find SPR_BENCH (set SPR_PATH).\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.cache_dsets\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# ============== Load data & vocab =====================\nDATA_PATH = resolve_spr_path()\nspr = load_spr_bench(DATA_PATH)\n\ntok_counter = Counter(tok for s in spr[\"train\"][\"sequence\"] for tok in s.split())\nvocab = {PAD: 0, UNK: 1, CLS: 2}\nfor tok in tok_counter:\n    vocab[tok] = len(vocab)\nid2tok = {i: t for t, i in vocab.items()}\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nid2label = {i: l for l, i in label2id.items()}\nNUM_CLASSES = len(label2id)\nprint(f\"Vocab {len(vocab)} | Classes {NUM_CLASSES}\")\n\n\ndef encode_sequence(seq: str):\n    return [vocab[CLS]] + [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return dict(\n            input_ids=torch.tensor(encode_sequence(seq), dtype=torch.long),\n            labels=torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n            shape_cnt=torch.tensor([count_shape_variety(seq)], dtype=torch.float),\n            seq_str=seq,\n        )\n\n\ndef collate_fn(batch):\n    maxlen = max(len(b[\"input_ids\"]) for b in batch)\n    ids = torch.full((len(batch), maxlen), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    return dict(\n        input_ids=ids,\n        labels=torch.stack([b[\"labels\"] for b in batch]),\n        shape_cnt=torch.stack([b[\"shape_cnt\"] for b in batch]),\n        seq_strs=[b[\"seq_str\"] for b in batch],\n    )\n\n\nBATCH = 128\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), batch_size=BATCH, shuffle=False, collate_fn=collate_fn\n)\n\n# ============== Model =================================\nEMB = 128\nHID = 256\nNHEAD = 4\nNLAYER = 2\n\n\nclass NeuralSymbolicSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_cls):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = nn.Parameter(torch.randn(512, emb_dim))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=NHEAD, dim_feedforward=HID, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=NLAYER)\n        self.fc = nn.Sequential(\n            nn.Linear(emb_dim + 1, HID), nn.ReLU(), nn.Linear(HID, num_cls)\n        )\n\n    def forward(self, ids, shape_cnt):\n        B, L = ids.size()\n        pos = self.pos[:L]\n        x = self.emb(ids) + pos\n        key_padding = ids.eq(0)\n        h = self.transformer(x, src_key_padding_mask=key_padding)\n        cls = h[:, 0]  # representation of <cls>\n        feat = torch.cat([cls, shape_cnt / 10.0], dim=1)  # normalize count\n        return self.fc(feat)\n\n\nmodel = NeuralSymbolicSPR(len(vocab), EMB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ============== Train / Eval helpers ==================\ndef run_epoch(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    tot_loss, n = 0, 0\n    preds, labels, seqs = [], [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"shape_cnt\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        bs = batch[\"labels\"].size(0)\n        tot_loss += loss.item() * bs\n        n += bs\n        p = logits.argmax(1).cpu().tolist()\n        l = batch[\"labels\"].cpu().tolist()\n        preds.extend(p)\n        labels.extend(l)\n        seqs.extend(batch[\"seq_strs\"])\n    loss = tot_loss / max(n, 1)\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    return loss, swa, preds, labels\n\n\n# ============== Training loop =========================\nEPOCHS = 30\nPATIENCE = 5\nbest_swa = -1\npatience = 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_swa, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_swa, _, _ = run_epoch(dev_loader, train=False)\n    print(\n        f\"Epoch {epoch}: train_loss {tr_loss:.4f} | val_loss {val_loss:.4f} | val_SWA {val_swa:.4f}\"\n    )\n    # log\n    m = experiment_data[\"SPR_BENCH\"][\"metrics\"]\n    m[\"train_loss\"].append(tr_loss)\n    m[\"val_loss\"].append(val_loss)\n    m[\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n    # early stopping\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# restore best\nif best_state is not None:\n    model.load_state_dict(best_state)\n\n# ============== Final evaluation =====================\ndev_loss, dev_swa, dev_preds, dev_labels = run_epoch(dev_loader, False)\ntest_loss, test_swa, test_preds, test_labels = run_epoch(test_loader, False)\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_labels\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_labels\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, math, pathlib, numpy as np, torch, random\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------- working dir --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------- device -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------- experiment container -----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# -------------------------- utils --------------------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _ld(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1.0)\n\n\n# -------------------------- data ---------------------------------\nDATA_PATH = resolve_spr_path()\ndsets = load_spr(DATA_PATH)\n\n# vocab\ncounter = Counter(tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nid2lab = {i: l for l, i in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Vocabulary size: {len(vocab)} | Classes: {num_classes}\")\n\n\ndef encode(seq):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.lab = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                [len(seq.split()), count_shape_variety(seq), count_color_variety(seq)],\n                dtype=torch.float32,\n            ),\n            \"label\": torch.tensor(lab2id[self.lab[idx]], dtype=torch.long),\n            \"seq_str\": seq,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    input_ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        input_ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": input_ids, \"sym_feats\": sym, \"labels\": labels, \"seqs\": seqs}\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# -------------------------- model --------------------------------\nEMB = 128\nSYM_DIM = 3\nSYM_PROJ = 32\nN_HEAD = 4\nN_LAY = 2\nHIDDEN = 256\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # 1 x max_len x d_model\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass NeuralSymbolicSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = PositionalEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=N_HEAD, dim_feedforward=emb_dim * 2, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=N_LAY)\n        self.sym_proj = nn.Sequential(nn.Linear(SYM_DIM, SYM_PROJ), nn.ReLU())\n        self.fc = nn.Linear(emb_dim + SYM_PROJ, num_labels)\n\n    def forward(self, input_ids, sym_feats):\n        mask = input_ids == 0\n        x = self.emb(input_ids)\n        x = self.pos(x)\n        h = self.enc(x, src_key_padding_mask=mask)\n        seq_emb = h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-6\n        ).unsqueeze(-1)\n        sym = self.sym_proj(sym_feats)\n        combined = torch.cat([seq_emb, sym], dim=-1)\n        return self.fc(combined)\n\n\nmodel = NeuralSymbolicSPR(len(vocab), EMB, num_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# -------------------------- helpers ------------------------------\ndef evaluate(loader):\n    model.eval()\n    tot, n = 0.0, 0\n    all_pred, all_lab, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot += loss.item() * bs\n            n += bs\n            preds = logits.argmax(1).cpu().tolist()\n            labs = batch[\"labels\"].cpu().tolist()\n            seqs = batch[\"seqs\"]\n            all_pred.extend(preds)\n            all_lab.extend(labs)\n            all_seq.extend(seqs)\n    swa = shape_weighted_accuracy(all_seq, all_lab, all_pred)\n    return tot / n, swa, all_pred, all_lab\n\n\n# -------------------------- training -----------------------------\nEPOCHS = 20\nPATIENCE = 3\nbest_swa = -1\npatience = 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tloss, seen = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        bs = batch[\"labels\"].size(0)\n        tloss += loss.item() * bs\n        seen += bs\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch:02d}: train_loss={tloss/seen:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(tloss / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n\n# -------------------------- evaluation ---------------------------\nif best_state:\n    model.load_state_dict(best_state)\n\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, math, numpy as np, torch\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict, disable_caching\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment recorder ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# ---------- hyper-params ----------\nEMB_DIM = 128\nHID_DIM = 256\nBATCH = 128\nEPOCHS = 30\nLR = 3e-4\nPATIENCE = 4\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\n# ---------- utils ----------\ndef spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fallback = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fallback / \"train.csv\").exists():\n        return fallback\n    raise FileNotFoundError(\"SPR_BENCH not found; set SPR_PATH\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) else 0.0\n\n\n# -------------- data --------------\ndisable_caching()\nDATA_PATH = spr_path()\nspr = load_spr_bench(DATA_PATH)\ntrain_sequences = spr[\"train\"][\"sequence\"]\ntoken_counter = Counter(tok for seq in train_sequences for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in token_counter:\n    vocab.setdefault(tok, len(vocab))\ninv_vocab = {i: t for t, i in vocab.items()}\n\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlbl2id = {l: i for i, l in enumerate(label_set)}\nid2lbl = {i: l for l, i in lbl2id.items()}\n\n\ndef encode(seq: str):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                [\n                    count_shape_variety(seq),\n                    len(set(tok[1] for tok in seq.split() if len(tok) > 1)),\n                ],\n                dtype=torch.float,\n            ),\n            \"labels\": torch.tensor(lbl2id[self.labels[idx]], dtype=torch.long),\n            \"seq_str\": seq,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    return {\n        \"input_ids\": ids,\n        \"sym_feats\": torch.stack([b[\"sym_feats\"] for b in batch]),\n        \"labels\": torch.stack([b[\"labels\"] for b in batch]),\n        \"seq_strs\": [b[\"seq_str\"] for b in batch],\n    }\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# -------------- model --------------\nclass NeuralSymbolic(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, n_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc1 = nn.Linear(emb_dim + 2, hid_dim)\n        self.act = nn.ReLU()\n        self.fc2 = nn.Linear(hid_dim, n_classes)\n\n    def forward(self, ids, sym):\n        mask = (ids != 0).float().unsqueeze(-1)\n        avg = (self.emb(ids) * mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n        x = torch.cat([avg, sym], dim=1)\n        return self.fc2(self.act(self.fc1(x)))\n\n\nmodel = NeuralSymbolic(len(vocab), EMB_DIM, HID_DIM, len(label_set)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=LR)\n\n\n# -------------- evaluation --------------\ndef evaluate(loader):\n    model.eval()\n    tot_loss, n = 0, 0\n    preds, gts, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot_loss += loss.item() * bs\n            n += bs\n            p = logits.argmax(1).cpu().tolist()\n            preds.extend(p)\n            gts.extend(batch[\"labels\"].cpu().tolist())\n            seqs.extend(batch[\"seq_strs\"])\n    return tot_loss / max(n, 1), shape_weighted_accuracy(seqs, gts, preds), preds, gts\n\n\n# -------------- training --------------\nbest_swa, patience = -1, 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, seen = 0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        optim.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        run_loss += loss.item() * batch[\"labels\"].size(0)\n        seen += batch[\"labels\"].size(0)\n    train_loss = run_loss / seen\n\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | SWA={val_swa:.4f}\"\n    )\n\n    # record\n    exp = experiment_data[\"SPR_BENCH\"][\"metrics\"]\n    exp[\"train_loss\"].append(train_loss)\n    exp[\"val_loss\"].append(val_loss)\n    exp[\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    # early stopping\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# -------------- restore best --------------\nif best_state:\n    model.load_state_dict(best_state)\n\n# -------------- final eval --------------\ndev_loss, dev_swa, dev_pred, dev_gt = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_gt = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\n# save predictions\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_gt\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_gt\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, math, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom datetime import datetime\nfrom datasets import load_dataset, DatasetDict, disable_caching\n\n# ---------- working directory ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data store ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n\n# ---------- helper : locate dataset ----------\ndef locate_spr() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found \u2013 set $SPR_PATH\")\n\n\n# ---------- load dataset ----------\ndisable_caching()\n\n\ndef load_spr(folder: pathlib.Path) -> DatasetDict:\n    def _load(csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(folder / csv),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr_path = locate_spr()\nspr = load_spr(spr_path)\nprint(\"Loaded SPR_BENCH from\", spr_path)\n\n\n# ---------- evaluation metric ----------\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    corr = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(corr) / sum(weights) if sum(weights) > 0 else 0.0\n\n\n# ---------- build vocab ----------\ntrain_seqs = spr[\"train\"][\"sequence\"]\ntok_counter = Counter(tok for seq in train_seqs for tok in seq.split())\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor tok in tok_counter:\n    vocab[tok] = len(vocab)\ninv_vocab = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nprint(f\"Vocab size={len(vocab)} | num classes={len(labels)}\")\n\n\n# ---------- encoding helpers ----------\ndef encode_seq(seq: str):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\ndef symbolic_feats(seq: str):\n    shapes = [tok[0] for tok in seq.split() if tok]\n    colors = [tok[1] if len(tok) > 1 else \"_\" for tok in seq.split()]\n    return np.array(\n        [\n            len(set(shapes)),\n            len(set(colors)),\n            len(shapes),\n            ord(shapes[0]) - 65 if shapes else -1,\n            ord(shapes[-1]) - 65 if shapes else -1,\n        ],\n        dtype=np.float32,\n    )\n\n\nSYM_FEAT_DIM = 5\n\n\n# ---------- dataset class ----------\nclass SPRSet(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym\": torch.tensor(symbolic_feats(self.seqs[idx]), dtype=torch.float32),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n            \"seq_str\": self.seqs[idx],\n        }\n\n\n# ---------- collate ----------\ndef collate(batch):\n    lengths = [len(b[\"input_ids\"]) for b in batch]\n    maxlen = max(lengths)\n    ids = torch.full((len(batch), maxlen), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : lengths[i]] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym\"] for b in batch])\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    seq_strs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": ids, \"sym\": sym, \"labels\": labels, \"seq_strs\": seq_strs}\n\n\n# ---------- dataloaders ----------\nBATCH = 128\ntrain_loader = DataLoader(\n    SPRSet(spr[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRSet(spr[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRSet(spr[\"test\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ---------- model ----------\nclass HybridClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_sz,\n        tok_emb=64,\n        hid=128,\n        nclass=len(labels),\n        n_heads=2,\n        sym_dim=SYM_FEAT_DIM,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, tok_emb, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=tok_emb,\n            nhead=n_heads,\n            dim_feedforward=tok_emb * 2,\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, num_layers=1)\n        self.sym_proj = nn.Linear(sym_dim, hid)\n        self.cls = nn.Sequential(\n            nn.Linear(tok_emb + hid, hid), nn.ReLU(), nn.Linear(hid, nclass)\n        )\n\n    def forward(self, ids, sym):\n        mask = ids == 0\n        x = self.emb(ids)\n        x = self.enc(x, src_key_padding_mask=mask)\n        pooled = x.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(\n            1, keepdim=True\n        ).clamp(min=1e-6)\n        sym_h = torch.relu(self.sym_proj(sym))\n        cat = torch.cat([pooled, sym_h], dim=-1)\n        return self.cls(cat)\n\n\nmodel = HybridClassifier(len(vocab)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- training helpers ----------\ndef move_batch(batch):\n    return {k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()}\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, n = 0, 0\n    preds, gold, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = move_batch(batch)\n            logits = model(batch[\"input_ids\"], batch[\"sym\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot_loss += loss.item() * bs\n            n += bs\n            p = logits.argmax(1).cpu().tolist()\n            g = batch[\"labels\"].cpu().tolist()\n            preds.extend(p)\n            gold.extend(g)\n            seqs.extend(batch[\"seq_strs\"])\n    loss = tot_loss / max(n, 1)\n    swa = shape_weighted_accuracy(seqs, gold, preds)\n    return loss, swa, preds, gold\n\n\n# ---------- training loop ----------\nEPOCHS = 20\nPATIENCE = 3\nbest_swa, patience = -1, 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, seen = 0, 0\n    for batch in train_loader:\n        batch = move_batch(batch)\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"sym\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * batch[\"labels\"].size(0)\n        seen += batch[\"labels\"].size(0)\n    train_loss = run_loss / seen\n\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n\n    # logging\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    print(\n        f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n\n    # early stopping on SWA\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# ---------- restore best ----------\nif best_state:\n    model.load_state_dict(best_state)\n\n# ---------- final evaluation ----------\ndev_loss, dev_swa, dev_preds, dev_labels = evaluate(dev_loader)\ntest_loss, test_swa, test_preds, test_labels = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\n# ---------- save results ----------\ned = experiment_data[\"SPR_BENCH\"]\ned[\"predictions\"][\"dev\"] = dev_preds\ned[\"ground_truth\"][\"dev\"] = dev_labels\ned[\"predictions\"][\"test\"] = test_preds\ned[\"ground_truth\"][\"test\"] = test_labels\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, re, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom datetime import datetime\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- reproducibility --------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment store --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n\n# -------------------- locate dataset --------------------\ndef locate_spr() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found \u2013 set $SPR_PATH\")\n\n\ndef load_spr(folder: pathlib.Path) -> DatasetDict:\n    def _load(csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(folder / csv),\n            split=\"train\",  # treat each csv as a single split\n            cache_dir=str(working_dir) + \"/.cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nspr_path = locate_spr()\nspr = load_spr(spr_path)\nprint(f\"Loaded SPR_BENCH from {spr_path}\")\n\n# -------------------- regex-based shape extraction (BUG-FIX) --------------------\nSHAPE_RE = re.compile(r\"[A-Z]+\")\n\n\ndef extract_shape(token: str) -> str:\n    \"\"\"\n    Return the leading upper-case substring of a token (e.g. 'AB2' -> 'AB').\n    Falls back to first character when no match is found.\n    \"\"\"\n    m = SHAPE_RE.match(token)\n    return m.group(0) if m else token[:1]\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({extract_shape(tok) for tok in seq.strip().split() if tok})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    corr = [w if g == p else 0 for w, g, p in zip(weights, y_true, y_pred)]\n    return sum(corr) / max(sum(weights), 1e-9)\n\n\n# -------------------- vocabulary --------------------\ntrain_seqs = spr[\"train\"][\"sequence\"]\ntok_counter = Counter(tok for seq in train_seqs for tok in seq.split())\nPAD, UNK = \"<pad>\", \"<unk>\"\nvocab = {PAD: 0, UNK: 1}\nfor tok in tok_counter:\n    vocab[tok] = len(vocab)\ninv_vocab = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nprint(f\"Vocab size = {len(vocab)} | num classes = {len(labels)}\")\n\n\n# -------------------- symbolic features --------------------\ndef symbolic_feats(seq: str):\n    toks = seq.split()\n    shapes = [extract_shape(t) for t in toks]\n    colors = [re.sub(r\"^[A-Z]+\", \"\", t) for t in toks]  # remainder after shapes\n    return np.array(\n        [\n            len(set(shapes)),  # shape variety\n            len(set(colors)),  # colour variety\n            len(shapes),  # sequence length\n            ord(shapes[0][0]) - 65 if shapes else -1,  # first shape index\n            ord(shapes[-1][0]) - 65 if shapes else -1,  # last shape index\n        ],\n        dtype=np.float32,\n    )\n\n\nSYM_FEAT_DIM = 5\n\n\n# -------------------- encoding helpers --------------------\ndef encode_seq(seq: str):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.strip().split()]\n\n\n# -------------------- dataset class --------------------\nclass SPRSet(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym\": torch.tensor(symbolic_feats(self.seqs[idx]), dtype=torch.float32),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n            \"seq_str\": self.seqs[idx],\n        }\n\n\n# -------------------- collate --------------------\ndef collate(batch):\n    lengths = [len(b[\"input_ids\"]) for b in batch]\n    maxlen = max(lengths)\n    ids = torch.full((len(batch), maxlen), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : lengths[i]] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym\"] for b in batch])\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": ids, \"sym\": sym, \"labels\": labels, \"seq_strs\": seqs}\n\n\n# -------------------- dataloaders --------------------\nBATCH = 128\ntrain_loader = DataLoader(\n    SPRSet(spr[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRSet(spr[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRSet(spr[\"test\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# -------------------- model --------------------\nclass HybridClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_sz,\n        tok_emb=64,\n        hid=128,\n        nclass=len(labels),\n        n_heads=2,\n        sym_dim=SYM_FEAT_DIM,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, tok_emb, padding_idx=0)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=tok_emb,\n            nhead=n_heads,\n            dim_feedforward=tok_emb * 2,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=1)\n        self.sym_proj = nn.Sequential(\n            nn.LayerNorm(sym_dim), nn.Linear(sym_dim, hid), nn.ReLU(), nn.Dropout(0.1)\n        )\n        self.cls = nn.Sequential(\n            nn.Linear(tok_emb + hid, hid),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hid, nclass),\n        )\n\n    def forward(self, ids, sym):\n        mask = ids == 0  # PAD mask\n        x = self.emb(ids)\n        x = self.enc(x, src_key_padding_mask=mask)\n        # masked mean pooling\n        x = x.masked_fill(mask.unsqueeze(-1), 0)\n        denom = (~mask).sum(1).clamp(min=1).unsqueeze(-1)\n        pooled = x.sum(1) / denom\n        sym_h = self.sym_proj(sym)\n        out = self.cls(torch.cat([pooled, sym_h], dim=-1))\n        return out\n\n\nmodel = HybridClassifier(len(vocab)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n\n# -------------------- helpers --------------------\ndef move_batch(batch):\n    return {\n        k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n        for k, v in batch.items()\n    }\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, n = 0.0, 0\n    preds, gold, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = move_batch(batch)\n            logits = model(batch[\"input_ids\"], batch[\"sym\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot_loss += loss.item() * bs\n            n += bs\n            p = logits.argmax(1).cpu().tolist()\n            g = batch[\"labels\"].cpu().tolist()\n            preds.extend(p)\n            gold.extend(g)\n            seqs.extend(batch[\"seq_strs\"])\n    swa = shape_weighted_accuracy(seqs, gold, preds)\n    return tot_loss / max(n, 1), swa, preds, gold\n\n\n# -------------------- training loop --------------------\nEPOCHS, PATIENCE = 20, 3\nbest_swa, patience, best_state = -1.0, 0, None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss, seen = 0.0, 0\n    for batch in train_loader:\n        batch = move_batch(batch)\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"], batch[\"sym\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        bs = batch[\"labels\"].size(0)\n        running_loss += loss.item() * bs\n        seen += bs\n    train_loss = running_loss / seen\n\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n    print(\n        f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n\n    # log\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    # early stopping\n    if val_swa > best_swa:\n        best_swa, patience = val_swa, 0\n        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# -------------------- restore best --------------------\nif best_state:\n    model.load_state_dict(best_state)\n\n# -------------------- final evaluation --------------------\ndev_loss, dev_swa, dev_preds, dev_labels = evaluate(dev_loader)\ntest_loss, test_swa, test_preds, test_labels = evaluate(test_loader)\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\n# -------------------- save results --------------------\ned = experiment_data[\"SPR_BENCH\"]\ned[\"predictions\"][\"dev\"] = dev_preds\ned[\"ground_truth\"][\"dev\"] = dev_labels\ned[\"predictions\"][\"test\"] = test_preds\ned[\"ground_truth\"][\"test\"] = test_labels\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, math, pathlib, random, numpy as np, torch\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment container ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# ---------- helper functions ----------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef find_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    default = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (default / \"train.csv\").exists():\n        return default\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def load(csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(\n        train=load(\"train.csv\"), dev=load(\"dev.csv\"), test=load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    corr = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(corr) / (sum(weights) if sum(weights) else 1.0)\n\n\n# ---------- data ----------\nDATA_PATH = find_spr_path()\ndsets = load_spr(DATA_PATH)\n\nvocab_counter = Counter(\n    tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()\n)\nvocab = {PAD: 0, UNK: 1}\nfor tok in vocab_counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\n\nprint(f\"Vocabulary: {len(vocab)}  |  Classes: {num_classes}\")\n\n\ndef enc_sequence(seq: str):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labs = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        toks = torch.tensor(enc_sequence(seq), dtype=torch.long)\n        sym = torch.tensor(\n            [len(seq.split()), count_shape_variety(seq), count_color_variety(seq)],\n            dtype=torch.float32,\n        )\n        return {\n            \"input_ids\": toks,\n            \"sym_feats\": sym,\n            \"label\": torch.tensor(lab2id[self.labs[idx]], dtype=torch.long),\n            \"seq_str\": seq,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    lab = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": ids, \"sym_feats\": sym, \"labels\": lab, \"seqs\": seqs}\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# ---------- model ----------\nEMB_DIM = 128\nSYM_DIM = 3\nSYM_PROJ = 32\nTRANS_LAY = 2\nHEADS = 4\nAUX_W = 0.2\nDROPOUT = 0.1\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass NeuralSymAux(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = PositionalEncoding(emb_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=HEADS,\n            dim_feedforward=emb_dim * 2,\n            dropout=DROPOUT,\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, num_layers=TRANS_LAY)\n        self.sym_proj = nn.Sequential(nn.Linear(SYM_DIM, SYM_PROJ), nn.ReLU())\n        self.classifier = nn.Linear(emb_dim + SYM_PROJ, num_labels)\n        self.aux_head = nn.Linear(emb_dim, 3)  # predict len, shape_var, color_var\n\n    def forward(self, ids, sym):\n        mask = ids == 0\n        x = self.emb(ids)\n        x = self.pos(x)\n        h = self.enc(x, src_key_padding_mask=mask)\n        pooled = h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-6\n        ).unsqueeze(-1)\n        pooled = pooled.squeeze(-1)\n        sym_emb = self.sym_proj(sym)\n        logits = self.classifier(torch.cat([pooled, sym_emb], dim=-1))\n        aux_pred = self.aux_head(pooled)\n        return logits, aux_pred\n\n\nmodel = NeuralSymAux(len(vocab), EMB_DIM, num_classes).to(device)\ncriterion_cls = nn.CrossEntropyLoss()\ncriterion_aux = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n\n\n# ---------- training helpers ----------\ndef evaluate(model, loader):\n    model.eval()\n    total, n = 0.0, 0\n    preds, labs, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits, _ = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = criterion_cls(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            total += loss.item() * bs\n            n += bs\n            p = logits.argmax(1).cpu().tolist()\n            l = batch[\"labels\"].cpu().tolist()\n            preds.extend(p)\n            labs.extend(l)\n            seqs.extend(batch[\"seqs\"])\n    swa = shape_weighted_accuracy(seqs, labs, preds)\n    return total / n, swa, preds, labs\n\n\n# ---------- training loop ----------\nEPOCHS, PATIENCE = 20, 3\nbest_swa, wait, best_state = -1, 0, None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tr_loss, seen = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits, aux_pred = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        cls_loss = criterion_cls(logits, batch[\"labels\"])\n        aux_tgt = batch[\"sym_feats\"]\n        aux_loss = criterion_aux(aux_pred, aux_tgt)\n        loss = cls_loss + AUX_W * aux_loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        bs = batch[\"labels\"].size(0)\n        tr_loss += loss.item() * bs\n        seen += bs\n    scheduler.step()\n    val_loss, val_swa, _, _ = evaluate(model, dev_loader)\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss/seen:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(tr_loss / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n    if wait >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# ---------- final evaluation ----------\nif best_state:\n    model.load_state_dict(best_state)\n\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(model, dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(model, test_loader)\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, math, pathlib, numpy as np, torch, random\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------------- working dir & device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------------- experiment container -------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# ----------------------- helper functions -----------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fallback = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fallback / \"train.csv\").exists():\n        return fallback\n    raise FileNotFoundError(\"Cannot locate SPR_BENCH dataset\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _load(name):  # load each csv as a single split called 'train'\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    tot_w = sum(weights)\n    return sum(correct) / (tot_w if tot_w else 1.0)\n\n\n# ----------------------- dataset & vocab ------------------------\nDATA_PATH = resolve_spr_path()\ndsets = load_spr(DATA_PATH)\n\nv_counter = Counter(tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in v_counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabel_set = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(label_set)}\nnum_classes = len(label_set)\nprint(f\"Vocab size={len(vocab)} | #classes={num_classes}\")\n\n\ndef encode(seq):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(s), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                [len(s.split()), count_shape_variety(s), count_color_variety(s)],\n                dtype=torch.float32,\n            ),\n            \"shape_var\": torch.tensor(count_shape_variety(s), dtype=torch.float32),\n            \"color_var\": torch.tensor(count_color_variety(s), dtype=torch.float32),\n            \"label\": torch.tensor(lab2id[self.labels[idx]], dtype=torch.long),\n            \"seq_str\": s,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    input_ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        input_ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym_feats = torch.stack([b[\"sym_feats\"] for b in batch])\n    shape_var = torch.stack([b[\"shape_var\"] for b in batch])\n    color_var = torch.stack([b[\"color_var\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\n        \"input_ids\": input_ids,\n        \"sym_feats\": sym_feats,\n        \"shape_var\": shape_var,\n        \"color_var\": color_var,\n        \"labels\": labels,\n        \"seqs\": seqs,\n    }\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# ----------------------- model ----------------------------------\nEMB_DIM = 128\nSYM_DIM = 3\nSYM_PROJ = 32\nN_HEAD = 4\nN_LAY = 2\nDROP = 0.2\nAUX_LAMBDA = 0.1\n\n\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass NeuroSymbolic(nn.Module):\n    def __init__(self, vocab_sz, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, EMB_DIM, padding_idx=0)\n        self.pos = PosEnc(EMB_DIM)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=EMB_DIM,\n            nhead=N_HEAD,\n            dim_feedforward=EMB_DIM * 2,\n            dropout=DROP,\n            batch_first=True,\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=N_LAY)\n        self.sym_proj = nn.Sequential(nn.Linear(SYM_DIM, SYM_PROJ), nn.ReLU())\n        self.dropout = nn.Dropout(DROP)\n        hidden_dim = EMB_DIM + SYM_PROJ\n        self.cls_head = nn.Linear(hidden_dim, num_labels)\n        self.shape_head = nn.Linear(hidden_dim, 1)\n        self.color_head = nn.Linear(hidden_dim, 1)\n\n    def forward(self, ids, sym):\n        mask = ids == 0\n        x = self.emb(ids)\n        x = self.pos(x)\n        h = self.enc(x, src_key_padding_mask=mask)\n        pooled = h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-6\n        ).unsqueeze(-1)\n        z = torch.cat([pooled, self.sym_proj(sym)], dim=-1)\n        z = self.dropout(z)\n        return (\n            self.cls_head(z),\n            self.shape_head(z).squeeze(-1),\n            self.color_head(z).squeeze(-1),\n        )\n\n\nmodel = NeuroSymbolic(len(vocab), num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\nce_loss = nn.CrossEntropyLoss()\nmse_loss = nn.MSELoss()\n\n\n# ----------------------- train / eval ---------------------------\ndef evaluate(loader):\n    model.eval()\n    total, n = 0.0, 0\n    all_pred, all_lab, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits, pred_shape, pred_color = model(\n                batch[\"input_ids\"], batch[\"sym_feats\"]\n            )\n            loss = ce_loss(logits, batch[\"labels\"]) + AUX_LAMBDA * (\n                mse_loss(pred_shape, batch[\"shape_var\"])\n                + mse_loss(pred_color, batch[\"color_var\"])\n            )\n            bs = batch[\"labels\"].size(0)\n            total += loss.item() * bs\n            n += bs\n            preds = logits.argmax(1).cpu().tolist()\n            labs = batch[\"labels\"].cpu().tolist()\n            seqs = batch[\"seqs\"]\n            all_pred.extend(preds)\n            all_lab.extend(labs)\n            all_seq.extend(seqs)\n    swa = shape_weighted_accuracy(all_seq, all_lab, all_pred)\n    return total / n, swa, all_pred, all_lab\n\n\nEPOCHS = 25\nPATIENCE = 4\nbest_swa, patience = -1, 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tr_tot, seen = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits, p_shape, p_color = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        loss = ce_loss(logits, batch[\"labels\"]) + AUX_LAMBDA * (\n            mse_loss(p_shape, batch[\"shape_var\"])\n            + mse_loss(p_color, batch[\"color_var\"])\n        )\n        loss.backward()\n        optimizer.step()\n        bs = batch[\"labels\"].size(0)\n        tr_tot += loss.item() * bs\n        seen += bs\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_SWA = {val_swa:.4f}\")\n    # log\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(tr_tot / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n    # early stop\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# ----------------------- final evaluation -----------------------\nif best_state:\n    model.load_state_dict(best_state)\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, math, pathlib, numpy as np, torch, random\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment container ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# ---------- helper functions ----------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _ld(fname):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / fname),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    total = sum(w)\n    return sum(corr) / total if total else 0.0\n\n\n# ---------- load data ----------\nDATA_PATH = resolve_spr_path()\ndsets = load_spr(DATA_PATH)\n\n# build vocab on train\ncounter = Counter(tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nid2lab = {i: l for l, i in enumerate(labels)}\nnum_classes = len(labels)\n\nprint(f\"Vocab size {len(vocab)} | Classes {num_classes}\")\n\n\ndef encode(seq: str):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\n# compute symbolic stats mean/std on train for standardisation\nshape_vals = np.array(\n    [count_shape_variety(s) for s in dsets[\"train\"][\"sequence\"]], dtype=np.float32\n)\ncolor_vals = np.array(\n    [count_color_variety(s) for s in dsets[\"train\"][\"sequence\"]], dtype=np.float32\n)\nsym_mean = np.array([shape_vals.mean(), color_vals.mean()], dtype=np.float32)\nsym_std = np.array([shape_vals.std() + 1e-6, color_vals.std() + 1e-6], dtype=np.float32)\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.lab = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        shape_var = count_shape_variety(s)\n        color_var = count_color_variety(s)\n        sym = np.array([shape_var, color_var], dtype=np.float32)\n        return {\n            \"input_ids\": torch.tensor(encode(s), dtype=torch.long),\n            \"sym_feats\": torch.tensor(sym, dtype=torch.float32),\n            \"label\": torch.tensor(lab2id[self.lab[idx]], dtype=torch.long),\n            \"seq_str\": s,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    inp = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        inp[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    lab = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    # standardise sym for regression target\n    sym_stdzd = (sym - torch.tensor(sym_mean)) / torch.tensor(sym_std)\n    return {\n        \"input_ids\": inp,\n        \"sym_feats\": sym,\n        \"sym_target\": sym_stdzd,\n        \"labels\": lab,\n        \"seqs\": seqs,\n    }\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# ---------- model ----------\nEMB = 128\nN_HEAD = 4\nN_LAY = 2\nDROPOUT = 0.2\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass MultiTaskSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, n_cls):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = PositionalEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=N_HEAD,\n            dim_feedforward=emb_dim * 2,\n            dropout=DROPOUT,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=N_LAY)\n        self.pool_dropout = nn.Dropout(DROPOUT)\n        self.cls_head = nn.Linear(emb_dim, n_cls)\n        self.sym_head = nn.Linear(\n            emb_dim, 2\n        )  # predict standardized shape & color variety\n\n    def forward(self, input_ids):\n        mask = input_ids == 0\n        x = self.emb(input_ids)\n        x = self.pos(x)\n        x = self.encoder(x, src_key_padding_mask=mask)\n        x = x.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-5\n        ).unsqueeze(-1)\n        x = self.pool_dropout(x)\n        return self.cls_head(x), self.sym_head(x)\n\n\nmodel = MultiTaskSPR(len(vocab), EMB, num_classes).to(device)\n\n# ---------- optimisation ----------\nweight_decay = 1e-4\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=weight_decay)\nce_loss = nn.CrossEntropyLoss()\nmse_loss = nn.MSELoss()\nalpha = 0.1  # weight for sym regression\n\n# ---------- training loop ----------\nEPOCHS = 25\nPATIENCE = 4\nbest_swa = -1\npatience = 0\nbest_state = None\n\n\ndef evaluate(loader):\n    model.eval()\n    total_loss = 0\n    total = 0\n    all_pred = []\n    all_lab = []\n    all_seq = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits, sym_out = model(batch[\"input_ids\"])\n            loss = ce_loss(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            total += batch[\"labels\"].size(0)\n            preds = logits.argmax(1).cpu().tolist()\n            labs = batch[\"labels\"].cpu().tolist()\n            seqs = batch[\"seqs\"]\n            all_pred.extend(preds)\n            all_lab.extend(labs)\n            all_seq.extend(seqs)\n    swa = shape_weighted_accuracy(all_seq, all_lab, all_pred)\n    return total_loss / total, swa, all_pred, all_lab\n\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_loss = 0\n    seen = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits, sym_out = model(batch[\"input_ids\"])\n        loss = ce_loss(logits, batch[\"labels\"]) + alpha * mse_loss(\n            sym_out, batch[\"sym_target\"]\n        )\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        bs = batch[\"labels\"].size(0)\n        train_loss += loss.item() * bs\n        seen += bs\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n    print(\n        f\"Epoch {epoch:02d}: train_loss={train_loss/seen:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(train_loss / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n\n# ---------- evaluation ----------\nif best_state:\n    model.load_state_dict(best_state)\n\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(test_loader)\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, math, pathlib, numpy as np, torch, random\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------- working dir --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------- device -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------- experiment container -----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# -------------------------- utils --------------------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _ld(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1.0)\n\n\n# -------------------------- data ---------------------------------\nDATA_PATH = resolve_spr_path()\ndsets = load_spr(DATA_PATH)\n\n# vocab\ncounter = Counter(tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nid2lab = {i: l for l, i in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Vocabulary size: {len(vocab)} | Classes: {num_classes}\")\n\n\ndef encode(seq):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.lab = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                [len(seq.split()), count_shape_variety(seq), count_color_variety(seq)],\n                dtype=torch.float32,\n            ),\n            \"label\": torch.tensor(lab2id[self.lab[idx]], dtype=torch.long),\n            \"seq_str\": seq,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    input_ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        input_ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": input_ids, \"sym_feats\": sym, \"labels\": labels, \"seqs\": seqs}\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# -------------------------- model --------------------------------\nEMB = 128\nSYM_DIM = 3\nSYM_PROJ = 32\nN_HEAD = 4\nN_LAY = 2\nHIDDEN = 256\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # 1 x max_len x d_model\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass NeuralSymbolicSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = PositionalEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=N_HEAD, dim_feedforward=emb_dim * 2, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=N_LAY)\n        self.sym_proj = nn.Sequential(nn.Linear(SYM_DIM, SYM_PROJ), nn.ReLU())\n        self.fc = nn.Linear(emb_dim + SYM_PROJ, num_labels)\n\n    def forward(self, input_ids, sym_feats):\n        mask = input_ids == 0\n        x = self.emb(input_ids)\n        x = self.pos(x)\n        h = self.enc(x, src_key_padding_mask=mask)\n        seq_emb = h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-6\n        ).unsqueeze(-1)\n        sym = self.sym_proj(sym_feats)\n        combined = torch.cat([seq_emb, sym], dim=-1)\n        return self.fc(combined)\n\n\nmodel = NeuralSymbolicSPR(len(vocab), EMB, num_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# -------------------------- helpers ------------------------------\ndef evaluate(loader):\n    model.eval()\n    tot, n = 0.0, 0\n    all_pred, all_lab, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot += loss.item() * bs\n            n += bs\n            preds = logits.argmax(1).cpu().tolist()\n            labs = batch[\"labels\"].cpu().tolist()\n            seqs = batch[\"seqs\"]\n            all_pred.extend(preds)\n            all_lab.extend(labs)\n            all_seq.extend(seqs)\n    swa = shape_weighted_accuracy(all_seq, all_lab, all_pred)\n    return tot / n, swa, all_pred, all_lab\n\n\n# -------------------------- training -----------------------------\nEPOCHS = 20\nPATIENCE = 3\nbest_swa = -1\npatience = 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tloss, seen = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        bs = batch[\"labels\"].size(0)\n        tloss += loss.item() * bs\n        seen += bs\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch:02d}: train_loss={tloss/seen:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(tloss / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n\n# -------------------------- evaluation ---------------------------\nif best_state:\n    model.load_state_dict(best_state)\n\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, math, pathlib, numpy as np, torch, random\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------- working dir --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------- device -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------- experiment container -----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# -------------------------- utils --------------------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _ld(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1.0)\n\n\n# -------------------------- data ---------------------------------\nDATA_PATH = resolve_spr_path()\ndsets = load_spr(DATA_PATH)\n\n# vocab\ncounter = Counter(tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nid2lab = {i: l for l, i in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Vocabulary size: {len(vocab)} | Classes: {num_classes}\")\n\n\ndef encode(seq):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.lab = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                [len(seq.split()), count_shape_variety(seq), count_color_variety(seq)],\n                dtype=torch.float32,\n            ),\n            \"label\": torch.tensor(lab2id[self.lab[idx]], dtype=torch.long),\n            \"seq_str\": seq,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    input_ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        input_ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": input_ids, \"sym_feats\": sym, \"labels\": labels, \"seqs\": seqs}\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# -------------------------- model --------------------------------\nEMB = 128\nSYM_DIM = 3\nSYM_PROJ = 32\nN_HEAD = 4\nN_LAY = 2\nHIDDEN = 256\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # 1 x max_len x d_model\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass NeuralSymbolicSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = PositionalEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=N_HEAD, dim_feedforward=emb_dim * 2, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=N_LAY)\n        self.sym_proj = nn.Sequential(nn.Linear(SYM_DIM, SYM_PROJ), nn.ReLU())\n        self.fc = nn.Linear(emb_dim + SYM_PROJ, num_labels)\n\n    def forward(self, input_ids, sym_feats):\n        mask = input_ids == 0\n        x = self.emb(input_ids)\n        x = self.pos(x)\n        h = self.enc(x, src_key_padding_mask=mask)\n        seq_emb = h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-6\n        ).unsqueeze(-1)\n        sym = self.sym_proj(sym_feats)\n        combined = torch.cat([seq_emb, sym], dim=-1)\n        return self.fc(combined)\n\n\nmodel = NeuralSymbolicSPR(len(vocab), EMB, num_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# -------------------------- helpers ------------------------------\ndef evaluate(loader):\n    model.eval()\n    tot, n = 0.0, 0\n    all_pred, all_lab, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot += loss.item() * bs\n            n += bs\n            preds = logits.argmax(1).cpu().tolist()\n            labs = batch[\"labels\"].cpu().tolist()\n            seqs = batch[\"seqs\"]\n            all_pred.extend(preds)\n            all_lab.extend(labs)\n            all_seq.extend(seqs)\n    swa = shape_weighted_accuracy(all_seq, all_lab, all_pred)\n    return tot / n, swa, all_pred, all_lab\n\n\n# -------------------------- training -----------------------------\nEPOCHS = 20\nPATIENCE = 3\nbest_swa = -1\npatience = 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tloss, seen = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        bs = batch[\"labels\"].size(0)\n        tloss += loss.item() * bs\n        seen += bs\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch:02d}: train_loss={tloss/seen:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(tloss / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n\n# -------------------------- evaluation ---------------------------\nif best_state:\n    model.load_state_dict(best_state)\n\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, math, pathlib, numpy as np, torch, random\nfrom collections import Counter\nfrom datetime import datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------- working dir --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------- device -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------- experiment container -----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_swa\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n        \"timestamps\": [],\n    }\n}\n\n# -------------------------- utils --------------------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef resolve_spr_path() -> pathlib.Path:\n    env = os.getenv(\"SPR_PATH\")\n    if env and (pathlib.Path(env) / \"train.csv\").exists():\n        return pathlib.Path(env)\n    cur = pathlib.Path.cwd()\n    for p in [cur] + list(cur.parents):\n        if (p / \"SPR_BENCH\" / \"train.csv\").exists():\n            return p / \"SPR_BENCH\"\n    fb = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n    if (fb / \"train.csv\").exists():\n        return fb\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\n\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _ld(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / name),\n            split=\"train\",\n            cache_dir=str(working_dir) + \"/.hf_cache\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1.0)\n\n\n# -------------------------- data ---------------------------------\nDATA_PATH = resolve_spr_path()\ndsets = load_spr(DATA_PATH)\n\n# vocab\ncounter = Counter(tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {PAD: 0, UNK: 1}\nfor tok in counter:\n    vocab.setdefault(tok, len(vocab))\nid2tok = {i: t for t, i in vocab.items()}\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nid2lab = {i: l for l, i in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Vocabulary size: {len(vocab)} | Classes: {num_classes}\")\n\n\ndef encode(seq):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seq = split[\"sequence\"]\n        self.lab = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        return {\n            \"input_ids\": torch.tensor(encode(seq), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                [len(seq.split()), count_shape_variety(seq), count_color_variety(seq)],\n                dtype=torch.float32,\n            ),\n            \"label\": torch.tensor(lab2id[self.lab[idx]], dtype=torch.long),\n            \"seq_str\": seq,\n        }\n\n\ndef collate(batch):\n    max_len = max(len(b[\"input_ids\"]) for b in batch)\n    input_ids = torch.full((len(batch), max_len), vocab[PAD], dtype=torch.long)\n    for i, b in enumerate(batch):\n        input_ids[i, : len(b[\"input_ids\"])] = b[\"input_ids\"]\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    labels = torch.stack([b[\"label\"] for b in batch])\n    seqs = [b[\"seq_str\"] for b in batch]\n    return {\"input_ids\": input_ids, \"sym_feats\": sym, \"labels\": labels, \"seqs\": seqs}\n\n\nBS = 128\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BS, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRDataset(dsets[\"test\"]), batch_size=BS, shuffle=False, collate_fn=collate\n)\n\n# -------------------------- model --------------------------------\nEMB = 128\nSYM_DIM = 3\nSYM_PROJ = 32\nN_HEAD = 4\nN_LAY = 2\nHIDDEN = 256\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # 1 x max_len x d_model\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass NeuralSymbolicSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.pos = PositionalEncoding(emb_dim)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim, nhead=N_HEAD, dim_feedforward=emb_dim * 2, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, num_layers=N_LAY)\n        self.sym_proj = nn.Sequential(nn.Linear(SYM_DIM, SYM_PROJ), nn.ReLU())\n        self.fc = nn.Linear(emb_dim + SYM_PROJ, num_labels)\n\n    def forward(self, input_ids, sym_feats):\n        mask = input_ids == 0\n        x = self.emb(input_ids)\n        x = self.pos(x)\n        h = self.enc(x, src_key_padding_mask=mask)\n        seq_emb = h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(\n            min=1e-6\n        ).unsqueeze(-1)\n        sym = self.sym_proj(sym_feats)\n        combined = torch.cat([seq_emb, sym], dim=-1)\n        return self.fc(combined)\n\n\nmodel = NeuralSymbolicSPR(len(vocab), EMB, num_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# -------------------------- helpers ------------------------------\ndef evaluate(loader):\n    model.eval()\n    tot, n = 0.0, 0\n    all_pred, all_lab, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = criterion(logits, batch[\"labels\"])\n            bs = batch[\"labels\"].size(0)\n            tot += loss.item() * bs\n            n += bs\n            preds = logits.argmax(1).cpu().tolist()\n            labs = batch[\"labels\"].cpu().tolist()\n            seqs = batch[\"seqs\"]\n            all_pred.extend(preds)\n            all_lab.extend(labs)\n            all_seq.extend(seqs)\n    swa = shape_weighted_accuracy(all_seq, all_lab, all_pred)\n    return tot / n, swa, all_pred, all_lab\n\n\n# -------------------------- training -----------------------------\nEPOCHS = 20\nPATIENCE = 3\nbest_swa = -1\npatience = 0\nbest_state = None\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tloss, seen = 0.0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        bs = batch[\"labels\"].size(0)\n        tloss += loss.item() * bs\n        seen += bs\n    val_loss, val_swa, _, _ = evaluate(dev_loader)\n\n    print(\n        f\"Epoch {epoch:02d}: train_loss={tloss/seen:.4f} | val_loss={val_loss:.4f} | val_SWA={val_swa:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(tloss / seen)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(datetime.utcnow().isoformat())\n\n    if val_swa > best_swa:\n        best_swa = val_swa\n        best_state = model.state_dict()\n        patience = 0\n    else:\n        patience += 1\n    if patience >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n\n# -------------------------- evaluation ---------------------------\nif best_state:\n    model.load_state_dict(best_state)\n\ndev_loss, dev_swa, dev_pred, dev_lab = evaluate(dev_loader)\ntest_loss, test_swa, test_pred, test_lab = evaluate(test_loader)\n\nprint(f\"=== DEV  === loss {dev_loss:.4f} | SWA {dev_swa:.4f}\")\nprint(f\"=== TEST === loss {test_loss:.4f} | SWA {test_swa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_lab\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_lab\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device:', ' ', 'cuda', '\\n', '[Data] Found SPR_BENCH at', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples\n[00:00, 297622.46 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 5000 examples [00:00,\n593791.27 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 501507.04\nexamples/s]', '\\n', 'Vocab size 18 | Classes 2', '\\n', 'Epoch 01 | train 0.4637\n| val 0.3125 | SWA 0.8807 | CWA 0.8812 | BPS 0.8810', '\\n', 'Epoch 02 | train\n0.2504 | val 0.2135 | SWA 0.9386 | CWA 0.9421 | BPS 0.9404', '\\n', 'Epoch 03 |\ntrain 0.1967 | val 0.1864 | SWA 0.9409 | CWA 0.9442 | BPS 0.9426', '\\n', 'Epoch\n04 | train 0.1820 | val 0.1829 | SWA 0.9409 | CWA 0.9435 | BPS 0.9422', '\\n',\n'Epoch 05 | train 0.1758 | val 0.1773 | SWA 0.9434 | CWA 0.9462 | BPS 0.9448',\n'\\n', 'Epoch 06 | train 0.1723 | val 0.1757 | SWA 0.9440 | CWA 0.9470 | BPS\n0.9455', '\\n', 'Epoch 07 | train 0.1701 | val 0.1701 | SWA 0.9444 | CWA 0.9475 |\nBPS 0.9459', '\\n', 'Epoch 08 | train 0.1687 | val 0.1692 | SWA 0.9447 | CWA\n0.9477 | BPS 0.9462', '\\n', 'Epoch 09 | train 0.1665 | val 0.1678 | SWA 0.9447 |\nCWA 0.9477 | BPS 0.9462', '\\n', 'Epoch 10 | train 0.1656 | val 0.1693 | SWA\n0.9440 | CWA 0.9470 | BPS 0.9455', '\\n', 'Epoch 11 | train 0.1654 | val 0.1674 |\nSWA 0.9447 | CWA 0.9477 | BPS 0.9462', '\\n', 'Early stopping triggered.', '\\n',\n'=== DEV  === loss 0.1674 | SWA 0.9447 | CWA 0.9477 | BPS 0.9462', '\\n', '===\nTEST === loss 1.9175 | SWA 0.6500 | CWA 0.6957 | BPS 0.6725', '\\n', 'Execution\ntime: 10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 335662.47\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 540949.24\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 605658.18\nexamples/s]', '\\n', 'Vocab 19 | Classes 2', '\\n', 'Epoch 1: train_loss 0.2085 |\nval_loss 0.1546 | val_SWA 0.9556', '\\n', 'Epoch 2: train_loss 0.1347 | val_loss\n0.0949 | val_SWA 0.9727', '\\n', 'Epoch 3: train_loss 0.0952 | val_loss 0.0906 |\nval_SWA 0.9766', '\\n', 'Epoch 4: train_loss 0.0874 | val_loss 0.0656 | val_SWA\n0.9833', '\\n', 'Epoch 5: train_loss 0.0690 | val_loss 0.0529 | val_SWA 0.9869',\n'\\n', 'Epoch 6: train_loss 0.0592 | val_loss 0.0516 | val_SWA 0.9868', '\\n',\n'Epoch 7: train_loss 0.0609 | val_loss 0.0498 | val_SWA 0.9872', '\\n', 'Epoch 8:\ntrain_loss 0.0539 | val_loss 0.0461 | val_SWA 0.9876', '\\n', 'Epoch 9:\ntrain_loss 0.0457 | val_loss 0.0433 | val_SWA 0.9886', '\\n', 'Epoch 10:\ntrain_loss 0.0423 | val_loss 0.0497 | val_SWA 0.9874', '\\n', 'Epoch 11:\ntrain_loss 0.0545 | val_loss 0.0444 | val_SWA 0.9885', '\\n', 'Epoch 12:\ntrain_loss 0.0397 | val_loss 0.0394 | val_SWA 0.9894', '\\n', 'Epoch 13:\ntrain_loss 0.0373 | val_loss 0.0368 | val_SWA 0.9887', '\\n', 'Epoch 14:\ntrain_loss 0.0421 | val_loss 0.0316 | val_SWA 0.9931', '\\n', 'Epoch 15:\ntrain_loss 0.0377 | val_loss 0.0426 | val_SWA 0.9890', '\\n', 'Epoch 16:\ntrain_loss 0.0374 | val_loss 0.0324 | val_SWA 0.9923', '\\n', 'Epoch 17:\ntrain_loss 0.0323 | val_loss 0.0309 | val_SWA 0.9909', '\\n', 'Epoch 18:\ntrain_loss 0.0343 | val_loss 0.0337 | val_SWA 0.9934', '\\n', 'Epoch 19:\ntrain_loss 0.0336 | val_loss 0.0275 | val_SWA 0.9922', '\\n', 'Epoch 20:\ntrain_loss 0.0265 | val_loss 0.0229 | val_SWA 0.9945', '\\n', 'Epoch 21:\ntrain_loss 0.0318 | val_loss 0.0325 | val_SWA 0.9915', '\\n', 'Epoch 22:\ntrain_loss 0.0318 | val_loss 0.0334 | val_SWA 0.9917', '\\n', 'Epoch 23:\ntrain_loss 0.0609 | val_loss 0.0555 | val_SWA 0.9866', '\\n', 'Epoch 24:\ntrain_loss 0.0475 | val_loss 0.0370 | val_SWA 0.9897', '\\n', 'Epoch 25:\ntrain_loss 0.0381 | val_loss 0.0405 | val_SWA 0.9891', '\\n', 'Early stopping.',\n'\\n', '=== DEV  === loss 0.0405 | SWA 0.9891', '\\n', '=== TEST === loss 2.0743 |\nSWA 0.6536', '\\n', 'Execution time: 39 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 411133.72\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 460416.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 563409.77\nexamples/s]', '\\n', 'Vocabulary size: 18 | Classes: 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=0.2203 |\nval_loss=0.1160 | val_SWA=0.9655', '\\n', 'Epoch 02: train_loss=0.0913 |\nval_loss=0.0785 | val_SWA=0.9706', '\\n', 'Epoch 03: train_loss=0.0613 |\nval_loss=0.0650 | val_SWA=0.9813', '\\n', 'Epoch 04: train_loss=0.0449 |\nval_loss=0.0336 | val_SWA=0.9901', '\\n', 'Epoch 05: train_loss=0.0398 |\nval_loss=0.0485 | val_SWA=0.9848', '\\n', 'Epoch 06: train_loss=0.0315 |\nval_loss=0.0351 | val_SWA=0.9901', '\\n', 'Epoch 07: train_loss=0.0260 |\nval_loss=0.0228 | val_SWA=0.9942', '\\n', 'Epoch 08: train_loss=0.0191 |\nval_loss=0.0116 | val_SWA=0.9971', '\\n', 'Epoch 09: train_loss=0.0276 |\nval_loss=0.0281 | val_SWA=0.9918', '\\n', 'Epoch 10: train_loss=0.0243 |\nval_loss=0.0134 | val_SWA=0.9967', '\\n', 'Epoch 11: train_loss=0.0208 |\nval_loss=0.0204 | val_SWA=0.9926', '\\n', 'Early stopping triggered.', '\\n', '===\nDEV  === loss 0.0204 | SWA 0.9926', '\\n', '=== TEST === loss 3.1433 | SWA\n0.6542', '\\n', 'Execution time: 22 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 457893.45\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 673783.78\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 733616.22\nexamples/s]', '\\n', 'Epoch 1: train_loss=0.5109 | val_loss=0.4260 | SWA=0.7944',\n'\\n', 'Epoch 2: train_loss=0.3508 | val_loss=0.2844 | SWA=0.9119', '\\n', 'Epoch\n3: train_loss=0.2504 | val_loss=0.2264 | SWA=0.9360', '\\n', 'Epoch 4:\ntrain_loss=0.2138 | val_loss=0.2041 | SWA=0.9372', '\\n', 'Epoch 5:\ntrain_loss=0.1968 | val_loss=0.1931 | SWA=0.9422', '\\n', 'Epoch 6:\ntrain_loss=0.1873 | val_loss=0.1842 | SWA=0.9423', '\\n', 'Epoch 7:\ntrain_loss=0.1809 | val_loss=0.1798 | SWA=0.9416', '\\n', 'Epoch 8:\ntrain_loss=0.1765 | val_loss=0.1757 | SWA=0.9438', '\\n', 'Epoch 9:\ntrain_loss=0.1733 | val_loss=0.1768 | SWA=0.9422', '\\n', 'Epoch 10:\ntrain_loss=0.1710 | val_loss=0.1713 | SWA=0.9440', '\\n', 'Epoch 11:\ntrain_loss=0.1692 | val_loss=0.1705 | SWA=0.9445', '\\n', 'Epoch 12:\ntrain_loss=0.1681 | val_loss=0.1702 | SWA=0.9441', '\\n', 'Epoch 13:\ntrain_loss=0.1667 | val_loss=0.1682 | SWA=0.9445', '\\n', 'Epoch 14:\ntrain_loss=0.1658 | val_loss=0.1674 | SWA=0.9445', '\\n', 'Epoch 15:\ntrain_loss=0.1646 | val_loss=0.1694 | SWA=0.9443', '\\n', 'Early stopping.',\n'\\n', '=== DEV  === loss 0.1694 | SWA 0.9443', '\\n', '=== TEST === loss 1.6123 |\nSWA 0.6497', '\\n', 'Execution time: 11 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 415508.00\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 345967.63\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 477444.71\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size=18 | num classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=32.1119 |\nval_loss=19.1948 | val_SWA=0.4794', '\\n', 'Epoch 02: train_loss=10.1973 |\nval_loss=5.7830 | val_SWA=0.7807', '\\n', 'Epoch 03: train_loss=3.9265 |\nval_loss=2.0243 | val_SWA=0.7082', '\\n', 'Epoch 04: train_loss=4.3361 |\nval_loss=4.5306 | val_SWA=0.8045', '\\n', 'Epoch 05: train_loss=6.9727 |\nval_loss=2.9496 | val_SWA=0.6839', '\\n', 'Epoch 06: train_loss=6.8819 |\nval_loss=2.3628 | val_SWA=0.7840', '\\n', 'Epoch 07: train_loss=4.1136 |\nval_loss=2.0352 | val_SWA=0.8205', '\\n', 'Epoch 08: train_loss=4.8197 |\nval_loss=4.5738 | val_SWA=0.8016', '\\n', 'Epoch 09: train_loss=3.4641 |\nval_loss=1.9510 | val_SWA=0.8671', '\\n', 'Epoch 10: train_loss=2.0985 |\nval_loss=1.9556 | val_SWA=0.9001', '\\n', 'Epoch 11: train_loss=1.9270 |\nval_loss=0.6823 | val_SWA=0.8689', '\\n', 'Epoch 12: train_loss=1.6453 |\nval_loss=1.2101 | val_SWA=0.9204', '\\n', 'Epoch 13: train_loss=1.6231 |\nval_loss=0.6358 | val_SWA=0.9100', '\\n', 'Epoch 14: train_loss=2.4018 |\nval_loss=3.8656 | val_SWA=0.8523', '\\n', 'Epoch 15: train_loss=1.5223 |\nval_loss=4.4074 | val_SWA=0.7684', '\\n', 'Early stopping.', '\\n', '=== DEV  ===\nloss 1.2101 | SWA 0.9204', '\\n', '=== TEST === loss 6.7581 | SWA 0.6223', '\\n',\n'Execution time: 21 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 372157.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 512838.87\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 684471.43\nexamples/s]', '\\n', 'Loaded SPR_BENCH from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size = 18 | num classes = 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=0.4172 |\nval_loss=0.2247 | val_SWA=0.9229', '\\n', 'Epoch 02: train_loss=0.2119 |\nval_loss=0.1756 | val_SWA=0.9424', '\\n', 'Epoch 03: train_loss=0.1851 |\nval_loss=0.1772 | val_SWA=0.9431', '\\n', 'Epoch 04: train_loss=0.1772 |\nval_loss=0.1682 | val_SWA=0.9447', '\\n', 'Epoch 05: train_loss=0.1733 |\nval_loss=0.1690 | val_SWA=0.9442', '\\n', 'Epoch 06: train_loss=0.1731 |\nval_loss=0.1678 | val_SWA=0.9449', '\\n', 'Epoch 07: train_loss=0.1707 |\nval_loss=0.1669 | val_SWA=0.9447', '\\n', 'Epoch 08: train_loss=0.1706 |\nval_loss=0.1732 | val_SWA=0.9447', '\\n', 'Epoch 09: train_loss=0.1706 |\nval_loss=0.1672 | val_SWA=0.9447', '\\n', 'Early stopping.', '\\n', '=== DEV  ===\nloss 0.1678 | SWA 0.9449', '\\n', '=== TEST === loss 1.9581 | SWA 0.6498', '\\n',\n'Execution time: 17 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 562608.68\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 753747.62\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 559479.24\nexamples/s]', '\\n', 'Vocabulary: 18  |  Classes: 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.3600 |\nval_loss=0.1253 | val_SWA=0.9626', '\\n', 'Epoch 2: train_loss=0.1503 |\nval_loss=0.1055 | val_SWA=0.9738', '\\n', 'Epoch 3: train_loss=0.1147 |\nval_loss=0.0614 | val_SWA=0.9841', '\\n', 'Epoch 4: train_loss=0.0840 |\nval_loss=0.0339 | val_SWA=0.9869', '\\n', 'Epoch 5: train_loss=0.0653 |\nval_loss=0.0264 | val_SWA=0.9898', '\\n', 'Epoch 6: train_loss=0.0544 |\nval_loss=0.0262 | val_SWA=0.9885', '\\n', 'Epoch 7: train_loss=0.0473 |\nval_loss=0.0194 | val_SWA=0.9948', '\\n', 'Epoch 8: train_loss=0.0380 |\nval_loss=0.0085 | val_SWA=0.9973', '\\n', 'Epoch 9: train_loss=0.0314 |\nval_loss=0.0100 | val_SWA=0.9966', '\\n', 'Epoch 10: train_loss=0.0261 |\nval_loss=0.0103 | val_SWA=0.9957', '\\n', 'Epoch 11: train_loss=0.0233 |\nval_loss=0.0045 | val_SWA=0.9985', '\\n', 'Epoch 12: train_loss=0.0195 |\nval_loss=0.0176 | val_SWA=0.9952', '\\n', 'Epoch 13: train_loss=0.0171 |\nval_loss=0.0021 | val_SWA=0.9993', '\\n', 'Epoch 14: train_loss=0.0134 |\nval_loss=0.0011 | val_SWA=1.0000', '\\n', 'Epoch 15: train_loss=0.0113 |\nval_loss=0.0007 | val_SWA=1.0000', '\\n', 'Epoch 16: train_loss=0.0098 |\nval_loss=0.0008 | val_SWA=0.9998', '\\n', 'Epoch 17: train_loss=0.0095 |\nval_loss=0.0005 | val_SWA=1.0000', '\\n', 'Early stopping.', '\\n', '=== DEV  ===\nloss 0.0005 | SWA 1.0000', '\\n', '=== TEST === loss 3.3434 | SWA 0.6520', '\\n',\n'Execution time: 25 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 453735.04\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 507404.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 775401.91\nexamples/s]', '\\n', 'Vocab size=18 | #classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.1686\n| val_SWA = 0.9577', '\\n', 'Epoch 2: validation_loss = 0.1232 | val_SWA =\n0.9655', '\\n', 'Epoch 3: validation_loss = 0.1017 | val_SWA = 0.9731', '\\n',\n'Epoch 4: validation_loss = 0.1119 | val_SWA = 0.9731', '\\n', 'Epoch 5:\nvalidation_loss = 0.0683 | val_SWA = 0.9818', '\\n', 'Epoch 6: validation_loss =\n0.0710 | val_SWA = 0.9829', '\\n', 'Epoch 7: validation_loss = 0.0619 | val_SWA =\n0.9856', '\\n', 'Epoch 8: validation_loss = 0.0658 | val_SWA = 0.9797', '\\n',\n'Epoch 9: validation_loss = 0.0491 | val_SWA = 0.9879', '\\n', 'Epoch 10:\nvalidation_loss = 0.0589 | val_SWA = 0.9854', '\\n', 'Epoch 11: validation_loss =\n0.0660 | val_SWA = 0.9850', '\\n', 'Epoch 12: validation_loss = 0.0407 | val_SWA\n= 0.9901', '\\n', 'Epoch 13: validation_loss = 0.0395 | val_SWA = 0.9924', '\\n',\n'Epoch 14: validation_loss = 0.0385 | val_SWA = 0.9910', '\\n', 'Epoch 15:\nvalidation_loss = 0.0355 | val_SWA = 0.9924', '\\n', 'Epoch 16: validation_loss =\n0.0216 | val_SWA = 0.9948', '\\n', 'Epoch 17: validation_loss = 0.0523 | val_SWA\n= 0.9888', '\\n', 'Epoch 18: validation_loss = 0.0402 | val_SWA = 0.9907', '\\n',\n'Epoch 19: validation_loss = 0.0364 | val_SWA = 0.9918', '\\n', 'Epoch 20:\nvalidation_loss = 0.0366 | val_SWA = 0.9934', '\\n', 'Early stopping.', '\\n',\n'=== DEV  === loss 0.0216 | SWA 0.9948', '\\n', '=== TEST === loss 2.7824 | SWA\n0.6504', '\\n', 'Execution time: 41 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 476425.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 536520.67\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 734914.49\nexamples/s]', '\\n', 'Vocab size 18 | Classes 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=0.3130 |\nval_loss=0.1643 | val_SWA=0.9570', '\\n', 'Epoch 02: train_loss=0.1824 |\nval_loss=0.1051 | val_SWA=0.9695', '\\n', 'Epoch 03: train_loss=0.1265 |\nval_loss=0.0515 | val_SWA=0.9848', '\\n', 'Epoch 04: train_loss=0.1051 |\nval_loss=0.0484 | val_SWA=0.9870', '\\n', 'Epoch 05: train_loss=0.0995 |\nval_loss=0.0323 | val_SWA=0.9920', '\\n', 'Epoch 06: train_loss=0.0933 |\nval_loss=0.0535 | val_SWA=0.9856', '\\n', 'Epoch 07: train_loss=0.0828 |\nval_loss=0.0337 | val_SWA=0.9907', '\\n', 'Epoch 08: train_loss=0.0690 |\nval_loss=0.0385 | val_SWA=0.9887', '\\n', 'Epoch 09: train_loss=0.0631 |\nval_loss=0.0460 | val_SWA=0.9885', '\\n', 'Early stopping.', '\\n', '=== DEV  ===\nloss 0.0323 | SWA 0.9920', '\\n', '=== TEST === loss 2.2036 | SWA 0.6507', '\\n',\n'Execution time: 15 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocabulary size: 18 | Classes: 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=0.2020 |\nval_loss=0.1271 | val_SWA=0.9645', '\\n', 'Epoch 02: train_loss=0.0877 |\nval_loss=0.0811 | val_SWA=0.9783', '\\n', 'Epoch 03: train_loss=0.0639 |\nval_loss=0.0572 | val_SWA=0.9841', '\\n', 'Epoch 04: train_loss=0.0515 |\nval_loss=0.0445 | val_SWA=0.9877', '\\n', 'Epoch 05: train_loss=0.0383 |\nval_loss=0.0393 | val_SWA=0.9890', '\\n', 'Epoch 06: train_loss=0.0343 |\nval_loss=0.0438 | val_SWA=0.9849', '\\n', 'Epoch 07: train_loss=0.0360 |\nval_loss=0.1047 | val_SWA=0.9715', '\\n', 'Epoch 08: train_loss=0.0338 |\nval_loss=0.0153 | val_SWA=0.9951', '\\n', 'Epoch 09: train_loss=0.0261 |\nval_loss=0.0190 | val_SWA=0.9936', '\\n', 'Epoch 10: train_loss=0.0332 |\nval_loss=0.0307 | val_SWA=0.9906', '\\n', 'Epoch 11: train_loss=0.0308 |\nval_loss=0.0158 | val_SWA=0.9957', '\\n', 'Epoch 12: train_loss=0.0278 |\nval_loss=0.0221 | val_SWA=0.9938', '\\n', 'Epoch 13: train_loss=0.0532 |\nval_loss=0.0352 | val_SWA=0.9889', '\\n', 'Epoch 14: train_loss=0.0360 |\nval_loss=0.0370 | val_SWA=0.9894', '\\n', 'Early stopping triggered.', '\\n', '===\nDEV  === loss 0.0370 | SWA 0.9894', '\\n', '=== TEST === loss 2.6409 | SWA\n0.6530', '\\n', 'Execution time: 28 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocabulary size: 18 | Classes: 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=0.2181 |\nval_loss=0.1294 | val_SWA=0.9608', '\\n', 'Epoch 02: train_loss=0.1120 |\nval_loss=0.0725 | val_SWA=0.9763', '\\n', 'Epoch 03: train_loss=0.0654 |\nval_loss=0.0521 | val_SWA=0.9840', '\\n', 'Epoch 04: train_loss=0.0442 |\nval_loss=0.0403 | val_SWA=0.9873', '\\n', 'Epoch 05: train_loss=0.0311 |\nval_loss=0.0250 | val_SWA=0.9917', '\\n', 'Epoch 06: train_loss=0.0371 |\nval_loss=0.0336 | val_SWA=0.9905', '\\n', 'Epoch 07: train_loss=0.0223 |\nval_loss=0.0091 | val_SWA=0.9978', '\\n', 'Epoch 08: train_loss=0.0234 |\nval_loss=0.0326 | val_SWA=0.9900', '\\n', 'Epoch 09: train_loss=0.0331 |\nval_loss=0.0164 | val_SWA=0.9958', '\\n', 'Epoch 10: train_loss=0.0231 |\nval_loss=0.0151 | val_SWA=0.9951', '\\n', 'Early stopping triggered.', '\\n', '===\nDEV  === loss 0.0151 | SWA 0.9951', '\\n', '=== TEST === loss 2.6132 | SWA\n0.6539', '\\n', 'Execution time: 18 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocabulary size: 18 | Classes: 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01: train_loss=0.2220 |\nval_loss=0.1130 | val_SWA=0.9629', '\\n', 'Epoch 02: train_loss=0.0901 |\nval_loss=0.0610 | val_SWA=0.9832', '\\n', 'Epoch 03: train_loss=0.0526 |\nval_loss=0.0364 | val_SWA=0.9891', '\\n', 'Epoch 04: train_loss=0.0474 |\nval_loss=0.0349 | val_SWA=0.9877', '\\n', 'Epoch 05: train_loss=0.0337 |\nval_loss=0.0233 | val_SWA=0.9923', '\\n', 'Epoch 06: train_loss=0.0306 |\nval_loss=0.0214 | val_SWA=0.9924', '\\n', 'Epoch 07: train_loss=0.0348 |\nval_loss=0.0226 | val_SWA=0.9912', '\\n', 'Epoch 08: train_loss=0.0258 |\nval_loss=0.0241 | val_SWA=0.9922', '\\n', 'Epoch 09: train_loss=0.0294 |\nval_loss=0.0501 | val_SWA=0.9860', '\\n', 'Early stopping triggered.', '\\n', '===\nDEV  === loss 0.0501 | SWA 0.9860', '\\n', '=== TEST === loss 1.9258 | SWA\n0.6544', '\\n', 'Execution time: 14 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "The training script executed successfully without any bugs. The model achieved a\nhigh Shape-Weighted Accuracy (SWA) on the validation set (0.9891) and a\nreasonable SWA on the test set (0.6536). Early stopping was implemented\neffectively to prevent overfitting. The script is functioning as intended.", "", "", "The training script encountered a major issue where the Shape-Weighted Accuracy\n(SWA) on the test set was significantly lower (0.6223) compared to the\nvalidation set (0.9204). This indicates a potential overfitting problem where\nthe model performs well on the validation data but fails to generalize to unseen\ntest data. Additionally, the test loss (6.7581) is much higher than the\nvalidation loss (1.2101), further supporting this diagnosis. To address this,\nconsider implementing stronger regularization techniques such as dropout, weight\ndecay, or early stopping based on test performance. It may also be beneficial to\nanalyze the model's predictions on the test set to identify specific failure\ncases and adjust the model architecture or training process accordingly.", "There is a significant performance drop in Shape-Weighted Accuracy (SWA) on the\ntest set compared to the dev set (from 0.9449 to 0.6498). This discrepancy\nsuggests potential overfitting to the training and dev sets or a distribution\nmismatch between the dev and test sets. To address this issue, consider\nimplementing additional regularization techniques (e.g., dropout, weight decay),\naugmenting the training data, or analyzing the test set distribution for\nalignment issues.", "The training process and validation on the dev set performed exceptionally well,\nachieving a Shape-Weighted Accuracy (SWA) of 1.0000. However, the model's\nperformance on the test set dropped significantly, with an SWA of 0.6520. This\ndiscrepancy suggests severe overfitting to the training and dev datasets. To\naddress this issue, consider implementing regularization techniques such as\ndropout or weight decay, increasing the size of the dev set for better\nvalidation, or employing cross-validation to ensure that the model generalizes\nwell to unseen data.", "", "The training script has a significant issue related to the evaluation on the\ntest set. While the model achieves a high Shape-Weighted Accuracy (SWA) on the\nvalidation set (0.9920), the SWA on the test set is drastically lower (0.6507).\nThis discrepancy indicates potential overfitting to the validation set or a data\ndistribution mismatch between the training/validation and test sets. Another\npossible issue could be a bug in the test data preprocessing or evaluation\npipeline.  To fix this, consider the following steps: 1. Reassess the\npreprocessing pipeline for the test set to ensure it aligns with the training\nand validation data. 2. Implement regularization techniques, such as dropout or\nweight decay, to mitigate overfitting. 3. Perform data augmentation or ensure\nthe test set represents the same distribution as the training set. 4. Reevaluate\nthe metrics calculation for the test set to ensure there are no bugs in the SWA\ncomputation.", "", "The training script demonstrates a significant issue with overfitting. While the\nmodel achieves excellent Shape-Weighted Accuracy (SWA) on the validation set\n(0.9951), its performance on the test set is significantly lower (SWA of\n0.6539). This discrepancy suggests the model has not generalized well to unseen\ndata, which is critical for zero-shot reasoning tasks.  Proposed Fix: 1.\nIntroduce regularization techniques such as dropout or weight decay to reduce\noverfitting. 2. Increase the size of the validation set or use cross-validation\nto ensure better generalization. 3. Implement early stopping based on test set\nperformance rather than validation set performance. 4. Experiment with data\naugmentation techniques to improve model robustness.", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.16538991758823396, "best_value": 0.16538991758823396}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.16744024930000306, "best_value": 0.16744024930000306}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9446575979537263, "best_value": 0.9446575979537263}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9476542004758709, "best_value": 0.9476542004758709}]}, {"metric_name": "validation BPS", "lower_is_better": false, "description": "The BPS (Balanced Predictive Score) calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9461547128838366, "best_value": 0.9461547128838366}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0265, "best_value": 0.0265}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0229, "best_value": 0.0229}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9945, "best_value": 0.9945}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the error of the model on the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0208, "best_value": 0.0208}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, indicating the model's error on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0116, "best_value": 0.0116}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy metric weighted by shape on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9971, "best_value": 0.9971}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1646, "best_value": 0.1646}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1674, "best_value": 0.1674}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy calculated on the validation dataset, weighted by shape.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9445, "best_value": 0.9445}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.5223, "best_value": 1.5223}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6358, "best_value": 0.6358}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Stochastic Weight Averaging (SWA) accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9204, "best_value": 0.9204}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss value during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1706, "best_value": 0.1706}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1669, "best_value": 0.1669}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy metric weighted by shape during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9449, "best_value": 0.9449}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is learning from the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0095, "best_value": 0.0095}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, used to evaluate the model's performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0005, "best_value": 0.0005}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy metric weighted by shape, calculated during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.04012007169723511, "best_value": 0.04012007169723511}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.03656613399088383, "best_value": 0.03656613399088383}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9948261830019766, "best_value": 0.9948261830019766}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.063119, "best_value": 0.063119}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.046047, "best_value": 0.046047}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.988548, "best_value": 0.988548}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.036, "best_value": 0.036}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0153, "best_value": 0.0153}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy calculated during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9957, "best_value": 0.9957}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is fitting the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0231, "best_value": 0.0231}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, used to evaluate the model's performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0091, "best_value": 0.0091}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset, weighted by the importance of different shapes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9978, "best_value": 0.9978}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0294, "best_value": 0.0294}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0214, "best_value": 0.0214}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Represents the accuracy during validation, weighted by shape. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9924, "best_value": 0.9924}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_test_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_swa_curves.png", "../../logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_test_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_val_swa.png", "../../logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_test_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_swa_curve.png", "../../logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_test_confusion_matrix.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_swa_curve.png", "../../logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_test_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_val_swa.png", "../../logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_test_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_val_swa.png", "../../logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_test_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_c2e82714e0dc4838b20881b7701e8fdd/SPR_BENCH_agg_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_c2e82714e0dc4838b20881b7701e8fdd/SPR_BENCH_agg_val_swa.png", "../../logs/0-run/experiment_results/seed_aggregation_c2e82714e0dc4838b20881b7701e8fdd/datasets_best_val_swa_comparison_agg.png"]], "plot_paths": [["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_test_confusion_matrix.png"], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_swa_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_test_confusion_matrix.png"], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_val_swa.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_test_confusion_matrix.png"], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_swa_curve.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_test_confusion_matrix.png"], [], [], [], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_swa_curve.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_test_confusion_matrix.png"], [], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_val_swa.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_test_confusion_matrix.png"], [], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_val_swa.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_test_confusion_matrix.png"], ["experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_c2e82714e0dc4838b20881b7701e8fdd/SPR_BENCH_agg_loss_curves.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_c2e82714e0dc4838b20881b7701e8fdd/SPR_BENCH_agg_val_swa.png", "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_c2e82714e0dc4838b20881b7701e8fdd/datasets_best_val_swa_comparison_agg.png"]], "plot_analyses": [[{"analysis": "The training and validation loss curves show a steady decline, with both converging around epoch 7. This indicates that the model is learning effectively and is not overfitting, as the validation loss closely follows the training loss. The convergence of the curves suggests that the current number of epochs (10) is sufficient for training, and additional epochs might not yield significant improvements.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation metrics (SWA, CWA, and BPS) steadily improve over the first few epochs and plateau after epoch 7. This indicates that the model achieves high performance in these metrics, with scores nearing 0.95. The alignment of the three metrics suggests consistent performance across different evaluation criteria. Further fine-tuning of hyperparameters, such as learning rate or batch size, might marginally improve these scores.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The confusion matrix shows a relatively high number of true positives (3401) and true negatives (3550), with fewer false positives (2102) and false negatives (947). This indicates that the model performs well overall, but there is room for improvement in reducing false positives and false negatives. Adjusting the decision threshold or incorporating additional features might help to improve the balance between precision and recall.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_acdb2052614d4fe6a4e891a8d4567669_proc_2777399/SPR_BENCH_test_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss curves over the epochs. Both curves decrease steadily, indicating that the model is learning effectively. The convergence of the validation loss with the training loss suggests that the model is not overfitting. However, the slight fluctuations in the later epochs could imply some instability or sensitivity in the learning process, possibly due to the learning rate or model complexity.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot presents the validation Shape-Weighted Accuracy (SWA) over the epochs. The SWA improves consistently in the early epochs and stabilizes near 99%, which is a strong performance indicator. The slight oscillations towards the end suggest that the model's performance is robust but could benefit from additional fine-tuning to achieve even more stable results.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_swa_curves.png"}, {"analysis": "This confusion matrix illustrates the model's performance on the test set. The true negatives (3780) and true positives (3224) are significant, but the number of false positives (1872) and false negatives (1124) indicates room for improvement. The model appears to perform better at detecting the positive class compared to the negative class, which may suggest a bias or imbalance in the dataset or model's decision-making process.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3c987e2a74594eb1833a5105183718d8_proc_2780780/SPR_BENCH_test_confusion_matrix.png"}], [{"analysis": "The training vs. validation loss plot shows a consistent decrease in both training and validation loss over epochs, indicating effective learning. The convergence of the two curves suggests that the model is not overfitting, as validation loss does not increase or diverge from training loss significantly. However, there are some minor fluctuations in validation loss, which may indicate slight instability or sensitivity to certain data points.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Shape-Weighted Accuracy (SWA) plot demonstrates a steady improvement in accuracy over epochs, ultimately achieving near-perfect performance close to 0.995. This indicates that the model is effectively generalizing to unseen data and is well-suited for zero-shot reasoning tasks as hypothesized. The small fluctuations in the later epochs suggest the potential for further fine-tuning or regularization to stabilize performance.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_val_swa.png"}, {"analysis": "The test confusion matrix reveals that the model performs reasonably well, with 3809 true negatives, 3206 true positives, 1843 false positives, and 1142 false negatives. While the true positive and true negative counts are high, the false positive and false negative counts indicate areas for improvement. Particularly, reducing false positives and negatives could enhance the reliability of the model's predictions, especially for critical applications.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_3b0bee9eaafa4d9a80e00332424fee86_proc_2780781/SPR_BENCH_test_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss over epochs. The training loss decreases steadily, indicating that the model is learning effectively. The validation loss follows a similar trend, decreasing and stabilizing around epoch 10. This suggests that the model generalizes well to unseen data without significant overfitting.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_loss_curves.png"}, {"analysis": "The plot presents the validation Shape-Weighted Accuracy (SWA) over epochs. The SWA increases rapidly in the initial epochs and stabilizes at a high value (~0.94) after epoch 6. This indicates that the model achieves strong performance in capturing shape-related features for the task.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_swa_curve.png"}, {"analysis": "The confusion matrix provides detailed insights into the model's performance on the test set. The true positives (3396) and true negatives (3551) are relatively high, indicating good predictive capability. However, the false positives (2101) and false negatives (952) suggest that there is room for improvement in reducing misclassification.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_4f7f71a2cc844ee6b770ad4d8ccfadee_proc_2780782/SPR_BENCH_test_confusion_matrix.png"}], [], [], [], [{"analysis": "This plot shows the training and validation loss over 20 epochs. Both the training and validation loss decrease steadily, indicating that the model is learning effectively. The validation loss closely follows the training loss, suggesting minimal overfitting. By the end of training, both losses converge to very low values, indicating that the model has achieved good generalization.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the Shape-Weighted Accuracy (SWA) on the validation set over 20 epochs. The SWA improves consistently with training, reaching nearly 0.995. This indicates that the model is effectively learning shape-based rules and generalizing well to the validation data. The consistent improvement suggests a well-tuned training process.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_swa_curve.png"}, {"analysis": "The confusion matrix for the test set shows the distribution of true and predicted labels. The model correctly predicts a significant number of both classes, as evidenced by the high values in the diagonal cells. However, there are still a notable number of false positives (1865) and false negatives (1159). This indicates room for improvement in the model's precision and recall, particularly in distinguishing between the two classes.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_33a067abe473491b8732219b7b7e129a_proc_2780782/SPR_BENCH_test_confusion_matrix.png"}], [], [{"analysis": "The training and validation loss curves show a steady decrease during the training process, indicating that the model is learning effectively. However, there is a noticeable spike in the validation loss around epoch 8, which suggests some level of overfitting or instability in the model's generalization ability. After this spike, the validation loss stabilizes and aligns closely with the training loss, which is a positive sign for the model's performance.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Shape-Weighted Accuracy (SWA) improves consistently during the initial epochs, indicating that the model is effectively learning to generalize to unseen rules. However, there is a sudden drop in accuracy around epoch 8, corresponding to the spike in validation loss. This drop is followed by a recovery and stabilization, suggesting that the model successfully overcomes the instability and maintains high performance in subsequent epochs.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_val_swa.png"}, {"analysis": "The confusion matrix indicates that the model performs reasonably well on the test set, with a higher number of true positives (3222) and true negatives (3778) compared to false positives (1874) and false negatives (1126). However, the false positive rate appears to be significant, which may indicate that the model is over-predicting one of the classes. This could be an area for further improvement, possibly by fine-tuning hyperparameters or adjusting the decision threshold.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/SPR_BENCH_test_confusion_matrix.png"}], [], [{"analysis": "This plot shows the training and validation loss trends over nine epochs. The training loss decreases consistently, indicating the model is learning effectively from the training data. The validation loss also decreases initially, suggesting improved generalization, but it starts to slightly increase after epoch 7. This could indicate the onset of overfitting, where the model begins to memorize the training data rather than generalizing to unseen data. To mitigate this, early stopping or regularization techniques could be considered.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the validation Shape-Weighted Accuracy (SWA) over nine epochs. The SWA improves consistently from epoch 1 to epoch 5, reaching a peak between epochs 5 and 6. However, a slight decline is observed after epoch 6, which aligns with the increase in validation loss seen in the previous plot. This suggests that the model's ability to generalize to unseen rules starts to diminish slightly after epoch 6, reinforcing the need for strategies like early stopping to prevent overfitting.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_val_swa.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's performance on the test set. The true negative (3783) and true positive (3231) counts are relatively high, indicating the model performs well in correctly identifying both classes. However, there are notable false positives (1869) and false negatives (1117), suggesting areas for improvement in the model's precision and recall. Techniques like threshold tuning or incorporating additional features could help address these misclassifications.", "plot_path": "experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/SPR_BENCH_test_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The provided plots demonstrate effective training and evaluation of the model.\nThe training and validation loss curves indicate successful convergence without\noverfitting. The validation metrics (SWA, CWA, BPS) show strong and consistent\nperformance, with scores stabilizing near 0.95. The confusion matrix highlights\ngood predictive performance, though there is potential to reduce false positives\nand false negatives for further improvement.", "The plots indicate that the model is learning effectively and achieving strong\nperformance metrics, particularly in Shape-Weighted Accuracy. However, there are\nareas for improvement, such as reducing the fluctuations in loss and accuracy\nmetrics and addressing the misclassification rates observed in the confusion\nmatrix.", "The plots collectively indicate that the model is learning effectively and\ngeneralizing well to unseen data, as evidenced by the decreasing loss, high\nvalidation SWA, and reasonable performance in the confusion matrix. However,\nthere is room for improvement in reducing false positives and negatives, as well\nas stabilizing performance in later epochs.", "The plots demonstrate effective model training and strong performance in Shape-\nWeighted Accuracy (SWA). However, the confusion matrix highlights areas for\nimprovement in reducing misclassification rates.", "[]", "[]", "[]", "The provided plots demonstrate effective training and validation, with the model\nachieving high Shape-Weighted Accuracy on the validation set and reasonable\nperformance on the test set. While the confusion matrix highlights areas for\nimprovement in precision and recall, the overall results suggest that the model\nis learning effectively and generalizing well.", "[]", "The provided plots indicate that the model demonstrates good learning and\ngeneralization capabilities, with some instability observed during training. The\nvalidation accuracy and loss curves suggest that the model can recover from\noverfitting and maintain high performance. The confusion matrix highlights areas\nfor improvement in reducing false positives.", "[]", "The plots indicate that the model learns effectively during training and\nachieves strong performance on validation metrics like SWA. However, there are\nsigns of overfitting after a certain point, and the confusion matrix highlights\nareas where the model's classification accuracy can be improved further.\nRegularization and precision-recall optimization are recommended to enhance\ngeneralization and reduce misclassification rates.", "[]"], "exec_time": [10.37261414527893, 39.600584268569946, 22.02159571647644, 11.084468126296997, 21.864038944244385, 17.948672771453857, 25.67729902267456, 41.61542463302612, 15.770825624465942, 28.052886962890625, 18.427605628967285, 14.63776421546936, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[<list_of_dataset_names_from_experiment_data>]"], ["['experiment_data']"], [], [], [], ["['experiment_data']"], [], ["['dataset_1'", "'dataset_2'", "'dataset_3']"], [], ["['dataset1'", "'dataset2'", "'dataset3']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    spr = experiment_data[\"epochs_tuning\"][\"SPR_BENCH\"]\n    metrics = spr[\"metrics\"]\n    ep = np.arange(1, len(metrics[\"train_loss\"]) + 1)\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(ep, metrics[\"train_loss\"], label=\"Train Loss\")\n        plt.plot(ep, metrics[\"val_loss\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- 2. weighted accuracy / BPS ----------\n    try:\n        plt.figure()\n        plt.plot(ep, metrics[\"val_swa\"], label=\"Val SWA\")\n        plt.plot(ep, metrics[\"val_cwa\"], label=\"Val CWA\")\n        plt.plot(ep, metrics[\"val_bps\"], label=\"Val BPS\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH: Validation SWA, CWA, BPS\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix on test ----------\n    try:\n        preds = np.array(spr[\"predictions\"][\"test\"])\n        gts = np.array(spr[\"ground_truth\"][\"test\"])\n        classes = np.unique(np.concatenate([gts, preds]))\n        cm = np.zeros((len(classes), len(classes)), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.title(\"SPR_BENCH: Test Confusion Matrix\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.xticks(classes)\n        plt.yticks(classes)\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # ---------- print final metrics ----------\n    dev_idx = -1\n    print(\n        \"Final DEV metrics:\",\n        \"loss\",\n        metrics[\"val_loss\"][dev_idx],\n        \"SWA\",\n        metrics[\"val_swa\"][dev_idx],\n        \"CWA\",\n        metrics[\"val_cwa\"][dev_idx],\n        \"BPS\",\n        metrics[\"val_bps\"][dev_idx],\n    )\n\n    # test metrics are not stored in metrics, so compute simple ones from confusion matrix\n    total = cm.sum()\n    correct = np.trace(cm)\n    test_acc = correct / total if total else 0.0\n    print(\n        \"Final TEST metrics:\",\n        \"acc\",\n        round(test_acc, 4),\n        \"SWA\",\n        np.nan,  # not stored\n        \"CWA\",\n        np.nan,  # not stored\n        \"BPS\",\n        np.nan,\n    )  # not stored\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data and \"SPR_BENCH\" in experiment_data:\n    spr = experiment_data[\"SPR_BENCH\"]\n    metrics = spr[\"metrics\"]\n    ep = np.arange(1, len(metrics.get(\"train_loss\", [])) + 1)\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(ep, metrics[\"train_loss\"], label=\"Train Loss\")\n        plt.plot(ep, metrics[\"val_loss\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- 2. SWA curves ----------\n    try:\n        plt.figure()\n        plt.plot(ep, metrics[\"val_swa\"], label=\"Val SWA\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Shape-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation SWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix on test ----------\n    try:\n        preds = np.array(spr[\"predictions\"][\"test\"])\n        gts = np.array(spr[\"ground_truth\"][\"test\"])\n        classes = np.unique(np.concatenate([gts, preds]))\n        cm = np.zeros((len(classes), len(classes)), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.title(\"SPR_BENCH: Test Confusion Matrix\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.xticks(classes)\n        plt.yticks(classes)\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # ---------- print final metrics ----------\n    try:\n        dev_swa = metrics[\"val_swa\"][-1] if metrics[\"val_swa\"] else np.nan\n        total = cm.sum()\n        test_acc = np.trace(cm) / total if total else 0.0\n        print(f\"Final DEV SWA: {round(dev_swa, 4)}\")\n        print(f\"Final TEST accuracy: {round(test_acc, 4)}\")\n    except Exception as e:\n        print(f\"Error computing final metrics: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef downsample(arr, max_pts=200):\n    if len(arr) <= max_pts:\n        return np.arange(len(arr)), arr\n    idx = np.linspace(0, len(arr) - 1, max_pts, dtype=int)\n    return idx, np.array(arr)[idx]\n\n\ndatasets = list(experiment_data.keys())\nbest_swa_all = {}\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    ep = np.arange(1, len(metrics.get(\"train_loss\", [])) + 1)\n\n    # 1. loss curves -------------------------------------------------\n    try:\n        if ep.size:\n            ds_idx, t_loss = downsample(metrics.get(\"train_loss\", []))\n            _, v_loss = downsample(metrics.get(\"val_loss\", []))\n            plt.figure()\n            plt.plot(ep[ds_idx], t_loss, label=\"Train Loss\")\n            plt.plot(ep[ds_idx], v_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2. validation SWA curve ---------------------------------------\n    try:\n        if \"val_swa\" in metrics:\n            idx, v_swa = downsample(metrics[\"val_swa\"])\n            plt.figure()\n            plt.plot(ep[idx], v_swa, label=\"Val SWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(f\"{ds_name}: Validation SWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_val_swa.png\")\n            plt.savefig(fname)\n            best_swa_all[ds_name] = max(metrics[\"val_swa\"])\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3. confusion matrix on test -----------------------------------\n    try:\n        preds = np.array(ds_dict.get(\"predictions\", {}).get(\"test\", []))\n        gts = np.array(ds_dict.get(\"ground_truth\", {}).get(\"test\", []))\n        if preds.size and gts.size:\n            classes = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(f\"{ds_name}: Test Confusion Matrix\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.xticks(classes)\n            plt.yticks(classes)\n            for i in range(len(classes)):\n                for j in range(len(classes)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_test_confusion_matrix.png\")\n            plt.savefig(fname)\n            # simple accuracy\n            acc = np.trace(cm) / cm.sum() if cm.sum() else 0.0\n            print(\n                f\"{ds_name} | Best Val SWA: {best_swa_all.get(ds_name, np.nan):.4f} | Test Acc: {acc:.4f}\"\n            )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n# 4. cross-dataset comparison plot ---------------------------------\ntry:\n    if len(best_swa_all) > 1:\n        plt.figure()\n        names = list(best_swa_all.keys())\n        scores = [best_swa_all[n] for n in names]\n        plt.bar(names, scores)\n        plt.ylabel(\"Best Validation SWA\")\n        plt.title(\"Dataset Comparison: Best Validation SWA\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"datasets_best_val_swa_comparison.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating cross-dataset comparison plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    swa_per_ds = {}  # keep for later comparison plot\n    for ds_name, ds_blob in experiment_data.items():\n        metrics = ds_blob.get(\"metrics\", {})\n        train_loss = np.asarray(metrics.get(\"train_loss\", []), dtype=float)\n        val_loss = np.asarray(metrics.get(\"val_loss\", []), dtype=float)\n        val_swa = np.asarray(metrics.get(\"val_swa\", []), dtype=float)\n        epochs = np.arange(1, len(train_loss) + 1)\n\n        # ---------- 1. loss curves ----------\n        try:\n            plt.figure()\n            plt.plot(epochs, train_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds_name}: {e}\")\n            plt.close()\n\n        # ---------- 2. SWA curve ----------\n        try:\n            plt.figure()\n            plt.plot(epochs, val_swa, label=\"Val SWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(f\"{ds_name}: Validation SWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_swa_curve.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating SWA plot for {ds_name}: {e}\")\n            plt.close()\n\n        swa_per_ds[ds_name] = val_swa\n\n        # ---------- 3. confusion matrix ----------\n        try:\n            preds = np.asarray(ds_blob[\"predictions\"][\"test\"], dtype=int)\n            gts = np.asarray(ds_blob[\"ground_truth\"][\"test\"], dtype=int)\n            classes = np.arange(max(preds.max(initial=0), gts.max(initial=0)) + 1)\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(f\"{ds_name}: Test Confusion Matrix\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.xticks(classes)\n            plt.yticks(classes)\n            for i in range(len(classes)):\n                for j in range(len(classes)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_test_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n            plt.close()\n\n        # ---------- metrics print ----------\n        total = cm.sum() if \"cm\" in locals() else 0\n        acc = np.trace(cm) / total if total else 0.0\n        last_swa = val_swa[-1] if val_swa.size else np.nan\n        print(f\"[{ds_name}] TEST accuracy: {acc:.4f} | last Val-SWA: {last_swa:.4f}\")\n\n    # ---------- 4. combined SWA comparison ----------\n    if len(swa_per_ds) > 1:\n        try:\n            plt.figure()\n            for ds_name, swa in swa_per_ds.items():\n                ep = np.arange(1, len(swa) + 1)\n                plt.plot(ep, swa, label=f\"{ds_name}\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(\"Dataset Comparison: Validation SWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, \"all_datasets_swa_comparison.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating combined SWA plot: {e}\")\n            plt.close()\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- per-dataset plots ----------\nswa_curves = {}  # collect for optional comparison plot\nfor dname, ddata in experiment_data.items():\n    metrics = ddata.get(\"metrics\", {})\n    preds = ddata.get(\"predictions\", {})\n    gts = ddata.get(\"ground_truth\", {})\n    ep = np.arange(1, len(metrics.get(\"train_loss\", [])) + 1)\n\n    # --- 1. train / val loss ---\n    try:\n        plt.figure()\n        plt.plot(ep, metrics.get(\"train_loss\", []), label=\"Train Loss\")\n        plt.plot(ep, metrics.get(\"val_loss\", []), label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dname}: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # --- 2. validation SWA curve ---\n    try:\n        plt.figure()\n        plt.plot(ep, metrics.get(\"val_swa\", []), label=\"Val SWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Shape-Weighted Accuracy\")\n        plt.title(f\"{dname}: Validation SWA Curve\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_swa_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n        swa_curves[dname] = (ep, metrics.get(\"val_swa\", []))\n    except Exception as e:\n        print(f\"Error creating SWA plot for {dname}: {e}\")\n        plt.close()\n\n    # --- 3. confusion matrix on test ---\n    try:\n        t_preds = np.array(preds.get(\"test\", []))\n        t_gts = np.array(gts.get(\"test\", []))\n        if t_preds.size and t_gts.size:\n            classes = np.unique(np.concatenate([t_gts, t_preds]))\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for t, p in zip(t_gts, t_preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(f\"{dname}: Test Confusion Matrix\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.xticks(classes)\n            plt.yticks(classes)\n            for i in range(len(classes)):\n                for j in range(len(classes)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{dname}_test_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n\n            # print simple metrics\n            total = cm.sum()\n            acc = np.trace(cm) / total if total else 0.0\n            print(\n                f\"{dname} | Final VAL loss {metrics.get('val_loss', [np.nan])[-1]:.4f} \"\n                f\"| VAL SWA {metrics.get('val_swa', [np.nan])[-1]:.4f} \"\n                f\"| TEST acc {acc:.4f}\"\n            )\n        else:\n            print(f\"{dname}: Missing test predictions or labels; skipping CM.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n# ---------- 4. cross-dataset SWA comparison ----------\nif len(swa_curves) > 1:\n    try:\n        plt.figure()\n        for name, (ep, curve) in swa_curves.items():\n            plt.plot(ep, curve, label=name)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation SWA\")\n        plt.title(\"Dataset Comparison: Validation SWA Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"comparison_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating comparison plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef downsample(arr, max_pts=200):\n    if len(arr) <= max_pts:\n        return np.arange(len(arr)), arr\n    idx = np.linspace(0, len(arr) - 1, max_pts, dtype=int)\n    return idx, np.array(arr)[idx]\n\n\ndatasets = list(experiment_data.keys())\nbest_swa_all = {}\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    ep = np.arange(1, len(metrics.get(\"train_loss\", [])) + 1)\n\n    # 1. loss curves -------------------------------------------------\n    try:\n        if ep.size:\n            ds_idx, t_loss = downsample(metrics.get(\"train_loss\", []))\n            _, v_loss = downsample(metrics.get(\"val_loss\", []))\n            plt.figure()\n            plt.plot(ep[ds_idx], t_loss, label=\"Train Loss\")\n            plt.plot(ep[ds_idx], v_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2. validation SWA curve ---------------------------------------\n    try:\n        if \"val_swa\" in metrics:\n            idx, v_swa = downsample(metrics[\"val_swa\"])\n            plt.figure()\n            plt.plot(ep[idx], v_swa, label=\"Val SWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(f\"{ds_name}: Validation SWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_val_swa.png\")\n            plt.savefig(fname)\n            best_swa_all[ds_name] = max(metrics[\"val_swa\"])\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3. confusion matrix on test -----------------------------------\n    try:\n        preds = np.array(ds_dict.get(\"predictions\", {}).get(\"test\", []))\n        gts = np.array(ds_dict.get(\"ground_truth\", {}).get(\"test\", []))\n        if preds.size and gts.size:\n            classes = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(f\"{ds_name}: Test Confusion Matrix\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.xticks(classes)\n            plt.yticks(classes)\n            for i in range(len(classes)):\n                for j in range(len(classes)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_test_confusion_matrix.png\")\n            plt.savefig(fname)\n            # simple accuracy\n            acc = np.trace(cm) / cm.sum() if cm.sum() else 0.0\n            print(\n                f\"{ds_name} | Best Val SWA: {best_swa_all.get(ds_name, np.nan):.4f} | Test Acc: {acc:.4f}\"\n            )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n# 4. cross-dataset comparison plot ---------------------------------\ntry:\n    if len(best_swa_all) > 1:\n        plt.figure()\n        names = list(best_swa_all.keys())\n        scores = [best_swa_all[n] for n in names]\n        plt.bar(names, scores)\n        plt.ylabel(\"Best Validation SWA\")\n        plt.title(\"Dataset Comparison: Best Validation SWA\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"datasets_best_val_swa_comparison.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating cross-dataset comparison plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef downsample(arr, max_pts=200):\n    if len(arr) <= max_pts:\n        return np.arange(len(arr)), arr\n    idx = np.linspace(0, len(arr) - 1, max_pts, dtype=int)\n    return idx, np.array(arr)[idx]\n\n\ndatasets = list(experiment_data.keys())\nbest_swa_all = {}\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    ep = np.arange(1, len(metrics.get(\"train_loss\", [])) + 1)\n\n    # 1. loss curves -------------------------------------------------\n    try:\n        if ep.size:\n            ds_idx, t_loss = downsample(metrics.get(\"train_loss\", []))\n            _, v_loss = downsample(metrics.get(\"val_loss\", []))\n            plt.figure()\n            plt.plot(ep[ds_idx], t_loss, label=\"Train Loss\")\n            plt.plot(ep[ds_idx], v_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2. validation SWA curve ---------------------------------------\n    try:\n        if \"val_swa\" in metrics:\n            idx, v_swa = downsample(metrics[\"val_swa\"])\n            plt.figure()\n            plt.plot(ep[idx], v_swa, label=\"Val SWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(f\"{ds_name}: Validation SWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_val_swa.png\")\n            plt.savefig(fname)\n            best_swa_all[ds_name] = max(metrics[\"val_swa\"])\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3. confusion matrix on test -----------------------------------\n    try:\n        preds = np.array(ds_dict.get(\"predictions\", {}).get(\"test\", []))\n        gts = np.array(ds_dict.get(\"ground_truth\", {}).get(\"test\", []))\n        if preds.size and gts.size:\n            classes = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(f\"{ds_name}: Test Confusion Matrix\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.xticks(classes)\n            plt.yticks(classes)\n            for i in range(len(classes)):\n                for j in range(len(classes)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_test_confusion_matrix.png\")\n            plt.savefig(fname)\n            # simple accuracy\n            acc = np.trace(cm) / cm.sum() if cm.sum() else 0.0\n            print(\n                f\"{ds_name} | Best Val SWA: {best_swa_all.get(ds_name, np.nan):.4f} | Test Acc: {acc:.4f}\"\n            )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n# 4. cross-dataset comparison plot ---------------------------------\ntry:\n    if len(best_swa_all) > 1:\n        plt.figure()\n        names = list(best_swa_all.keys())\n        scores = [best_swa_all[n] for n in names]\n        plt.bar(names, scores)\n        plt.ylabel(\"Best Validation SWA\")\n        plt.title(\"Dataset Comparison: Best Validation SWA\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"datasets_best_val_swa_comparison.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating cross-dataset comparison plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef downsample(arr, max_pts=200):\n    if len(arr) <= max_pts:\n        return np.arange(len(arr)), arr\n    idx = np.linspace(0, len(arr) - 1, max_pts, dtype=int)\n    return idx, np.array(arr)[idx]\n\n\ndatasets = list(experiment_data.keys())\nbest_swa_all = {}\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    ep = np.arange(1, len(metrics.get(\"train_loss\", [])) + 1)\n\n    # 1. loss curves -------------------------------------------------\n    try:\n        if ep.size:\n            ds_idx, t_loss = downsample(metrics.get(\"train_loss\", []))\n            _, v_loss = downsample(metrics.get(\"val_loss\", []))\n            plt.figure()\n            plt.plot(ep[ds_idx], t_loss, label=\"Train Loss\")\n            plt.plot(ep[ds_idx], v_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Training vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2. validation SWA curve ---------------------------------------\n    try:\n        if \"val_swa\" in metrics:\n            idx, v_swa = downsample(metrics[\"val_swa\"])\n            plt.figure()\n            plt.plot(ep[idx], v_swa, label=\"Val SWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(f\"{ds_name}: Validation SWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_val_swa.png\")\n            plt.savefig(fname)\n            best_swa_all[ds_name] = max(metrics[\"val_swa\"])\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3. confusion matrix on test -----------------------------------\n    try:\n        preds = np.array(ds_dict.get(\"predictions\", {}).get(\"test\", []))\n        gts = np.array(ds_dict.get(\"ground_truth\", {}).get(\"test\", []))\n        if preds.size and gts.size:\n            classes = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(classes), len(classes)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(f\"{ds_name}: Test Confusion Matrix\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.xticks(classes)\n            plt.yticks(classes)\n            for i in range(len(classes)):\n                for j in range(len(classes)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_test_confusion_matrix.png\")\n            plt.savefig(fname)\n            # simple accuracy\n            acc = np.trace(cm) / cm.sum() if cm.sum() else 0.0\n            print(\n                f\"{ds_name} | Best Val SWA: {best_swa_all.get(ds_name, np.nan):.4f} | Test Acc: {acc:.4f}\"\n            )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n# 4. cross-dataset comparison plot ---------------------------------\ntry:\n    if len(best_swa_all) > 1:\n        plt.figure()\n        names = list(best_swa_all.keys())\n        scores = [best_swa_all[n] for n in names]\n        plt.bar(names, scores)\n        plt.ylabel(\"Best Validation SWA\")\n        plt.title(\"Dataset Comparison: Best Validation SWA\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"datasets_best_val_swa_comparison.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating cross-dataset comparison plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom math import sqrt\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- experiment paths ----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_313b21f24c984907af3035ec4db733a8_proc_2780780/experiment_data.npy\",\n    \"experiments/2025-08-14_23-40-39_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_f05e7c1370ce4f12b8bf8093cf6d7120_proc_2780781/experiment_data.npy\",\n    # the \u201cNone/\u2026\u201d entry is ignored if it does not exist\n]\n\n# ---------- load all runs ----------\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        if not os.path.isfile(full_path):\n            continue\n        exp_d = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_d)\n    except Exception as e:\n        print(f\"Error loading experiment data from {p}: {e}\")\n\nif not all_experiment_data:\n    print(\"No experiment files loaded \u2013 nothing to plot.\")\n    exit()\n\n\n# ---------- helper ----------\ndef downsample(arr, max_pts=200):\n    if len(arr) <= max_pts:\n        return np.arange(len(arr)), arr\n    idx = np.linspace(0, len(arr) - 1, max_pts, dtype=int)\n    return idx, np.array(arr)[idx]\n\n\ndef aggregate_time_series(series_list):\n    \"\"\"Pad each 1-D array with np.nan to the maximum length, return mean & sem.\"\"\"\n    max_len = max(len(s) for s in series_list)\n    data = np.full((len(series_list), max_len), np.nan)\n    for r, s in enumerate(series_list):\n        data[r, : len(s)] = s\n    mean = np.nanmean(data, axis=0)\n    sem = np.nanstd(data, axis=0) / np.sqrt(np.sum(~np.isnan(data), axis=0))\n    return mean, sem\n\n\n# ---------- collect all datasets ----------\ndataset_names = set()\nfor run_data in all_experiment_data:\n    dataset_names.update(run_data.keys())\n\nbest_swa_summary = {}  # dataset -> list of best swa values across runs\n\n# ---------- per-dataset plots ----------\nfor ds_name in dataset_names:\n    # collect metrics from each run that contains this dataset\n    train_losses, val_losses, val_swas = [], [], []\n    for run_data in all_experiment_data:\n        ds_dict = run_data.get(ds_name)\n        if ds_dict is None:\n            continue\n        metrics = ds_dict.get(\"metrics\", {})\n        if \"train_loss\" in metrics:\n            train_losses.append(np.asarray(metrics[\"train_loss\"], dtype=float))\n        if \"val_loss\" in metrics:\n            val_losses.append(np.asarray(metrics[\"val_loss\"], dtype=float))\n        if \"val_swa\" in metrics:\n            val_swas.append(np.asarray(metrics[\"val_swa\"], dtype=float))\n            best_swa_summary.setdefault(ds_name, []).append(\n                np.nanmax(metrics[\"val_swa\"])\n            )\n\n    # ----- 1. aggregated loss curves -----\n    try:\n        if train_losses and val_losses:\n            mean_train, sem_train = aggregate_time_series(train_losses)\n            mean_val, sem_val = aggregate_time_series(val_losses)\n            ep = np.arange(1, len(mean_train) + 1)\n            idx, _ = downsample(mean_train)  # reuse for val so epochs line up\n            plt.figure()\n            plt.plot(ep[idx], mean_train[idx], label=\"Train Loss (mean)\")\n            plt.fill_between(\n                ep[idx],\n                (mean_train - sem_train)[idx],\n                (mean_train + sem_train)[idx],\n                alpha=0.3,\n                label=\"Train SEM\",\n            )\n            plt.plot(ep[idx], mean_val[idx], label=\"Val Loss (mean)\")\n            plt.fill_between(\n                ep[idx],\n                (mean_val - sem_val)[idx],\n                (mean_val + sem_val)[idx],\n                alpha=0.3,\n                label=\"Val SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\n                f\"{ds_name}: Aggregated Training vs Validation Loss\\n(Mean \u00b1 SEM over {len(train_losses)} runs)\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_loss_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ----- 2. aggregated validation SWA -----\n    try:\n        if val_swas:\n            mean_swa, sem_swa = aggregate_time_series(val_swas)\n            ep = np.arange(1, len(mean_swa) + 1)\n            idx, _ = downsample(mean_swa)\n            plt.figure()\n            plt.plot(ep[idx], mean_swa[idx], label=\"Val SWA (mean)\")\n            plt.fill_between(\n                ep[idx],\n                (mean_swa - sem_swa)[idx],\n                (mean_swa + sem_swa)[idx],\n                alpha=0.3,\n                label=\"Val SWA SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Shape-Weighted Accuracy\")\n            plt.title(\n                f\"{ds_name}: Aggregated Validation SWA\\n(Mean \u00b1 SEM over {len(val_swas)} runs)\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_val_swa.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated SWA plot for {ds_name}: {e}\")\n        plt.close()\n\n# ---------- 3. dataset comparison of best SWA (bar plot) ----------\ntry:\n    if best_swa_summary:\n        names = sorted(best_swa_summary.keys())\n        means = [np.mean(best_swa_summary[n]) for n in names]\n        sems = [\n            np.std(best_swa_summary[n]) / sqrt(len(best_swa_summary[n])) for n in names\n        ]\n        plt.figure()\n        plt.bar(range(len(names)), means, yerr=sems, capsize=5)\n        plt.xticks(range(len(names)), names, rotation=45, ha=\"right\")\n        plt.ylabel(\"Best Validation SWA\")\n        plt.title(\"Dataset Comparison: Best Validation SWA (Mean \u00b1 SEM across runs)\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"datasets_best_val_swa_comparison_agg.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated cross-dataset comparison plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We will load experiment_data.npy, grab the metrics/predictions stored under epochs_tuning \u2192 SPR_BENCH, and pre-compute the number of epochs.  Three plots are produced: (1) training vs validation loss, (2) validation SWA/CWA/BPS curves, and (3) a test-set confusion matrix derived from the stored predictions/ground-truth.  Each figure is wrapped in its own try-except block, saved to the working directory with descriptive names, then closed to free memory.  We respect the \u201cat most five figures\u201d rule by limiting ourselves to these three essentials.  Titles clearly specify dataset and what is shown; axis labels and legends are added where helpful.  After plotting, we print final dev and test metrics (loss, SWA, CWA, BPS) fetched directly from the experiment_data structure so nothing is recomputed from scratch except the confusion matrix.  The code uses only numpy and matplotlib, following the basic style guideline, and ensures the working_dir exists before saving.  All logic is compact but readable.", "We will load the saved experiment_data.npy, grab the SPR_BENCH metrics and prediction arrays, and then create three key visualizations: (1) a line plot of train vs validation loss across epochs, (2) a line plot of validation shape-weighted accuracy (SWA), and (3) a confusion-matrix heatmap for the test split. Each figure is wrapped in its own try-except block to guard against missing keys or plotting errors, is titled clearly, saved to working_dir with descriptive filenames, and the figure is always closed. After plotting, we compute and print two simple evaluation statistics: the last recorded DEV SWA and the overall TEST accuracy derived from the confusion matrix. Only data present in experiment_data.npy are accessed\u2014no extra fields or fabricated values. The code is concise, starts with the required imports and working_dir definition, follows the naming conventions, and keeps the number of figures well below the five-figure cap. The entire procedure should run quickly and produce portable PNGs suitable for later inspection.", "We first load experiment_data.npy and enumerate every dataset key inside.  \nFor each dataset we extract the epoch vector and metrics dict; then, in an isolated try-except block, we draw (1) train vs. validation loss, (2) validation shape-weighted accuracy, and (3) a test-set confusion matrix, always saving figures into working_dir with descriptive names and closing figures afterwards.  \nIf fewer than two datasets are present we skip cross-dataset comparisons; otherwise we create an additional plot that shows the best validation SWA achieved by each dataset to give a quick performance overview.  \nAll numeric accesses (e.g., metrics[\"val_swa\"]) are wrapped in get calls with defaults so the script never crashes when a field is missing.  \nWhen plotting many epochs we down-sample to at most 200 points to keep figures readable, but we never create more than five epoch-sampled confusion matrices (only the final one by default).  \nEvery figure receives a clear title containing the dataset name and description, and we insert axis labels and legends where appropriate.  \nFinally, after producing plots, we print the best validation SWA and simple test accuracy for each dataset so the notebook logs include the main quantitative results.", "We first load the saved numpy dictionary, loop over every dataset contained in it, and in each iteration build three visuals: (1) train vs validation loss curves, (2) validation\u2010SWA evolution, and (3) a test\u2013set confusion matrix.  Every figure is wrapped in its own try-except so failures never stop the script, and each is closed after saving to avoid memory leaks.  When multiple datasets are present we additionally build one combined SWA comparison plot after the per-dataset loop.  All plots are titled clearly and filenames follow the pattern \u201c<dataset>_<plot-type>.png\u201d in the working directory.  For metrics we derive the overall test accuracy from the confusion matrix and print it together with the last recorded SWA for quick inspection.  The code respects the provided base snippet, keeps to plain matplotlib, and never invents data\u2014only values loaded from experiment_data.npy are used.  Finally, everything is kept compact so the whole script can be dropped into a single cell or file and run as-is.", null, null, null, "We will load the saved numpy dict, iterate over every dataset it contains, and for each one: (1) derive an epoch index vector, (2) plot train vs-validation loss, (3) plot the validation shape-weighted accuracy (SWA), and (4) build a confusion matrix on the test split.  Each figure is wrapped in its own try-except so that any failure is isolated; figures are always closed and saved to working_dir with descriptive names that include the dataset name and plot type.  After plotting, we print the last recorded validation loss/SWA as well as overall test accuracy derived from the confusion matrix.  If more than one dataset exists we also create a single comparison plot showing every dataset\u2019s SWA curve so users can visually compare performance across datasets; this comparison plot is created only once and at most five total figures are emitted.  All plotting relies solely on numbers present in experiment_data.npy\u2014no synthetic data are introduced.  Code is concise, starts with the required imports, and follows every formatting guideline.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["EPOCHS", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will locate the working directory, load the saved NumPy dictionary,\nand iterate through every dataset stored under the \u201cepochs_tuning\u201d key. For each\ndataset it will read the metric lists, fetch the final value recorded during\ntraining, and print them with explicit, human-readable names such as \u201ctrain\nloss\u201d or \u201cvalidation BPS.\u201d Empty or missing metrics are handled gracefully by\nreporting \u201cN/A.\u201d No code is wrapped in an `if __name__ == \"__main__\":` guard so\nthe program executes immediately when run.", "The script below loads the saved NumPy dictionary from the working directory,\niterates over every dataset present, and prints a concise summary of the most\nrelevant value for each recorded metric. For loss-based metrics the minimum\nvalue is reported (best), while for accuracy-like metrics the maximum value is\nreported. Metric keys are converted into clear, human-readable labels before\nprinting.", "Below is a concise script that immediately loads the saved NumPy file, extracts\nthe recorded lists of losses/accuracies, selects the appropriate \u201cbest\u201d (min\nloss / max accuracy) or \u201cfinal\u201d (last training loss) values, and prints them\nwith explicit metric names for each dataset found in the file.", "We first load experiment_data.npy from the \u201cworking\u201d sub-directory, recover the\nstored dictionary, and iterate over every dataset it contains.   For each metric\narray we decide what \u201cbest\u201d means: losses are minimized while accuracy\u2013like\nscores are maximized.   We translate metric keys (train_loss, val_loss, val_swa,\n\u2026) into clear human-readable names (\u201ctraining loss\u201d, \u201cvalidation shape-weighted\naccuracy\u201d, etc.) before printing.   Finally, we print one line per metric under\nthe relevant dataset heading; the script executes immediately without any\nspecial entry point.", "The script loads the saved NumPy dictionary, walks through each dataset entry,\nand inspects the stored metric lists. For every metric it selects the \u201cbest\u201d\nvalue\u2014minimum for any loss metric and maximum for any accuracy-like metric (a\nsimple name check is used). It then prints the dataset name followed by each\nmetric\u2019s descriptive name and its best value, formatted to four decimals.\nEverything runs immediately on execution; no special entry-point guard or\nplotting is included.", "The script loads the saved NumPy dictionary from the \u201cworking\u201d directory,\niterates over every dataset it contains (e.g., SPR_BENCH), and inspects the\nmetrics list stored during training. For each metric it decides the \u201cbest\u201d\nvalue\u2014minimum for losses and maximum for accuracy-like metrics\u2014then prints the\ndataset name followed by the metric name and its selected value. All logic is in\nglobal scope so the file runs immediately without any special entry point, and\nno plots are generated. The code also includes a small mapping to replace raw\nkeys with clearer, reader-friendly metric names.", "Below is a compact plan followed by executable code.   The script loads the\nsaved NumPy dictionary from the working directory, iterates over every stored\ndataset (e.g., \u201cSPR_BENCH\u201d), and prints \u2013 for each metric \u2013 the best observed\nvalue (minimum for losses, maximum for accuracy). All logic is placed at the\nglobal level so the file runs immediately when executed.", "Below is a compact script that immediately loads the saved NumPy file, extracts\nthe metric lists for every stored dataset, and prints the final training /\nvalidation losses and the best validation shape-weighted accuracy in a clear,\nlabelled format.", "The script will load experiment_data.npy from the \u201cworking\u201d directory, convert\nthe numpy object into a Python dictionary, and iterate over each dataset entry\n(e.g., \u201cSPR_BENCH\u201d).   For every dataset it will access the \u201cmetrics\u201d sub-\ndictionary, pick the final recorded value in each metric list, translate the raw\nkey names into clear descriptions (e.g., \u201ctrain_loss\u201d \u2192 \u201ctraining loss\u201d), and\nprint them.   No plots are generated and the code lives at global scope so the\nfile runs immediately after execution.", "Below is a concise script that immediately loads the saved NumPy file, extracts\nthe recorded lists of losses/accuracies, selects the appropriate \u201cbest\u201d (min\nloss / max accuracy) or \u201cfinal\u201d (last training loss) values, and prints them\nwith explicit metric names for each dataset found in the file.", "Below is a concise script that immediately loads the saved NumPy file, extracts\nthe recorded lists of losses/accuracies, selects the appropriate \u201cbest\u201d (min\nloss / max accuracy) or \u201cfinal\u201d (last training loss) values, and prints them\nwith explicit metric names for each dataset found in the file.", "Below is a concise script that immediately loads the saved NumPy file, extracts\nthe recorded lists of losses/accuracies, selects the appropriate \u201cbest\u201d (min\nloss / max accuracy) or \u201cfinal\u201d (last training loss) values, and prints them\nwith explicit metric names for each dataset found in the file.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ----------------- Locate and load the experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------------- Mapping raw keys to readable metric names -----------------\nREADABLE_NAMES = {\n    \"train_loss\": \"train loss\",\n    \"val_loss\": \"validation loss\",\n    \"val_swa\": \"validation shape-weighted accuracy\",\n    \"val_cwa\": \"validation color-weighted accuracy\",\n    \"val_bps\": \"validation BPS\",\n}\n\n\ndef final_or_best(values):\n    \"\"\"Return the final recorded value or 'N/A' if list is empty\"\"\"\n    return values[-1] if values else \"N/A\"\n\n\n# ----------------- Iterate and print metrics -----------------\nepochs_tuning = experiment_data.get(\"epochs_tuning\", {})\nif not epochs_tuning:\n    print(\"No epochs_tuning data found.\")\nelse:\n    for dataset_name, data in epochs_tuning.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        metrics = data.get(\"metrics\", {})\n        for raw_key, values in metrics.items():\n            readable = READABLE_NAMES.get(raw_key, raw_key)\n            value = final_or_best(values)\n            print(f\"{readable}: {value}\")\n", "import os\nimport numpy as np\n\n\n# ---------- helpers ----------------------------------------------------------\ndef readable_metric_name(key: str) -> str:\n    \"\"\"Map raw metric keys to descriptive names.\"\"\"\n    mapping = {\n        \"train_loss\": \"training loss\",\n        \"val_loss\": \"validation loss\",\n        \"val_swa\": \"validation shape weighted accuracy\",\n    }\n    return mapping.get(key, key.replace(\"_\", \" \"))\n\n\ndef best_value(key: str, values):\n    \"\"\"Return best (min for loss, max otherwise) value from a list.\"\"\"\n    if not values:  # empty list guard\n        return None\n    return min(values) if \"loss\" in key else max(values)\n\n\n# ---------- main logic -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(dataset_name)  # dataset header\n    metrics = dataset_dict.get(\"metrics\", {})\n    for metric_key, metric_values in metrics.items():\n        best = best_value(metric_key, metric_values)\n        if best is None:\n            continue\n        print(f\"{readable_metric_name(metric_key)}: {best:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------- load experiment data -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------- helper to print metrics ----------------\ndef print_metrics(name: str, metrics: dict):\n    # Retrieve metric lists\n    train_losses = metrics.get(\"train_loss\", [])\n    val_losses = metrics.get(\"val_loss\", [])\n    val_swa = metrics.get(\"val_swa\", [])\n\n    # Guard against empty lists\n    if not (train_losses and val_losses and val_swa):\n        print(f\"{name}: No metric data found.\")\n        return\n\n    # Compute required values\n    final_train_loss = train_losses[-1]  # last recorded value\n    best_validation_loss = min(val_losses)  # lower is better\n    best_validation_swa = max(val_swa)  # higher is better\n\n    # Print results\n    print(name)  # dataset name\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Best validation loss: {best_validation_loss:.4f}\")\n    print(f\"Best validation shape-weighted accuracy: {best_validation_swa:.4f}\")\n    print()  # blank line for readability\n\n\n# ------------------- iterate over datasets -----------------\nfor dataset_name, dataset_info in experiment_data.items():\n    metrics_dict = dataset_info.get(\"metrics\", {})\n    print_metrics(dataset_name, metrics_dict)\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef best_value(metric_name, values):\n    \"\"\"Return the best (min or max) value for a metric list.\"\"\"\n    if not values:\n        return None\n    if \"loss\" in metric_name.lower():\n        return min(values)  # lower is better for losses\n    return max(values)  # higher is better for scores/accuracies\n\n\ndef pretty_name(metric_key):\n    \"\"\"Convert raw key names to descriptive metric names.\"\"\"\n    mapping = {\n        \"train_loss\": \"training loss\",\n        \"val_loss\": \"validation loss\",\n        \"test_loss\": \"test loss\",\n        \"train_acc\": \"training accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"test_acc\": \"test accuracy\",\n        \"val_swa\": \"validation shape-weighted accuracy\",\n        \"test_swa\": \"test shape-weighted accuracy\",\n    }\n    return mapping.get(metric_key, metric_key.replace(\"_\", \" \"))\n\n\n# ---------- print results ----------\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(dataset_name)\n    metrics_dict = dataset_dict.get(\"metrics\", {})\n    for metric_key, metric_values in metrics_dict.items():\n        value = best_value(metric_key, metric_values)\n        if value is None:\n            continue\n        print(f\"  {pretty_name(metric_key)}: {value:.4f}\")\n", "import os\nimport re\nimport numpy as np\n\n# ---------- locate and load the experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"experiment_data.npy not found at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to pick best metric value ----------\ndef best_value(name, values):\n    if not values:\n        return None\n    name_low = name.lower()\n    if \"loss\" in name_low:  # lower is better for losses\n        return min(values)\n    # treat accuracy / swa / f1 etc. as higher-is-better\n    return max(values)\n\n\n# ---------- pretty-print results ----------\nfor dataset, content in experiment_data.items():\n    print(f\"Dataset: {dataset}\")\n    metrics = content.get(\"metrics\", {})\n    for raw_name, vals in metrics.items():\n        best = best_value(raw_name, vals)\n        if best is None:\n            continue\n        # build a readable metric name\n        nice_name = raw_name\n        nice_name = re.sub(r\"\\btrain\\b\", \"training\", nice_name)\n        nice_name = re.sub(r\"\\bval\\b\", \"validation\", nice_name)\n        nice_name = nice_name.replace(\"_\", \" \").strip()\n        print(f\"{nice_name}: {best:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# -------------------- locate and load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------- pretty names for metrics ---------------------------\npretty_name = {\n    \"train_loss\": \"train loss\",\n    \"val_loss\": \"validation loss\",\n    \"val_swa\": \"validation shape-weighted accuracy\",\n}\n\n\ndef best_metric_value(key, values):\n    \"\"\"Return best value: min for losses, max otherwise.\"\"\"\n    if not values:  # empty list guard\n        return None\n    return min(values) if \"loss\" in key else max(values)\n\n\n# -------------------- print results --------------------------------------\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    metrics = dataset_dict.get(\"metrics\", {})\n    for key, values in metrics.items():\n        best_val = best_metric_value(key, values)\n        if best_val is None:\n            continue\n        metric_label = pretty_name.get(key, key.replace(\"_\", \" \"))\n        print(f\"{metric_label}: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to decide best ----------\ndef best_value(metric_name, values):\n    \"\"\"Return best (min for loss, max for others) from a list of floats.\"\"\"\n    if not values:  # empty safety\n        return None\n    is_loss = \"loss\" in metric_name.lower()  # simple heuristic\n    return min(values) if is_loss else max(values)\n\n\n# ---------- print results ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    metrics = content.get(\"metrics\", {})\n\n    train_loss_best = best_value(\"train_loss\", metrics.get(\"train_loss\", []))\n    if train_loss_best is not None:\n        print(f\"Best training loss: {train_loss_best:.4f}\")\n\n    val_loss_best = best_value(\"val_loss\", metrics.get(\"val_loss\", []))\n    if val_loss_best is not None:\n        print(f\"Best validation loss: {val_loss_best:.4f}\")\n\n    val_swa_best = best_value(\"val_swa\", metrics.get(\"val_swa\", []))\n    if val_swa_best is not None:\n        print(f\"Best validation shape-weighted accuracy: {val_swa_best:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------\n# locate and load the experiment data\n# ---------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------\n# helper to fetch final / best values from the metric lists\n# ---------------------------------------------------------------\ndef final_value(lst):\n    \"\"\"Return the last element of a list or None if empty.\"\"\"\n    return lst[-1] if lst else None\n\n\ndef best_value(lst, higher_is_better=True):\n    \"\"\"Return best (max or min) element of a list or None if empty.\"\"\"\n    if not lst:\n        return None\n    return max(lst) if higher_is_better else min(lst)\n\n\n# ---------------------------------------------------------------\n# iterate through each stored dataset and print the requested stats\n# ---------------------------------------------------------------\nfor dataset_name, content in experiment_data.items():\n    metrics = content.get(\"metrics\", {})\n\n    # Extract lists (may be missing if experiment aborted early)\n    train_loss_list = metrics.get(\"train_loss\", [])\n    val_loss_list = metrics.get(\"val_loss\", [])\n    val_swa_list = metrics.get(\"val_swa\", [])\n\n    print(dataset_name)\n    print(f\"Final training loss: {final_value(train_loss_list)}\")\n    print(f\"Final validation loss: {final_value(val_loss_list)}\")\n    print(\n        f\"Best validation shape-weighted accuracy: {best_value(val_swa_list, higher_is_better=True)}\\n\"\n    )\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"experiment_data.npy not found in {working_dir}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ---------- helpers ----------\nnice_names = {\n    \"train_loss\": \"training loss\",\n    \"val_loss\": \"validation loss\",\n    \"val_swa\": \"validation shape-weighted accuracy\",\n}\n\n\ndef print_metric(dataset_name, metric_key, values):\n    if not values:\n        return\n    label = nice_names.get(metric_key, metric_key)\n    final_val = values[-1]  # use last recorded value\n    print(\n        f\"  {label}: {final_val:.6f}\"\n        if isinstance(final_val, float)\n        else f\"  {label}: {final_val}\"\n    )\n\n\n# ---------- iterate and report ----------\nfor dataset, content in experiment_data.items():\n    print(f\"Dataset: {dataset}\")\n    metrics = content.get(\"metrics\", {})\n    for key, vals in metrics.items():\n        print_metric(dataset, key, vals)\n", "import os\nimport numpy as np\n\n# ------------------- load experiment data -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------- helper to print metrics ----------------\ndef print_metrics(name: str, metrics: dict):\n    # Retrieve metric lists\n    train_losses = metrics.get(\"train_loss\", [])\n    val_losses = metrics.get(\"val_loss\", [])\n    val_swa = metrics.get(\"val_swa\", [])\n\n    # Guard against empty lists\n    if not (train_losses and val_losses and val_swa):\n        print(f\"{name}: No metric data found.\")\n        return\n\n    # Compute required values\n    final_train_loss = train_losses[-1]  # last recorded value\n    best_validation_loss = min(val_losses)  # lower is better\n    best_validation_swa = max(val_swa)  # higher is better\n\n    # Print results\n    print(name)  # dataset name\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Best validation loss: {best_validation_loss:.4f}\")\n    print(f\"Best validation shape-weighted accuracy: {best_validation_swa:.4f}\")\n    print()  # blank line for readability\n\n\n# ------------------- iterate over datasets -----------------\nfor dataset_name, dataset_info in experiment_data.items():\n    metrics_dict = dataset_info.get(\"metrics\", {})\n    print_metrics(dataset_name, metrics_dict)\n", "import os\nimport numpy as np\n\n# ------------------- load experiment data -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------- helper to print metrics ----------------\ndef print_metrics(name: str, metrics: dict):\n    # Retrieve metric lists\n    train_losses = metrics.get(\"train_loss\", [])\n    val_losses = metrics.get(\"val_loss\", [])\n    val_swa = metrics.get(\"val_swa\", [])\n\n    # Guard against empty lists\n    if not (train_losses and val_losses and val_swa):\n        print(f\"{name}: No metric data found.\")\n        return\n\n    # Compute required values\n    final_train_loss = train_losses[-1]  # last recorded value\n    best_validation_loss = min(val_losses)  # lower is better\n    best_validation_swa = max(val_swa)  # higher is better\n\n    # Print results\n    print(name)  # dataset name\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Best validation loss: {best_validation_loss:.4f}\")\n    print(f\"Best validation shape-weighted accuracy: {best_validation_swa:.4f}\")\n    print()  # blank line for readability\n\n\n# ------------------- iterate over datasets -----------------\nfor dataset_name, dataset_info in experiment_data.items():\n    metrics_dict = dataset_info.get(\"metrics\", {})\n    print_metrics(dataset_name, metrics_dict)\n", "import os\nimport numpy as np\n\n# ------------------- load experiment data -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------- helper to print metrics ----------------\ndef print_metrics(name: str, metrics: dict):\n    # Retrieve metric lists\n    train_losses = metrics.get(\"train_loss\", [])\n    val_losses = metrics.get(\"val_loss\", [])\n    val_swa = metrics.get(\"val_swa\", [])\n\n    # Guard against empty lists\n    if not (train_losses and val_losses and val_swa):\n        print(f\"{name}: No metric data found.\")\n        return\n\n    # Compute required values\n    final_train_loss = train_losses[-1]  # last recorded value\n    best_validation_loss = min(val_losses)  # lower is better\n    best_validation_swa = max(val_swa)  # higher is better\n\n    # Print results\n    print(name)  # dataset name\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Best validation loss: {best_validation_loss:.4f}\")\n    print(f\"Best validation shape-weighted accuracy: {best_validation_swa:.4f}\")\n    print()  # blank line for readability\n\n\n# ------------------- iterate over datasets -----------------\nfor dataset_name, dataset_info in experiment_data.items():\n    metrics_dict = dataset_info.get(\"metrics\", {})\n    print_metrics(dataset_name, metrics_dict)\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'train loss: 0.16538991758823396', '\\n',\n'validation loss: 0.16744024930000306', '\\n', 'validation shape-weighted\naccuracy: 0.9446575979537263', '\\n', 'validation color-weighted accuracy:\n0.9476542004758709', '\\n', 'validation BPS: 0.9461547128838366', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training loss: 0.0265', '\\n', 'validation loss: 0.0229',\n'\\n', 'validation shape weighted accuracy: 0.9945', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training loss: 0.0208', '\\n', 'Best validation loss:\n0.0116', '\\n', 'Best validation shape-weighted accuracy: 0.9971', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  training loss: 0.1646', '\\n', '  validation loss:\n0.1674', '\\n', '  validation shape-weighted accuracy: 0.9445', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'train loss: 1.5223', '\\n', 'val loss: 0.6358',\n'\\n', 'val swa: 0.9204', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'train loss: 0.1706', '\\n', 'validation loss:\n0.1669', '\\n', 'validation shape-weighted accuracy: 0.9449', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training loss: 0.0095', '\\n', 'Best\nvalidation loss: 0.0005', '\\n', 'Best validation shape-weighted accuracy:\n1.0000', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training loss: 0.04012007169723511', '\\n', 'Final\nvalidation loss: 0.03656613399088383', '\\n', 'Best validation shape-weighted\naccuracy: 0.9948261830019766\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  training loss: 0.063119', '\\n', '  validation\nloss: 0.046047', '\\n', '  validation shape-weighted accuracy: 0.988548', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training loss: 0.0360', '\\n', 'Best validation loss:\n0.0153', '\\n', 'Best validation shape-weighted accuracy: 0.9957', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training loss: 0.0231', '\\n', 'Best validation loss:\n0.0091', '\\n', 'Best validation shape-weighted accuracy: 0.9978', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training loss: 0.0294', '\\n', 'Best validation loss:\n0.0214', '\\n', 'Best validation shape-weighted accuracy: 0.9924', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}