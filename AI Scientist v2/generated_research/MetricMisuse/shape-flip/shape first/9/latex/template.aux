\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{goodfellow2016deep,tsamoura2020neuralsymbolicia}
\citation{mul2019siamesern}
\citation{tsamoura2020neuralsymbolicia,mul2019siamesern,xu2024macab,yuasa2025neurosymbolicgo}
\citation{dickens2024modelingpf}
\citation{chung2014empiricaleo}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\citation{dickens2024modelingpf}
\bibdata{references}
\bibcite{chung2014empiricaleo}{{1}{2014}{{Chung et~al.}}{{Chung, Çaglar Gülçehre, Cho, and Bengio}}}
\bibcite{dickens2024modelingpf}{{2}{2024}{{Dickens et~al.}}{{Dickens, Pryor, and Getoor}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Baseline Results.} (a) Smaller batch sizes achieve faster training convergence. (b--c) Validation and test metrics consistently favor batch sizes of 16--64, with large-batch methods lagging behind.}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss Curves}}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Val HWA}}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Test HWA}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:baseline}{{1}{2}{\textbf {Baseline Results.} (a) Smaller batch sizes achieve faster training convergence. (b--c) Validation and test metrics consistently favor batch sizes of 16--64, with large-batch methods lagging behind}{figure.1}{}}
\newlabel{fig:baseline@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{2}{section.5}\protected@file@percent }
\bibcite{goodfellow2016deep}{{3}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{mul2019siamesern}{{4}{2019}{{Mul \& Zuidema}}{{Mul and Zuidema}}}
\bibcite{tsamoura2020neuralsymbolicia}{{5}{2020}{{Tsamoura \& Michael}}{{Tsamoura and Michael}}}
\bibcite{xu2024macab}{{6}{2024}{{Xu et~al.}}{{Xu, Wang, Hu, Lin, Du, and Wu}}}
\bibcite{yuasa2025neurosymbolicgo}{{7}{2025}{{Yuasa et~al.}}{{Yuasa, Sreenivas, and Tran}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Neuro-Symbolic Model.} The training loss quickly reaches zero; the shape-weighted validation accuracy is perfect; and the confusion matrix on the test set indicates no errors.}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss}}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Val SWA}}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Confusion}}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:research}{{2}{3}{\textbf {Neuro-Symbolic Model.} The training loss quickly reaches zero; the shape-weighted validation accuracy is perfect; and the confusion matrix on the test set indicates no errors}{figure.2}{}}
\newlabel{fig:research@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1}Training Details and Hyperparameters}{3}{subsection.Alph0.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model Hyperparameters}}{3}{table.1}\protected@file@percent }
\newlabel{tab:hyperparams}{{1}{3}{Model Hyperparameters}{table.1}{}}
\newlabel{tab:hyperparams@cref}{{[table][1][2147483647]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.2}Additional Ablation Studies}{3}{subsection.Alph0.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Ablation experiments showing performance variations with different architectural tweaks. Although exact numbers differed slightly, the overall trend of improved zero-shot shape accuracy for hybrid methods remained consistent.}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Binary Symbolic}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Late Fusion}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Neural Only}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {No Projection}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Unidirectional GRU}}}{4}{figure.3}\protected@file@percent }
\gdef \@abspage@last{4}
