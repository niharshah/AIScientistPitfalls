{"edges": [[0, 8], [0, 10], [0, 9], [0, 11]], "layout": [[0.17647058823529413, 0.0], [0.29411764705882354, 0.0], [0.4117647058823529, 0.0], [0.5294117647058824, 0.0], [0.6470588235294118, 0.0], [0.7647058823529411, 0.0], [0.8823529411764706, 0.0], [1.0, 0.0], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["Hyperparam tuning name: EPOCHS (num_training_epochs). Below we simply loop over\nseveral candidate values for `num_training_epochs` (5 \u00b7 10 \u00b7 20 \u00b7 30).   For\nevery setting we (re-)initialise the model, train for the given number of epochs\nwhile logging train/validation metrics, and finally evaluate on the test set.\nAll results are collected in `experiment_data['EPOCHS']` and stored in\n`experiment_data.npy`.", "Hyperparam tuning name: learning_rate. We sweep four Adam learning-rates (3e-4,\n5e-4, 1e-3, 2e-3).   For each lr we re-initialise the MLP, train for a modest 8\nepochs, evaluate on dev after every epoch, and finally compute test accuracy &\nURA.   All per-epoch curves as well as the final test numbers and predictions\nare stored inside experiment_data under the \u201clearning_rate \u279c SPR_BENCH\u201d keys and\ndumped to experiment_data.npy so they can be plotted later.", "Hyperparam tuning name: batch_size. We sweep the training-time DataLoader\nbatch_size over five values (16 / 32 / 64 / 128 / 256) while keeping the\nevaluation loader large (256).   For every candidate batch_size we re-initialise\nthe model, train for 5 epochs, evaluate on the dev split after each epoch, and\nfinally measure test accuracy and unseen-rule accuracy (URA).   All per-epoch\nlosses and metrics as well as the final test predictions for each setting are\ncollected into the experiment_data dictionary under the key 'batch_size_tuning'\nand saved to working/experiment_data.npy, allowing later analysis and plotting.", "Hyperparam tuning name: hidden_size. We explore model capacity by sweeping the\nMLP hidden layer size over {32, 64, 128, 256}.   For every size we train the\nnetwork for a few epochs, record training/validation losses, accuracy and URA,\nand finally evaluate the best\u2010performing configuration on the test set.\nResults (per size) together with predictions and ground-truth labels are stored\nin an `experiment_data` dict and saved to `experiment_data.npy` for later\ninspection.", "Hyperparam tuning name: weight_decay. The script below extends the baseline by\nsweeping Adam\u2019s weight_decay over six logarithmically-spaced values.   For each\nvalue the model is re-initialised, trained for five epochs, evaluated on\ndev/test (including URA), and all plottable data are stored in an\n`experiment_data` dictionary keyed by the weight-decay value.   Finally the\nwhole dictionary is saved to `experiment_data.npy` for later analysis.", "Hyperparam tuning name: dropout_rate. Proposed solution: add a Dropout layer\nafter the first ReLU in the MLP and search several dropout probabilities.   For\neach dropout value (0.0 \u2026 0.5) we train the model for a few epochs, record\ntrain/validation metrics, evaluate on the test set and keep all plottable data\nin a nested experiment_data dict that is finally saved as \u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: num_hidden_layers. We extend the baseline by introducing\na variable-depth MLP whose depth (number of hidden layers) is tuned over {1, 2,\n3}.   For every depth we (1) build a fresh model, (2) train with the same\nsettings, (3) early-stop with a small patience, (4) evaluate on dev & test, and\n(5) log metrics/losses/predictions into a hierarchical experiment_data\ndictionary that follows the requested saving convention.   All logic remains in\na single self-contained script; at the end we persist experiment_data as\nexperiment_data.npy.", "Hyperparam tuning name: activation_function. The script below runs the same\nexperiment once for each hidden-layer non-linearity (ReLU, LeakyReLU, GELU,\nTanh), logs accuracy + URA on dev / test, and stores every plottable series in a\nnested NumPy dict called experiment_data. The model architecture and all other\nhyper-parameters stay fixed, so any difference in results comes purely from the\nactivation choice.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, string, time, json, math\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ----------------- reproducibility -----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper symbolic functions -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ----------------- fallback synthetic data -----------------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ----------------- feature encoding -----------------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shape hist + colour hist + {seq_len,varieties}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        vec[shape_to_idx[tok[0]]] += 1\n        vec[26 + colour_to_idx[tok[1]]] += 1\n    vec[-3] = len(toks)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# encode once\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ntrain_loader_full = DataLoader(\n    SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ----------------- model -----------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ----------------- evaluation util -----------------\ndef eval_loader(model, loader, sigs_all, unseen_signatures):\n    model.eval()\n    correct = total = correct_unseen = total_unseen = 0\n    preds_all = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(dim=1)\n            preds_all.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_signatures:\n                    total_unseen += 1\n                    if p == y_true:\n                        correct_unseen += 1\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, preds_all\n\n\n# Identify unseen signatures\ntrain_signatures = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_signatures}\nunseen_test_sigs = {s for s in sig_test if s not in train_signatures}\n\n# ----------------- hyper-parameter tuning -----------------\nepoch_options = [5, 10, 20, 30]\nexperiment_data = {\"EPOCHS\": {}}\n\nfor num_epochs in epoch_options:\n    print(f\"\\n=== Training with {num_epochs} epochs ===\")\n    model = MLP(feature_dim).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # prepare fresh data loaders each run (shuffling matters)\n    train_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\n\n    run_key = f\"epochs_{num_epochs}\"\n    experiment_data[\"EPOCHS\"][run_key] = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        run_loss = run_corr = run_total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * y.size(0)\n            run_corr += (logits.argmax(dim=1) == y).sum().item()\n            run_total += y.size(0)\n        train_loss = run_loss / run_total\n        train_acc = run_corr / run_total\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev_sigs)\n\n        print(\n            f\"Epoch {epoch}/{num_epochs}: \"\n            f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n        ed = experiment_data[\"EPOCHS\"][run_key]\n        ed[\"losses\"][\"train\"].append(train_loss)\n        ed[\"metrics\"][\"train_acc\"].append(train_acc)\n        ed[\"metrics\"][\"val_acc\"].append(val_acc)\n        ed[\"metrics\"][\"val_ura\"].append(val_ura)\n        ed[\"timestamps\"].append(time.time())\n\n    # ----- final test evaluation -----\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test_sigs\n    )\n    print(f\"Test  acc={test_acc:.3f}  Test URA={test_ura:.3f}\")\n\n    experiment_data[\"EPOCHS\"][run_key][\"predictions\"] = test_preds\n    experiment_data[\"EPOCHS\"][run_key][\"test_acc\"] = test_acc\n    experiment_data[\"EPOCHS\"][run_key][\"test_ura\"] = test_ura\n\n# ----------------- save -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "import os, pathlib, random, string, time, math, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ------------ experiment data container ------------\nexperiment_data = {\n    \"learning_rate\": {\n        \"SPR_BENCH\": {\n            \"lr_values\": [],\n            \"metrics\": {\n                \"train_acc\": [],  # shape : [n_lr , n_epochs]\n                \"val_acc\": [],\n                \"val_ura\": [],\n                \"test_acc\": [],\n                \"test_ura\": [],\n            },\n            \"losses\": {\"train\": [], \"val\": []},  # same shape convention\n            \"predictions\": [],  # list of list\n            \"ground_truth\": [],  # set once later\n            \"timestamps\": [],\n        }\n    }\n}\n\n# ------------ device ------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------ helper symbolic functions ------------\ndef count_shape_variety(sequence):\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence):\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef rule_signature(sequence):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ------------ synthetic data fallback ------------\ndef random_token():\n    return random.choice(string.ascii_uppercase[:10]) + random.choice(string.digits[:5])\n\n\ndef generate_synthetic_split(n, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for i in range(n):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        label = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(label)\n    return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root: pathlib.Path):\n    if root.exists():\n        print(\"Loading real SPR_BENCH\")\n        _ld = lambda f: load_dataset(\"csv\", data_files=str(root / f), split=\"train\")\n        return DatasetDict(\n            train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, 1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, 2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, 3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ------------ feature encoding ------------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # histograms + len/varieties\n\n\ndef encode_sequence(seq):\n    vec = np.zeros(feature_dim, np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        vec[shape_to_idx[tok[0]]] += 1\n        vec[26 + colour_to_idx[tok[1]]] += 1\n    vec[-3] = len(toks)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.asarray(hf_ds[\"label\"], np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"ground_truth\"] = y_test.tolist()\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = torch.tensor(X), torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ntrain_base_loader = DataLoader(\n    SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ------------ model ------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ------------ evaluation helper ------------\ndef eval_loader(model, loader, sigs_all, unseen_sigs):\n    model.eval()\n    correct = tot = correct_u = tot_u = 0\n    preds_all = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(1)\n            preds_all.extend(preds.cpu().numpy())\n            tot += y.size(0)\n            correct += (preds == y).sum().item()\n            for p_, y_ in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                if sigs_all[idx] in unseen_sigs:\n                    tot_u += 1\n                    if p_ == y_:\n                        correct_u += 1\n                idx += 1\n    acc = correct / tot\n    ura = correct_u / tot_u if tot_u else 0.0\n    return acc, ura, preds_all\n\n\ntrain_sigs_set = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_sigs_set}\nunseen_test_sigs = {s for s in sig_test if s not in train_sigs_set}\n\n# ------------ learning-rate sweep ------------\nlr_values = [3e-4, 5e-4, 1e-3, 2e-3]\nEPOCHS = 8\n\n\ndef train_loader_from_base():\n    # fresh iterator every epoch due to shuffle; reuse Dataset object\n    return DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\n\n\nfor lr in lr_values:\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"lr_values\"].append(lr)\n    per_lr_train_acc, per_lr_val_acc, per_lr_val_ura = [], [], []\n    per_lr_train_loss, per_lr_val_loss = [], []\n\n    model = MLP(feature_dim).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        run_loss = run_corr = run_tot = 0\n        for batch in train_loader_from_base():\n            optimizer.zero_grad()\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * y.size(0)\n            run_corr += (logits.argmax(1) == y).sum().item()\n            run_tot += y.size(0)\n        train_loss = run_loss / run_tot\n        train_acc = run_corr / run_tot\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev_sigs)\n\n        per_lr_train_loss.append(train_loss)\n        per_lr_val_loss.append(np.nan)  # placeholder, val loss not computed\n        per_lr_train_acc.append(train_acc)\n        per_lr_val_acc.append(val_acc)\n        per_lr_val_ura.append(val_ura)\n\n        print(\n            f\"[lr={lr:.4g}] epoch {epoch}/{EPOCHS}: \"\n            f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n    # final test evaluation\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test_sigs\n    )\n    print(f\"--> lr={lr:.4g}  TEST acc={test_acc:.3f}  URA={test_ura:.3f}\\n\")\n\n    # store results\n    ed = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\n    ed[\"metrics\"][\"train_acc\"].append(per_lr_train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(per_lr_val_acc)\n    ed[\"metrics\"][\"val_ura\"].append(per_lr_val_ura)\n    ed[\"losses\"][\"train\"].append(per_lr_train_loss)\n    ed[\"losses\"][\"val\"].append(per_lr_val_loss)\n    ed[\"metrics\"][\"test_acc\"].append(test_acc)\n    ed[\"metrics\"][\"test_ura\"].append(test_ura)\n    ed[\"predictions\"].append(test_preds)\n    ed[\"timestamps\"].append(time.time())\n\n# ------------ save experiment data ------------\nos.makedirs(\"working\", exist_ok=True)\nnp.save(os.path.join(\"working\", \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, random, string, time\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ------------------- housekeeping -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ------------------- reproducibility -------------------\nRNG_SEED = 42\nrandom.seed(RNG_SEED)\nnp.random.seed(RNG_SEED)\ntorch.manual_seed(RNG_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(RNG_SEED)\n\n\n# ------------------- helper functions -------------------\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(seq: str):\n    return (count_shape_variety(seq), count_color_variety(seq))\n\n\n# ---------- fallback synthetic data ----------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\n# ------------------- load data -------------------\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shapes hist + colours hist + {len, shapeVar, colourVar}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    tokens = seq.split()\n    for tok in tokens:\n        if len(tok) < 2:\n            continue\n        s, c = tok[0], tok[1]\n        vec[shape_to_idx[s]] += 1\n        vec[26 + colour_to_idx[c]] += 1\n    vec[-3] = len(tokens)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# encode once\nX_train_all, y_train_all, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\n# unseen signatures for URA\ntrain_sigs_set = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_sigs_set}\nunseen_test_sigs = {s for s in sig_test if s not in train_sigs_set}\n\n\n# ------------------- torch dataset -------------------\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ------------------- model -------------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(loader, sigs_all, unseen_sigs, model):\n    model.eval()\n    total = correct = total_unseen = correct_unseen = 0\n    all_preds = []\n    with torch.no_grad():\n        idx = 0\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_sigs:\n                    total_unseen += 1\n                    if p == y_true:\n                        correct_unseen += 1\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, all_preds\n\n\n# ------------------- hyperparameter sweep -------------------\nbatch_sizes = [16, 32, 64, 128, 256]\nEPOCHS = 5\n\nexperiment_data = {\n    \"batch_size_tuning\": {\n        \"SPR_BENCH\": {\n            \"batch_sizes\": batch_sizes,\n            \"metrics\": {\n                \"train_acc\": [],\n                \"val_acc\": [],\n                \"val_ura\": [],\n                \"test_acc\": [],\n                \"test_ura\": [],\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_test.tolist(),\n            \"timestamps\": [],\n        }\n    }\n}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size={bs} ===\")\n    # fresh model & optimiser\n    model = MLP(feature_dim).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # make loader using the current batch size\n    train_loader = DataLoader(\n        SPRTorchDS(X_train_all, y_train_all), batch_size=bs, shuffle=True\n    )\n\n    epoch_train_acc, epoch_val_acc, epoch_val_ura, epoch_train_loss, epoch_val_loss = (\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        running_loss = running_correct = running_total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * y.size(0)\n            preds = logits.argmax(dim=1)\n            running_correct += (preds == y).sum().item()\n            running_total += y.size(0)\n\n        train_loss = running_loss / running_total\n        train_acc = running_correct / running_total\n        val_acc, val_ura, _ = evaluate(dev_loader, sig_dev, unseen_dev_sigs, model)\n\n        epoch_train_loss.append(train_loss)\n        epoch_train_acc.append(train_acc)\n        epoch_val_acc.append(val_acc)\n        epoch_val_ura.append(val_ura)\n        epoch_val_loss.append(0.0)  # placeholder, val loss not computed\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n    # evaluation on test set\n    test_acc, test_ura, test_preds = evaluate(\n        test_loader, sig_test, unseen_test_sigs, model\n    )\n    print(f\"Test accuracy={test_acc:.3f}  Test URA={test_ura:.3f}\")\n\n    # store metrics\n    ed = experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"]\n    ed[\"metrics\"][\"train_acc\"].append(epoch_train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(epoch_val_acc)\n    ed[\"metrics\"][\"val_ura\"].append(epoch_val_ura)\n    ed[\"metrics\"][\"test_acc\"].append(test_acc)\n    ed[\"metrics\"][\"test_ura\"].append(test_ura)\n    ed[\"losses\"][\"train\"].append(epoch_train_loss)\n    ed[\"losses\"][\"val\"].append(epoch_val_loss)\n    ed[\"predictions\"].append(test_preds)\n    ed[\"timestamps\"].append(time.time())\n\n# ------------------- save experiment data -------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# hidden_size_tuning.py\n# Hyper-parameter sweep over the hidden layer size of an MLP classifier\nimport os, pathlib, random, string, time\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ----------------- reproducibility -----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper symbolic functions -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ----------------- fallback synthetic data -----------------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    rnd = random.Random(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = rnd.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(f):\n            return load_dataset(\"csv\", data_files=str(root_path / f), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ----------------- feature encoding -----------------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shape histogram + colour histogram + 3 global features\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        vec[shape_to_idx[tok[0]]] += 1\n        vec[26 + colour_to_idx[tok[1]]] += 1\n    vec[-3] = len(toks)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# ----------------- torch dataset -----------------\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\n# ----------------- evaluation helper -----------------\ndef evaluate(model, loader, sigs_all, unseen_sigs):\n    model.eval()\n    correct = total = correct_unseen = total_unseen = 0\n    preds_all = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(1)\n            preds_all.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_sigs:\n                    total_unseen += 1\n                    correct_unseen += int(p == y_true)\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, preds_all\n\n\n# ----------------- experiment bookkeeping -----------------\nexperiment_data = {\n    \"hidden_size\": {\n        \"SPR_BENCH\": {\n            \"hidden_sizes\": [],\n            \"metrics\": {\n                \"train_acc\": [],\n                \"val_acc\": [],\n                \"test_acc\": [],\n                \"val_ura\": [],\n                \"test_ura\": [],\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],  # test predictions for the best size\n            \"ground_truth\": [],  # test ground truth (once)\n            \"timestamps\": [],\n        }\n    }\n}\n\n# keep ground truth once\n_, y_test_np, _ = encode_dataset(dsets[\"test\"])\nexperiment_data[\"hidden_size\"][\"SPR_BENCH\"][\"ground_truth\"] = y_test_np.tolist()\n\n# ----------------- hyper-parameter sweep -----------------\nhidden_sizes = [32, 64, 128, 256]\nEPOCHS = 5\nbest_val_acc = -1\nbest_preds = []\nbest_size = None\n\nfor hid in hidden_sizes:\n    print(f\"\\n--- Training with hidden size = {hid} ---\")\n    # data loaders freshly encoded each sweep (cheap)\n    X_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\n    X_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\n    X_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n    train_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\n    dev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\n    test_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n    train_sigs_set = set(sig_train)\n    unseen_dev_sigs = {s for s in sig_dev if s not in train_sigs_set}\n    unseen_test_sigs = {s for s in sig_test if s not in train_sigs_set}\n\n    class MLP(nn.Module):\n        def __init__(self, in_dim, hidden, n_classes):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    model = MLP(feature_dim, hid, len(set(y_train))).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # train\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        running_loss = running_correct = total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * y.size(0)\n            preds = logits.argmax(1)\n            running_correct += (preds == y).sum().item()\n            total += y.size(0)\n        train_loss = running_loss / total\n        train_acc = running_correct / total\n\n    # one final dev evaluation\n    val_acc, val_ura, _ = evaluate(model, dev_loader, sig_dev, unseen_dev_sigs)\n    train_loss, _ = train_loss, train_acc  # already computed\n\n    # store results\n    ed = experiment_data[\"hidden_size\"][\"SPR_BENCH\"]\n    ed[\"hidden_sizes\"].append(hid)\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"val_ura\"].append(val_ura)\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(0.0)  # placeholder (not tracked every epoch)\n    ed[\"timestamps\"].append(time.time())\n\n    print(\n        f\"Hidden {hid}: train_acc={train_acc:.3f}  val_acc={val_acc:.3f}  URA={val_ura:.3f}\"\n    )\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_size = hid\n        # evaluate on test with the current best\n        test_acc, test_ura, test_preds = evaluate(\n            model, test_loader, sig_test, unseen_test_sigs\n        )\n        best_preds = test_preds\n        ed_best = (test_acc, test_ura)\n\n# log best test metrics\ned = experiment_data[\"hidden_size\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test_acc\"].append(ed_best[0])\ned[\"metrics\"][\"test_ura\"].append(ed_best[1])\ned[\"predictions\"] = best_preds\nprint(\n    f\"\\nBest hidden size = {best_size}  Test Acc = {ed_best[0]:.3f}  Test URA = {ed_best[1]:.3f}\"\n)\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# --------- experiment bookkeeping ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"weight_decay_tuning\": {}}  # will fill per weight-decay value\n\n# --------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# --------- helper symbolic functions ----------\ndef count_shape_variety(seq):\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq):\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef rule_signature(seq):\n    return (count_shape_variety(seq), count_color_variety(seq))\n\n\n# --------- fallback synthetic data ----------\ndef random_token():\n    return random.choice(string.ascii_uppercase[:10]) + random.choice(string.digits[:5])\n\n\ndef generate_synth_split(n, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        label = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(label)\n    return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    if root.exists():\n        print(\"Loading real SPR_BENCH from\", root)\n        _load = lambda f: load_dataset(\"csv\", data_files=str(root / f), split=\"train\")\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synth_split(2000, 1)),\n        dev=HFDataset.from_dict(generate_synth_split(500, 2)),\n        test=HFDataset.from_dict(generate_synth_split(1000, 3)),\n    )\n\n\ndata_path = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(data_path)\n\n# --------- feature encoding ----------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = (\n    26 + 10 + 3\n)  # shapes hist + colours hist + [seq_len, #shapeVar, #colourVar]\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    v = np.zeros(feature_dim, np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        v[shape_to_idx[tok[0]]] += 1\n        v[26 + colour_to_idx[tok[1]]] += 1\n    v[-3:] = [len(toks), count_shape_variety(seq), count_color_variety(seq)]\n    return v\n\n\ndef encode_dataset(hf):\n    feats = np.stack([encode_sequence(s) for s in hf[\"sequence\"]])\n    labels = np.array(hf[\"label\"], np.int64)\n    sigs = [rule_signature(s) for s in hf[\"sequence\"]]\n    return feats, labels, sigs\n\n\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\ntrain_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# --------- model ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_cls=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_cls)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# --------- evaluation ----------\ndef eval_loader(model, loader, sigs_all, unseen_sigs):\n    model.eval()\n    correct = total = correct_u = total_u = idx = 0\n    preds_all = []\n    with torch.no_grad():\n        for batch in loader:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            preds = model(x).argmax(1)\n            preds_all.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_sigs:\n                    total_u += 1\n                    correct_u += int(p == y_true)\n                idx += 1\n    acc = correct / total\n    ura = correct_u / total_u if total_u else 0.0\n    return acc, ura, preds_all\n\n\ntrain_sigs = set(sig_train)\nunseen_dev = {s for s in sig_dev if s not in train_sigs}\nunseen_test = {s for s in sig_test if s not in train_sigs}\n\n# --------- hyperparameter sweep ----------\nweight_decays = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\nEPOCHS = 5\ncriterion = nn.CrossEntropyLoss()\n\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    model = MLP(feature_dim).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    run_rec = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        running_loss = running_correct = running_total = 0\n        for batch in train_loader:\n            opt.zero_grad()\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            loss = criterion(model(x), y)\n            loss.backward()\n            opt.step()\n            running_loss += loss.item() * y.size(0)\n            running_correct += (model(x).argmax(1) == y).sum().item()\n            running_total += y.size(0)\n        train_loss = running_loss / running_total\n        train_acc = running_correct / running_total\n\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev)\n        print(\n            f\"epoch {epoch}: loss {train_loss:.4f}, train_acc {train_acc:.3f}, \"\n            f\"val_acc {val_acc:.3f}, URA {val_ura:.3f}\"\n        )\n\n        run_rec[\"losses\"][\"train\"].append(train_loss)\n        run_rec[\"metrics\"][\"train_acc\"].append(train_acc)\n        run_rec[\"metrics\"][\"val_acc\"].append(val_acc)\n        run_rec[\"metrics\"][\"val_ura\"].append(val_ura)\n        run_rec[\"timestamps\"].append(time.time())\n\n    # final test evaluation\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test\n    )\n    print(f\"Test: acc {test_acc:.3f}, URA {test_ura:.3f}\")\n    run_rec[\"metrics\"][\"test_acc\"] = test_acc\n    run_rec[\"metrics\"][\"test_ura\"] = test_ura\n    run_rec[\"predictions\"] = test_preds\n\n    # store under string key to keep numpy compatibility\n    experiment_data[\"weight_decay_tuning\"][str(wd)] = run_rec\n\n# --------- save ---------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# --------------------  Dropout-rate hyperparameter search  --------------------\nimport os, pathlib, random, string, time, warnings\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\nwarnings.filterwarnings(\"ignore\")\ntorch.backends.cudnn.deterministic, torch.backends.cudnn.benchmark = True, False\n\n# ---------- reproducibility ----------\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- helper symbolic functions ----------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ---------- synthetic fallback ----------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ---------- feature encoding ----------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shape hist + colour hist + {seq_len, shapeVar, colourVar}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    for tok in seq.split():\n        if len(tok) < 2:\n            continue\n        s, c = tok[0], tok[1]\n        vec[shape_to_idx[s]] += 1\n        vec[26 + colour_to_idx[c]] += 1\n    vec[-3], vec[-2], vec[-1] = (\n        len(seq.split()),\n        count_shape_variety(seq),\n        count_color_variety(seq),\n    )\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# ---------- torch dataset ----------\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = torch.tensor(X), torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\ntrain_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\ntrain_signatures = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_signatures}\nunseen_test_sigs = {s for s in sig_test if s not in train_signatures}\n\n\n# ---------- model ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2, dropout_rate=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden, n_classes),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ---------- evaluation ----------\ndef eval_model(model, loader, sigs_all, unseen_signatures, criterion):\n    model.eval()\n    total, correct = 0, 0\n    total_unseen, correct_unseen = 0, 0\n    all_preds, losses = [], []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            preds = logits.argmax(dim=1)\n            losses.append(loss.item() * y.size(0))\n            all_preds.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                if sigs_all[idx] in unseen_signatures:\n                    total_unseen += 1\n                    correct_unseen += int(p == y_true)\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    loss = sum(losses) / total\n    return acc, ura, loss, all_preds\n\n\n# ---------- experiment data container ----------\nexperiment_data = {\"dropout_rate\": {\"SPR_BENCH\": {\"runs\": {}}}}\n\n# ---------- hyperparameter search ----------\ndropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\nEPOCHS = 5\n\nfor dr in dropout_rates:\n    print(f\"\\n===== Training with dropout_rate = {dr} =====\")\n    model = MLP(\n        feature_dim, hidden=64, n_classes=len(set(y_train)), dropout_rate=dr\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    run_data = {\n        \"hyperparam\": dr,\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        tr_loss_sum, tr_corr, tr_total = 0.0, 0, 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            tr_loss_sum += loss.item() * y.size(0)\n            tr_corr += (logits.argmax(dim=1) == y).sum().item()\n            tr_total += y.size(0)\n        train_loss = tr_loss_sum / tr_total\n        train_acc = tr_corr / tr_total\n\n        val_acc, val_ura, val_loss, _ = eval_model(\n            model, dev_loader, sig_dev, unseen_dev_sigs, criterion\n        )\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_loss={val_loss:.4f} val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n        run_data[\"losses\"][\"train\"].append(train_loss)\n        run_data[\"losses\"][\"val\"].append(val_loss)\n        run_data[\"metrics\"][\"train_acc\"].append(train_acc)\n        run_data[\"metrics\"][\"val_acc\"].append(val_acc)\n        run_data[\"metrics\"][\"val_ura\"].append(val_ura)\n        run_data[\"timestamps\"].append(time.time())\n\n    # final test evaluation\n    test_acc, test_ura, _, test_preds = eval_model(\n        model, test_loader, sig_test, unseen_test_sigs, criterion\n    )\n    print(f\"TEST  acc={test_acc:.3f}  URA={test_ura:.3f}\")\n    run_data[\"test_acc\"] = test_acc\n    run_data[\"test_ura\"] = test_ura\n    run_data[\"predictions\"] = test_preds\n\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"runs\"][str(dr)] = run_data\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nAll experiments finished and saved to 'working/experiment_data.npy'\")\n", "import os, pathlib, random, string, time, copy\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ---------- experiment data dict ----------\nexperiment_data = {\"num_hidden_layers\": {\"SPR_BENCH\": {}}}\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- helper symbolic functions ----------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ---------- fallback synthetic data ----------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    else:\n        print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n        train_ds = HFDataset.from_dict(generate_synthetic_split(2000, seed=1))\n        dev_ds = HFDataset.from_dict(generate_synthetic_split(500, seed=2))\n        test_ds = HFDataset.from_dict(generate_synthetic_split(1000, seed=3))\n        return DatasetDict(train=train_ds, dev=dev_ds, test=test_ds)\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ---------- feature encoding ----------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shapes hist + colours hist + {seq_len, shapeVar, colourVar}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    for tok in seq.split():\n        if len(tok) < 2:\n            continue\n        s, c = tok[0], tok[1]\n        vec[shape_to_idx[s]] += 1\n        vec[26 + colour_to_idx[c]] += 1\n    vec[-3] = len(seq.split())\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# prepare data\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\ntrain_loader = (\n    DataLoader(\n        Dataset(torch.tensor(X_train), torch.tensor(y_train)),\n        batch_size=64,\n        shuffle=True,\n    )\n    if False\n    else DataLoader\n)  # placeholder (ignored due to subclass below)\n\n\n# ---------- torch dataset ----------\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ntrain_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ---------- model ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_hidden_layers=1, n_classes=2):\n        super().__init__()\n        layers = [nn.Linear(in_dim, hidden), nn.ReLU()]\n        for _ in range(n_hidden_layers - 1):\n            layers += [nn.Linear(hidden, hidden), nn.ReLU()]\n        layers.append(nn.Linear(hidden, n_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ---------- evaluation ----------\ndef eval_loader(model, loader, sigs_all, unseen_signatures):\n    model.eval()\n    correct = total = correct_unseen = total_unseen = 0\n    all_preds = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            preds = model(x).argmax(1)\n            all_preds.extend(preds.cpu().numpy())\n            correct += (preds == y).sum().item()\n            total += y.size(0)\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_signatures:\n                    total_unseen += 1\n                    if p == y_true:\n                        correct_unseen += 1\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, all_preds\n\n\ntrain_signatures = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_signatures}\nunseen_test_sigs = {s for s in sig_test if s not in train_signatures}\n\n# ---------- hyperparameter tuning ----------\nlayer_options = [1, 2, 3]\nEPOCHS = 20\nearly_patience = 3\n\nfor n_layers in layer_options:\n    run_key = f\"layers_{n_layers}\"\n    experiment_data[\"num_hidden_layers\"][\"SPR_BENCH\"][run_key] = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    model = MLP(\n        feature_dim, hidden=64, n_hidden_layers=n_layers, n_classes=len(set(y_train))\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    best_val, patience = 0.0, 0\n    best_state = copy.deepcopy(model.state_dict())\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        running_loss = running_correct = running_total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * y.size(0)\n            running_correct += (logits.argmax(1) == y).sum().item()\n            running_total += y.size(0)\n        train_loss = running_loss / running_total\n        train_acc = running_correct / running_total\n\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev_sigs)\n        print(\n            f\"[layers={n_layers}] Epoch {epoch}  loss={train_loss:.4f}  \"\n            f\"train_acc={train_acc:.3f}  val_acc={val_acc:.3f}  URA={val_ura:.3f}\"\n        )\n\n        data_ref = experiment_data[\"num_hidden_layers\"][\"SPR_BENCH\"][run_key]\n        data_ref[\"losses\"][\"train\"].append(train_loss)\n        data_ref[\"metrics\"][\"train_acc\"].append(train_acc)\n        data_ref[\"metrics\"][\"val_acc\"].append(val_acc)\n        data_ref[\"metrics\"][\"val_ura\"].append(val_ura)\n        data_ref[\"timestamps\"].append(time.time())\n\n        # early stopping\n        if val_acc > best_val:\n            best_val = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n            patience = 0\n        else:\n            patience += 1\n        if patience >= early_patience:\n            print(f\"Early stopping triggered for depth {n_layers}\")\n            break\n\n    model.load_state_dict(best_state)\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test_sigs\n    )\n    print(\n        f\"[layers={n_layers}] Test Accuracy={test_acc:.3f}  Test URA={test_ura:.3f}\\n\"\n    )\n    experiment_data[\"num_hidden_layers\"][\"SPR_BENCH\"][run_key][\n        \"predictions\"\n    ] = test_preds\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, string, time, warnings, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\nwarnings.filterwarnings(\"ignore\")\n# ------------------- dirs & device -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using\", device)\n\n\n# ------------------- helpers -------------------------\ndef count_shape_variety(seq):\n    return len({tok[0] for tok in seq.split() if tok})\n\n\ndef count_color_variety(seq):\n    return len({tok[1] for tok in seq.split() if len(tok) > 1})\n\n\ndef rule_signature(seq):\n    return (count_shape_variety(seq), count_color_variety(seq))\n\n\ndef random_token():\n    return random.choice(string.ascii_uppercase[:10]) + random.choice(string.digits[:5])\n\n\ndef generate_synth(n, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n):\n        toks = [random_token() for _ in range(random.randint(3, 10))]\n        seq = \" \".join(toks)\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(path):\n    if path.exists():\n\n        def _ld(f):\n            return load_dataset(\"csv\", data_files=str(path / f), split=\"train\")\n\n        print(\"Loading real SPR_BENCH\")\n        return DatasetDict(\n            train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 using synthetic\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synth(2000, 1)),\n        dev=HFDataset.from_dict(generate_synth(500, 2)),\n        test=HFDataset.from_dict(generate_synth(1000, 3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr(DATA_PATH)\n\n# ------------------- encoding ------------------------\nshape_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3\n\n\ndef encode(seq):\n    v = np.zeros(feature_dim, dtype=np.float32)\n    toks = seq.split()\n    for t in toks:\n        if len(t) < 2:\n            continue\n        v[shape_idx[t[0]]] += 1\n        v[26 + colour_idx[t[1]]] += 1\n    v[-3:] = [len(toks), count_shape_variety(seq), count_color_variety(seq)]\n    return v\n\n\ndef encode_set(ds):\n    feats = np.stack([encode(s) for s in ds[\"sequence\"]])\n    labels = np.array(ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\nX_train, y_train, sig_train = encode_set(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_set(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_set(dsets[\"test\"])\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(s, X, y):\n        s.X = torch.tensor(X)\n        s.y = torch.tensor(y)\n\n    def __len__(s):\n        return len(s.X)\n\n    def __getitem__(s, i):\n        return {\"x\": s.X[i], \"y\": s.y[i]}\n\n\ntrain_loader = lambda bs: DataLoader(\n    SPRTorchDS(X_train, y_train), batch_size=bs, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ------------------- model ---------------------------\nclass MLP(nn.Module):\n    def __init__(self, act_name, hidden=64, in_dim=feature_dim, out=2):\n        super().__init__()\n        act_layer = {\n            \"relu\": nn.ReLU(),\n            \"leaky_relu\": nn.LeakyReLU(0.01),\n            \"gelu\": nn.GELU(),\n            \"tanh\": nn.Tanh(),\n        }[act_name]\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), act_layer, nn.Linear(hidden, out)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ---------------- evaluation -------------------------\ndef eval_model(model, loader, sigs_all, unseen):\n    model.eval()\n    tot = correct = tot_u = cor_u = 0\n    preds = []\n    with torch.no_grad():\n        idx = 0\n        for b in loader:\n            x, y = b[\"x\"].to(device), b[\"y\"].to(device)\n            pr = model(x).argmax(1)\n            preds.extend(pr.cpu().numpy())\n            tot += y.size(0)\n            correct += (pr == y).sum().item()\n            for p_true in pr.cpu().numpy():\n                sig = sigs_all[idx]\n                if sig in unseen:\n                    tot_u += 1\n                    if p_true == y.cpu().numpy()[idx % y.size(0)]:\n                        cor_u += 1\n                idx += 1\n    return correct / tot, (cor_u / tot_u if tot_u else 0.0), preds\n\n\n# ------------------- experiment loop -----------------\nexperiment_data = {\"activation_function\": {\"SPR_BENCH\": {}}}\nunseen_dev = {s for s in sig_dev if s not in set(sig_train)}\nunseen_test = {s for s in sig_test if s not in set(sig_train)}\nEPOCHS, LR, BS = 5, 1e-3, 64\nfor act in [\"relu\", \"leaky_relu\", \"gelu\", \"tanh\"]:\n    print(f\"\\n=== Activation: {act} ===\")\n    model = MLP(act).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=LR)\n    crit = nn.CrossEntropyLoss()\n    edict = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        loss_sum = 0\n        cor = tot = 0\n        for batch in train_loader(BS):\n            optim.zero_grad()\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            out = model(x)\n            loss = crit(out, y)\n            loss.backward()\n            optim.step()\n            loss_sum += loss.item() * y.size(0)\n            pr = out.argmax(1)\n            cor += (pr == y).sum().item()\n            tot += y.size(0)\n        train_loss, train_acc = loss_sum / tot, cor / tot\n        val_acc, val_ura, _ = eval_model(model, dev_loader, sig_dev, unseen_dev)\n        print(\n            f\"Ep{epoch}  loss={train_loss:.3f} acc={train_acc:.3f}  val={val_acc:.3f} ura={val_ura:.3f}\"\n        )\n        edict[\"losses\"][\"train\"].append(train_loss)\n        edict[\"metrics\"][\"train_acc\"].append(train_acc)\n        edict[\"metrics\"][\"val_acc\"].append(val_acc)\n        edict[\"metrics\"][\"val_ura\"].append(val_ura)\n        edict[\"timestamps\"].append(time.time())\n    test_acc, test_ura, test_preds = eval_model(\n        model, test_loader, sig_test, unseen_test\n    )\n    print(f\"Test acc={test_acc:.3f}  URA={test_ura:.3f}\")\n    edict[\"metrics\"][\"test_acc\"] = test_acc\n    edict[\"metrics\"][\"test_ura\"] = test_ura\n    edict[\"predictions\"] = test_preds\n    experiment_data[\"activation_function\"][\"SPR_BENCH\"][act] = edict\n\n# ------------------- save ----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, json, math\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ----------------- reproducibility -----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper symbolic functions -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ----------------- fallback synthetic data -----------------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ----------------- feature encoding -----------------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shape hist + colour hist + {seq_len,varieties}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        vec[shape_to_idx[tok[0]]] += 1\n        vec[26 + colour_to_idx[tok[1]]] += 1\n    vec[-3] = len(toks)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# encode once\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ntrain_loader_full = DataLoader(\n    SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ----------------- model -----------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ----------------- evaluation util -----------------\ndef eval_loader(model, loader, sigs_all, unseen_signatures):\n    model.eval()\n    correct = total = correct_unseen = total_unseen = 0\n    preds_all = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(dim=1)\n            preds_all.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_signatures:\n                    total_unseen += 1\n                    if p == y_true:\n                        correct_unseen += 1\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, preds_all\n\n\n# Identify unseen signatures\ntrain_signatures = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_signatures}\nunseen_test_sigs = {s for s in sig_test if s not in train_signatures}\n\n# ----------------- hyper-parameter tuning -----------------\nepoch_options = [5, 10, 20, 30]\nexperiment_data = {\"EPOCHS\": {}}\n\nfor num_epochs in epoch_options:\n    print(f\"\\n=== Training with {num_epochs} epochs ===\")\n    model = MLP(feature_dim).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # prepare fresh data loaders each run (shuffling matters)\n    train_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\n\n    run_key = f\"epochs_{num_epochs}\"\n    experiment_data[\"EPOCHS\"][run_key] = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        run_loss = run_corr = run_total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * y.size(0)\n            run_corr += (logits.argmax(dim=1) == y).sum().item()\n            run_total += y.size(0)\n        train_loss = run_loss / run_total\n        train_acc = run_corr / run_total\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev_sigs)\n\n        print(\n            f\"Epoch {epoch}/{num_epochs}: \"\n            f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n        ed = experiment_data[\"EPOCHS\"][run_key]\n        ed[\"losses\"][\"train\"].append(train_loss)\n        ed[\"metrics\"][\"train_acc\"].append(train_acc)\n        ed[\"metrics\"][\"val_acc\"].append(val_acc)\n        ed[\"metrics\"][\"val_ura\"].append(val_ura)\n        ed[\"timestamps\"].append(time.time())\n\n    # ----- final test evaluation -----\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test_sigs\n    )\n    print(f\"Test  acc={test_acc:.3f}  Test URA={test_ura:.3f}\")\n\n    experiment_data[\"EPOCHS\"][run_key][\"predictions\"] = test_preds\n    experiment_data[\"EPOCHS\"][run_key][\"test_acc\"] = test_acc\n    experiment_data[\"EPOCHS\"][run_key][\"test_ura\"] = test_ura\n\n# ----------------- save -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, json, math\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ----------------- reproducibility -----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper symbolic functions -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ----------------- fallback synthetic data -----------------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ----------------- feature encoding -----------------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shape hist + colour hist + {seq_len,varieties}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        vec[shape_to_idx[tok[0]]] += 1\n        vec[26 + colour_to_idx[tok[1]]] += 1\n    vec[-3] = len(toks)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# encode once\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ntrain_loader_full = DataLoader(\n    SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ----------------- model -----------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ----------------- evaluation util -----------------\ndef eval_loader(model, loader, sigs_all, unseen_signatures):\n    model.eval()\n    correct = total = correct_unseen = total_unseen = 0\n    preds_all = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(dim=1)\n            preds_all.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_signatures:\n                    total_unseen += 1\n                    if p == y_true:\n                        correct_unseen += 1\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, preds_all\n\n\n# Identify unseen signatures\ntrain_signatures = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_signatures}\nunseen_test_sigs = {s for s in sig_test if s not in train_signatures}\n\n# ----------------- hyper-parameter tuning -----------------\nepoch_options = [5, 10, 20, 30]\nexperiment_data = {\"EPOCHS\": {}}\n\nfor num_epochs in epoch_options:\n    print(f\"\\n=== Training with {num_epochs} epochs ===\")\n    model = MLP(feature_dim).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # prepare fresh data loaders each run (shuffling matters)\n    train_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\n\n    run_key = f\"epochs_{num_epochs}\"\n    experiment_data[\"EPOCHS\"][run_key] = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        run_loss = run_corr = run_total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * y.size(0)\n            run_corr += (logits.argmax(dim=1) == y).sum().item()\n            run_total += y.size(0)\n        train_loss = run_loss / run_total\n        train_acc = run_corr / run_total\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev_sigs)\n\n        print(\n            f\"Epoch {epoch}/{num_epochs}: \"\n            f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n        ed = experiment_data[\"EPOCHS\"][run_key]\n        ed[\"losses\"][\"train\"].append(train_loss)\n        ed[\"metrics\"][\"train_acc\"].append(train_acc)\n        ed[\"metrics\"][\"val_acc\"].append(val_acc)\n        ed[\"metrics\"][\"val_ura\"].append(val_ura)\n        ed[\"timestamps\"].append(time.time())\n\n    # ----- final test evaluation -----\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test_sigs\n    )\n    print(f\"Test  acc={test_acc:.3f}  Test URA={test_ura:.3f}\")\n\n    experiment_data[\"EPOCHS\"][run_key][\"predictions\"] = test_preds\n    experiment_data[\"EPOCHS\"][run_key][\"test_acc\"] = test_acc\n    experiment_data[\"EPOCHS\"][run_key][\"test_ura\"] = test_ura\n\n# ----------------- save -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, json, math\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, Dataset as HFDataset, DatasetDict\n\n# ----------------- reproducibility -----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper symbolic functions -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef rule_signature(sequence: str):\n    return (count_shape_variety(sequence), count_color_variety(sequence))\n\n\n# ----------------- fallback synthetic data -----------------\ndef random_token():\n    shape = random.choice(string.ascii_uppercase[:10])  # 10 shapes\n    colour = random.choice(string.digits[:5])  # 5 colours\n    return shape + colour\n\n\ndef generate_synthetic_split(n_rows: int, seed=0):\n    random.seed(seed)\n    seqs, labels = [], []\n    for _ in range(n_rows):\n        length = random.randint(3, 10)\n        seq = \" \".join(random_token() for _ in range(length))\n        lbl = int(count_shape_variety(seq) == count_color_variety(seq))\n        seqs.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(n_rows)), \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr_bench(root_path: pathlib.Path) -> DatasetDict:\n    if root_path.exists():\n        print(f\"Loading real SPR_BENCH from {root_path}\")\n\n        def _load(fname):\n            return load_dataset(\"csv\", data_files=str(root_path / fname), split=\"train\")\n\n        return DatasetDict(\n            train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n        )\n    print(\"SPR_BENCH not found \u2013 generating synthetic data\")\n    return DatasetDict(\n        train=HFDataset.from_dict(generate_synthetic_split(2000, seed=1)),\n        dev=HFDataset.from_dict(generate_synthetic_split(500, seed=2)),\n        test=HFDataset.from_dict(generate_synthetic_split(1000, seed=3)),\n    )\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\ndsets = load_spr_bench(DATA_PATH)\n\n# ----------------- feature encoding -----------------\nshape_to_idx = {ch: i for i, ch in enumerate(string.ascii_uppercase[:26])}\ncolour_to_idx = {d: i for i, d in enumerate(string.digits[:10])}\nfeature_dim = 26 + 10 + 3  # shape hist + colour hist + {seq_len,varieties}\n\n\ndef encode_sequence(seq: str) -> np.ndarray:\n    vec = np.zeros(feature_dim, dtype=np.float32)\n    toks = seq.split()\n    for tok in toks:\n        if len(tok) < 2:\n            continue\n        vec[shape_to_idx[tok[0]]] += 1\n        vec[26 + colour_to_idx[tok[1]]] += 1\n    vec[-3] = len(toks)\n    vec[-2] = count_shape_variety(seq)\n    vec[-1] = count_color_variety(seq)\n    return vec\n\n\ndef encode_dataset(hf_ds):\n    feats = np.stack([encode_sequence(s) for s in hf_ds[\"sequence\"]])\n    labels = np.array(hf_ds[\"label\"], dtype=np.int64)\n    sigs = [rule_signature(s) for s in hf_ds[\"sequence\"]]\n    return feats, labels, sigs\n\n\n# encode once\nX_train, y_train, sig_train = encode_dataset(dsets[\"train\"])\nX_dev, y_dev, sig_dev = encode_dataset(dsets[\"dev\"])\nX_test, y_test, sig_test = encode_dataset(dsets[\"test\"])\n\n\nclass SPRTorchDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X)\n        self.y = torch.tensor(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\ntrain_loader_full = DataLoader(\n    SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDS(X_dev, y_dev), batch_size=256)\ntest_loader = DataLoader(SPRTorchDS(X_test, y_test), batch_size=256)\n\n\n# ----------------- model -----------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden=64, n_classes=2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ----------------- evaluation util -----------------\ndef eval_loader(model, loader, sigs_all, unseen_signatures):\n    model.eval()\n    correct = total = correct_unseen = total_unseen = 0\n    preds_all = []\n    idx = 0\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            preds = logits.argmax(dim=1)\n            preds_all.extend(preds.cpu().numpy())\n            total += y.size(0)\n            correct += (preds == y).sum().item()\n            for p, y_true in zip(preds.cpu().numpy(), y.cpu().numpy()):\n                sig = sigs_all[idx]\n                if sig in unseen_signatures:\n                    total_unseen += 1\n                    if p == y_true:\n                        correct_unseen += 1\n                idx += 1\n    acc = correct / total\n    ura = correct_unseen / total_unseen if total_unseen else 0.0\n    return acc, ura, preds_all\n\n\n# Identify unseen signatures\ntrain_signatures = set(sig_train)\nunseen_dev_sigs = {s for s in sig_dev if s not in train_signatures}\nunseen_test_sigs = {s for s in sig_test if s not in train_signatures}\n\n# ----------------- hyper-parameter tuning -----------------\nepoch_options = [5, 10, 20, 30]\nexperiment_data = {\"EPOCHS\": {}}\n\nfor num_epochs in epoch_options:\n    print(f\"\\n=== Training with {num_epochs} epochs ===\")\n    model = MLP(feature_dim).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # prepare fresh data loaders each run (shuffling matters)\n    train_loader = DataLoader(SPRTorchDS(X_train, y_train), batch_size=64, shuffle=True)\n\n    run_key = f\"epochs_{num_epochs}\"\n    experiment_data[\"EPOCHS\"][run_key] = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_ura\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": y_test.tolist(),\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        run_loss = run_corr = run_total = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n            run_loss += loss.item() * y.size(0)\n            run_corr += (logits.argmax(dim=1) == y).sum().item()\n            run_total += y.size(0)\n        train_loss = run_loss / run_total\n        train_acc = run_corr / run_total\n        val_acc, val_ura, _ = eval_loader(model, dev_loader, sig_dev, unseen_dev_sigs)\n\n        print(\n            f\"Epoch {epoch}/{num_epochs}: \"\n            f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n            f\"val_acc={val_acc:.3f} URA={val_ura:.3f}\"\n        )\n\n        ed = experiment_data[\"EPOCHS\"][run_key]\n        ed[\"losses\"][\"train\"].append(train_loss)\n        ed[\"metrics\"][\"train_acc\"].append(train_acc)\n        ed[\"metrics\"][\"val_acc\"].append(val_acc)\n        ed[\"metrics\"][\"val_ura\"].append(val_ura)\n        ed[\"timestamps\"].append(time.time())\n\n    # ----- final test evaluation -----\n    test_acc, test_ura, test_preds = eval_loader(\n        model, test_loader, sig_test, unseen_test_sigs\n    )\n    print(f\"Test  acc={test_acc:.3f}  Test URA={test_ura:.3f}\")\n\n    experiment_data[\"EPOCHS\"][run_key][\"predictions\"] = test_preds\n    experiment_data[\"EPOCHS\"][run_key][\"test_acc\"] = test_acc\n    experiment_data[\"EPOCHS\"][run_key][\"test_ura\"] = test_ura\n\n# ----------------- save -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '\\n=== Training with 5 epochs ===', '\\n', 'Epoch 1/5: train_loss=0.5398\ntrain_acc=0.744 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/5: train_loss=0.4946\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/5: train_loss=0.4730\ntrain_acc=0.757 val_acc=0.742 URA=1.000', '\\n', 'Epoch 4/5: train_loss=0.4503\ntrain_acc=0.745 val_acc=0.734 URA=1.000', '\\n', 'Epoch 5/5: train_loss=0.4275\ntrain_acc=0.735 val_acc=0.726 URA=1.000', '\\n', 'Test  acc=0.732  Test\nURA=1.000', '\\n', '\\n=== Training with 10 epochs ===', '\\n', 'Epoch 1/10:\ntrain_loss=0.5750 train_acc=0.715 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/10:\ntrain_loss=0.5175 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/10:\ntrain_loss=0.4916 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 4/10:\ntrain_loss=0.4625 train_acc=0.756 val_acc=0.738 URA=1.000', '\\n', 'Epoch 5/10:\ntrain_loss=0.4375 train_acc=0.736 val_acc=0.716 URA=1.000', '\\n', 'Epoch 6/10:\ntrain_loss=0.4135 train_acc=0.721 val_acc=0.704 URA=1.000', '\\n', 'Epoch 7/10:\ntrain_loss=0.3960 train_acc=0.712 val_acc=0.688 URA=1.000', '\\n', 'Epoch 8/10:\ntrain_loss=0.3820 train_acc=0.722 val_acc=0.736 URA=1.000', '\\n', 'Epoch 9/10:\ntrain_loss=0.3696 train_acc=0.750 val_acc=0.722 URA=1.000', '\\n', 'Epoch 10/10:\ntrain_loss=0.3609 train_acc=0.768 val_acc=0.778 URA=1.000', '\\n', 'Test\nacc=0.802  Test URA=1.000', '\\n', '\\n=== Training with 20 epochs ===', '\\n',\n'Epoch 1/20: train_loss=0.5951 train_acc=0.661 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 2/20: train_loss=0.5174 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 3/20: train_loss=0.4997 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 4/20: train_loss=0.4839 train_acc=0.758 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 5/20: train_loss=0.4683 train_acc=0.755 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 6/20: train_loss=0.4470 train_acc=0.738 val_acc=0.724 URA=1.000', '\\n',\n'Epoch 7/20: train_loss=0.4284 train_acc=0.732 val_acc=0.718 URA=1.000', '\\n',\n'Epoch 8/20: train_loss=0.4106 train_acc=0.735 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 9/20: train_loss=0.3959 train_acc=0.757 val_acc=0.698 URA=1.000', '\\n',\n'Epoch 10/20: train_loss=0.3849 train_acc=0.761 val_acc=0.700 URA=1.000', '\\n',\n'Epoch 11/20: train_loss=0.3755 train_acc=0.766 val_acc=0.770 URA=1.000', '\\n',\n'Epoch 12/20: train_loss=0.3694 train_acc=0.799 val_acc=0.702 URA=1.000', '\\n',\n'Epoch 13/20: train_loss=0.3660 train_acc=0.781 val_acc=0.806 URA=1.000', '\\n',\n'Epoch 14/20: train_loss=0.3559 train_acc=0.819 val_acc=0.808 URA=1.000', '\\n',\n'Epoch 15/20: train_loss=0.3507 train_acc=0.826 val_acc=0.738 URA=1.000', '\\n',\n'Epoch 16/20: train_loss=0.3503 train_acc=0.817 val_acc=0.872 URA=1.000', '\\n',\n'Epoch 17/20: train_loss=0.3491 train_acc=0.837 val_acc=0.822 URA=1.000', '\\n',\n'Epoch 18/20: train_loss=0.3391 train_acc=0.846 val_acc=0.796 URA=1.000', '\\n',\n'Epoch 19/20: train_loss=0.3353 train_acc=0.833 val_acc=0.830 URA=1.000', '\\n',\n'Epoch 20/20: train_loss=0.3339 train_acc=0.856 val_acc=0.854 URA=1.000', '\\n',\n'Test  acc=0.878  Test URA=1.000', '\\n', '\\n=== Training with 30 epochs ===',\n'\\n', 'Epoch 1/30: train_loss=0.5483 train_acc=0.737 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2/30: train_loss=0.4979 train_acc=0.759 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 3/30: train_loss=0.4682 train_acc=0.759 val_acc=0.748 URA=1.000',\n'\\n', 'Epoch 4/30: train_loss=0.4374 train_acc=0.738 val_acc=0.702 URA=1.000',\n'\\n', 'Epoch 5/30: train_loss=0.4128 train_acc=0.714 val_acc=0.690 URA=1.000',\n'\\n', 'Epoch 6/30: train_loss=0.3936 train_acc=0.726 val_acc=0.764 URA=1.000',\n'\\n', 'Epoch 7/30: train_loss=0.3801 train_acc=0.749 val_acc=0.734 URA=1.000',\n'\\n', 'Epoch 8/30: train_loss=0.3687 train_acc=0.766 val_acc=0.742 URA=1.000',\n'\\n', 'Epoch 9/30: train_loss=0.3644 train_acc=0.801 val_acc=0.810 URA=1.000',\n'\\n', 'Epoch 10/30: train_loss=0.3535 train_acc=0.804 val_acc=0.812 URA=1.000',\n'\\n', 'Epoch 11/30: train_loss=0.3450 train_acc=0.819 val_acc=0.798 URA=1.000',\n'\\n', 'Epoch 12/30: train_loss=0.3400 train_acc=0.842 val_acc=0.834 URA=1.000',\n'\\n', 'Epoch 13/30: train_loss=0.3357 train_acc=0.851 val_acc=0.856 URA=1.000',\n'\\n', 'Epoch 14/30: train_loss=0.3256 train_acc=0.846 val_acc=0.908 URA=1.000',\n'\\n', 'Epoch 15/30: train_loss=0.3229 train_acc=0.866 val_acc=0.732 URA=1.000',\n'\\n', 'Epoch 16/30: train_loss=0.3108 train_acc=0.873 val_acc=0.836 URA=1.000',\n'\\n', 'Epoch 17/30: train_loss=0.3012 train_acc=0.877 val_acc=0.850 URA=1.000',\n'\\n', 'Epoch 18/30: train_loss=0.2930 train_acc=0.883 val_acc=0.884 URA=1.000',\n'\\n', 'Epoch 19/30: train_loss=0.2832 train_acc=0.900 val_acc=0.896 URA=1.000',\n'\\n', 'Epoch 20/30: train_loss=0.2767 train_acc=0.890 val_acc=0.904 URA=1.000',\n'\\n', 'Epoch 21/30: train_loss=0.2637 train_acc=0.907 val_acc=0.912 URA=1.000',\n'\\n', 'Epoch 22/30: train_loss=0.2514 train_acc=0.917 val_acc=0.898 URA=1.000',\n'\\n', 'Epoch 23/30: train_loss=0.2373 train_acc=0.921 val_acc=0.914 URA=1.000',\n'\\n', 'Epoch 24/30: train_loss=0.2248 train_acc=0.933 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 25/30: train_loss=0.2120 train_acc=0.934 val_acc=0.918 URA=1.000',\n'\\n', 'Epoch 26/30: train_loss=0.2001 train_acc=0.943 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 27/30: train_loss=0.1887 train_acc=0.945 val_acc=0.922 URA=1.000',\n'\\n', 'Epoch 28/30: train_loss=0.1816 train_acc=0.948 val_acc=0.924 URA=1.000',\n'\\n', 'Epoch 29/30: train_loss=0.1726 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Epoch 30/30: train_loss=0.1646 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Test  acc=0.939  Test URA=1.000', '\\n', '\\nSaved experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic\ndata', '\\n', '[lr=0.0003] epoch 1/8: train_loss=0.7435 train_acc=0.445\nval_acc=0.754 URA=1.000', '\\n', '[lr=0.0003] epoch 2/8: train_loss=0.5553\ntrain_acc=0.758 val_acc=0.756 URA=1.000', '\\n', '[lr=0.0003] epoch 3/8:\ntrain_loss=0.5225 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', '[lr=0.0003]\nepoch 4/8: train_loss=0.5172 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'[lr=0.0003] epoch 5/8: train_loss=0.5118 train_acc=0.759 val_acc=0.756\nURA=1.000', '\\n', '[lr=0.0003] epoch 6/8: train_loss=0.5061 train_acc=0.759\nval_acc=0.756 URA=1.000', '\\n', '[lr=0.0003] epoch 7/8: train_loss=0.4998\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', '[lr=0.0003] epoch 8/8:\ntrain_loss=0.4918 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', '--> lr=0.0003\nTEST acc=0.772  URA=1.000\\n', '\\n', '[lr=0.0005] epoch 1/8: train_loss=0.6362\ntrain_acc=0.623 val_acc=0.756 URA=1.000', '\\n', '[lr=0.0005] epoch 2/8:\ntrain_loss=0.5189 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', '[lr=0.0005]\nepoch 3/8: train_loss=0.5105 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'[lr=0.0005] epoch 4/8: train_loss=0.4981 train_acc=0.759 val_acc=0.756\nURA=1.000', '\\n', '[lr=0.0005] epoch 5/8: train_loss=0.4841 train_acc=0.758\nval_acc=0.756 URA=1.000', '\\n', '[lr=0.0005] epoch 6/8: train_loss=0.4703\ntrain_acc=0.758 val_acc=0.746 URA=1.000', '\\n', '[lr=0.0005] epoch 7/8:\ntrain_loss=0.4552 train_acc=0.749 val_acc=0.744 URA=1.000', '\\n', '[lr=0.0005]\nepoch 8/8: train_loss=0.4419 train_acc=0.741 val_acc=0.726 URA=1.000', '\\n',\n'--> lr=0.0005  TEST acc=0.742  URA=1.000\\n', '\\n', '[lr=0.001] epoch 1/8:\ntrain_loss=0.5699 train_acc=0.699 val_acc=0.756 URA=1.000', '\\n', '[lr=0.001]\nepoch 2/8: train_loss=0.5158 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'[lr=0.001] epoch 3/8: train_loss=0.4955 train_acc=0.759 val_acc=0.756\nURA=1.000', '\\n', '[lr=0.001] epoch 4/8: train_loss=0.4781 train_acc=0.758\nval_acc=0.752 URA=1.000', '\\n', '[lr=0.001] epoch 5/8: train_loss=0.4591\ntrain_acc=0.754 val_acc=0.750 URA=1.000', '\\n', '[lr=0.001] epoch 6/8:\ntrain_loss=0.4403 train_acc=0.748 val_acc=0.734 URA=1.000', '\\n', '[lr=0.001]\nepoch 7/8: train_loss=0.4241 train_acc=0.734 val_acc=0.710 URA=1.000', '\\n',\n'[lr=0.001] epoch 8/8: train_loss=0.4059 train_acc=0.725 val_acc=0.710\nURA=1.000', '\\n', '--> lr=0.001  TEST acc=0.734  URA=1.000\\n', '\\n', '[lr=0.002]\nepoch 1/8: train_loss=0.5125 train_acc=0.758 val_acc=0.756 URA=1.000', '\\n',\n'[lr=0.002] epoch 2/8: train_loss=0.4647 train_acc=0.754 val_acc=0.746\nURA=1.000', '\\n', '[lr=0.002] epoch 3/8: train_loss=0.4307 train_acc=0.747\nval_acc=0.716 URA=1.000', '\\n', '[lr=0.002] epoch 4/8: train_loss=0.3941\ntrain_acc=0.729 val_acc=0.792 URA=1.000', '\\n', '[lr=0.002] epoch 5/8:\ntrain_loss=0.3731 train_acc=0.771 val_acc=0.732 URA=1.000', '\\n', '[lr=0.002]\nepoch 6/8: train_loss=0.3586 train_acc=0.810 val_acc=0.814 URA=1.000', '\\n',\n'[lr=0.002] epoch 7/8: train_loss=0.3418 train_acc=0.835 val_acc=0.798\nURA=1.000', '\\n', '[lr=0.002] epoch 8/8: train_loss=0.3309 train_acc=0.851\nval_acc=0.860 URA=1.000', '\\n', '--> lr=0.002  TEST acc=0.876  URA=1.000\\n',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds\n(time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic\ndata', '\\n', '\\n=== Training with batch_size=16 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5047 train_acc=0.756 val_acc=0.754 URA=1.000', '\\n', 'Epoch 2:\ntrain_loss=0.4457 train_acc=0.742 val_acc=0.728 URA=1.000', '\\n', 'Epoch 3:\ntrain_loss=0.4051 train_acc=0.752 val_acc=0.708 URA=1.000', '\\n', 'Epoch 4:\ntrain_loss=0.3799 train_acc=0.778 val_acc=0.706 URA=1.000', '\\n', 'Epoch 5:\ntrain_loss=0.3592 train_acc=0.792 val_acc=0.748 URA=1.000', '\\n', 'Test\naccuracy=0.772  Test URA=1.000', '\\n', '\\n=== Training with batch_size=32 ===',\n'\\n', 'Epoch 1: train_loss=0.5487 train_acc=0.736 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.4933 train_acc=0.759 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 3: train_loss=0.4497 train_acc=0.749 val_acc=0.726 URA=1.000',\n'\\n', 'Epoch 4: train_loss=0.4140 train_acc=0.721 val_acc=0.696 URA=1.000',\n'\\n', 'Epoch 5: train_loss=0.3924 train_acc=0.749 val_acc=0.686 URA=1.000',\n'\\n', 'Test accuracy=0.713  Test URA=1.000', '\\n', '\\n=== Training with\nbatch_size=64 ===', '\\n', 'Epoch 1: train_loss=0.5112 train_acc=0.759\nval_acc=0.756 URA=1.000', '\\n', 'Epoch 2: train_loss=0.4799 train_acc=0.758\nval_acc=0.750 URA=1.000', '\\n', 'Epoch 3: train_loss=0.4517 train_acc=0.753\nval_acc=0.742 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4271 train_acc=0.732\nval_acc=0.708 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4042 train_acc=0.714\nval_acc=0.698 URA=1.000', '\\n', 'Test accuracy=0.716  Test URA=1.000', '\\n',\n'\\n=== Training with batch_size=128 ===', '\\n', 'Epoch 1: train_loss=0.5499\ntrain_acc=0.753 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2: train_loss=0.5100\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3: train_loss=0.4898\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4730\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4566\ntrain_acc=0.757 val_acc=0.756 URA=1.000', '\\n', 'Test accuracy=0.769  Test\nURA=1.000', '\\n', '\\n=== Training with batch_size=256 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5988 train_acc=0.717 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2:\ntrain_loss=0.5216 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3:\ntrain_loss=0.5222 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 4:\ntrain_loss=0.5152 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 5:\ntrain_loss=0.5052 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Test\naccuracy=0.772  Test URA=1.000', '\\n', '\\nSaved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-14_21-45-\n52_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '\\n--- Training with hidden size = 32 ---', '\\n', 'Hidden 32:\ntrain_acc=0.756  val_acc=0.760  URA=0.000', '\\n', '\\n--- Training with hidden\nsize = 64 ---', '\\n', 'Hidden 64: train_acc=0.742  val_acc=0.738  URA=0.000',\n'\\n', '\\n--- Training with hidden size = 128 ---', '\\n', 'Hidden 128:\ntrain_acc=0.733  val_acc=0.748  URA=0.000', '\\n', '\\n--- Training with hidden\nsize = 256 ---', '\\n', 'Hidden 256: train_acc=0.781  val_acc=0.830  URA=0.000',\n'\\n', '\\nBest hidden size = 256  Test Acc = 0.806  Test URA = 1.000', '\\n',\n'Saved results to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Execution time: 2 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic\ndata', '\\n', '\\n=== Training with weight_decay=0.0 ===', '\\n', 'epoch 1: loss\n0.5901, train_acc 0.672, val_acc 0.756, URA 1.000', '\\n', 'epoch 2: loss 0.5155,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 3: loss 0.4974,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 4: loss 0.4776,\ntrain_acc 0.757, val_acc 0.756, URA 1.000', '\\n', 'epoch 5: loss 0.4486,\ntrain_acc 0.748, val_acc 0.734, URA 1.000', '\\n', 'Test: acc 0.750, URA 1.000',\n'\\n', '\\n=== Training with weight_decay=1e-06 ===', '\\n', 'epoch 1: loss 0.6126,\ntrain_acc 0.673, val_acc 0.756, URA 1.000', '\\n', 'epoch 2: loss 0.5241,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 3: loss 0.5013,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 4: loss 0.4838,\ntrain_acc 0.758, val_acc 0.756, URA 1.000', '\\n', 'epoch 5: loss 0.4618,\ntrain_acc 0.755, val_acc 0.752, URA 1.000', '\\n', 'Test: acc 0.767, URA 1.000',\n'\\n', '\\n=== Training with weight_decay=1e-05 ===', '\\n', 'epoch 1: loss 0.5486,\ntrain_acc 0.739, val_acc 0.756, URA 1.000', '\\n', 'epoch 2: loss 0.4930,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 3: loss 0.4697,\ntrain_acc 0.758, val_acc 0.730, URA 1.000', '\\n', 'epoch 4: loss 0.4452,\ntrain_acc 0.749, val_acc 0.714, URA 1.000', '\\n', 'epoch 5: loss 0.4228,\ntrain_acc 0.728, val_acc 0.712, URA 1.000', '\\n', 'Test: acc 0.727, URA 1.000',\n'\\n', '\\n=== Training with weight_decay=0.0001 ===', '\\n', 'epoch 1: loss\n0.5247, train_acc 0.749, val_acc 0.756, URA 1.000', '\\n', 'epoch 2: loss 0.4887,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 3: loss 0.4636,\ntrain_acc 0.750, val_acc 0.744, URA 1.000', '\\n', 'epoch 4: loss 0.4417,\ntrain_acc 0.741, val_acc 0.720, URA 1.000', '\\n', 'epoch 5: loss 0.4223,\ntrain_acc 0.722, val_acc 0.702, URA 1.000', '\\n', 'Test: acc 0.719, URA 1.000',\n'\\n', '\\n=== Training with weight_decay=0.001 ===', '\\n', 'epoch 1: loss 0.5301,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 2: loss 0.5013,\ntrain_acc 0.759, val_acc 0.756, URA 1.000', '\\n', 'epoch 3: loss 0.4647,\ntrain_acc 0.754, val_acc 0.750, URA 1.000', '\\n', 'epoch 4: loss 0.4278,\ntrain_acc 0.731, val_acc 0.706, URA 1.000', '\\n', 'epoch 5: loss 0.4028,\ntrain_acc 0.731, val_acc 0.756, URA 1.000', '\\n', 'Test: acc 0.791, URA 1.000',\n'\\n', '\\n=== Training with weight_decay=0.01 ===', '\\n', 'epoch 1: loss 0.5154,\ntrain_acc 0.760, val_acc 0.756, URA 1.000', '\\n', 'epoch 2: loss 0.4825,\ntrain_acc 0.758, val_acc 0.754, URA 1.000', '\\n', 'epoch 3: loss 0.4593,\ntrain_acc 0.753, val_acc 0.748, URA 1.000', '\\n', 'epoch 4: loss 0.4370,\ntrain_acc 0.737, val_acc 0.720, URA 1.000', '\\n', 'epoch 5: loss 0.4156,\ntrain_acc 0.717, val_acc 0.718, URA 1.000', '\\n', 'Test: acc 0.728, URA 1.000',\n'\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-10/working/experiment_data.npy', '\\n', 'Execution time:\n3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '\\n===== Training with dropout_rate = 0.0 =====', '\\n', 'Epoch 1:\ntrain_loss=0.6356 train_acc=0.636 val_loss=0.5284 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.5201 train_acc=0.759 val_loss=0.5089 val_acc=0.756\nURA=1.000', '\\n', 'Epoch 3: train_loss=0.5012 train_acc=0.759 val_loss=0.4897\nval_acc=0.756 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4787 train_acc=0.759\nval_loss=0.4651 val_acc=0.754 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4521\ntrain_acc=0.749 val_loss=0.4442 val_acc=0.726 URA=1.000', '\\n', 'TEST  acc=0.745\nURA=1.000', '\\n', '\\n===== Training with dropout_rate = 0.1 =====', '\\n', 'Epoch\n1: train_loss=0.5569 train_acc=0.726 val_loss=0.5149 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.5153 train_acc=0.759 val_loss=0.4989 val_acc=0.756\nURA=1.000', '\\n', 'Epoch 3: train_loss=0.4957 train_acc=0.759 val_loss=0.4824\nval_acc=0.756 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4780 train_acc=0.759\nval_loss=0.4654 val_acc=0.756 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4562\ntrain_acc=0.751 val_loss=0.4543 val_acc=0.756 URA=1.000', '\\n', 'TEST  acc=0.772\nURA=1.000', '\\n', '\\n===== Training with dropout_rate = 0.2 =====', '\\n', 'Epoch\n1: train_loss=0.5080 train_acc=0.758 val_loss=0.4816 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.4818 train_acc=0.758 val_loss=0.4618 val_acc=0.756\nURA=1.000', '\\n', 'Epoch 3: train_loss=0.4551 train_acc=0.750 val_loss=0.4396\nval_acc=0.732 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4253 train_acc=0.739\nval_loss=0.4221 val_acc=0.720 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4115\ntrain_acc=0.737 val_loss=0.4053 val_acc=0.688 URA=1.000', '\\n', 'TEST  acc=0.714\nURA=1.000', '\\n', '\\n===== Training with dropout_rate = 0.3 =====', '\\n', 'Epoch\n1: train_loss=0.5250 train_acc=0.757 val_loss=0.4921 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.4975 train_acc=0.759 val_loss=0.4717 val_acc=0.756\nURA=1.000', '\\n', 'Epoch 3: train_loss=0.4759 train_acc=0.756 val_loss=0.4565\nval_acc=0.756 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4576 train_acc=0.751\nval_loss=0.4439 val_acc=0.740 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4488\ntrain_acc=0.739 val_loss=0.4305 val_acc=0.740 URA=1.000', '\\n', 'TEST  acc=0.756\nURA=1.000', '\\n', '\\n===== Training with dropout_rate = 0.4 =====', '\\n', 'Epoch\n1: train_loss=0.6013 train_acc=0.688 val_loss=0.5186 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.5317 train_acc=0.755 val_loss=0.4995 val_acc=0.756\nURA=1.000', '\\n', 'Epoch 3: train_loss=0.5173 train_acc=0.757 val_loss=0.4869\nval_acc=0.756 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4954 train_acc=0.757\nval_loss=0.4748 val_acc=0.752 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4882\ntrain_acc=0.752 val_loss=0.4606 val_acc=0.752 URA=1.000', '\\n', 'TEST  acc=0.767\nURA=1.000', '\\n', '\\n===== Training with dropout_rate = 0.5 =====', '\\n', 'Epoch\n1: train_loss=0.5804 train_acc=0.698 val_loss=0.4939 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2: train_loss=0.5121 train_acc=0.750 val_loss=0.4761 val_acc=0.756\nURA=1.000', '\\n', 'Epoch 3: train_loss=0.4937 train_acc=0.759 val_loss=0.4673\nval_acc=0.756 URA=1.000', '\\n', 'Epoch 4: train_loss=0.4750 train_acc=0.755\nval_loss=0.4549 val_acc=0.754 URA=1.000', '\\n', 'Epoch 5: train_loss=0.4635\ntrain_acc=0.752 val_loss=0.4463 val_acc=0.752 URA=1.000', '\\n', 'TEST  acc=0.767\nURA=1.000', '\\n', \"\\nAll experiments finished and saved to\n'working/experiment_data.npy'\", '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '[layers=1] Epoch 1  loss=0.5901  train_acc=0.669  val_acc=0.756\nURA=1.000', '\\n', '[layers=1] Epoch 2  loss=0.5155  train_acc=0.759\nval_acc=0.756  URA=1.000', '\\n', '[layers=1] Epoch 3  loss=0.4974\ntrain_acc=0.759  val_acc=0.756  URA=1.000', '\\n', '[layers=1] Epoch 4\nloss=0.4776  train_acc=0.758  val_acc=0.756  URA=1.000', '\\n', 'Early stopping\ntriggered for depth 1', '\\n', '[layers=1] Test Accuracy=0.772  Test\nURA=1.000\\n', '\\n', '[layers=2] Epoch 1  loss=0.5226  train_acc=0.759\nval_acc=0.756  URA=1.000', '\\n', '[layers=2] Epoch 2  loss=0.4925\ntrain_acc=0.759  val_acc=0.756  URA=1.000', '\\n', '[layers=2] Epoch 3\nloss=0.4421  train_acc=0.750  val_acc=0.720  URA=1.000', '\\n', '[layers=2] Epoch\n4  loss=0.3865  train_acc=0.764  val_acc=0.772  URA=1.000', '\\n', '[layers=2]\nEpoch 5  loss=0.3386  train_acc=0.827  val_acc=0.902  URA=1.000', '\\n',\n'[layers=2] Epoch 6  loss=0.3046  train_acc=0.878  val_acc=0.920  URA=1.000',\n'\\n', '[layers=2] Epoch 7  loss=0.2720  train_acc=0.909  val_acc=0.890\nURA=1.000', '\\n', '[layers=2] Epoch 8  loss=0.2378  train_acc=0.927\nval_acc=0.918  URA=1.000', '\\n', '[layers=2] Epoch 9  loss=0.2083\ntrain_acc=0.929  val_acc=0.916  URA=1.000', '\\n', 'Early stopping triggered for\ndepth 2', '\\n', '[layers=2] Test Accuracy=0.926  Test URA=1.000\\n', '\\n',\n'[layers=3] Epoch 1  loss=0.5566  train_acc=0.737  val_acc=0.756  URA=1.000',\n'\\n', '[layers=3] Epoch 2  loss=0.4973  train_acc=0.759  val_acc=0.756\nURA=1.000', '\\n', '[layers=3] Epoch 3  loss=0.4262  train_acc=0.752\nval_acc=0.712  URA=1.000', '\\n', '[layers=3] Epoch 4  loss=0.2970\ntrain_acc=0.829  val_acc=0.836  URA=1.000', '\\n', '[layers=3] Epoch 5\nloss=0.1965  train_acc=0.926  val_acc=0.920  URA=1.000', '\\n', '[layers=3] Epoch\n6  loss=0.1305  train_acc=0.943  val_acc=0.978  URA=1.000', '\\n', '[layers=3]\nEpoch 7  loss=0.0635  train_acc=0.996  val_acc=1.000  URA=1.000', '\\n',\n'[layers=3] Epoch 8  loss=0.0236  train_acc=1.000  val_acc=1.000  URA=1.000',\n'\\n', '[layers=3] Epoch 9  loss=0.0108  train_acc=1.000  val_acc=1.000\nURA=1.000', '\\n', '[layers=3] Epoch 10  loss=0.0064  train_acc=1.000\nval_acc=1.000  URA=1.000', '\\n', 'Early stopping triggered for depth 3', '\\n',\n'[layers=3] Test Accuracy=1.000  Test URA=1.000\\n', '\\n', 'Execution time: 3\nseconds seconds (time limit is 30 minutes).']", "['Using', ' ', 'cuda', '\\n', 'SPR_BENCH not found \u2013 using synthetic', '\\n',\n'\\n=== Activation: relu ===', '\\n', 'Ep1  loss=0.590 acc=0.669  val=0.756\nura=1.000', '\\n', 'Ep2  loss=0.515 acc=0.759  val=0.756 ura=1.000', '\\n', 'Ep3\nloss=0.497 acc=0.759  val=0.756 ura=1.000', '\\n', 'Ep4  loss=0.478 acc=0.758\nval=0.756 ura=1.000', '\\n', 'Ep5  loss=0.449 acc=0.749  val=0.734 ura=1.000',\n'\\n', 'Test acc=0.750  URA=1.000', '\\n', '\\n=== Activation: leaky_relu ===',\n'\\n', 'Ep1  loss=0.613 acc=0.654  val=0.756 ura=1.000', '\\n', 'Ep2  loss=0.525\nacc=0.759  val=0.756 ura=1.000', '\\n', 'Ep3  loss=0.503 acc=0.759  val=0.756\nura=1.000', '\\n', 'Ep4  loss=0.485 acc=0.758  val=0.756 ura=1.000', '\\n', 'Ep5\nloss=0.464 acc=0.755  val=0.752 ura=1.000', '\\n', 'Test acc=0.767  URA=1.000',\n'\\n', '\\n=== Activation: gelu ===', '\\n', 'Ep1  loss=0.549 acc=0.720  val=0.756\nura=1.000', '\\n', 'Ep2  loss=0.490 acc=0.759  val=0.756 ura=1.000', '\\n', 'Ep3\nloss=0.467 acc=0.758  val=0.726 ura=1.000', '\\n', 'Ep4  loss=0.443 acc=0.743\nval=0.714 ura=1.000', '\\n', 'Ep5  loss=0.421 acc=0.728  val=0.708 ura=1.000',\n'\\n', 'Test acc=0.726  URA=1.000', '\\n', '\\n=== Activation: tanh ===', '\\n',\n'Ep1  loss=0.609 acc=0.659  val=0.756 ura=1.000', '\\n', 'Ep2  loss=0.514\nacc=0.759  val=0.756 ura=1.000', '\\n', 'Ep3  loss=0.488 acc=0.756  val=0.746\nura=1.000', '\\n', 'Ep4  loss=0.466 acc=0.750  val=0.728 ura=1.000', '\\n', 'Ep5\nloss=0.446 acc=0.741  val=0.724 ura=1.000', '\\n', 'Test acc=0.738  URA=1.000',\n'\\n', '\\nSaved experiment_data.npy', '\\n', 'Execution time: 2 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '\\n=== Training with 5 epochs ===', '\\n', 'Epoch 1/5: train_loss=0.5398\ntrain_acc=0.744 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/5: train_loss=0.4946\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/5: train_loss=0.4730\ntrain_acc=0.757 val_acc=0.742 URA=1.000', '\\n', 'Epoch 4/5: train_loss=0.4503\ntrain_acc=0.745 val_acc=0.734 URA=1.000', '\\n', 'Epoch 5/5: train_loss=0.4275\ntrain_acc=0.735 val_acc=0.726 URA=1.000', '\\n', 'Test  acc=0.732  Test\nURA=1.000', '\\n', '\\n=== Training with 10 epochs ===', '\\n', 'Epoch 1/10:\ntrain_loss=0.5750 train_acc=0.715 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/10:\ntrain_loss=0.5175 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/10:\ntrain_loss=0.4916 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 4/10:\ntrain_loss=0.4625 train_acc=0.756 val_acc=0.738 URA=1.000', '\\n', 'Epoch 5/10:\ntrain_loss=0.4375 train_acc=0.736 val_acc=0.716 URA=1.000', '\\n', 'Epoch 6/10:\ntrain_loss=0.4135 train_acc=0.721 val_acc=0.704 URA=1.000', '\\n', 'Epoch 7/10:\ntrain_loss=0.3960 train_acc=0.712 val_acc=0.688 URA=1.000', '\\n', 'Epoch 8/10:\ntrain_loss=0.3820 train_acc=0.722 val_acc=0.736 URA=1.000', '\\n', 'Epoch 9/10:\ntrain_loss=0.3696 train_acc=0.750 val_acc=0.722 URA=1.000', '\\n', 'Epoch 10/10:\ntrain_loss=0.3609 train_acc=0.768 val_acc=0.778 URA=1.000', '\\n', 'Test\nacc=0.802  Test URA=1.000', '\\n', '\\n=== Training with 20 epochs ===', '\\n',\n'Epoch 1/20: train_loss=0.5951 train_acc=0.661 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 2/20: train_loss=0.5174 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 3/20: train_loss=0.4997 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 4/20: train_loss=0.4839 train_acc=0.758 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 5/20: train_loss=0.4683 train_acc=0.755 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 6/20: train_loss=0.4470 train_acc=0.738 val_acc=0.724 URA=1.000', '\\n',\n'Epoch 7/20: train_loss=0.4284 train_acc=0.732 val_acc=0.718 URA=1.000', '\\n',\n'Epoch 8/20: train_loss=0.4106 train_acc=0.735 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 9/20: train_loss=0.3959 train_acc=0.757 val_acc=0.698 URA=1.000', '\\n',\n'Epoch 10/20: train_loss=0.3849 train_acc=0.761 val_acc=0.700 URA=1.000', '\\n',\n'Epoch 11/20: train_loss=0.3755 train_acc=0.766 val_acc=0.770 URA=1.000', '\\n',\n'Epoch 12/20: train_loss=0.3694 train_acc=0.799 val_acc=0.702 URA=1.000', '\\n',\n'Epoch 13/20: train_loss=0.3660 train_acc=0.781 val_acc=0.806 URA=1.000', '\\n',\n'Epoch 14/20: train_loss=0.3559 train_acc=0.819 val_acc=0.808 URA=1.000', '\\n',\n'Epoch 15/20: train_loss=0.3507 train_acc=0.826 val_acc=0.738 URA=1.000', '\\n',\n'Epoch 16/20: train_loss=0.3503 train_acc=0.817 val_acc=0.872 URA=1.000', '\\n',\n'Epoch 17/20: train_loss=0.3491 train_acc=0.837 val_acc=0.822 URA=1.000', '\\n',\n'Epoch 18/20: train_loss=0.3391 train_acc=0.846 val_acc=0.796 URA=1.000', '\\n',\n'Epoch 19/20: train_loss=0.3353 train_acc=0.833 val_acc=0.830 URA=1.000', '\\n',\n'Epoch 20/20: train_loss=0.3339 train_acc=0.856 val_acc=0.854 URA=1.000', '\\n',\n'Test  acc=0.878  Test URA=1.000', '\\n', '\\n=== Training with 30 epochs ===',\n'\\n', 'Epoch 1/30: train_loss=0.5483 train_acc=0.737 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2/30: train_loss=0.4979 train_acc=0.759 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 3/30: train_loss=0.4682 train_acc=0.759 val_acc=0.748 URA=1.000',\n'\\n', 'Epoch 4/30: train_loss=0.4374 train_acc=0.738 val_acc=0.702 URA=1.000',\n'\\n', 'Epoch 5/30: train_loss=0.4128 train_acc=0.714 val_acc=0.690 URA=1.000',\n'\\n', 'Epoch 6/30: train_loss=0.3936 train_acc=0.726 val_acc=0.764 URA=1.000',\n'\\n', 'Epoch 7/30: train_loss=0.3801 train_acc=0.749 val_acc=0.734 URA=1.000',\n'\\n', 'Epoch 8/30: train_loss=0.3687 train_acc=0.766 val_acc=0.742 URA=1.000',\n'\\n', 'Epoch 9/30: train_loss=0.3644 train_acc=0.801 val_acc=0.810 URA=1.000',\n'\\n', 'Epoch 10/30: train_loss=0.3535 train_acc=0.804 val_acc=0.812 URA=1.000',\n'\\n', 'Epoch 11/30: train_loss=0.3450 train_acc=0.819 val_acc=0.798 URA=1.000',\n'\\n', 'Epoch 12/30: train_loss=0.3400 train_acc=0.842 val_acc=0.834 URA=1.000',\n'\\n', 'Epoch 13/30: train_loss=0.3357 train_acc=0.851 val_acc=0.856 URA=1.000',\n'\\n', 'Epoch 14/30: train_loss=0.3256 train_acc=0.846 val_acc=0.908 URA=1.000',\n'\\n', 'Epoch 15/30: train_loss=0.3229 train_acc=0.866 val_acc=0.732 URA=1.000',\n'\\n', 'Epoch 16/30: train_loss=0.3108 train_acc=0.873 val_acc=0.836 URA=1.000',\n'\\n', 'Epoch 17/30: train_loss=0.3012 train_acc=0.877 val_acc=0.850 URA=1.000',\n'\\n', 'Epoch 18/30: train_loss=0.2930 train_acc=0.883 val_acc=0.884 URA=1.000',\n'\\n', 'Epoch 19/30: train_loss=0.2832 train_acc=0.900 val_acc=0.896 URA=1.000',\n'\\n', 'Epoch 20/30: train_loss=0.2767 train_acc=0.890 val_acc=0.904 URA=1.000',\n'\\n', 'Epoch 21/30: train_loss=0.2637 train_acc=0.907 val_acc=0.912 URA=1.000',\n'\\n', 'Epoch 22/30: train_loss=0.2514 train_acc=0.917 val_acc=0.898 URA=1.000',\n'\\n', 'Epoch 23/30: train_loss=0.2373 train_acc=0.921 val_acc=0.914 URA=1.000',\n'\\n', 'Epoch 24/30: train_loss=0.2248 train_acc=0.933 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 25/30: train_loss=0.2120 train_acc=0.934 val_acc=0.918 URA=1.000',\n'\\n', 'Epoch 26/30: train_loss=0.2001 train_acc=0.943 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 27/30: train_loss=0.1887 train_acc=0.945 val_acc=0.922 URA=1.000',\n'\\n', 'Epoch 28/30: train_loss=0.1816 train_acc=0.948 val_acc=0.924 URA=1.000',\n'\\n', 'Epoch 29/30: train_loss=0.1726 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Epoch 30/30: train_loss=0.1646 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Test  acc=0.939  Test URA=1.000', '\\n', '\\nSaved experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '\\n=== Training with 5 epochs ===', '\\n', 'Epoch 1/5: train_loss=0.5398\ntrain_acc=0.744 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/5: train_loss=0.4946\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/5: train_loss=0.4730\ntrain_acc=0.757 val_acc=0.742 URA=1.000', '\\n', 'Epoch 4/5: train_loss=0.4503\ntrain_acc=0.745 val_acc=0.734 URA=1.000', '\\n', 'Epoch 5/5: train_loss=0.4275\ntrain_acc=0.735 val_acc=0.726 URA=1.000', '\\n', 'Test  acc=0.732  Test\nURA=1.000', '\\n', '\\n=== Training with 10 epochs ===', '\\n', 'Epoch 1/10:\ntrain_loss=0.5750 train_acc=0.715 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/10:\ntrain_loss=0.5175 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/10:\ntrain_loss=0.4916 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 4/10:\ntrain_loss=0.4625 train_acc=0.756 val_acc=0.738 URA=1.000', '\\n', 'Epoch 5/10:\ntrain_loss=0.4375 train_acc=0.736 val_acc=0.716 URA=1.000', '\\n', 'Epoch 6/10:\ntrain_loss=0.4135 train_acc=0.721 val_acc=0.704 URA=1.000', '\\n', 'Epoch 7/10:\ntrain_loss=0.3960 train_acc=0.712 val_acc=0.688 URA=1.000', '\\n', 'Epoch 8/10:\ntrain_loss=0.3820 train_acc=0.722 val_acc=0.736 URA=1.000', '\\n', 'Epoch 9/10:\ntrain_loss=0.3696 train_acc=0.750 val_acc=0.722 URA=1.000', '\\n', 'Epoch 10/10:\ntrain_loss=0.3609 train_acc=0.768 val_acc=0.778 URA=1.000', '\\n', 'Test\nacc=0.802  Test URA=1.000', '\\n', '\\n=== Training with 20 epochs ===', '\\n',\n'Epoch 1/20: train_loss=0.5951 train_acc=0.661 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 2/20: train_loss=0.5174 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 3/20: train_loss=0.4997 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 4/20: train_loss=0.4839 train_acc=0.758 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 5/20: train_loss=0.4683 train_acc=0.755 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 6/20: train_loss=0.4470 train_acc=0.738 val_acc=0.724 URA=1.000', '\\n',\n'Epoch 7/20: train_loss=0.4284 train_acc=0.732 val_acc=0.718 URA=1.000', '\\n',\n'Epoch 8/20: train_loss=0.4106 train_acc=0.735 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 9/20: train_loss=0.3959 train_acc=0.757 val_acc=0.698 URA=1.000', '\\n',\n'Epoch 10/20: train_loss=0.3849 train_acc=0.761 val_acc=0.700 URA=1.000', '\\n',\n'Epoch 11/20: train_loss=0.3755 train_acc=0.766 val_acc=0.770 URA=1.000', '\\n',\n'Epoch 12/20: train_loss=0.3694 train_acc=0.799 val_acc=0.702 URA=1.000', '\\n',\n'Epoch 13/20: train_loss=0.3660 train_acc=0.781 val_acc=0.806 URA=1.000', '\\n',\n'Epoch 14/20: train_loss=0.3559 train_acc=0.819 val_acc=0.808 URA=1.000', '\\n',\n'Epoch 15/20: train_loss=0.3507 train_acc=0.826 val_acc=0.738 URA=1.000', '\\n',\n'Epoch 16/20: train_loss=0.3503 train_acc=0.817 val_acc=0.872 URA=1.000', '\\n',\n'Epoch 17/20: train_loss=0.3491 train_acc=0.837 val_acc=0.822 URA=1.000', '\\n',\n'Epoch 18/20: train_loss=0.3391 train_acc=0.846 val_acc=0.796 URA=1.000', '\\n',\n'Epoch 19/20: train_loss=0.3353 train_acc=0.833 val_acc=0.830 URA=1.000', '\\n',\n'Epoch 20/20: train_loss=0.3339 train_acc=0.856 val_acc=0.854 URA=1.000', '\\n',\n'Test  acc=0.878  Test URA=1.000', '\\n', '\\n=== Training with 30 epochs ===',\n'\\n', 'Epoch 1/30: train_loss=0.5483 train_acc=0.737 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2/30: train_loss=0.4979 train_acc=0.759 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 3/30: train_loss=0.4682 train_acc=0.759 val_acc=0.748 URA=1.000',\n'\\n', 'Epoch 4/30: train_loss=0.4374 train_acc=0.738 val_acc=0.702 URA=1.000',\n'\\n', 'Epoch 5/30: train_loss=0.4128 train_acc=0.714 val_acc=0.690 URA=1.000',\n'\\n', 'Epoch 6/30: train_loss=0.3936 train_acc=0.726 val_acc=0.764 URA=1.000',\n'\\n', 'Epoch 7/30: train_loss=0.3801 train_acc=0.749 val_acc=0.734 URA=1.000',\n'\\n', 'Epoch 8/30: train_loss=0.3687 train_acc=0.766 val_acc=0.742 URA=1.000',\n'\\n', 'Epoch 9/30: train_loss=0.3644 train_acc=0.801 val_acc=0.810 URA=1.000',\n'\\n', 'Epoch 10/30: train_loss=0.3535 train_acc=0.804 val_acc=0.812 URA=1.000',\n'\\n', 'Epoch 11/30: train_loss=0.3450 train_acc=0.819 val_acc=0.798 URA=1.000',\n'\\n', 'Epoch 12/30: train_loss=0.3400 train_acc=0.842 val_acc=0.834 URA=1.000',\n'\\n', 'Epoch 13/30: train_loss=0.3357 train_acc=0.851 val_acc=0.856 URA=1.000',\n'\\n', 'Epoch 14/30: train_loss=0.3256 train_acc=0.846 val_acc=0.908 URA=1.000',\n'\\n', 'Epoch 15/30: train_loss=0.3229 train_acc=0.866 val_acc=0.732 URA=1.000',\n'\\n', 'Epoch 16/30: train_loss=0.3108 train_acc=0.873 val_acc=0.836 URA=1.000',\n'\\n', 'Epoch 17/30: train_loss=0.3012 train_acc=0.877 val_acc=0.850 URA=1.000',\n'\\n', 'Epoch 18/30: train_loss=0.2930 train_acc=0.883 val_acc=0.884 URA=1.000',\n'\\n', 'Epoch 19/30: train_loss=0.2832 train_acc=0.900 val_acc=0.896 URA=1.000',\n'\\n', 'Epoch 20/30: train_loss=0.2767 train_acc=0.890 val_acc=0.904 URA=1.000',\n'\\n', 'Epoch 21/30: train_loss=0.2637 train_acc=0.907 val_acc=0.912 URA=1.000',\n'\\n', 'Epoch 22/30: train_loss=0.2514 train_acc=0.917 val_acc=0.898 URA=1.000',\n'\\n', 'Epoch 23/30: train_loss=0.2373 train_acc=0.921 val_acc=0.914 URA=1.000',\n'\\n', 'Epoch 24/30: train_loss=0.2248 train_acc=0.933 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 25/30: train_loss=0.2120 train_acc=0.934 val_acc=0.918 URA=1.000',\n'\\n', 'Epoch 26/30: train_loss=0.2001 train_acc=0.943 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 27/30: train_loss=0.1887 train_acc=0.945 val_acc=0.922 URA=1.000',\n'\\n', 'Epoch 28/30: train_loss=0.1816 train_acc=0.948 val_acc=0.924 URA=1.000',\n'\\n', 'Epoch 29/30: train_loss=0.1726 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Epoch 30/30: train_loss=0.1646 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Test  acc=0.939  Test URA=1.000', '\\n', '\\nSaved experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data',\n'\\n', '\\n=== Training with 5 epochs ===', '\\n', 'Epoch 1/5: train_loss=0.5398\ntrain_acc=0.744 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/5: train_loss=0.4946\ntrain_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/5: train_loss=0.4730\ntrain_acc=0.757 val_acc=0.742 URA=1.000', '\\n', 'Epoch 4/5: train_loss=0.4503\ntrain_acc=0.745 val_acc=0.734 URA=1.000', '\\n', 'Epoch 5/5: train_loss=0.4275\ntrain_acc=0.735 val_acc=0.726 URA=1.000', '\\n', 'Test  acc=0.732  Test\nURA=1.000', '\\n', '\\n=== Training with 10 epochs ===', '\\n', 'Epoch 1/10:\ntrain_loss=0.5750 train_acc=0.715 val_acc=0.756 URA=1.000', '\\n', 'Epoch 2/10:\ntrain_loss=0.5175 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 3/10:\ntrain_loss=0.4916 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n', 'Epoch 4/10:\ntrain_loss=0.4625 train_acc=0.756 val_acc=0.738 URA=1.000', '\\n', 'Epoch 5/10:\ntrain_loss=0.4375 train_acc=0.736 val_acc=0.716 URA=1.000', '\\n', 'Epoch 6/10:\ntrain_loss=0.4135 train_acc=0.721 val_acc=0.704 URA=1.000', '\\n', 'Epoch 7/10:\ntrain_loss=0.3960 train_acc=0.712 val_acc=0.688 URA=1.000', '\\n', 'Epoch 8/10:\ntrain_loss=0.3820 train_acc=0.722 val_acc=0.736 URA=1.000', '\\n', 'Epoch 9/10:\ntrain_loss=0.3696 train_acc=0.750 val_acc=0.722 URA=1.000', '\\n', 'Epoch 10/10:\ntrain_loss=0.3609 train_acc=0.768 val_acc=0.778 URA=1.000', '\\n', 'Test\nacc=0.802  Test URA=1.000', '\\n', '\\n=== Training with 20 epochs ===', '\\n',\n'Epoch 1/20: train_loss=0.5951 train_acc=0.661 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 2/20: train_loss=0.5174 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 3/20: train_loss=0.4997 train_acc=0.759 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 4/20: train_loss=0.4839 train_acc=0.758 val_acc=0.756 URA=1.000', '\\n',\n'Epoch 5/20: train_loss=0.4683 train_acc=0.755 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 6/20: train_loss=0.4470 train_acc=0.738 val_acc=0.724 URA=1.000', '\\n',\n'Epoch 7/20: train_loss=0.4284 train_acc=0.732 val_acc=0.718 URA=1.000', '\\n',\n'Epoch 8/20: train_loss=0.4106 train_acc=0.735 val_acc=0.750 URA=1.000', '\\n',\n'Epoch 9/20: train_loss=0.3959 train_acc=0.757 val_acc=0.698 URA=1.000', '\\n',\n'Epoch 10/20: train_loss=0.3849 train_acc=0.761 val_acc=0.700 URA=1.000', '\\n',\n'Epoch 11/20: train_loss=0.3755 train_acc=0.766 val_acc=0.770 URA=1.000', '\\n',\n'Epoch 12/20: train_loss=0.3694 train_acc=0.799 val_acc=0.702 URA=1.000', '\\n',\n'Epoch 13/20: train_loss=0.3660 train_acc=0.781 val_acc=0.806 URA=1.000', '\\n',\n'Epoch 14/20: train_loss=0.3559 train_acc=0.819 val_acc=0.808 URA=1.000', '\\n',\n'Epoch 15/20: train_loss=0.3507 train_acc=0.826 val_acc=0.738 URA=1.000', '\\n',\n'Epoch 16/20: train_loss=0.3503 train_acc=0.817 val_acc=0.872 URA=1.000', '\\n',\n'Epoch 17/20: train_loss=0.3491 train_acc=0.837 val_acc=0.822 URA=1.000', '\\n',\n'Epoch 18/20: train_loss=0.3391 train_acc=0.846 val_acc=0.796 URA=1.000', '\\n',\n'Epoch 19/20: train_loss=0.3353 train_acc=0.833 val_acc=0.830 URA=1.000', '\\n',\n'Epoch 20/20: train_loss=0.3339 train_acc=0.856 val_acc=0.854 URA=1.000', '\\n',\n'Test  acc=0.878  Test URA=1.000', '\\n', '\\n=== Training with 30 epochs ===',\n'\\n', 'Epoch 1/30: train_loss=0.5483 train_acc=0.737 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 2/30: train_loss=0.4979 train_acc=0.759 val_acc=0.756 URA=1.000',\n'\\n', 'Epoch 3/30: train_loss=0.4682 train_acc=0.759 val_acc=0.748 URA=1.000',\n'\\n', 'Epoch 4/30: train_loss=0.4374 train_acc=0.738 val_acc=0.702 URA=1.000',\n'\\n', 'Epoch 5/30: train_loss=0.4128 train_acc=0.714 val_acc=0.690 URA=1.000',\n'\\n', 'Epoch 6/30: train_loss=0.3936 train_acc=0.726 val_acc=0.764 URA=1.000',\n'\\n', 'Epoch 7/30: train_loss=0.3801 train_acc=0.749 val_acc=0.734 URA=1.000',\n'\\n', 'Epoch 8/30: train_loss=0.3687 train_acc=0.766 val_acc=0.742 URA=1.000',\n'\\n', 'Epoch 9/30: train_loss=0.3644 train_acc=0.801 val_acc=0.810 URA=1.000',\n'\\n', 'Epoch 10/30: train_loss=0.3535 train_acc=0.804 val_acc=0.812 URA=1.000',\n'\\n', 'Epoch 11/30: train_loss=0.3450 train_acc=0.819 val_acc=0.798 URA=1.000',\n'\\n', 'Epoch 12/30: train_loss=0.3400 train_acc=0.842 val_acc=0.834 URA=1.000',\n'\\n', 'Epoch 13/30: train_loss=0.3357 train_acc=0.851 val_acc=0.856 URA=1.000',\n'\\n', 'Epoch 14/30: train_loss=0.3256 train_acc=0.846 val_acc=0.908 URA=1.000',\n'\\n', 'Epoch 15/30: train_loss=0.3229 train_acc=0.866 val_acc=0.732 URA=1.000',\n'\\n', 'Epoch 16/30: train_loss=0.3108 train_acc=0.873 val_acc=0.836 URA=1.000',\n'\\n', 'Epoch 17/30: train_loss=0.3012 train_acc=0.877 val_acc=0.850 URA=1.000',\n'\\n', 'Epoch 18/30: train_loss=0.2930 train_acc=0.883 val_acc=0.884 URA=1.000',\n'\\n', 'Epoch 19/30: train_loss=0.2832 train_acc=0.900 val_acc=0.896 URA=1.000',\n'\\n', 'Epoch 20/30: train_loss=0.2767 train_acc=0.890 val_acc=0.904 URA=1.000',\n'\\n', 'Epoch 21/30: train_loss=0.2637 train_acc=0.907 val_acc=0.912 URA=1.000',\n'\\n', 'Epoch 22/30: train_loss=0.2514 train_acc=0.917 val_acc=0.898 URA=1.000',\n'\\n', 'Epoch 23/30: train_loss=0.2373 train_acc=0.921 val_acc=0.914 URA=1.000',\n'\\n', 'Epoch 24/30: train_loss=0.2248 train_acc=0.933 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 25/30: train_loss=0.2120 train_acc=0.934 val_acc=0.918 URA=1.000',\n'\\n', 'Epoch 26/30: train_loss=0.2001 train_acc=0.943 val_acc=0.920 URA=1.000',\n'\\n', 'Epoch 27/30: train_loss=0.1887 train_acc=0.945 val_acc=0.922 URA=1.000',\n'\\n', 'Epoch 28/30: train_loss=0.1816 train_acc=0.948 val_acc=0.924 URA=1.000',\n'\\n', 'Epoch 29/30: train_loss=0.1726 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Epoch 30/30: train_loss=0.1646 train_acc=0.954 val_acc=0.934 URA=1.000',\n'\\n', 'Test  acc=0.939  Test URA=1.000', '\\n', '\\nSaved experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution output shows that the training script ran successfully without any\nerrors or bugs. The model was trained with different epoch configurations (5,\n10, 20, 30 epochs), and the results indicate an improvement in both training and\nvalidation metrics as the number of epochs increased. The Test URA metric\nconsistently achieved a perfect score of 1.000 across all configurations,\nindicating strong generalization to unseen rules. The experiment data was saved\nsuccessfully as 'experiment_data.npy'. No bugs or issues were found in the\nexecution.", "", "", "", "", "The training script executed successfully without any errors or bugs. The\ndropout rate hyperparameter search was conducted systematically, and results for\nvarious dropout rates were properly logged. The training, validation, and test\nmetrics were computed and displayed as expected. The outputs were saved to a\nfile for further analysis. No issues were detected in the code or output.", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.9535, "best_value": 0.9535}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.1646, "best_value": 0.1646}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 0.934, "best_value": 0.934}]}, {"metric_name": "validation URA", "lower_is_better": false, "description": "URA metric of the model on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 0.939, "best_value": 0.939}]}, {"metric_name": "test URA", "lower_is_better": false, "description": "URA metric of the model on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": 0.759, "best_value": 0.759}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": 0.7405, "best_value": 0.7405}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": 0.7245, "best_value": 0.7245}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": 0.851, "best_value": 0.851}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": 0.4918, "best_value": 0.4918}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": 0.4419, "best_value": 0.4419}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": 0.4059, "best_value": 0.4059}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": 0.3309, "best_value": 0.3309}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": 0.756, "best_value": 0.756}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": 0.756, "best_value": 0.756}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": 0.756, "best_value": 0.756}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": 0.86, "best_value": 0.86}]}, {"metric_name": "validation URA", "lower_is_better": false, "description": "URA metric for validation dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": null, "best_value": null}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": null, "best_value": null}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": null, "best_value": null}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": null, "best_value": null}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": 0.772, "best_value": 0.772}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": 0.742, "best_value": 0.742}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": 0.734, "best_value": 0.734}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": 0.876, "best_value": 0.876}]}, {"metric_name": "test URA", "lower_is_better": false, "description": "URA metric for test dataset.", "data": [{"dataset_name": "SPR_BENCH (learning rate = 0.0003)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (learning rate = 0.0005)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (learning rate = 0.001)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (learning rate = 0.002)", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "The ratio of correctly predicted instances to the total instances.", "data": [{"dataset_name": "train", "final_value": 0.792, "best_value": 0.792}, {"dataset_name": "validation", "final_value": 0.756, "best_value": 0.756}, {"dataset_name": "test", "final_value": 0.772, "best_value": 0.772}]}, {"metric_name": "URA", "lower_is_better": false, "description": "URA metric for validation and test datasets.", "data": [{"dataset_name": "validation", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "test", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The error value calculated during training and validation.", "data": [{"dataset_name": "train", "final_value": 0.3592, "best_value": 0.3592}, {"dataset_name": "validation", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "The proportion of correct predictions made by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.806, "best_value": 0.83}]}, {"metric_name": "URA", "lower_is_better": false, "description": "Unspecified metric URA; higher values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The measure of error or difference between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Final training accuracy of the model for each dataset.", "data": [{"dataset_name": "weight_decay = 0.0", "final_value": 0.748, "best_value": 0.748}, {"dataset_name": "weight_decay = 1e-06", "final_value": 0.755, "best_value": 0.755}, {"dataset_name": "weight_decay = 1e-05", "final_value": 0.7275, "best_value": 0.7275}, {"dataset_name": "weight_decay = 0.0001", "final_value": 0.722, "best_value": 0.722}, {"dataset_name": "weight_decay = 0.001", "final_value": 0.731, "best_value": 0.731}, {"dataset_name": "weight_decay = 0.01", "final_value": 0.7165, "best_value": 0.7165}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss of the model for each dataset.", "data": [{"dataset_name": "weight_decay = 0.0", "final_value": 0.4486, "best_value": 0.4486}, {"dataset_name": "weight_decay = 1e-06", "final_value": 0.4618, "best_value": 0.4618}, {"dataset_name": "weight_decay = 1e-05", "final_value": 0.4228, "best_value": 0.4228}, {"dataset_name": "weight_decay = 0.0001", "final_value": 0.4223, "best_value": 0.4223}, {"dataset_name": "weight_decay = 0.001", "final_value": 0.4028, "best_value": 0.4028}, {"dataset_name": "weight_decay = 0.01", "final_value": 0.4156, "best_value": 0.4156}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy of the model for each dataset.", "data": [{"dataset_name": "weight_decay = 0.0", "final_value": 0.734, "best_value": 0.734}, {"dataset_name": "weight_decay = 1e-06", "final_value": 0.752, "best_value": 0.752}, {"dataset_name": "weight_decay = 1e-05", "final_value": 0.712, "best_value": 0.712}, {"dataset_name": "weight_decay = 0.0001", "final_value": 0.702, "best_value": 0.702}, {"dataset_name": "weight_decay = 0.001", "final_value": 0.756, "best_value": 0.756}, {"dataset_name": "weight_decay = 0.01", "final_value": 0.718, "best_value": 0.718}]}, {"metric_name": "validation unseen-rule accuracy", "lower_is_better": false, "description": "Final validation unseen-rule accuracy of the model for each dataset.", "data": [{"dataset_name": "weight_decay = 0.0", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 1e-06", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 1e-05", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 0.0001", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 0.001", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 0.01", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Final test accuracy of the model for each dataset.", "data": [{"dataset_name": "weight_decay = 0.0", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "weight_decay = 1e-06", "final_value": 0.767, "best_value": 0.767}, {"dataset_name": "weight_decay = 1e-05", "final_value": 0.727, "best_value": 0.727}, {"dataset_name": "weight_decay = 0.0001", "final_value": 0.719, "best_value": 0.719}, {"dataset_name": "weight_decay = 0.001", "final_value": 0.791, "best_value": 0.791}, {"dataset_name": "weight_decay = 0.01", "final_value": 0.728, "best_value": 0.728}]}, {"metric_name": "test unseen-rule accuracy", "lower_is_better": false, "description": "Final test unseen-rule accuracy of the model for each dataset.", "data": [{"dataset_name": "weight_decay = 0.0", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 1e-06", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 1e-05", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 0.0001", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 0.001", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "weight_decay = 0.01", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.752, "best_value": 0.752}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.756, "best_value": 0.756}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4463, "best_value": 0.4053}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.772, "best_value": 0.772}]}, {"metric_name": "test URA", "lower_is_better": false, "description": "URA metric of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation URA", "lower_is_better": false, "description": "Measures the validation URA metric, where higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0064, "best_value": 0.0064}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training. Lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4206, "best_value": 0.4206}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy on the training dataset. Higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7555, "best_value": 0.7555}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset. Higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.752, "best_value": 0.752}]}, {"metric_name": "validation unseen-rule accuracy", "lower_is_better": false, "description": "The accuracy on unseen rules in the validation dataset. Higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy on the test dataset. Higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.767, "best_value": 0.767}]}, {"metric_name": "test unseen-rule accuracy", "lower_is_better": false, "description": "The accuracy on unseen rules in the test dataset. Higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.9535, "best_value": 0.9535}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.1646, "best_value": 0.1646}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 0.934, "best_value": 0.934}]}, {"metric_name": "validation URA", "lower_is_better": false, "description": "The URA metric of the model on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 0.939, "best_value": 0.939}]}, {"metric_name": "test URA", "lower_is_better": false, "description": "The URA metric of the model on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.9535, "best_value": 0.9535}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.1646, "best_value": 0.1646}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 0.934, "best_value": 0.934}]}, {"metric_name": "validation URA", "lower_is_better": false, "description": "URA metric on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 0.939, "best_value": 0.939}]}, {"metric_name": "test URA", "lower_is_better": false, "description": "URA metric on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.9535, "best_value": 0.9535}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "Train", "final_value": 0.1646, "best_value": 0.1646}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 0.934, "best_value": 0.934}]}, {"metric_name": "validation URA", "lower_is_better": false, "description": "URA metric for the validation dataset.", "data": [{"dataset_name": "Validation", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "Test", "final_value": 0.939, "best_value": 0.939}]}, {"metric_name": "test URA", "lower_is_better": false, "description": "URA metric for the test dataset.", "data": [{"dataset_name": "Test", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_5_acc_curves.png", "../../logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_10_acc_curves.png", "../../logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_20_acc_curves.png", "../../logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_30_acc_curves.png", "../../logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_test_metrics_comparison.png"], ["../../logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_training_loss_curves.png", "../../logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_validation_URA_curves.png", "../../logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_test_accuracy_per_lr.png"], ["../../logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_train_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_val_URA.png", "../../logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_train_loss.png", "../../logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_test_URA_bar.png"], ["../../logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_train_acc_vs_hidden.png", "../../logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_val_acc_vs_hidden.png", "../../logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_val_ura_vs_hidden.png", "../../logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_best_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_train_loss.png", "../../logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_train_val_acc.png", "../../logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_val_ura.png", "../../logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_test_URA.png"], ["../../logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_URA_curves.png", "../../logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_test_accuracy_vs_dropout.png"], ["../../logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_train_loss_by_layers.png", "../../logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_val_accuracy_by_layers.png", "../../logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_val_URA_by_layers.png", "../../logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_test_accuracy_bar.png"], ["../../logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_val_ura_curves.png", "../../logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_training_loss_curves.png", "../../logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_test_accuracy_bars.png", "../../logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_test_ura_bars.png"], ["../../logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_5_acc_curves.png", "../../logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_10_acc_curves.png", "../../logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_20_acc_curves.png", "../../logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_30_acc_curves.png", "../../logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_test_metrics_comparison.png"], ["../../logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_5_acc_curves.png", "../../logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_10_acc_curves.png", "../../logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_20_acc_curves.png", "../../logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_30_acc_curves.png", "../../logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_test_metrics_comparison.png"], ["../../logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_5_acc_curves.png", "../../logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_10_acc_curves.png", "../../logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_20_acc_curves.png", "../../logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_30_acc_curves.png", "../../logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_test_metrics_comparison.png"], ["../../logs/0-run/experiment_results/seed_aggregation_1a8024e471ea446194763985414b58c2/SPR_BENCH_aggregate_epoch_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_1a8024e471ea446194763985414b58c2/SPR_BENCH_aggregate_test_metrics.png"]], "plot_paths": [["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_5_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_10_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_20_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_30_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_test_metrics_comparison.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_training_loss_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_validation_URA_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_test_accuracy_per_lr.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_train_val_accuracy.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_val_URA.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_train_loss.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_test_URA_bar.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_train_acc_vs_hidden.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_val_acc_vs_hidden.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_val_ura_vs_hidden.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_best_test_metrics.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_train_loss.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_train_val_acc.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_val_ura.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_test_accuracy.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_test_URA.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_loss_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_URA_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_test_accuracy_vs_dropout.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_train_loss_by_layers.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_val_accuracy_by_layers.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_val_URA_by_layers.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_test_accuracy_bar.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_val_ura_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_training_loss_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_test_accuracy_bars.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_test_ura_bars.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_5_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_10_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_20_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_30_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_test_metrics_comparison.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_5_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_10_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_20_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_30_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_test_metrics_comparison.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_5_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_10_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_20_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_30_acc_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_test_metrics_comparison.png"], ["experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_1a8024e471ea446194763985414b58c2/SPR_BENCH_aggregate_epoch_curves.png", "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_1a8024e471ea446194763985414b58c2/SPR_BENCH_aggregate_test_metrics.png"]], "plot_analyses": [[{"analysis": "The plot shows that the training accuracy starts at around 0.75 and slightly increases during the first few epochs but subsequently decreases. Validation accuracy follows a similar trend, peaking early and then declining. On the other hand, the validation URA remains consistently high at 1.0 throughout all epochs, indicating that the model maintains its ability to generalize to unseen rules. The decline in both training and validation accuracy suggests potential underfitting or a mismatch between the training process and the evaluation metrics.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_5_acc_curves.png"}, {"analysis": "In this plot, the training accuracy gradually increases after an initial dip, while the validation accuracy follows a similar pattern but with more pronounced fluctuations. The validation URA remains consistently high, indicating good generalization. The fluctuations in validation accuracy suggest that the model might be sensitive to the data or hyperparameter configurations, which could be addressed by fine-tuning the learning rate or batch size.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_10_acc_curves.png"}, {"analysis": "The training accuracy shows a steady improvement over 20 epochs, while the validation accuracy fluctuates but exhibits a general upward trend. The validation URA remains constant and high, demonstrating consistent generalization capabilities. The fluctuations in validation accuracy may point to overfitting to specific patterns in the validation set or variability in the data.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_20_acc_curves.png"}, {"analysis": "Training accuracy continues to improve over 30 epochs, reaching close to 0.95. Validation accuracy also improves significantly, stabilizing at around 0.9 after initial fluctuations. The validation URA remains consistently high, indicating that the model maintains its ability to generalize to unseen rules. The convergence of training and validation metrics suggests that the model is well-optimized under this configuration, with reduced overfitting or underfitting.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_epochs_30_acc_curves.png"}, {"analysis": "The bar chart compares test accuracy and test URA across different epoch settings. Test accuracy improves consistently as the number of epochs increases, peaking at around 0.9 for 30 epochs. Test URA remains at 1.0 across all settings, reaffirming the model's strong generalization capabilities. This indicates that longer training durations allow the model to better capture patterns in the data without compromising its ability to generalize.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_a0b741023dd44bacac647c1883961cd3_proc_2753346/SPR_BENCH_test_metrics_comparison.png"}], [{"analysis": "The plot compares training and validation accuracy for different learning rates over 8 epochs. The learning rate of 2e-3 (red curve) shows the fastest improvement in both training and validation accuracy, achieving the highest performance by the final epoch. However, the validation accuracy for this learning rate fluctuates slightly, indicating potential overfitting. Learning rates 3e-4 and 5e-4 exhibit slower but more stable convergence, with consistent validation accuracy throughout. The learning rate of 1e-3 shows moderate improvement but does not outperform 2e-3.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot visualizes the training loss over epochs for different learning rates. The learning rate of 2e-3 has the steepest decline in loss, indicating rapid learning, and achieves the lowest final loss. The learning rates of 1e-3 and 5e-4 also show consistent decline, but at a slower rate. The learning rate of 3e-4 demonstrates the slowest convergence, with the highest final loss among the tested rates. This suggests that a higher learning rate (e.g., 2e-3) may be more effective for faster convergence in this setup.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_training_loss_curves.png"}, {"analysis": "The plot shows that the Validation URA remains constant at 1.0 across all learning rates and epochs. This indicates that the model consistently generalizes well to unseen rules, regardless of the learning rate. The lack of variation suggests that the URA metric is not sensitive to hyperparameter changes in this experimental setup.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_validation_URA_curves.png"}, {"analysis": "The bar chart compares test accuracy across different learning rates, with URA values annotated above each bar. All learning rates achieve a URA of 1.0, confirming consistent generalization to unseen rules. However, the test accuracy varies, with the learning rate of 2e-3 achieving the highest accuracy. This reinforces the observation that 2e-3 is the most effective learning rate for this experiment, balancing both generalization and accuracy.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_df886e2c3cd34c8ba1722158151c1ce4_proc_2753347/SPR_BENCH_test_accuracy_per_lr.png"}], [{"analysis": "This plot illustrates the training and validation accuracy for various batch sizes over five epochs. Smaller batch sizes (e.g., 16 and 32) show more consistent validation accuracy trends compared to larger batch sizes (e.g., 128 and 256), which experience significant fluctuations. This indicates that smaller batch sizes might provide more stable generalization performance, though training accuracy for batch size 16 improves significantly towards the end, suggesting potential overfitting.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_train_val_accuracy.png"}, {"analysis": "This plot shows the Uniform Rule Accuracy (URA) across epochs for different batch sizes. The URA remains constant at 1.0 for all configurations, indicating that the model maintains perfect rule generalization across all epochs and batch sizes. This could suggest that URA is not sensitive to hyperparameter changes in this experiment.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_val_URA.png"}, {"analysis": "The training loss decreases consistently across epochs for all batch sizes, with smaller batch sizes (e.g., 16 and 32) showing a steeper decline. This indicates that smaller batch sizes might facilitate faster convergence. Larger batch sizes (e.g., 256) show slower loss reduction, which might be due to less frequent weight updates.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_train_loss.png"}, {"analysis": "This bar chart compares the final test accuracy for different batch sizes. The results are relatively close, with batch size 16 achieving the highest accuracy. This suggests that while batch size has some impact, the differences are not drastic, and smaller batch sizes might be slightly more effective for this task.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "This bar chart highlights the final test Uniform Rule Accuracy (URA) for different batch sizes. The URA is consistently 1.0 across all batch sizes, reinforcing the observation that the model is robust in generalizing rules regardless of batch size. This further confirms that URA is not a distinguishing metric for evaluating hyperparameter effects in this experiment.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0a932c7243324d73a97ea18a109a52a5_proc_2753348/SPR_BENCH_test_URA_bar.png"}], [{"analysis": "This plot shows the relationship between hidden size and training accuracy. Training accuracy initially decreases as hidden size increases from 50 to 150, reaching a minimum at 150. However, beyond 150, training accuracy improves significantly, peaking at a hidden size of 256. This suggests that larger hidden sizes are beneficial for capturing more complex patterns in the training data, although smaller sizes may lead to underfitting.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_train_acc_vs_hidden.png"}, {"analysis": "This plot illustrates how validation accuracy changes with hidden size. Similar to the training accuracy plot, validation accuracy decreases initially, reaching its lowest point at a hidden size of 100. It then increases steadily, achieving the highest validation accuracy at a hidden size of 256. This indicates that larger hidden sizes not only improve training accuracy but also enhance generalization to unseen data.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_val_acc_vs_hidden.png"}, {"analysis": "The plot indicates that the Validation URA metric remains constant across all hidden sizes. This suggests that the model's ability to generalize to unseen rules, as measured by URA, is not influenced by the size of the hidden layers. This could imply that the URA metric is more dependent on other factors, such as the model architecture or the nature of the dataset, rather than the hidden size.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_val_ura_vs_hidden.png"}, {"analysis": "This plot compares test accuracy and URA for the best-performing hidden size (256). The test accuracy is 0.806, which aligns with the trend observed in the earlier plots where larger hidden sizes improved performance. However, the URA metric remains constant at 1.0, consistent with the validation URA plot. This suggests that while accuracy improves with hidden size, the model's ability to generalize to unseen rules remains unaffected, indicating a potential limitation of the current approach in addressing zero-shot reasoning tasks.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_767a0e43d0484d1992ea88c907b69592_proc_2753349/SPR_BENCH_best_test_metrics.png"}], [{"analysis": "This plot shows the training loss over five epochs for different weight decay values. Lower weight decay values (e.g., wd=0.001 and wd=0.01) result in faster convergence and lower final training loss, indicating better optimization. Conversely, higher weight decay values (e.g., wd=0.0 and wd=1e-06) exhibit slower convergence and higher final training loss, suggesting potential under-regularization. This highlights the importance of tuning weight decay to balance regularization and optimization.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_train_loss.png"}, {"analysis": "This plot compares training and validation accuracy across epochs for various weight decay values. While training accuracy improves consistently for most configurations, validation accuracy fluctuates, especially for higher weight decay values (e.g., wd=0.01). This suggests overfitting in some cases. The weight decay values of wd=0.001 and wd=0.0001 appear to achieve a better balance between training and validation accuracy, indicating their effectiveness in promoting generalization.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_train_val_acc.png"}, {"analysis": "The plot demonstrates that the Unseen-Rule Accuracy (URA) remains constant at 1.0 across all epochs and weight decay values. This indicates that the model maintains perfect generalization to unseen rules regardless of the weight decay configuration. However, this flat trend might also imply that the metric is not sensitive to the experimental conditions or that the dataset is inherently easier for the model to generalize on.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_val_ura.png"}, {"analysis": "This bar chart reveals the final test accuracy for different weight decay values. The weight decay value of wd=0.001 achieves the highest accuracy, followed closely by wd=0.0001. Higher values like wd=0.01 and lower values like wd=0.0 result in slightly reduced accuracy, indicating that moderate weight decay is optimal for this task.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_test_accuracy.png"}, {"analysis": "This bar chart shows that the final Unseen-Rule Accuracy (URA) is consistently 1.0 across all weight decay values. This reinforces the observation from the earlier URA plot that the model is robust in generalizing to unseen rules, regardless of weight decay. However, the lack of variation in this metric limits its utility for distinguishing between configurations.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1d72aa6df5844bd2949dcddb7c831678_proc_2753346/SPR_BENCH_test_URA.png"}], [{"analysis": "This plot compares training and validation accuracy across different dropout rates (dr) over five epochs. The training accuracy generally increases with epochs, reaching a plateau by epoch 3 for most dropout rates. Validation accuracy follows a similar trend but shows slight divergence at higher dropout rates (e.g., dr=0.5). Dropout rates of 0.1 and 0.2 seem to balance training and validation accuracy well, suggesting they may be optimal for generalization. However, higher dropout rates (e.g., dr=0.5) result in reduced validation accuracy, indicating underfitting.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot illustrates training and validation loss trends across different dropout rates over five epochs. Loss decreases steadily for both training and validation sets, with losses stabilizing around epoch 3. Lower dropout rates (e.g., dr=0.1) achieve the lowest validation loss, indicating better model generalization. Higher dropout rates (e.g., dr=0.5) lead to higher validation loss, which aligns with the accuracy trends and suggests underfitting. The model performs best with moderate dropout rates that avoid overfitting while maintaining low validation loss.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows that the Validation URA metric remains constant at 1.0 across all epochs and dropout rates. This suggests that the model consistently retains its ability to generalize to unseen rules, regardless of the dropout rate. However, it also indicates that this metric might not be sensitive to dropout changes, and additional metrics should be considered for evaluating model generalization.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_URA_curves.png"}, {"analysis": "The plot shows final test accuracy as a function of dropout rate. Test accuracy peaks at a dropout rate of 0.1 (approximately 0.77) and decreases significantly at 0.2, suggesting over-regularization. Accuracy recovers at higher dropout rates (0.3-0.5) but does not surpass the peak at 0.1. This indicates that a dropout rate of 0.1 is optimal for maximizing test performance, while higher dropout rates lead to diminished performance due to underfitting.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_0111caa36b984185a59f6eb2fc713747_proc_2753349/SPR_BENCH_test_accuracy_vs_dropout.png"}], [{"analysis": "This plot shows the training loss decreasing over epochs for models with different numbers of hidden layers. The model with 3 hidden layers converges the fastest and achieves the lowest loss, indicating that it is learning the patterns in the data more effectively. The models with 1 and 2 hidden layers converge more slowly and have higher final losses, suggesting that they may lack the capacity to model the data as effectively as the 3-hidden-layer model. This demonstrates the benefit of deeper architectures in this task.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_train_loss_by_layers.png"}, {"analysis": "This plot illustrates the validation accuracy over epochs for models with different numbers of hidden layers. The model with 3 hidden layers achieves the highest validation accuracy and converges quickly, reaching near-perfect performance by the 6th epoch. The model with 2 hidden layers also performs well but has slightly lower accuracy and more fluctuations. The 1-hidden-layer model shows no improvement over epochs, remaining at a constant accuracy level, which suggests it might be underfitting or unable to capture the complexity of the task.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_val_accuracy_by_layers.png"}, {"analysis": "This plot shows the Validation URA (Unseen Rule Adaptation) metric remaining constant at 1.0 across all epochs and models. This indicates that all models are equally capable of generalizing to unseen rules, regardless of the number of hidden layers. This consistency in URA performance suggests that the neural-symbolic integration approach is effective in achieving robust zero-shot reasoning.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_val_URA_by_layers.png"}, {"analysis": "This bar chart depicts the final test accuracy for models with varying numbers of hidden layers. The model with 3 hidden layers achieves the highest test accuracy, followed by the model with 2 hidden layers, and then the model with 1 hidden layer. This trend highlights the positive impact of increasing the number of hidden layers on the model's ability to generalize to unseen test data. The results suggest that deeper models are better suited for the Synthetic PolyRule Reasoning task.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_1f764a0cd180465db11a8f8eb47369cb_proc_2753348/SPR_BENCH_test_accuracy_bar.png"}], [{"analysis": "The accuracy curves for train and validation sets across different activation functions (ReLU, Leaky ReLU, GELU, and Tanh) reveal that GELU shows a sharp increase in training accuracy but fails to maintain validation accuracy, indicating potential overfitting. ReLU and Leaky ReLU maintain a more stable validation accuracy, suggesting better generalization. Tanh appears to underperform in both training and validation accuracy, which could be due to its tendency to saturate gradients.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The URA (Unseen Rule Adaptation) metric remains constant at 1.0 across all epochs and activation functions. This suggests that the model consistently adapts to unseen rules regardless of the activation function used, indicating robustness in this aspect of the evaluation.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_val_ura_curves.png"}, {"analysis": "The training loss curves demonstrate that GELU achieves the lowest training loss, followed by Tanh, ReLU, and Leaky ReLU. However, considering the overfitting observed in the accuracy curves, the lower loss for GELU might not translate to better generalization. ReLU and Leaky ReLU exhibit a more balanced reduction in loss, aligning with their stable validation accuracy.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_training_loss_curves.png"}, {"analysis": "The test accuracy for different activation functions shows that ReLU, Leaky ReLU, and Tanh achieve comparable performance, with GELU slightly lagging behind. This aligns with the earlier observation that GELU might overfit to the training data.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_test_accuracy_bars.png"}, {"analysis": "The test URA metric is consistent across all activation functions, reinforcing that the model's ability to generalize to unseen rules is unaffected by the choice of activation function.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_158b00a089364abba8c9862257cb4b19_proc_2753347/SPR_BENCH_test_ura_bars.png"}], [{"analysis": "The plot shows accuracy and URA curves over 5 epochs. Train accuracy starts at a moderate value and gradually decreases, while validation accuracy starts slightly higher but also declines steadily. Validation URA remains constant at 1.0 throughout. The decreasing accuracy suggests potential underfitting or issues with the learning rate or batch size. The constant URA indicates that the model is consistently generalizing to unseen rules, but accuracy improvements are needed.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_5_acc_curves.png"}, {"analysis": "This plot extends the analysis to 10 epochs. Train accuracy initially increases slightly but then declines, while validation accuracy shows a similar declining trend with some fluctuations. Validation URA remains constant at 1.0. The fluctuations in validation accuracy indicate instability in the model's performance, potentially due to suboptimal hyperparameter settings. The consistent URA suggests that the model maintains generalization capabilities, but accuracy needs improvement for better performance.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_10_acc_curves.png"}, {"analysis": "With 20 epochs, train accuracy shows a gradual increase with fluctuations, while validation accuracy also fluctuates but shows an overall upward trend. Validation URA remains constant at 1.0. The increasing trend in both train and validation accuracy suggests that the model is starting to learn better with more epochs, but the fluctuations highlight instability, possibly due to overfitting or learning rate issues.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_20_acc_curves.png"}, {"analysis": "At 30 epochs, train accuracy continues to improve steadily, and validation accuracy shows a similar upward trend with reduced fluctuations. Validation URA remains constant at 1.0. This indicates that the model is stabilizing and learning effectively with more training. The consistent URA further confirms that the model retains its ability to generalize to unseen rules. This epoch setting appears to be the most promising for achieving better performance.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_epochs_30_acc_curves.png"}, {"analysis": "This bar chart compares test accuracy and URA across different epoch settings. Test accuracy improves significantly from 5 to 30 epochs, while test URA remains constant at 1.0 across all settings. This confirms that longer training periods lead to better accuracy without compromising the model's ability to generalize. The results highlight the importance of sufficient training epochs for this task.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/SPR_BENCH_test_metrics_comparison.png"}], [{"analysis": "The plot shows that the training accuracy starts at around 0.75 and decreases slightly as the epochs progress. Validation accuracy also starts at a similar value but decreases steadily over the epochs. Validation URA remains constant at 1.0 throughout, indicating that the model maintains its ability to generalize to unseen rules, despite a drop in accuracy metrics. This suggests potential overfitting or a learning rate that may need adjustment.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_5_acc_curves.png"}, {"analysis": "Training accuracy increases slightly in the earlier epochs but then declines, showing a non-monotonic trend. Validation accuracy decreases steadily, while validation URA remains constant at 1.0. This indicates that the model is not improving on unseen data and might be overfitting. The non-monotonic behavior in training accuracy suggests the need for better hyperparameter tuning, such as adjusting the learning rate or batch size.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_10_acc_curves.png"}, {"analysis": "Training accuracy shows a gradual increase over the epochs, while validation accuracy fluctuates significantly but shows an upward trend overall. Validation URA remains constant at 1.0. The fluctuations in validation accuracy suggest that the model might be sensitive to the data or hyperparameters. The upward trend in both training and validation accuracy indicates that the model benefits from additional epochs, but stability needs to be improved.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_20_acc_curves.png"}, {"analysis": "Training accuracy steadily increases and converges towards 0.95 by the end of 30 epochs. Validation accuracy also increases and stabilizes around 0.90. Validation URA remains constant at 1.0, indicating that the model consistently generalizes to unseen rules. The stability and convergence of both training and validation accuracy suggest that the chosen number of epochs and other hyperparameters are effective for this training setup.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_epochs_30_acc_curves.png"}, {"analysis": "Test accuracy improves as the number of epochs increases, with the highest accuracy achieved at 30 epochs. Test URA remains constant at 1.0 across all epoch settings, indicating consistent generalization to unseen rules. The results suggest that increasing the number of epochs improves performance, but further experiments with early stopping or fine-tuning other hyperparameters could optimize results further.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/SPR_BENCH_test_metrics_comparison.png"}], [{"analysis": "The plot shows the training accuracy, validation accuracy, and validation URA across 5 epochs. The training accuracy starts at a moderate level and decreases slightly over epochs, indicating some potential overfitting or instability in training. Validation accuracy shows a similar trend, dropping steadily over epochs, which might suggest that the model struggles to generalize well. Validation URA remains constant at 1.0, which could indicate that the metric is not sensitive to the changes in the model's performance or is saturated.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_5_acc_curves.png"}, {"analysis": "This plot extends the training and validation metrics to 10 epochs. Training accuracy initially increases slightly but then declines, while validation accuracy continues to drop, showing instability and poor generalization. Validation URA remains constant at 1.0, which raises concerns about whether it is effectively capturing performance variations. The model seems to underperform on validation data as training progresses.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_10_acc_curves.png"}, {"analysis": "The plot shows training and validation metrics over 20 epochs. Training accuracy improves gradually after an initial dip, reflecting some learning. Validation accuracy fluctuates significantly, indicating potential instability or sensitivity to the dataset. Validation URA remains constant at 1.0, suggesting it may not be a useful metric for capturing meaningful performance changes. The fluctuations in validation accuracy highlight challenges in achieving consistent generalization.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_20_acc_curves.png"}, {"analysis": "The plot illustrates training and validation metrics over 30 epochs. Training accuracy improves steadily and stabilizes, suggesting successful learning. Validation accuracy also stabilizes after initial fluctuations, indicating improved generalization. Validation URA remains constant at 1.0, continuing to question its utility. The overall trend indicates that longer training helps the model converge and generalize better.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_epochs_30_acc_curves.png"}, {"analysis": "The bar chart compares test accuracy and test URA across different epoch settings. Test accuracy improves with more epochs, reaching the highest value at 30 epochs. Test URA remains constant at 1.0 across all epoch settings, which is consistent with previous observations and further supports the notion that this metric might not be informative. The results suggest that increasing the number of epochs enhances test accuracy, but the lack of variability in URA limits its interpretability.", "plot_path": "experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/SPR_BENCH_test_metrics_comparison.png"}], []], "vlm_feedback_summary": ["The plots indicate consistent improvement in training and validation performance\nwith increasing epochs, with the validation URA metric remaining high\nthroughout. This suggests that the model effectively generalizes to unseen rules\nwhile benefiting from longer training durations. Fluctuations in validation\naccuracy during intermediate epochs highlight potential data variability or\nsensitivity to hyperparameters, which could be addressed with further fine-\ntuning.", "The plots provide valuable insights into the impact of learning rates on model\nperformance. The learning rate of 2e-3 emerges as the most effective, achieving\nthe highest accuracy and fastest convergence in training loss while maintaining\na URA of 1.0. Other learning rates show slower convergence or lower accuracy,\nbut the URA metric remains consistently high across all conditions, indicating\nrobust generalization to unseen rules.", "The analysis reveals that smaller batch sizes (e.g., 16 and 32) generally\nprovide better performance in terms of stability and convergence. The URA metric\nremains constant across all configurations, indicating that it is less sensitive\nto changes in hyperparameters. The training loss trends suggest faster\nconvergence with smaller batch sizes, while test accuracy differences are\nminimal but favor smaller batch sizes slightly. Overall, focusing on smaller\nbatch sizes might be beneficial for further optimization.", "The plots show a clear improvement in both training and validation accuracy with\nlarger hidden sizes, peaking at 256. However, the URA metric remains constant\nacross all configurations, suggesting that while accuracy improves, zero-shot\nreasoning capabilities do not. This highlights a potential limitation in the\ncurrent model's ability to generalize to unseen rules.", "The plots provide insights into the impact of weight decay on training loss,\naccuracy, and generalization. Moderate weight decay values (e.g., wd=0.001)\noptimize both training and validation performance while maintaining robustness\nin unseen-rule generalization (URA). However, the URA metric's lack of\nvariability suggests it may not be sensitive enough to evaluate finer\ndifferences in model performance.", "The analysis highlights that moderate dropout rates (e.g., 0.1) yield the best\nbalance between training and validation performance, as well as the highest test\naccuracy. Validation URA remains constant, suggesting stability in\ngeneralization to unseen rules, but this metric alone may not provide sufficient\ninsight into model performance.", "The experimental results demonstrate that increasing the number of hidden layers\nimproves the model's performance across training loss, validation accuracy, and\ntest accuracy. The Validation URA metric remains consistently high, indicating\neffective zero-shot reasoning capabilities across all models. The 3-hidden-layer\nmodel consistently outperforms the others, highlighting the benefits of deeper\narchitectures for this task.", "The provided plots indicate that ReLU and Leaky ReLU activation functions\nperform better in terms of generalization, as evidenced by stable validation\naccuracy and balanced loss reduction. GELU shows signs of overfitting despite\nachieving the lowest training loss. Tanh underperforms in accuracy but maintains\nconsistency in the URA metric. The URA metric remains robust across all\nactivation functions, suggesting the model's consistent ability to adapt to\nunseen rules.", "The plots indicate that increasing the number of epochs improves both train and\nvalidation accuracy, with the best performance observed at 30 epochs. Validation\nURA remains constant at 1.0 across all settings, demonstrating consistent\ngeneralization to unseen rules. The results suggest that longer training periods\nare beneficial, but further tuning of hyperparameters like learning rate and\nbatch size may help stabilize performance and reduce fluctuations.", "The plots indicate that increasing the number of epochs generally improves the\nmodel's performance, as seen in the upward trends in accuracy metrics.\nValidation and test URA remain consistently high, suggesting that the model\nmaintains its ability to generalize to unseen rules. However, fluctuations in\nvalidation accuracy and occasional signs of overfitting highlight the need for\nfurther hyperparameter tuning, such as adjusting the learning rate or\nimplementing early stopping.", "The plots reveal trends in training and validation performance across different\nepoch settings. Training and validation accuracy metrics show that longer\ntraining generally improves model performance, although early instability is\nevident. Validation URA remains constant at 1.0 across all plots, raising\nconcerns about its sensitivity and utility as a metric. The final bar chart\nconfirms that test accuracy benefits from extended training, while test URA\nremains constant, further questioning its effectiveness.", "[]"], "exec_time": [4.539248943328857, 3.286208152770996, 3.2545671463012695, 2.9346821308135986, 3.313300371170044, 3.2505505084991455, 3.1091134548187256, 2.8452260494232178, 4.810213327407837, 4.67740273475647, 4.859514951705933, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['plot 4'", "'plot 5']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"plot 4\"", "\"plot 5\"]"], ["['SPR_BENCH_30_epochs']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nsaved_files = []\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ----- per-run accuracy / URA curves -----\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    try:\n        epochs = list(range(1, len(run_data[\"metrics\"][\"train_acc\"]) + 1))\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, run_data[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_ura\"], label=\"Val URA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric\")\n        plt.title(f\"SPR_BENCH \u2013 {run_key} Accuracy/URA Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_{run_key}_acc_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {run_key}: {e}\")\n        plt.close()\n\n# ----- final test accuracy / URA bar plot -----\ntry:\n    run_names = []\n    test_accs = []\n    test_uras = []\n    for rk, rd in experiment_data.get(\"EPOCHS\", {}).items():\n        run_names.append(rk)\n        test_accs.append(rd.get(\"test_acc\", 0))\n        test_uras.append(rd.get(\"test_ura\", 0))\n    x = np.arange(len(run_names))\n    width = 0.35\n    plt.figure(figsize=(7, 4))\n    plt.bar(x - width / 2, test_accs, width, label=\"Test Acc\")\n    plt.bar(x + width / 2, test_uras, width, label=\"Test URA\")\n    plt.xticks(x, run_names, rotation=45)\n    plt.ylim(0, 1)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH \u2013 Test Accuracy vs URA by Epoch Setting\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_comparison.png\")\n    plt.tight_layout()\n    plt.savefig(fname)\n    saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison plot: {e}\")\n    plt.close()\n\nprint(\"Saved plots:\", saved_files)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- data loading ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nlr_values = ed[\"lr_values\"]  # list of lrs\ntrain_acc = ed[\"metrics\"][\"train_acc\"]  # list[n_lr][n_epoch]\nval_acc = ed[\"metrics\"][\"val_acc\"]\nval_ura = ed[\"metrics\"][\"val_ura\"]\ntrain_loss = ed[\"losses\"][\"train\"]\ntest_acc = ed[\"metrics\"][\"test_acc\"]  # list[n_lr]\ntest_ura = ed[\"metrics\"][\"test_ura\"]  # list[n_lr]\nepochs = np.arange(1, len(train_acc[0]) + 1)\n\n\n# ---------------- plotting helpers ----------------\ndef save_fig(fig_name):\n    plt.savefig(os.path.join(working_dir, fig_name), bbox_inches=\"tight\")\n    plt.close()\n\n\n# 1) accuracy curves ----------------------------------------------------------\ntry:\n    plt.figure()\n    for i, lr in enumerate(lr_values):\n        plt.plot(epochs, train_acc[i], label=f\"train lr={lr:.0e}\")\n        plt.plot(epochs, val_acc[i], \"--\", label=f\"val lr={lr:.0e}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\n        \"SPR_BENCH: Training vs Validation Accuracy\\nLeft: Train, Right (dashed): Val\"\n    )\n    plt.legend()\n    save_fig(\"SPR_BENCH_accuracy_curves.png\")\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# 2) training loss curves ------------------------------------------------------\ntry:\n    plt.figure()\n    for i, lr in enumerate(lr_values):\n        plt.plot(epochs, train_loss[i], label=f\"lr={lr:.0e}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training Loss over Epochs\")\n    plt.legend()\n    save_fig(\"SPR_BENCH_training_loss_curves.png\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 3) validation URA curves -----------------------------------------------------\ntry:\n    plt.figure()\n    for i, lr in enumerate(lr_values):\n        plt.plot(epochs, val_ura[i], label=f\"lr={lr:.0e}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Unseen Rule Accuracy (URA)\")\n    plt.title(\"SPR_BENCH: Validation URA over Epochs\")\n    plt.legend()\n    save_fig(\"SPR_BENCH_validation_URA_curves.png\")\nexcept Exception as e:\n    print(f\"Error creating URA plot: {e}\")\n    plt.close()\n\n# 4) test accuracy bar chart ---------------------------------------------------\ntry:\n    plt.figure()\n    x = np.arange(len(lr_values))\n    bars = plt.bar(x, test_acc, color=\"skyblue\")\n    plt.xticks(x, [f\"{lr:.0e}\" for lr in lr_values])\n    plt.ylim(0, 1)\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"SPR_BENCH: Test Accuracy per Learning-Rate\\nURA shown above each bar\")\n    for bar, ura in zip(bars, test_ura):\n        plt.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.02,\n            f\"URA={ura:.2f}\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=8,\n        )\n    save_fig(\"SPR_BENCH_test_accuracy_per_lr.png\")\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n\n# ---------------- console summary ----------------\nprint(\"\\n=== SPR_BENCH Test Metrics ===\")\nfor lr, acc, ura in zip(lr_values, test_acc, test_ura):\n    print(f\"lr={lr:.0e} | test_acc={acc:.3f} | test_URA={ura:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed is not None:\n    batch_sizes = ed[\"batch_sizes\"]\n    train_acc = ed[\"metrics\"][\"train_acc\"]  # list(len=batch_sizes)[epochs]\n    val_acc = ed[\"metrics\"][\"val_acc\"]\n    val_ura = ed[\"metrics\"][\"val_ura\"]\n    train_loss = ed[\"losses\"][\"train\"]\n    test_acc = ed[\"metrics\"][\"test_acc\"]  # list of scalars\n    test_ura = ed[\"metrics\"][\"test_ura\"]  # list of scalars\n    epochs = np.arange(1, len(train_acc[0]) + 1)\n\n    # 1. Train vs Val Accuracy\n    try:\n        plt.figure()\n        for bs, ta, va in zip(batch_sizes, train_acc, val_acc):\n            plt.plot(epochs, ta, \"--\", label=f\"Train bs={bs}\")\n            plt.plot(epochs, va, \"-\", label=f\"Val bs={bs}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\n            \"SPR_BENCH: Training vs Validation Accuracy\\nLeft: Train (dashed), Right: Validation (solid)\"\n        )\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_val_accuracy.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # 2. Validation URA\n    try:\n        plt.figure()\n        for bs, ura in zip(batch_sizes, val_ura):\n            plt.plot(epochs, ura, label=f\"bs={bs}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"URA\")\n        plt.title(\"SPR_BENCH: Validation URA across Epochs\\nDifferent batch sizes\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_URA.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating URA plot: {e}\")\n        plt.close()\n\n    # 3. Training Loss\n    try:\n        plt.figure()\n        for bs, tl in zip(batch_sizes, train_loss):\n            plt.plot(epochs, tl, label=f\"bs={bs}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training Loss across Epochs\\nPer batch size\")\n        plt.legend(fontsize=6)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 4. Final Test Accuracy\n    try:\n        plt.figure()\n        plt.bar([str(bs) for bs in batch_sizes], test_acc, color=\"skyblue\")\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Final Test Accuracy\\nPer batch size\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test accuracy bar: {e}\")\n        plt.close()\n\n    # 5. Final Test URA\n    try:\n        plt.figure()\n        plt.bar([str(bs) for bs in batch_sizes], test_ura, color=\"salmon\")\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"URA\")\n        plt.title(\"SPR_BENCH: Final Test URA\\nPer batch size\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_URA_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test URA bar: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    ed = experiment_data[\"hidden_size\"][\"SPR_BENCH\"]\n    hs = np.array(ed[\"hidden_sizes\"])\n    tr_acc = np.array(ed[\"metrics\"][\"train_acc\"])\n    val_acc = np.array(ed[\"metrics\"][\"val_acc\"])\n    val_ura = np.array(ed[\"metrics\"][\"val_ura\"])\n    best_test_acc = ed[\"metrics\"][\"test_acc\"][0] if ed[\"metrics\"][\"test_acc\"] else None\n    best_test_ura = ed[\"metrics\"][\"test_ura\"][0] if ed[\"metrics\"][\"test_ura\"] else None\n    best_hid = hs[np.argmax(val_acc)] if len(hs) else None\n\n    print(f\"Best hidden size: {best_hid}\")\n    print(f\"Test Accuracy   : {best_test_acc}\")\n    print(f\"Test URA        : {best_test_ura}\")\n\n    # ---------- PLOT 1: Training accuracy ----------\n    try:\n        plt.figure()\n        plt.plot(hs, tr_acc, marker=\"o\")\n        plt.title(\"SPR_BENCH \u2013 Training Accuracy vs. Hidden Size\")\n        plt.xlabel(\"Hidden Size\")\n        plt.ylabel(\"Training Accuracy\")\n        plt.grid(True)\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_acc_vs_hidden.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating training accuracy plot: {e}\")\n        plt.close()\n\n    # ---------- PLOT 2: Validation accuracy ----------\n    try:\n        plt.figure()\n        plt.plot(hs, val_acc, marker=\"s\", color=\"orange\")\n        plt.title(\"SPR_BENCH \u2013 Validation Accuracy vs. Hidden Size\")\n        plt.xlabel(\"Hidden Size\")\n        plt.ylabel(\"Validation Accuracy\")\n        plt.grid(True)\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_acc_vs_hidden.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation accuracy plot: {e}\")\n        plt.close()\n\n    # ---------- PLOT 3: Validation URA ----------\n    try:\n        plt.figure()\n        plt.plot(hs, val_ura, marker=\"^\", color=\"green\")\n        plt.title(\"SPR_BENCH \u2013 Validation URA vs. Hidden Size\")\n        plt.xlabel(\"Hidden Size\")\n        plt.ylabel(\"Validation URA\")\n        plt.grid(True)\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_ura_vs_hidden.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation URA plot: {e}\")\n        plt.close()\n\n    # ---------- PLOT 4: Best model test metrics ----------\n    try:\n        if best_test_acc is not None and best_test_ura is not None:\n            plt.figure()\n            plt.bar(\n                [\"Accuracy\", \"URA\"],\n                [best_test_acc, best_test_ura],\n                color=[\"steelblue\", \"salmon\"],\n            )\n            plt.ylim(0, 1)\n            plt.title(f\"SPR_BENCH \u2013 Test Metrics (Best Hidden Size = {best_hid})\")\n            for i, v in enumerate([best_test_acc, best_test_ura]):\n                plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_best_test_metrics.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating best test metrics plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"weight_decay_tuning\", {})\nif not runs:\n    print(\"No runs found in experiment_data.npy\")\n    exit()\n\nwds = sorted(runs.keys(), key=float)\nepochs = np.arange(1, len(next(iter(runs.values()))[\"losses\"][\"train\"]) + 1)\n\n\n# ---------- helper to fetch arrays ----------\ndef col(metric_path):\n    # metric_path like ('losses','train') or ('metrics','val_acc')\n    d1, d2 = metric_path\n    return [runs[wd][d1][d2] for wd in wds]\n\n\ntrain_loss = col((\"losses\", \"train\"))\ntrain_acc = col((\"metrics\", \"train_acc\"))\nval_acc = col((\"metrics\", \"val_acc\"))\nval_ura = col((\"metrics\", \"val_ura\"))\ntest_acc = [runs[wd][\"metrics\"][\"test_acc\"] for wd in wds]\ntest_ura = [runs[wd][\"metrics\"][\"test_ura\"] for wd in wds]\n\n# ---------- plotting ----------\nplots_info = [\n    (\"SPR_BENCH_train_loss.png\", train_loss, \"Training Loss\"),\n    (\"SPR_BENCH_train_val_acc.png\", None, \"Accuracy\"),\n    (\"SPR_BENCH_val_ura.png\", val_ura, \"Validation URA\"),\n]\n\n# 1) Train Loss\ntry:\n    plt.figure()\n    for wd, y in zip(wds, train_loss):\n        plt.plot(epochs, y, label=f\"wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss vs Epochs\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, plots_info[0][0]))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train-loss plot: {e}\")\n    plt.close()\n\n# 2) Train & Val Accuracy\ntry:\n    plt.figure()\n    for wd, y_tr, y_val in zip(wds, train_acc, val_acc):\n        plt.plot(epochs, y_tr, label=f\"train wd={wd}\")\n        plt.plot(epochs, y_val, \"--\", label=f\"val wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Training (solid) & Validation (dashed) Accuracy\")\n    plt.legend(ncol=2, fontsize=8)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, plots_info[1][0]))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# 3) Validation URA\ntry:\n    plt.figure()\n    for wd, y in zip(wds, val_ura):\n        plt.plot(epochs, y, label=f\"wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"URA\")\n    plt.title(\"Validation Unseen-Rule Accuracy (URA)\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, plots_info[2][0]))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating URA plot: {e}\")\n    plt.close()\n\n# 4) Test Accuracy bar chart\ntry:\n    plt.figure()\n    plt.bar(range(len(wds)), test_acc, tick_label=wds)\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"Final Test Accuracy per Weight Decay\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test-accuracy bar: {e}\")\n    plt.close()\n\n# 5) Test URA bar chart\ntry:\n    plt.figure()\n    plt.bar(range(len(wds)), test_ura, tick_label=wds)\n    plt.ylabel(\"Test URA\")\n    plt.title(\"Final Test URA per Weight Decay\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_URA.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test-URA bar: {e}\")\n    plt.close()\n\n# ---------- best runs summary ----------\nbest_val_idx = int(np.argmax([v[-1] for v in val_acc]))\nbest_test_idx = int(np.argmax(test_acc))\nprint(\n    f\"Best val-acc  : wd={wds[best_val_idx]}, val_acc={val_acc[best_val_idx][-1]:.3f}\"\n)\nprint(\n    f\"Best test-acc : wd={wds[best_test_idx]}, test_acc={test_acc[best_test_idx]:.3f}\"\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][\"runs\"]\n    dr_values = sorted(runs.keys(), key=float)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs, dr_values = {}, []\n\n# Figure 1: accuracy curves ---------------------------------------------------\ntry:\n    plt.figure()\n    for dr in dr_values:\n        ep = range(1, len(runs[dr][\"metrics\"][\"train_acc\"]) + 1)\n        plt.plot(ep, runs[dr][\"metrics\"][\"train_acc\"], \"-o\", label=f\"train dr={dr}\")\n        plt.plot(ep, runs[dr][\"metrics\"][\"val_acc\"], \"--o\", label=f\"val dr={dr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Training/Validation Accuracy - SPR_BENCH (Dropout Search)\")\n    plt.legend(fontsize=8)\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Figure 2: loss curves -------------------------------------------------------\ntry:\n    plt.figure()\n    for dr in dr_values:\n        ep = range(1, len(runs[dr][\"losses\"][\"train\"]) + 1)\n        plt.plot(ep, runs[dr][\"losses\"][\"train\"], \"-o\", label=f\"train dr={dr}\")\n        plt.plot(ep, runs[dr][\"losses\"][\"val\"], \"--o\", label=f\"val dr={dr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training/Validation Loss - SPR_BENCH (Dropout Search)\")\n    plt.legend(fontsize=8)\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Figure 3: URA curves --------------------------------------------------------\ntry:\n    plt.figure()\n    for dr in dr_values:\n        ep = range(1, len(runs[dr][\"metrics\"][\"val_ura\"]) + 1)\n        plt.plot(ep, runs[dr][\"metrics\"][\"val_ura\"], \"-o\", label=f\"dr={dr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation URA\")\n    plt.title(\"Validation URA - SPR_BENCH (Dropout Search)\")\n    plt.legend(fontsize=8)\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_URA_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating URA plot: {e}\")\n    plt.close()\n\n# Figure 4: final test accuracy vs dropout ------------------------------------\ntry:\n    plt.figure()\n    test_accs = [runs[dr][\"test_acc\"] for dr in dr_values]\n    plt.plot([float(d) for d in dr_values], test_accs, \"-o\")\n    plt.xlabel(\"Dropout rate\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"Final Test Accuracy vs Dropout - SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_accuracy_vs_dropout.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"num_hidden_layers\", {}).get(\"SPR_BENCH\", {})\ndepths = sorted(int(k.split(\"_\")[-1]) for k in runs.keys())\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n\n\n# helper to fetch per-depth series\ndef series(key_path, depth_key):\n    d = runs[depth_key]\n    for k in key_path[:-1]:\n        d = d[k]\n    return d[key_path[-1]]\n\n\n# ---------- plot 1: training loss ----------\ntry:\n    plt.figure()\n    for i, d in enumerate(depths):\n        rk = f\"layers_{d}\"\n        y = series([\"losses\", \"train\"], rk)\n        plt.plot(range(1, len(y) + 1), y, label=f\"{d} hidden\", color=colors[i])\n    plt.title(\"SPR_BENCH \u2013 Training Loss vs. Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_train_loss_by_layers.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating training loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: validation accuracy ----------\ntry:\n    plt.figure()\n    for i, d in enumerate(depths):\n        rk = f\"layers_{d}\"\n        y = series([\"metrics\", \"val_acc\"], rk)\n        plt.plot(range(1, len(y) + 1), y, label=f\"{d} hidden\", color=colors[i])\n    plt.title(\"SPR_BENCH \u2013 Validation Accuracy vs. Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_accuracy_by_layers.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation accuracy plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: validation URA ----------\ntry:\n    plt.figure()\n    for i, d in enumerate(depths):\n        rk = f\"layers_{d}\"\n        y = series([\"metrics\", \"val_ura\"], rk)\n        plt.plot(range(1, len(y) + 1), y, label=f\"{d} hidden\", color=colors[i])\n    plt.title(\"SPR_BENCH \u2013 Validation URA vs. Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"URA\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_URA_by_layers.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation URA plot: {e}\")\n    plt.close()\n\n# ---------- compute test accuracies ----------\ntest_accs = {}\nfor d in depths:\n    rk = f\"layers_{d}\"\n    preds = np.array(runs[rk][\"predictions\"])\n    gts = np.array(runs[rk][\"ground_truth\"])\n    test_accs[d] = (preds == gts).mean()\n\n# ---------- plot 4: test accuracy bar ----------\ntry:\n    plt.figure()\n    xs = np.arange(len(depths))\n    ys = [test_accs[d] for d in depths]\n    plt.bar(xs, ys, color=[colors[i] for i in range(len(depths))])\n    plt.xticks(xs, [f\"{d}\" for d in depths])\n    plt.title(\"SPR_BENCH \u2013 Final Test Accuracy vs. #Hidden Layers\")\n    plt.xlabel(\"# Hidden Layers\")\n    plt.ylabel(\"Accuracy\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar plot: {e}\")\n    plt.close()\n\n# ---------- print evaluation metrics ----------\nfor d in depths:\n    print(f\"Hidden layers: {d}, Test Accuracy: {test_accs[d]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to fetch metric array safely\ndef get_metric(act, metric):\n    return np.array(\n        experiment_data[\"activation_function\"][\"SPR_BENCH\"][act][\"metrics\"].get(\n            metric, []\n        )\n    )\n\n\nacts = list(experiment_data.get(\"activation_function\", {}).get(\"SPR_BENCH\", {}).keys())\nepochs = (\n    np.arange(1, len(get_metric(acts[0], \"train_acc\")) + 1) if acts else np.array([])\n)\n\n# 1) Accuracy Curves\ntry:\n    plt.figure()\n    for act in acts:\n        plt.plot(\n            epochs, get_metric(act, \"train_acc\"), label=f\"{act}-train\", linestyle=\"-\"\n        )\n        plt.plot(epochs, get_metric(act, \"val_acc\"), label=f\"{act}-val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH Accuracy Curves (Train vs Val)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# 2) URA Curves\ntry:\n    plt.figure()\n    for act in acts:\n        plt.plot(epochs, get_metric(act, \"val_ura\"), label=act)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"URA\")\n    plt.title(\"SPR_BENCH Validation URA Curves\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_ura_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating URA curve plot: {e}\")\n    plt.close()\n\n# 3) Training Loss Curves\ntry:\n    plt.figure()\n    for act in acts:\n        losses = np.array(\n            experiment_data[\"activation_function\"][\"SPR_BENCH\"][act][\"losses\"].get(\n                \"train\", []\n            )\n        )\n        plt.plot(epochs, losses, label=act)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH Training Loss Curves\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_training_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# 4) Test Accuracy Bar Chart\ntry:\n    plt.figure()\n    test_accs = [\n        experiment_data[\"activation_function\"][\"SPR_BENCH\"][act][\"metrics\"].get(\n            \"test_acc\", 0\n        )\n        for act in acts\n    ]\n    plt.bar(acts, test_accs)\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH Test Accuracy by Activation\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bars.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# 5) Test URA Bar Chart\ntry:\n    plt.figure()\n    test_uras = [\n        experiment_data[\"activation_function\"][\"SPR_BENCH\"][act][\"metrics\"].get(\n            \"test_ura\", 0\n        )\n        for act in acts\n    ]\n    plt.bar(acts, test_uras, color=\"orange\")\n    plt.ylabel(\"URA\")\n    plt.title(\"SPR_BENCH Test URA by Activation\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_ura_bars.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test URA bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nsaved_files = []\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ----- per-run accuracy / URA curves -----\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    try:\n        epochs = list(range(1, len(run_data[\"metrics\"][\"train_acc\"]) + 1))\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, run_data[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_ura\"], label=\"Val URA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric\")\n        plt.title(f\"SPR_BENCH \u2013 {run_key} Accuracy/URA Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_{run_key}_acc_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {run_key}: {e}\")\n        plt.close()\n\n# ----- final test accuracy / URA bar plot -----\ntry:\n    run_names = []\n    test_accs = []\n    test_uras = []\n    for rk, rd in experiment_data.get(\"EPOCHS\", {}).items():\n        run_names.append(rk)\n        test_accs.append(rd.get(\"test_acc\", 0))\n        test_uras.append(rd.get(\"test_ura\", 0))\n    x = np.arange(len(run_names))\n    width = 0.35\n    plt.figure(figsize=(7, 4))\n    plt.bar(x - width / 2, test_accs, width, label=\"Test Acc\")\n    plt.bar(x + width / 2, test_uras, width, label=\"Test URA\")\n    plt.xticks(x, run_names, rotation=45)\n    plt.ylim(0, 1)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH \u2013 Test Accuracy vs URA by Epoch Setting\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_comparison.png\")\n    plt.tight_layout()\n    plt.savefig(fname)\n    saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison plot: {e}\")\n    plt.close()\n\nprint(\"Saved plots:\", saved_files)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nsaved_files = []\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ----- per-run accuracy / URA curves -----\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    try:\n        epochs = list(range(1, len(run_data[\"metrics\"][\"train_acc\"]) + 1))\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, run_data[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_ura\"], label=\"Val URA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric\")\n        plt.title(f\"SPR_BENCH \u2013 {run_key} Accuracy/URA Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_{run_key}_acc_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {run_key}: {e}\")\n        plt.close()\n\n# ----- final test accuracy / URA bar plot -----\ntry:\n    run_names = []\n    test_accs = []\n    test_uras = []\n    for rk, rd in experiment_data.get(\"EPOCHS\", {}).items():\n        run_names.append(rk)\n        test_accs.append(rd.get(\"test_acc\", 0))\n        test_uras.append(rd.get(\"test_ura\", 0))\n    x = np.arange(len(run_names))\n    width = 0.35\n    plt.figure(figsize=(7, 4))\n    plt.bar(x - width / 2, test_accs, width, label=\"Test Acc\")\n    plt.bar(x + width / 2, test_uras, width, label=\"Test URA\")\n    plt.xticks(x, run_names, rotation=45)\n    plt.ylim(0, 1)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH \u2013 Test Accuracy vs URA by Epoch Setting\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_comparison.png\")\n    plt.tight_layout()\n    plt.savefig(fname)\n    saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison plot: {e}\")\n    plt.close()\n\nprint(\"Saved plots:\", saved_files)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nsaved_files = []\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ----- per-run accuracy / URA curves -----\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    try:\n        epochs = list(range(1, len(run_data[\"metrics\"][\"train_acc\"]) + 1))\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, run_data[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.plot(epochs, run_data[\"metrics\"][\"val_ura\"], label=\"Val URA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric\")\n        plt.title(f\"SPR_BENCH \u2013 {run_key} Accuracy/URA Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_{run_key}_acc_curves.png\")\n        plt.savefig(fname)\n        saved_files.append(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {run_key}: {e}\")\n        plt.close()\n\n# ----- final test accuracy / URA bar plot -----\ntry:\n    run_names = []\n    test_accs = []\n    test_uras = []\n    for rk, rd in experiment_data.get(\"EPOCHS\", {}).items():\n        run_names.append(rk)\n        test_accs.append(rd.get(\"test_acc\", 0))\n        test_uras.append(rd.get(\"test_ura\", 0))\n    x = np.arange(len(run_names))\n    width = 0.35\n    plt.figure(figsize=(7, 4))\n    plt.bar(x - width / 2, test_accs, width, label=\"Test Acc\")\n    plt.bar(x + width / 2, test_uras, width, label=\"Test URA\")\n    plt.xticks(x, run_names, rotation=45)\n    plt.ylim(0, 1)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH \u2013 Test Accuracy vs URA by Epoch Setting\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_comparison.png\")\n    plt.tight_layout()\n    plt.savefig(fname)\n    saved_files.append(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison plot: {e}\")\n    plt.close()\n\nprint(\"Saved plots:\", saved_files)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- basic setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nsaved_files = []\n\n# ---------- load every experiment_data.npy that actually exists ----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9d73a6a3847428f28acf9015fb60b_proc_2753348/experiment_data.npy\",\n    \"experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_5a37ba340bc448499729f60826d49df2_proc_2753346/experiment_data.npy\",\n    \"experiments/2025-08-14_21-45-52_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b952af9d95ff4037a35a7e120c8bdaf7_proc_2753349/experiment_data.npy\",\n]\n\nall_runs = []  # list of dicts with keys: metrics, test_acc, test_ura\ntry:\n    for rel_path in experiment_data_path_list:\n        abs_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), rel_path)\n        if os.path.isfile(abs_path):\n            exp_data = np.load(abs_path, allow_pickle=True).item()\n            for run_key, run_data in exp_data.get(\"EPOCHS\", {}).items():\n                all_runs.append(run_data)\n        else:\n            print(f\"Warning: file not found \u2013 {abs_path}\")\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# ---------- aggregate per-epoch curves ----------\ntry:\n    metrics_to_plot = [\"train_acc\", \"val_acc\", \"val_ura\"]\n    # collect arrays per metric\n    metric_dict = {m: [] for m in metrics_to_plot}\n    for run in all_runs:\n        for m in metrics_to_plot:\n            arr = run.get(\"metrics\", {}).get(m, None)\n            if arr is not None and len(arr) > 0:\n                metric_dict[m].append(np.asarray(arr, dtype=float))\n    # ensure we have at least one run for every metric\n    min_len = None\n    for m, lst in metric_dict.items():\n        if len(lst) == 0:\n            print(f\"No data for metric {m}; it will be skipped.\")\n            metric_dict[m] = []\n        else:\n            lmin = min(len(a) for a in lst)\n            min_len = lmin if min_len is None else min(min_len, lmin)\n\n    if min_len and min_len > 0:\n        epochs = np.arange(1, min_len + 1)\n        plt.figure(figsize=(7, 4))\n        for m, lst in metric_dict.items():\n            if not lst:\n                continue\n            stacked = np.stack([a[:min_len] for a in lst], axis=0)\n            mean = stacked.mean(axis=0)\n            stderr = stacked.std(axis=0, ddof=1) / np.sqrt(stacked.shape[0])\n            plt.plot(epochs, mean, label=f\"{m.replace('_', ' ').title()} (mean)\")\n            plt.fill_between(\n                epochs,\n                mean - stderr,\n                mean + stderr,\n                alpha=0.2,\n                label=f\"{m.replace('_', ' ').title()} (\u00b1SEM)\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH \u2013 Aggregate Epoch Curves\\nMean \u00b1 Standard Error\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_aggregate_epoch_curves.png\")\n        plt.tight_layout()\n        plt.savefig(fname)\n        saved_files.append(fname)\n        plt.close()\n    else:\n        print(\"No per-epoch metrics found to aggregate.\")\nexcept Exception as e:\n    print(f\"Error creating aggregate epoch plot: {e}\")\n    plt.close()\n\n# ---------- aggregate final test metrics ----------\ntry:\n    test_accs = [r.get(\"test_acc\") for r in all_runs if r.get(\"test_acc\") is not None]\n    test_uras = [r.get(\"test_ura\") for r in all_runs if r.get(\"test_ura\") is not None]\n\n    if test_accs and test_uras:\n        means = [np.mean(test_accs), np.mean(test_uras)]\n        sems = [\n            np.std(test_accs, ddof=1) / np.sqrt(len(test_accs)),\n            np.std(test_uras, ddof=1) / np.sqrt(len(test_uras)),\n        ]\n        x = np.arange(2)\n        plt.figure(figsize=(6, 4))\n        plt.bar(x, means, yerr=sems, capsize=5, tick_label=[\"Test Acc\", \"Test URA\"])\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH \u2013 Aggregate Test Metrics\\nMean \u00b1 Standard Error\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_aggregate_test_metrics.png\")\n        plt.tight_layout()\n        plt.savefig(fname)\n        saved_files.append(fname)\n        plt.close()\n    else:\n        print(\"No final test metrics found to aggregate.\")\nexcept Exception as e:\n    print(f\"Error creating aggregate test plot: {e}\")\n    plt.close()\n\nprint(\"Saved plots:\", saved_files)\n"], "plot_plan": ["We first load experiment_data.npy from the working directory and fetch every run stored under the \u201cEPOCHS\u201d key. For each run we construct one figure that overlays the epoch-wise train accuracy, validation accuracy and validation URA, giving an immediate view of convergence speed and generalisation. These four figures (one per run) satisfy the \u22645-figure limit. Afterwards we build a final bar plot that compares the test accuracy and test URA obtained after each run, letting us see how the number of training epochs affects final performance. All plotting is wrapped in individual try-except blocks, figures are titled with the dataset name, saved with descriptive filenames, and closed to avoid memory leaks. Only metrics available in the experiment_data structure are accessed\u2014no artificial data are generated. Paths are handled via the supplied working_dir variable, which is recreated if missing. Running the script will silently skip a plot if any key is absent while still saving the other figures. Finally, the script prints the list of saved filenames so the user knows where to look.", "We load the saved experiment_data, extract the SPR_BENCH learning-rate sweep results and generate four standard visualisations: (1) training vs validation accuracy curves, (2) training loss curves, (3) validation URA curves, and (4) a bar chart of test accuracy per learning-rate with URA annotations. Each figure is produced inside its own try-except block, given a clear title, saved to working_dir with descriptive names, and then closed. Only information present in experiment_data.npy is used, and we plot at most the eight recorded epochs, which is well under the five-figure limit. Finally, the script prints a concise table of learning-rates alongside their test accuracy and URA so that key evaluation metrics are visible in the console. The code respects all directory, naming, and matplotlib usage guidelines while remaining compact and self-contained.", "We first load the saved experiment_data.npy and pull out the \u201cSPR_BENCH\u201d entry produced during the batch-size sweep.  From this we extract per-epoch train accuracy, validation accuracy, validation URA, and train loss (each is a list of length = #batch_sizes with nested per-epoch arrays) plus the single-value final test accuracy/URA vectors.  Using basic matplotlib, we make at most five figures: (1) train vs. validation accuracy across epochs with one coloured line per batch size, (2) validation URA across epochs, (3) train loss across epochs, (4) final test accuracy as a bar chart, and (5) final test URA as a bar chart.  Each figure is wrapped in its own try-except so errors don\u2019t halt the script, and we always close figures afterward.  We include clear titles and subtitles describing the dataset and what each subplot shows, and save each figure into the working directory with descriptive names that include the dataset (\u201cSPR_BENCH\u201d) and plot type.  If there are more than five batch sizes we still plot all of them because that keeps the total figures \u22645; no epoch sampling is required since only 5 epochs were logged.  The code follows the required imports and path handling conventions and prints nothing except possible error messages.  All plots rely exclusively on the data already stored in experiment_data.npy.", "We first load the stored experiment_data.npy, recover the hidden\u2010size sweep information for the SPR_BENCH dataset, and identify the best hidden size together with its test accuracy and URA.  Four standard visualizations are then generated: (1) training accuracy vs. hidden size, (2) validation accuracy vs. hidden size, (3) validation URA vs. hidden size, and (4) a bar plot comparing the best model\u2019s test accuracy and URA.  Each figure is wrapped in its own try-except block, given an informative title/subtitle, saved inside the working directory with an explanatory name, and closed afterward.  Only data present in the experiment file are plotted; no synthetic values are fabricated.  At most four plots are produced (well below the five-plot limit).  Finally, the code prints the best hidden size together with its corresponding test metrics so the user can verify the result directly from the console output.", "We load the saved experiment_data.npy and iterate over the six weight-decay runs. For every run we extract per-epoch arrays for training loss, training accuracy, validation accuracy and URA, together with the final test accuracy/URA. We then generate at most five figures: (1) training loss curves, (2) training + validation accuracy curves (solid vs dashed lines), (3) validation URA curves, (4) bar chart of final test accuracy per weight-decay, and (5) bar chart of final test URA per weight-decay. Each figure is wrapped in its own try-except block, saved to working_dir with descriptive names, and the figure is always closed afterward. After plotting, we compute and print the weight-decay that achieved the best validation accuracy and the best test accuracy, along with the corresponding metric values. Only data present in experiment_data.npy are accessed; no synthetic values are created. The code strictly follows the required import order and directory handling. Finally, concise summaries of the evaluation metrics are printed to stdout.", "We first load experiment_data.npy, pull the \u201cruns\u201d dict for the SPR_BENCH dropout study, and sort the runs by dropout value.  \nFor each run we collect the per-epoch arrays: train_acc, val_acc, val_ura, train_loss, val_loss, and the final test_acc scalar.  \nWe then create four figures (\u22645 as required):  \n1) training/validation accuracy vs epoch (all dropout rates in one plot, two line styles per rate),  \n2) training/validation loss vs epoch,  \n3) validation URA vs epoch,  \n4) final test accuracy vs dropout (single bar/line plot).  \nEach figure is wrapped in its own try-except so failures in one plot don\u2019t stop the rest, the figure is closed in finally/except, and the file name includes dataset and metric.  \nTitles clarify content and the dataset; legends differentiate dropout rates.  \nAll plots are saved to the predefined working_dir path.  \nNothing is plotted if the needed data key is missing.", "We will load experiment_data.npy, iterate over the three runs (layers_1/2/3), and aggregate per-epoch curves.  Four figures will be produced: (1) training loss vs. epoch for each depth, (2) validation accuracy vs. epoch, (3) validation URA vs. epoch, and (4) a bar chart with final test accuracy per depth computed from stored predictions and ground-truth.  Each figure is wrapped in its own try-except, saved with descriptive names to working_dir, and closed afterwards.  We respect the five-figure limit and use only data available inside the .npy file.  After plotting we compute and print the test accuracies so the script outputs an evaluation metric.", "We will load experiment_data.npy, extract the SPR_BENCH results and reorganize them into numpy arrays for epochs and metrics. For concise visualization we will create up to five figures: (1) training vs validation accuracy curves per activation, (2) validation URA curves per activation, (3) training loss curves per activation, (4) a bar chart of final test accuracy, and (5) a bar chart of final test URA. Each figure is wrapped in its own try-except block, titled clearly, saved with descriptive names that include the dataset and plot type, and closed afterwards. We plot only when the underlying values exist, iterate through activation functions dynamically, and respect the epoch interval implicit in the stored lists (<=5 epochs, so no down-sampling needed). The script begins with the required imports, constructs/ensures the working directory, then prints the file locations of the saved plots so the user can verify successful execution. All plotting uses basic matplotlib without custom styles.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["EPOCHS (num_training_epochs)", "learning_rate", "batch_size", "hidden_size", "weight_decay", "dropout_rate", "num_hidden_layers", "activation_function", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will read the saved experiment_data.npy from the working directory,\nwalk through every run (e.g., \u201cepochs_5\u201d, \u201cepochs_10\u201d, \u2026), and for each run\nprint the final (last-epoch) training metrics, final validation metrics, and\nstored test metrics. For losses the last value is reported (lower is better);\nfor accuracies and URA the last value is also used (these were recorded after\nevery epoch). Each dataset section (\u201cTrain\u201d, \u201cValidation\u201d, \u201cTest\u201d) is announced\nbefore its metrics, and every metric is named explicitly.", "The script will load the saved NumPy dictionary, walk through the nested\nstructure (sweep \u2192 dataset), and collect the final or best values of every\nstored metric.   For per-epoch series (e.g. train accuracy) the last epoch is\nreported as the \u201cfinal\u201d value, while for validation metrics the maximum across\nepochs is shown as the \u201cbest\u201d value.   Results are printed in a clear, human-\nreadable format that always names the dataset first, then each metric with an\nexplicit label; learning-rate variants are indicated in parentheses.   All logic\nis placed at top level or in helper functions that are invoked immediately,\nensuring the file runs on execution without relying on a `__main__` guard.", "Below is a short script that immediately loads the saved NumPy file, traverses\nthe nested structure created during training, and prints the best (max / min)\nvalues for every recorded metric. It follows the required printing conventions\nand runs straight away when executed.", "The script will locate the NumPy file inside the \u201cworking\u201d directory, load the\nsaved `experiment_data` dictionary, iterate over each dataset contained in the\nhyper-parameter sweep, and extract the final (i.e.\\ the last stored) value for\nevery metric that was recorded. It then prints the dataset\u2019s name first,\nfollowed by clearly labelled lines for training accuracy, validation accuracy,\ntest accuracy, validation URA, test URA, training loss, and validation loss. All\ncode is placed at the top level so the file runs immediately when executed.", "The script will locate the working directory, load the saved NumPy file, and\niterate through the results stored for each weight-decay setting. For every such\n\u201cdataset,\u201d it will grab the last recorded (i.e., final) value of each available\nmetric list as well as the explicitly stored test metrics, then print them with\nclear, descriptive labels. Any metric list that is missing or empty will be\nskipped gracefully to avoid errors.", "The script below loads the serialized experiment dictionary, loops over every\ndataset that was evaluated (e.g., \u201cSPR_BENCH\u201d), and then over every dropout-rate\nrun.   For each run it prints: final train accuracy, best validation accuracy,\nbest validation loss, final test accuracy, and final test URA (unseen-rule\naccuracy).   Metric names are written in full, as required.   The code executes\nimmediately, needs no special entry point, and creates no plots.", "The script will load experiment_data.npy from the \u201cworking\u201d directory, traverse\nits nested structure, and pull out the lists of recorded values for each run\n(e.g., layers_1, layers_2, \u2026).   For every dataset present (here that is only\n\u201cSPR_BENCH\u201d), it will compute the \u201cbest\u201d value of each metric: maximum for\naccuracies/URA and minimum for losses.   Results are printed with the dataset\nname first, followed by clearly-labelled metrics for every run depth.   The code\nis placed at the top level so it executes immediately when the file is run.", "The script loads the saved numpy dictionary, locates the metrics for the\n\u201cSPR_BENCH\u201d dataset under each activation-function configuration, then prints\nthe final (last-epoch) values for every tracked metric. It labels every group\nwith its dataset name, clearly names each metric (e.g., \u201cfinal validation\naccuracy\u201d), and skips metrics whose lists are empty. The code is kept entirely\nat the global scope so that it runs immediately when executed.", "The script will read the saved experiment_data.npy from the working directory,\nwalk through every run (e.g., \u201cepochs_5\u201d, \u201cepochs_10\u201d, \u2026), and for each run\nprint the final (last-epoch) training metrics, final validation metrics, and\nstored test metrics. For losses the last value is reported (lower is better);\nfor accuracies and URA the last value is also used (these were recorded after\nevery epoch). Each dataset section (\u201cTrain\u201d, \u201cValidation\u201d, \u201cTest\u201d) is announced\nbefore its metrics, and every metric is named explicitly.", "The script will read the saved experiment_data.npy from the working directory,\nwalk through every run (e.g., \u201cepochs_5\u201d, \u201cepochs_10\u201d, \u2026), and for each run\nprint the final (last-epoch) training metrics, final validation metrics, and\nstored test metrics. For losses the last value is reported (lower is better);\nfor accuracies and URA the last value is also used (these were recorded after\nevery epoch). Each dataset section (\u201cTrain\u201d, \u201cValidation\u201d, \u201cTest\u201d) is announced\nbefore its metrics, and every metric is named explicitly.", "The script will read the saved experiment_data.npy from the working directory,\nwalk through every run (e.g., \u201cepochs_5\u201d, \u201cepochs_10\u201d, \u2026), and for each run\nprint the final (last-epoch) training metrics, final validation metrics, and\nstored test metrics. For losses the last value is reported (lower is better);\nfor accuracies and URA the last value is also used (these were recorded after\nevery epoch). Each dataset section (\u201cTrain\u201d, \u201cValidation\u201d, \u201cTest\u201d) is announced\nbefore its metrics, and every metric is named explicitly.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- utility ----------\ndef last(lst):\n    \"\"\"Return the last element of a list or None if the list is empty.\"\"\"\n    return lst[-1] if lst else None\n\n\n# ---------- iterate over runs ----------\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"\\n=== Results for run: {run_key} ===\")\n\n    # ---------- TRAIN ----------\n    print(\"Dataset: Train\")\n    train_acc = last(run_data[\"metrics\"][\"train_acc\"])\n    if train_acc is not None:\n        print(f\"train accuracy: {train_acc:.4f}\")\n    train_loss = last(run_data[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"train loss: {train_loss:.4f}\")\n\n    # ---------- VALIDATION ----------\n    print(\"Dataset: Validation\")\n    val_acc = last(run_data[\"metrics\"][\"val_acc\"])\n    if val_acc is not None:\n        print(f\"validation accuracy: {val_acc:.4f}\")\n    val_ura = last(run_data[\"metrics\"][\"val_ura\"])\n    if val_ura is not None:\n        print(f\"validation URA: {val_ura:.4f}\")\n    val_loss = last(run_data[\"losses\"].get(\"val\", []))\n    if val_loss is not None:\n        print(f\"validation loss: {val_loss:.4f}\")\n\n    # ---------- TEST ----------\n    print(\"Dataset: Test\")\n    test_acc = run_data.get(\"test_acc\")\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n    test_ura = run_data.get(\"test_ura\")\n    if test_ura is not None:\n        print(f\"test URA: {test_ura:.4f}\")\n", "import os\nimport numpy as np\nimport math\n\n\ndef load_experiment_dict():\n    work_dir = os.path.join(os.getcwd(), \"working\")\n    file_path = os.path.join(work_dir, \"experiment_data.npy\")\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"Could not find experiment_data.npy in {work_dir}\")\n    return np.load(file_path, allow_pickle=True).item()\n\n\ndef best_or_final(values, mode=\"final\"):\n    \"\"\"Helper to obtain either the last entry or the best (max) entry.\"\"\"\n    if not values:\n        return float(\"nan\")\n    if mode == \"final\":\n        return values[-1]\n    if mode == \"best\":\n        return max(values)\n    raise ValueError(\"mode must be 'final' or 'best'.\")\n\n\ndef print_metrics(exp_dict):\n    for sweep_name, sweep_data in exp_dict.items():  # e.g. \"learning_rate\"\n        for dataset_name, ds_data in sweep_data.items():  # e.g. \"SPR_BENCH\"\n            lr_vals = ds_data.get(\"lr_values\", [])\n            n_variants = len(lr_vals)\n            for idx in range(n_variants):\n                lr = lr_vals[idx]\n                header = f\"{dataset_name} (learning rate = {lr})\"\n                print(header)\n\n                # Training-phase metrics\n                train_acc_final = best_or_final(\n                    ds_data[\"metrics\"][\"train_acc\"][idx], mode=\"final\"\n                )\n                train_loss_final = best_or_final(\n                    ds_data[\"losses\"][\"train\"][idx], mode=\"final\"\n                )\n\n                # Validation-phase metrics\n                val_acc_best = best_or_final(\n                    ds_data[\"metrics\"][\"val_acc\"][idx], mode=\"best\"\n                )\n                val_ura_best = best_or_final(\n                    ds_data[\"metrics\"][\"val_ura\"][idx], mode=\"best\"\n                )\n                # Validation loss may be NaN placeholders; handle gracefully\n                val_loss_series = ds_data[\"losses\"][\"val\"][idx]\n                val_loss_best = (\n                    min(v for v in val_loss_series if not math.isnan(v))\n                    if any(not math.isnan(v) for v in val_loss_series)\n                    else float(\"nan\")\n                )\n\n                # Test-phase metrics\n                test_acc = ds_data[\"metrics\"][\"test_acc\"][idx]\n                test_ura = ds_data[\"metrics\"][\"test_ura\"][idx]\n\n                # Output with explicit metric names\n                print(f\"  train accuracy (final epoch): {train_acc_final:.4f}\")\n                print(f\"  train loss (final epoch): {train_loss_final:.4f}\")\n                print(f\"  validation accuracy (best): {val_acc_best:.4f}\")\n                print(f\"  validation URA (best): {val_ura_best:.4f}\")\n                print(f\"  validation loss (best): {val_loss_best:.4f}\")\n                print(f\"  test accuracy (final): {test_acc:.4f}\")\n                print(f\"  test URA (final): {test_ura:.4f}\\n\")\n\n\n# Execute immediately\nexperiment_data = load_experiment_dict()\nprint_metrics(experiment_data)\n", "import os\nimport numpy as np\n\n# --------- load experiment results ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\ndef flatten(list_of_lists):\n    return [item for sub in list_of_lists for item in sub]\n\n\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, float) else str(x)\n\n\n# --------- parse & display metrics ----------\nfor dataset_name, ds_dict in experiment_data.get(\"batch_size_tuning\", {}).items():\n    print(dataset_name)  # dataset heading\n\n    metrics = ds_dict[\"metrics\"]\n    losses = ds_dict[\"losses\"]\n\n    # accuracy / URA\n    best_train_acc = (\n        max(flatten(metrics[\"train_acc\"])) if metrics[\"train_acc\"] else None\n    )\n    best_val_acc = max(flatten(metrics[\"val_acc\"])) if metrics[\"val_acc\"] else None\n    best_val_ura = max(flatten(metrics[\"val_ura\"])) if metrics[\"val_ura\"] else None\n    best_test_acc = max(metrics[\"test_acc\"]) if metrics[\"test_acc\"] else None\n    best_test_ura = max(metrics[\"test_ura\"]) if metrics[\"test_ura\"] else None\n\n    # losses (lower is better)\n    lowest_train_loss = min(flatten(losses[\"train\"])) if losses[\"train\"] else None\n    lowest_val_loss = min(flatten(losses[\"val\"])) if losses[\"val\"] else None\n\n    # print results with explicit names\n    print(f\"best train accuracy: {fmt(best_train_acc)}\")\n    print(f\"best validation accuracy: {fmt(best_val_acc)}\")\n    print(f\"best validation URA: {fmt(best_val_ura)}\")\n    print(f\"best test accuracy: {fmt(best_test_acc)}\")\n    print(f\"best test URA: {fmt(best_test_ura)}\")\n    print(f\"lowest training loss: {fmt(lowest_train_loss)}\")\n    print(f\"lowest validation loss: {fmt(lowest_val_loss)}\")\n", "import os\nimport numpy as np\n\n# ----------------- locate and load the saved results -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------------- iterate through datasets and print metrics --------\nfor sweep_name, datasets in experiment_data.items():  # e.g. 'hidden_size'\n    for dataset_name, data in datasets.items():  # e.g. 'SPR_BENCH'\n        print(dataset_name)  # Dataset header\n\n        metrics = data.get(\"metrics\", {})\n        losses = data.get(\"losses\", {})\n\n        # Helper to safely fetch the \"final\" value (last element in list)\n        def last(lst, default=\"N/A\"):\n            return lst[-1] if isinstance(lst, list) and lst else default\n\n        # Accuracy metrics\n        print(\"training accuracy:\", last(metrics.get(\"train_acc\", [])))\n        print(\"validation accuracy:\", last(metrics.get(\"val_acc\", [])))\n        print(\"test accuracy:\", last(metrics.get(\"test_acc\", [])))\n\n        # URA metrics\n        print(\"validation URA:\", last(metrics.get(\"val_ura\", [])))\n        print(\"test URA:\", last(metrics.get(\"test_ura\", [])))\n\n        # Loss metrics\n        print(\"training loss:\", last(losses.get(\"train\", [])))\n        print(\"validation loss:\", last(losses.get(\"val\", [])))\n", "import os\nimport numpy as np\n\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 1. Navigate to the weight-decay sweep results\nruns = experiment_data.get(\"weight_decay_tuning\", {})\n\n\ndef safe_last(seq):\n    \"\"\"Return the last element of a non-empty list, or None.\"\"\"\n    return seq[-1] if isinstance(seq, (list, tuple)) and len(seq) else None\n\n\n# 2. Iterate over each run and print final metrics\nfor wd_key, run_rec in runs.items():\n    print(f\"\\nDataset (weight_decay = {wd_key})\")\n\n    # Fetch top-level metric containers\n    metrics = run_rec.get(\"metrics\", {})\n    losses = run_rec.get(\"losses\", {})\n\n    # Training metrics\n    final_train_acc = safe_last(metrics.get(\"train_acc\", []))\n    final_train_loss = safe_last(losses.get(\"train\", []))\n\n    if final_train_acc is not None:\n        print(f\"final train accuracy: {final_train_acc:.4f}\")\n    if final_train_loss is not None:\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # Validation metrics\n    final_val_acc = safe_last(metrics.get(\"val_acc\", []))\n    final_val_ura = safe_last(metrics.get(\"val_ura\", []))\n\n    if final_val_acc is not None:\n        print(f\"final validation accuracy: {final_val_acc:.4f}\")\n    if final_val_ura is not None:\n        print(f\"final validation unseen-rule accuracy: {final_val_ura:.4f}\")\n\n    # Test metrics (stored as scalars)\n    test_acc = metrics.get(\"test_acc\")\n    test_ura = metrics.get(\"test_ura\")\n\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n    if test_ura is not None:\n        print(f\"test unseen-rule accuracy: {test_ura:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 1. Load the saved experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# 2. Helper to extract \u201cbest\u201d / \u201cfinal\u201d values from a single run\n# ------------------------------------------------------------------\ndef summarise_run(run_dict):\n    \"\"\"Return a dictionary with the final / best values we care about.\"\"\"\n    train_acc_final = run_dict[\"metrics\"][\"train_acc\"][-1]\n    # best validation accuracy (highest)\n    val_acc_best = max(run_dict[\"metrics\"][\"val_acc\"])\n    # best (i.e. lowest) validation loss\n    val_loss_best = min(run_dict[\"losses\"][\"val\"])\n    test_acc_final = run_dict.get(\"test_acc\", None)\n    test_ura_final = run_dict.get(\"test_ura\", None)\n\n    return {\n        \"train accuracy\": train_acc_final,\n        \"best validation accuracy\": val_acc_best,\n        \"best validation loss\": val_loss_best,\n        \"test accuracy\": test_acc_final,\n        \"test URA\": test_ura_final,\n    }\n\n\n# ------------------------------------------------------------------\n# 3. Iterate through all datasets / runs and print metrics\n# ------------------------------------------------------------------\nfor hyperparam_group, datasets in experiment_data.items():\n    # datasets is something like {\"SPR_BENCH\": {...}}\n    for dataset_name, dataset_content in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        runs = dataset_content[\"runs\"]  # keys are dropout-rate strings\n        for run_name, run_data in runs.items():\n            print(f\"  Hyper-parameter setting (dropout rate): {run_name}\")\n            summary = summarise_run(run_data)\n            for metric_name, value in summary.items():\n                # Skip metrics that somehow ended up as None\n                if value is not None:\n                    print(f\"    {metric_name}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to summarise ----------\ndef summarise_run(run_dict):\n    \"\"\"Return a dict with the best (or final) summary values wanted.\"\"\"\n    best_train_acc = (\n        max(run_dict[\"metrics\"][\"train_acc\"])\n        if run_dict[\"metrics\"][\"train_acc\"]\n        else None\n    )\n    best_val_acc = (\n        max(run_dict[\"metrics\"][\"val_acc\"]) if run_dict[\"metrics\"][\"val_acc\"] else None\n    )\n    best_val_ura = (\n        max(run_dict[\"metrics\"][\"val_ura\"]) if run_dict[\"metrics\"][\"val_ura\"] else None\n    )\n    lowest_train_loss = (\n        min(run_dict[\"losses\"][\"train\"]) if run_dict[\"losses\"][\"train\"] else None\n    )\n    return {\n        \"best train accuracy\": best_train_acc,\n        \"best validation accuracy\": best_val_acc,\n        \"best validation URA\": best_val_ura,\n        \"lowest training loss\": lowest_train_loss,\n    }\n\n\n# ---------- iterate and print ----------\n# Top-level hyperparameter group (only 'num_hidden_layers' in this example)\nfor hp_group, datasets in experiment_data.items():\n    for dataset_name, runs in datasets.items():\n        # print dataset title once\n        print(f\"\\nDataset: {dataset_name}\")\n        for run_name, run_data in runs.items():\n            summary = summarise_run(run_data)\n            print(f\"  Run: {run_name}\")\n            for metric_name, value in summary.items():\n                if value is not None:\n                    print(f\"    {metric_name}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------- paths & loading ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------- helper to fetch last value --------\ndef last(lst):\n    \"\"\"Return last element of a list or None if the list is empty.\"\"\"\n    return lst[-1] if isinstance(lst, (list, tuple)) and len(lst) else None\n\n\n# -------------- print metrics ---------------------\nfor (\n    exp_family,\n    datasets,\n) in experiment_data.items():  # exp_family == \"activation_function\"\n    for dataset_name, configs in datasets.items():  # dataset_name == \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n        for config_name, edict in configs.items():  # config_name == activation function\n            print(f\"  Configuration: {config_name}\")\n\n            # Retrieve metrics\n            train_loss = last(edict.get(\"losses\", {}).get(\"train\", []))\n            val_loss = last(edict.get(\"losses\", {}).get(\"val\", []))\n            train_acc = last(edict.get(\"metrics\", {}).get(\"train_acc\", []))\n            val_acc = last(edict.get(\"metrics\", {}).get(\"val_acc\", []))\n            val_ura = last(edict.get(\"metrics\", {}).get(\"val_ura\", []))\n            test_acc = edict.get(\"metrics\", {}).get(\"test_acc\")\n            test_ura = edict.get(\"metrics\", {}).get(\"test_ura\")\n\n            # Print only the metrics that exist\n            if train_loss is not None:\n                print(f\"    final train loss: {train_loss:.4f}\")\n            if val_loss is not None:\n                print(f\"    final validation loss: {val_loss:.4f}\")\n            if train_acc is not None:\n                print(f\"    final train accuracy: {train_acc:.4f}\")\n            if val_acc is not None:\n                print(f\"    final validation accuracy: {val_acc:.4f}\")\n            if val_ura is not None:\n                print(f\"    final validation unseen-rule accuracy: {val_ura:.4f}\")\n            if test_acc is not None:\n                print(f\"    test accuracy: {test_acc:.4f}\")\n            if test_ura is not None:\n                print(f\"    test unseen-rule accuracy: {test_ura:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- utility ----------\ndef last(lst):\n    \"\"\"Return the last element of a list or None if the list is empty.\"\"\"\n    return lst[-1] if lst else None\n\n\n# ---------- iterate over runs ----------\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"\\n=== Results for run: {run_key} ===\")\n\n    # ---------- TRAIN ----------\n    print(\"Dataset: Train\")\n    train_acc = last(run_data[\"metrics\"][\"train_acc\"])\n    if train_acc is not None:\n        print(f\"train accuracy: {train_acc:.4f}\")\n    train_loss = last(run_data[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"train loss: {train_loss:.4f}\")\n\n    # ---------- VALIDATION ----------\n    print(\"Dataset: Validation\")\n    val_acc = last(run_data[\"metrics\"][\"val_acc\"])\n    if val_acc is not None:\n        print(f\"validation accuracy: {val_acc:.4f}\")\n    val_ura = last(run_data[\"metrics\"][\"val_ura\"])\n    if val_ura is not None:\n        print(f\"validation URA: {val_ura:.4f}\")\n    val_loss = last(run_data[\"losses\"].get(\"val\", []))\n    if val_loss is not None:\n        print(f\"validation loss: {val_loss:.4f}\")\n\n    # ---------- TEST ----------\n    print(\"Dataset: Test\")\n    test_acc = run_data.get(\"test_acc\")\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n    test_ura = run_data.get(\"test_ura\")\n    if test_ura is not None:\n        print(f\"test URA: {test_ura:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- utility ----------\ndef last(lst):\n    \"\"\"Return the last element of a list or None if the list is empty.\"\"\"\n    return lst[-1] if lst else None\n\n\n# ---------- iterate over runs ----------\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"\\n=== Results for run: {run_key} ===\")\n\n    # ---------- TRAIN ----------\n    print(\"Dataset: Train\")\n    train_acc = last(run_data[\"metrics\"][\"train_acc\"])\n    if train_acc is not None:\n        print(f\"train accuracy: {train_acc:.4f}\")\n    train_loss = last(run_data[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"train loss: {train_loss:.4f}\")\n\n    # ---------- VALIDATION ----------\n    print(\"Dataset: Validation\")\n    val_acc = last(run_data[\"metrics\"][\"val_acc\"])\n    if val_acc is not None:\n        print(f\"validation accuracy: {val_acc:.4f}\")\n    val_ura = last(run_data[\"metrics\"][\"val_ura\"])\n    if val_ura is not None:\n        print(f\"validation URA: {val_ura:.4f}\")\n    val_loss = last(run_data[\"losses\"].get(\"val\", []))\n    if val_loss is not None:\n        print(f\"validation loss: {val_loss:.4f}\")\n\n    # ---------- TEST ----------\n    print(\"Dataset: Test\")\n    test_acc = run_data.get(\"test_acc\")\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n    test_ura = run_data.get(\"test_ura\")\n    if test_ura is not None:\n        print(f\"test URA: {test_ura:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- utility ----------\ndef last(lst):\n    \"\"\"Return the last element of a list or None if the list is empty.\"\"\"\n    return lst[-1] if lst else None\n\n\n# ---------- iterate over runs ----------\nfor run_key, run_data in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"\\n=== Results for run: {run_key} ===\")\n\n    # ---------- TRAIN ----------\n    print(\"Dataset: Train\")\n    train_acc = last(run_data[\"metrics\"][\"train_acc\"])\n    if train_acc is not None:\n        print(f\"train accuracy: {train_acc:.4f}\")\n    train_loss = last(run_data[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"train loss: {train_loss:.4f}\")\n\n    # ---------- VALIDATION ----------\n    print(\"Dataset: Validation\")\n    val_acc = last(run_data[\"metrics\"][\"val_acc\"])\n    if val_acc is not None:\n        print(f\"validation accuracy: {val_acc:.4f}\")\n    val_ura = last(run_data[\"metrics\"][\"val_ura\"])\n    if val_ura is not None:\n        print(f\"validation URA: {val_ura:.4f}\")\n    val_loss = last(run_data[\"losses\"].get(\"val\", []))\n    if val_loss is not None:\n        print(f\"validation loss: {val_loss:.4f}\")\n\n    # ---------- TEST ----------\n    print(\"Dataset: Test\")\n    test_acc = run_data.get(\"test_acc\")\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n    test_ura = run_data.get(\"test_ura\")\n    if test_ura is not None:\n        print(f\"test URA: {test_ura:.4f}\")\n", ""], "parse_term_out": ["['\\n=== Results for run: epochs_5 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7350', '\\n', 'train loss: 0.4275', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7260', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.7320', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_10 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7680', '\\n', 'train loss: 0.3609', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7780', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8020', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_20 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.8555', '\\n', 'train loss: 0.3339', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.8540', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8780', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_30 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.9535', '\\n', 'train loss: 0.1646', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.9340', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.9390', '\\n', 'test URA: 1.0000', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH (learning rate = 0.0003)', '\\n', '  train accuracy (final epoch):\n0.7590', '\\n', '  train loss (final epoch): 0.4918', '\\n', '  validation\naccuracy (best): 0.7560', '\\n', '  validation URA (best): 1.0000', '\\n', '\nvalidation loss (best): nan', '\\n', '  test accuracy (final): 0.7720', '\\n', '\ntest URA (final): 1.0000\\n', '\\n', 'SPR_BENCH (learning rate = 0.0005)', '\\n', '\ntrain accuracy (final epoch): 0.7405', '\\n', '  train loss (final epoch):\n0.4419', '\\n', '  validation accuracy (best): 0.7560', '\\n', '  validation URA\n(best): 1.0000', '\\n', '  validation loss (best): nan', '\\n', '  test accuracy\n(final): 0.7420', '\\n', '  test URA (final): 1.0000\\n', '\\n', 'SPR_BENCH\n(learning rate = 0.001)', '\\n', '  train accuracy (final epoch): 0.7245', '\\n',\n'  train loss (final epoch): 0.4059', '\\n', '  validation accuracy (best):\n0.7560', '\\n', '  validation URA (best): 1.0000', '\\n', '  validation loss\n(best): nan', '\\n', '  test accuracy (final): 0.7340', '\\n', '  test URA\n(final): 1.0000\\n', '\\n', 'SPR_BENCH (learning rate = 0.002)', '\\n', '  train\naccuracy (final epoch): 0.8510', '\\n', '  train loss (final epoch): 0.3309',\n'\\n', '  validation accuracy (best): 0.8600', '\\n', '  validation URA (best):\n1.0000', '\\n', '  validation loss (best): nan', '\\n', '  test accuracy (final):\n0.8760', '\\n', '  test URA (final): 1.0000\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best train accuracy: 0.7920', '\\n', 'best validation\naccuracy: 0.7560', '\\n', 'best validation URA: 1.0000', '\\n', 'best test\naccuracy: 0.7720', '\\n', 'best test URA: 1.0000', '\\n', 'lowest training loss:\n0.3592', '\\n', 'lowest validation loss: 0.0000', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training accuracy:', ' ', '0.781', '\\n', 'validation\naccuracy:', ' ', '0.83', '\\n', 'test accuracy:', ' ', '0.806', '\\n', 'validation\nURA:', ' ', '0.0', '\\n', 'test URA:', ' ', '1.0', '\\n', 'training loss:', ' ',\n'0.3698476152420044', '\\n', 'validation loss:', ' ', '0.0', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset (weight_decay = 0.0)', '\\n', 'final train accuracy: 0.7480', '\\n',\n'final training loss: 0.4486', '\\n', 'final validation accuracy: 0.7340', '\\n',\n'final validation unseen-rule accuracy: 1.0000', '\\n', 'test accuracy: 0.7500',\n'\\n', 'test unseen-rule accuracy: 1.0000', '\\n', '\\nDataset (weight_decay =\n1e-06)', '\\n', 'final train accuracy: 0.7550', '\\n', 'final training loss:\n0.4618', '\\n', 'final validation accuracy: 0.7520', '\\n', 'final validation\nunseen-rule accuracy: 1.0000', '\\n', 'test accuracy: 0.7670', '\\n', 'test\nunseen-rule accuracy: 1.0000', '\\n', '\\nDataset (weight_decay = 1e-05)', '\\n',\n'final train accuracy: 0.7275', '\\n', 'final training loss: 0.4228', '\\n',\n'final validation accuracy: 0.7120', '\\n', 'final validation unseen-rule\naccuracy: 1.0000', '\\n', 'test accuracy: 0.7270', '\\n', 'test unseen-rule\naccuracy: 1.0000', '\\n', '\\nDataset (weight_decay = 0.0001)', '\\n', 'final train\naccuracy: 0.7220', '\\n', 'final training loss: 0.4223', '\\n', 'final validation\naccuracy: 0.7020', '\\n', 'final validation unseen-rule accuracy: 1.0000', '\\n',\n'test accuracy: 0.7190', '\\n', 'test unseen-rule accuracy: 1.0000', '\\n',\n'\\nDataset (weight_decay = 0.001)', '\\n', 'final train accuracy: 0.7310', '\\n',\n'final training loss: 0.4028', '\\n', 'final validation accuracy: 0.7560', '\\n',\n'final validation unseen-rule accuracy: 1.0000', '\\n', 'test accuracy: 0.7910',\n'\\n', 'test unseen-rule accuracy: 1.0000', '\\n', '\\nDataset (weight_decay =\n0.01)', '\\n', 'final train accuracy: 0.7165', '\\n', 'final training loss:\n0.4156', '\\n', 'final validation accuracy: 0.7180', '\\n', 'final validation\nunseen-rule accuracy: 1.0000', '\\n', 'test accuracy: 0.7280', '\\n', 'test\nunseen-rule accuracy: 1.0000', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Hyper-parameter setting (dropout rate): 0.0',\n'\\n', '    train accuracy: 0.7485', '\\n', '    best validation accuracy:\n0.7560', '\\n', '    best validation loss: 0.4442', '\\n', '    test accuracy:\n0.7450', '\\n', '    test URA: 1.0000', '\\n', '  Hyper-parameter setting (dropout\nrate): 0.1', '\\n', '    train accuracy: 0.7510', '\\n', '    best validation\naccuracy: 0.7560', '\\n', '    best validation loss: 0.4543', '\\n', '    test\naccuracy: 0.7720', '\\n', '    test URA: 1.0000', '\\n', '  Hyper-parameter\nsetting (dropout rate): 0.2', '\\n', '    train accuracy: 0.7365', '\\n', '\nbest validation accuracy: 0.7560', '\\n', '    best validation loss: 0.4053',\n'\\n', '    test accuracy: 0.7140', '\\n', '    test URA: 1.0000', '\\n', '  Hyper-\nparameter setting (dropout rate): 0.3', '\\n', '    train accuracy: 0.7390',\n'\\n', '    best validation accuracy: 0.7560', '\\n', '    best validation loss:\n0.4305', '\\n', '    test accuracy: 0.7560', '\\n', '    test URA: 1.0000', '\\n',\n'  Hyper-parameter setting (dropout rate): 0.4', '\\n', '    train accuracy:\n0.7525', '\\n', '    best validation accuracy: 0.7560', '\\n', '    best\nvalidation loss: 0.4606', '\\n', '    test accuracy: 0.7670', '\\n', '    test\nURA: 1.0000', '\\n', '  Hyper-parameter setting (dropout rate): 0.5', '\\n', '\ntrain accuracy: 0.7520', '\\n', '    best validation accuracy: 0.7560', '\\n', '\nbest validation loss: 0.4463', '\\n', '    test accuracy: 0.7670', '\\n', '\ntest URA: 1.0000', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Run: layers_1', '\\n', '    best train\naccuracy: 0.7590', '\\n', '    best validation accuracy: 0.7560', '\\n', '    best\nvalidation URA: 1.0000', '\\n', '    lowest training loss: 0.4776', '\\n', '  Run:\nlayers_2', '\\n', '    best train accuracy: 0.9295', '\\n', '    best validation\naccuracy: 0.9200', '\\n', '    best validation URA: 1.0000', '\\n', '    lowest\ntraining loss: 0.2083', '\\n', '  Run: layers_3', '\\n', '    best train accuracy:\n1.0000', '\\n', '    best validation accuracy: 1.0000', '\\n', '    best\nvalidation URA: 1.0000', '\\n', '    lowest training loss: 0.0064', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Configuration: relu', '\\n', '    final train\nloss: 0.4486', '\\n', '    final train accuracy: 0.7490', '\\n', '    final\nvalidation accuracy: 0.7340', '\\n', '    final validation unseen-rule accuracy:\n1.0000', '\\n', '    test accuracy: 0.7500', '\\n', '    test unseen-rule\naccuracy: 1.0000', '\\n', '  Configuration: leaky_relu', '\\n', '    final train\nloss: 0.4635', '\\n', '    final train accuracy: 0.7555', '\\n', '    final\nvalidation accuracy: 0.7520', '\\n', '    final validation unseen-rule accuracy:\n1.0000', '\\n', '    test accuracy: 0.7670', '\\n', '    test unseen-rule\naccuracy: 1.0000', '\\n', '  Configuration: gelu', '\\n', '    final train loss:\n0.4206', '\\n', '    final train accuracy: 0.7275', '\\n', '    final validation\naccuracy: 0.7080', '\\n', '    final validation unseen-rule accuracy: 1.0000',\n'\\n', '    test accuracy: 0.7260', '\\n', '    test unseen-rule accuracy:\n1.0000', '\\n', '  Configuration: tanh', '\\n', '    final train loss: 0.4458',\n'\\n', '    final train accuracy: 0.7410', '\\n', '    final validation accuracy:\n0.7240', '\\n', '    final validation unseen-rule accuracy: 1.0000', '\\n', '\ntest accuracy: 0.7380', '\\n', '    test unseen-rule accuracy: 1.0000', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== Results for run: epochs_5 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7350', '\\n', 'train loss: 0.4275', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7260', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.7320', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_10 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7680', '\\n', 'train loss: 0.3609', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7780', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8020', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_20 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.8555', '\\n', 'train loss: 0.3339', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.8540', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8780', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_30 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.9535', '\\n', 'train loss: 0.1646', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.9340', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.9390', '\\n', 'test URA: 1.0000', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== Results for run: epochs_5 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7350', '\\n', 'train loss: 0.4275', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7260', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.7320', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_10 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7680', '\\n', 'train loss: 0.3609', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7780', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8020', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_20 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.8555', '\\n', 'train loss: 0.3339', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.8540', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8780', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_30 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.9535', '\\n', 'train loss: 0.1646', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.9340', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.9390', '\\n', 'test URA: 1.0000', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== Results for run: epochs_5 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7350', '\\n', 'train loss: 0.4275', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7260', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.7320', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_10 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.7680', '\\n', 'train loss: 0.3609', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.7780', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8020', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_20 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.8555', '\\n', 'train loss: 0.3339', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.8540', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.8780', '\\n', 'test URA: 1.0000', '\\n',\n'\\n=== Results for run: epochs_30 ===', '\\n', 'Dataset: Train', '\\n', 'train\naccuracy: 0.9535', '\\n', 'train loss: 0.1646', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.9340', '\\n', 'validation URA: 1.0000', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.9390', '\\n', 'test URA: 1.0000', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}