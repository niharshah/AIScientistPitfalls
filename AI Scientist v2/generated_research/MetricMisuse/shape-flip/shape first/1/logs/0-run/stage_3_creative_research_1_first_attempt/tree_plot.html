<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 11], [0, 10], [0, 8], [0, 9], [0, 2], [0, 1], [0, 5], [0, 3], [0, 12], [3, 7], [3, 6]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.1111111111111111, 0.5], [0.2222222222222222, 0.5], [0.3333333333333333, 0.5], [0.4444444444444444, 0.5], [0.16666666666666666, 1.0], [0.2777777777777778, 1.0], [0.5555555555555556, 0.5], [0.6666666666666666, 0.5], [0.7777777777777778, 0.5], [0.8888888888888888, 0.5], [1.0, 0.5]], "plan": ["Hyperparam tuning name: hidden_dim. The solution iterates over a list of\ncandidate hidden\u2010state sizes (64 / 128 / 256 / 512).   For each size it builds a\nfresh SimpleSPRModel, trains it for a few epochs, evaluates on the dev and test\nsets, records all relevant statistics in a structured experiment_data dictionary\nkeyed by the hyper-parameter value, and finally saves the dictionary to\nexperiment_data.npy.", "We augment the baseline with an explicit symbolic channel: for every sequence we\nbuild a frequency vector of the shapes and colours that occur, normalise it, and\nfeed it through an MLP.  Simultaneously the raw token sequence is embedded and\nencoded by a bidirectional GRU with attention-style max-pooling.  The symbolic\nembedding and neural embedding are concatenated and classified, so the model can\nrely either on learned latent features or on directly provided reasoning\ncues\u2014facilitating zero-shot transfer to unseen rules.  We monitor standard\ntrain/validation accuracy, validation loss, and Unseen-Rule Accuracy (URA) at\nevery epoch, and use Shape-Weighted Accuracy (SWA) as the single headline metric\non the hidden test split.  All metrics and predictions are stored under working/\nfor later analysis.  The script obeys all GPU-handling, data-saving and\nexecution conventions requested.", "The model integrates a neural BiGRU encoder with symbolic reasoning by\nconcatenating two explicit rule-oriented features\u2014shape-variety count and color-\nvariety count\u2014before classification.  These counts act as a lightweight symbolic\nabstraction that helps the network recognise latent rules and boosts zero-shot\ngeneralisation.  We train once (few epochs for speed) and evaluate with Shape-\nWeighted Accuracy (SWA) as the single benchmark metric, plus Unseen-Rule\nAccuracy (URA) to gauge zero-shot transfer.  Validation loss, SWA and URA are\ntracked every epoch, then the full test set is scored.  All metrics, predictions\nand losses are stored in a numpy dictionary for later analysis.  The code\nrespects GPU/CPU handling, normalises inputs, and writes artefacts into\n./working.  It therefore provides a first creative step toward zero-shot neural-\nsymbolic PolyRule reasoning.", "Integrating simple symbolic reasoning features allows the network to \u201cexplain\u201d\npart of the decision with rule-relevant statistics while still benefiting from\ndistributed sequence representations.  We augment the Bi-GRU encoder with a\nsecond branch that receives three handcrafted inputs \u2013 number of distinct\nshapes, number of distinct colours and sequence length \u2013 processed by a tiny\nMLP.  The two representations are concatenated and fed to the classifier.\nBecause the symbolic features explicitly describe rule properties, they can\nguide the model on examples whose governing rule never appeared during training,\nimproving Unseen-Rule Accuracy (URA).  We train on SPR_BENCH, monitor validation\nloss every epoch, and report Shape-Weighted Accuracy (SWA) together with URA.\nAll metrics are stored in experiment_data and saved for later analysis.  The\nimplementation follows the mandatory GPU/CPU handling instructions, keeps data\nat \u201c./working\u201d, and finishes well within 30 minutes.  This first attempt seeks\nto show that even very small symbolic hints can noticeably boost zero-shot\ngeneralisation without any extra supervision.", "We extend the previous RNN baseline with a neural-symbolic hybrid: a Transformer\nencoder extracts sequence semantics while two symbolic features\u2014the number of\ndistinct shapes and colors\u2014are concatenated to the pooled embedding and\nprocessed by a small MLP.  This auxiliary information guides the model toward\nrule abstractions and, in zero-shot situations, acts like a learned \u201cmeta-rule\nprior.\u201d  We train on the original SPR_BENCH train split, monitor validation loss\neach epoch, and report Shape-Weighted Accuracy (our chosen metric) together with\nUnseen-Rule Accuracy (URA) on the test set.  All metrics, losses, predictions,\nand ground-truth labels are saved to ./working/experiment_data.npy for later\nanalysis.", "We augment the recurrent baseline with lightweight symbolic features\u2014shape-\nvariety, color-variety and sequence length\u2014and concatenate them with the GRU\nsentence vector before classification.  This hybrid \u201cneural + symbolic\u201d\nrepresentation should help the network reason about rule structure while\nremaining trainable end-to-end.  We train both the original baseline (neural\nonly) and the new hybrid model for three short epochs to keep runtime modest,\nlogging standard accuracy, our target metric Shape-Weighted Accuracy (SWA) and\nthe required Unseen-Rule Accuracy (URA).  Validation loss and metrics are\nprinted every epoch and stored together with predictions in the mandatory\nexperiment_data dict, which is finally saved to ./working/experiment_data.npy.\nAn ablation study is therefore obtained \u201cfor free\u2019\u2019 by directly comparing the\ntwo runs.  All tensors, models and optimizers are moved onto GPU if available,\nand every batch is transferred to the selected device.  The code is fully self-\ncontained, executes on launch, and respects all file-handling, logging and\nplotting conventions laid out in the instructions.", "A large drop from near-perfect dev accuracy to \u226470 % test accuracy indicates the\nmodel is unable to handle tokens it never saw during training.  In the original\ncode each token (e.g. \u201cCr\u201d) is treated as a single id, so any unseen shape-color\ncombination becomes <unk>.  This destroys the zero-shot generalisation we are\ntrying to measure.  We fix the bug by factorising every token into two\ncharacters \u2013 a shape and a colour \u2013 learn separate shape and colour embeddings,\nand combine them at run-time.  Because each character domain is small, every\nnovel combination is now covered even if it never appeared during training.", "We fix the zero-shot evaluation bug by explicitly holding out a subset of rule\nlabels (unseen_labels) from the training/dev splits, ensuring that Unseen-Rule\nAccuracy (URA) is measurable instead of NaN.  We also comply with the\nrequirement to evaluate on exactly one metric\u2014Color-Weighted Accuracy (CWA)\u2014and\ncache an id\u2192token lookup to reconstruct sequences reliably when computing CWA.\nBatch-to-device transfer is centralised, and the evaluation loop is simplified\nto avoid the earlier sequence-reconstruction errors.  The complete, self-\ncontained script below trains, validates (tracking loss/CWA/URA each epoch),\ntests, and saves all artefacts in the mandated working directory.", "We enrich the baseline with explicit symbolic attributes\u2014shape variety, color\nvariety, and sequence length\u2014then fuse these with a lightweight Transformer\nencoder so the model can reason both neurally and symbolically. Symbolic\nfeatures are z-score normalised (computed from the training split) and projected\nto the same dimensionality as the Transformer\u2019s hidden size; the two\nrepresentations are summed before classification. We optimise cross-entropy on\nthe seen-rule labels, monitor validation loss, SWA, and URA each epoch, and\nfinally evaluate on the test split, reporting SWA and URA. All metrics, losses,\npredictions, and ground-truth labels are stored in experiment_data.npy for later\nanalysis.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["# hyper_dim_tuning.py\nimport os, pathlib, time, random, json, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom typing import List, Dict\n\n# ----------------------------------------------------------------------\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ----------------------------------------------------------------------\n# experiment container --------------------------------------------------\nexperiment_data = {\"hidden_dim_tuning\": {\"SPR_BENCH\": {}}}\n# ----------------------------------------------------------------------\n# working dir / save path ----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n# ----------------------------------------------------------------------\n# device ----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------------------------------------------------------\n# util functions --------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ----------------------------------------------------------------------\n# load data -------------------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ----------------------------------------------------------------------\n# vocab + label mapping -------------------------------------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_seq(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"# seen rule labels: {num_labels}\")\n\n\n# ----------------------------------------------------------------------\n# Torch dataset ---------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, vocab, label2id, train_mode=True):\n        self.seq_enc = [encode_seq(s, vocab) for s in split[\"sequence\"]]\n        self.labels = split[\"label\"]\n        self.train_mode = train_mode\n        self.label2id = label2id\n\n    def __len__(self):\n        return len(self.seq_enc)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.seq_enc[idx], dtype=torch.long)\n        if self.train_mode:\n            y = torch.tensor(self.label2id[self.labels[idx]], dtype=torch.long)\n            return {\"input\": x, \"label\": y}\n        else:\n            return {\"input\": x, \"label_str\": self.labels[idx]}\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    out = {\"input\": xs_pad, \"lengths\": torch.tensor(lens, dtype=torch.long)}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\n# create loaders (same across experiments)\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, label2id, False)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------------------------------------------------------\n# Model ----------------------------------------------------------------\nclass SimpleSPRModel(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(hidden_dim * 2, num_labels)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        return self.lin(h_cat)\n\n\n# ----------------------------------------------------------------------\n# training / eval helpers ----------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(inp, lens)\n            loss = criterion(logits, lbl)\n            if train:\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n            tot_loss += loss.item() * inp.size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == lbl).sum().item()\n            tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate_test(model):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            logits = model(inp, lens)\n            preds = logits.argmax(1).cpu().tolist()\n            label_strs = batch[\"label_str\"]\n            all_preds.extend([id2label.get(p, \"UNK\") for p in preds])\n            all_labels.extend(label_strs)\n            all_seqs.extend(\n                [\n                    \" \".join(\n                        [list(vocab.keys())[tok] for tok in seq.tolist() if tok != 0]\n                    )\n                    for seq in batch[\"input\"]\n                ]\n            )\n    overall_acc = np.mean([p == t for p, t in zip(all_preds, all_labels)])\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    seen_rules = set(train_labels)\n    zs_idx = [i for i, lbl in enumerate(all_labels) if lbl not in seen_rules]\n    zs_acc = (\n        np.mean([all_preds[i] == all_labels[i] for i in zs_idx])\n        if zs_idx\n        else float(\"nan\")\n    )\n    return overall_acc, swa, cwa, zs_acc, all_preds, all_labels\n\n\n# ----------------------------------------------------------------------\n# hyperparameter search -------------------------------------------------\nhidden_dims = [64, 128, 256, 512]\nEPOCHS = 5\nfor hd in hidden_dims:\n    print(f\"\\n=== Training with hidden_dim={hd} ===\")\n    # init model/optim/criterion\n    model = SimpleSPRModel(len(vocab), 64, hd, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # containers\n    mdata = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_loss\": [], \"ZSRTA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    # train loop\n    for epoch in range(1, EPOCHS + 1):\n        tr_loss, tr_acc = run_epoch(model, train_loader, criterion, opt)\n        val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n        mdata[\"metrics\"][\"train_acc\"].append(tr_acc)\n        mdata[\"metrics\"][\"val_acc\"].append(val_acc)\n        mdata[\"metrics\"][\"val_loss\"].append(val_loss)\n        mdata[\"losses\"][\"train\"].append(tr_loss)\n        mdata[\"losses\"][\"val\"].append(val_loss)\n        mdata[\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {epoch}: train_acc={tr_acc:.4f} val_acc={val_acc:.4f} val_loss={val_loss:.4f}\"\n        )\n    # final evaluation\n    overall_acc, swa, cwa, zs_acc, all_preds, all_labels = evaluate_test(model)\n    mdata[\"metrics\"][\"ZSRTA\"].append(zs_acc)\n    mdata[\"predictions\"] = all_preds\n    mdata[\"ground_truth\"] = all_labels\n    print(\n        f\"TEST Acc={overall_acc:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | ZSRTA={zs_acc:.4f}\"\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][f\"hidden_{hd}\"] = mdata\n\n# ----------------------------------------------------------------------\n# save ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, time, random, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------------------------------------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_loss\": [], \"URA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(fn):\n        return load_dataset(\n            \"csv\", data_files=str(root / fn), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        d[sp] = _load(f\"{sp}.csv\")\n    return d\n\n\n# try several likely locations\ncandidate_paths = [\n    os.getenv(\"SPR_PATH\", \"\"),\n    \"./SPR_BENCH\",\n    \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",\n]\nDATA_PATH = None\nfor p in candidate_paths:\n    if p and os.path.exists(p):\n        DATA_PATH = pathlib.Path(p)\n        break\nif DATA_PATH is None:\n    raise FileNotFoundError(\"SPR_BENCH dataset not found in any default location.\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ---------------------------------------------------------------------\n# vocab for tokens, shapes, colours\ndef build_vocab(dataset):\n    tok2id = {\"<pad>\": 0, \"<unk>\": 1}\n    shape_set = set()\n    color_set = set()\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            shape_set.add(tok[0])\n            if len(tok) > 1:\n                color_set.add(tok[1])\n            if tok not in tok2id:\n                tok2id[tok] = len(tok2id)\n    return tok2id, sorted(shape_set), sorted(color_set)\n\n\ntok2id, shape_list, color_list = build_vocab(spr[\"train\"])\nshape2idx = {s: i for i, s in enumerate(shape_list)}\ncolor2idx = {c: i for i, c in enumerate(color_list)}\nnum_shapes = len(shape_list)\nnum_colors = len(color_list)\nprint(f\"Vocab size={len(tok2id)}, shapes={num_shapes}, colors={num_colors}\")\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nseen_rule_set = set(train_labels)\n\n\n# ---------------------------------------------------------------------\ndef encode_seq(seq):\n    return [tok2id.get(tok, tok2id[\"<unk>\"]) for tok in seq.strip().split()]\n\n\ndef symbolic_vector(seq):\n    shape_counts = np.zeros(num_shapes, dtype=np.float32)\n    color_counts = np.zeros(num_colors, dtype=np.float32)\n    toks = seq.strip().split()\n    for tok in toks:\n        s = tok[0]\n        shape_counts[shape2idx.get(s, 0)] += 1\n        if len(tok) > 1:\n            c = tok[1]\n            color_counts[color2idx.get(c, 0)] += 1\n    if len(toks) > 0:\n        shape_counts /= len(toks)\n        color_counts /= len(toks)\n    return np.concatenate([shape_counts, color_counts])\n\n\n# ---------------------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, train=True):\n        self.seqs = [encode_seq(s) for s in split[\"sequence\"]]\n        self.sym = [symbolic_vector(s) for s in split[\"sequence\"]]\n        self.train = train\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        item = {\n            \"input\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"sym\": torch.tensor(self.sym[idx], dtype=torch.float32),\n        }\n        if self.train:\n            item[\"label\"] = torch.tensor(label2id[self.labels[idx]], dtype=torch.long)\n        else:\n            item[\"label_str\"] = self.labels[idx]\n        return item\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    out = {\n        \"input\": xs_pad,\n        \"lengths\": torch.tensor(lens, dtype=torch.long),\n        \"sym\": torch.stack([b[\"sym\"] for b in batch]),\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], True),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"], True), batch_size=256, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(spr[\"test\"], False),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------------------------\nclass HybridSPR(nn.Module):\n    def __init__(self, vocab, sym_dim, emb_dim=64, hid=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.sym_proj = nn.Linear(sym_dim, hid * 2)\n        self.classifier = nn.Linear(hid * 4, num_labels)\n\n    def forward(self, x, lengths, sym):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        out, _ = self.gru(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        # max-pool over time\n        mask = (x != 0).unsqueeze(-1)\n        out = out.masked_fill(~mask, -1e9)\n        pooled = out.max(1).values\n        sym_emb = torch.relu(self.sym_proj(sym))\n        feat = torch.cat([pooled, sym_emb], dim=-1)\n        return self.classifier(feat)\n\n\nmodel = HybridSPR(len(tok2id), num_shapes + num_colors).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train_mode=False):\n    if train_mode:\n        model.train()\n    else:\n        model.eval()\n    total_loss = total_ok = total = 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"input\"], batch_t[\"lengths\"], batch_t[\"sym\"])\n            loss = criterion(logits, batch_t[\"label\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch_t[\"input\"].size(0)\n            preds = logits.argmax(1)\n            total_ok += (preds == batch_t[\"label\"]).sum().item()\n            total += batch_t[\"input\"].size(0)\n    return total_loss / total, total_ok / total\n\n\ndef evaluate_URA(loader, seen_rules):\n    model.eval()\n    ok = tot = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            unseen_mask = [l.item() not in seen_rules for l in batch_t[\"label\"]]\n            if not any(unseen_mask):\n                continue\n            logits = model(batch_t[\"input\"], batch_t[\"lengths\"], batch_t[\"sym\"])\n            preds = logits.argmax(1).cpu()\n            for i, um in enumerate(unseen_mask):\n                if um:\n                    tot += 1\n                    if preds[i].item() == batch_t[\"label\"][i].cpu().item():\n                        ok += 1\n    return ok / tot if tot else float(\"nan\")\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    def cnt(s):\n        return len(set(tok[0] for tok in s.strip().split() if tok))\n\n    w = [cnt(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(train_loader, True)\n    val_loss, val_acc = run_epoch(dev_loader, False)\n    ura = evaluate_URA(dev_loader, seen_rule_set)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"URA\"].append(ura)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(\n        f\"Epoch {epoch}: train_acc={tr_acc:.4f} | val_acc={val_acc:.4f} | val_loss={val_loss:.4f} | URA={ura:.4f}\"\n    )\n\n# ---------------------------------------------------------------------\n# Final test evaluation\nmodel.eval()\nall_seqs, all_labels, all_preds = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        inp = batch[\"input\"].to(device)\n        logits = model(inp, batch[\"lengths\"].to(device), batch[\"sym\"].to(device))\n        preds = logits.argmax(1).cpu().tolist()\n        all_preds.extend([id2label.get(p, \"UNK\") for p in preds])\n        all_labels.extend(batch[\"label_str\"])\n        all_seqs.extend(batch[\"input\"])\n# reconstruct sequences for SWA\nrev_vocab = {v: k for k, v in tok2id.items()}\n\n\ndef seq_from_tensor(t):\n    toks = [rev_vocab[i.item()] for i in t if i.item() != 0]\n    return \" \".join(toks)\n\n\nall_seqs_txt = [seq_from_tensor(t) for t in all_seqs]\nswa = shape_weighted_accuracy(all_seqs_txt, all_labels, all_preds)\nura_test = np.mean(\n    [p == t for p, t in zip(all_preds, all_labels) if t not in seen_rule_set]\n)\noverall_acc = np.mean([p == t for p, t in zip(all_preds, all_labels)])\nprint(f\"\\nTEST: overall_acc={overall_acc:.4f} | SWA={swa:.4f} | URA={ura_test:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------------------------------------------------------------\n# working directory & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------------------------------------------------------------\n# experiment container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"val_loss\": [],\n            \"val_SWA\": [],\n            \"val_URA\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ----------------------------------------------------------------------\n# reproducibility\nSEED = 1337\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# ----------------------------------------------------------------------\n# load SPR_BENCH --------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_root = pathlib.Path(os.getenv(\"SPR_DATA\", \"SPR_BENCH\"))\nassert data_root.exists(), f\"Folder {data_root} with SPR_BENCH *.csv files not found.\"\nspr = load_spr_bench(data_root)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ----------------------------------------------------------------------\n# helper metrics --------------------------------------------------------\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split() if tok))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# ----------------------------------------------------------------------\n# vocab & label mappings ------------------------------------------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for s in dataset[\"sequence\"]:\n        for tok in s.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\n\n\n# ----------------------------------------------------------------------\n# torch Dataset ---------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, vocab, label2id, with_label=True):\n        self.seqs = hf_split[\"sequence\"]\n        self.encoded = [self.encode(x) for x in self.seqs]\n        self.with_label = with_label\n        if with_label:\n            self.labels = [label2id.get(lbl, -1) for lbl in hf_split[\"label\"]]\n        # cache symbolic features\n        self.sym = [(count_shape_variety(s), count_color_variety(s)) for s in self.seqs]\n\n    def encode(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        item = {\n            \"input_ids\": torch.tensor(self.encoded[idx], dtype=torch.long),\n            \"sym_feats\": torch.tensor(self.sym[idx], dtype=torch.float),\n        }\n        if self.with_label:\n            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n            item[\"label_str\"] = spr[\"train\"][\"label\"][0]  # dummy to keep key\n        else:\n            item[\"label_str\"] = spr[\"test\"][\"label\"][idx]\n        return item\n\n\ndef collate(batch):\n    xs = [b[\"input_ids\"] for b in batch]\n    lens = torch.tensor([len(x) for x in xs], dtype=torch.long)\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    sym = torch.stack([b[\"sym_feats\"] for b in batch])\n    out = {\"input_ids\": xs_pad, \"lengths\": lens, \"sym_feats\": sym}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\n# ----------------------------------------------------------------------\n# DataLoaders -----------------------------------------------------------\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, label2id, False)\n\ntrain_dl = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_dl = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_dl = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\nunseen_rule_set = set(lbl for lbl in spr[\"dev\"][\"label\"] if lbl not in train_labels)\n\n\n# ----------------------------------------------------------------------\n# Model ----------------------------------------------------------------\nclass NeuroSymbolicSPR(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, hid_dim, sym_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n        self.sym_lin = nn.Linear(2, sym_dim)  # two symbolic counts\n        self.classifier = nn.Linear(hid_dim * 2 + sym_dim, num_labels)\n\n    def forward(self, ids, lengths, sym_feats):\n        emb = self.emb(ids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)  # (batch, hid*2)\n        sym_proj = torch.relu(self.sym_lin(sym_feats))\n        feat = torch.cat([h_cat, sym_proj], dim=-1)\n        return self.classifier(feat)\n\n\n# ----------------------------------------------------------------------\n# train / eval helpers --------------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    model.train() if opt else model.eval()\n    tot_loss = tot_ok = tot = 0\n    for batch in loader:\n        ids = batch[\"input_ids\"].to(device)\n        lens = batch[\"lengths\"].to(device)\n        sym = batch[\"sym_feats\"].to(device)\n        lbls = batch[\"label\"].to(device) if \"label\" in batch else None\n        logits = model(ids, lens, sym)\n        if lbls is not None:\n            loss = criterion(logits, lbls)\n            if opt:\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n            tot_loss += loss.item() * ids.size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == lbls).sum().item()\n        tot += ids.size(0)\n    return (tot_loss / tot if tot else 0.0, tot_ok / tot if tot else 0.0)\n\n\ndef evaluate(loader, model):\n    model.eval()\n    preds, labels, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            ids = batch[\"input_ids\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            sym = batch[\"sym_feats\"].to(device)\n            logits = model(ids, lens, sym)\n            pr = logits.argmax(1).cpu().tolist()\n            preds.extend([id2label.get(p, \"UNK\") for p in pr])\n            labels.extend(batch[\"label_str\"])\n            seqs.extend(batch[\"input_ids\"])\n    # rebuild original sequences (for SWA)\n    seq_strings = []\n    inv_vocab = {i: t for t, i in vocab.items()}\n    for tens in seqs:\n        toks = [inv_vocab[idx.item()] for idx in tens if idx.item() != 0]\n        seq_strings.append(\" \".join(toks))\n    acc = np.mean([p == t for p, t in zip(preds, labels)])\n    swa = shape_weighted_accuracy(seq_strings, labels, preds)\n    unseen_idx = [i for i, l in enumerate(labels) if l in unseen_rule_set]\n    ura = (\n        np.mean([preds[i] == labels[i] for i in unseen_idx])\n        if unseen_idx\n        else float(\"nan\")\n    )\n    return acc, swa, ura, preds, labels\n\n\n# ----------------------------------------------------------------------\n# training loop ---------------------------------------------------------\nmodel = NeuroSymbolicSPR(\n    len(vocab), emb_dim=64, hid_dim=256, sym_dim=16, num_labels=num_labels\n).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nEPOCHS = 5\nfor ep in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(model, train_dl, criterion, optimizer)\n    val_loss, val_acc = run_epoch(model, dev_dl, criterion)\n    _, val_swa, val_ura, _, _ = evaluate(dev_dl, model)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_SWA\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_URA\"].append(val_ura)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | \"\n        f\"VAL_ACC={val_acc:.4f} | VAL_SWA={val_swa:.4f} | VAL_URA={val_ura:.4f}\"\n    )\n\n# ----------------------------------------------------------------------\n# final test evaluation -------------------------------------------------\ntest_acc, test_swa, test_ura, test_preds, test_labels = evaluate(test_dl, model)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nprint(f\"\\nTEST: ACC={test_acc:.4f} | SWA={test_swa:.4f} | URA={test_ura:.4f}\")\n\n# ----------------------------------------------------------------------\n# save ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, time, random, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------\n# workspace & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# -------------------------------------------------------------\n# helpers to load dataset -------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0.0\n\n\n# -------------------------------------------------------------\n# load data ----------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# -------------------------------------------------------------\n# vocabulary & label mapping ----------------------------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"vocab size:\", len(vocab))\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"#train labels:\", num_labels)\n\n\n# -------------------------------------------------------------\n# dataset class ------------------------------------------------\ndef encode_seq(seq, vocab):\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, vocab, label2id, training=True):\n        self.training = training\n        self.seqs = split[\"sequence\"]\n        self.seq_enc = [encode_seq(s, vocab) for s in self.seqs]\n        if training:\n            self.labels = [label2id[l] for l in split[\"label\"]]\n        else:\n            self.labels = split[\"label\"]  # keep string for evaluation\n\n    def __len__(self):\n        return len(self.seq_enc)\n\n    def __getitem__(self, idx):\n        seq_tensor = torch.tensor(self.seq_enc[idx], dtype=torch.long)\n        feat = torch.tensor(\n            [\n                count_shape_variety(self.seqs[idx]),\n                count_color_variety(self.seqs[idx]),\n                len(self.seq_enc[idx]),\n            ],\n            dtype=torch.float,\n        )\n        if self.training:\n            return {\n                \"input\": seq_tensor,\n                \"sym\": feat,\n                \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            }\n        else:\n            return {\n                \"input\": seq_tensor,\n                \"sym\": feat,\n                \"label_str\": self.labels[idx],\n                \"raw_seq\": self.seqs[idx],\n            }\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    sym = torch.stack([b[\"sym\"] for b in batch])\n    out = {\"input\": xs_pad, \"lengths\": torch.tensor(lens, dtype=torch.long), \"sym\": sym}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n        out[\"raw_seq\"] = [b[\"raw_seq\"] for b in batch]\n    return out\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, label2id, False)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------------------------------------------------------\n# model --------------------------------------------------------\nclass NeuroSymbolicSPR(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU()\n        )\n        self.out = nn.Linear(hidden_dim * 2 + 16, num_labels)\n\n    def forward(self, x, lens, sym):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.rnn(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        sym_feat = self.sym_mlp(sym)\n        cat = torch.cat([h_cat, sym_feat], dim=-1)\n        return self.out(cat)\n\n\n# -------------------------------------------------------------\n# training helpers --------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            sym = batch[\"sym\"].to(device)\n            logits = model(inp, lens, sym)\n            if train:\n                lbl = batch[\"label\"].to(device)\n                loss = criterion(logits, lbl)\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n                tot_loss += loss.item() * inp.size(0)\n                preds = logits.argmax(1)\n                tot_ok += (preds == lbl).sum().item()\n                tot += inp.size(0)\n            else:\n                lbl = batch[\"label\"].to(device)\n                loss = criterion(logits, lbl)\n                tot_loss += loss.item() * inp.size(0)\n                preds = logits.argmax(1)\n                tot_ok += (preds == lbl).sum().item()\n                tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\n@torch.no_grad()\ndef evaluate(loader, model):\n    model.eval()\n    preds, labels, seqs = [], [], []\n    for batch in loader:\n        inp = batch[\"input\"].to(device)\n        lens = batch[\"lengths\"].to(device)\n        sym = batch[\"sym\"].to(device)\n        logits = model(inp, lens, sym)\n        p = logits.argmax(1).cpu().tolist()\n        preds.extend([id2label.get(idx, \"UNK\") for idx in p])\n        if \"label_str\" in batch:\n            labels.extend(batch[\"label_str\"])\n        else:\n            labels.extend([id2label[idx.item()] for idx in batch[\"label\"]])\n        seqs.extend(\n            batch[\"raw_seq\"]\n            if \"raw_seq\" in batch\n            else (\n                batch[\"input_seq\"]\n                if \"input_seq\" in batch\n                else [\n                    \" \".join(\n                        [list(vocab.keys())[tok] for tok in row.tolist() if tok != 0]\n                    )\n                    for row in batch[\"input\"]\n                ]\n            )\n        )\n    acc = np.mean([p == t for p, t in zip(preds, labels)])\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    # URA\n    unseen_idx = [i for i, lbl in enumerate(labels) if lbl not in train_labels]\n    ura = (\n        np.mean([preds[i] == labels[i] for i in unseen_idx])\n        if unseen_idx\n        else float(\"nan\")\n    )\n    return acc, swa, ura\n\n\n# -------------------------------------------------------------\n# experiment container ----------------------------------------\nexperiment_data = {\n    \"NeuroSymbolicSPR\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"val_loss\": [],\n            \"val_SWA\": [],\n            \"URA\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# -------------------------------------------------------------\n# training -----------------------------------------------------\nmodel = NeuroSymbolicSPR(len(vocab), 64, 128, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n    _, val_swa, val_ura = evaluate(dev_loader, model)\n    experiment_data[\"NeuroSymbolicSPR\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"NeuroSymbolicSPR\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"NeuroSymbolicSPR\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"NeuroSymbolicSPR\"][\"metrics\"][\"val_SWA\"].append(val_swa)\n    experiment_data[\"NeuroSymbolicSPR\"][\"metrics\"][\"URA\"].append(val_ura)\n    experiment_data[\"NeuroSymbolicSPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"NeuroSymbolicSPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"NeuroSymbolicSPR\"][\"timestamps\"].append(time.time())\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_acc={val_acc:.4f} | val_SWA={val_swa:.4f} | URA={val_ura:.4f}\"\n    )\n\n# -------------------------------------------------------------\n# final test evaluation ---------------------------------------\ntest_acc, test_swa, test_ura = evaluate(test_loader, model)\nprint(f\"\\nTEST  Acc={test_acc:.4f}  SWA={test_swa:.4f}  URA={test_ura:.4f}\")\n\n# store predictions / ground truth\nwith torch.no_grad():\n    preds, labels = [], []\n    for batch in test_loader:\n        inp = batch[\"input\"].to(device)\n        lens = batch[\"lengths\"].to(device)\n        sym = batch[\"sym\"].to(device)\n        logits = model(inp, lens, sym)\n        p = logits.argmax(1).cpu().tolist()\n        preds.extend([id2label.get(idx, \"UNK\") for idx in p])\n        labels.extend(batch[\"label_str\"])\nexperiment_data[\"NeuroSymbolicSPR\"][\"predictions\"] = preds\nexperiment_data[\"NeuroSymbolicSPR\"][\"ground_truth\"] = labels\n\n# -------------------------------------------------------------\n# save experiment data ----------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, time, random, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- reproducibility ----------\nSEED = 1234\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ---------- experiment container ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"val_loss\": [],\n            \"URA\": [],\n            \"SWA\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\n# ---------- helpers ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.split()})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(1, sum(w))\n\n\ndef unseen_rule_accuracy(train_labels_set, y_true, y_pred):\n    idx = [i for i, l in enumerate(y_true) if l not in train_labels_set]\n    if not idx:\n        return float(\"nan\")\n    return np.mean([y_true[i] == y_pred[i] for i in idx])\n\n\n# ---------- load data ----------\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- vocab ----------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nv2i = vocab\ni2v = [None] * len(vocab)\nfor k, v in v2i.items():\n    i2v[v] = k\n\n\ndef encode(seq: str):\n    return [v2i.get(tok, 1) for tok in seq.split()]\n\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {\n    l: i for i, l in enumerate(train_labels)\n}  # unknown label will be handled later\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id) + 1  # +1 for unseen label token\n\nUNK_LABEL_ID = num_labels - 1\n\n\n# ---------- dataset ----------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, train_mode=True):\n        self.seqs = [encode(s) for s in split[\"sequence\"]]\n        self.shape_var = [count_shape_variety(s) for s in split[\"sequence\"]]\n        self.color_var = [count_color_variety(s) for s in split[\"sequence\"]]\n        self.labels = split[\"label\"]\n        self.train_mode = train_mode\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.seqs[idx], dtype=torch.long)\n        shp = torch.tensor(self.shape_var[idx], dtype=torch.float)\n        col = torch.tensor(self.color_var[idx], dtype=torch.float)\n        if self.train_mode:\n            y = torch.tensor(label2id[self.labels[idx]], dtype=torch.long)\n            return {\"input\": x, \"shape\": shp, \"color\": col, \"label\": y}\n        else:\n            return {\n                \"input\": x,\n                \"shape\": shp,\n                \"color\": col,\n                \"label_str\": self.labels[idx],\n            }\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    shapes = torch.tensor([b[\"shape\"] for b in batch], dtype=torch.float)\n    colors = torch.tensor([b[\"color\"] for b in batch], dtype=torch.float)\n    out = {\n        \"input\": xs_pad,\n        \"lengths\": torch.tensor(lens, dtype=torch.long),\n        \"shape\": shapes,\n        \"color\": colors,\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], True)\ntest_ds = SPRTorchDataset(spr[\"test\"], False)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------- model ----------\nclass HybridSPR(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=8,\n            dim_feedforward=hidden_dim,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        self.mlp_sym = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 16))\n        self.classifier = nn.Linear(emb_dim + 16, num_labels)\n\n    def forward(self, x, lens, shape_var, color_var):\n        e = self.emb(x)  # (B,L,E)\n        mask = x == 0  # pad mask\n        h = self.encoder(e, src_key_padding_mask=mask)  # (B,L,E)\n        # masked mean pooling\n        mask_inv = (~mask).unsqueeze(-1)\n        h_sum = (h * mask_inv).sum(1)\n        h_mean = h_sum / torch.clamp(mask_inv.sum(1), min=1)\n        sym_feat = self.mlp_sym(torch.stack([shape_var, color_var], dim=1))\n        logits = self.classifier(torch.cat([h_mean, sym_feat], dim=-1))\n        return logits\n\n\n# ---------- training helpers ----------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    for batch in loader:\n        inp = batch[\"input\"].to(device)\n        lens = batch[\"lengths\"].to(device)\n        shp = batch[\"shape\"].to(device)\n        col = batch[\"color\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        logits = model(inp, lens, shp, col)\n        loss = criterion(logits, lbl)\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        tot_loss += loss.item() * inp.size(0)\n        tot_ok += (logits.argmax(1) == lbl).sum().item()\n        tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate_test(model):\n    model.eval()\n    preds_all = []\n    labels_all = []\n    seqs = []\n    with torch.no_grad():\n        for batch in test_loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            shp = batch[\"shape\"].to(device)\n            col = batch[\"color\"].to(device)\n            logits = model(inp, lens, shp, col)\n            preds = logits.argmax(1).cpu().tolist()\n            preds_lbl = [id2label.get(p, \"UNK\") for p in preds]\n            preds_all.extend(preds_lbl)\n            labels_all.extend(batch[\"label_str\"])\n            for seq in batch[\"input\"]:\n                tokens = [i2v[tok] for tok in seq.tolist() if tok != 0]\n                seqs.append(\" \".join(tokens))\n    swa = shape_weighted_accuracy(seqs, labels_all, preds_all)\n    ura = unseen_rule_accuracy(set(train_labels), labels_all, preds_all)\n    acc = np.mean([p == t for p, t in zip(preds_all, labels_all)])\n    return acc, swa, ura, preds_all, labels_all\n\n\n# ---------- training ----------\nmodel = HybridSPR(len(vocab), emb_dim=128, hidden_dim=256, num_labels=num_labels).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\nEPOCHS = 8\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_acc = {val_acc:.4f}\")\n\n# ---------- final evaluation ----------\nacc, swa, ura, preds, labels = evaluate_test(model)\nprint(f\"\\nTEST  Acc={acc:.4f} | SWA={swa:.4f} | URA={ura:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SWA\"].append(swa)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"URA\"].append(ura)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, time, random, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ----------------------------------------------------------------------\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# experiment container\nexperiment_data = {\"SPR_BENCH\": {\"baseline\": {}, \"hybrid\": {}}}\n\n\n# ----------------------------------------------------------------------\n# 1. Data ----------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\n# path \u2013 allow env override\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_PATH\", \"SPR_BENCH\"))\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\"SPR_BENCH folder not found \u2013 set $SPR_PATH\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# vocab\ndef build_vocab(dsets):\n    voc = {\"<pad>\": 0, \"<unk>\": 1}\n    for split in dsets.values():\n        for seq in split[\"sequence\"]:\n            for tok in seq.strip().split():\n                if tok not in voc:\n                    voc[tok] = len(voc)\n    return voc\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# label mapping (train set only \u2013 unseen labels stay strings)\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"# train labels:\", num_labels)\n\n\n# feature helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\n# torch dataset\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, mode: str):\n        self.mode = mode  # 'train','dev','test'\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        toks = [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n        shp = count_shape_variety(seq)\n        col = count_color_variety(seq)\n        ln = len(toks)\n        item = {\n            \"input\": torch.tensor(toks, dtype=torch.long),\n            \"feats\": torch.tensor([shp, col, ln], dtype=torch.float),\n        }\n        if self.mode == \"train\":\n            item[\"label\"] = torch.tensor(label2id[self.labels[idx]], dtype=torch.long)\n        else:\n            item[\"label_str\"] = self.labels[idx]\n        return item\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    feats = torch.stack([b[\"feats\"] for b in batch])\n    out = {\n        \"input\": xs_pad,\n        \"lengths\": torch.tensor(lens, dtype=torch.long),\n        \"feats\": feats,\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], \"train\"),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"], \"dev\"),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(spr[\"test\"], \"test\"),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ----------------------------------------------------------------------\n# 2. Models -------------------------------------------------------------\nclass BaseModel(nn.Module):\n    def __init__(self, use_feats: bool):\n        super().__init__()\n        emb_dim, hid_dim = 64, 128\n        self.use_feats = use_feats\n        self.emb = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n        final_in = hid_dim * 2 + (3 if use_feats else 0)\n        self.lin = nn.Linear(final_in, num_labels)\n\n    def forward(self, x, lengths, feats=None):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)  # (batch, 2*hid)\n        if self.use_feats:\n            feats = feats.to(h_cat)\n            h_cat = torch.cat([h_cat, feats / 10.0], dim=-1)  # simple normalisation\n        return self.lin(h_cat)\n\n\n# ----------------------------------------------------------------------\n# 3. Train / Eval helpers ----------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            feats = batch[\"feats\"].to(device)\n            lbls = batch.get(\"label\")\n            if lbls is not None:\n                lbls = lbls.to(device)\n            logits = model(inp, lens, feats)\n            if lbls is not None:\n                loss = criterion(logits, lbls)\n                if train:\n                    opt.zero_grad()\n                    loss.backward()\n                    opt.step()\n                tot_loss += loss.item() * inp.size(0)\n                preds = logits.argmax(1)\n                tot_ok += (preds == lbls).sum().item()\n                tot += inp.size(0)\n    if tot == 0:  # eval on sets without labels\n        return 0, 0\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate(model, loader):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            feats = batch[\"feats\"].to(device)\n            logits = model(inp, lens, feats)\n            preds = logits.argmax(1).cpu().tolist()\n            pred_labels = [id2label.get(p, \"UNK\") for p in preds]\n            all_preds.extend(pred_labels)\n            lab = batch.get(\"label_str\", [])\n            all_labels.extend(lab)\n            # recover original seq string (optional for metric)\n            for seq_tensor in batch[\"input\"]:\n                toks = [\n                    list(vocab.keys())[tid] for tid in seq_tensor.tolist() if tid != 0\n                ]\n                all_seqs.append(\" \".join(toks))\n    # overall acc (only for examples with seen labels)\n    acc = np.mean([p == t for p, t in zip(all_preds, all_labels)])\n    # SWA\n    weights = [count_shape_variety(s) for s in all_seqs]\n    swa = sum(w if p == t else 0 for w, p, t in zip(weights, all_preds, all_labels)) / (\n        sum(weights) or 1\n    )\n    # URA\n    seen_rules = set(train_labels)\n    idx_zs = [i for i, l in enumerate(all_labels) if l not in seen_rules]\n    ura = (\n        np.mean([all_preds[i] == all_labels[i] for i in idx_zs])\n        if idx_zs\n        else float(\"nan\")\n    )\n    return acc, swa, ura, all_preds, all_labels\n\n\n# ----------------------------------------------------------------------\n# 4. Training loops -----------------------------------------------------\nvariants = {\"baseline\": False, \"hybrid\": True}\nEPOCHS = 3  # short run\n\nfor name, use_feats in variants.items():\n    print(f\"\\n=== {name.upper()} (use_feats={use_feats}) ===\")\n    model = BaseModel(use_feats).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    edata = {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"val_loss\": [],\n            \"URA\": [],\n            \"SWA\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        tr_loss, tr_acc = run_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n        acc_dev, swa_dev, ura_dev, _, _ = evaluate(model, dev_loader)\n        edata[\"metrics\"][\"train_acc\"].append(tr_acc)\n        edata[\"metrics\"][\"val_acc\"].append(val_acc)\n        edata[\"metrics\"][\"val_loss\"].append(val_loss)\n        edata[\"metrics\"][\"URA\"].append(ura_dev)\n        edata[\"metrics\"][\"SWA\"].append(swa_dev)\n        edata[\"losses\"][\"train\"].append(tr_loss)\n        edata[\"losses\"][\"val\"].append(val_loss)\n        edata[\"timestamps\"].append(time.time())\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    # test evaluation\n    acc_test, swa_test, ura_test, preds, gts = evaluate(model, test_loader)\n    print(f\"TEST: Acc={acc_test:.4f} | SWA={swa_test:.4f} | URA={ura_test:.4f}\")\n    edata[\"predictions\"] = preds\n    edata[\"ground_truth\"] = gts\n    edata[\"metrics\"][\"test_acc\"] = acc_test\n    edata[\"metrics\"][\"test_SWA\"] = swa_test\n    edata[\"metrics\"][\"test_URA\"] = ura_test\n    experiment_data[\"SPR_BENCH\"][name] = edata\n\n# ----------------------------------------------------------------------\n# 5. Save ---------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, time, random, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# workspace\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# ------------------------------------------------------------------\n# dataset helpers\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# ------------------------------------------------------------------\n# load raw data\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------------------------------------------------------------\n# vocabularies (factorised)\ndef build_char_vocab(dataset, idx):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            ch = tok[idx] if len(tok) > idx else \"\"\n            if ch and ch not in vocab:\n                vocab[ch] = len(vocab)\n    return vocab\n\n\nshape_vocab = build_char_vocab(spr[\"train\"], 0)\ncolor_vocab = build_char_vocab(spr[\"train\"], 1)\nprint(\"shape_vocab:\", shape_vocab)\nprint(\"color_vocab:\", color_vocab)\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"#labels:\", num_labels)\n\n\n# ------------------------------------------------------------------\n# encoding utilities\ndef encode_token(token, svocab, cvocab):\n    shape_id = svocab.get(token[0], svocab[\"<unk>\"]) if token else svocab[\"<unk>\"]\n    color_id = (\n        cvocab.get(token[1], cvocab[\"<unk>\"]) if len(token) > 1 else cvocab[\"<unk>\"]\n    )\n    return shape_id, color_id\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, svocab, cvocab, label2id, with_labels=True):\n        self.raw_seqs = split[\"sequence\"]\n        self.tokens = [s.strip().split() for s in self.raw_seqs]\n        self.shape_ids = [\n            [encode_token(tok, svocab, cvocab)[0] for tok in seq] for seq in self.tokens\n        ]\n        self.color_ids = [\n            [encode_token(tok, svocab, cvocab)[1] for tok in seq] for seq in self.tokens\n        ]\n        self.with_labels = with_labels\n        if with_labels:\n            self.labels = [label2id[l] for l in split[\"label\"]]\n        else:\n            self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.shape_ids)\n\n    def __getitem__(self, idx):\n        item = {\n            \"shape_ids\": torch.tensor(self.shape_ids[idx], dtype=torch.long),\n            \"color_ids\": torch.tensor(self.color_ids[idx], dtype=torch.long),\n            \"sym\": torch.tensor(\n                [\n                    count_shape_variety(self.raw_seqs[idx]),\n                    count_color_variety(self.raw_seqs[idx]),\n                    len(self.shape_ids[idx]),\n                ],\n                dtype=torch.float,\n            ),\n            \"raw_seq\": self.raw_seqs[idx],\n        }\n        if self.with_labels:\n            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        else:\n            item[\"label_str\"] = self.labels[idx]\n        return item\n\n\ndef collate(batch):\n    shape = [b[\"shape_ids\"] for b in batch]\n    color = [b[\"color_ids\"] for b in batch]\n    lens = torch.tensor([len(s) for s in shape], dtype=torch.long)\n\n    shape_pad = nn.utils.rnn.pad_sequence(shape, batch_first=True, padding_value=0)\n    color_pad = nn.utils.rnn.pad_sequence(color, batch_first=True, padding_value=0)\n    sym = torch.stack([b[\"sym\"] for b in batch])\n\n    out = {\n        \"shape_ids\": shape_pad,\n        \"color_ids\": color_pad,\n        \"lengths\": lens,\n        \"sym\": sym,\n        \"raw_seq\": [b[\"raw_seq\"] for b in batch],\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], shape_vocab, color_vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], shape_vocab, color_vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], shape_vocab, color_vocab, label2id, False)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------------------------------------------------------\n# model\nclass NeuroSymbolicSPR(nn.Module):\n    def __init__(self, s_vocab, c_vocab, emb_dim=32, rnn_h=128, num_labels=2):\n        super().__init__()\n        self.s_emb = nn.Embedding(len(s_vocab), emb_dim, padding_idx=0)\n        self.c_emb = nn.Embedding(len(c_vocab), emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim * 2, rnn_h, batch_first=True, bidirectional=True)\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU()\n        )\n        self.out = nn.Linear(rnn_h * 2 + 16, num_labels)\n\n    def forward(self, shape_ids, color_ids, lens, sym):\n        s = self.s_emb(shape_ids)\n        c = self.c_emb(color_ids)\n        x = torch.cat([s, c], dim=-1)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.rnn(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        sym_feat = self.sym_mlp(sym)\n        logits = self.out(torch.cat([h_cat, sym_feat], dim=-1))\n        return logits\n\n\nmodel = NeuroSymbolicSPR(\n    shape_vocab, color_vocab, emb_dim=32, rnn_h=128, num_labels=num_labels\n).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------------------------------------------------------\n# training / evaluation\n@torch.no_grad()\ndef evaluate(loader, model):\n    model.eval()\n    preds, gts, seqs = [], [], []\n    for batch in loader:\n        # move tensors\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(\n            batch[\"shape_ids\"], batch[\"color_ids\"], batch[\"lengths\"], batch[\"sym\"]\n        )\n        pred_idx = logits.argmax(1).cpu().tolist()\n        preds.extend([id2label[i] for i in pred_idx])\n        if \"label_str\" in batch:\n            gts.extend(batch[\"label_str\"])\n        else:\n            gts.extend([id2label[i.item()] for i in batch[\"label\"]])\n        seqs.extend(batch[\"raw_seq\"])\n    acc = np.mean([p == t for p, t in zip(preds, gts)])\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    unseen_idx = [i for i, lbl in enumerate(gts) if lbl not in train_labels]\n    ura = (\n        np.mean([preds[i] == gts[i] for i in unseen_idx])\n        if unseen_idx\n        else float(\"nan\")\n    )\n    return acc, swa, ura, preds, gts\n\n\ndef run_epoch(loader, model, criterion, opt=None):\n    train_mode = opt is not None\n    model.train() if train_mode else model.eval()\n    tot_loss = tot_correct = tot = 0\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(\n            batch[\"shape_ids\"], batch[\"color_ids\"], batch[\"lengths\"], batch[\"sym\"]\n        )\n        labels = batch[\"label\"]\n        loss = criterion(logits, labels)\n        if train_mode:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        tot_loss += loss.item() * labels.size(0)\n        tot_correct += (logits.argmax(1) == labels).sum().item()\n        tot += labels.size(0)\n    return tot_loss / tot, tot_correct / tot\n\n\n# ------------------------------------------------------------------\n# experiment tracking\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_SWA\": [], \"URA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ------------------------------------------------------------------\n# train loop\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(train_loader, model, criterion, optimizer)\n    val_loss, val_acc = run_epoch(dev_loader, model, criterion)\n    _, val_swa, val_ura, _, _ = evaluate(dev_loader, model)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_SWA\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"URA\"].append(val_ura)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | val_SWA={val_swa:.4f} | URA={val_ura:.4f}\"\n    )\n\n# ------------------------------------------------------------------\n# final test evaluation\ntest_acc, test_swa, test_ura, preds, gts = evaluate(test_loader, model)\nprint(f\"\\nTEST  Acc={test_acc:.4f}  SWA={test_swa:.4f}  URA={test_ura:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\n\n# ------------------------------------------------------------------\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, time, random, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# I/O & device set-up ------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# Reproducibility ---------------------------------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# ------------------------------------------------------------------\n# Data helpers ------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len({tok[0] for tok in seq.strip().split() if tok})\n\n\ndef count_color_variety(seq: str) -> int:\n    return len({tok[1] for tok in seq.strip().split() if len(tok) > 1})\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------------------------------------------------------------\n# Load SPR_BENCH ----------------------------------------------------\nDATA_PATH = pathlib.Path(\n    \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\"\n)  # adapt if needed\nspr_raw = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr_raw.items()})\n\n# ------------------------------------------------------------------\n# Zero-shot split: withhold a subset of rule labels -----------------\nall_labels = sorted(set(spr_raw[\"train\"][\"label\"]))\n# at least 1 unseen label (20% of total rounded up)\nn_unseen = max(1, int(0.2 * len(all_labels)))\nunseen_labels = set(random.sample(all_labels, n_unseen))\nprint(\"Unseen labels (held out from training):\", unseen_labels)\n\n\ndef filter_by_labels(dataset, allowed_labels):\n    idx = [i for i, l in enumerate(dataset[\"label\"]) if l in allowed_labels]\n    return dataset.select(idx)\n\n\nseen_labels = [l for l in all_labels if l not in unseen_labels]\n\nspr = DatasetDict()\nspr[\"train\"] = filter_by_labels(spr_raw[\"train\"], seen_labels)\nspr[\"dev\"] = filter_by_labels(spr_raw[\"dev\"], seen_labels)\nspr[\"test\"] = spr_raw[\"test\"]  # keep full test set\n\nprint(\n    {k: len(v) for k, v in spr.items()},\n    \"| #seen:\",\n    len(seen_labels),\n    \"#unseen:\",\n    len(unseen_labels),\n)\n\n\n# ------------------------------------------------------------------\n# Vocabulary & label mapping ---------------------------------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nid2tok = {i: t for t, i in vocab.items()}\n\nlabel2id = {l: i for i, l in enumerate(all_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Vocab size:\", len(vocab), \"| #labels:\", num_labels)\n\n\n# ------------------------------------------------------------------\n# PyTorch Dataset ---------------------------------------------------\ndef encode_seq(seq):\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, training=True):\n        self.training = training\n        self.seqs = hf_split[\"sequence\"]\n        self.seq_enc = [encode_seq(s) for s in self.seqs]\n        if training:\n            self.labels = [label2id[l] for l in hf_split[\"label\"]]\n        else:\n            self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq_enc)\n\n    def __getitem__(self, idx):\n        seq_ids = torch.tensor(self.seq_enc[idx], dtype=torch.long)\n        sym_feat = torch.tensor(\n            [\n                count_shape_variety(self.seqs[idx]),\n                count_color_variety(self.seqs[idx]),\n                len(self.seq_enc[idx]),\n            ],\n            dtype=torch.float,\n        )\n        item = {\n            \"input\": seq_ids,\n            \"sym\": sym_feat,\n            \"length\": torch.tensor(len(seq_ids), dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n        if self.training:\n            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        else:\n            item[\"label_str\"] = self.labels[idx]\n        return item\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = torch.tensor([len(x) for x in xs], dtype=torch.long)\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    sym = torch.stack([b[\"sym\"] for b in batch])\n    out = {\n        \"input\": xs_pad,\n        \"length\": lens,\n        \"sym\": sym,\n        \"raw_seq\": [b[\"raw_seq\"] for b in batch],\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], True)\ntest_ds = SPRTorchDataset(spr[\"test\"], False)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------------------------------------------------------\n# Neuro-Symbolic model ---------------------------------------------\nclass NeuroSymbolicSPR(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU()\n        )\n        self.out = nn.Linear(hid_dim * 2 + 16, num_labels)\n\n    def forward(self, x, lens, sym):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.rnn(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        sym_feat = self.sym_mlp(sym)\n        z = torch.cat([h_cat, sym_feat], dim=-1)\n        return self.out(z)\n\n\nmodel = NeuroSymbolicSPR(len(vocab), 64, 128, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------------------------------------------------------\n# Train / eval utilities -------------------------------------------\ndef batch_to_device(batch):\n    return {\n        k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n        for k, v in batch.items()\n    }\n\n\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = batch_to_device(batch)\n            logits = model(batch[\"input\"], batch[\"length\"], batch[\"sym\"])\n            if train:\n                loss = criterion(logits, batch[\"label\"])\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            else:\n                loss = criterion(logits, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"input\"].size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == batch[\"label\"]).sum().item()\n            tot += batch[\"input\"].size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    preds, labels, seqs = [], [], []\n    for batch in loader:\n        batch_d = batch_to_device(batch)\n        logits = model(batch_d[\"input\"], batch_d[\"length\"], batch_d[\"sym\"])\n        p_ids = logits.argmax(1).cpu().tolist()\n        preds.extend([id2label[i] for i in p_ids])\n        if \"label_str\" in batch:\n            labels.extend(batch[\"label_str\"])\n        else:\n            labels.extend([id2label[i.item()] for i in batch[\"label\"]])\n        seqs.extend(batch[\"raw_seq\"])\n    acc = np.mean([p == t for p, t in zip(preds, labels)])\n    cwa = color_weighted_accuracy(seqs, labels, preds)\n    unseen_idx = [i for i, l in enumerate(labels) if l in unseen_labels]\n    ura = (\n        np.mean([preds[i] == labels[i] for i in unseen_idx])\n        if unseen_idx\n        else float(\"nan\")\n    )\n    return acc, cwa, ura\n\n\n# ------------------------------------------------------------------\n# Experiment container ---------------------------------------------\nexperiment_data = {\n    \"ZeroShotSPR\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"val_CWA\": [],\n            \"URA\": [],\n            \"val_loss\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ------------------------------------------------------------------\n# Training loop -----------------------------------------------------\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n    val_loss, val_acc = run_epoch(dev_loader, train=False)\n    _, val_cwa, val_ura = evaluate(dev_loader)\n\n    experiment_data[\"ZeroShotSPR\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"ZeroShotSPR\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"ZeroShotSPR\"][\"metrics\"][\"val_CWA\"].append(val_cwa)\n    experiment_data[\"ZeroShotSPR\"][\"metrics\"][\"URA\"].append(val_ura)\n    experiment_data[\"ZeroShotSPR\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"ZeroShotSPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"ZeroShotSPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"ZeroShotSPR\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | CWA={val_cwa:.4f} | URA={val_ura:.4f}\"\n    )\n\n# ------------------------------------------------------------------\n# Final test evaluation --------------------------------------------\ntest_acc, test_cwa, test_ura = evaluate(test_loader)\nprint(f\"\\nTEST  Acc={test_acc:.4f}  CWA={test_cwa:.4f}  URA={test_ura:.4f}\")\n\n# store final predictions / ground truth\nwith torch.no_grad():\n    preds, labels = [], []\n    for batch in test_loader:\n        batch_d = batch_to_device(batch)\n        logits = model(batch_d[\"input\"], batch_d[\"length\"], batch_d[\"sym\"])\n        p = logits.argmax(1).cpu().tolist()\n        preds.extend([id2label[idx] for idx in p])\n        labels.extend(batch[\"label_str\"])\nexperiment_data[\"ZeroShotSPR\"][\"predictions\"] = preds\nexperiment_data[\"ZeroShotSPR\"][\"ground_truth\"] = labels\n\n# ------------------------------------------------------------------\n# Save artefacts ----------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, time, random, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom typing import Dict, List\n\n# ----------------------------------------------------------------------\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# container to save everything\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"SWA\": {\"val\": []},\n        \"URA\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ----------------------------------------------------------------------\n# SPR helpers -----------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ----------------------------------------------------------------------\n# data path -------------------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ----------------------------------------------------------------------\n# vocab + labels --------------------------------------------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nv2i = vocab\ni2v = {i: t for t, i in v2i.items()}\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nseen_rules = set(train_labels)\n\n\n# ----------------------------------------------------------------------\n# Symbolic feature stats ------------------------------------------------\ndef symbolic_vector(seq: str):\n    return np.array(\n        [count_shape_variety(seq), count_color_variety(seq), len(seq.strip().split())],\n        dtype=np.float32,\n    )\n\n\ntrain_sym = np.stack([symbolic_vector(s) for s in spr[\"train\"][\"sequence\"]])\nsym_mean = train_sym.mean(0)\nsym_std = train_sym.std(0) + 1e-6\n\n\n# ----------------------------------------------------------------------\n# encode ----------------------------------------------------------------\ndef encode_seq(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\n# Torch Dataset ---------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, vocab, train_mode=True):\n        self.seqs = [encode_seq(s, vocab) for s in split[\"sequence\"]]\n        self.sym = [symbolic_vector(s) for s in split[\"sequence\"]]\n        self.labels = split[\"label\"]\n        self.train_mode = train_mode\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"input\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"sym\": torch.tensor(self.sym[idx], dtype=torch.float32),\n            \"seq_raw\": \" \".join([i2v[i] for i in self.seqs[idx]]),\n        }\n        if self.train_mode:\n            sample[\"label\"] = torch.tensor(label2id[self.labels[idx]], dtype=torch.long)\n        else:\n            sample[\"label_str\"] = self.labels[idx]\n        return sample\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    out = {\n        \"input\": xs_pad,\n        \"lengths\": torch.tensor(lens, dtype=torch.long),\n        \"sym\": torch.stack([b[\"sym\"] for b in batch]),\n        \"seq_raw\": [b[\"seq_raw\"] for b in batch],\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, False)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------------------------------------------------------\n# Model -----------------------------------------------------------------\nclass HybridSPR(nn.Module):\n    def __init__(\n        self, vocab_size, d_model, nhead, num_layers, sym_dim, num_labels, max_len=200\n    ):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_emb = nn.Embedding(max_len, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.sym_fc = nn.Sequential(nn.Linear(sym_dim, d_model), nn.ReLU())\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, x, lens, sym):\n        B, L = x.size()\n        pos = torch.arange(0, L, device=x.device).unsqueeze(0).expand(B, L)\n        h = self.token_emb(x) + self.pos_emb(pos)\n        h = self.transformer(h, src_key_padding_mask=(x == 0))\n        # mean pooling over valid tokens\n        mask = (x != 0).unsqueeze(-1)\n        h_sum = (h * mask).sum(1)\n        h_len = mask.sum(1).clamp(min=1)\n        h_mean = h_sum / h_len\n        h_sym = self.sym_fc(sym)\n        h_comb = h_mean + h_sym\n        return self.cls(h_comb)\n\n\n# ----------------------------------------------------------------------\nd_model = 64\nnhead = 4\nlayers = 2\nmodel = HybridSPR(len(vocab), d_model, nhead, layers, 3, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ----------------------------------------------------------------------\ndef run_epoch(model, loader, train=False):\n    tot_loss = tot_ok = tot = 0\n    if train:\n        model.train()\n    else:\n        model.eval()\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            sym = ((batch[\"sym\"] - sym_mean) / sym_std).to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(inp, lens, sym)\n            loss = criterion(logits, lbl)\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * inp.size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == lbl).sum().item()\n            tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate(loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_seqs = []\n    with torch.no_grad():\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            sym = ((batch[\"sym\"] - sym_mean) / sym_std).to(device)\n            logits = model(inp, lens, sym)\n            preds = logits.argmax(1).cpu().tolist()\n            all_preds.extend([id2label.get(p, \"UNK\") for p in preds])\n            all_labels.extend(batch[\"label_str\"])\n            all_seqs.extend(batch[\"seq_raw\"])\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    ura_indices = [i for i, lbl in enumerate(all_labels) if lbl not in seen_rules]\n    ura = (\n        float(\"nan\")\n        if len(ura_indices) == 0\n        else np.mean([all_preds[i] == all_labels[i] for i in ura_indices])\n    )\n    return swa, ura, all_preds, all_labels\n\n\n# ----------------------------------------------------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = run_epoch(model, train_loader, train=True)\n    val_loss, val_acc = run_epoch(model, dev_loader, train=False)\n    swa, ura, _, _ = evaluate(dev_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append({\"acc\": tr_acc})\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"acc\": val_acc})\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"SWA\"][\"val\"].append(swa)\n    experiment_data[\"SPR_BENCH\"][\"URA\"][\"val\"].append(ura)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_acc={val_acc:.4f} | SWA={swa:.4f} | URA={ura}\"\n    )\n\n# ----------------------------------------------------------------------\n# Final test evaluation -------------------------------------------------\nswa_test, ura_test, preds_gt, labels_gt = evaluate(test_loader)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_gt\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_gt\nexperiment_data[\"SPR_BENCH\"][\"SWA\"][\"test\"] = swa_test\nexperiment_data[\"SPR_BENCH\"][\"URA\"][\"test\"] = ura_test\nprint(f\"\\nTEST --> SWA={swa_test:.4f} | URA={ura_test}\")\n\n# save all\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Data saved to {working_dir}/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# hyper_dim_tuning.py\nimport os, pathlib, time, random, json, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom typing import List, Dict\n\n# ----------------------------------------------------------------------\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ----------------------------------------------------------------------\n# experiment container --------------------------------------------------\nexperiment_data = {\"hidden_dim_tuning\": {\"SPR_BENCH\": {}}}\n# ----------------------------------------------------------------------\n# working dir / save path ----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n# ----------------------------------------------------------------------\n# device ----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------------------------------------------------------\n# util functions --------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ----------------------------------------------------------------------\n# load data -------------------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ----------------------------------------------------------------------\n# vocab + label mapping -------------------------------------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_seq(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"# seen rule labels: {num_labels}\")\n\n\n# ----------------------------------------------------------------------\n# Torch dataset ---------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, vocab, label2id, train_mode=True):\n        self.seq_enc = [encode_seq(s, vocab) for s in split[\"sequence\"]]\n        self.labels = split[\"label\"]\n        self.train_mode = train_mode\n        self.label2id = label2id\n\n    def __len__(self):\n        return len(self.seq_enc)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.seq_enc[idx], dtype=torch.long)\n        if self.train_mode:\n            y = torch.tensor(self.label2id[self.labels[idx]], dtype=torch.long)\n            return {\"input\": x, \"label\": y}\n        else:\n            return {\"input\": x, \"label_str\": self.labels[idx]}\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    out = {\"input\": xs_pad, \"lengths\": torch.tensor(lens, dtype=torch.long)}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\n# create loaders (same across experiments)\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, label2id, False)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------------------------------------------------------\n# Model ----------------------------------------------------------------\nclass SimpleSPRModel(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(hidden_dim * 2, num_labels)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        return self.lin(h_cat)\n\n\n# ----------------------------------------------------------------------\n# training / eval helpers ----------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(inp, lens)\n            loss = criterion(logits, lbl)\n            if train:\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n            tot_loss += loss.item() * inp.size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == lbl).sum().item()\n            tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate_test(model):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            logits = model(inp, lens)\n            preds = logits.argmax(1).cpu().tolist()\n            label_strs = batch[\"label_str\"]\n            all_preds.extend([id2label.get(p, \"UNK\") for p in preds])\n            all_labels.extend(label_strs)\n            all_seqs.extend(\n                [\n                    \" \".join(\n                        [list(vocab.keys())[tok] for tok in seq.tolist() if tok != 0]\n                    )\n                    for seq in batch[\"input\"]\n                ]\n            )\n    overall_acc = np.mean([p == t for p, t in zip(all_preds, all_labels)])\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    seen_rules = set(train_labels)\n    zs_idx = [i for i, lbl in enumerate(all_labels) if lbl not in seen_rules]\n    zs_acc = (\n        np.mean([all_preds[i] == all_labels[i] for i in zs_idx])\n        if zs_idx\n        else float(\"nan\")\n    )\n    return overall_acc, swa, cwa, zs_acc, all_preds, all_labels\n\n\n# ----------------------------------------------------------------------\n# hyperparameter search -------------------------------------------------\nhidden_dims = [64, 128, 256, 512]\nEPOCHS = 5\nfor hd in hidden_dims:\n    print(f\"\\n=== Training with hidden_dim={hd} ===\")\n    # init model/optim/criterion\n    model = SimpleSPRModel(len(vocab), 64, hd, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # containers\n    mdata = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_loss\": [], \"ZSRTA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    # train loop\n    for epoch in range(1, EPOCHS + 1):\n        tr_loss, tr_acc = run_epoch(model, train_loader, criterion, opt)\n        val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n        mdata[\"metrics\"][\"train_acc\"].append(tr_acc)\n        mdata[\"metrics\"][\"val_acc\"].append(val_acc)\n        mdata[\"metrics\"][\"val_loss\"].append(val_loss)\n        mdata[\"losses\"][\"train\"].append(tr_loss)\n        mdata[\"losses\"][\"val\"].append(val_loss)\n        mdata[\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {epoch}: train_acc={tr_acc:.4f} val_acc={val_acc:.4f} val_loss={val_loss:.4f}\"\n        )\n    # final evaluation\n    overall_acc, swa, cwa, zs_acc, all_preds, all_labels = evaluate_test(model)\n    mdata[\"metrics\"][\"ZSRTA\"].append(zs_acc)\n    mdata[\"predictions\"] = all_preds\n    mdata[\"ground_truth\"] = all_labels\n    print(\n        f\"TEST Acc={overall_acc:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | ZSRTA={zs_acc:.4f}\"\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][f\"hidden_{hd}\"] = mdata\n\n# ----------------------------------------------------------------------\n# save ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# hyper_dim_tuning.py\nimport os, pathlib, time, random, json, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom typing import List, Dict\n\n# ----------------------------------------------------------------------\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ----------------------------------------------------------------------\n# experiment container --------------------------------------------------\nexperiment_data = {\"hidden_dim_tuning\": {\"SPR_BENCH\": {}}}\n# ----------------------------------------------------------------------\n# working dir / save path ----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n# ----------------------------------------------------------------------\n# device ----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------------------------------------------------------\n# util functions --------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ----------------------------------------------------------------------\n# load data -------------------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ----------------------------------------------------------------------\n# vocab + label mapping -------------------------------------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_seq(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"# seen rule labels: {num_labels}\")\n\n\n# ----------------------------------------------------------------------\n# Torch dataset ---------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, vocab, label2id, train_mode=True):\n        self.seq_enc = [encode_seq(s, vocab) for s in split[\"sequence\"]]\n        self.labels = split[\"label\"]\n        self.train_mode = train_mode\n        self.label2id = label2id\n\n    def __len__(self):\n        return len(self.seq_enc)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.seq_enc[idx], dtype=torch.long)\n        if self.train_mode:\n            y = torch.tensor(self.label2id[self.labels[idx]], dtype=torch.long)\n            return {\"input\": x, \"label\": y}\n        else:\n            return {\"input\": x, \"label_str\": self.labels[idx]}\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    out = {\"input\": xs_pad, \"lengths\": torch.tensor(lens, dtype=torch.long)}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\n# create loaders (same across experiments)\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, label2id, False)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------------------------------------------------------\n# Model ----------------------------------------------------------------\nclass SimpleSPRModel(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(hidden_dim * 2, num_labels)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        return self.lin(h_cat)\n\n\n# ----------------------------------------------------------------------\n# training / eval helpers ----------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(inp, lens)\n            loss = criterion(logits, lbl)\n            if train:\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n            tot_loss += loss.item() * inp.size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == lbl).sum().item()\n            tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate_test(model):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            logits = model(inp, lens)\n            preds = logits.argmax(1).cpu().tolist()\n            label_strs = batch[\"label_str\"]\n            all_preds.extend([id2label.get(p, \"UNK\") for p in preds])\n            all_labels.extend(label_strs)\n            all_seqs.extend(\n                [\n                    \" \".join(\n                        [list(vocab.keys())[tok] for tok in seq.tolist() if tok != 0]\n                    )\n                    for seq in batch[\"input\"]\n                ]\n            )\n    overall_acc = np.mean([p == t for p, t in zip(all_preds, all_labels)])\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    seen_rules = set(train_labels)\n    zs_idx = [i for i, lbl in enumerate(all_labels) if lbl not in seen_rules]\n    zs_acc = (\n        np.mean([all_preds[i] == all_labels[i] for i in zs_idx])\n        if zs_idx\n        else float(\"nan\")\n    )\n    return overall_acc, swa, cwa, zs_acc, all_preds, all_labels\n\n\n# ----------------------------------------------------------------------\n# hyperparameter search -------------------------------------------------\nhidden_dims = [64, 128, 256, 512]\nEPOCHS = 5\nfor hd in hidden_dims:\n    print(f\"\\n=== Training with hidden_dim={hd} ===\")\n    # init model/optim/criterion\n    model = SimpleSPRModel(len(vocab), 64, hd, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # containers\n    mdata = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_loss\": [], \"ZSRTA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    # train loop\n    for epoch in range(1, EPOCHS + 1):\n        tr_loss, tr_acc = run_epoch(model, train_loader, criterion, opt)\n        val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n        mdata[\"metrics\"][\"train_acc\"].append(tr_acc)\n        mdata[\"metrics\"][\"val_acc\"].append(val_acc)\n        mdata[\"metrics\"][\"val_loss\"].append(val_loss)\n        mdata[\"losses\"][\"train\"].append(tr_loss)\n        mdata[\"losses\"][\"val\"].append(val_loss)\n        mdata[\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {epoch}: train_acc={tr_acc:.4f} val_acc={val_acc:.4f} val_loss={val_loss:.4f}\"\n        )\n    # final evaluation\n    overall_acc, swa, cwa, zs_acc, all_preds, all_labels = evaluate_test(model)\n    mdata[\"metrics\"][\"ZSRTA\"].append(zs_acc)\n    mdata[\"predictions\"] = all_preds\n    mdata[\"ground_truth\"] = all_labels\n    print(\n        f\"TEST Acc={overall_acc:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | ZSRTA={zs_acc:.4f}\"\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][f\"hidden_{hd}\"] = mdata\n\n# ----------------------------------------------------------------------\n# save ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# hyper_dim_tuning.py\nimport os, pathlib, time, random, json, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom typing import List, Dict\n\n# ----------------------------------------------------------------------\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# ----------------------------------------------------------------------\n# experiment container --------------------------------------------------\nexperiment_data = {\"hidden_dim_tuning\": {\"SPR_BENCH\": {}}}\n# ----------------------------------------------------------------------\n# working dir / save path ----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n# ----------------------------------------------------------------------\n# device ----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------------------------------------------------------\n# util functions --------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ----------------------------------------------------------------------\n# load data -------------------------------------------------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ----------------------------------------------------------------------\n# vocab + label mapping -------------------------------------------------\ndef build_vocab(dataset) -> Dict[str, int]:\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n\ndef encode_seq(seq: str, vocab: Dict[str, int]) -> List[int]:\n    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n\ntrain_labels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(train_labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"# seen rule labels: {num_labels}\")\n\n\n# ----------------------------------------------------------------------\n# Torch dataset ---------------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split, vocab, label2id, train_mode=True):\n        self.seq_enc = [encode_seq(s, vocab) for s in split[\"sequence\"]]\n        self.labels = split[\"label\"]\n        self.train_mode = train_mode\n        self.label2id = label2id\n\n    def __len__(self):\n        return len(self.seq_enc)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.seq_enc[idx], dtype=torch.long)\n        if self.train_mode:\n            y = torch.tensor(self.label2id[self.labels[idx]], dtype=torch.long)\n            return {\"input\": x, \"label\": y}\n        else:\n            return {\"input\": x, \"label_str\": self.labels[idx]}\n\n\ndef collate(batch):\n    xs = [b[\"input\"] for b in batch]\n    lens = [len(x) for x in xs]\n    xs_pad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n    out = {\"input\": xs_pad, \"lengths\": torch.tensor(lens, dtype=torch.long)}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    else:\n        out[\"label_str\"] = [b[\"label_str\"] for b in batch]\n    return out\n\n\n# create loaders (same across experiments)\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, label2id, True)\ndev_ds = SPRTorchDataset(spr[\"dev\"], vocab, label2id, True)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, label2id, False)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ----------------------------------------------------------------------\n# Model ----------------------------------------------------------------\nclass SimpleSPRModel(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_labels):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(hidden_dim * 2, num_labels)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h_cat = torch.cat([h[0], h[1]], dim=-1)\n        return self.lin(h_cat)\n\n\n# ----------------------------------------------------------------------\n# training / eval helpers ----------------------------------------------\ndef run_epoch(model, loader, criterion, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_ok = tot = 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(inp, lens)\n            loss = criterion(logits, lbl)\n            if train:\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n            tot_loss += loss.item() * inp.size(0)\n            preds = logits.argmax(1)\n            tot_ok += (preds == lbl).sum().item()\n            tot += inp.size(0)\n    return tot_loss / tot, tot_ok / tot\n\n\ndef evaluate_test(model):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            inp = batch[\"input\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            logits = model(inp, lens)\n            preds = logits.argmax(1).cpu().tolist()\n            label_strs = batch[\"label_str\"]\n            all_preds.extend([id2label.get(p, \"UNK\") for p in preds])\n            all_labels.extend(label_strs)\n            all_seqs.extend(\n                [\n                    \" \".join(\n                        [list(vocab.keys())[tok] for tok in seq.tolist() if tok != 0]\n                    )\n                    for seq in batch[\"input\"]\n                ]\n            )\n    overall_acc = np.mean([p == t for p, t in zip(all_preds, all_labels)])\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    seen_rules = set(train_labels)\n    zs_idx = [i for i, lbl in enumerate(all_labels) if lbl not in seen_rules]\n    zs_acc = (\n        np.mean([all_preds[i] == all_labels[i] for i in zs_idx])\n        if zs_idx\n        else float(\"nan\")\n    )\n    return overall_acc, swa, cwa, zs_acc, all_preds, all_labels\n\n\n# ----------------------------------------------------------------------\n# hyperparameter search -------------------------------------------------\nhidden_dims = [64, 128, 256, 512]\nEPOCHS = 5\nfor hd in hidden_dims:\n    print(f\"\\n=== Training with hidden_dim={hd} ===\")\n    # init model/optim/criterion\n    model = SimpleSPRModel(len(vocab), 64, hd, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # containers\n    mdata = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_loss\": [], \"ZSRTA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    # train loop\n    for epoch in range(1, EPOCHS + 1):\n        tr_loss, tr_acc = run_epoch(model, train_loader, criterion, opt)\n        val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n        mdata[\"metrics\"][\"train_acc\"].append(tr_acc)\n        mdata[\"metrics\"][\"val_acc\"].append(val_acc)\n        mdata[\"metrics\"][\"val_loss\"].append(val_loss)\n        mdata[\"losses\"][\"train\"].append(tr_loss)\n        mdata[\"losses\"][\"val\"].append(val_loss)\n        mdata[\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {epoch}: train_acc={tr_acc:.4f} val_acc={val_acc:.4f} val_loss={val_loss:.4f}\"\n        )\n    # final evaluation\n    overall_acc, swa, cwa, zs_acc, all_preds, all_labels = evaluate_test(model)\n    mdata[\"metrics\"][\"ZSRTA\"].append(zs_acc)\n    mdata[\"predictions\"] = all_preds\n    mdata[\"ground_truth\"] = all_labels\n    print(\n        f\"TEST Acc={overall_acc:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | ZSRTA={zs_acc:.4f}\"\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][f\"hidden_{hd}\"] = mdata\n\n# ----------------------------------------------------------------------\n# save ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {working_dir}/experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Vocab size: 18', '\\n', '# seen rule labels: 2', '\\n', '\\n=== Training\nwith hidden_dim=64 ===', '\\n', 'Epoch 1: train_acc=0.9258 val_acc=0.9614\nval_loss=0.1383', '\\n', 'Epoch 2: train_acc=0.9757 val_acc=0.9854\nval_loss=0.0432', '\\n', 'Epoch 3: train_acc=0.9917 val_acc=0.9952\nval_loss=0.0208', '\\n', 'Epoch 4: train_acc=0.9970 val_acc=0.9966\nval_loss=0.0118', '\\n', 'Epoch 5: train_acc=0.9982 val_acc=0.9986\nval_loss=0.0071', '\\n', 'TEST Acc=0.7014 | SWA=0.6538 | CWA=0.7020 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=128 ===', '\\n', 'Epoch 1: train_acc=0.9344\nval_acc=0.9622 val_loss=0.1232', '\\n', 'Epoch 2: train_acc=0.9768 val_acc=0.9886\nval_loss=0.0360', '\\n', 'Epoch 3: train_acc=0.9939 val_acc=0.9968\nval_loss=0.0181', '\\n', 'Epoch 4: train_acc=0.9981 val_acc=0.9980\nval_loss=0.0076', '\\n', 'Epoch 5: train_acc=0.9993 val_acc=0.9992\nval_loss=0.0039', '\\n', 'TEST Acc=0.7003 | SWA=0.6527 | CWA=0.7009 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=256 ===', '\\n', 'Epoch 1: train_acc=0.9455\nval_acc=0.9800 val_loss=0.0610', '\\n', 'Epoch 2: train_acc=0.9898 val_acc=0.9950\nval_loss=0.0159', '\\n', 'Epoch 3: train_acc=0.9968 val_acc=0.9934\nval_loss=0.0215', '\\n', 'Epoch 4: train_acc=0.9992 val_acc=1.0000\nval_loss=0.0011', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0007', '\\n', 'TEST Acc=0.7001 | SWA=0.6523 | CWA=0.7006 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=512 ===', '\\n', 'Epoch 1: train_acc=0.9528\nval_acc=0.9832 val_loss=0.0609', '\\n', 'Epoch 2: train_acc=0.9882 val_acc=0.9910\nval_loss=0.0336', '\\n', 'Epoch 3: train_acc=0.9950 val_acc=1.0000\nval_loss=0.0026', '\\n', 'Epoch 4: train_acc=0.9999 val_acc=0.9998\nval_loss=0.0009', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0005', '\\n', 'TEST Acc=0.7000 | SWA=0.6522 | CWA=0.7005 | ZSRTA=nan',\n'\\n', '\\nSaved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n44 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 449150.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 513454.12\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 595358.98\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize=18, shapes=4, colors=4', '\\n', 'Epoch 1: train_acc=0.9267 | val_acc=0.9682\n| val_loss=0.0953 | URA=nan', '\\n', 'Epoch 2: train_acc=0.9806 | val_acc=0.9814\n| val_loss=0.0502 | URA=nan', '\\n', 'Epoch 3: train_acc=0.9923 | val_acc=0.9968\n| val_loss=0.0221 | URA=nan', '\\n', 'Epoch 4: train_acc=0.9970 | val_acc=0.9982\n| val_loss=0.0096 | URA=nan', '\\n', 'Epoch 5: train_acc=0.9981 | val_acc=0.9994\n| val_loss=0.0047 | URA=nan', '\\n', 'Epoch 6: train_acc=1.0000 | val_acc=0.9982\n| val_loss=0.0040 | URA=nan', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered\nin scalar divide\\n  ret = ret.dtype.type(ret / rcount)\\n', '\\nTEST:\noverall_acc=0.7002 | SWA=0.6525 | URA=nan', '\\n', '\\nSaved experiment data to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-07-27_23-49-\n14_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 60, in <module>\\n    assert data_root.exists(), f\"Folder\n{data_root} with SPR_BENCH *.csv files not found.\"\\n\n^^^^^^^^^^^^^^^^^^\\nAssertionError: Folder SPR_BENCH with SPR_BENCH *.csv files\nnot found.\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 415322.86\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 472970.68\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 572202.83\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'vocab\nsize:', ' ', '18', '\\n', '#train labels:', ' ', '2', '\\n', 'Epoch 1:\nvalidation_loss = 0.0980 | val_acc=0.9722 | val_SWA=0.9722 | URA=nan', '\\n',\n'Epoch 2: validation_loss = 0.0300 | val_acc=0.9902 | val_SWA=0.9898 | URA=nan',\n'\\n', 'Epoch 3: validation_loss = 0.0270 | val_acc=0.9936 | val_SWA=0.9928 |\nURA=nan', '\\n', 'Epoch 4: validation_loss = 0.0043 | val_acc=0.9992 |\nval_SWA=0.9991 | URA=nan', '\\n', 'Epoch 5: validation_loss = 0.0023 |\nval_acc=0.9996 | val_SWA=0.9996 | URA=nan', '\\n', 'Epoch 6: validation_loss =\n0.0020 | val_acc=0.9998 | val_SWA=0.9998 | URA=nan', '\\n', '\\nTEST  Acc=0.7000\nSWA=0.6523  URA=nan', '\\n', '\\nSaved experiment data to /home/zxl240011/AI-Scien\ntist-v2/experiments/2025-07-27_23-49-\n14_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 18 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 430313.02\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 451758.22\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 625987.49\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Epoch\n1: validation_loss = 0.2426 | val_acc = 0.9194', '\\n', 'Epoch 2: validation_loss\n= 0.1879 | val_acc = 0.9464', '\\n', 'Epoch 3: validation_loss = 0.1706 | val_acc\n= 0.9492', '\\n', 'Epoch 4: validation_loss = 0.1702 | val_acc = 0.9492', '\\n',\n'Epoch 5: validation_loss = 0.1679 | val_acc = 0.9492', '\\n', 'Epoch 6:\nvalidation_loss = 0.1687 | val_acc = 0.9492', '\\n', 'Epoch 7: validation_loss =\n0.1748 | val_acc = 0.9490', '\\n', 'Epoch 8: validation_loss = 0.1685 | val_acc =\n0.9492', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '\\nTEST  Acc=0.6951 | SWA=0.6500 |\nURA=nan', '\\n', 'Saved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/experiment_data.npy', '\\n', 'Execution time:\n16 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 48, in <module>\\n    raise FileNotFoundError(\"SPR_BENCH\nfolder not found \u2013 set $SPR_PATH\")\\nFileNotFoundError: SPR_BENCH folder not\nfound \u2013 set $SPR_PATH\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'shape_vocab:', ' ', \"{'<pad>': 0, '<unk>': 1, '\u25cf': 2, '\u25a0': 3, '\u25b2': 4,\n'\u25c6': 5}\", '\\n', 'color_vocab:', ' ', \"{'<pad>': 0, '<unk>': 1, 'y': 2, 'g': 3,\n'r': 4, 'b': 5}\", '\\n', '#labels:', ' ', '2', '\\n', 'Epoch 1: val_loss=0.0572 |\nval_acc=0.9886 | val_SWA=0.9885 | URA=nan', '\\n', 'Epoch 2: val_loss=0.0156 |\nval_acc=0.9974 | val_SWA=0.9972 | URA=nan', '\\n', 'Epoch 3: val_loss=0.0038 |\nval_acc=0.9998 | val_SWA=0.9998 | URA=nan', '\\n', 'Epoch 4: val_loss=0.0018 |\nval_acc=0.9998 | val_SWA=0.9998 | URA=nan', '\\n', 'Epoch 5: val_loss=0.0011 |\nval_acc=0.9998 | val_SWA=0.9998 | URA=nan', '\\n', 'Epoch 6: val_loss=0.0010 |\nval_acc=0.9998 | val_SWA=0.9998 | URA=nan', '\\n', '\\nTEST  Acc=0.7000\nSWA=0.6522  URA=nan', '\\n', '\\nSaved experiment data to /home/zxl240011/AI-Scien\ntist-v2/experiments/2025-07-27_23-49-\n14_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Unseen labels (held out from training):', ' ', '{0}', '\\n', \"{'train':\n10000, 'dev': 2500, 'test': 10000}\", ' ', '| #seen:', ' ', '1', ' ', '#unseen:',\n' ', '1', '\\n', 'Vocab size:', ' ', '18', ' ', '| #labels:', ' ', '2', '\\n',\n'Epoch 1: val_loss=0.0000 | val_acc=1.0000 | CWA=1.0000 | URA=nan', '\\n', 'Epoch\n2: val_loss=0.0000 | val_acc=1.0000 | CWA=1.0000 | URA=nan', '\\n', 'Epoch 3:\nval_loss=0.0000 | val_acc=1.0000 | CWA=1.0000 | URA=nan', '\\n', 'Epoch 4:\nval_loss=0.0000 | val_acc=1.0000 | CWA=1.0000 | URA=nan', '\\n', 'Epoch 5:\nval_loss=0.0000 | val_acc=1.0000 | CWA=1.0000 | URA=nan', '\\n', 'Epoch 6:\nval_loss=0.0000 | val_acc=1.0000 | CWA=1.0000 | URA=nan', '\\n', '\\nTEST\nAcc=0.4348  CWA=0.4339  URA=0.0000', '\\n', '\\nSaved experiment data to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-07-27_23-49-\n14_neural_symbolic_zero_shot_spr_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Traceback (most recent call\nlast):\\n  File \"runfile.py\", line 262, in <module>\\n    swa, ura, _, _ =\nevaluate(dev_loader)\\n                     ^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 245, in evaluate\\n    all_labels.extend(batch[\"label_str\"])\\n\n~~~~~^^^^^^^^^^^^^\\nKeyError: \\'label_str\\'\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Vocab size: 18', '\\n', '# seen rule labels: 2', '\\n', '\\n=== Training\nwith hidden_dim=64 ===', '\\n', 'Epoch 1: train_acc=0.9258 val_acc=0.9614\nval_loss=0.1383', '\\n', 'Epoch 2: train_acc=0.9757 val_acc=0.9854\nval_loss=0.0432', '\\n', 'Epoch 3: train_acc=0.9917 val_acc=0.9952\nval_loss=0.0208', '\\n', 'Epoch 4: train_acc=0.9970 val_acc=0.9966\nval_loss=0.0118', '\\n', 'Epoch 5: train_acc=0.9982 val_acc=0.9986\nval_loss=0.0071', '\\n', 'TEST Acc=0.7014 | SWA=0.6538 | CWA=0.7020 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=128 ===', '\\n', 'Epoch 1: train_acc=0.9344\nval_acc=0.9622 val_loss=0.1232', '\\n', 'Epoch 2: train_acc=0.9768 val_acc=0.9886\nval_loss=0.0360', '\\n', 'Epoch 3: train_acc=0.9939 val_acc=0.9968\nval_loss=0.0181', '\\n', 'Epoch 4: train_acc=0.9981 val_acc=0.9980\nval_loss=0.0076', '\\n', 'Epoch 5: train_acc=0.9993 val_acc=0.9992\nval_loss=0.0039', '\\n', 'TEST Acc=0.7003 | SWA=0.6527 | CWA=0.7009 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=256 ===', '\\n', 'Epoch 1: train_acc=0.9455\nval_acc=0.9800 val_loss=0.0610', '\\n', 'Epoch 2: train_acc=0.9898 val_acc=0.9950\nval_loss=0.0159', '\\n', 'Epoch 3: train_acc=0.9968 val_acc=0.9934\nval_loss=0.0215', '\\n', 'Epoch 4: train_acc=0.9992 val_acc=1.0000\nval_loss=0.0011', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0007', '\\n', 'TEST Acc=0.7001 | SWA=0.6523 | CWA=0.7006 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=512 ===', '\\n', 'Epoch 1: train_acc=0.9528\nval_acc=0.9832 val_loss=0.0609', '\\n', 'Epoch 2: train_acc=0.9882 val_acc=0.9910\nval_loss=0.0336', '\\n', 'Epoch 3: train_acc=0.9950 val_acc=1.0000\nval_loss=0.0026', '\\n', 'Epoch 4: train_acc=0.9999 val_acc=0.9998\nval_loss=0.0009', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0005', '\\n', 'TEST Acc=0.7000 | SWA=0.6522 | CWA=0.7005 | ZSRTA=nan',\n'\\n', '\\nSaved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/experiment_data.npy', '\\n', 'Execution time:\n16 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 242976.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 656406.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 597453.67\nexamples/s]', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\", '\\n', 'Vocab\nsize: 18', '\\n', '# seen rule labels: 2', '\\n', '\\n=== Training with\nhidden_dim=64 ===', '\\n', 'Epoch 1: train_acc=0.9258 val_acc=0.9614\nval_loss=0.1383', '\\n', 'Epoch 2: train_acc=0.9757 val_acc=0.9854\nval_loss=0.0432', '\\n', 'Epoch 3: train_acc=0.9917 val_acc=0.9952\nval_loss=0.0208', '\\n', 'Epoch 4: train_acc=0.9970 val_acc=0.9966\nval_loss=0.0118', '\\n', 'Epoch 5: train_acc=0.9982 val_acc=0.9986\nval_loss=0.0071', '\\n', 'TEST Acc=0.7014 | SWA=0.6538 | CWA=0.7020 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=128 ===', '\\n', 'Epoch 1: train_acc=0.9344\nval_acc=0.9622 val_loss=0.1232', '\\n', 'Epoch 2: train_acc=0.9768 val_acc=0.9886\nval_loss=0.0360', '\\n', 'Epoch 3: train_acc=0.9939 val_acc=0.9968\nval_loss=0.0181', '\\n', 'Epoch 4: train_acc=0.9981 val_acc=0.9980\nval_loss=0.0076', '\\n', 'Epoch 5: train_acc=0.9993 val_acc=0.9992\nval_loss=0.0039', '\\n', 'TEST Acc=0.7003 | SWA=0.6527 | CWA=0.7009 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=256 ===', '\\n', 'Epoch 1: train_acc=0.9455\nval_acc=0.9800 val_loss=0.0610', '\\n', 'Epoch 2: train_acc=0.9898 val_acc=0.9950\nval_loss=0.0159', '\\n', 'Epoch 3: train_acc=0.9968 val_acc=0.9934\nval_loss=0.0215', '\\n', 'Epoch 4: train_acc=0.9992 val_acc=1.0000\nval_loss=0.0011', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0007', '\\n', 'TEST Acc=0.7001 | SWA=0.6523 | CWA=0.7006 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=512 ===', '\\n', 'Epoch 1: train_acc=0.9528\nval_acc=0.9832 val_loss=0.0609', '\\n', 'Epoch 2: train_acc=0.9882 val_acc=0.9910\nval_loss=0.0336', '\\n', 'Epoch 3: train_acc=0.9950 val_acc=1.0000\nval_loss=0.0026', '\\n', 'Epoch 4: train_acc=0.9999 val_acc=0.9998\nval_loss=0.0009', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0005', '\\n', 'TEST Acc=0.7000 | SWA=0.6522 | CWA=0.7005 | ZSRTA=nan',\n'\\n', '\\nSaved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-12/working/experiment_data.npy', '\\n', 'Execution time:\n17 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Vocab size: 18', '\\n', '# seen rule labels: 2', '\\n', '\\n=== Training\nwith hidden_dim=64 ===', '\\n', 'Epoch 1: train_acc=0.9258 val_acc=0.9614\nval_loss=0.1383', '\\n', 'Epoch 2: train_acc=0.9757 val_acc=0.9854\nval_loss=0.0432', '\\n', 'Epoch 3: train_acc=0.9917 val_acc=0.9952\nval_loss=0.0208', '\\n', 'Epoch 4: train_acc=0.9970 val_acc=0.9966\nval_loss=0.0118', '\\n', 'Epoch 5: train_acc=0.9982 val_acc=0.9986\nval_loss=0.0071', '\\n', 'TEST Acc=0.7014 | SWA=0.6538 | CWA=0.7020 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=128 ===', '\\n', 'Epoch 1: train_acc=0.9344\nval_acc=0.9622 val_loss=0.1232', '\\n', 'Epoch 2: train_acc=0.9768 val_acc=0.9886\nval_loss=0.0360', '\\n', 'Epoch 3: train_acc=0.9939 val_acc=0.9968\nval_loss=0.0181', '\\n', 'Epoch 4: train_acc=0.9981 val_acc=0.9980\nval_loss=0.0076', '\\n', 'Epoch 5: train_acc=0.9993 val_acc=0.9992\nval_loss=0.0039', '\\n', 'TEST Acc=0.7003 | SWA=0.6527 | CWA=0.7009 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=256 ===', '\\n', 'Epoch 1: train_acc=0.9455\nval_acc=0.9800 val_loss=0.0610', '\\n', 'Epoch 2: train_acc=0.9898 val_acc=0.9950\nval_loss=0.0159', '\\n', 'Epoch 3: train_acc=0.9968 val_acc=0.9934\nval_loss=0.0215', '\\n', 'Epoch 4: train_acc=0.9992 val_acc=1.0000\nval_loss=0.0011', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0007', '\\n', 'TEST Acc=0.7001 | SWA=0.6523 | CWA=0.7006 | ZSRTA=nan',\n'\\n', '\\n=== Training with hidden_dim=512 ===', '\\n', 'Epoch 1: train_acc=0.9528\nval_acc=0.9832 val_loss=0.0609', '\\n', 'Epoch 2: train_acc=0.9882 val_acc=0.9910\nval_loss=0.0336', '\\n', 'Epoch 3: train_acc=0.9950 val_acc=1.0000\nval_loss=0.0026', '\\n', 'Epoch 4: train_acc=0.9999 val_acc=0.9998\nval_loss=0.0009', '\\n', 'Epoch 5: train_acc=1.0000 val_acc=1.0000\nval_loss=0.0005', '\\n', 'TEST Acc=0.7000 | SWA=0.6522 | CWA=0.7005 | ZSRTA=nan',\n'\\n', '\\nSaved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/experiment_data.npy', '\\n', 'Execution time:\n36 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "The execution output reveals a bug where the URA (Unseen Rule Accuracy) metric\nis consistently 'nan' during training and testing. This issue arises because the\nevaluation function for URA does not handle cases where there are no unseen\nrules in the validation or test data, leading to a division by zero or no\ncomputation for URA. To fix this, modify the `evaluate_URA` function to check if\nthere are any unseen rules in the batch before proceeding with the accuracy\ncalculation. If no unseen rules are present, it should return a default value\n(e.g., 0.0 or skip the batch entirely).", "The script failed because the required folder 'SPR_BENCH' containing the dataset\nfiles was not found. This caused an assertion error when checking for the\nexistence of the dataset directory. To fix this, ensure that the 'SPR_BENCH'\nfolder with the required CSV files (train.csv, dev.csv, test.csv) is present in\nthe working directory or specify the correct path using the 'SPR_DATA'\nenvironment variable.", "The code execution successfully trained a model and evaluated it on the\nSPR_BENCH dataset. However, there is a critical issue with the Unseen Rule\nAccuracy (URA) metric, which consistently shows 'nan' (not a number) during both\nvalidation and testing phases. This indicates that the model was not evaluated\non unseen rules as intended. The likely cause is that the unseen_idx list, which\nis supposed to identify examples with labels not present in the training set, is\nempty. This suggests that the test and dev datasets do not contain any labels\noutside the training label set. To fix this, ensure that the test and dev\ndatasets include sequences governed by new rules unseen in the training set.\nThis might involve modifying or regenerating the SPR_BENCH dataset to include\nsuch examples. Additionally, verify that the unseen_idx logic correctly\nidentifies unseen labels.", "", "The execution failed because the SPR_BENCH dataset directory could not be found.\nThe code expects the SPR_BENCH folder to be present in the current working\ndirectory or at the path specified by the SPR_PATH environment variable. To fix\nthis issue, ensure that the SPR_BENCH folder is correctly located and contains\nthe required train.csv, dev.csv, and test.csv files. Alternatively, set the\nSPR_PATH environment variable to the correct path of the SPR_BENCH folder before\nrunning the script.", "The implementation has a bug in the URA (Unseen Rule Accuracy) evaluation. The\nURA metric is calculated as 'nan' throughout the training and testing process.\nThis happens because the unseen_idx list, which identifies indices of unseen\nlabels, is empty. This indicates that there are no unseen labels in the\nvalidation or test sets compared to the training set. To fix this, ensure that\nthe dataset splits contain sequences governed by entirely new rules (unseen\nlabels) for proper zero-shot evaluation. This could involve modifying the\ndataset generation or preprocessing pipeline to guarantee unseen rules in the\ntest set.", "The training process for the neural-symbolic model seems to have a bug. Despite\nachieving perfect validation accuracy (val_acc=1.0000) and Color-Weighted\nAccuracy (CWA=1.0000) during training, the model performs poorly on the test set\nwith test accuracy (Acc=0.4348) and test Color-Weighted Accuracy (CWA=0.4339).\nAdditionally, the Unseen Rule Accuracy (URA) is 0.0000, indicating that the\nmodel fails completely on unseen rules. This suggests that the model has\noverfitted to the training data and cannot generalize to unseen labels or even\nthe test set. To fix this issue, consider the following steps: 1. Increase the\ndiversity of training data to include more examples of unseen rules. 2. Add\nregularization techniques such as dropout or weight decay to prevent\noverfitting. 3. Use early stopping based on validation performance to avoid\noverfitting. 4. Revisit the model architecture to ensure it effectively\nintegrates the neural and symbolic components for better generalization.", "The execution failed due to a KeyError: 'label_str' in the evaluate function.\nThis error occurs because the 'label_str' key is not available in the batch\ndictionary during evaluation. This is likely due to the test dataset being\ninitialized with train_mode=True, which excludes 'label_str' from the dataset\nitems. To fix this, ensure that the test dataset is initialized with\ntrain_mode=False when passed to the DataLoader. Specifically, change the\ninitialization of test_ds to: test_ds = SPRTorchDataset(spr['test'], vocab,\ntrain_mode=False).", "The execution output indicates that the Zero-Shot Reasoning Test Accuracy\n(ZSRTA) is reported as 'nan' for all hidden dimensions. This suggests that the\nzero-shot evaluation is not functioning correctly. The issue appears to be in\nthe `evaluate_test` function, specifically where the zero-shot index (`zs_idx`)\nis calculated. It assumes that unseen rules can be identified by checking if\nlabels are not in the `seen_rules` set. However, this logic fails if the test\ndataset does not actually contain any unseen labels. To fix this, ensure that\nthe test dataset includes labels that are genuinely unseen during training, or\nhandle the case where no unseen labels exist to avoid returning 'nan'.", "The execution output reveals that the model training and evaluation process\ncompleted without runtime errors, but there is an issue with the zero-shot\nreasoning evaluation. The ZSRTA (Zero-Shot Reasoning Test Accuracy) metric\nconsistently reports 'nan' across all hidden dimensions. This indicates that the\nmodel failed to handle zero-shot scenarios, which is a critical part of the\nresearch goal.   The root cause appears to be the absence of unseen rules in the\ntest dataset, as the ZSRTA calculation relies on identifying test samples with\nlabels not seen during training. To fix this, ensure that the test dataset\ncontains sequences governed by entirely new rules not present in the training\nset. Additionally, verify the logic for identifying unseen rules in the ZSRTA\ncalculation.", "", ""], "exc_type": [null, null, "AssertionError", null, null, "FileNotFoundError", null, null, "KeyError", null, null, null, null], "exc_info": [null, null, {"args": ["Folder SPR_BENCH with SPR_BENCH *.csv files not found."]}, null, null, {"args": ["SPR_BENCH folder not found \u2013 set $SPR_PATH"]}, null, null, {"args": ["label_str"]}, null, null, null, null], "exc_stack": [null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 60, "<module>", "assert data_root.exists(), f\"Folder {data_root} with SPR_BENCH *.csv files not found.\""]], null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 48, "<module>", "raise FileNotFoundError(\"SPR_BENCH folder not found \u2013 set $SPR_PATH\")"]], null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 262, "<module>", "swa, ura, _, _ = evaluate(dev_loader)"], ["runfile.py", 245, "evaluate", "all_labels.extend(batch[\"label_str\"])"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0005, "best_value": 0.0005}]}, {"metric_name": "zero-shot rule transfer accuracy", "lower_is_better": false, "description": "Accuracy of the model in a zero-shot rule transfer scenario.", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9994, "best_value": 0.9994}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.004, "best_value": 0.004}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0019, "best_value": 0.0019}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "training dataset", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "validation dataset", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "validation dataset", "final_value": 0.002, "best_value": 0.002}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "validation dataset", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "unseen relation accuracy", "lower_is_better": false, "description": "Accuracy of the model on unseen relations in the dataset.", "data": [{"dataset_name": "unseen relation dataset", "final_value": null, "best_value": null}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "training dataset", "final_value": 0.0007, "best_value": 0.0007}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9478, "best_value": 0.9478}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9492, "best_value": 0.9492}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1685, "best_value": 0.1685}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1685, "best_value": 0.1685}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.65, "best_value": 0.65}]}, {"metric_name": "unseen rule accuracy", "lower_is_better": false, "description": "Accuracy of the model on unseen rules.", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Stochastic Weight Averaging on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "URA", "lower_is_better": false, "description": "URA metric value.", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": null}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0005, "best_value": 0.0005}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.001, "best_value": 0.001}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "ZeroShotSPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "ZeroShotSPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Closed-world assumption accuracy on the validation dataset.", "data": [{"dataset_name": "ZeroShotSPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "unseen rule accuracy", "lower_is_better": false, "description": "Accuracy of the model on unseen rules.", "data": [{"dataset_name": "ZeroShotSPR", "final_value": null, "best_value": null}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "ZeroShotSPR", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "ZeroShotSPR", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy achieved on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0005, "best_value": 0.0005}]}, {"metric_name": "zero-shot rule transfer accuracy", "lower_is_better": false, "description": "The accuracy for zero-shot rule transfer evaluation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0005, "best_value": 0.0005}]}, {"metric_name": "zero-shot rule transfer accuracy", "lower_is_better": false, "description": "The accuracy of the model on zero-shot rule transfer tasks.", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "Train Accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "Validation Accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0005, "best_value": 0.0005}]}, {"metric_name": "Zero-shot Rule Transfer Accuracy", "lower_is_better": false, "description": "Accuracy of the model's zero-shot rule transfer capability.", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_hidden_dim_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_hidden_dim_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_final_val_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_ZSRTA_bar.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_test_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_confusion_matrix_top10.png"], [], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_hidden_dim_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_hidden_dim_loss_curves.png", "../../logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_final_val_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_ZSRTA_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_75ab9bcca92c42d78b9ab2db10db7677/SPR_BENCH_val_accuracy_mean_stderr.png", "../../logs/0-run/experiment_results/seed_aggregation_75ab9bcca92c42d78b9ab2db10db7677/SPR_BENCH_loss_mean_stderr.png", "../../logs/0-run/experiment_results/seed_aggregation_75ab9bcca92c42d78b9ab2db10db7677/SPR_BENCH_final_metrics_bar.png"]], "plot_paths": [["experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_hidden_dim_accuracy_curves.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_hidden_dim_loss_curves.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_final_val_accuracy_bar.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_ZSRTA_bar.png"], [], [], [], ["experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_accuracy_curves.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_loss_curves.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_test_metrics_bar.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_confusion_matrix_top10.png"], [], [], [], [], [], [], ["experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_hidden_dim_accuracy_curves.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_hidden_dim_loss_curves.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_final_val_accuracy_bar.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_ZSRTA_bar.png"], ["experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_75ab9bcca92c42d78b9ab2db10db7677/SPR_BENCH_val_accuracy_mean_stderr.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_75ab9bcca92c42d78b9ab2db10db7677/SPR_BENCH_loss_mean_stderr.png", "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_75ab9bcca92c42d78b9ab2db10db7677/SPR_BENCH_final_metrics_bar.png"]], "plot_analyses": [[{"analysis": "In this plot, the training and validation accuracy for different hidden dimensions (hd64, hd128, hd256, hd512) are shown over five epochs. All configurations exhibit rapid convergence to near-perfect accuracy, with validation accuracy closely tracking training accuracy. The larger hidden dimensions (hd256 and hd512) achieve slightly higher accuracy earlier, but the differences are minimal by the final epoch. This suggests that the model's performance is robust to changes in hidden dimension size, and all configurations generalize well to the validation set.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_hidden_dim_accuracy_curves.png"}, {"analysis": "This plot depicts the training and validation loss for various hidden dimensions over five epochs. The loss decreases rapidly for all configurations, with minimal differences between training and validation losses, indicating no overfitting. Larger hidden dimensions (hd256 and hd512) show slightly faster convergence to lower loss values, but the differences are negligible by the fifth epoch. The results suggest efficient training and good alignment between the training and validation datasets.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_hidden_dim_loss_curves.png"}, {"analysis": "This plot shows the final validation accuracy at epoch 5 for different hidden dimensions. All configurations achieve nearly identical accuracy, indicating that the hidden dimension size does not significantly impact the final performance. The model appears to generalize well across all tested configurations, which suggests that the architecture and training process are robust to variations in this hyperparameter.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_final_val_accuracy_bar.png"}, {"analysis": "This plot aims to show the Zero-Shot Rule Transfer Accuracy (ZSRTA) for different hidden dimensions, but it is empty. This could indicate that the data for ZSRTA was not collected, the metric is not applicable, or there was an issue in the experiment setup. Further investigation is needed to determine the cause of the absence of results in this plot.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_2d2ef9d4cf08449ba8e6d7a8f5a36a88_proc_312327/SPR_BENCH_ZSRTA_bar.png"}], [], [], [], [{"analysis": "The plot shows the training and validation accuracy over epochs. The model achieves rapid improvement within the first two epochs, reaching above 94% accuracy for both training and validation. After that, the accuracy plateaus, indicating convergence. The close alignment between the training and validation curves suggests that the model generalizes well without overfitting.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot depicts the cross-entropy loss for training and validation over epochs. The loss decreases sharply in the initial epochs and stabilizes at a low value by the third epoch. The consistency between the training and validation loss curves further supports that the model is not overfitting and is learning effectively.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart compares the final test metrics for accuracy and Shape-Weighted Accuracy (SWA). The overall accuracy is very high at 94.9%, while the SWA is significantly lower at 65.0%. This disparity suggests that while the model performs well overall, it struggles with sequences that involve a higher variety of shapes, highlighting a potential area for improvement.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_test_metrics_bar.png"}, {"analysis": "The confusion matrix provides a detailed breakdown of the model's predictions for the top-10 labels. The diagonal dominance indicates that the model performs well in correctly predicting both classes. However, there is some misclassification, as seen in the off-diagonal values, which could be further analyzed to identify patterns or specific areas of difficulty.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_b32485af49ba4ecba574e32e9e66d40e_proc_316523/SPR_BENCH_confusion_matrix_top10.png"}], [], [], [], [], [], [], [{"analysis": "The plot shows the training and validation accuracy over epochs for different hidden dimensions (hd64, hd128, hd256, and hd512). The accuracy improves rapidly during the first two epochs and stabilizes at nearly 100% by the fifth epoch for all configurations. The validation accuracy closely mirrors the training accuracy, indicating that the model generalizes well to the validation set. Increasing the hidden dimension does not appear to significantly impact the final accuracy, as all configurations converge to similar performance levels.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_hidden_dim_accuracy_curves.png"}, {"analysis": "This plot depicts the training and validation loss over epochs for different hidden dimensions. The loss decreases sharply in the first two epochs and approaches near-zero values by the fifth epoch for all configurations. The validation loss aligns closely with the training loss, suggesting minimal overfitting. Larger hidden dimensions do not seem to provide a significant advantage in reducing loss, as all configurations perform similarly by the end of training.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_hidden_dim_loss_curves.png"}, {"analysis": "This bar chart illustrates the final validation accuracy at epoch 5 for different hidden dimensions. All configurations achieve nearly identical validation accuracy, close to 1.0. This indicates that the choice of hidden dimension does not significantly impact the model's final validation performance, suggesting that the model's capacity is sufficient for the task across all tested configurations.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_final_val_accuracy_bar.png"}, {"analysis": "This plot is intended to show the Zero-Shot Rule Transfer Accuracy (ZSRTA) for a hidden dimension of 64. However, the plot appears empty, indicating that no meaningful results were obtained for ZSRTA in this configuration. This could imply a failure in the zero-shot evaluation or an issue in the experimental setup for this specific metric.", "plot_path": "experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/SPR_BENCH_ZSRTA_bar.png"}], []], "vlm_feedback_summary": ["The plots demonstrate that the model achieves near-perfect accuracy and low loss\nacross various hidden dimensions, indicating robustness and effective training.\nHowever, the absence of data in the ZSRTA plot raises questions about the\nexperiment's coverage or setup.", "[]", "[]", "[]", "The plots indicate that the model achieves strong performance in terms of\noverall accuracy and generalization. However, there is room for improvement in\nShape-Weighted Accuracy (SWA), suggesting the need for enhanced handling of\nsequences with diverse shape varieties. The confusion matrix confirms good\npredictive performance but also highlights some misclassification that warrants\nfurther investigation.", "[]", "[]", "[]", "[]", "[]", "[]", "The plots reveal that the model achieves high accuracy and low loss on both\ntraining and validation sets across all tested hidden dimensions, suggesting\neffective learning and generalization. However, the zero-shot evaluation plot is\nempty, indicating a lack of results or potential issues in the zero-shot\ntransfer experiment.", "[]"], "exec_time": [44.15459990501404, 9.429043769836426, 0.38666439056396484, 18.491085290908813, 16.260234117507935, 0.4361701011657715, 10.494627714157104, 10.246256351470947, 4.317037343978882, 16.351303815841675, 17.782896757125854, 36.04651737213135, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], [], [], [], ["[\"SPR_BENCH\"]"], [], [], [], [], [], [], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------ load data ------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Keys & helpers --------------------------------------------------------\nhidden_dims = sorted([int(k.split(\"_\")[-1]) for k in exp.keys()])\nepochs = list(range(1, 1 + len(next(iter(exp.values()))[\"metrics\"][\"train_acc\"])))\n\n# ------------------------ FIGURE 1 : accuracy curves ------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_acc = exp[k][\"metrics\"][\"train_acc\"]\n        val_acc = exp[k][\"metrics\"][\"val_acc\"]\n        plt.plot(epochs, tr_acc, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_acc, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Accuracy vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 2 : loss curves ----------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_loss = exp[k][\"losses\"][\"train\"]\n        val_loss = exp[k][\"losses\"][\"val\"]\n        plt.plot(epochs, tr_loss, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_loss, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Loss vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 3 : final val accuracy ---------------\ntry:\n    plt.figure()\n    finals = [exp[f\"hidden_{hd}\"][\"metrics\"][\"val_acc\"][-1] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], finals, color=\"skyblue\")\n    plt.title(\"SPR_BENCH: Final Validation Accuracy by Hidden Dim\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"Validation Accuracy (Epoch 5)\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_val_accuracy_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val accuracy bar: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 4 : ZSRTA bar ------------------------\ntry:\n    plt.figure()\n    zsrtas = [exp[f\"hidden_{hd}\"][\"metrics\"][\"ZSRTA\"][0] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], zsrtas, color=\"salmon\")\n    plt.title(\"SPR_BENCH: Zero-Shot Rule Transfer Accuracy (ZSRTA)\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"ZSRTA\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_ZSRTA_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ZSRTA bar: {e}\")\n    plt.close()\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom collections import Counter, defaultdict\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# ---------- helpers ----------\nepochs = list(range(1, 1 + len(exp.get(\"metrics\", {}).get(\"train_acc\", []))))\n\n# ------------------ FIG 1: accuracy curves ---------------------------\ntry:\n    plt.figure()\n    tr_acc = exp[\"metrics\"][\"train_acc\"]\n    val_acc = exp[\"metrics\"][\"val_acc\"]\n    plt.plot(epochs, tr_acc, marker=\"o\", label=\"Train\")\n    plt.plot(epochs, val_acc, marker=\"x\", linestyle=\"--\", label=\"Validation\")\n    plt.title(\"SPR_BENCH: Train & Val Accuracy vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ------------------ FIG 2: loss curves --------------------------------\ntry:\n    plt.figure()\n    tr_loss = exp[\"losses\"][\"train\"]\n    val_loss = exp[\"losses\"][\"val\"]\n    plt.plot(epochs, tr_loss, marker=\"o\", label=\"Train\")\n    plt.plot(epochs, val_loss, marker=\"x\", linestyle=\"--\", label=\"Validation\")\n    plt.title(\"SPR_BENCH: Train & Val Loss vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------ FIG 3: test metrics bar ---------------------------\ntry:\n    plt.figure()\n    acc = exp[\"metrics\"][\"val_acc\"][-1] if exp[\"metrics\"][\"val_acc\"] else np.nan\n    swa = exp[\"metrics\"][\"SWA\"][0] if exp[\"metrics\"][\"SWA\"] else np.nan\n    ura = exp[\"metrics\"][\"URA\"][0] if exp[\"metrics\"][\"URA\"] else np.nan\n    metrics = [acc, swa, ura]\n    names = [\"Accuracy\", \"SWA\", \"URA\"]\n    plt.bar(names, metrics, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n    plt.title(\"SPR_BENCH: Final Test Metrics\")\n    for i, v in enumerate(metrics):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metric bar plot: {e}\")\n    plt.close()\n\n# ------------------ FIG 4: confusion matrix (top 10 labels) ----------\ntry:\n    preds = exp.get(\"predictions\", [])\n    gts = exp.get(\"ground_truth\", [])\n    if preds and gts:\n        # determine top 10 labels by occurrence in ground truth\n        top_labels = [l for l, _ in Counter(gts).most_common(10)]\n        idx_map = {l: i for i, l in enumerate(top_labels)}\n        cm = np.zeros((len(top_labels), len(top_labels)), dtype=int)\n        for gt, pr in zip(gts, preds):\n            if gt in idx_map and pr in idx_map:\n                cm[idx_map[gt], idx_map[pr]] += 1\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xticks(range(len(top_labels)), top_labels, rotation=90)\n        plt.yticks(range(len(top_labels)), top_labels)\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix (Top-10 Labels)\\nLeft: Ground Truth, Top: Predicted\"\n        )\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"Ground Truth Label\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix_top10.png\")\n        plt.tight_layout()\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------ load data ------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Keys & helpers --------------------------------------------------------\nhidden_dims = sorted([int(k.split(\"_\")[-1]) for k in exp.keys()])\nepochs = list(range(1, 1 + len(next(iter(exp.values()))[\"metrics\"][\"train_acc\"])))\n\n# ------------------------ FIGURE 1 : accuracy curves ------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_acc = exp[k][\"metrics\"][\"train_acc\"]\n        val_acc = exp[k][\"metrics\"][\"val_acc\"]\n        plt.plot(epochs, tr_acc, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_acc, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Accuracy vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 2 : loss curves ----------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_loss = exp[k][\"losses\"][\"train\"]\n        val_loss = exp[k][\"losses\"][\"val\"]\n        plt.plot(epochs, tr_loss, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_loss, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Loss vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 3 : final val accuracy ---------------\ntry:\n    plt.figure()\n    finals = [exp[f\"hidden_{hd}\"][\"metrics\"][\"val_acc\"][-1] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], finals, color=\"skyblue\")\n    plt.title(\"SPR_BENCH: Final Validation Accuracy by Hidden Dim\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"Validation Accuracy (Epoch 5)\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_val_accuracy_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val accuracy bar: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 4 : ZSRTA bar ------------------------\ntry:\n    plt.figure()\n    zsrtas = [exp[f\"hidden_{hd}\"][\"metrics\"][\"ZSRTA\"][0] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], zsrtas, color=\"salmon\")\n    plt.title(\"SPR_BENCH: Zero-Shot Rule Transfer Accuracy (ZSRTA)\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"ZSRTA\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_ZSRTA_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ZSRTA bar: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------ load data ------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Keys & helpers --------------------------------------------------------\nhidden_dims = sorted([int(k.split(\"_\")[-1]) for k in exp.keys()])\nepochs = list(range(1, 1 + len(next(iter(exp.values()))[\"metrics\"][\"train_acc\"])))\n\n# ------------------------ FIGURE 1 : accuracy curves ------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_acc = exp[k][\"metrics\"][\"train_acc\"]\n        val_acc = exp[k][\"metrics\"][\"val_acc\"]\n        plt.plot(epochs, tr_acc, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_acc, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Accuracy vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 2 : loss curves ----------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_loss = exp[k][\"losses\"][\"train\"]\n        val_loss = exp[k][\"losses\"][\"val\"]\n        plt.plot(epochs, tr_loss, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_loss, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Loss vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 3 : final val accuracy ---------------\ntry:\n    plt.figure()\n    finals = [exp[f\"hidden_{hd}\"][\"metrics\"][\"val_acc\"][-1] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], finals, color=\"skyblue\")\n    plt.title(\"SPR_BENCH: Final Validation Accuracy by Hidden Dim\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"Validation Accuracy (Epoch 5)\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_val_accuracy_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val accuracy bar: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 4 : ZSRTA bar ------------------------\ntry:\n    plt.figure()\n    zsrtas = [exp[f\"hidden_{hd}\"][\"metrics\"][\"ZSRTA\"][0] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], zsrtas, color=\"salmon\")\n    plt.title(\"SPR_BENCH: Zero-Shot Rule Transfer Accuracy (ZSRTA)\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"ZSRTA\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_ZSRTA_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ZSRTA bar: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------ load data ------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Keys & helpers --------------------------------------------------------\nhidden_dims = sorted([int(k.split(\"_\")[-1]) for k in exp.keys()])\nepochs = list(range(1, 1 + len(next(iter(exp.values()))[\"metrics\"][\"train_acc\"])))\n\n# ------------------------ FIGURE 1 : accuracy curves ------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_acc = exp[k][\"metrics\"][\"train_acc\"]\n        val_acc = exp[k][\"metrics\"][\"val_acc\"]\n        plt.plot(epochs, tr_acc, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_acc, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Accuracy vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 2 : loss curves ----------------------\ntry:\n    plt.figure()\n    for k in sorted(exp.keys(), key=lambda x: int(x.split(\"_\")[-1])):\n        hd = int(k.split(\"_\")[-1])\n        tr_loss = exp[k][\"losses\"][\"train\"]\n        val_loss = exp[k][\"losses\"][\"val\"]\n        plt.plot(epochs, tr_loss, marker=\"o\", label=f\"train hd{hd}\")\n        plt.plot(epochs, val_loss, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd}\")\n    plt.title(\"SPR_BENCH: Train & Val Loss vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_hidden_dim_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 3 : final val accuracy ---------------\ntry:\n    plt.figure()\n    finals = [exp[f\"hidden_{hd}\"][\"metrics\"][\"val_acc\"][-1] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], finals, color=\"skyblue\")\n    plt.title(\"SPR_BENCH: Final Validation Accuracy by Hidden Dim\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"Validation Accuracy (Epoch 5)\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_val_accuracy_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val accuracy bar: {e}\")\n    plt.close()\n\n# ------------------------ FIGURE 4 : ZSRTA bar ------------------------\ntry:\n    plt.figure()\n    zsrtas = [exp[f\"hidden_{hd}\"][\"metrics\"][\"ZSRTA\"][0] for hd in hidden_dims]\n    plt.bar([str(hd) for hd in hidden_dims], zsrtas, color=\"salmon\")\n    plt.title(\"SPR_BENCH: Zero-Shot Rule Transfer Accuracy (ZSRTA)\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"ZSRTA\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_ZSRTA_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ZSRTA bar: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------ set up work dir -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------ locate experiment files -----\nexperiment_data_path_list = [\n    \"None/experiment_data.npy\",\n    \"None/experiment_data.npy\",\n    \"experiments/2025-07-27_23-49-14_neural_symbolic_zero_shot_spr_attempt_0/logs/0-run/experiment_results/experiment_841e59e95c3445b0bf7bf71ccd94a9d2_proc_316521/experiment_data.npy\",\n]\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\n# ------------- aggregate  -----------------\nspr_runs = []\nfor exp in all_experiment_data:\n    try:\n        spr_runs.append(exp[\"hidden_dim_tuning\"][\"SPR_BENCH\"])\n    except Exception as e:\n        print(f\"Bad experiment structure skipped: {e}\")\n\nif len(spr_runs) == 0:\n    print(\"No valid SPR_BENCH runs found; aborting plotting.\")\n    exit()\n\n# determine hidden dims present in every run\nall_hidden_dims = sorted(\n    set(int(k.split(\"_\")[-1]) for run in spr_runs for k in run.keys())\n)\n# keep at most 5 for readability\nhidden_dims = all_hidden_dims[:5]\n\n# epochs length assumes all runs same length; fall back otherwise\nfirst_run = next(iter(spr_runs[0].values()))\nepochs = list(range(1, 1 + len(first_run[\"metrics\"][\"val_acc\"])))\n\n# helper dict to store arrays per hd\nacc_tensor = {hd: [] for hd in hidden_dims}\nloss_train_tensor = {hd: [] for hd in hidden_dims}\nloss_val_tensor = {hd: [] for hd in hidden_dims}\nfinal_val_acc = {hd: [] for hd in hidden_dims}\nzsrtas = {hd: [] for hd in hidden_dims}\n\nfor run in spr_runs:\n    for hd in hidden_dims:\n        key = f\"hidden_{hd}\"\n        if key not in run:\n            continue\n        acc_tensor[hd].append(run[key][\"metrics\"][\"val_acc\"])\n        loss_train_tensor[hd].append(run[key][\"losses\"][\"train\"])\n        loss_val_tensor[hd].append(run[key][\"losses\"][\"val\"])\n        final_val_acc[hd].append(run[key][\"metrics\"][\"val_acc\"][-1])\n        zsrtas[hd].append(run[key][\"metrics\"][\"ZSRTA\"][0])\n\n# ------------ FIGURE 1: Val Accuracy mean \u00b1 stderr -----------\ntry:\n    plt.figure()\n    for hd in hidden_dims:\n        if len(acc_tensor[hd]) == 0:\n            continue\n        acc_arr = np.array(acc_tensor[hd])  # shape (runs, epochs)\n        mean = acc_arr.mean(axis=0)\n        stderr = acc_arr.std(axis=0, ddof=1) / np.sqrt(acc_arr.shape[0])\n        plt.plot(epochs, mean, marker=\"o\", label=f\"hd{hd} mean\")\n        plt.fill_between(epochs, mean - stderr, mean + stderr, alpha=0.2)\n    plt.title(\"SPR_BENCH: Validation Accuracy (mean \u00b1 s.e.)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_accuracy_mean_stderr.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated accuracy plot: {e}\")\n    plt.close()\n\n# ------------ FIGURE 2: Loss curves mean \u00b1 stderr -----------\ntry:\n    plt.figure()\n    for hd in hidden_dims:\n        if len(loss_train_tensor[hd]) == 0:\n            continue\n        tr = np.array(loss_train_tensor[hd])\n        vl = np.array(loss_val_tensor[hd])\n        tr_m, tr_se = tr.mean(axis=0), tr.std(axis=0, ddof=1) / np.sqrt(tr.shape[0])\n        vl_m, vl_se = vl.mean(axis=0), vl.std(axis=0, ddof=1) / np.sqrt(vl.shape[0])\n        plt.plot(epochs, tr_m, marker=\"o\", label=f\"train hd{hd} mean\")\n        plt.fill_between(epochs, tr_m - tr_se, tr_m + tr_se, alpha=0.15)\n        plt.plot(epochs, vl_m, marker=\"x\", linestyle=\"--\", label=f\"val hd{hd} mean\")\n        plt.fill_between(epochs, vl_m - vl_se, vl_m + vl_se, alpha=0.15)\n    plt.title(\"SPR_BENCH: Cross-Entropy Loss (mean \u00b1 s.e.)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_mean_stderr.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# ------------ FIGURE 3: Final Val Acc & ZSRTA bars ----------\ntry:\n    plt.figure(figsize=(10, 4))\n    x = np.arange(len(hidden_dims))\n    bar_w = 0.35\n\n    # Final validation accuracy\n    val_means = [np.mean(final_val_acc[hd]) for hd in hidden_dims]\n    val_se = [\n        (\n            np.std(final_val_acc[hd], ddof=1) / np.sqrt(len(final_val_acc[hd]))\n            if len(final_val_acc[hd]) > 1\n            else 0\n        )\n        for hd in hidden_dims\n    ]\n    plt.bar(\n        x - bar_w / 2,\n        val_means,\n        yerr=val_se,\n        width=bar_w,\n        label=\"Final Val Acc\",\n        color=\"skyblue\",\n        capsize=5,\n    )\n\n    # ZSRTA\n    z_means = [np.mean(zsrtas[hd]) for hd in hidden_dims]\n    z_se = [\n        (\n            np.std(zsrtas[hd], ddof=1) / np.sqrt(len(zsrtas[hd]))\n            if len(zsrtas[hd]) > 1\n            else 0\n        )\n        for hd in hidden_dims\n    ]\n    plt.bar(\n        x + bar_w / 2,\n        z_means,\n        yerr=z_se,\n        width=bar_w,\n        label=\"ZSRTA\",\n        color=\"salmon\",\n        capsize=5,\n    )\n\n    plt.xticks(x, [str(hd) for hd in hidden_dims])\n    plt.title(\"SPR_BENCH: Final Validation Accuracy and ZSRTA (mean \u00b1 s.e.)\")\n    plt.xlabel(\"Hidden Dimension\")\n    plt.ylabel(\"Score\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_metrics_bar.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated bar plot: {e}\")\n    plt.close()\n\n# ------------- print numerical summary -------------\nfor hd in hidden_dims:\n    if len(final_val_acc[hd]) == 0:\n        continue\n    mean = np.mean(final_val_acc[hd])\n    se = (\n        np.std(final_val_acc[hd], ddof=1) / np.sqrt(len(final_val_acc[hd]))\n        if len(final_val_acc[hd]) > 1\n        else 0\n    )\n    print(\n        f\"Hidden {hd}: Final Val Acc = {mean:.3f} \u00b1 {se:.3f} (n={len(final_val_acc[hd])})\"\n    )\n"], "plot_plan": ["We first load experiment_data.npy, pull out the \u201chidden_dim_tuning/SPR_BENCH\u201d sub-dictionary and iterate over the four hidden-dimension settings.  For every setting we cache the per-epoch lists of train/val accuracy and train/val loss, as well as the final ZSRTA value.  Using these cached lists we build four compact plots: (1) train vs validation accuracy across epochs for every hidden size, (2) train vs validation loss across epochs for every hidden size, (3) a bar-chart of the final validation accuracy (epoch 5) versus hidden size, and (4) a bar-chart of the zero-shot rule transfer accuracy (ZSRTA) versus hidden size.  Each figure is wrapped in its own try-except block, given an explicit title that includes the dataset name, saved into working_dir with descriptive filenames, and then closed.  We limit ourselves to four figures, well under the five-plot ceiling.  The code starts with the required imports and creation of working_dir, uses only values that already exist in experiment_data.npy, and prints a short confirmation message after each save.  All plotting calls employ basic matplotlib API without any non-standard styles, and figures are closed in a finally-like manner even if errors occur.  No simulated data or additional epochs are introduced.", null, null, null, "We load experiment_data.npy, extract the single dataset key \u2018SPR_BENCH\u2019, and pull out training/validation accuracies and losses to form epoch-wise curves. Using these, we create two line plots: (1) accuracy vs. epochs and (2) loss vs. epochs. Next, we read the saved test-set metrics\u2014overall accuracy, shape-weighted accuracy (SWA), and unseen-rule accuracy (URA)\u2014and summarise them in a simple bar chart to give a quick end-of-run snapshot. If predictions and ground-truth labels are present, we build a confusion-matrix heat-map (limited to the first 10 labels by occurrence to avoid overcrowding) so that users can visually inspect where the model fails. Every figure is produced inside its own try-except block, saved to the shared working directory with descriptive filenames, and the figure is always closed afterward. All plotting relies solely on the values stored in experiment_data.npy; no synthetic data are introduced. The code is concise, employs consistent naming, and respects the 5-figure upper bound by generating at most four plots. Finally, it prints the paths of the saved images so the user knows where to look.", null, null, null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["hidden_dim", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["Below is a compact script that immediately loads the saved numpy file, walks\nthrough the stored results, and prints the best (highest for accuracies, lowest\nfor losses) value of each tracked metric for every hyper-parameter setting. The\ndataset name is printed first, followed by clearly labelled metric values.", "Below is a short script that immediately loads the saved NumPy file, extracts\nthe recorded metrics, decides the \u201cbest\u201d value for each metric (minimum for any\nmetric containing the word \u201closs\u201d, otherwise maximum), and prints the results in\nthe required, clearly-labeled format.", "", "The script below immediately loads the saved NumPy dictionary, walks through\neach stored experiment (here only \u201cNeuroSymbolicSPR\u201d), determines the best value\nfor every recorded metric (maximum for scores/accuracies, minimum for losses),\nand prints the dataset name followed by each metric name and its best value in a\nhuman-readable form. No plots are created and no special entry point is used.", "The script will load the NumPy file from the working directory, read the\nexperiment_data dictionary, and iterate through every dataset it contains.   For\neach dataset it will extract the last (i.e., final) value of every tracked\nmetric: train accuracy, validation accuracy, train loss, validation loss, Shape-\nWeighted Accuracy (SWA), and Unseen Rule Accuracy (URA).   The script prints the\ndataset name once, followed by each metric name and its value with four-decimal\nprecision.   No figures are generated, and the code runs immediately when\nexecuted because everything is placed at the global scope.", "", "This script will load the saved NumPy dictionary, iterate over every dataset key\n(e.g., \u201cSPR_BENCH\u201d), pick the best value for each stored metric or loss (max for\naccuracies/SWA/URA, min for losses), and print them in a clearly-labelled,\nhuman-readable form. The working directory is derived exactly as in the original\ntraining script, so the file is found reliably. Execution is immediate because\nall logic sits at the global scope; no special entry point is required.", "Below is a small script that immediately loads the saved NumPy file from the\nrequired working directory, extracts the stored lists of metric values, and\nprints the final value for each metric. It iterates over every dataset entry\n(here only \u201cZeroShotSPR\u201d) and clearly labels each number (e.g., \u201ctrain\naccuracy,\u201d \u201cvalidation loss,\u201d etc.) before printing it.", "", "Below is a compact script that immediately loads the saved numpy file, walks\nthrough the stored results, and prints the best (highest for accuracies, lowest\nfor losses) value of each tracked metric for every hyper-parameter setting. The\ndataset name is printed first, followed by clearly labelled metric values.", "Below is a compact script that immediately loads the saved numpy file, walks\nthrough the stored results, and prints the best (highest for accuracies, lowest\nfor losses) value of each tracked metric for every hyper-parameter setting. The\ndataset name is printed first, followed by clearly labelled metric values.", "Below is a compact script that immediately loads the saved numpy file, walks\nthrough the stored results, and prints the best (highest for accuracies, lowest\nfor losses) value of each tracked metric for every hyper-parameter setting. The\ndataset name is printed first, followed by clearly labelled metric values.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the experiment data ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# iterate over datasets and hyper-parameter settings ------------------\nsearch_space = experiment_data.get(\"hidden_dim_tuning\", {})\nfor dataset_name, runs in search_space.items():\n    print(f\"\\nDataset: {dataset_name}\")  # requirement (3)\n\n    for run_name, mdata in runs.items():\n        # retrieve recorded metric sequences\n        train_accs = mdata[\"metrics\"].get(\"train_acc\", [])\n        val_accs = mdata[\"metrics\"].get(\"val_acc\", [])\n        val_losses = mdata[\"metrics\"].get(\"val_loss\", [])\n        zsrtas = mdata[\"metrics\"].get(\"ZSRTA\", [])\n\n        # derive best/final values as specified\n        best_train_acc = max(train_accs) if train_accs else float(\"nan\")\n        best_val_acc = max(val_accs) if val_accs else float(\"nan\")\n        best_val_loss = min(val_losses) if val_losses else float(\"nan\")\n        final_zsrta = zsrtas[-1] if zsrtas else float(\"nan\")\n\n        # output with explicit, descriptive labels --------------------\n        print(f\"  Hyper-parameter setting: {run_name}\")\n        print(f\"    Train accuracy: {best_train_acc:.4f}\")\n        print(f\"    Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"    Validation loss: {best_val_loss:.4f}\")\n        print(f\"    Zero-shot rule transfer accuracy: {final_zsrta:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\ndef best_value(metric_name: str, values):\n    \"\"\"Return best value depending on metric type (min for loss, max otherwise).\"\"\"\n    if not values:  # empty list guard\n        return float(\"nan\")\n    if \"loss\" in metric_name.lower():\n        return min(values)\n    return max(values)\n\n\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # --- Metrics stored under the \"metrics\" key --------------------------------\n    for metric_name, value_list in content.get(\"metrics\", {}).items():\n        value = best_value(metric_name, value_list)\n        clean_name = metric_name.replace(\"_\", \" \")\n        print(f\"{clean_name}: {value:.4f}\")\n\n    # --- Losses stored under the separate \"losses\" key --------------------------\n    for split_name, value_list in content.get(\"losses\", {}).items():\n        metric_name = f\"{split_name} loss\"\n        value = best_value(metric_name, value_list)\n        print(f\"{metric_name}: {value:.4f}\")\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# helper: map raw metric keys to descriptive labels\npretty_names = {\n    \"train_acc\": \"train accuracy\",\n    \"val_acc\": \"validation accuracy\",\n    \"val_loss\": \"validation loss\",\n    \"val_SWA\": \"validation shape-weighted accuracy\",\n    \"URA\": \"unseen relation accuracy\",\n    \"train\": \"train loss\",\n    \"val\": \"validation loss\",\n}\n\n# ------------------------------------------------------------------\n# iterate over each stored experiment and print best metrics\nfor dataset_name, content in experiment_data.items():\n    print(dataset_name)  # Dataset header\n\n    # --- metrics recorded under the \"metrics\" key -----------------\n    metrics = content.get(\"metrics\", {})\n    for m_key, values in metrics.items():\n        if not values:  # skip empty lists\n            continue\n        # choose best value: max for scores, min for losses\n        best_val = min(values) if \"loss\" in m_key.lower() else max(values)\n        metric_label = pretty_names.get(m_key, m_key)\n        print(f\"  {metric_label}: {best_val:.4f}\")\n\n    # --- losses recorded separately under the \"losses\" key --------\n    losses = content.get(\"losses\", {})\n    for split_key, values in losses.items():\n        # avoid printing a duplicate validation loss (already handled above)\n        if not values or (split_key == \"val\" and \"val_loss\" in metrics):\n            continue\n        best_loss = min(values)\n        loss_label = pretty_names.get(split_key, f\"{split_key} loss\")\n        print(f\"  {loss_label}: {best_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to fetch last element safely ----------\ndef _last_or_none(lst):\n    return lst[-1] if isinstance(lst, (list, tuple)) and lst else None\n\n\n# ---------- iterate over datasets and print metrics ----------\nfor ds_name, ds_content in experiment_data.items():\n    print(ds_name)  # dataset header\n\n    # ---- accuracies ----\n    train_acc = _last_or_none(ds_content.get(\"metrics\", {}).get(\"train_acc\", []))\n    if train_acc is not None:\n        print(f\"train accuracy: {train_acc:.4f}\")\n\n    val_acc = _last_or_none(ds_content.get(\"metrics\", {}).get(\"val_acc\", []))\n    if val_acc is not None:\n        print(f\"validation accuracy: {val_acc:.4f}\")\n\n    # ---- losses ----\n    train_loss = _last_or_none(ds_content.get(\"losses\", {}).get(\"train\", []))\n    if train_loss is not None:\n        print(f\"train loss: {train_loss:.4f}\")\n\n    val_loss = _last_or_none(ds_content.get(\"losses\", {}).get(\"val\", []))\n    if val_loss is not None:\n        print(f\"validation loss: {val_loss:.4f}\")\n\n    # ---- special metrics ----\n    swa = _last_or_none(ds_content.get(\"metrics\", {}).get(\"SWA\", []))\n    if swa is not None:\n        print(f\"shape-weighted accuracy: {swa:.4f}\")\n\n    ura = _last_or_none(ds_content.get(\"metrics\", {}).get(\"URA\", []))\n    if ura is not None:\n        print(f\"unseen rule accuracy: {ura:.4f}\")\n\n    print()  # blank line between datasets\n", "", "import os\nimport numpy as np\nimport math\n\n# ------------------------------------------------------------------\n# locate and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper to pick best value\ndef _best(values, minimize=False):\n    \"\"\"Return best (min or max) value from list while ignoring NaNs.\"\"\"\n    clean_vals = [v for v in values if not (isinstance(v, float) and math.isnan(v))]\n    if not clean_vals:  # all NaNs or empty\n        return float(\"nan\")\n    return min(clean_vals) if minimize else max(clean_vals)\n\n\n# ------------------------------------------------------------------\n# iterate over datasets and report\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # metrics -------------------------------------------------------\n    metrics = data.get(\"metrics\", {})\n    if metrics:\n        tr_acc = _best(metrics.get(\"train_acc\", []), minimize=False)\n        val_acc = _best(metrics.get(\"val_acc\", []), minimize=False)\n        val_swa = _best(metrics.get(\"val_SWA\", []), minimize=False)\n        ura = _best(metrics.get(\"URA\", []), minimize=False)\n\n        print(f\"  train accuracy: {tr_acc:.4f}\")\n        print(f\"  validation accuracy: {val_acc:.4f}\")\n        print(f\"  validation SWA: {val_swa:.4f}\")\n        print(f\"  URA: {ura:.4f}\")\n\n    # losses --------------------------------------------------------\n    losses = data.get(\"losses\", {})\n    if losses:\n        tr_loss = _best(losses.get(\"train\", []), minimize=True)\n        val_loss = _best(losses.get(\"val\", []), minimize=True)\n\n        print(f\"  training loss: {tr_loss:.4f}\")\n        print(f\"  validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data ------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to fetch the final (last) value in a list -----------------\ndef final_value(lst):\n    return lst[-1] if lst else None\n\n\n# ------------------------------------------------------------------\n# Iterate over datasets and report metrics -------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    metrics = data.get(\"metrics\", {})\n    losses = data.get(\"losses\", {})\n\n    metric_map = [\n        (\"train_acc\", \"train accuracy\"),\n        (\"val_acc\", \"validation accuracy\"),\n        (\"val_CWA\", \"validation CWA\"),\n        (\"URA\", \"unseen rule accuracy\"),\n        (\"val_loss\", \"validation loss\"),\n    ]\n\n    # Print final metric values\n    for key, label in metric_map:\n        value = final_value(metrics.get(key, []))\n        if value is not None:\n            print(f\"{label}: {value:.4f}\")\n\n    # Also print final training loss if available\n    train_loss = final_value(losses.get(\"train\", []))\n    if train_loss is not None:\n        print(f\"training loss: {train_loss:.4f}\")\n\n    print()  # blank line between datasets\n", "", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the experiment data ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# iterate over datasets and hyper-parameter settings ------------------\nsearch_space = experiment_data.get(\"hidden_dim_tuning\", {})\nfor dataset_name, runs in search_space.items():\n    print(f\"\\nDataset: {dataset_name}\")  # requirement (3)\n\n    for run_name, mdata in runs.items():\n        # retrieve recorded metric sequences\n        train_accs = mdata[\"metrics\"].get(\"train_acc\", [])\n        val_accs = mdata[\"metrics\"].get(\"val_acc\", [])\n        val_losses = mdata[\"metrics\"].get(\"val_loss\", [])\n        zsrtas = mdata[\"metrics\"].get(\"ZSRTA\", [])\n\n        # derive best/final values as specified\n        best_train_acc = max(train_accs) if train_accs else float(\"nan\")\n        best_val_acc = max(val_accs) if val_accs else float(\"nan\")\n        best_val_loss = min(val_losses) if val_losses else float(\"nan\")\n        final_zsrta = zsrtas[-1] if zsrtas else float(\"nan\")\n\n        # output with explicit, descriptive labels --------------------\n        print(f\"  Hyper-parameter setting: {run_name}\")\n        print(f\"    Train accuracy: {best_train_acc:.4f}\")\n        print(f\"    Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"    Validation loss: {best_val_loss:.4f}\")\n        print(f\"    Zero-shot rule transfer accuracy: {final_zsrta:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the experiment data ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# iterate over datasets and hyper-parameter settings ------------------\nsearch_space = experiment_data.get(\"hidden_dim_tuning\", {})\nfor dataset_name, runs in search_space.items():\n    print(f\"\\nDataset: {dataset_name}\")  # requirement (3)\n\n    for run_name, mdata in runs.items():\n        # retrieve recorded metric sequences\n        train_accs = mdata[\"metrics\"].get(\"train_acc\", [])\n        val_accs = mdata[\"metrics\"].get(\"val_acc\", [])\n        val_losses = mdata[\"metrics\"].get(\"val_loss\", [])\n        zsrtas = mdata[\"metrics\"].get(\"ZSRTA\", [])\n\n        # derive best/final values as specified\n        best_train_acc = max(train_accs) if train_accs else float(\"nan\")\n        best_val_acc = max(val_accs) if val_accs else float(\"nan\")\n        best_val_loss = min(val_losses) if val_losses else float(\"nan\")\n        final_zsrta = zsrtas[-1] if zsrtas else float(\"nan\")\n\n        # output with explicit, descriptive labels --------------------\n        print(f\"  Hyper-parameter setting: {run_name}\")\n        print(f\"    Train accuracy: {best_train_acc:.4f}\")\n        print(f\"    Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"    Validation loss: {best_val_loss:.4f}\")\n        print(f\"    Zero-shot rule transfer accuracy: {final_zsrta:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the experiment data ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# iterate over datasets and hyper-parameter settings ------------------\nsearch_space = experiment_data.get(\"hidden_dim_tuning\", {})\nfor dataset_name, runs in search_space.items():\n    print(f\"\\nDataset: {dataset_name}\")  # requirement (3)\n\n    for run_name, mdata in runs.items():\n        # retrieve recorded metric sequences\n        train_accs = mdata[\"metrics\"].get(\"train_acc\", [])\n        val_accs = mdata[\"metrics\"].get(\"val_acc\", [])\n        val_losses = mdata[\"metrics\"].get(\"val_loss\", [])\n        zsrtas = mdata[\"metrics\"].get(\"ZSRTA\", [])\n\n        # derive best/final values as specified\n        best_train_acc = max(train_accs) if train_accs else float(\"nan\")\n        best_val_acc = max(val_accs) if val_accs else float(\"nan\")\n        best_val_loss = min(val_losses) if val_losses else float(\"nan\")\n        final_zsrta = zsrtas[-1] if zsrtas else float(\"nan\")\n\n        # output with explicit, descriptive labels --------------------\n        print(f\"  Hyper-parameter setting: {run_name}\")\n        print(f\"    Train accuracy: {best_train_acc:.4f}\")\n        print(f\"    Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"    Validation loss: {best_val_loss:.4f}\")\n        print(f\"    Zero-shot rule transfer accuracy: {final_zsrta:.4f}\")\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', '  Hyper-parameter setting: hidden_64', '\\n', '\nTrain accuracy: 0.9982', '\\n', '    Validation accuracy: 0.9986', '\\n', '\nValidation loss: 0.0071', '\\n', '    Zero-shot rule transfer accuracy: nan',\n'\\n', '  Hyper-parameter setting: hidden_128', '\\n', '    Train accuracy:\n0.9993', '\\n', '    Validation accuracy: 0.9992', '\\n', '    Validation loss:\n0.0039', '\\n', '    Zero-shot rule transfer accuracy: nan', '\\n', '  Hyper-\nparameter setting: hidden_256', '\\n', '    Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Validation loss: 0.0007', '\\n', '\nZero-shot rule transfer accuracy: nan', '\\n', '  Hyper-parameter setting:\nhidden_512', '\\n', '    Train accuracy: 1.0000', '\\n', '    Validation accuracy:\n1.0000', '\\n', '    Validation loss: 0.0005', '\\n', '    Zero-shot rule transfer\naccuracy: nan', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'train acc: 1.0000', '\\n', 'val acc: 0.9994',\n'\\n', 'val loss: 0.0040', '\\n', 'URA: nan', '\\n', 'train loss: 0.0019', '\\n',\n'val loss: 0.0040', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "", "['NeuroSymbolicSPR', '\\n', '  train accuracy: 1.0000', '\\n', '  validation\naccuracy: 0.9998', '\\n', '  validation loss: 0.0020', '\\n', '  validation shape-\nweighted accuracy: 0.9998', '\\n', '  unseen relation accuracy: nan', '\\n', '\ntrain loss: 0.0007', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.9478', '\\n', 'validation accuracy:\n0.9492', '\\n', 'train loss: 0.1685', '\\n', 'validation loss: 0.1685', '\\n',\n'shape-weighted accuracy: 0.6500', '\\n', 'unseen rule accuracy: nan', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['Dataset: SPR_BENCH', '\\n', '  train accuracy: 1.0000', '\\n', '  validation\naccuracy: 0.9998', '\\n', '  validation SWA: 0.9998', '\\n', '  URA: nan', '\\n', '\ntraining loss: 0.0005', '\\n', '  validation loss: 0.0010', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: ZeroShotSPR', '\\n', 'train accuracy: 1.0000', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation CWA: 1.0000', '\\n', 'unseen rule accuracy:\nnan', '\\n', 'validation loss: 0.0000', '\\n', 'training loss: 0.0000', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['\\nDataset: SPR_BENCH', '\\n', '  Hyper-parameter setting: hidden_64', '\\n', '\nTrain accuracy: 0.9982', '\\n', '    Validation accuracy: 0.9986', '\\n', '\nValidation loss: 0.0071', '\\n', '    Zero-shot rule transfer accuracy: nan',\n'\\n', '  Hyper-parameter setting: hidden_128', '\\n', '    Train accuracy:\n0.9993', '\\n', '    Validation accuracy: 0.9992', '\\n', '    Validation loss:\n0.0039', '\\n', '    Zero-shot rule transfer accuracy: nan', '\\n', '  Hyper-\nparameter setting: hidden_256', '\\n', '    Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Validation loss: 0.0007', '\\n', '\nZero-shot rule transfer accuracy: nan', '\\n', '  Hyper-parameter setting:\nhidden_512', '\\n', '    Train accuracy: 1.0000', '\\n', '    Validation accuracy:\n1.0000', '\\n', '    Validation loss: 0.0005', '\\n', '    Zero-shot rule transfer\naccuracy: nan', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Hyper-parameter setting: hidden_64', '\\n', '\nTrain accuracy: 0.9982', '\\n', '    Validation accuracy: 0.9986', '\\n', '\nValidation loss: 0.0071', '\\n', '    Zero-shot rule transfer accuracy: nan',\n'\\n', '  Hyper-parameter setting: hidden_128', '\\n', '    Train accuracy:\n0.9993', '\\n', '    Validation accuracy: 0.9992', '\\n', '    Validation loss:\n0.0039', '\\n', '    Zero-shot rule transfer accuracy: nan', '\\n', '  Hyper-\nparameter setting: hidden_256', '\\n', '    Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Validation loss: 0.0007', '\\n', '\nZero-shot rule transfer accuracy: nan', '\\n', '  Hyper-parameter setting:\nhidden_512', '\\n', '    Train accuracy: 1.0000', '\\n', '    Validation accuracy:\n1.0000', '\\n', '    Validation loss: 0.0005', '\\n', '    Zero-shot rule transfer\naccuracy: nan', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Hyper-parameter setting: hidden_64', '\\n', '\nTrain accuracy: 0.9982', '\\n', '    Validation accuracy: 0.9986', '\\n', '\nValidation loss: 0.0071', '\\n', '    Zero-shot rule transfer accuracy: nan',\n'\\n', '  Hyper-parameter setting: hidden_128', '\\n', '    Train accuracy:\n0.9993', '\\n', '    Validation accuracy: 0.9992', '\\n', '    Validation loss:\n0.0039', '\\n', '    Zero-shot rule transfer accuracy: nan', '\\n', '  Hyper-\nparameter setting: hidden_256', '\\n', '    Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Validation loss: 0.0007', '\\n', '\nZero-shot rule transfer accuracy: nan', '\\n', '  Hyper-parameter setting:\nhidden_512', '\\n', '    Train accuracy: 1.0000', '\\n', '    Validation accuracy:\n1.0000', '\\n', '    Validation loss: 0.0005', '\\n', '    Zero-shot rule transfer\naccuracy: nan', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
