{"edges": [[3, 4], [3, 5], [3, 7], [3, 6]], "layout": [[0.0, 0.0], [0.2222222222222222, 0.0], [0.4444444444444444, 0.0], [0.6666666666666666, 0.0], [0.3333333333333333, 1.0], [0.5555555555555556, 1.0], [0.7777777777777778, 1.0], [1.0, 1.0]], "plan": ["We load the SPR_BENCH splits, build a vocabulary of symbol-tokens, and convert\nevery sequence into a PyG graph whose nodes are the tokens and whose edges\nconnect successive tokens (bi-directional).  Node features are one-hot vectors\nconcatenating shape and color indicators.  A small Graph Convolutional Network\n(two GCNConv layers, global mean-pool, linear classifier) is trained with cross-\nentropy; the optimizer is Adam.  After every epoch we evaluate on the dev set,\nprinting validation loss and Dual-Weighted Accuracy (DWA = (CWA+SWA)/2).  All\nmetrics, losses, predictions, and ground-truth labels are stored in an\nexperiment_data dictionary and saved under ./working.  Finally, test-set scores\nare computed and printed.  The implementation follows the mandatory GPU/CPU\nhandling and data-saving guidelines, runs end-to-end without extra entry points,\nand should finish within minutes on a single GPU.", "We convert each SPR sequence into a simple chain graph where every token becomes\na node and consecutive tokens are bidirectionally connected.  Each node feature\nis a concatenated one-hot vector for its shape symbol and colour symbol,\nallowing the GNN to reason over both attributes explicitly.  A two-layer GCN\nfollowed by global mean-pooling and a linear classifier predicts the sequence\nlabel; training uses cross-entropy loss.  After every epoch we evaluate on the\ndev split, reporting loss and our headline Dual-Weighted Accuracy (DWA), the\nmean of the provided CWA and SWA metrics.  All metrics, losses, predictions and\nground\u2013truth labels are stored in the required experiment_data structure and\npersisted under ./working.  At the end we also evaluate on the hidden test split\nand save results.  The whole script obeys the specified GPU/CPU handling rules\nand runs within a few minutes on a single GPU/CPU.  This baseline establishes a\nclean, end-to-end pipeline on which we can iterate with more sophisticated\nrelational inductive biases later.", "We first load the SPR_BENCH splits (or fall back to synthetically-generated data\nif they are not found).   Each sequence is converted into a PyTorch-Geometric\ngraph whose nodes are the sequence tokens; bidirectional edges link consecutive\ntokens.   A global token-vocabulary is built from the training split and each\ntoken id is stored as the node feature (to be embedded later).   A lightweight\ntwo-layer GCN with a learnable token-embedding and global mean-pooling performs\ngraph-level classification.   We train for a few epochs, printing validation\nloss and computing Color-Weighted Accuracy (CWA), Shape-Weighted Accuracy (SWA)\nand their mean Dual-Weighted Accuracy (DWA) after every epoch.   All\nlosses/metrics as well as test predictions are stored inside the required\nexperiment_data dictionary and finally saved to disk.   The code respects the\nmandatory GPU/CPU handling rules, keeps everything runnable in a single file,\nand finishes within minutes on modest hardware.", "We will convert each input sequence into a simple chain-graph whose nodes are\nthe two-character tokens and whose edges connect consecutive positions, then\ntrain a lightweight GCN to predict the hidden label.   Tokens are first mapped\nto integer ids and embedded; two stacked GCNConv layers followed by global mean\npooling and a linear classifier form the model.   The provided helper functions\nload the SPR_BENCH splits; if the files are missing we fall back to a tiny\nsynthetic dataset so the script is fully self-contained.   During training we\niterate for a small number of epochs, compute loss with cross-entropy, and after\nevery epoch evaluate on the dev set, printing validation loss and the Dual-\nWeighted Accuracy (mean of CWA and SWA).   All tensors and the model are moved\nto GPU when available, adhering to the required device handling rules.\nMetrics, losses, predictions, and ground-truth labels are stored in the\nprescribed experiment_data dictionary and saved to disk at the end of the run.\nThe code is laid out as a single file that executes immediately, creates a\n\u201cworking\u201d directory for outputs, and finishes comfortably within the time limit.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, json, math, random, numpy as np, torch\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport torch.nn.functional as F\n\n# ---------- mandatory working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data structure ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\n# ---------- helper: dataset loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- data path ----------\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_PATH\", \"SPR_BENCH\"))\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        \"SPR_BENCH directory not found. \"\n        \"Set env var SPR_PATH or place folder in working directory.\"\n    )\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr)\n\n# ---------- build vocab for shapes & colors ----------\nshapes, colors = set(), set()\nfor row in spr[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        if len(tok) >= 2:\n            shapes.add(tok[0])\n            colors.add(tok[1])\nshapes = sorted(list(shapes))\ncolors = sorted(list(colors))\nshape2idx = {s: i for i, s in enumerate(shapes)}\ncolor2idx = {c: i for i, c in enumerate(colors)}\nD_shape, D_color = len(shapes), len(colors)\nfeat_dim = D_shape + D_color  # one-hot length\n\nprint(f\"Shape types: {D_shape}, Color types: {D_color}, feature dim: {feat_dim}\")\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    # node features\n    xs = []\n    for tok in tokens:\n        if len(tok) >= 2:\n            s_idx = shape2idx[tok[0]]\n            c_idx = color2idx[tok[1]]\n        else:  # fallback\n            s_idx = c_idx = 0\n        feat = torch.zeros(feat_dim, dtype=torch.float)\n        feat[s_idx] = 1.0\n        feat[D_shape + c_idx] = 1.0\n        xs.append(feat)\n    if n == 0:\n        xs = [torch.zeros(feat_dim)]\n        n = 1\n    x = torch.stack(xs, dim=0)\n    # edges (i<->i+1)\n    idx_src, idx_dst = [], []\n    for i in range(n - 1):\n        idx_src += [i, i + 1]\n        idx_dst += [i + 1, i]\n    if len(idx_src) == 0:  # single node graph\n        idx_src, idx_dst = [0], [0]\n    edge_index = torch.tensor([idx_src, idx_dst], dtype=torch.long)\n    y = torch.tensor([label], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef build_graph_list(dset_split):\n    graphs = []\n    for row in dset_split:\n        graphs.append(seq_to_graph(row[\"sequence\"], row[\"label\"]))\n    return graphs\n\n\ngraphs_train = build_graph_list(spr[\"train\"])\ngraphs_dev = build_graph_list(spr[\"dev\"])\ngraphs_test = build_graph_list(spr[\"test\"])\n\nnum_classes = len(set([int(r[\"label\"]) for r in spr[\"train\"]]))\nprint(\"Num classes:\", num_classes)\n\n# ---------- DataLoaders ----------\nbatch_size = 64\ntrain_loader = DataLoader(graphs_train, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(graphs_dev, batch_size=batch_size)\ntest_loader = DataLoader(graphs_test, batch_size=batch_size)\n\n\n# ---------- model ----------\nclass GCN(torch.nn.Module):\n    def __init__(self, in_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.lin = torch.nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GCN(feat_dim, 64, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# ---------- training loop ----------\nepochs = 5\n\n\ndef run_loader(loader, train=False):\n    total_loss, all_pred, all_y, all_seq = 0.0, [], [], []\n    if train:\n        model.train()\n    else:\n        model.eval()\n    for batch in loader:\n        batch = batch.to(device)\n        if train:\n            optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        if train:\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n        preds = out.argmax(dim=1).detach().cpu().tolist()\n        ys = batch.y.detach().cpu().tolist()\n        all_pred.extend(preds)\n        all_y.extend(ys)\n        # recover sequences for metrics\n        seqs = [\n            (\n                spr[\"train\"][i][\"sequence\"]\n                if train\n                else (\n                    spr[\"dev\"][i][\"sequence\"]\n                    if loader is val_loader\n                    else spr[\"test\"][i][\"sequence\"]\n                )\n            )\n            for i in batch.ptr[:-1].cpu().tolist()\n        ]\n        # above pointer indexing not straightforward; fall back to dummy list\n    avg_loss = total_loss / len(loader.dataset)\n    return avg_loss, all_y, all_pred\n\n\ndef eval_metrics(seqs, y_true, y_pred):\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    return 0.5 * (cwa + swa)\n\n\n# for metric computation we need sequences again; easiest: store them list\ntrain_seqs = [row[\"sequence\"] for row in spr[\"train\"]]\nval_seqs = [row[\"sequence\"] for row in spr[\"dev\"]]\ntest_seqs = [row[\"sequence\"] for row in spr[\"test\"]]\n\nfor epoch in range(1, epochs + 1):\n    t0 = time.time()\n    tr_loss, tr_y, tr_pred = run_loader(train_loader, train=True)\n    val_loss, val_y, val_pred = run_loader(val_loader, train=False)\n\n    tr_dwa = dual_weighted_accuracy(train_seqs, tr_y, tr_pred)\n    val_dwa = dual_weighted_accuracy(val_seqs, val_y, val_pred)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_dwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_dwa)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n        f\"val_DWA={val_dwa:.4f}, time={time.time()-t0:.1f}s\"\n    )\n\n# ---------- test evaluation ----------\n_, test_y, test_pred = run_loader(test_loader, train=False)\ntest_dwa = dual_weighted_accuracy(test_seqs, test_y, test_pred)\nprint(f\"\\nTest Dual-Weighted Accuracy (DWA): {test_dwa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_y\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Experiment data saved to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# evaluation helpers (given)\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) if sum(w) else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) if sum(w) else 1)\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ------------------------------------------------------------------\n# dataset loader from provided spec\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(file):  # wrap csv as a split\n        return load_dataset(\n            \"csv\", data_files=str(root / file), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if needed\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------------\n# Build vocabularies\nshapes, colors, labels = set(), set(), set()\nfor ex in spr[\"train\"]:\n    for tok in ex[\"sequence\"].split():\n        if tok:\n            shapes.add(tok[0])\n            colors.add(tok[1])\n    labels.add(ex[\"label\"])\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncolor2id = {c: i for i, c in enumerate(sorted(colors))}\nlabel2id = {l: i for i, l in enumerate(sorted(labels))}\nfeat_dim = len(shape2id) + len(color2id)\nprint(f\"found {len(shape2id)} shapes, {len(color2id)} colors, {len(label2id)} labels\")\n\n\n# ------------------------------------------------------------------\n# helper to build graph\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    n = len(toks)\n    x = torch.zeros((n, feat_dim), dtype=torch.float)\n    for i, tok in enumerate(toks):\n        if not tok:\n            continue\n        x[i, shape2id[tok[0]]] = 1.0\n        x[i, len(shape2id) + color2id[tok[1]]] = 1.0\n    edges = []\n    for i in range(n - 1):\n        edges.append([i, i + 1])\n        edges.append([i + 1, i])\n    edge_index = (\n        torch.tensor(edges, dtype=torch.long).t().contiguous()\n        if edges\n        else torch.empty((2, 0), dtype=torch.long)\n    )\n    y = torch.tensor([label2id[label]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y, seq=seq)\n\n\n# ------------------------------------------------------------------\n# Build PyG datasets\ndef make_dataset(split):\n    return [seq_to_graph(ex[\"sequence\"], ex[\"label\"]) for ex in spr[split]]\n\n\ntrain_graphs = make_dataset(\"train\")\ndev_graphs = make_dataset(\"dev\")\ntest_graphs = make_dataset(\"test\")\n\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128)\ntest_loader = DataLoader(test_graphs, batch_size=128)\n\n\n# ------------------------------------------------------------------\n# Model\nclass GCNClassifier(nn.Module):\n    def __init__(self, in_dim, hidden, num_cls):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, hidden)\n        self.conv2 = GCNConv(hidden, hidden)\n        self.lin = nn.Linear(hidden, num_cls)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = torch.relu(self.conv1(x, edge_index))\n        x = torch.relu(self.conv2(x, edge_index))\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\nmodel = GCNClassifier(feat_dim, 64, len(label2id)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ------------------------------------------------------------------\n# experiment data storage\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_DWA\": [], \"val_DWA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ------------------------------------------------------------------\n# training loop\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    avg_train_loss = total_loss / len(train_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(avg_train_loss)\n\n    # evaluate on dev\n    model.eval()\n    preds, gts, seqs = [], [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            p = out.argmax(dim=-1).cpu().tolist()\n            preds.extend(p)\n            gts.extend(batch.y.cpu().tolist())\n            seqs.extend(batch.seq)\n\n    avg_val_loss = val_loss / len(dev_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(avg_val_loss)\n\n    train_DWA = None  # optional; can compute similarly if desired\n    val_DWA = dual_weighted_accuracy(seqs, gts, preds)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_DWA\"].append(val_DWA)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}  val_DWA={val_DWA:.4f}\"\n    )\n\n# ------------------------------------------------------------------\n# final evaluation on test\nmodel.eval()\npreds, gts, seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        out = model(batch)\n        p = out.argmax(dim=-1).cpu().tolist()\n        preds.extend(p)\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq)\ntest_cwa = color_weighted_accuracy(seqs, gts, preds)\ntest_swa = shape_weighted_accuracy(seqs, gts, preds)\ntest_dwa = 0.5 * (test_cwa + test_swa)\nprint(f\"TEST: CWA={test_cwa:.4f}  SWA={test_swa:.4f}  DWA={test_dwa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, string, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- basic set-up ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- util: metrics ----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(1, sum(w))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(1, sum(w))\n\n\n# ---------------- util: load spr_bench ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\n# ---------------- synthetic fallback ----------------\ndef synthesize_dataset(n):\n    shapes, colors = list(string.ascii_uppercase[:6]), list(\"1234\")\n    seqs, labels = [], []\n    for _ in range(n):\n        ln = random.randint(3, 8)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(ln))\n        seqs.append(seq)\n        labels.append(random.randint(0, 1))\n    return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n\ndef get_dataset():\n    data_root = pathlib.Path(os.getcwd()) / \"SPR_BENCH\"\n    if data_root.exists():\n        try:\n            return load_spr_bench(data_root)\n        except Exception as e:\n            print(f\"Problem loading real data ({e}). Falling back to synthetic.\")\n    print(\"Real dataset not found \u2013 generating synthetic SPR data.\")\n    train = synthesize_dataset(2000)\n    dev = synthesize_dataset(500)\n    test = synthesize_dataset(500)\n    return DatasetDict(\n        {\n            \"train\": load_dataset(\"json\", data_files={\"train\": train}, split=\"train\"),\n            \"dev\": load_dataset(\"json\", data_files={\"dev\": dev}, split=\"train\"),\n            \"test\": load_dataset(\"json\", data_files={\"test\": test}, split=\"train\"),\n        }\n    )\n\n\nspr = get_dataset()\nnum_classes = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Classes: {num_classes}\")\n\n# ---------------- token vocab ----------------\ntoken_set = set()\nfor seq in spr[\"train\"][\"sequence\"]:\n    token_set.update(seq.split())\ntoken2idx = {tok: i for i, tok in enumerate(sorted(token_set))}\nvocab_size = len(token2idx)\nprint(f\"Token vocab size: {vocab_size}\")\n\n\n# ---------------- seq -> graph ----------------\ndef seq_to_graph(seq, y):\n    tokens = seq.split()\n    node_ids = torch.tensor([token2idx[tok] for tok in tokens], dtype=torch.long)\n    # edges: consecutive + reverse\n    src = torch.arange(len(tokens) - 1, dtype=torch.long)\n    dst = src + 1\n    edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    return Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([y], dtype=torch.long),\n    )\n\n\ntrain_graphs = [\n    seq_to_graph(s, y) for s, y in zip(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, y) for s, y in zip(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, y) for s, y in zip(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"])\n]\n\n\n# ---------------- model ----------------\nclass SPRGNN(nn.Module):\n    def __init__(self, vocab, emb_dim=32, h_dim=64, classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim)\n        self.gcn1 = GCNConv(emb_dim, h_dim)\n        self.gcn2 = GCNConv(h_dim, h_dim)\n        self.lin = nn.Sequential(\n            nn.Linear(h_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, classes)\n        )\n\n    def forward(self, data):\n        x = self.emb(data.x.squeeze()).to(device)\n        x = self.gcn1(x, data.edge_index)\n        x = torch.relu(x)\n        x = self.gcn2(x, data.edge_index)\n        x = torch.relu(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nmodel = SPRGNN(vocab_size, classes=num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ---------------- loaders ----------------\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=128, shuffle=False)\n\n# ---------------- experiment data dict ----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------------- training loop ----------------\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    train_loss = total_loss / len(train_loader.dataset)\n\n    # validation\n    model.eval()\n    val_loss, preds, truths, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch, raw in zip(dev_loader, spr[\"dev\"][\"sequence\"]):\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds.extend(out.argmax(dim=1).cpu().tolist())\n            truths.extend(batch.y.cpu().tolist())\n        val_loss /= len(dev_loader.dataset)\n    cwa = color_weighted_accuracy(spr[\"dev\"][\"sequence\"], truths, preds)\n    swa = shape_weighted_accuracy(spr[\"dev\"][\"sequence\"], truths, preds)\n    dwa = (cwa + swa) / 2\n\n    print(\n        f\"Epoch {epoch:02d}: train_loss={train_loss:.4f}, validation_loss = {val_loss:.4f}, \"\n        f\"CWA={cwa:.3f}, SWA={swa:.3f}, DWA={dwa:.3f}  [{time.time()-t0:.1f}s]\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"epoch\": epoch, \"dwa\": None}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"epoch\": epoch, \"dwa\": dwa})\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        {\"epoch\": epoch, \"loss\": train_loss}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        {\"epoch\": epoch, \"loss\": val_loss}\n    )\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ---------------- testing ----------------\nmodel.eval()\ntest_preds, test_truths = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        out = model(batch)\n        test_preds.extend(out.argmax(dim=1).cpu().tolist())\n        test_truths.extend(batch.y.cpu().tolist())\n\ncwa_test = color_weighted_accuracy(spr[\"test\"][\"sequence\"], test_truths, test_preds)\nswa_test = shape_weighted_accuracy(spr[\"test\"][\"sequence\"], test_truths, test_preds)\ndwa_test = (cwa_test + swa_test) / 2\nprint(f\"\\nTEST   CWA={cwa_test:.3f}, SWA={swa_test:.3f}, DWA={dwa_test:.3f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_truths\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"CWA\": cwa_test,\n    \"SWA\": swa_test,\n    \"DWA\": dwa_test,\n}\n\n# ---------------- save everything ----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        # synthetic fallback\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes = \"ABC\"\n            colors = \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[tok] for tok in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(n - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(seq, lbl) for seq, lbl in zip(split[\"sequence\"], split[\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = self.conv1(x, data.edge_index)\n        x = torch.relu(x)\n        x = self.conv2(x, data.edge_index)\n        x = torch.relu(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nmodel = GCN(vocab_size, num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * batch.num_graphs\n    train_loss /= len(train_graphs)\n\n    # validation\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    val_loss /= len(dev_graphs)\n    dwa = dual_weighted_accuracy(all_seqs, all_labels, all_preds)\n\n    # logging\n    print(\n        f\"Epoch {epoch}: training_loss = {train_loss:.4f}, validation_loss = {val_loss:.4f}, DWA = {dwa:.4f}\"\n    )\n    ts = time.time()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((ts, train_loss))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((ts, val_loss))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((ts, dwa))\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(ts)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        # synthetic fallback\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes = \"ABC\"\n            colors = \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[tok] for tok in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(n - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(seq, lbl) for seq, lbl in zip(split[\"sequence\"], split[\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = self.conv1(x, data.edge_index)\n        x = torch.relu(x)\n        x = self.conv2(x, data.edge_index)\n        x = torch.relu(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nmodel = GCN(vocab_size, num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * batch.num_graphs\n    train_loss /= len(train_graphs)\n\n    # validation\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    val_loss /= len(dev_graphs)\n    dwa = dual_weighted_accuracy(all_seqs, all_labels, all_preds)\n\n    # logging\n    print(\n        f\"Epoch {epoch}: training_loss = {train_loss:.4f}, validation_loss = {val_loss:.4f}, DWA = {dwa:.4f}\"\n    )\n    ts = time.time()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((ts, train_loss))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((ts, val_loss))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((ts, dwa))\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(ts)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        # synthetic fallback\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes = \"ABC\"\n            colors = \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[tok] for tok in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(n - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(seq, lbl) for seq, lbl in zip(split[\"sequence\"], split[\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = self.conv1(x, data.edge_index)\n        x = torch.relu(x)\n        x = self.conv2(x, data.edge_index)\n        x = torch.relu(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nmodel = GCN(vocab_size, num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * batch.num_graphs\n    train_loss /= len(train_graphs)\n\n    # validation\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    val_loss /= len(dev_graphs)\n    dwa = dual_weighted_accuracy(all_seqs, all_labels, all_preds)\n\n    # logging\n    print(\n        f\"Epoch {epoch}: training_loss = {train_loss:.4f}, validation_loss = {val_loss:.4f}, DWA = {dwa:.4f}\"\n    )\n    ts = time.time()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((ts, train_loss))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((ts, val_loss))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((ts, dwa))\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(ts)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        # synthetic fallback\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes = \"ABC\"\n            colors = \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[tok] for tok in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(n - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(seq, lbl) for seq, lbl in zip(split[\"sequence\"], split[\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = self.conv1(x, data.edge_index)\n        x = torch.relu(x)\n        x = self.conv2(x, data.edge_index)\n        x = torch.relu(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nmodel = GCN(vocab_size, num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * batch.num_graphs\n    train_loss /= len(train_graphs)\n\n    # validation\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    val_loss /= len(dev_graphs)\n    dwa = dual_weighted_accuracy(all_seqs, all_labels, all_preds)\n\n    # logging\n    print(\n        f\"Epoch {epoch}: training_loss = {train_loss:.4f}, validation_loss = {val_loss:.4f}, DWA = {dwa:.4f}\"\n    )\n    ts = time.time()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((ts, train_loss))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((ts, val_loss))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((ts, dwa))\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(ts)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 74, in <module>\\n    raise\nFileNotFoundError(\\nFileNotFoundError: SPR_BENCH directory not found. Set env\nvar SPR_PATH or place folder in working directory.\\n', 'Execution time: 2\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 60, in <module>\\n    spr = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 53, in load_spr_bench\\n\nd[\"train\"] = _load(\"train.csv\")\\n                 ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 48, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-2/SPR_BENCH/train.csv\\'\\n',\n'Execution time: 2 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Real dataset not found \u2013 generating synthetic SPR\ndata.', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line 84,\nin <module>\\n    spr = get_dataset()\\n          ^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 77, in get_dataset\\n    \"train\": load_dataset(\"json\",\ndata_files={\"train\": train}, split=\"train\"),\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 344, in resolve_pattern\\n    if\nis_relative_path(pattern):\\n       ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/file_utils.py\", line 88, in is_relative_path\\n    return\nurlparse(url_or_filename).scheme == \"\" and not os.path.isabs(url_or_filename)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py\",\nline 394, in urlparse\\n    url, scheme, _coerce_result = _coerce_args(url,\nscheme)\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py\",\nline 133, in _coerce_args\\n    return _decode_args(args) + (_encode_result,)\\n\n^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py\",\nline 117, in _decode_args\\n    return tuple(x.decode(encoding, errors) if x else\n\\'\\' for x in args)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py\",\nline 117, in <genexpr>\\n    return tuple(x.decode(encoding, errors) if x else\n\\'\\' for x in args)\\n                 ^^^^^^^^\\nAttributeError: \\'dict\\' object\nhas no attribute \\'decode\\'\\n', 'Execution time: 2 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 538912.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 657270.19\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 722545.44\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Epoch 1: training_loss = 0.4599, validation_loss\n= 0.4034, DWA = 0.8367', '\\n', 'Epoch 2: training_loss = 0.3646, validation_loss\n= 0.3432, DWA = 0.8741', '\\n', 'Epoch 3: training_loss = 0.3209, validation_loss\n= 0.2977, DWA = 0.8959', '\\n', 'Epoch 4: training_loss = 0.2836, validation_loss\n= 0.2824, DWA = 0.9078', '\\n', 'Epoch 5: training_loss = 0.2564, validation_loss\n= 0.2475, DWA = 0.9160', '\\n', 'Execution time: 27 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 583478.22\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 495991.67\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 660489.11\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Epoch 1: training_loss = 0.4621, validation_loss\n= 0.3922, DWA = 0.8410', '\\n', 'Epoch 2: training_loss = 0.3728, validation_loss\n= 0.3606, DWA = 0.8626', '\\n', 'Epoch 3: training_loss = 0.3310, validation_loss\n= 0.3138, DWA = 0.8875', '\\n', 'Epoch 4: training_loss = 0.2969, validation_loss\n= 0.2867, DWA = 0.9052', '\\n', 'Epoch 5: training_loss = 0.2717, validation_loss\n= 0.2557, DWA = 0.9138', '\\n', 'Execution time: 17 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 505155.25\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 531233.88\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 684337.41\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Epoch 1: training_loss = 0.4685, validation_loss\n= 0.4040, DWA = 0.8292', '\\n', 'Epoch 2: training_loss = 0.3794, validation_loss\n= 0.3604, DWA = 0.8561', '\\n', 'Epoch 3: training_loss = 0.3383, validation_loss\n= 0.3235, DWA = 0.8747', '\\n', 'Epoch 4: training_loss = 0.2994, validation_loss\n= 0.2894, DWA = 0.8934', '\\n', 'Epoch 5: training_loss = 0.2746, validation_loss\n= 0.2704, DWA = 0.9120', '\\n', 'Execution time: 12 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 592759.08\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 687297.87\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 791497.58\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Epoch 1: training_loss = 0.4685, validation_loss\n= 0.4040, DWA = 0.8292', '\\n', 'Epoch 2: training_loss = 0.3794, validation_loss\n= 0.3602, DWA = 0.8563', '\\n', 'Epoch 3: training_loss = 0.3381, validation_loss\n= 0.3232, DWA = 0.8750', '\\n', 'Epoch 4: training_loss = 0.2992, validation_loss\n= 0.2894, DWA = 0.8923', '\\n', 'Epoch 5: training_loss = 0.2746, validation_loss\n= 0.2704, DWA = 0.9123', '\\n', 'Execution time: 13 seconds seconds (time limit\nis 30 minutes).']", ""], "analysis": ["The execution failed due to a missing dataset directory 'SPR_BENCH'. The script\nrequires the SPR_BENCH directory to be present in the working directory or its\npath to be set using the SPR_PATH environment variable. To fix this, ensure that\nthe SPR_BENCH directory is correctly placed in the working directory, or set the\nSPR_PATH environment variable to the correct path of the SPR_BENCH directory\nbefore running the script.", "The execution failed because the script could not locate the dataset file\n'train.csv' in the specified path '/home/zxl240011/AI-Scientist-v2/SPR_BENCH/'.\nThis suggests either the dataset is missing, the path is incorrect, or the\ndirectory structure does not match the expected layout. To fix this, ensure that\nthe dataset files ('train.csv', 'dev.csv', 'test.csv') exist in the specified\ndirectory and that the path is correctly set in the script. If the dataset is\nlocated elsewhere, update the DATA_PATH variable to the correct location.", "The error occurs because the `load_dataset` function is incorrectly provided\nwith a dictionary (`train`, `dev`, and `test`) instead of file paths or URLs.\nThe `load_dataset` function expects a file path or dataset format, not a Python\ndictionary. To fix this, the synthetic dataset generated by `synthesize_dataset`\nshould first be saved to temporary JSON files, and then these file paths should\nbe passed to the `load_dataset` function. For example, use `json.dump` to save\nthe synthetic data to files and provide their paths to `load_dataset`.", "", "", "", "", ""], "exc_type": ["FileNotFoundError", "FileNotFoundError", "AttributeError", null, null, null, null, null], "exc_info": [{"args": ["SPR_BENCH directory not found. Set env var SPR_PATH or place folder in working directory."]}, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-2/SPR_BENCH/train.csv'"]}, {"args": ["'dict' object has no attribute 'decode'"], "name": "decode", "obj": "{'id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999], 'sequence': ['B1 D3 F1', 'A4 F3 B2 F3 D1 F2 D3 F3', 'E1 E2 E1 A3 A2 A4', 'C3 B3 E2 B4', 'C3 F2 E4 E1', 'D4 A4 B1 B3 D4', 'D1 E3 A4', 'D3 D3 D3', 'C3 A4 D3 B4 A2', 'E4 F2 B4', 'C3 E4 D1 C4', 'A3 A2 C1 A2 D3', 'A4 E4 A1 A2 C1 D3 D4 E1', 'C4 F2 B1 D2 A3 F3 C4', 'E3 E3 A3 B1 B1', 'E2 F1 A4', 'D4 A4 E1 F1 B1 C1 B3', 'B3 A3 C3 D2 D1', 'D1 A4 C3 D4 E3 E4 B3 D1', 'A1 A1 E3 B1 E4', 'A1 D4 B4 E2 F1 C3 D4 A1', 'E4 C2 E4 E3 C4 D2 A3', 'F1 B2 C4 F1 A2 F4 F4 A2', 'F2 C4 F2 C1 E4', 'A3 B1 E3 C4 A1 E1', 'C1 B2 B3', 'C2 D4 D3', 'A3 E4 B4 B3 D3 C1 A1 E3', 'B1 E4 E2 F1', 'B2 D3 C3', 'C2 A1 B4 F2 F2', 'F3 C4 E4 F4 A3 D4 A2', 'A4 F2 A3 E2 C2 D1', 'A2 D2 D1 B1 C1 F4', 'F2 A3 B1 B4 E4', 'C3 B2 C4 E3 E3', 'B1 F3 B2 D2', 'C4 F2 B3 B1 D3 A1 A4 C2', 'E1 B3 A2 D4', 'A1 E3 D4 D4', 'E4 D4 E2 A1 C2', 'F1 F1 D3 D4 E1 A1 D1 C2', 'E4 B2 F1', 'B4 C2 C4', 'B1 B1 F1', 'C1 A2 E1 F4 A2', 'F4 C2 A1', 'F3 E4 B1', 'E3 B3 A3 F4 C3', 'F4 D4 A1 D2 B3 D2 A3', 'D3 A4 A4', 'A3 C2 B3 A1 D2', 'B4 D2 A3 C3 C4 F4 C1', 'B4 C2 F4 C3', 'C2 C2 B4 A4 F2 D2 E3', 'A1 C4 F3 B2 E4 B1 A2', 'E2 C4 A1 E3 C1 C3 C3 E1', 'E3 D3 D2 F1 F4 E4', 'F3 A3 A2 E1 F2', 'D1 A3 D1 D2 E4 E2 E2 E4', 'C4 B2 E3 D2 F4 B1', 'C4 B2 A3', 'A2 C2 F1 A2 A3 F4 F4 A3', 'A3 D1 C2', 'C4 C3 A4 C4', 'F2 A1 D2 C2 D1 B3 F1 B4', 'B2 A2 E3', 'E1 D3 F4 C2 D3 C3 D2 D1', 'E1 B3 B1 C4 C4 E3', 'B3 D2 B2', 'B4 A3 C1 B1 B1 B4 B4 A1', 'E2 E1 E1 D4 F3 D2', 'A3 E4 D3 F1 A1 D1', 'E4 C2 F4', 'E3 E4 A2', 'D4 D3 E3 E3', 'E2 C2 D4 D3 B4', 'D4 D2 F4 E1 C4 D1 E3', 'C1 B1 D3 D3 E1', 'F3 C2 D3', 'B4 E1 E1', 'A4 A2 E1 F4 C1 E3', 'E1 A2 E3', 'B2 E1 F4 C1', 'F2 F2 F2 F3 D4 F3 F2', 'B1 D4 F2 B4', 'B3 F1 A2 D4 F2 B4', 'D3 B4 C1', 'E1 D3 F3 A4', 'D4 A4 D2 E2 C2', 'A3 A2 E1 F1 D1', 'C2 B2 A3 D1 E3 A1 C3', 'E3 D3 B1 C1', 'B1 D1 D4 E2 F4', 'A3 E4 D2 A4 F4 E3 A2', 'B2 F1 A4 A1 C1 D1', 'D1 B3 D1 C1 E3 A4 A2 E4', 'C1 F1 F3 D2', 'D1 A2 F2 A1 A2 D1 D1 D1', 'E2 F4 E3 A4 E4 B3 A1', 'A1 B4 E3 A1', 'E2 B1 E1', 'A4 C2 C1 C3', 'A3 A1 C3 A2 C1 C4 F4 B4', 'D1 E1 A4 E1 B1 E3 A1 B2', 'B4 A3 F4 A1 F2 B4 F3 C4', 'C1 E2 A2 F1 B4 D2 B4 D2', 'C3 D2 F2', 'D2 D4 C3 C4 A2', 'B2 B1 B2 D2 E4', 'C3 E1 C1 C3 D2 A3 F4 C1', 'E4 F1 C3 A3 D2 D3', 'C4 F4 A4 B2 D2 B4 B4', 'C4 F1 D3 B3 B1 D2 A3', 'B2 B1 B1 F2 B1 D3', 'B1 D1 E2', 'A4 D1 A4 E1 F1 D3', 'C1 F4 C3 C3 B3 C2', 'B1 D2 E3 C4 E3', 'D2 A4 C4 D3', 'F2 B1 B1', 'B3 E3 E3 E4 A4 C3 F2 D2', 'C4 E1 A3 E4 E3', 'F1 C3 F2', 'F3 F2 F2 B3 F2 D3 F4', 'F4 A3 A4 A3 B2', 'E1 A1 C2 A2 A4 F3 B3', 'D4 D4 E2 A1', 'C4 C3 F4 A3', 'B4 D3 D3 F3', 'C1 B1 D2', 'D2 C4 F3 E3 B1 C2 A2 C4', 'C4 C3 A3 F3 D2 B2 B4 B2', 'F4 A1 E3 B1 F1 E1', 'B2 B1 D3 F2', 'E3 E3 B4 E3 A2', 'B4 B2 B3 A4', 'F1 D4 F4', 'D4 A3 D4', 'C4 C3 F3 C4', 'D4 F1 F4 E1 C4 E3 B4 A3', 'A2 A4 E2 F4 A2 B3', 'A4 A3 B1 B3 F1', 'F4 B3 E4 C4 B2 C4 F3', 'E2 E2 C2 B2 D3 F2', 'D3 D1 E2 B4 E1 E4 C3', 'C1 B2 D1 A1 B4 C3 E2 B2', 'D4 F1 E1 A1', 'D1 E2 E3 E1 C3 D1 B2 B3', 'E1 A3 B2 E3 A3 E2', 'B1 E1 F4 A4 F3 A1', 'B1 D3 D1 C3 D4 F4', 'E3 D3 B4', 'A3 E4 F3 C3 C4 E2', 'A3 F4 F2 B4', 'B4 B3 D2 A3 F3 E3', 'D1 F1 B2', 'A1 C2 F4 C1 D4 F3', 'D1 B2 D1 D2 C1 D3 A1 D4', 'A3 B2 C1 F2 D1', 'A3 F2 A1 E3 E3 C1 B4', 'A3 A4 B1 B1 F3', 'D3 D2 B2 D1 C1 E4', 'E1 A3 F3', 'F2 A4 D1 F2 C4', 'A1 F4 A1 B2 B2 A1 E4', 'F2 C1 F2 C3 B3 C4 C2', 'F2 A2 A1 E4 A4', 'F4 B3 E3 C1', 'A1 A3 F1', 'F2 B2 D3', 'A2 C2 F3', 'D3 B1 F3 D3 D2', 'F4 B2 D3', 'C2 C2 D4 A3 F2 D3', 'D1 E1 E3', 'A4 B4 F2 A2 F3 C1 C1', 'E2 A3 B4 B1 B2 D2 D2', 'D4 E2 B4 E3 B1', 'B1 C2 D2', 'B4 B2 B2', 'A2 E1 B1 E3', 'A1 B2 D4', 'E3 C3 E2', 'B1 D3 C3 F2', 'F2 B1 A4 C1 B4 D2', 'A2 F3 D2 C3 C3', 'E3 D1 F3 E4', 'C4 D2 E3 D1 D3', 'D3 D1 A4 A1', 'B2 D2 F1', 'D3 C1 E3 B1 C2 C4', 'A2 D1 F3 E3 A3', 'C1 B1 A4', 'D4 A1 E1 E2 C1 D1 B3 B1', 'D3 D1 D4 E4 D1', 'A2 E3 B3 D4', 'A2 E1 F4 F3 B1 C3', 'A4 F1 D4 D4 D2 C1 D3', 'E4 D1 F3 A3 C2 B1 C2 D1', 'B1 B2 A1 D1 C1 D3 E2 B2', 'D3 E1 A1 A4', 'C2 C3 A3', 'B2 E4 F2 B2 B1', 'A4 C4 C3 F2 C2 B2 B1 A1', 'F1 E2 A2 A1', 'A1 A1 E4 F2 F1', 'F1 D3 F2 E2 C2 E1 C3', 'B4 C3 C4 F4 D4 D3 E4', 'F4 F4 D1 F4 E4 C2 D4 A1', 'F3 D3 E1 C3 B1', 'E2 B2 E1 D4 F4 C3 A4 D3', 'B1 C4 C4 F2 A2 C1', 'B3 D1 C2 B2', 'F4 A3 F3 C4 F1 A4 A1', 'B2 D4 D2 E3 F3', 'A1 E3 B4 D2', 'F3 E3 E3 A2 B2', 'D1 A4 E1 B2 C2 F4 A3 A1', 'B1 F4 D4 E2 B2 D3 F4', 'F1 B4 C2', 'F2 F4 F1 D2', 'C2 E1 A1 C2', 'F3 D2 C2', 'E3 D3 A4', 'B3 C4 E3 A4 A2 A4 C2', 'C2 C4 D4 F2 E4 E1 E3', 'A4 F2 E1 C4 F4 A4', 'B2 B3 D3', 'D1 D3 A4 B1 A4 E4 B3', 'B3 D4 E3 B1', 'F2 A3 C4 B4 D4 E3 E4', 'B1 A2 E2 A2 F2 D4 F4', 'C3 C2 F1 C1 A2', 'D3 C2 D1', 'B4 C3 A2 F1 C2', 'F4 F1 C3 F3 E3 C1', 'C1 B1 C4 F3 C2 C3', 'D3 F3 F1 D1 C2 A1', 'C3 F1 D3 F3 E1', 'A4 F3 D3 A4 B4 A4 C2 C4', 'B3 D1 A4 F2 F1 E2', 'D1 B4 E1 A1 E2', 'A1 A1 D4', 'E2 E2 C3 A2 A3 B2 E4', 'E1 D2 F4 A3 C4 D2 B2', 'B2 C3 A1 F2 C1 A1 E4 C4', 'A1 C1 B4 C1 D4 B3 B3 A2', 'C3 D1 D4 B1', 'B4 D1 E3 A3 D3 F1 F2', 'A4 B2 C4 E2', 'A2 B3 D3', 'E1 D2 B4 B2 E3', 'E3 F1 D3 D3 E3 B3', 'C1 C4 A2 B2', 'E1 F3 A1 C2', 'C1 C2 B4 E4 B2', 'D2 B1 B1 D3 A1 F4 C3', 'B1 D4 F4 B4', 'F4 D1 D2 A2 D1', 'A2 D3 E3 C2 D3 A3 F4 D3', 'B1 A3 C3 A4 C4 F2 E2', 'E4 F2 B4 E3 F3 A4 A1 B1', 'D2 E2 C2 B4 A1', 'B2 A3 A1', 'C2 D2 F2 B4 B3 A2', 'B3 F2 B4 F2 B2 D4 A3 B4', 'C1 F3 B3 E1 A3', 'A2 C4 F1 C4 C1', 'C2 D1 D2 F3 A2 A4 C1 E4', 'E4 A1 E1', 'B2 C2 A3', 'A2 B2 F4 E3 B4', 'C2 E2 B1 C4', 'E2 D1 D2 F4 F1', 'D1 A1 A3 E4', 'E4 D4 C3 D1', 'A1 C4 E3 D4', 'B3 C2 B4 D4 A3 B3 A1', 'E4 B1 D2 C4', 'C3 A4 A4 A2 C4', 'F2 C2 D1 F4 C2 C3', 'D1 C3 D2 C3 A3 D3 D3', 'D3 D2 E1 D1 C4 E4', 'D2 A2 A3 B2 F1', 'E2 D2 C1', 'F4 F1 A1 F3 C1 F2 E2', 'E3 C3 D4 D4 A1 C4 F4 E3', 'A3 E2 A2 B1 B4 E3', 'F2 D3 C4 C2', 'D1 F2 A4 A3 D2 B2 D4', 'C2 A4 D2 D2 A2', 'B2 D2 B4 B1 E1 C2 E1', 'A3 C1 E2', 'D1 E4 F3 C4 E3 E4', 'B2 A1 B2 F2 E1 C1 F4 F4', 'A2 A2 D2 B3 D3 C2 A2', 'C3 F2 E3 F1 E3', 'A4 D4 E3 B2', 'E2 C2 E3 F4 D1 F2', 'F2 F1 E4 D4', 'C1 D3 D1 D2 E2', 'E4 D4 C2 D1 D3', 'C4 A4 B4 B2 D3 B2', 'A3 D4 E3 C4 E3 B3 C2 E3', 'F2 F1 E2', 'C2 D3 B3 C3 B4 D4 C2', 'F4 C3 F4', 'C1 F1 C2', 'A3 A4 D1', 'B4 B2 B3 D1', 'E2 F1 A1 A3 B2 B1 C3', 'F4 C1 F1 D4', 'D4 F4 C2 E1', 'A4 A1 F3 A3 D2 D3 B1 E1', 'D3 A1 C2 D2 D2 D1 F3 D3', 'E1 D2 E2 D2 D1', 'E1 F3 F1 A3 C2 A2 D1 F4', 'D2 A4 C3 A3', 'C3 E4 E1 A3', 'A3 F1 A4 A2 A3', 'D4 C4 B4 D1', 'B1 C1 F2 C1 B2', 'D4 D2 E2 F3 E1', 'E2 C4 A1 B2', 'A2 A3 E3 B4 C4 C2 F3', 'E1 E4 E3 E2 E2 E3 A4 A1', 'F3 B4 B2 C3 F2 D2 F3', 'A3 E1 F3 F2 B1 E4 E3 B3', 'A3 D2 F3 A2 D2', 'C4 E1 A1', 'B1 A2 D4 B4 C1 C2', 'C2 A4 B2 E3 E3', 'B1 E3 D3 B2 D4 F3', 'A4 B3 C4 D1', 'A4 D1 E1', 'D2 C1 C1 D3', 'F4 B3 F3 C3 A1 E3 D1 A4', 'D1 E1 B3 F1 D3 B2 B2', 'F3 A4 F1 A2 E2 E3 B4 E4', 'E2 C1 D4 E2 D2 A1 B1 E4', 'B4 B4 C1 A1 A3 A3 B1', 'B1 E3 B1 E1', 'E1 C2 E1', 'A3 A4 E3', 'B4 E2 C3', 'C2 D2 A1 E2 C4 E1 E2 E1', 'F4 E4 A3 B1', 'F1 E2 B4 D4', 'B2 A1 E1 E1', 'B2 F4 C2', 'E3 C4 A4', 'E3 D2 F3 F2', 'B1 A4 F3', 'E2 C4 F4', 'D2 B1 F4 A3 F4 F4 F2 B3', 'B3 F2 A2 F4', 'D2 C2 A4 B2 A3', 'D3 D3 D2 D3 B1 A3', 'A1 F3 C4 D4 D2 F1 D4', 'E4 F2 E1 C4 D4 C2', 'F2 E3 A3', 'E3 E4 C4 F4 C4 E3', 'D4 E1 B2 B3 D3', 'B1 B1 E4 A2 E2 A3 F3', 'F1 D4 B3 A3', 'E3 A4 F2 E3 F2 C3 B1 E2', 'C3 B4 A1 F1 E2', 'B4 E3 E3 E2 A4 D4', 'E4 C3 C2', 'A4 F2 F2 A2 B3 F2 A2 A4', 'A2 A3 C4 A4 D2 D1 B3 F1', 'D1 B4 D2', 'C2 D2 A1 E1 B2 A3 C3', 'D3 D3 C1 E1', 'C1 C1 B1 C3 F3 A1 A4 F4', 'B3 A3 B3', 'E4 C2 B1 F3', 'E3 B4 A4 A3 A2 F1 B2', 'C4 D3 E1 F2 E2 F2', 'B3 C1 C1 C3 A4', 'B4 B4 C2 A4 E3 A2 A1 E1', 'C1 F3 D3 B1 D3 F1 D1 C2', 'F4 F3 A4 C1', 'A3 D2 A3 C1 A3 A2 F4 A4', 'B4 B4 E1 B3', 'E2 B4 E4', 'A1 D2 B2 F2 A1 D2 C1', 'C4 C1 D2 D3 E4 B3 F3 C4', 'E1 B2 F1 D1 B1 C1 E1 E1', 'D1 D2 E2 C4 E4', 'B2 D1 D1 F4', 'A1 D3 A4', 'B1 B2 E2', 'A1 D1 D3 B3 B2 A2', 'A1 C2 C4 B3 F2 B3', 'F3 A4 B2 C4 B4', 'E2 B2 C4 D4 D2 C3 A1', 'F1 E4 E3 C3 B1 F1 A4 A4', 'F4 F1 C2 C2 F3', 'F2 B4 C3 B3 C2 C1', 'C1 D1 D2 E1 F2 B4 A4 B2', 'D3 F1 D3 E2 F3 A1', 'E1 F1 A2', 'C3 D3 B3 D4 E4 D1 A4 B3', 'B4 C1 D1 C4 C4 C4', 'C1 B1 A3 D2 B2 C2', 'E2 C1 E3 F4 E1', 'D1 C4 B2 D3 B4', 'E2 E2 D3 A2 A3', 'B4 A2 B4', 'D1 C1 D1 D2 C1 B4 A3', 'B3 A3 F4 C1 B4 B1', 'E2 C4 D2 B4 F4 F4 D2 B4', 'F1 D4 B4 B2 A2 B3 B2 D4', 'C2 F1 A1 E4', 'B2 B2 B4 C2 A3 E3', 'E1 D1 A3 D1 B3 A1', 'A1 B3 D2 D4 E4 A3 F4', 'D4 B3 F3 B4 C4 D3 B2 F4', 'B4 B4 C3 A4 C3 E1 E1 E1', 'A4 C2 D1 F4 B2 D2 A4', 'F4 D1 F3 A3 E2 F3 A1', 'A4 C1 C4 A4 F1 C3', 'D4 E4 F1 F4', 'C3 C2 D1 E2', 'D2 C3 B1 C2 A4 A4', 'D3 F2 A2', 'F3 D1 D1', 'F4 C3 C4 C1', 'F2 D1 D2', 'B3 A1 D2 F1 E1', 'B3 E4 A2', 'B4 E4 C3 B3 B3', 'E3 C3 D3 D2 C2 A4 B1', 'A4 F4 B4', 'A2 E2 D4 D4 B2 A4', 'B1 D3 D3 E1 E4 A2', 'A1 F3 E4', 'A2 D1 D2 E4', 'F2 A3 C4 D3', 'C3 A4 A4 A1 F2 D4', 'E2 A3 B1 C4 A3', 'E4 E4 A2 F4 B4 C4 C3', 'E3 E2 E3 A2 E4', 'E4 B2 D4 A4 C1 A4', 'C1 D4 D2 B3', 'A1 F3 E2 E4 B1 E3 C1 E4', 'B3 B3 D1', 'A1 B2 B4 D4 E4', 'B3 E3 F4 D1', 'C1 A4 F2', 'B3 D1 F2 D4', 'A3 A2 F1 F2 F4 F3', 'B3 A1 D3 C4', 'E3 E4 F2 F3 C1', 'A1 F4 E4 B1 E3 C4 F4', 'C4 C1 D1 B2 D2 C2 F4 A2', 'D1 F3 C4 F3 C3 E2 D3', 'E3 B4 C2 D1 D1 A3', 'D2 E1 D3 A2', 'A1 F2 A1 C2', 'E3 B1 E3 B2 C1', 'D3 B3 A4', 'A3 D1 A2 B2 D2', 'B2 C4 B4 B1 B4', 'A1 B1 A3', 'B2 B4 D2 B4 A3', 'F2 A3 C4 E1 B3', 'F3 A4 C3 E3 A1 C4 F4 C2', 'E2 C1 D3 B4 A3 F2 F3 C1', 'A4 E1 A4 F3 E1', 'F2 C3 D4 C3', 'D1 F4 B1 E3 E2 A2 C1', 'D1 E3 B1', 'B1 A2 F2 E4 B3 D1 E1 F4', 'B4 D1 E1 E1 E2', 'B1 B4 B2 F4 D3 F3', 'E1 C4 B4', 'F1 C2 C2 C3', 'B4 E3 F2', 'C2 E4 C2 D1', 'E3 C4 B1 E1 C1', 'F2 E4 D4', 'A2 B4 E2 C1', 'D1 F4 E3 F4 B4 E3 F3 B4', 'B3 F2 A4 B1 D1 C2 E2', 'B3 B4 C4 E2 E3 F3 F1 A3', 'B4 A3 A4 B2 C3 B2', 'D3 B1 D3', 'E2 B1 C2 B1 C2 B2 C4 C1', 'A3 B3 C4 B3 A1', 'F2 F3 C4 C1 B3 C2', 'D1 E3 B2 E1 A4', 'B1 B2 F1 D3 E3 B2', 'D1 C1 D3 F3 E1 D3 D2', 'A2 A4 A3 E3 A4 A4 B1', 'F3 C4 D2 E4 A3 E3', 'D1 D2 D4 B4', 'F1 E3 B4 B4', 'A3 B3 B4 D3', 'F3 A3 C2 D1 F3 C2 D3 A1', 'D3 B3 E4 A3 D2', 'E1 E2 C2', 'A1 F4 F1 E4 A2 B2 E4 E3', 'E1 F1 D4 D2 D1 F3 A3 A4', 'F1 E4 F2 F2 F3 F3 B2', 'B4 E3 F1 D3', 'A1 C2 E2 F4 D1 B3', 'A1 E3 F4 E4 E4', 'B3 E2 E4', 'E2 D3 D1', 'A3 E3 F4 E3 C4 B3 C1 F4', 'D3 B3 B2 C1 A4 E1 A2', 'C1 B2 A2 B3 E2 D2 C3', 'B4 F3 C3 E4 A1 D4 E1 B1', 'C1 E4 F4 D3 D4 F2 C2', 'E1 C1 D4 E4 C1 E2 F2 E2', 'E1 E2 E2 E3 F3', 'B1 F1 E4 A1', 'B4 F2 E4 F2 B4 A1 E2 F2', 'B3 F2 C2 A3 D4 A1 F2 A2', 'D4 A3 C4 C4 B4', 'D2 C4 C2 A1 E4', 'B3 D3 C4', 'E4 B3 D3 A3 C2 E3 B3 E4', 'F4 A3 F4 E4', 'E4 A4 C2', 'C4 E1 F2 D3 E4', 'F4 A3 C2 A3', 'A3 A1 D1', 'D2 C4 C1 C3', 'C3 C4 B4 B1 B2 E4', 'B3 F3 C4 D4', 'F4 A2 F1 D1 C1 D4 F2', 'A3 B1 E3 D4 B1 E4 D3 B1', 'B3 B3 A4 F2', 'D1 F2 E4 A2 C4 D4', 'E1 E2 D4 F2 E1 E4', 'A2 C1 C2 B3 A2 F1 C3', 'E2 E4 C1 D3 D4 A3 D3', 'A4 F2 A3 B2 E3 A3', 'C1 C4 A1 E3 B3 F1', 'A2 C2 C3 D4', 'C2 A3 D3 B4', 'E2 F4 E2 B3 F1', 'A3 D3 B4', 'A2 A4 A4 A2 F3 E2 F2', 'D1 D2 A4 B4 C4 A1 D2', 'C4 F3 D4 F3', 'E2 D4 D4 E3 B4 F4', 'F4 E2 B4', 'D4 F3 A3', 'D3 B2 D4 E2 F3 C2', 'F1 E4 E4 F3 D2 B4 D4 D2', 'C1 F2 D2 D1 C4', 'A1 F3 D2 D2', 'C2 F3 B1 C1', 'F1 E2 D1 D3', 'D4 D3 B1 C2 A3 D3 A2', 'B3 D2 F3 A1 D1 A4 C2 A2', 'B2 E2 C3 B4 C3 A2', 'E3 E4 F4', 'A4 A4 B4 B1 E2 B2', 'A3 E3 E1 A3', 'F2 D1 A2 F2 B3 F4', 'D2 D4 D3 C1', 'A2 E2 C4 D2 B2', 'E2 D1 C1 F1 C3 E2 F2', 'E1 D3 A1 E2', 'C2 B3 D3 D4', 'C1 B4 E1 E1 E2 C2 B3', 'D4 C2 F2 D2 C1 A4', 'F4 B1 B1 C3', 'B2 B2 A2', 'D3 B3 F1 D2 A2 A3 D1 B2', 'B4 C1 E2 B2', 'B3 C2 A3 E4 A4 E1', 'D2 E4 C3 F4 E2 A1 C4 E4', 'D1 E1 A2', 'A2 F3 E1 F2 C3 E3 D3', 'E3 E2 C4 C2', 'B4 C2 C2 A4', 'C4 E4 A2 E2 D4 E2', 'C4 E1 E2 A2 C4 C4 D3', 'E3 C1 A4 B2', 'A1 B3 B1 E2 A1 F4 D3', 'A4 E4 B2 D4 F4 C3 E4', 'B3 C2 F4 A3 E4 B2 C3 A4', 'C3 A4 C1', 'B2 E1 E3 B4 D3 D3', 'C2 C4 B2 A4', 'C4 D1 E2 D4', 'C2 C2 B1 D4 D4 A2', 'F2 F3 A2 D4 B3', 'B4 B2 D2 F1', 'D2 F3 C3 F2 E1 B3 B4 D2', 'D4 B2 B4 E4 A1 F1 B3 F4', 'B3 E3 B2', 'A3 C3 A4 A3 E2 C2', 'E4 D1 F1 D1 E4 F1 C3', 'C4 E2 F3 A4 F1', 'B4 F2 C2', 'F4 C4 C1 A1 C3 B1 E2', 'E3 E4 E1 F4', 'C4 C1 E4 C4', 'E1 C4 D1 E4 A4 C2 B1', 'C2 F4 C3 E1 D3', 'F2 D4 C3 A3 B3', 'F3 F2 D1', 'E2 D2 E2 B1 E4 C1', 'C4 F4 B3 D2 C2 B2 C3', 'A2 C2 A3 E1 E1 D4 B4 E2', 'A2 B2 C4 F4 D1 D3 F1 D3', 'C3 A3 F4 B1 F1 A3 E2 F1', 'D4 E1 A1 D4 E1 B4', 'E1 B2 E3 C4 A3', 'A3 D3 E1', 'B2 F2 C3 F1 F4', 'F2 C2 F4 C4 C4', 'F4 C2 B3 A1 F2 D2 F1 F2', 'C3 A1 D3', 'F2 B4 D2 C1 E2', 'A4 C3 F3 B3 D3', 'D1 D3 D3 F2 B4 A4 E2', 'D3 F3 C3 D4', 'C1 B1 C2 D4 B2', 'F3 B3 A3 A3 C4 B2 F3', 'C2 F3 A4 B2 B2', 'F3 B3 A2 A2 B3 F1 C4', 'F2 B4 A2 C2', 'D1 B3 F2 C3 D4', 'B1 A2 E2 A2 C2 D2 A1 A3', 'E3 A1 C2 E1', 'F1 E3 B1', 'F4 F2 D1', 'C2 C3 B3 D2 B2 F3 F3 F2', 'F3 B2 F1', 'A4 B4 F2 B2 A4 A4 C3', 'C2 C1 B1', 'B4 C3 D1 B4', 'D3 A2 B3 E2 A1 B2 E1', 'B1 B2 F3 A4 F3 B2', 'A4 C4 E3 F2', 'E2 E2 A2 D2 B1 E2 C1', 'C2 C3 D1 D1 A4 F1', 'D4 B1 F4 E2 E1', 'E4 B3 D2 A3 F1', 'E2 E1 E4 B2 D3', 'B3 A4 F3 F4 B3 B1 B4', 'F2 D1 D1 E4 A2 C3 D4', 'C4 B3 F1 A3 F1 C4 A1', 'D1 E3 E4', 'A1 C4 D3 A3 A3 E1', 'A4 A1 B3 B4 F1 B4 A4 F1', 'B4 C1 D3 E4 B3 B1 D3 D3', 'D2 D4 A1 B1 D3 B1 D4 A4', 'A4 E3 E2 E1 E1', 'A4 C1 C1 D1 A1 D3 E3 B2', 'E4 B2 B4', 'F4 F1 E4 A2', 'A4 E2 D4 F3', 'D1 C4 D1 F4 B4 C4 E3 A4', 'E4 A4 D3', 'B1 A3 B3 D3', 'D1 F4 A4 C4 B3 F4', 'D4 A4 D2 A4 E4 C4 F3 E3', 'F1 C4 C3 A1 A2', 'A1 A4 A4 A4', 'A1 A1 D4 C2 F4', 'B4 E2 C3 E2 F2 C3 D4 B2', 'F1 B2 F2', 'A4 B4 D2 B4 C4', 'E4 E1 D3 B4', 'E1 A4 D1 F2 B4 D4 C3', 'D1 D4 B4 D4 B2', 'B2 B4 A3 E1 A1 B3 D4', 'A4 A1 F2 A3 E4 C4', 'C3 B1 C1 D2', 'A2 E4 E4 B4 E3 F2', 'A3 E2 F4 B4 E4', 'C1 C4 D2 B1 A4', 'B3 A4 B1 A2 A4 E1', 'F4 A2 E4', 'B4 B1 A4', 'C1 E1 C2 F2 E3 A4 E4', 'C1 C4 A1', 'B1 B3 B3 E3 F3 D3 A1', 'B1 E3 B4', 'F2 D1 F2 D1 C2', 'A3 A4 A1', 'F1 D2 C2 F2 C3 D1 B1', 'B4 B4 C2 C2', 'D1 D4 E1', 'A1 B4 B2 C4 E1 F4 E1', 'C2 D1 B4 E1 D4 C1 C2', 'B2 E4 D3', 'C2 A2 C1 B2 B4', 'E4 B1 F2 C2 F3 E2 B3', 'B2 A3 F2 A3 B2 A1 A4 C4', 'B4 D4 F3 E3', 'B4 A2 C4 B3 D3', 'B1 F1 C4 F1 E3', 'B2 A1 E1 C4', 'C2 F2 D1 A1 D1 A4 B1', 'D2 B2 F3 E2 B1 C1 F4 F3', 'B1 A3 E3 B3 B4', 'B1 B4 E3 E4 B2 F4 D2', 'F3 E3 B1 F3 C1', 'A4 F2 B1 E3 B4', 'C3 C1 F2', 'C3 C3 E1', 'C4 C1 C1 F3', 'A4 A3 C3 B4', 'D2 E4 F2 F3 D3 C2 D1', 'C3 B1 A1 A3 A2 D1 A2 C3', 'D4 F4 A3 C3', 'F1 C2 D3 B4 C1 E2 D1', 'A1 F1 A1 A1 C3', 'C1 C2 E2', 'A2 F2 A1 F1 D1 C3 F1 B1', 'C3 E3 C3', 'D1 A3 F2 A2', 'B3 A3 F2', 'F2 F2 E4 C2 C2', 'F4 E3 F4 C3 C2 E4 D4', 'A4 E3 A3 A4 F3 D3 E4', 'C1 C4 C4 F4 D4 F4 E3', 'F1 B1 A1 F2 B1 F3 D3', 'C1 E2 A4 B2 B2 B4 E4', 'F1 F3 A2', 'C4 B3 F3 B1 E1', 'C4 D1 C4 C1 B2 D3 C2', 'E3 D1 D3 F3 A3 B4 E1', 'A1 A1 D1 F1 D2 E2 D2 D1', 'C2 D4 B2 E2 A3 D4 D4', 'A2 A2 D1 E3', 'C4 C4 C3 E2 A3 A4', 'B4 F2 F3 D2 A4 C1 D2 D1', 'C1 F3 A3 F2', 'F3 F1 E2 B1 B3', 'D1 D2 B3 D2 F4 F4', 'D2 F3 D3 A4 C3 D4', 'D3 B3 A1 D2', 'A4 B2 A4 A4 C3 C1', 'C4 E3 A4', 'F4 E2 D4', 'E4 A1 C2 C3 A2 B2', 'B1 D4 E1 E1 E3 D2', 'A2 D2 B1 E4 B1', 'E3 E3 D1 B1', 'A2 C3 D1 C2', 'D2 B3 F2 B2 C1 C2', 'C4 F3 D4 A1', 'E3 A1 D2 C1 B3 B2', 'A2 C3 E3 A3 C2 C1', 'B4 C4 A2 C1 C4', 'F1 A1 E3 E4 B1', 'E1 C3 E4 B2 D3 B3 F3', 'E4 B3 C4 A3', 'B1 A4 E4 E1 C2 E3 C4', 'D1 A1 F1 A1 B2 B2 F2', 'A2 E1 E4', 'C3 D2 A1 A1 F2 C1 C1 D2', 'E4 D4 D3 E1 B4', 'D1 E3 C4 E2 A3 D2 D3', 'F4 F3 F1 D1', 'C3 A1 D4 A4 D2 F4 A1', 'D1 C4 F2 A3', 'F4 B2 A2 F3 D3 D3', 'A2 B4 F4 F3 E2', 'D3 E2 F4 A3 D3 C1 D2', 'C1 F3 D2 F4 E2 C3 E1 F1', 'F2 A4 C1', 'D3 F1 C4 A3', 'E3 F1 A3', 'C2 C2 C2 A3 C4', 'B2 F3 C3 E1 F3 E4 E4', 'F3 A2 F2', 'F1 F1 A4 A1 E1 D1 A3 A1', 'D3 D4 F1 C1 E2 A4 D3 C2', 'A3 B2 D2 B3 D3', 'E4 D1 E1 F3 B3 F4 D2', 'E3 A4 F4 C1 B1 E4 F2', 'F3 F1 B2 F4 B1 C3 D4 E3', 'E4 D1 B2 E1 E4', 'C1 B4 A2 E2 C2 C1 E4', 'C4 C2 A2 E2 F2 D1 E4', 'F4 C3 A4 B3 E3', 'B1 B4 C1 F3 F2', 'C3 B4 E3 D1', 'B3 F4 A1 F4 A3 B1', 'B3 B4 B2 C3 B4 F1 F3', 'B3 B3 E3 F1', 'A4 A4 C1 F2 A3 A1', 'A4 C2 A3 B3 B1 E2 F4 A3', 'D2 B2 E2 C1 A1 C2', 'A4 F4 E2 A4 F3', 'F4 B4 E4 F3 B2 F2 B3 F2', 'E3 E4 D3', 'F3 A2 F3 B2 C3 F2 D2 C2', 'C1 B4 F3 C3 E3 B4 D1 F1', 'B2 F4 D4 B2', 'A4 C4 B4 B1', 'B2 D3 F2 B2', 'A1 C2 D3 D2', 'C4 A1 D4 D2 D2 D1 A2 B3', 'B3 D4 A3 C4 C1 D2', 'F1 B4 B1', 'C4 D4 C4 C3', 'C2 F3 C2', 'E4 B4 D4 A1', 'E4 B1 A4', 'B4 D4 C4 F4', 'B4 E1 C2', 'D3 A4 D3 D2', 'B3 D1 E2 D1', 'A2 A2 C3 A1 D1 F1 D3', 'E3 F4 C1 B2 E3 D3 C4 B3', 'A4 E4 B4 E3 A1 B3', 'D4 E4 C3 F1 C1 F3 A4 E1', 'B4 E3 C4 C3 B4', 'C4 C2 E1 A4', 'D2 A4 A2 B1 B1 D1', 'B2 A3 B1 C2 A2 C2 E1', 'A2 E2 B4 F1 C4 E2', 'B2 D1 E2', 'C3 A2 C1 A3 A1', 'C3 F2 D2', 'C4 F2 C1 A1', 'D3 C1 B1 B2 F3 C4 D3 F4', 'C2 F4 D4 B1', 'A4 C2 D1 A2 D3 A2 C4', 'D4 A4 B4 C2 F1', 'A3 B3 E2 E2 B2', 'E3 B4 F2 F3 F1 E2', 'C2 E3 F1 C3 C3 D3 E2 D4', 'F3 C1 C2 B1', 'B1 F4 D2', 'A1 A4 F4', 'C4 B3 F1', 'F3 C3 D3 F2 D4 F4', 'D2 E3 F3 B3', 'C4 B2 C2 B3 A1 A1', 'C4 B2 E3', 'C4 C4 A4 D2 F2 D1 A4 F4', 'A4 F3 E2 A3 A2 D2', 'B2 C1 D1 C4 F2 D2', 'B3 F1 E3 D4 D2', 'E4 B2 D1 E4 F1 D1', 'B3 B1 D3 A3 C2 F3', 'E4 B2 B2 F3 A1 B4', 'A3 C2 A4 D3 B1', 'D4 D2 C3 A1 B2 B3', 'C2 C2 F1 F1 C2 D4', 'C2 F2 C3 C1 A1 B2 C2', 'A1 E3 B4 B3 B3 C3 F4', 'F1 C2 B3', 'C2 E3 D4 E2 F1 E1 A1 E3', 'E2 E4 A2 F4 A4 B4 A1', 'D2 F3 F1 D3', 'D1 A3 F4 E1 E4 E1', 'C2 E1 B2 C2 A4', 'A3 A1 E3 F4 A1', 'D2 F3 C2 A2 B1 F4', 'E1 F3 B4 C4 F1 E4 C4 B4', 'D3 A2 A3 F4 F4 B2 A2', 'E3 D4 D4 A4', 'A1 C2 D1 D3 E1 B2', 'A2 B1 B2 A1 C3 C4 C2 E3', 'C4 A4 C3 C2', 'E2 D2 E3', 'A3 F3 D4', 'C1 B4 D2', 'B3 C4 B3 C1 A2 E1 A2', 'C1 A1 B4', 'E3 E2 A4 A4 A3', 'B1 B3 B3 B3 C2', 'E3 A3 D4 A2 C3', 'E4 F3 E4 C2 A2', 'B3 A4 D1', 'C3 D1 D3 F2', 'F4 E4 A1 F3 E3 A4 F4 E4', 'F3 E4 C1 B3 D3 D2 E4', 'B1 A4 E3', 'A4 E3 E1 C1 C1 D2 B4 E4', 'C3 A1 C1 F4 E3 A2 F2 C2', 'A2 C4 D1 D2 F1', 'A4 A4 A4 E2 F1', 'E1 E1 E3 D1 A3 A1 D1 A4', 'B4 A1 F3 D2 E3', 'F2 D4 B2 F2 F3', 'C2 E3 D4 B4 E4', 'E4 E1 D3 B2 D2 A4', 'E4 B4 A1 E3 C4', 'F3 E2 C4 F1 A2 E1', 'E2 F2 F1 D4 C3 C2 E1', 'D1 A1 C4 F4 F2', 'B1 E2 B1', 'E2 D1 F3 C2 B2 E3 B4', 'F1 C2 A3 D4 C3 E4 C4', 'D1 C3 D2 E2 F3 B1', 'E3 A3 D4', 'E3 C2 A1', 'E2 A3 F4 E4 F4 F2', 'F3 B4 A2 F1 E4', 'E1 C3 C3 C3 E2 A3 A4 C2', 'D3 E1 D3 F4', 'F1 A1 C4', 'E2 A2 C3 B3 A3 C3', 'C2 A1 C2', 'C2 C4 F3 C1', 'D3 B1 B4 D3', 'E4 B4 E2 A1 D1 C1', 'A1 C3 E1 B4 C2 A2 D3 C4', 'F4 B1 A1 E3', 'D4 B3 C3', 'D2 F3 A3', 'D1 E2 F3 F4 B3', 'D1 A1 F3', 'A3 E1 F2 B3 E1 F3', 'C4 A1 C1 D1 A2 B3 B2 C4', 'E3 B4 C3 C3', 'A1 B2 E2', 'F4 B1 A4 B4 D2', 'B3 A1 A2 C1 D1 E4 D2', 'C4 A3 E3 B4 B3 A2', 'A3 A4 D1 E4 E2 D1', 'A2 E4 F3', 'F3 B4 B4 E2 E4 F2 C2', 'A1 E2 F1 B3 D2 A4', 'B2 D1 B4', 'B3 C1 F4 B2 E2', 'F1 E2 F3 B3 F2', 'F3 F2 E4 E1 D2 D2', 'A1 C1 B3 E1 B2', 'F1 F4 B2 A2 A4 E3 B3', 'F4 A2 B2 E4 A1 D1 D4', 'A2 B1 D2 F4 D4 B2', 'D4 F1 F2 B3 F2', 'E4 E3 B3 C4', 'A1 A4 B4 F2 E1 A3 E3 B3', 'B1 F2 F2 D2 A1 F4 E3', 'A1 C1 A1 A4 F2 C3 E3', 'F1 A3 F2 E4 A1 A2 F1 C4', 'B2 B1 D2 C3 C2 A2', 'F1 B4 E3 A2 F4 C2 E1 A3', 'A4 E2 F4 F1 F2 C4', 'A2 F4 F4 A4 D4 C1 C1', 'E3 B2 C3 C1', 'E4 A1 F1 C2 F2 F3 C1 B4', 'B4 D4 E1 F2', 'B3 E4 E1 C1 A4 F2', 'B1 D4 D4 C4 A3', 'B1 E3 E2 A2 A4 A3 B1', 'B2 C3 B4 F3 B3 B4 D1 C4', 'B1 E4 B1 B1 E4 D2', 'E3 C2 F2 D4 B2 C3', 'A3 E1 B3', 'F3 C3 D3 C1 B2 D4 B2 F4', 'E1 D2 D4 F3 D3 B2 D2', 'E4 F2 D3 B1 D2', 'C2 E2 B3 F4 D1 B4 B4 A3', 'B2 A3 E2 C2 D3', 'F1 A3 D2 B4 D2 B3 B3', 'F2 D3 A2 D2 A2', 'E3 E4 F1 D2 D4 F4 E1 E2', 'E2 E1 A1 D2', 'E4 C4 D3 F4 D4 A3 D4', 'F2 A4 F3 A2 C4', 'C4 E1 E4 E2', 'F2 A2 C1 C4 D2 E3 A3 D2', 'A4 E4 B3 A2 A4 A3 E2', 'B3 C4 F1', 'A2 E1 E1 C4 F2', 'F4 A4 E2 E1 D3 D1 B2 B1', 'C1 E1 E1 C3 A1', 'B1 E4 F3 F1 B1 C2 C4 B1', 'F4 C4 E1 C3 E1 A1', 'E1 D3 B2 C3 D1 D2 F3 C2', 'B4 A3 E2 E2 A2 E3', 'E4 F4 C3 F3 F2 D3 E3', 'A4 B3 B2 D4 C4 C3', 'B3 C4 E2 B1 A2 F4', 'E3 B2 F1 A2 E1 C3 A3 D2', 'E3 B4 A3 B2 B3 A2 D1', 'D2 A2 B1 F4', 'F1 E3 B1 D2 C2 A4 D1 F1', 'B3 F4 F4 D4', 'B4 E3 A2 F3 C3', 'C3 E2 F4', 'B1 F2 F3 E1 D1 C3 C4', 'F3 D3 F4', 'F2 C2 D3 C1 A1', 'D4 B3 C1 E4 C3 B4 E2 A1', 'A2 D2 B1 C3 C1 F3', 'B2 B4 B1 D3 E1 B3 F2 A4', 'A2 A2 A2 A1', 'E1 D2 D2 C4 A1 F2 F4 B1', 'D3 B2 A3', 'F3 A3 D1 D2 A2 C3 F4 F3', 'B3 E2 B4 A4', 'D1 F1 F1', 'E2 E2 E4 B4 E3 B2 B1 F2', 'F1 F3 B2 F3 D4 F2 B4 C2', 'B4 C2 C2 C2 B2 F2 A2', 'B3 F2 B3', 'F2 B1 A2 F3 F4 D1 C1', 'B1 E2 E3 C3 C3 A1 C1', 'B4 D2 F2 A1 D1 D2 C2', 'C2 D4 C4 E4', 'F1 F1 C2 B2 F4 B2 A2 F4', 'A4 E1 A3 A2 B1 A4 E4 C4', 'E3 D1 F1', 'E2 C1 D4 A3', 'E1 B4 C1 C2 E1 C1 A3', 'D3 F1 A2 E3 F2', 'E3 D3 E1 E4 B4 E1 F1', 'C1 B4 C1 E4 B3 D4 D1 D2', 'D2 B4 A3 F3', 'D1 F3 A2 E4 C1 A1 E4', 'A4 A1 E3 C1', 'D1 C2 D2 F2 E2 E1', 'E1 A2 B3 B2 A4 C2', 'F1 E1 C2 C4 E4', 'E2 D2 F2 C2 D4 E2 A4 B2', 'D2 C1 E2 C1 C1 B2 F3 B1', 'A1 B2 D3 B1 D3 C2 E2 D4', 'A4 D3 E2 A2 B1 C2', 'A1 F3 E3 E2', 'C1 D3 E4 D1 F4 E4', 'D3 F4 E2 A3 C3 C1 A3 D2', 'A1 C4 A2', 'C3 A2 B3 B1 A2 B1 D2 D4', 'B2 C4 B4 A4 A2 A3 E3 E1', 'D3 F2 D4', 'B1 D2 C3 A1 A3 F1', 'B4 F2 A2 B1 E3', 'F4 B2 B1 C3 D2 B4', 'C4 D1 F1', 'C4 A2 F1 E4 A3 C2 F3 D3', 'B1 A4 F1', 'C1 A1 F3 E4 F3 B3', 'E1 E3 B3', 'E3 D3 A1 C1 F4 C2', 'A1 C1 C2', 'C1 A3 C2 B1 C2 A3 B1 F3', 'F3 D2 F2 C2', 'D2 A1 F2', 'E1 F2 C1 D2 F2 B1 F3 C2', 'F1 E1 B1 F1 F2 A4 B4', 'C1 A1 A4 B3 E4 B3 F2 E3', 'F2 B4 D4 D3 C2 D2 A1', 'D3 D4 E4 E2', 'B2 A2 D3 B4', 'C4 B3 F2 F2 B1 A4', 'A2 F4 B2 F3 E2 E4 B3', 'F4 A2 E1', 'D4 C1 D2 A1 A1 F2', 'B1 F1 D4', 'F3 E1 B1 D2 C2 D4 D1', 'F4 F4 C3 A1', 'E1 B4 C1', 'C1 E4 C3 D1', 'C1 B1 B4 B2 D4 A1 E4', 'B4 E2 A1', 'D4 B2 D4 B4 A3', 'E1 E3 E4 A2 A2', 'B3 C2 D3 D4 F3 D1 F1 E1', 'C2 D4 D2 A3 D2 A3 C2 F3', 'E1 A3 E1 A1 E4 D2', 'C4 F2 C4 A1 E4 D4 C3', 'C3 D3 B4', 'D4 C2 A3 A1', 'B1 A4 C4 A3 B2', 'C2 D4 A4 E3 C3', 'B1 E1 D3 C1 F2', 'F3 F1 A1 E1 D2', 'F1 D3 B1 E4', 'C3 A2 C3 B4 C2 D3 A1', 'C4 C2 E2', 'E4 E1 C4 C1', 'E1 D1 C3 E3 E1', 'B2 E1 F2 F3', 'F1 C2 B1', 'E1 B3 C2 B4 A1 B2 B2', 'D1 F4 C4', 'A1 B3 C3 E4 E2 F1 F2 D1', 'A2 A4 E3 C2 B1 F4 C3 A1', 'E3 F2 D3 F3 C2 E4 A3', 'C3 E2 A1 E1 A1', 'B4 C1 B2 D1 E3 B2', 'F4 A3 B3 A3 C1', 'E1 C3 E3 D3 E2 A1 E1 C1', 'C3 D2 E1 F4', 'B3 F2 A1 C2 C3 F4 F3 B1', 'F1 E2 E3 E2', 'D2 E1 E3 C1', 'B3 B3 E3 C2 E4', 'F1 E2 A2 B2', 'F1 D4 B1 E3 C1 C4 E2 D2', 'D1 F3 B1', 'D4 B4 A4 C3', 'C3 A2 A3 A3 C1', 'F3 C1 D2 C4', 'D4 A1 C2 A2 E4 B3', 'B4 D3 C3 A1', 'B1 B3 D3 B1 D1 D4 D2', 'E4 E3 C4 C1 D2 F2 F4 F2', 'B2 E3 F2 E3', 'B3 B4 B2 F2', 'A2 D2 D4 A4 A2 C3 D1', 'C1 E1 C2 B2 A2 C1 D1', 'F1 B1 E4 A4 C1', 'D4 C4 F2 B2 E2 C1', 'F4 F4 A1 A1 B2 B2 F2', 'D4 B2 E1 E2 B1 B1 C2 D3', 'C4 D2 D4 F1 A4', 'D2 C4 A1', 'C1 D4 F1', 'B2 C1 C4 C1 D4', 'B2 B4 C3 A3 A4 E2 B1', 'D1 E1 A4 E1 C2 D1 D3', 'C2 A3 E4 F1', 'F3 A4 B1 F1', 'F4 F3 A3 C2 B2', 'B4 D1 D2 C3', 'E1 D3 F2 B3 F4 A3 C3', 'C3 E3 B3 A1', 'B3 D3 E4 F3 D3 F4 D3 E2', 'A4 D1 D4 D2 D1', 'E4 B1 E3', 'D2 F1 F3 A1 A4 B1 B2', 'E1 F3 C4 B3 F3 A1', 'E4 F4 D4 B3 D3 D4 B2', 'F1 A1 E3', 'E3 E2 C4 E4 F3 C1 A3 E2', 'C2 E2 F4 B2 F2 A3', 'B4 F3 B3 B1 B3 B2 C1', 'E4 A4 E2 E2 F3 A1 D2', 'C2 E1 A2 A2 B1 C3', 'B3 E1 D4 E3 D2 F2 E4 D3', 'B2 C2 F2 A2 F4 C1 E2', 'B4 F3 E3 D1 E1', 'B4 A4 B4 D4 A3 F3 B3 F3', 'B1 A4 D3 F2 B3', 'B1 E4 D4 A3 E2 E3 B2 C3', 'B1 E2 E2 B4 B1 D2 D3 C2', 'F3 D1 F3 C3 C2 B1 C4', 'A2 E3 A1 F2', 'B2 C4 E3', 'F4 B4 A4 B3 E4 F2 F2', 'F4 C1 E4 A3', 'E1 D3 A3 A2 F4', 'D2 B1 A1 D4 C1 C3 E3 F2', 'C1 E3 E1 D2', 'E3 B1 B1', 'B3 E4 C1 B3 C2 E2', 'C3 D4 B1 E2', 'A1 C4 E4 A3', 'A1 A4 A3', 'F4 D3 A1 D2', 'C3 F1 D2 B2', 'D3 B3 F1 C1 A4 A4', 'D1 B4 B1 D4 A4 A1', 'F1 E2 C1 E1 C4 C2 C2 C4', 'C3 F1 F2 F2 D4 E3 E1', 'D3 F1 A4', 'F4 F1 F1 C2 C4', 'E4 D1 A4 E4 A2 B1 A2 A4', 'F4 A2 A2 A4', 'D1 B1 E1', 'F1 B3 D3', 'B4 C2 B4 B4', 'E3 B3 F3 B4 A2', 'C3 D2 B1 E3 B4 E1 E1', 'B3 B2 D2 B1 C3', 'E1 A4 F1 D4', 'F2 B4 D3 E3 B3', 'D3 C2 C1 F2 A1 A1 F1 C4', 'B1 F1 B4 F1 A2 C1', 'C4 A1 F4', 'C3 B2 E4 A1', 'B4 E4 C3 F3 E1 B3 A2 C3', 'B3 F2 E1 A2 A1 C2 E2 C3', 'A4 C1 F4 C3 A2 D1', 'B4 D3 F4 C4 A4', 'E3 A1 F4 F1 E2 B2', 'E1 D1 A1 C1 F2', 'A2 F4 E1', 'C3 B4 A1 B3', 'D3 D1 C2', 'C4 E3 A1 E3', 'F4 F4 C4 F4 A4 A4', 'C3 E3 A3 D3', 'F4 F3 A2 B4 B2 E1 F2', 'C1 F3 B4 B4 B4', 'F4 D4 D2', 'B4 B4 A4 D4 B2 E4 B3', 'C1 D1 C2 A4 E2 F1 E3 A4', 'B1 F4 C3 B1 E1', 'E4 A3 C3 D4', 'A3 A4 C4 B2 E2 D2', 'E4 A4 E4 A4 C4 C2', 'C3 A4 B1 C4', 'C2 F4 C3 E1 B2 E4', 'D1 F4 F4 E2 D1 F3', 'C3 E1 D3 F2 D3 A3', 'F3 F2 A3 C1', 'E1 D3 F2 A4 C2 D2', 'F4 A1 F4 D4 E3', 'C1 F3 E4', 'E3 B1 A4 E1 F2 A3', 'A4 A2 F2', 'D3 E4 D2 E4 A4 C3 F4', 'F4 F4 F1 F2 C3 C1 E4 D3', 'D2 A3 E3 A4 C1 C3', 'D4 C4 F3 C2 E4 E4 D3 E4', 'D2 B1 B1 B2 F4 C2 B2 E2', 'B3 E1 D4', 'A4 D4 A1 C4 E3 A4 D4 F3', 'E2 F1 C3 D1 A4 B2', 'D4 D3 D1 F2 C3 F3 A3 B1', 'D2 B3 A2 A4 E1', 'D3 C4 D1 A2 A2', 'A3 C1 C4 C3 F1', 'B1 E1 B3 B4 F1 A1 C1 B4', 'B2 A3 D2 D2', 'C4 B1 E3 F1 D2 F4 F2 C1', 'E4 A4 D4 C3 E4 F4 C1 F4', 'B4 F3 C4 A4 F2 F4 D4 A4', 'B3 A2 D2', 'E1 B3 B1 B1 F2', 'A2 F4 F4', 'A1 E3 A2', 'C4 B4 B4 D3 E4 D2 F4 C1', 'D1 B3 D3', 'E1 F2 D1 D4 D4', 'D2 D2 C2 B3 D4 A1 A4 D1', 'E4 D2 B2', 'B4 F1 F4', 'A3 B3 C3 F1 C1', 'D4 D4 A1 D3 A1', 'B4 C3 E4 E1 A3', 'F2 F3 E3', 'C2 A1 E4 F2', 'D4 D2 A3', 'C2 A1 E2 F4 F3 F4 D4', 'B2 F1 A2 B4', 'C1 F4 A1 A4 C4', 'D4 C2 F4 B2 D2', 'B1 C4 D4 E3', 'A1 E2 F4 B3 D2 C4', 'B2 A1 D3 C3', 'F1 A2 F2', 'B4 C2 D3 C1 E4', 'B2 C4 B2 E1 C2 F1 A3', 'E1 B1 C4 F3 A4 C4', 'D4 C1 E3 C4 D2 F2 A2', 'E2 F4 F1 D2 D4 B2', 'D4 C2 C1 E3 F2', 'C2 A4 B4 A2', 'F3 B1 C3 C1 B3', 'E4 B3 B3', 'B2 A3 A4 C3', 'F3 A3 B3 A3 B1', 'E4 E4 E1 F2 F3', 'C2 C4 E2', 'A1 E1 B1 D1 A1 A4', 'C2 E3 D3 E2 E1 D3', 'C3 C1 C4 B4', 'D2 D3 E1 B1 F1 F1', 'A3 C3 F4 C2 E3 C4 C4 F3', 'D1 A1 D1 F2 D4 B3 E2 A2', 'D2 F1 F2', 'F2 F3 F2 E4 B4 B3', 'E2 C1 D3 F2 F4 D1 E3 A1', 'A1 C2 A1 A4', 'A1 F1 E1 B1 F3', 'A4 E3 B3', 'C3 B1 E3', 'E3 E2 F1 C2', 'C2 B1 E1 D1', 'F1 B4 B1 B4 C2', 'C1 B2 F4 D1 F2', 'A3 B4 E1 F4 E2 A3', 'B2 A3 E1 C3 F4 F2 B3 A1', 'B4 B4 D3 D2 C3 B4 A3 F1', 'F1 F3 F2 E1 A2 B3', 'B4 E2 B3 B1 A4', 'E3 B3 B3 E1 E4 F3 F4', 'C3 E3 E2 B3 C3 B1 A4', 'A1 F4 B1 C4 D3 A2 B1 D2', 'E3 F2 B1 C1 D1', 'B4 C3 B3 E1 A1 E3', 'F2 F2 C1', 'E1 B3 F1 A3 F1', 'B3 C1 C1 E1 C4 E1 D1 A2', 'E2 E1 F3 C2 C1 A1 D2', 'E3 B3 E2 B4 F2 C3 E1 F2', 'E3 A4 F3', 'B1 E3 E3 C4 F4 D2 C1 C1', 'B4 C4 C3 E1 F1 F2 A2 A2', 'F2 E1 B1 D1 F4 B1', 'C1 F2 C4 B2 B3 B1', 'F4 B2 E4 A3 A3 F3', 'D3 F1 B4 E2 D4', 'D2 D1 F2 D1', 'F3 B3 A1 C1', 'C3 E2 E2', 'C1 A3 C3 B3 A2 B3', 'C2 E1 B4', 'C1 A3 F3 C3 C2 F2 D3', 'A2 A4 D3 F4 C1 E4', 'A2 B4 D2 E1 D1 F3', 'F2 F2 A4 F4 C2 A4 E3 C1', 'C1 E3 E4 A2 D4 D1 B4 D2', 'E3 A1 C3 C3 C2 F2', 'E3 B3 B1 A1 A3 F4 D1 F3', 'C2 E1 D4 B4 B2 A3 F1', 'D3 D3 D4 B3 D4 E4 F4 A2', 'C3 E1 C1 E3 A1 F3', 'F1 F1 C2 F3 D3 F2 A4 F3', 'D4 B2 F3 D4 A3 C3 C4', 'F3 F1 D3', 'C1 D1 E3 F1 F2 A1 D4', 'E4 A2 A1 B1 D2 F1 C3 D1', 'D3 C3 E3 D4 B1 F4 A2', 'A3 D3 C4 D3', 'D3 F1 D4 C2 A3', 'B4 A1 F1', 'D1 D3 B1 F4 B4 B1', 'A2 B4 F3 C3 B4 B4 C3', 'E4 C1 A4', 'A3 C3 C4 F1 D4', 'B2 C2 D1 F2 C4 A1 F2 C3', 'F1 E1 E3', 'A3 B3 B1', 'E4 D2 F3 D4 A1 A2 B1', 'E2 E3 D4', 'A3 C3 D3 F2 F4 D3 B4 F4', 'C4 E2 B3 E2 E1 E2', 'E1 B4 E3', 'E3 C3 C2 F3', 'F1 A1 A4 C3 A1', 'D1 D2 F2 D2 D2 E1 C1', 'E4 D1 F2 D1 A2 D4', 'C2 E4 D1 C3 B1', 'C2 D2 E4', 'D1 B2 A4 B1 F2', 'D2 E1 D4 D2 D2', 'A2 E4 D2 E2 F3 D4 D4', 'C3 C4 B2 A1 B3', 'B3 A2 B4 D4 C1 F3', 'A3 E3 A1 B4 E1 D3', 'C2 E4 D2 F1 A4', 'E2 D1 D2 D4 C1 E2', 'B4 A3 E4', 'D4 B2 B3 A3 A1', 'F1 C2 D2', 'B2 A2 F2', 'B1 A2 D3', 'C4 B3 B3 E1 A1 D4', 'A3 C3 F1 C2 D1 B2 C2 B4', 'F1 B2 F4 E1 F3 D1 C2 C2', 'F1 D2 F3 E3', 'C4 F4 D2 C2 C1 F1 D1 F2', 'E4 D4 E4 B2 B2 A2 B1 C3', 'A3 D2 F3 B4 C4 A2 F2 A2', 'E4 C3 F3 D3 F1 D4', 'F1 D2 E2 A2 B2 C4 F3', 'B3 F2 A4 A1', 'F4 D2 A4 D3', 'A4 F2 D1', 'F4 E2 D1 C2 E4 F3 F1', 'D1 A2 C2 F3', 'B4 A3 D4 F3 D2 A4', 'C2 B4 C3 B3 E1 A3 B2 A1', 'B3 B3 C3 E1', 'A2 E2 B1 F1 D3 C1 A4', 'F4 A1 E2 D1', 'B4 B4 E2 C4 E4', 'A1 E3 B3 A2 E3 F2', 'A2 A2 A3 A1 A3 A1 B1', 'F3 A1 B3', 'C3 C1 B1 B4 D2 D1', 'E3 D2 E1 C4 B4 C3 A3', 'E1 D3 B2 B1 D1', 'C2 F2 A1 C2 F3 E2 D4 B2', 'A1 D4 E3 E2', 'C3 E4 A2 C3', 'E1 C4 C1 E4 B1 B3 A1', 'F1 D1 C1 D1 D1 E1 F2 B4', 'F4 C1 D2 D1 D4 C3 F4', 'A3 A4 D4 A4 E4 E2 B2 F2', 'E2 B1 E4 E4 D1 E1 B1', 'E3 B3 E1 D4 B4 B4', 'F4 E2 F3 A2 A4 E2 D3', 'A3 A4 F3 D4 E2 D1 C1 B1', 'A3 C4 B2 E1 F1 F2 D3 C3', 'A3 A3 F4 F1 F1 F4', 'A2 E1 B2 C3', 'F2 D1 E3', 'C1 B3 F1 D1', 'E3 E3 C1', 'B1 C1 C1 D4 B3 A4 E4 D1', 'D2 C1 E3 E3 D3', 'D4 D4 D4 E2 C3 E2', 'F3 A2 B4 E2 D2 A1 E3', 'A4 F3 A2 C4 F3 D1 E4', 'D1 F4 D4 B1 B2 B1 A1 A2', 'A2 C4 C1 D3 D4', 'B2 F1 A1 E3', 'A4 F2 D2 A1 B2', 'D4 F2 D4 D4 D1', 'A1 C2 C1 B4 D4', 'F3 F1 B4 B4 B2 E4 E2', 'C3 B2 A4 F2', 'C1 C1 E2 C4 E1 A3 F1', 'D3 A2 D1 D2', 'C2 C2 E1 F2 D1 F3', 'F2 F1 A2 B3 D4', 'F4 E1 A2', 'E1 C3 F3 C3 A3 D3', 'D3 D1 A4 C3 F2 F2 C2', 'D4 F4 B2', 'C2 E1 A4 E2 D3 E2 B2', 'C4 D1 F2 A2 A2 F1 F1 C1', 'C3 C4 A3 A3 D2 F2 B4 A1', 'A1 C3 D3 A1 A3 E1 C2', 'E2 F4 C3 C1 F1 B4 D1 F3', 'F4 A2 A4 D4 B1 A1', 'D4 D4 C4 F3 C4 D4 C1', 'F3 D1 F1', 'C3 A2 D3', 'B2 C2 B2 F3 D1 A2 B2', 'C1 E4 F1 A2 B3 E4 F2', 'E4 C4 A3', 'C1 E2 C1 B1 D4 B1 D4 D1', 'A1 C4 E1', 'B2 D1 A3 C2', 'B4 D3 B2 B1 E1 F1 F4', 'C1 D1 C4', 'C2 A1 F3 A2 D3', 'A4 E1 E2 E4 F1 A1 D4', 'A2 A4 E2 C4 A3 C4', 'E1 D1 F3', 'B2 C1 E3 E2 C2', 'A2 E4 A3 A1', 'F3 B2 F2', 'C3 D1 D1 A1 F4 E3', 'A2 A2 F4', 'E1 D1 A3 B1 C3', 'C2 D4 A1 D2 C3 F3 D1', 'B4 B2 F4 F2 B2 F2 E2 D3', 'B1 D1 E4 E1', 'A4 A4 B4 C4 D3 B4', 'B4 A2 D1 E3', 'C3 C4 D1 F4 C2 E3', 'B1 D3 C1 E1 B1', 'E4 B4 F1', 'D4 A2 B3 E4', 'B4 E2 A4', 'E2 C4 A1 F2 C1 A4 A4', 'B4 C1 F3 A3 B3', 'C3 B1 D4 C3 B1 C3 A2', 'B2 A1 A4 E4 A4 A3', 'F4 F1 B2', 'A1 E4 B4 C3 E2 B3', 'B1 C4 F2 D4', 'D2 B2 E2 A1 B4 A4 C2 E4', 'C4 D2 F4 F3 A1 A2 C2', 'A4 F1 A4', 'E1 A3 D2 A1 E1', 'C4 F2 B4 C2 C1 A2', 'F2 A1 F3', 'E3 B3 D4 A4 C2', 'D2 E2 D4 D3 E1 C1 F2', 'B4 A2 D4', 'A4 A3 B2 B4 C3 A4 D1 F2', 'F1 B3 B2 E2 A1 F2', 'C3 E2 F2 E4', 'A2 A4 D2 B2 D1 D3', 'D3 E4 C4 E4 A4 B4', 'B1 B3 F4', 'B2 C2 C1 E4', 'F1 D1 B1 E4 F1 D4', 'F4 D4 A4 A1 A4 B3 C3', 'A4 A1 F4 F2', 'A2 D3 E4 B4 C2 D1', 'F4 D4 F3 C4 A2 E4 C3 E2', 'C2 C2 B3 A3', 'B4 C4 A3 B3 C2 E1 A4', 'B3 C3 E2 A1 D2 F1', 'C2 C3 E3 C4 E1 F2', 'F2 B4 D3 F2 C3 F1 E3', 'F4 B2 A2 E3 B2 D1', 'A1 A2 B4 B2 C1 A4', 'A2 B4 A2 A2 A3', 'E4 B4 F1 F4', 'B1 F3 E3 D2 A3', 'D2 F4 B1 B2 A3', 'A3 F4 F3 D4 E4 E4 F1 A4', 'B3 B1 E4 B3 C1 F4 E4', 'F4 F2 C4', 'F2 E3 F1 B3 E2', 'B1 B1 C4 F4 D4', 'A3 A2 E1 F3 C4 C4', 'F3 E4 D2', 'E2 A4 F2 D3', 'D4 F2 C1 F1 A4 D4', 'D1 C4 A3 D3 C1 E2 F3', 'F3 B4 F2 F4 E4 B4', 'C2 C3 E4 A2 E2 B1', 'F1 F4 A4 E4 D1', 'F2 F2 B2 E3 C4 C4 E3 B1', 'E1 B3 A4 B4 E1', 'C3 D4 F4', 'C4 B3 D3 F4 C3', 'C4 F4 C2', 'B3 F2 C1 D2', 'A3 C1 F3 B4 B2', 'C2 D1 C1 E1 B1 C1 B1 F2', 'F1 E1 A4 A1', 'A4 F2 F4', 'E3 C2 F2 A4', 'C4 F1 A2 D3 A3 F2 C2', 'E4 E3 B2 F1', 'C2 A1 A3', 'C1 B2 D3 A1', 'D3 C3 E4 D1 B3', 'F4 C3 D1 A4', 'E3 F2 F2 D4 B2 C1 E4 B4', 'C2 E3 F3 C4 B1', 'F3 F4 B3 D4 A2', 'F4 A1 F3 D3 F4 D2', 'D3 A3 B2', 'F4 D4 C4 F3', 'A3 D1 F3', 'D3 F2 D2', 'B1 B4 E3 D1 A2', 'B3 D4 A3 A1 B1 E3 C2', 'F3 D4 F3 C1', 'C3 B3 A1 B1 B1 A1', 'D2 C3 B3 D1 E4 B1', 'F4 A1 D2 A4 B2 B4 D3 C3', 'B3 A4 C3 F2 F1 D3', 'B1 F1 E3 F4 C1', 'D3 E1 A4 F4', 'D2 B3 D2 B3 E3 B3 D3', 'A3 A1 D1 F2', 'F4 B2 F4', 'D3 B4 A2 D3', 'F2 B1 E4', 'F4 B3 C1 F2 C1', 'E3 E2 E4 D3', 'B3 B3 C2 A3 F2', 'E2 C3 A3', 'E3 E1 E1 C2 B2 D1', 'E4 F1 F1', 'D3 A1 A3 C2 D1 D1 A4 C1', 'E1 B3 B4', 'B3 D4 B4', 'E2 B3 A4 F2 D4 A2 A4 F3', 'D4 C3 A4', 'F1 B2 A2', 'F4 E2 F2 C3 E4 C4 B4 E4', 'D4 C4 C4 F1 C4', 'D2 E4 E3 A3 A2', 'E1 C2 C2 B1 E1 D2', 'D2 D1 B1 C4 B2', 'A1 F4 C1 B4 E2 A2 F3 B3', 'C4 F1 C3 E3 C4 E4 F1 A2', 'D1 C4 C1 C2 F1 A4 A4 A1', 'F4 D3 E2 A4 C2 D4 C1', 'E3 F4 E2', 'F2 C4 C4 C3', 'C3 D1 C2 A1 F3 F1', 'B2 B3 D3 D3 E4 D3', 'B2 D4 B4 D4 D4 B3 D3 A1', 'F3 C3 F4 D3 F3 E3 B4', 'D2 B4 D4 E1', 'A1 D1 A1 B1 E4 E2 B1', 'E2 D1 B2 B1 A1 B4 C4', 'B3 A2 C3 F3 B1', 'F1 A1 C3 B3 A2 C2 A3', 'B4 A2 D4 A3 B2', 'F3 F3 A4 B4 E2', 'D3 F3 D4 C2', 'E1 E3 C3 F3 B3', 'C2 F1 C4 E3 A3 A1 B2', 'F1 F3 F2 F3 F4', 'D4 D2 D3 E3 F3 B4 C2 F2', 'C3 D4 D1 E4 A2 D1', 'D3 A1 F4 E3 A2', 'D2 C4 E3 E2 F3 D4 B3', 'E2 E1 C1', 'E2 D2 B2', 'F3 E3 D1 C1 E4 B1 A2 A2', 'F4 A4 E1 A1', 'B3 E1 C1 E2 E4', 'E3 C4 B4 A2 A3 D2', 'B2 A4 A4 B4', 'D2 A1 C1 D3 D4 D2 C1 F2', 'D3 F2 A1 E3', 'D1 C1 F3 A4 B1 D4', 'B4 D1 F2 F3 E2 F1 F1', 'E3 A4 F3', 'B4 F1 A4 F4 F1', 'C4 B4 E3 B3 E3 C1', 'B2 A2 E1 C4 D1 F4', 'E3 E3 C2 E1', 'B3 C1 A2', 'E2 A4 C1 F3 F4 A1', 'E1 B1 F2 C3 E1 F3 D1', 'C4 E1 D3 A3 F2 E1 C3 F4', 'E1 F1 B3 A2 E4 D1', 'F2 F1 A4 A3 B1 D3 F1', 'F1 C4 B2 F1 A4', 'C4 E3 F3', 'C2 C2 C2', 'A2 B3 E3 D3 E2', 'E2 B4 B2 C2', 'D1 C4 E1 C3 F4', 'B4 F4 A2 E2 F1 F1', 'E2 A3 D1 E3 E4 D3 E4', 'B3 F4 C3 A1 F1 E4 E4 E4', 'D1 D4 C2 B4 D3 B3 F2 A1', 'D3 B1 A3 B4', 'D3 D4 E1 E4', 'D1 D4 F2 C4 E4 F2 E1', 'C1 A1 F1 D2 B1 A1', 'E1 D3 F3 A3 E4 D1 B3', 'D4 B1 B2 A1', 'E3 C2 E3 B3 F2', 'B2 A2 F3 A1 C2 F1 B2 C4', 'D2 D1 A4 E4 F2 F4 F1', 'B2 B1 A1 E1 D2', 'C4 F2 D4 C4 B2 A4', 'A3 D4 A3', 'B4 D3 E2 B1 D4 A2 E3 F3', 'A1 E3 B2 A1 C4 C1 B2 B1', 'E1 F4 B2 B1 C1 B1', 'B2 C2 A1 A3 F2', 'E1 E3 D4 C2 B1 E3 E1 F4', 'E1 F3 B2 E1 D4 F2 F3', 'E2 F2 F2 B4 A3', 'A1 F2 E3 F4 D1 F4 D4 D1', 'C3 C4 E1 C3 A4 C2 A1 C1', 'A2 F1 A1 A3 E4 D1 B1', 'B1 C2 F2 D1 F3', 'B2 C3 E4 E1 E1 A1', 'A2 C4 C2 B2 B1', 'A2 E4 F4 A2', 'F2 D2 E1 D2 C4 C2', 'E1 E2 B4 C3 A2 A4 E2 A2', 'B4 A1 D1', 'B1 E1 C2 F1 F1', 'B1 F4 C3 C1 E3 B2', 'E1 C1 D1 B4 E3 E1 B3 C2', 'F1 A1 C2 A4 D2 B3 C1 F2', 'F3 A2 D1 F3 B4', 'E3 F1 A1 A3 C2 D2', 'C1 A3 F3 A3 A1 A1', 'F3 F1 B2 F4', 'A3 A2 B2', 'F1 C2 A1 C3 C4 A2 B2 E4', 'A2 C3 D2 A2 A4 F4 D4', 'C3 D4 F2 F1 E4 D3 F2 C1', 'A3 B4 F4 F1 E2', 'B3 A4 B2 E1 D4 F2', 'A2 C4 A3 F2', 'C2 C1 E1', 'D3 A4 A2 B2 E3 C3', 'E3 D1 C2 C3 C1 A4 F1', 'F2 E3 F2 D3 F1 A3 A4', 'C4 A1 C2 B3 C1', 'A1 E2 F4 B2 A4 B3 B2 B3', 'C3 B1 E1 D3 B4', 'F3 F3 B3 B1', 'D4 B3 F4 A3', 'F3 D2 C4 E1 F1 F3 A2', 'B3 A2 A3 C2 A3', 'B1 B4 B4 F4 A4 C3 C4', 'A1 A3 C1', 'A4 C4 D1 C3 D2 B4 E2', 'A2 C3 D2 E1 D4 E2 E2', 'B1 A1 A1', 'B4 C2 E4 A2 C4 E1', 'E1 C1 C2 C4 A3 D2 C2', 'F3 E4 C1 A4 E3', 'A2 A1 B3 F3 B3 A4 C4 C1', 'C3 B1 F1 B3', 'C2 D2 F3 C3', 'A1 E3 F1 D4 D2 B4 C2 F3', 'A3 D1 C1 D3 E4', 'D2 B1 E1 B1', 'D3 F3 D1 B2 B2 F2', 'E4 E2 C3 A3 F4 F2 C1', 'A2 A1 A1 D4', 'C4 B1 E2 D1', 'B2 D4 E2 C1', 'B4 F1 A3 C3 B2 A4', 'D3 F4 D3 C3 B4 D3 F4 B2', 'D1 E4 E3 F2 D1 E1 B3 D3', 'B2 A3 B3 A4 A1', 'D1 A4 F2', 'F1 B3 B2 F1 F1', 'E2 F1 A2 B4 D1 D2 B3 F4', 'D1 D2 D2 E1', 'A4 D1 B2 B2 E1 A3 B1', 'D2 B3 B1 D4 F4', 'F3 B2 E2 F3 F1 A1 C3 D4', 'C2 D3 F2 F4', 'A3 D4 E3 C1 E2', 'C3 A4 B2 A2 C1', 'F2 E3 C4', 'F1 D2 D1', 'E3 F1 E3 E1 F1', 'F4 C4 E3', 'F3 D1 E2 B1 C4', 'D3 A2 D2 E1 C2 A1', 'F4 E2 A4 C4 A3', 'F2 F1 D4 C2', 'E2 F4 C4', 'F2 F1 B1 E2 A1 A3 B1 C3', 'F1 B1 D4 D1', 'C4 C2 D4 C2 D1', 'F3 E3 D3 F1 B1 A4 E1', 'B1 A3 A2 F1 E1 B2', 'C4 B1 E2 D4 E4', 'B2 F2 E3', 'B2 E1 F2 B2 F4 E2', 'B1 C3 D3 E4 B4 D1 D3', 'B1 D4 C4 C2 F4 C3 D1 B2', 'B2 E3 A2 C1', 'E4 E1 C2 B3 D4', 'E3 F3 B4 D4', 'A2 D3 C1 A3 A1', 'F4 B3 B2 A4', 'F4 C3 C1 F3 B2 D2 C4', 'F3 D3 F3 D4 B2', 'C1 D1 D2 C3 B3 B2 F2 F1', 'D3 D1 B3 C2', 'D1 A1 C4', 'B1 E3 B4 C2 C2 B2', 'D2 C1 F3 A2 D3 A3 E1 E4', 'C3 B3 D1', 'E4 B4 B4 D3 B4', 'F1 D4 A2 A1 D4', 'E1 F4 D1 C4 E2 E2 A4', 'B2 C4 D2 F4 B1 B4 C1', 'E3 D4 E1 E4 F2 A4', 'E3 B4 D3 B1 F3', 'A2 A3 C4 B2 C4 D1 D4 D1', 'A2 D1 F3 F1 A4', 'C1 B1 B2 B4 D1 B3', 'B4 E1 E4 C2 B4 A1 D1', 'A3 F3 D3', 'F1 B3 F2 D3 F2 F4', 'C4 C4 B1 E4 E2 D2 C1 A4', 'A1 D2 D1 E4 C3 E4 B1', 'D1 A4 B4 F2 C3 F2 F1 B2', 'C2 A2 F3 E4 F2', 'C3 A2 C2', 'E4 E1 C4 E2 E3 D3 C1', 'B2 D4 A3 D4 C1 F1 E1', 'A3 D3 E4 C1 F3', 'E3 A3 B1 E2', 'E1 C2 D2 A3 B3 D3 F3', 'C2 E1 B1 C3 D2', 'E4 B1 B1 A1 D3 B4 A4', 'B3 A2 B4 E2 F1 B2 D4', 'C1 D2 A2 B2 A1 F2 D4 F3', 'B4 A2 E1 B1 F1', 'D2 A2 C3 D1 C2 C1 D2 F4', 'B3 D3 D4 D4 D2 E4 D2', 'C2 F2 C1 F1 A4 E3 B3 B1', 'D2 F2 C1 E3 F2', 'F2 A1 C2 F2 E3 B1 B1', 'C1 E1 F4 F1', 'E3 B4 F1 B2 D1 F3', 'F1 A1 C2 A1 B1 E3 C4 C3', 'E1 E1 C1 E3 B4 B3', 'D4 A1 E2', 'D3 F2 C2 D1 C4 C2', 'B3 A1 D1', 'B4 F4 C2 F1 B2 C4 E2 F3', 'C4 B1 F2 A3 A2 D4 F1', 'B4 A3 D2 C4', 'E2 B4 B3 A4 C2', 'F3 F2 A1', 'A2 D4 E4', 'D2 B4 A4 D4 D1 E3 E2 C2', 'E2 C3 D1 F4 C2 C3 F1 C1', 'C2 D2 D2', 'A2 F3 C3 D3 E4 F2 E1 B4', 'D3 A4 E1 D4 D2 E2 F3', 'D4 D3 F2', 'D4 B2 B4 A4 C2 F1 D2 A3', 'D2 F1 F3 E4 F2 C1 E3', 'F3 C3 D3 C4 F1 E3 E1 C2', 'E2 E4 A4 F4 C1 E1 D3', 'E4 D3 E4 A1 B1', 'C2 C2 E4', 'A3 C2 F3 C3 F4 C3', 'F2 C1 B4 E1', 'F3 C1 D1 E4 A4 D2 C3', 'F1 E2 C2 A3 A2 E4', 'A3 E1 D3', 'E1 D4 F1 F2 B3 B1 E4 B1', 'E3 B3 C1 F4', 'A4 E4 A2 F3 B3', 'D3 B3 A2', 'D4 F3 D2 D2', 'C1 C4 B2 C4 D1', 'F3 E2 A1 E3 A3 A2 D2', 'F1 A1 F2 A3 A3', 'C2 E4 D3 F4 A3 F3 E2 F1', 'A4 B3 B3 E4 F3 D2 C2', 'A3 F1 F4 B1 F3 D1', 'C4 E3 F2 D3', 'B1 F4 C3 B3', 'A4 A2 D3 A4 D1 B1 E3 C3', 'F4 E1 A1 C3 A4 C2', 'C3 A3 D4 E3 D3 F4 B4 A4', 'A2 C1 D3 E3 C4 F2', 'D3 E3 D3 B3 D2 B2', 'B1 B2 F3 D1', 'D3 C4 D1 C4 A1 B2 F4 D4', 'B1 E4 B3 E3', 'C2 F2 A2 F2 E3 F3 C3 D3', 'C4 A1 E3 E2 D1 D3 B3', 'C4 E3 A1 D3 D4 D3', 'D4 A4 D1 C1 E4 B3 A3 B3', 'E1 E1 A3 F2 E2 A1 C2 B4', 'E1 C4 B1', 'D4 C4 D2 C4 E3 C1 B2', 'B4 F2 C1 D4 C3 A4', 'C1 F2 A2 D3', 'E2 C2 F4 B4 A4 A4 D2', 'C1 A2 F4 F1 A3 E3', 'A3 A2 B3 A2 D2', 'E3 D2 F1 F4 C1', 'D1 B1 A1 A1 A1 C1 B3', 'D1 D4 A4 F2', 'D2 B3 E2 B4 C3 C4 E1 B1', 'C4 F4 D3 B2 C2 D2 B4 A2', 'B3 B3 B2', 'E3 D4 C1 D4 B2 B3', 'F4 D3 D2 B1', 'C4 A2 B3 E4', 'E4 D3 C1', 'C1 F1 C3', 'E3 E4 A1 A1 C4 F2 B4', 'F4 C2 B1 B1 A4 D4 B4', 'A1 E1 E3 B4 C3 C3 C2', 'B1 C1 D3 A2 F1', 'D2 E3 B4 E2 A4 A3 E2 B3', 'C2 A3 D1 F3 F2 B1 A1 A2', 'C3 A3 E4 E3 B4 A3 E4 C4', 'D2 C1 B3 D2 F3 A3', 'E4 C3 F1 C2 D3', 'B2 E2 C4 D4 D3 D1 D3', 'E3 D2 C3 D4 A2 D2 D4 F4', 'F3 C2 D3 C3 F1 F2 A4', 'C2 D1 D2', 'B2 F1 F1 E1', 'F3 B2 A3 E4 C4', 'B4 C2 E1', 'A2 E4 D3 E1 D3', 'E2 C4 E4 F1 C4 D1 A2 E4', 'A4 B1 A1 B3 D1 A3', 'B1 C2 C1', 'B1 A2 E4 C1', 'A4 D3 C3', 'C1 A2 B1 B4 A3 B2 A3', 'C1 C4 D4 B4 F2 D1 A4 B4', 'E2 F3 D3 E1 A3', 'C3 F4 D2 C3 F3 C3', 'B1 E3 F1', 'A3 C1 B4 A4 C3 A1', 'E1 B1 A1 C1 E2', 'D1 B3 E3 E1 E1', 'E4 A2 E4', 'A3 B4 E3 A1 F2 B3 D3', 'E2 E2 F4 A4 C3 A2 E2 E1', 'B1 A4 D3', 'A2 E2 C4 D4', 'A1 E2 C2 A3 A1 C4 E1', 'C3 A2 D4 E3 A3 F1', 'F2 B1 D4 A4 A3 D4', 'D2 D4 B4', 'C1 B4 D3 C1 E3 C4 D1 F1', 'B3 A4 F3 B4 B1 C4 C4', 'D1 E4 C4 D4 A4 D3 A2', 'C4 A4 F1 F2 A1 A4', 'A2 E2 C4 A1 D3 E2 B1', 'B1 E4 E4 E1 E3 A2', 'B1 A3 B1 E2 A2', 'A4 D4 A1', 'E2 A2 D1 B3 C1 B1 F4', 'F4 E1 E2 B3 B2 C1 F3', 'E1 B3 A2', 'B2 D3 E3 D4 D2 A1 C2', 'C4 E3 A3 E1', 'A1 C1 F1 F4 B2 F3', 'F4 C4 C1', 'C2 D2 A4 F2 A2', 'E4 A3 A4', 'F2 E4 D4 B4', 'D2 C3 A1 E2 C2 F2', 'A3 F3 E3 F1 E2', 'E2 B1 C4', 'C3 C1 B3', 'C3 F4 B3', 'E2 F3 A3 B3 C1 A2 E3', 'C3 B4 E3', 'C1 F2 D3 A1 A1 B2 A1 E4', 'D1 E1 F4 A1', 'D1 F3 C4', 'A2 B1 D4', 'F3 F2 C4 A4 A4 D4 F4', 'B3 F2 B3 E3 D2 D2 F2 E4', 'A1 F3 C3', 'A4 F3 E1 A1 A2 C1 F3 F4', 'B1 F4 B4 B1 F3', 'A4 C4 D3 E1 B3 D1 D3 B4', 'F1 E4 D2 B1', 'E1 F4 F4 D1', 'A2 E1 A4', 'D1 F2 C1 E4 C1', 'B3 D4 B2 E2 A1', 'E3 C2 B2 E1 D1', 'F4 C2 B4 E2 C4', 'C4 E4 B4 D1', 'E2 E2 D2 D3 E4 C1 A2 E3', 'C4 B2 C3', 'A2 E2 A3', 'B3 D4 E4 D3 C2 D4 C2 B2', 'C4 E3 E1 C4 F4 B1 C1 F1', 'A2 A4 D4 D1 D3', 'B1 B3 A1 D4 E1 D4 B2', 'D2 B2 C3 F4 E3', 'A3 C3 D1 A3 A1 C2', 'A2 C2 F2', 'C1 E2 D1 A1 B4 B2', 'D2 B2 A3', 'A1 E3 D2 F3 F1 C1 F1', 'F1 B4 A1 D1 A3 F3', 'D2 C4 F2 C1 E2 F2', 'E1 B3 E2 B2 A1 A3 C2 D4', 'A1 C4 F2 C3 D1 F3 C3', 'A2 C2 E1 C4', 'C2 E4 F1', 'C2 C4 C3 D4 E2 F2', 'F2 D1 D2', 'E1 F4 F3 F2 F2 E1', 'A3 D4 C2', 'A4 B2 F1 A2 C4 F1 C4', 'D4 E4 D3', 'C1 E3 A4', 'B3 D3 E2 E3 F3 C1 B1 F2', 'C3 F4 D3 E2 F4 A2 A3', 'A3 D3 F3 A3 E4', 'C1 A4 F1 A1 B2 F3', 'D2 E4 F3 B3 E2 C1 E2', 'F2 A2 F3 B3 B1', 'C3 A1 A2 A1 E2 C3 B3', 'B3 F2 F4', 'C1 F3 F3 C4 F3 E1 A2 C3', 'C4 F4 D4 E1 D1', 'B2 B1 D3', 'A3 D2 F3 B3 F1', 'A4 D2 E1 D2', 'C4 D2 C1 A2 B1', 'D1 A1 B3 D4 B4 B3 F3', 'D3 B2 F1 A1', 'F1 D1 D4 D3 E3 A3 F3', 'D3 D2 E4', 'B2 A1 F4', 'A1 A3 F1 F4 A4 A3', 'B3 E2 F1 E2 F2 D3', 'F3 C2 E3 E2 C4 C3 F1', 'D4 C4 C3', 'B4 A2 E1 C3 F1 B2 D4 A2', 'F1 F4 E3 C2 D2', 'C2 E4 D2', 'D1 E3 C4 B1 C3 E4 D2 A1', 'B1 B1 B1 B4 D2 D3', 'E3 D2 D2', 'F4 F2 C3 F3', 'B4 C1 F2 F2 D1 F3 F3 C3', 'F3 D3 B1', 'B4 D4 E2 A2 C1 B2 A4', 'D3 E2 F4 B2 C2 D4', 'E1 F3 A3 E3', 'B1 F1 B1 D3 D1 D4 D2 B1', 'E3 B3 F3 F2 F2', 'B3 A1 D4'], 'label': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1]}"}, null, null, null, null, null], "exc_stack": [[["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 74, "<module>", "raise FileNotFoundError("]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 60, "<module>", "spr = load_spr_bench(DATA_PATH)"], ["runfile.py", 53, "load_spr_bench", "d[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 48, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 84, "<module>", "spr = get_dataset()"], ["runfile.py", 77, "get_dataset", "\"train\": load_dataset(\"json\", data_files={\"train\": train}, split=\"train\"),"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 344, "resolve_pattern", "if is_relative_path(pattern):"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/file_utils.py", 88, "is_relative_path", "return urlparse(url_or_filename).scheme == \"\" and not os.path.isabs(url_or_filename)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py", 394, "urlparse", "url, scheme, _coerce_result = _coerce_args(url, scheme)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py", 133, "_coerce_args", "return _decode_args(args) + (_encode_result,)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py", 117, "_decode_args", "return tuple(x.decode(encoding, errors) if x else '' for x in args)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/urllib/parse.py", 117, "<genexpr>", "return tuple(x.decode(encoding, errors) if x else '' for x in args)"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Represents the final loss value during the training process, where lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2564, "best_value": 0.2564}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Represents the final loss value during the validation process, where lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2475, "best_value": 0.2475}]}, {"metric_name": "dual weighted accuracy (validation)", "lower_is_better": false, "description": "Represents the accuracy metric during validation, where higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.916, "best_value": 0.916}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the model's error.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2717, "best_value": 0.2717}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating the model's error on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2557, "best_value": 0.2557}]}, {"metric_name": "dual weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy metric calculated during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9138, "best_value": 0.9138}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2746, "best_value": 0.2746}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2704, "best_value": 0.2704}]}, {"metric_name": "dual weighted accuracy (validation)", "lower_is_better": false, "description": "The accuracy computed using a dual weighting scheme on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.912, "best_value": 0.912}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model fits the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2746, "best_value": 0.2746}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2704, "best_value": 0.2704}]}, {"metric_name": "dual weighted accuracy", "lower_is_better": false, "description": "The accuracy metric calculated with dual weighting, measuring the model's predictive performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9123, "best_value": 0.9123}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, true, false, false, false, false], "plots": [[], [], [], ["../../logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_DWA_curve.png", "../../logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_DWA_curve.png", "../../logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_DWA_curve.png", "../../logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_DWA_curve.png", "../../logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_45d666708d6a4c43858d06e2603a37fd/SPR_BENCH_loss_curves_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_45d666708d6a4c43858d06e2603a37fd/SPR_BENCH_DWA_curve_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_45d666708d6a4c43858d06e2603a37fd/SPR_BENCH_confusion_matrix_aggregated.png"]], "plot_paths": [[], [], [], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_DWA_curve.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_DWA_curve.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_DWA_curve.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_DWA_curve.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_45d666708d6a4c43858d06e2603a37fd/SPR_BENCH_loss_curves_aggregated.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_45d666708d6a4c43858d06e2603a37fd/SPR_BENCH_DWA_curve_aggregated.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_45d666708d6a4c43858d06e2603a37fd/SPR_BENCH_confusion_matrix_aggregated.png"]], "plot_analyses": [[], [], [], [{"analysis": "The plot shows the training and validation loss trends over five epochs. Both losses decrease steadily, indicating that the model is learning effectively. The validation loss closely follows the training loss, suggesting that the model generalizes well without significant overfitting. By the fifth epoch, the losses converge, which is a positive sign of stable training.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot demonstrates the improvement of the model\u2019s validation dual-weighted accuracy over five epochs. The accuracy increases consistently, starting from approximately 84% and reaching over 91%. This steady improvement indicates that the model is effectively learning and improving its performance on the validation set.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_DWA_curve.png"}, {"analysis": "The confusion matrix shows the model's performance in terms of true positives, true negatives, false positives, and false negatives. The model correctly predicted 2196 true negatives and 2387 true positives while misclassifying 304 false positives and 113 false negatives. This indicates that the model performs well, with a relatively low number of misclassifications, particularly in terms of false negatives.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the cross-entropy loss for both training and validation datasets over five epochs. The training loss consistently decreases, indicating that the model is learning effectively from the data. Similarly, the validation loss also decreases over epochs, suggesting that the model is generalizing well to unseen data. The close convergence of training and validation loss curves implies that the model is not overfitting, which is a positive outcome for the experiment.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot demonstrates the validation dual-weighted accuracy across five epochs. The steady increase in accuracy, reaching over 91% by the fifth epoch, indicates that the model is progressively improving its performance on the validation set. This supports the hypothesis that the GNN-based approach is effective for the SPR task, as the dual-weighted accuracy metric reflects the model's ability to handle both color and shape variations in sequences.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_DWA_curve.png"}, {"analysis": "The confusion matrix provides a breakdown of the model's predictions against the ground truth. The high values along the diagonal (true positives and true negatives) indicate that the model is making accurate predictions for both classes. The relatively low off-diagonal values (false positives and false negatives) suggest that misclassifications are minimal. This performance further supports the model's effectiveness in handling the SPR task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss over 5 epochs. Both losses decrease steadily, indicating that the model is learning effectively. The validation loss is consistently lower than the training loss, suggesting that the model generalizes well without overfitting. This is a positive sign of the model's performance during training.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the validation dual-weighted accuracy over 5 epochs. The metric improves steadily, reaching over 90% by the final epoch. This suggests that the model is capturing the dual-weighted aspects of the task (likely related to both color and shape) effectively, and its performance improves consistently with training.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_DWA_curve.png"}, {"analysis": "The confusion matrix indicates that the model performs well on both classes. There are 2103 true negatives and 2457 true positives, with relatively few false negatives (43) and false positives (397). This suggests a strong balance in the model's ability to correctly predict both classes, though there is some room for improvement in reducing the false positives.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over five epochs. Both curves decrease steadily, indicating that the model is learning effectively without overfitting. The validation loss is consistently lower than the training loss, which might suggest that the model generalizes well on the validation set or that some form of regularization is helping reduce overfitting. The convergence of the two losses suggests a stable training process.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the validation dual-weighted accuracy across epochs. The accuracy increases consistently, reaching over 90% by the fifth epoch. This indicates that the model's performance is improving with more training and is likely capturing the underlying patterns in the data effectively. The steady upward trend is a positive sign of the model's ability to generalize well on the validation set.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_DWA_curve.png"}, {"analysis": "This confusion matrix illustrates the classification performance on a binary task. The model correctly classified 2,107 samples as class 0 and 2,454 samples as class 1. There are 393 false positives and 46 false negatives. The low number of false negatives compared to true positives suggests that the model is particularly effective at identifying class 1. However, the false positives are relatively higher, which might indicate room for improvement in precision for class 0.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["[]", "[]", "[]", "The plots indicate that the model training is stable, with decreasing losses and\nincreasing accuracy. The confusion matrix highlights good classification\nperformance with a low error rate.", "The plots collectively demonstrate that the GNN-based model is learning\neffectively, generalizing well, and achieving high accuracy on the SPR task. The\ndecreasing loss curves, increasing validation accuracy, and favorable confusion\nmatrix all point to a successful implementation of the proposed approach.", "The plots show strong evidence of effective training and performance\nimprovements. The loss curves indicate good generalization, the accuracy metric\ndemonstrates consistent improvement, and the confusion matrix reflects a\nbalanced and accurate classification performance with minimal errors.", "The plots indicate that the model is learning effectively, with decreasing loss,\nimproving accuracy, and a strong classification performance as seen in the\nconfusion matrix. The results suggest that the GNN-based approach is promising\nfor the SPR task.", "[]"], "exec_time": [2.515814781188965, 2.328580379486084, 2.5310757160186768, 27.51313853263855, 17.45432686805725, 12.60893177986145, 13.45234203338623, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], [], [], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], []], "plot_code": [null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    # ---- unpack helpers ----\n    def unpack(pairs):  # list of (ts,val) or None\n        return [v for ts, v in pairs if v is not None] if pairs else []\n\n    epochs = range(1, len(data[\"losses\"][\"train\"]) + 1)\n    train_loss = unpack(data[\"losses\"][\"train\"])\n    val_loss = unpack(data[\"losses\"][\"val\"])\n    val_dwa = unpack(data[\"metrics\"][\"val\"])\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n    classes = sorted(set(gts) | set(preds))\n    cm = np.zeros((len(classes), len(classes)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[classes.index(t), classes.index(p)] += 1\n\n    # -------------- plots --------------\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"-o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, \"-o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. DWA curve\n    try:\n        plt.figure()\n        plt.plot(epochs, val_dwa, \"-o\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dual-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Dual-Weighted Accuracy\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_DWA_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft axis: Ground Truth, Bottom axis: Predicted\"\n        )\n        plt.xticks(range(len(classes)), classes)\n        plt.yticks(range(len(classes)), classes)\n        # annotate cells\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    # ---- unpack helpers ----\n    def unpack(pairs):  # list of (ts,val) or None\n        return [v for ts, v in pairs if v is not None] if pairs else []\n\n    epochs = range(1, len(data[\"losses\"][\"train\"]) + 1)\n    train_loss = unpack(data[\"losses\"][\"train\"])\n    val_loss = unpack(data[\"losses\"][\"val\"])\n    val_dwa = unpack(data[\"metrics\"][\"val\"])\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n    classes = sorted(set(gts) | set(preds))\n    cm = np.zeros((len(classes), len(classes)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[classes.index(t), classes.index(p)] += 1\n\n    # -------------- plots --------------\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"-o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, \"-o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. DWA curve\n    try:\n        plt.figure()\n        plt.plot(epochs, val_dwa, \"-o\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dual-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Dual-Weighted Accuracy\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_DWA_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft axis: Ground Truth, Bottom axis: Predicted\"\n        )\n        plt.xticks(range(len(classes)), classes)\n        plt.yticks(range(len(classes)), classes)\n        # annotate cells\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    # ---- unpack helpers ----\n    def unpack(pairs):  # list of (ts,val) or None\n        return [v for ts, v in pairs if v is not None] if pairs else []\n\n    epochs = range(1, len(data[\"losses\"][\"train\"]) + 1)\n    train_loss = unpack(data[\"losses\"][\"train\"])\n    val_loss = unpack(data[\"losses\"][\"val\"])\n    val_dwa = unpack(data[\"metrics\"][\"val\"])\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n    classes = sorted(set(gts) | set(preds))\n    cm = np.zeros((len(classes), len(classes)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[classes.index(t), classes.index(p)] += 1\n\n    # -------------- plots --------------\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"-o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, \"-o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. DWA curve\n    try:\n        plt.figure()\n        plt.plot(epochs, val_dwa, \"-o\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dual-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Dual-Weighted Accuracy\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_DWA_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft axis: Ground Truth, Bottom axis: Predicted\"\n        )\n        plt.xticks(range(len(classes)), classes)\n        plt.yticks(range(len(classes)), classes)\n        # annotate cells\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    # ---- unpack helpers ----\n    def unpack(pairs):  # list of (ts,val) or None\n        return [v for ts, v in pairs if v is not None] if pairs else []\n\n    epochs = range(1, len(data[\"losses\"][\"train\"]) + 1)\n    train_loss = unpack(data[\"losses\"][\"train\"])\n    val_loss = unpack(data[\"losses\"][\"val\"])\n    val_dwa = unpack(data[\"metrics\"][\"val\"])\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n    classes = sorted(set(gts) | set(preds))\n    cm = np.zeros((len(classes), len(classes)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[classes.index(t), classes.index(p)] += 1\n\n    # -------------- plots --------------\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"-o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, \"-o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. DWA curve\n    try:\n        plt.figure()\n        plt.plot(epochs, val_dwa, \"-o\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dual-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Dual-Weighted Accuracy\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_DWA_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft axis: Ground Truth, Bottom axis: Predicted\"\n        )\n        plt.xticks(range(len(classes)), classes)\n        plt.yticks(range(len(classes)), classes)\n        # annotate cells\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- setup -----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------- paths -----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_29bbd6bde4ab44fd9c48098b83b32ec9_proc_1437146/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3e594b895c8c494faff0cdea61726d42_proc_1437144/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_899352698ddc417ebbc401f3ac717803_proc_1437145/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n    for p in experiment_data_path_list:\n        full_path = os.path.join(root, p) if root else p\n        all_experiment_data.append(np.load(full_path, allow_pickle=True).item())\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n\n# ----------- helper -----------\ndef unpack(pairs):\n    \"\"\"convert list of (ts,val) into list[val]\"\"\"\n    return [v for ts, v in pairs if v is not None] if pairs else []\n\n\n# ----------- plotting per dataset -----------\nfor dataset_name in all_experiment_data[0].keys() if all_experiment_data else []:\n    # collect run-specific arrays\n    train_losses, val_losses, val_dwas = [], [], []\n    preds_all, gts_all = [], []\n    min_epochs = np.inf\n\n    for run in all_experiment_data:\n        data = run.get(dataset_name, {})\n        if not data:\n            continue\n        tl = np.array(unpack(data[\"losses\"][\"train\"]))\n        vl = np.array(unpack(data[\"losses\"][\"val\"]))\n        vd = np.array(unpack(data[\"metrics\"][\"val\"]))\n        min_epochs = min(min_epochs, len(tl), len(vl), len(vd))\n        train_losses.append(tl)\n        val_losses.append(vl)\n        val_dwas.append(vd)\n        preds_all.append(np.array(data[\"predictions\"]))\n        gts_all.append(np.array(data[\"ground_truth\"]))\n\n    if len(train_losses) == 0:\n        continue  # nothing to plot\n\n    # align to common epoch length\n    min_epochs = int(min_epochs)\n    train_mat = np.vstack([tl[:min_epochs] for tl in train_losses])\n    val_mat = np.vstack([vl[:min_epochs] for vl in val_losses])\n    dwa_mat = np.vstack([vd[:min_epochs] for vd in val_dwas])\n    epochs = np.arange(1, min_epochs + 1)\n\n    # compute mean & SEM\n    def mean_sem(mat):\n        mean = mat.mean(axis=0)\n        sem = mat.std(axis=0, ddof=1) / np.sqrt(mat.shape[0])\n        return mean, sem\n\n    mean_train, sem_train = mean_sem(train_mat)\n    mean_val, sem_val = mean_sem(val_mat)\n    mean_dwa, sem_dwa = mean_sem(dwa_mat)\n\n    # -------- Loss plot --------\n    try:\n        plt.figure()\n        plt.plot(epochs, mean_train, \"-o\", label=\"Mean Train Loss\")\n        plt.fill_between(\n            epochs,\n            mean_train - sem_train,\n            mean_train + sem_train,\n            alpha=0.3,\n            label=\"Train SEM\",\n        )\n        plt.plot(epochs, mean_val, \"-o\", label=\"Mean Val Loss\")\n        plt.fill_between(\n            epochs, mean_val - sem_val, mean_val + sem_val, alpha=0.3, label=\"Val SEM\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\n            f\"{dataset_name}: Mean Training vs Validation Loss\\n(n={train_mat.shape[0]} runs)\"\n        )\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"{dataset_name}_loss_curves_aggregated.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dataset_name}: {e}\")\n        plt.close()\n\n    # -------- DWA plot --------\n    try:\n        plt.figure()\n        plt.plot(epochs, mean_dwa, \"-o\", color=\"green\", label=\"Mean Val DWA\")\n        plt.fill_between(\n            epochs,\n            mean_dwa - sem_dwa,\n            mean_dwa + sem_dwa,\n            alpha=0.3,\n            color=\"green\",\n            label=\"DWA SEM\",\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dual-Weighted Accuracy\")\n        plt.title(f\"{dataset_name}: Mean Validation DWA\\n(n={dwa_mat.shape[0]} runs)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"{dataset_name}_DWA_curve_aggregated.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated DWA plot for {dataset_name}: {e}\")\n        plt.close()\n\n    # -------- Confusion matrix --------\n    try:\n        # aggregate predictions & ground truths\n        preds = np.concatenate(preds_all)\n        gts = np.concatenate(gts_all)\n        classes = sorted(set(gts) | set(preds))\n        cm = np.zeros((len(classes), len(classes)), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[classes.index(t), classes.index(p)] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            f\"{dataset_name}: Aggregated Confusion Matrix\\nLeft: Ground Truth, Bottom: Predicted\"\n        )\n        plt.xticks(range(len(classes)), classes)\n        plt.yticks(range(len(classes)), classes)\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"{dataset_name}_confusion_matrix_aggregated.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated confusion matrix for {dataset_name}: {e}\")\n        plt.close()\n"], "plot_plan": [null, null, null, "We first load the saved experiment_data.npy, then pull out timestamps, training/validation losses, and validation dual-weighted accuracy (DWA) recorded for the SPR_BENCH run. Three separate figures are produced: (1) a line plot comparing training and validation losses versus epoch index, (2) a line plot of validation DWA versus epoch index, and (3) a confusion-matrix heat-map of ground-truth versus predicted class labels on the dev set from the final epoch. Each figure is created inside its own try-except block, saved to the working directory with descriptive names that include the dataset and plot type, and then closed to free resources. If more than five epochs were present we would only plot at evenly spaced intervals, but the current data already contains \u22645 epochs. All plots have clear titles and, where relevant, subtitles that clarify what the left/right axes or panels represent. No values are fabricated\u2014everything comes directly from experiment_data.npy. The code follows the mandated import/working_dir boilerplate and relies only on matplotlib/numpy (plus the tiny helper from numpy for the confusion matrix).", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["", "", "", "We will load the numpy file from the working directory, convert the stored\nobject back to a Python dict, and iterate over its top-level keys (datasets).\nFor every dataset we will pull the last entry of each metric/loss list, because\nthe training script always appends the newest result to the end; this serves as\nthe \u201cfinal\u201d value.   Only numeric tuples are considered\u2014`None` placeholders\n(e.g., train metric) are ignored.   Each value is printed with an explicit,\ndescriptive label so that the output is unambiguous.", "We will load the numpy file from the working directory, convert the stored\nobject back to a Python dict, and iterate over its top-level keys (datasets).\nFor every dataset we will pull the last entry of each metric/loss list, because\nthe training script always appends the newest result to the end; this serves as\nthe \u201cfinal\u201d value.   Only numeric tuples are considered\u2014`None` placeholders\n(e.g., train metric) are ignored.   Each value is printed with an explicit,\ndescriptive label so that the output is unambiguous.", "We will load the numpy file from the working directory, convert the stored\nobject back to a Python dict, and iterate over its top-level keys (datasets).\nFor every dataset we will pull the last entry of each metric/loss list, because\nthe training script always appends the newest result to the end; this serves as\nthe \u201cfinal\u201d value.   Only numeric tuples are considered\u2014`None` placeholders\n(e.g., train metric) are ignored.   Each value is printed with an explicit,\ndescriptive label so that the output is unambiguous.", "We will load the numpy file from the working directory, convert the stored\nobject back to a Python dict, and iterate over its top-level keys (datasets).\nFor every dataset we will pull the last entry of each metric/loss list, because\nthe training script always appends the newest result to the end; this serves as\nthe \u201cfinal\u201d value.   Only numeric tuples are considered\u2014`None` placeholders\n(e.g., train metric) are ignored.   Each value is printed with an explicit,\ndescriptive label so that the output is unambiguous.", ""], "parse_metrics_code": ["", "", "", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef get_last_numeric_entry(lst):\n    \"\"\"Return the last element whose second field is a number.\"\"\"\n    for entry in reversed(lst):\n        # skip `None` placeholders\n        if entry is None:\n            continue\n        # entry is expected to be (timestamp, value)\n        if isinstance(entry, (list, tuple)) and len(entry) == 2:\n            return entry[1]\n    return None\n\n\n# ---------- iterate and print ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}\")  # dataset header\n\n    # losses\n    train_loss = get_last_numeric_entry(content[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"final training loss: {train_loss:.4f}\")\n\n    val_loss = get_last_numeric_entry(content[\"losses\"][\"val\"])\n    if val_loss is not None:\n        print(f\"final validation loss: {val_loss:.4f}\")\n\n    # metrics\n    val_dwa = get_last_numeric_entry(content[\"metrics\"][\"val\"])\n    if val_dwa is not None:\n        print(f\"final dual weighted accuracy (validation): {val_dwa:.4f}\")\n\n    # add more metrics here if they appear in future experiment_data structures\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef get_last_numeric_entry(lst):\n    \"\"\"Return the last element whose second field is a number.\"\"\"\n    for entry in reversed(lst):\n        # skip `None` placeholders\n        if entry is None:\n            continue\n        # entry is expected to be (timestamp, value)\n        if isinstance(entry, (list, tuple)) and len(entry) == 2:\n            return entry[1]\n    return None\n\n\n# ---------- iterate and print ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}\")  # dataset header\n\n    # losses\n    train_loss = get_last_numeric_entry(content[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"final training loss: {train_loss:.4f}\")\n\n    val_loss = get_last_numeric_entry(content[\"losses\"][\"val\"])\n    if val_loss is not None:\n        print(f\"final validation loss: {val_loss:.4f}\")\n\n    # metrics\n    val_dwa = get_last_numeric_entry(content[\"metrics\"][\"val\"])\n    if val_dwa is not None:\n        print(f\"final dual weighted accuracy (validation): {val_dwa:.4f}\")\n\n    # add more metrics here if they appear in future experiment_data structures\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef get_last_numeric_entry(lst):\n    \"\"\"Return the last element whose second field is a number.\"\"\"\n    for entry in reversed(lst):\n        # skip `None` placeholders\n        if entry is None:\n            continue\n        # entry is expected to be (timestamp, value)\n        if isinstance(entry, (list, tuple)) and len(entry) == 2:\n            return entry[1]\n    return None\n\n\n# ---------- iterate and print ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}\")  # dataset header\n\n    # losses\n    train_loss = get_last_numeric_entry(content[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"final training loss: {train_loss:.4f}\")\n\n    val_loss = get_last_numeric_entry(content[\"losses\"][\"val\"])\n    if val_loss is not None:\n        print(f\"final validation loss: {val_loss:.4f}\")\n\n    # metrics\n    val_dwa = get_last_numeric_entry(content[\"metrics\"][\"val\"])\n    if val_dwa is not None:\n        print(f\"final dual weighted accuracy (validation): {val_dwa:.4f}\")\n\n    # add more metrics here if they appear in future experiment_data structures\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef get_last_numeric_entry(lst):\n    \"\"\"Return the last element whose second field is a number.\"\"\"\n    for entry in reversed(lst):\n        # skip `None` placeholders\n        if entry is None:\n            continue\n        # entry is expected to be (timestamp, value)\n        if isinstance(entry, (list, tuple)) and len(entry) == 2:\n            return entry[1]\n    return None\n\n\n# ---------- iterate and print ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}\")  # dataset header\n\n    # losses\n    train_loss = get_last_numeric_entry(content[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"final training loss: {train_loss:.4f}\")\n\n    val_loss = get_last_numeric_entry(content[\"losses\"][\"val\"])\n    if val_loss is not None:\n        print(f\"final validation loss: {val_loss:.4f}\")\n\n    # metrics\n    val_dwa = get_last_numeric_entry(content[\"metrics\"][\"val\"])\n    if val_dwa is not None:\n        print(f\"final dual weighted accuracy (validation): {val_dwa:.4f}\")\n\n    # add more metrics here if they appear in future experiment_data structures\n", ""], "parse_term_out": ["", "", "", "['SPR_BENCH', '\\n', 'final training loss: 0.2564', '\\n', 'final validation loss:\n0.2475', '\\n', 'final dual weighted accuracy (validation): 0.9160', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.2717', '\\n', 'final validation loss:\n0.2557', '\\n', 'final dual weighted accuracy (validation): 0.9138', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.2746', '\\n', 'final validation loss:\n0.2704', '\\n', 'final dual weighted accuracy (validation): 0.9120', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.2746', '\\n', 'final validation loss:\n0.2704', '\\n', 'final dual weighted accuracy (validation): 0.9123', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]}