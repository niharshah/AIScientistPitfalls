{"edges": [[0, 2], [0, 8], [0, 1], [0, 3], [0, 7], [0, 5], [0, 4], [0, 6], [7, 11], [7, 10], [7, 9], [7, 12]], "layout": [[0.4666666666666667, 0.0], [0.0, 0.5], [0.13333333333333333, 0.5], [0.26666666666666666, 0.5], [0.4, 0.5], [0.5333333333333333, 0.5], [0.6666666666666666, 0.5], [0.8, 0.5], [0.9333333333333333, 0.5], [0.6, 1.0], [0.7333333333333333, 1.0], [0.8666666666666667, 1.0], [1.0, 1.0]], "plan": ["We will convert each input sequence into a simple chain-graph whose nodes are\nthe two-character tokens and whose edges connect consecutive positions, then\ntrain a lightweight GCN to predict the hidden label.   Tokens are first mapped\nto integer ids and embedded; two stacked GCNConv layers followed by global mean\npooling and a linear classifier form the model.   The provided helper functions\nload the SPR_BENCH splits; if the files are missing we fall back to a tiny\nsynthetic dataset so the script is fully self-contained.   During training we\niterate for a small number of epochs, compute loss with cross-entropy, and after\nevery epoch evaluate on the dev set, printing validation loss and the Dual-\nWeighted Accuracy (mean of CWA and SWA).   All tensors and the model are moved\nto GPU when available, adhering to the required device handling rules.\nMetrics, losses, predictions, and ground-truth labels are stored in the\nprescribed experiment_data dictionary and saved to disk at the end of the run.\nThe code is laid out as a single file that executes immediately, creates a\n\u201cworking\u201d directory for outputs, and finishes comfortably within the time limit.", "Hyperparam tuning name: num_epochs. We extend the baseline by sweeping over\nlarger epoch budgets (10 / 20 / 30 / 50) with optional early-stopping (patience\n= 3).   For every epoch budget we (re)initialise the same GCN, train, track\ntraining & validation loss plus Dual-Weighted-Accuracy, remember the best\nvalidation snapshot, and finally save all plottable data in the required\nexperiment_data structure under the hyper-parameter group \u201cnum_epochs\u201d.\nEverything is kept in one executable file and the results are stored in\nworking/experiment_data.npy.", "Hyperparam tuning name: learning_rate. We loop over the candidate learning-rates\n{3e-4, 1e-4, 3e-3}, re-initialising the GCN, optimiser and dataloaders each\ntime, and train for five epochs exactly as in the baseline.   For every (epoch,\nlr) we log train/validation losses, validation Dual-Weighted-Accuracy (DWA),\npredictions and ground-truth into the experiment_data dictionary under the\nhierarchy learning_rate \u2192 SPR_BENCH \u2192 \"<lr>\".   After finishing all sweeps we\npersist the whole structure to working/experiment_data.npy so it can be plotted\nlater.", "Hyperparam tuning name: batch_size. We sweep over several mini-batch sizes (16,\n32, 64, 128).   For each value we rebuild the DataLoader, re-initialise a fresh\nGCN model, train for a few epochs, evaluate on the dev split, and log losses\nplus the dual-weighted accuracy after every epoch.   Results are stored in\nexperiment_data under\nexperiment_data['batch_size_tuning']['SPR_BENCH'][<batch_size>] and finally\npersisted with np.save.", "Hyperparam tuning name: weight_decay. We loop over five candidate weight-decay\nvalues, re-initialising the GCN and its Adam optimiser each time.  For every\nsetting we train for 5 epochs, evaluate Dual-Weighted Accuracy on the dev split\nafter every epoch, log losses/metrics, and finally write everything into a\nnested experiment_data dictionary under the top key \"weight_decay\" \u2192 \"SPR_BENCH\"\n\u2192 f\"wd_{value}\".  The whole script is self-contained and ends by saving\nexperiment_data.npy.", "Hyperparam tuning name: embedding_dim. We load the SPR_BENCH dataset (or\ngenerate a toy fallback), build graph objects once, and then sweep several\nembedding dimensionalities (32 / 64 / 128).   For each embedding size we\ninstantiate a fresh GCN whose embedding layer uses that dimension, train for a\nfew epochs, evaluate on the dev split, log losses and the dual-weighted\naccuracy, and store all results in a hierarchical experiment_data dictionary\nunder the top-level key \"embedding_dim\".   Finally we persist everything with\nnp.save(\"experiment_data.npy\") in the required format.", "Hyperparam tuning name: dropout_rate. The solution loops over dropout\nprobabilities from 0.0 \u2192 0.5 (step = 0.1), instantiates a fresh GCN that\ncontains `nn.Dropout(p=rate)` layers, trains for five epochs, evaluates after\neach epoch, and logs losses and DWA.   For every rate the script stores train /\nvalidation curves, predictions, ground-truth labels and timestamps inside the\nrequired `experiment_data` dictionary and finally saves everything to\n`experiment_data.npy`.", "Hyperparam tuning name: pooling_type. We explore four graph-level readout\nfunctions \u2013 mean, max, add and attention \u2013 by training the same GCN for a few\nepochs with each pooling strategy and logging losses, dual-weighted accuracy and\npredictions. Results for every pooling type are stored under\nexperiment_data['pooling_type'][pool_name]['SPR_BENCH'] and the whole dictionary\nis written to experiment_data.npy.", "Hyperparam tuning name: gcn_hidden_dim. The script iterates over\nhidden\u2010dimension values {64, 128, 256}.   For each setting it builds a fresh\nGCN, trains five epochs, evaluates dual-weighted accuracy on the dev set, and\nlogs per-epoch losses/metrics plus final predictions.   All results are\norganised in the experiment_data dict under the key \"gcn_hidden_dim\" \u2192\n\"SPR_BENCH\" \u2192 hidden_dim and finally saved to experiment_data.npy for later\nplotting/analysis.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        # synthetic fallback\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes = \"ABC\"\n            colors = \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[tok] for tok in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(n - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(seq, lbl) for seq, lbl in zip(split[\"sequence\"], split[\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = self.conv1(x, data.edge_index)\n        x = torch.relu(x)\n        x = self.conv2(x, data.edge_index)\n        x = torch.relu(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nmodel = GCN(vocab_size, num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * batch.num_graphs\n    train_loss /= len(train_graphs)\n\n    # validation\n    model.eval()\n    val_loss = 0\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    val_loss /= len(dev_graphs)\n    dwa = dual_weighted_accuracy(all_seqs, all_labels, all_preds)\n\n    # logging\n    print(\n        f\"Epoch {epoch}: training_loss = {train_loss:.4f}, validation_loss = {val_loss:.4f}, DWA = {dwa:.4f}\"\n    )\n    ts = time.time()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((ts, train_loss))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((ts, val_loss))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((ts, dwa))\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(ts)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, string, time, numpy as np, torch, warnings\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\nwarnings.filterwarnings(\"ignore\")\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes, colors = \"ABC\", \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(500))\n        dset[\"dev\"] = Dataset.from_dict(make_split(100))\n        dset[\"test\"] = Dataset.from_dict(make_split(100))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0-padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    x = torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n    edge = []\n    for i in range(len(tokens) - 1):\n        edge.append([i, i + 1])\n        edge.append([i + 1, i])\n    edge_index = torch.tensor(edge, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- hyper-parameter sweep ----------\nepoch_options = [10, 20, 30, 50]\nearly_patience = 3\nexperiment_data = {\"num_epochs\": {}}\n\n\ndef evaluate(model, loader):\n    model.eval()\n    tot_loss, preds, labels, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            tot_loss += loss.item() * batch.num_graphs\n            preds.extend(out.argmax(1).cpu().tolist())\n            labels.extend(batch.y.view(-1).cpu().tolist())\n            seqs.extend(batch.seq)\n    tot_loss /= len(loader.dataset)\n    dwa = dual_weighted_accuracy(seqs, labels, preds)\n    return tot_loss, dwa, preds, labels, seqs\n\n\nfor num_epochs in epoch_options:\n    print(f\"\\n==== Training for {num_epochs} epochs ====\")\n    model = GCN(vocab_size, num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    run_data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n        \"best_epoch\": None,\n        \"best_dwa\": -1,\n    }\n\n    best_state = None\n    patience_cnt = 0\n    for epoch in range(1, num_epochs + 1):\n        # --- train ---\n        model.train()\n        train_loss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.num_graphs\n        train_loss /= len(train_graphs)\n\n        # --- validate ---\n        val_loss, dwa, preds, labels, seqs = evaluate(model, dev_loader)\n\n        ts = time.time()\n        run_data[\"losses\"][\"train\"].append((ts, train_loss))\n        run_data[\"losses\"][\"val\"].append((ts, val_loss))\n        run_data[\"metrics\"][\"train\"].append(None)\n        run_data[\"metrics\"][\"val\"].append((ts, dwa))\n        run_data[\"timestamps\"].append(ts)\n\n        print(\n            f\"Epoch {epoch}/{num_epochs}  \"\n            f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  DWA={dwa:.4f}\"\n        )\n\n        # best tracking\n        if dwa > run_data[\"best_dwa\"]:\n            run_data[\"best_dwa\"] = dwa\n            run_data[\"best_epoch\"] = epoch\n            run_data[\"predictions\"] = preds\n            run_data[\"ground_truth\"] = labels\n            best_state = model.state_dict()\n            patience_cnt = 0\n        else:\n            patience_cnt += 1\n        if patience_cnt >= early_patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    # reload best state and store\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    experiment_data[\"num_epochs\"][str(num_epochs)] = {\"SPR_BENCH\": run_data}\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes, colors = \"ABC\", \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[tok] for tok in tokens], dtype=torch.long)\n    edge_pairs = [[i, i + 1] for i in range(n - 1)]\n    edge_index = (\n        torch.tensor(edge_pairs + [[j, i] for i, j in edge_pairs], dtype=torch.long)\n        .t()\n        .contiguous()\n    )\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(seq, lbl) for seq, lbl in zip(split[\"sequence\"], split[\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ---------- experiment data ----------\nexperiment_data = {\"learning_rate\": {\"SPR_BENCH\": {}}}\n\n# ---------- hyperparameter sweep ----------\nlearning_rates = [3e-4, 1e-4, 3e-3]\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nepochs = 5\n\nfor lr in learning_rates:\n    key = f\"lr={lr}\"\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    # fresh loaders each sweep to reshuffle consistently\n    train_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n    model = GCN(vocab_size, num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(1, epochs + 1):\n        # ---- train ----\n        model.train()\n        running = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            running += loss.item() * batch.num_graphs\n        train_loss = running / len(train_graphs)\n\n        # ---- validation ----\n        model.eval()\n        val_running, preds, labels, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = criterion(out, batch.y.view(-1))\n                val_running += loss.item() * batch.num_graphs\n                preds.extend(out.argmax(dim=1).cpu().tolist())\n                labels.extend(batch.y.view(-1).cpu().tolist())\n                seqs.extend(batch.seq)\n        val_loss = val_running / len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n\n        ts = time.time()\n        data_slot = experiment_data[\"learning_rate\"][\"SPR_BENCH\"][key]\n        data_slot[\"losses\"][\"train\"].append((ts, train_loss))\n        data_slot[\"losses\"][\"val\"].append((ts, val_loss))\n        data_slot[\"metrics\"][\"train\"].append(None)\n        data_slot[\"metrics\"][\"val\"].append((ts, dwa))\n        data_slot[\"predictions\"] = preds\n        data_slot[\"ground_truth\"] = labels\n        data_slot[\"timestamps\"].append(ts)\n\n        print(\n            f\"[{key}] Epoch {epoch}: train_loss={train_loss:.4f}, \"\n            f\"val_loss={val_loss:.4f}, DWA={dwa:.4f}\"\n        )\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- experiment dict ----------\nexperiment_data = {\"batch_size_tuning\": {}}\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_t, y_p):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_t, y_p)\n        + shape_weighted_accuracy(seqs, y_t, y_p)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        d = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, generating synthetic:\", e)\n\n        def rand_seq():\n            shapes = \"ABC\"\n            colors = \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        d = DatasetDict(\n            train=Dataset.from_dict(make_split(200)),\n            dev=Dataset.from_dict(make_split(50)),\n            test=Dataset.from_dict(make_split(50)),\n        )\n    return d\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set(\n    tok\n    for seq in dset[\"train\"][\"sequence\"]\n    + dset[\"dev\"][\"sequence\"]\n    + dset[\"test\"][\"sequence\"]\n    for tok in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 padding\n\n# ---------- graph helpers ----------\nfrom torch_geometric.data import Data\n\n\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(n - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- hyperparameter sweep ----------\nbatch_sizes = [16, 32, 64, 128]\nexperiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"] = {}\nepochs = 5\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch_size={bs} ===\")\n    # loaders\n    train_loader = DataLoader(train_graphs, batch_size=bs, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=max(bs * 2, 128), shuffle=False)\n    # model & optimizer\n    model = GCN(vocab_size, num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    # storage\n    exp_entry = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        train_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.num_graphs\n        train_loss /= len(train_graphs)\n        # val\n        model.eval()\n        val_loss = 0.0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = criterion(out, batch.y.view(-1))\n                val_loss += loss.item() * batch.num_graphs\n                preds.extend(out.argmax(1).cpu().tolist())\n                labels.extend(batch.y.view(-1).cpu().tolist())\n                seqs.extend(batch.seq)\n        val_loss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        # log\n        ts = time.time()\n        exp_entry[\"losses\"][\"train\"].append((ts, train_loss))\n        exp_entry[\"losses\"][\"val\"].append((ts, val_loss))\n        exp_entry[\"metrics\"][\"train\"].append(None)\n        exp_entry[\"metrics\"][\"val\"].append((ts, dwa))\n        exp_entry[\"predictions\"] = preds\n        exp_entry[\"ground_truth\"] = labels\n        exp_entry[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | DWA {dwa:.4f}\"\n        )\n    experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"][str(bs)] = exp_entry\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- reproducibility --------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n# -------------------- working dir ------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n# -------------------- device ----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------- metrics ---------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# -------------------- dataset ---------------------------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef get_dataset():\n    path = pathlib.Path(\n        os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    )\n    try:\n        d = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        d = DatasetDict(\n            train=Dataset.from_dict(make_split(200)),\n            dev=Dataset.from_dict(make_split(50)),\n            test=Dataset.from_dict(make_split(50)),\n        )\n    return d\n\n\ndset = get_dataset()\n# -------------------- vocab -----------------------------\nall_tokens = set(\n    tok for split in dset.values() for seq in split[\"sequence\"] for tok in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1\n\n\n# -------------------- graph utils -----------------------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n    edge_index = [[i, i + 1] for i in range(n - 1)] + [[i + 1, i] for i in range(n - 1)]\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build_graph_list(dset[\"train\"]), build_graph_list(\n    dset[\"dev\"]\n)\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# -------------------- model -----------------------------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\n# -------------------- hyperparameter tuning -------------\nweight_decays = [0.0, 1e-6, 1e-5, 1e-4, 1e-3]\nexperiment_data = {\"weight_decay\": {\"SPR_BENCH\": {}}}\ncriterion = nn.CrossEntropyLoss()\nepochs = 5\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    model = GCN(vocab_size, num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    run_data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        train_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.num_graphs\n        train_loss /= len(train_graphs)\n        # validate\n        model.eval()\n        val_loss = 0.0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = criterion(out, batch.y.view(-1))\n                val_loss += loss.item() * batch.num_graphs\n                preds.extend(out.argmax(1).cpu().tolist())\n                labels.extend(batch.y.view(-1).cpu().tolist())\n                seqs.extend(batch.seq)\n        val_loss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        ts = time.time()\n        run_data[\"losses\"][\"train\"].append((ts, train_loss))\n        run_data[\"losses\"][\"val\"].append((ts, val_loss))\n        run_data[\"metrics\"][\"train\"].append(None)\n        run_data[\"metrics\"][\"val\"].append((ts, dwa))\n        run_data[\"predictions\"] = preds\n        run_data[\"ground_truth\"] = labels\n        run_data[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, DWA={dwa:.4f}\"\n        )\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][f\"wd_{wd}\"] = run_data\n# -------------------- save ------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _load(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _load(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    path = pathlib.Path(path_env)\n    try:\n        dset = load_spr_bench(path)\n        print(\"Loaded SPR_BENCH from\", path)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n\n        def rand_seq():\n            shapes, colors = \"ABC\", \"XYZ\"\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(200))\n        dset[\"dev\"] = Dataset.from_dict(make_split(50))\n        dset[\"test\"] = Dataset.from_dict(make_split(50))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 for padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    x = torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n    edge_index = []\n    for i in range(len(tokens) - 1):\n        edge_index.append([i, i + 1])\n        edge_index.append([i + 1, i])\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x,\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq=sequence,\n    )\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs = build_graph_list(dset[\"train\"])\ndev_graphs = build_graph_list(dset[\"dev\"])\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes, emb_dim):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim)\n        self.conv1 = GCNConv(emb_dim, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ---------- experiment data container ----------\nexperiment_data = {\"embedding_dim\": {}}\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nembedding_dims = [32, 64, 128]  # hyper-parameter sweep\nepochs = 5\nfor emb_dim in embedding_dims:\n    print(f\"\\n======== Training with embedding_dim = {emb_dim} ========\")\n    # dataloaders (recreated each sweep for shuffling independence)\n    train_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n    model = GCN(vocab_size, num_classes, emb_dim).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    key = f\"dim_{emb_dim}\"\n    experiment_data[\"embedding_dim\"].setdefault(\"SPR_BENCH\", {})[key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    store = experiment_data[\"embedding_dim\"][\"SPR_BENCH\"][key]\n\n    for epoch in range(1, epochs + 1):\n        # --- train ---\n        model.train()\n        train_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.num_graphs\n        train_loss /= len(train_graphs)\n\n        # --- dev ---\n        model.eval()\n        val_loss, preds, labels, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                val_loss += criterion(out, batch.y.view(-1)).item() * batch.num_graphs\n                preds.extend(out.argmax(1).cpu().tolist())\n                labels.extend(batch.y.view(-1).cpu().tolist())\n                seqs.extend(batch.seq)\n        val_loss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n\n        ts = time.time()\n        store[\"losses\"][\"train\"].append((ts, train_loss))\n        store[\"losses\"][\"val\"].append((ts, val_loss))\n        store[\"metrics\"][\"train\"].append(None)\n        store[\"metrics\"][\"val\"].append((ts, dwa))\n        store[\"predictions\"] = preds\n        store[\"ground_truth\"] = labels\n        store[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {epoch:02d} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | DWA {dwa:.4f}\"\n        )\n\n# ---------- save all ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_t, y_p):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_t, y_p)\n        + shape_weighted_accuracy(seqs, y_t, y_p)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(fname):  # helper\n        return load_dataset(\n            \"csv\", data_files=str(path / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for split in [\"train.csv\", \"dev.csv\", \"test.csv\"]:\n        d[split.split(\".\")[0]] = _load(split)\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        ds = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, using synthetic set:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        ds = DatasetDict()\n        ds[\"train\"] = Dataset.from_dict(make_split(400))\n        ds[\"dev\"] = Dataset.from_dict(make_split(100))\n        ds[\"test\"] = Dataset.from_dict(make_split(100))\n    return ds\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set()\nfor split in dset.values():\n    for seq in split[\"sequence\"]:\n        all_tokens.update(seq.split())\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # padding=0\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    toks = sequence.split()\n    n = len(toks)\n    x = torch.tensor([token2id[t] for t in toks], dtype=torch.long)\n    edge = []\n    for i in range(n - 1):\n        edge.append([i, i + 1])\n        edge.append([i + 1, i])\n    edge = torch.tensor(edge, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x, edge_index=edge, y=torch.tensor([label], dtype=torch.long), seq=sequence\n    )\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build_graph_list(dset[\"train\"]), build_graph_list(\n    dset[\"dev\"]\n)\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\n\n\n# ---------- model with dropout ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, classes, dropout_rate: float):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1, self.conv2 = GCNConv(64, 128), GCNConv(128, 128)\n        self.dp = nn.Dropout(p=dropout_rate)\n        self.lin = nn.Linear(128, classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = self.dp(x)\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = self.dp(x)\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ---------- experiment data ----------\nexperiment_data = {\"dropout_rate_tuning\": {\"SPR_BENCH\": {}}}\n\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- hyper-parameter sweep ----------\nfor rate in np.arange(0.0, 0.51, 0.1):\n    rate = round(float(rate), 2)\n    print(f\"\\n=== Training with dropout={rate} ===\")\n    model = GCN(vocab_size, num_classes, dropout_rate=rate).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    data_record = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    epochs = 5\n    for ep in range(1, epochs + 1):\n        # ----- train -----\n        model.train()\n        tr_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            tr_loss += loss.item() * batch.num_graphs\n        tr_loss /= len(train_graphs)\n\n        # ----- validation -----\n        model.eval()\n        val_loss = 0.0\n        preds, labels, seqs = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = criterion(out, batch.y.view(-1))\n                val_loss += loss.item() * batch.num_graphs\n                preds.extend(out.argmax(1).cpu().tolist())\n                labels.extend(batch.y.view(-1).cpu().tolist())\n                seqs.extend(batch.seq)\n        val_loss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n\n        ts = time.time()\n        data_record[\"losses\"][\"train\"].append((ts, tr_loss))\n        data_record[\"losses\"][\"val\"].append((ts, val_loss))\n        data_record[\"metrics\"][\"train\"].append(None)\n        data_record[\"metrics\"][\"val\"].append((ts, dwa))\n        data_record[\"predictions\"] = preds\n        data_record[\"ground_truth\"] = labels\n        data_record[\"timestamps\"].append(ts)\n\n        print(\n            f\"Ep {ep} | tr_loss {tr_loss:.4f} | val_loss {val_loss:.4f} | DWA {dwa:.4f}\"\n        )\n    experiment_data[\"dropout_rate_tuning\"][\"SPR_BENCH\"][str(rate)] = data_record\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# hyperparameter-tuning : pooling_type\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import (\n    GCNConv,\n    global_mean_pool,\n    global_max_pool,\n    global_add_pool,\n    GlobalAttention,\n)\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence):\n    return len(set(t[1] for t in sequence.split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence):\n    return len(set(t[0] for t in sequence.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y, p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y, p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y, p):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y, p) + shape_weighted_accuracy(seqs, y, p)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path):\n    def _l(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _l(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _l(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        d = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic data:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        d = DatasetDict()\n        d[\"train\"] = Dataset.from_dict(make(200))\n        d[\"dev\"] = Dataset.from_dict(make(50))\n        d[\"test\"] = Dataset.from_dict(make(50))\n    return d\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set(\n    t for split in dset.values() for seq in split[\"sequence\"] for t in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1\n\n# ---------- graphs ----------\nfrom torch_geometric.data import Data\n\n\ndef seq_to_graph(seq, lbl):\n    toks = seq.split()\n    n = len(toks)\n    x = torch.tensor([token2id[t] for t in toks], dtype=torch.long)\n    edges = [[i, i + 1] for i in range(n - 1)] + [[i + 1, i] for i in range(n - 1)]\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long), seq=seq\n    )\n\n\ndef build(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build(dset[\"train\"]), build(dset[\"dev\"])\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes, pooling=\"mean\"):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1, self.conv2 = GCNConv(64, 128), GCNConv(128, 128)\n        self.pooling_type = pooling\n        if pooling == \"mean\":\n            self.pool = lambda x, b: global_mean_pool(x, b)\n        elif pooling == \"max\":\n            self.pool = lambda x, b: global_max_pool(x, b)\n        elif pooling == \"add\":\n            self.pool = lambda x, b: global_add_pool(x, b)\n        elif pooling == \"attn\":\n            gate = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n            self.attn = GlobalAttention(gate)\n            self.pool = lambda x, b: self.attn(x, b)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = self.pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\npool_options = [\"mean\", \"max\", \"add\", \"attn\"]\nexperiment_data = {\"pooling_type\": {}}\nepochs = 5\n\nfor pool in pool_options:\n    print(f\"\\n=== Training with {pool} pooling ===\")\n    model = GCN(vocab_size, num_classes, pool).to(device)\n    opt = Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tloss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            opt.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y.view(-1))\n            loss.backward()\n            opt.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(train_graphs)\n        # val\n        model.eval()\n        vloss = 0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = crit(out, batch.y.view(-1))\n                vloss += loss.item() * batch.num_graphs\n                preds += out.argmax(1).cpu().tolist()\n                labels += batch.y.view(-1).cpu().tolist()\n                seqs += batch.seq\n        vloss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        ts = time.time()\n        log[\"losses\"][\"train\"].append((ts, tloss))\n        log[\"losses\"][\"val\"].append((ts, vloss))\n        log[\"metrics\"][\"train\"].append(None)\n        log[\"metrics\"][\"val\"].append((ts, dwa))\n        log[\"predictions\"], log[\"ground_truth\"] = preds, labels\n        log[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {ep}/{epochs} | TrainLoss {tloss:.4f} | ValLoss {vloss:.4f} | DWA {dwa:.4f}\"\n        )\n    experiment_data[\"pooling_type\"][pool] = {\"SPR_BENCH\": log}\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import load_dataset, DatasetDict, Dataset\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_color_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    w = [count_shape_variety(s) for s in sequences]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y_true, y_pred):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y_true, y_pred)\n        + shape_weighted_accuracy(seqs, y_true, y_pred)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        dset = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic toy data:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make_split(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        dset = DatasetDict()\n        dset[\"train\"] = Dataset.from_dict(make_split(400))\n        dset[\"dev\"] = Dataset.from_dict(make_split(80))\n        dset[\"test\"] = Dataset.from_dict(make_split(80))\n    return dset\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set(\n    tok for seqs in dset.values() for seq in seqs[\"sequence\"] for tok in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1  # 0 = padding\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(sequence: str, label: int):\n    tokens = sequence.split()\n    x = torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n    edges = []\n    for i in range(len(tokens) - 1):\n        edges.extend([[i, i + 1], [i + 1, i]])\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    return Data(x=x, edge_index=edge_index, y=torch.tensor([label]), seq=sequence)\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build_graph_list(dset[\"train\"]), build_graph_list(\n    dset[\"dev\"]\n)\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes, hidden_dim=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1 = GCNConv(64, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\n\n# ---------- experiment data ----------\nexperiment_data = {\"gcn_hidden_dim\": {\"SPR_BENCH\": {}}}\n\nhidden_dims = [64, 128, 256]\nepochs = 5\n\nfor hdim in hidden_dims:\n    print(f\"\\n--- Training with hidden_dim = {hdim} ---\")\n    model = GCN(vocab_size, num_classes, hdim).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    exp_entry = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tr_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            tr_loss += loss.item() * batch.num_graphs\n        tr_loss /= len(train_graphs)\n\n        model.eval()\n        val_loss, preds, labels, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = criterion(out, batch.y.view(-1))\n                val_loss += loss.item() * batch.num_graphs\n                preds.extend(out.argmax(1).cpu().tolist())\n                labels.extend(batch.y.view(-1).cpu().tolist())\n                seqs.extend(batch.seq)\n        val_loss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        ts = time.time()\n        exp_entry[\"losses\"][\"train\"].append((ts, tr_loss))\n        exp_entry[\"losses\"][\"val\"].append((ts, val_loss))\n        exp_entry[\"metrics\"][\"train\"].append(None)\n        exp_entry[\"metrics\"][\"val\"].append((ts, dwa))\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, DWA={dwa:.4f}\"\n        )\n    # save last epoch preds / gts\n    exp_entry[\"predictions\"] = preds\n    exp_entry[\"ground_truth\"] = labels\n    experiment_data[\"gcn_hidden_dim\"][\"SPR_BENCH\"][str(hdim)] = exp_entry\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# hyperparameter-tuning : pooling_type\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import (\n    GCNConv,\n    global_mean_pool,\n    global_max_pool,\n    global_add_pool,\n    GlobalAttention,\n)\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence):\n    return len(set(t[1] for t in sequence.split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence):\n    return len(set(t[0] for t in sequence.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y, p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y, p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y, p):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y, p) + shape_weighted_accuracy(seqs, y, p)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path):\n    def _l(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _l(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _l(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        d = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic data:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        d = DatasetDict()\n        d[\"train\"] = Dataset.from_dict(make(200))\n        d[\"dev\"] = Dataset.from_dict(make(50))\n        d[\"test\"] = Dataset.from_dict(make(50))\n    return d\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set(\n    t for split in dset.values() for seq in split[\"sequence\"] for t in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1\n\n# ---------- graphs ----------\nfrom torch_geometric.data import Data\n\n\ndef seq_to_graph(seq, lbl):\n    toks = seq.split()\n    n = len(toks)\n    x = torch.tensor([token2id[t] for t in toks], dtype=torch.long)\n    edges = [[i, i + 1] for i in range(n - 1)] + [[i + 1, i] for i in range(n - 1)]\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long), seq=seq\n    )\n\n\ndef build(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build(dset[\"train\"]), build(dset[\"dev\"])\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes, pooling=\"mean\"):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1, self.conv2 = GCNConv(64, 128), GCNConv(128, 128)\n        self.pooling_type = pooling\n        if pooling == \"mean\":\n            self.pool = lambda x, b: global_mean_pool(x, b)\n        elif pooling == \"max\":\n            self.pool = lambda x, b: global_max_pool(x, b)\n        elif pooling == \"add\":\n            self.pool = lambda x, b: global_add_pool(x, b)\n        elif pooling == \"attn\":\n            gate = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n            self.attn = GlobalAttention(gate)\n            self.pool = lambda x, b: self.attn(x, b)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = self.pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\npool_options = [\"mean\", \"max\", \"add\", \"attn\"]\nexperiment_data = {\"pooling_type\": {}}\nepochs = 5\n\nfor pool in pool_options:\n    print(f\"\\n=== Training with {pool} pooling ===\")\n    model = GCN(vocab_size, num_classes, pool).to(device)\n    opt = Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tloss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            opt.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y.view(-1))\n            loss.backward()\n            opt.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(train_graphs)\n        # val\n        model.eval()\n        vloss = 0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = crit(out, batch.y.view(-1))\n                vloss += loss.item() * batch.num_graphs\n                preds += out.argmax(1).cpu().tolist()\n                labels += batch.y.view(-1).cpu().tolist()\n                seqs += batch.seq\n        vloss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        ts = time.time()\n        log[\"losses\"][\"train\"].append((ts, tloss))\n        log[\"losses\"][\"val\"].append((ts, vloss))\n        log[\"metrics\"][\"train\"].append(None)\n        log[\"metrics\"][\"val\"].append((ts, dwa))\n        log[\"predictions\"], log[\"ground_truth\"] = preds, labels\n        log[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {ep}/{epochs} | TrainLoss {tloss:.4f} | ValLoss {vloss:.4f} | DWA {dwa:.4f}\"\n        )\n    experiment_data[\"pooling_type\"][pool] = {\"SPR_BENCH\": log}\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# hyperparameter-tuning : pooling_type\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import (\n    GCNConv,\n    global_mean_pool,\n    global_max_pool,\n    global_add_pool,\n    GlobalAttention,\n)\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence):\n    return len(set(t[1] for t in sequence.split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence):\n    return len(set(t[0] for t in sequence.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y, p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y, p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y, p):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y, p) + shape_weighted_accuracy(seqs, y, p)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path):\n    def _l(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _l(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _l(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        d = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic data:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        d = DatasetDict()\n        d[\"train\"] = Dataset.from_dict(make(200))\n        d[\"dev\"] = Dataset.from_dict(make(50))\n        d[\"test\"] = Dataset.from_dict(make(50))\n    return d\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set(\n    t for split in dset.values() for seq in split[\"sequence\"] for t in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1\n\n# ---------- graphs ----------\nfrom torch_geometric.data import Data\n\n\ndef seq_to_graph(seq, lbl):\n    toks = seq.split()\n    n = len(toks)\n    x = torch.tensor([token2id[t] for t in toks], dtype=torch.long)\n    edges = [[i, i + 1] for i in range(n - 1)] + [[i + 1, i] for i in range(n - 1)]\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long), seq=seq\n    )\n\n\ndef build(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build(dset[\"train\"]), build(dset[\"dev\"])\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes, pooling=\"mean\"):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1, self.conv2 = GCNConv(64, 128), GCNConv(128, 128)\n        self.pooling_type = pooling\n        if pooling == \"mean\":\n            self.pool = lambda x, b: global_mean_pool(x, b)\n        elif pooling == \"max\":\n            self.pool = lambda x, b: global_max_pool(x, b)\n        elif pooling == \"add\":\n            self.pool = lambda x, b: global_add_pool(x, b)\n        elif pooling == \"attn\":\n            gate = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n            self.attn = GlobalAttention(gate)\n            self.pool = lambda x, b: self.attn(x, b)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = self.pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\npool_options = [\"mean\", \"max\", \"add\", \"attn\"]\nexperiment_data = {\"pooling_type\": {}}\nepochs = 5\n\nfor pool in pool_options:\n    print(f\"\\n=== Training with {pool} pooling ===\")\n    model = GCN(vocab_size, num_classes, pool).to(device)\n    opt = Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tloss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            opt.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y.view(-1))\n            loss.backward()\n            opt.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(train_graphs)\n        # val\n        model.eval()\n        vloss = 0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = crit(out, batch.y.view(-1))\n                vloss += loss.item() * batch.num_graphs\n                preds += out.argmax(1).cpu().tolist()\n                labels += batch.y.view(-1).cpu().tolist()\n                seqs += batch.seq\n        vloss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        ts = time.time()\n        log[\"losses\"][\"train\"].append((ts, tloss))\n        log[\"losses\"][\"val\"].append((ts, vloss))\n        log[\"metrics\"][\"train\"].append(None)\n        log[\"metrics\"][\"val\"].append((ts, dwa))\n        log[\"predictions\"], log[\"ground_truth\"] = preds, labels\n        log[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {ep}/{epochs} | TrainLoss {tloss:.4f} | ValLoss {vloss:.4f} | DWA {dwa:.4f}\"\n        )\n    experiment_data[\"pooling_type\"][pool] = {\"SPR_BENCH\": log}\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# hyperparameter-tuning : pooling_type\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import (\n    GCNConv,\n    global_mean_pool,\n    global_max_pool,\n    global_add_pool,\n    GlobalAttention,\n)\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- metrics ----------\ndef count_color_variety(sequence):\n    return len(set(t[1] for t in sequence.split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence):\n    return len(set(t[0] for t in sequence.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y, p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y, p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == q else 0 for wt, t, q in zip(w, y, p)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef dual_weighted_accuracy(seqs, y, p):\n    return 0.5 * (\n        color_weighted_accuracy(seqs, y, p) + shape_weighted_accuracy(seqs, y, p)\n    )\n\n\n# ---------- dataset loader ----------\ndef load_spr_bench(path):\n    def _l(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _l(pathlib.Path(\"train.csv\"))\n    d[\"dev\"] = _l(pathlib.Path(\"dev.csv\"))\n    d[\"test\"] = _l(pathlib.Path(\"test.csv\"))\n    return d\n\n\ndef get_dataset():\n    path_env = os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        d = load_spr_bench(pathlib.Path(path_env))\n        print(\"Loaded SPR_BENCH from\", path_env)\n    except Exception as e:\n        print(\"Dataset not found, creating synthetic data:\", e)\n        shapes, colors = \"ABC\", \"XYZ\"\n\n        def rand_seq():\n            return \" \".join(\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(3, 8))\n            )\n\n        def make(n):\n            return {\n                \"id\": list(range(n)),\n                \"sequence\": [rand_seq() for _ in range(n)],\n                \"label\": [random.randint(0, 3) for _ in range(n)],\n            }\n\n        from datasets import Dataset\n\n        d = DatasetDict()\n        d[\"train\"] = Dataset.from_dict(make(200))\n        d[\"dev\"] = Dataset.from_dict(make(50))\n        d[\"test\"] = Dataset.from_dict(make(50))\n    return d\n\n\ndset = get_dataset()\n\n# ---------- vocab ----------\nall_tokens = set(\n    t for split in dset.values() for seq in split[\"sequence\"] for t in seq.split()\n)\ntoken2id = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}\nvocab_size = len(token2id) + 1\n\n# ---------- graphs ----------\nfrom torch_geometric.data import Data\n\n\ndef seq_to_graph(seq, lbl):\n    toks = seq.split()\n    n = len(toks)\n    x = torch.tensor([token2id[t] for t in toks], dtype=torch.long)\n    edges = [[i, i + 1] for i in range(n - 1)] + [[i + 1, i] for i in range(n - 1)]\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    return Data(\n        x=x, edge_index=edge_index, y=torch.tensor([lbl], dtype=torch.long), seq=seq\n    )\n\n\ndef build(split):\n    return [seq_to_graph(s, l) for s, l in zip(split[\"sequence\"], split[\"label\"])]\n\n\ntrain_graphs, dev_graphs = build(dset[\"train\"]), build(dset[\"dev\"])\ntrain_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GCN(nn.Module):\n    def __init__(self, vocab, num_classes, pooling=\"mean\"):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, 64)\n        self.conv1, self.conv2 = GCNConv(64, 128), GCNConv(128, 128)\n        self.pooling_type = pooling\n        if pooling == \"mean\":\n            self.pool = lambda x, b: global_mean_pool(x, b)\n        elif pooling == \"max\":\n            self.pool = lambda x, b: global_max_pool(x, b)\n        elif pooling == \"add\":\n            self.pool = lambda x, b: global_add_pool(x, b)\n        elif pooling == \"attn\":\n            gate = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n            self.attn = GlobalAttention(gate)\n            self.pool = lambda x, b: self.attn(x, b)\n        self.lin = nn.Linear(128, num_classes)\n\n    def forward(self, data):\n        x = self.emb(data.x).to(device)\n        x = torch.relu(self.conv1(x, data.edge_index))\n        x = torch.relu(self.conv2(x, data.edge_index))\n        x = self.pool(x, data.batch)\n        return self.lin(x)\n\n\nnum_classes = len(set(dset[\"train\"][\"label\"]))\npool_options = [\"mean\", \"max\", \"add\", \"attn\"]\nexperiment_data = {\"pooling_type\": {}}\nepochs = 5\n\nfor pool in pool_options:\n    print(f\"\\n=== Training with {pool} pooling ===\")\n    model = GCN(vocab_size, num_classes, pool).to(device)\n    opt = Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tloss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            opt.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y.view(-1))\n            loss.backward()\n            opt.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(train_graphs)\n        # val\n        model.eval()\n        vloss = 0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                out = model(batch)\n                loss = crit(out, batch.y.view(-1))\n                vloss += loss.item() * batch.num_graphs\n                preds += out.argmax(1).cpu().tolist()\n                labels += batch.y.view(-1).cpu().tolist()\n                seqs += batch.seq\n        vloss /= len(dev_graphs)\n        dwa = dual_weighted_accuracy(seqs, labels, preds)\n        ts = time.time()\n        log[\"losses\"][\"train\"].append((ts, tloss))\n        log[\"losses\"][\"val\"].append((ts, vloss))\n        log[\"metrics\"][\"train\"].append(None)\n        log[\"metrics\"][\"val\"].append((ts, dwa))\n        log[\"predictions\"], log[\"ground_truth\"] = preds, labels\n        log[\"timestamps\"].append(ts)\n        print(\n            f\"Epoch {ep}/{epochs} | TrainLoss {tloss:.4f} | ValLoss {vloss:.4f} | DWA {dwa:.4f}\"\n        )\n    experiment_data[\"pooling_type\"][pool] = {\"SPR_BENCH\": log}\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 538912.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 657270.19\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 722545.44\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Epoch 1: training_loss = 0.4599, validation_loss\n= 0.4034, DWA = 0.8367', '\\n', 'Epoch 2: training_loss = 0.3646, validation_loss\n= 0.3432, DWA = 0.8741', '\\n', 'Epoch 3: training_loss = 0.3209, validation_loss\n= 0.2977, DWA = 0.8959', '\\n', 'Epoch 4: training_loss = 0.2836, validation_loss\n= 0.2824, DWA = 0.9078', '\\n', 'Epoch 5: training_loss = 0.2564, validation_loss\n= 0.2475, DWA = 0.9160', '\\n', 'Execution time: 27 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 671701.23\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 788314.10\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 887965.28\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n==== Training for 10 epochs ====', '\\n',\n'Epoch 1/10  train_loss=0.4599  val_loss=0.4034  DWA=0.8367', '\\n', 'Epoch 2/10\ntrain_loss=0.3646  val_loss=0.3433  DWA=0.8736', '\\n', 'Epoch 3/10\ntrain_loss=0.3209  val_loss=0.2976  DWA=0.8955', '\\n', 'Epoch 4/10\ntrain_loss=0.2835  val_loss=0.2825  DWA=0.9092', '\\n', 'Epoch 5/10\ntrain_loss=0.2563  val_loss=0.2473  DWA=0.9151', '\\n', 'Epoch 6/10\ntrain_loss=0.2368  val_loss=0.2392  DWA=0.9128', '\\n', 'Epoch 7/10\ntrain_loss=0.2234  val_loss=0.2260  DWA=0.9237', '\\n', 'Epoch 8/10\ntrain_loss=0.2108  val_loss=0.2107  DWA=0.9370', '\\n', 'Epoch 9/10\ntrain_loss=0.2017  val_loss=0.2048  DWA=0.9395', '\\n', 'Epoch 10/10\ntrain_loss=0.1950  val_loss=0.1949  DWA=0.9381', '\\n', '\\n==== Training for 20\nepochs ====', '\\n', 'Epoch 1/20  train_loss=0.4696  val_loss=0.4024\nDWA=0.8256', '\\n', 'Epoch 2/20  train_loss=0.3744  val_loss=0.3535  DWA=0.8687',\n'\\n', 'Epoch 3/20  train_loss=0.3327  val_loss=0.3144  DWA=0.8902', '\\n', 'Epoch\n4/20  train_loss=0.2981  val_loss=0.2839  DWA=0.9021', '\\n', 'Epoch 5/20\ntrain_loss=0.2687  val_loss=0.2619  DWA=0.9204', '\\n', 'Epoch 6/20\ntrain_loss=0.2486  val_loss=0.2465  DWA=0.9167', '\\n', 'Epoch 7/20\ntrain_loss=0.2326  val_loss=0.2658  DWA=0.9023', '\\n', 'Epoch 8/20\ntrain_loss=0.2201  val_loss=0.2618  DWA=0.8950', '\\n', 'Early stopping\ntriggered.', '\\n', '\\n==== Training for 30 epochs ====', '\\n', 'Epoch 1/30\ntrain_loss=0.4615  val_loss=0.3902  DWA=0.8368', '\\n', 'Epoch 2/30\ntrain_loss=0.3667  val_loss=0.3434  DWA=0.8674', '\\n', 'Epoch 3/30\ntrain_loss=0.3250  val_loss=0.3294  DWA=0.8667', '\\n', 'Epoch 4/30\ntrain_loss=0.2892  val_loss=0.2800  DWA=0.8971', '\\n', 'Epoch 5/30\ntrain_loss=0.2669  val_loss=0.2545  DWA=0.9127', '\\n', 'Epoch 6/30\ntrain_loss=0.2439  val_loss=0.2430  DWA=0.9159', '\\n', 'Epoch 7/30\ntrain_loss=0.2293  val_loss=0.2417  DWA=0.9151', '\\n', 'Epoch 8/30\ntrain_loss=0.2171  val_loss=0.2141  DWA=0.9312', '\\n', 'Epoch 9/30\ntrain_loss=0.2113  val_loss=0.2048  DWA=0.9396', '\\n', 'Epoch 10/30\ntrain_loss=0.2001  val_loss=0.1985  DWA=0.9417', '\\n', 'Epoch 11/30\ntrain_loss=0.1964  val_loss=0.1974  DWA=0.9402', '\\n', 'Epoch 12/30\ntrain_loss=0.1898  val_loss=0.1896  DWA=0.9455', '\\n', 'Epoch 13/30\ntrain_loss=0.1845  val_loss=0.1947  DWA=0.9363', '\\n', 'Epoch 14/30\ntrain_loss=0.1815  val_loss=0.1859  DWA=0.9452', '\\n', 'Epoch 15/30\ntrain_loss=0.1760  val_loss=0.1920  DWA=0.9401', '\\n', 'Early stopping\ntriggered.', '\\n', '\\n==== Training for 50 epochs ====', '\\n', 'Epoch 1/50\ntrain_loss=0.4676  val_loss=0.3915  DWA=0.8410', '\\n', 'Epoch 2/50\ntrain_loss=0.3645  val_loss=0.3498  DWA=0.8614', '\\n', 'Epoch 3/50\ntrain_loss=0.3169  val_loss=0.3047  DWA=0.8947', '\\n', 'Epoch 4/50\ntrain_loss=0.2837  val_loss=0.2872  DWA=0.8842', '\\n', 'Epoch 5/50\ntrain_loss=0.2600  val_loss=0.2486  DWA=0.9150', '\\n', 'Epoch 6/50\ntrain_loss=0.2402  val_loss=0.2304  DWA=0.9267', '\\n', 'Epoch 7/50\ntrain_loss=0.2236  val_loss=0.2298  DWA=0.9203', '\\n', 'Epoch 8/50\ntrain_loss=0.2134  val_loss=0.2137  DWA=0.9328', '\\n', 'Epoch 9/50\ntrain_loss=0.2060  val_loss=0.2155  DWA=0.9265', '\\n', 'Epoch 10/50\ntrain_loss=0.1993  val_loss=0.2053  DWA=0.9399', '\\n', 'Epoch 11/50\ntrain_loss=0.1955  val_loss=0.2030  DWA=0.9406', '\\n', 'Epoch 12/50\ntrain_loss=0.1899  val_loss=0.1869  DWA=0.9455', '\\n', 'Epoch 13/50\ntrain_loss=0.1828  val_loss=0.1878  DWA=0.9474', '\\n', 'Epoch 14/50\ntrain_loss=0.1784  val_loss=0.2001  DWA=0.9385', '\\n', 'Epoch 15/50\ntrain_loss=0.1791  val_loss=0.1795  DWA=0.9460', '\\n', 'Epoch 16/50\ntrain_loss=0.1748  val_loss=0.1896  DWA=0.9421', '\\n', 'Early stopping\ntriggered.', '\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scient\nist-v2/experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-6/working/experiment_data.npy', '\\n', 'Execution time: 4\nminutes seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 620973.59\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 637335.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 824984.56\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '[lr=0.0003] Epoch 1: train_loss=0.5263,\nval_loss=0.4590, DWA=0.8027', '\\n', '[lr=0.0003] Epoch 2: train_loss=0.4241,\nval_loss=0.4070, DWA=0.8320', '\\n', '[lr=0.0003] Epoch 3: train_loss=0.3883,\nval_loss=0.3813, DWA=0.8449', '\\n', '[lr=0.0003] Epoch 4: train_loss=0.3686,\nval_loss=0.3720, DWA=0.8485', '\\n', '[lr=0.0003] Epoch 5: train_loss=0.3518,\nval_loss=0.3518, DWA=0.8649', '\\n', '[lr=0.0001] Epoch 1: train_loss=0.6056,\nval_loss=0.5337, DWA=0.7335', '\\n', '[lr=0.0001] Epoch 2: train_loss=0.5048,\nval_loss=0.4925, DWA=0.7748', '\\n', '[lr=0.0001] Epoch 3: train_loss=0.4717,\nval_loss=0.4619, DWA=0.7955', '\\n', '[lr=0.0001] Epoch 4: train_loss=0.4424,\nval_loss=0.4356, DWA=0.8113', '\\n', '[lr=0.0001] Epoch 5: train_loss=0.4200,\nval_loss=0.4181, DWA=0.8211', '\\n', '[lr=0.003] Epoch 1: train_loss=0.4328,\nval_loss=0.3910, DWA=0.8315', '\\n', '[lr=0.003] Epoch 2: train_loss=0.3305,\nval_loss=0.2967, DWA=0.9002', '\\n', '[lr=0.003] Epoch 3: train_loss=0.2773,\nval_loss=0.2519, DWA=0.9124', '\\n', '[lr=0.003] Epoch 4: train_loss=0.2422,\nval_loss=0.2276, DWA=0.9317', '\\n', '[lr=0.003] Epoch 5: train_loss=0.2193,\nval_loss=0.1997, DWA=0.9419', '\\n', 'Execution time: 23 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n537565.88 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 683356.25\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 800210.63\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH/', '\\n', '\\n=== Training with batch_size=16 ===', '\\n',\n'Epoch 1 | train_loss 0.4196 | val_loss 0.3468 | DWA 0.8693', '\\n', 'Epoch 2 |\ntrain_loss 0.3164 | val_loss 0.2883 | DWA 0.8959', '\\n', 'Epoch 3 | train_loss\n0.2663 | val_loss 0.2625 | DWA 0.9037', '\\n', 'Epoch 4 | train_loss 0.2344 |\nval_loss 0.2329 | DWA 0.9277', '\\n', 'Epoch 5 | train_loss 0.2199 | val_loss\n0.2184 | DWA 0.9279', '\\n', '\\n=== Training with batch_size=32 ===', '\\n',\n'Epoch 1 | train_loss 0.4411 | val_loss 0.3772 | DWA 0.8459', '\\n', 'Epoch 2 |\ntrain_loss 0.3469 | val_loss 0.3224 | DWA 0.8804', '\\n', 'Epoch 3 | train_loss\n0.3035 | val_loss 0.2763 | DWA 0.9082', '\\n', 'Epoch 4 | train_loss 0.2668 |\nval_loss 0.2559 | DWA 0.9152', '\\n', 'Epoch 5 | train_loss 0.2493 | val_loss\n0.2414 | DWA 0.9213', '\\n', '\\n=== Training with batch_size=64 ===', '\\n',\n'Epoch 1 | train_loss 0.4694 | val_loss 0.4047 | DWA 0.8233', '\\n', 'Epoch 2 |\ntrain_loss 0.3835 | val_loss 0.3700 | DWA 0.8605', '\\n', 'Epoch 3 | train_loss\n0.3437 | val_loss 0.3261 | DWA 0.8823', '\\n', 'Epoch 4 | train_loss 0.3065 |\nval_loss 0.3050 | DWA 0.9019', '\\n', 'Epoch 5 | train_loss 0.2773 | val_loss\n0.2726 | DWA 0.9148', '\\n', '\\n=== Training with batch_size=128 ===', '\\n',\n'Epoch 1 | train_loss 0.4941 | val_loss 0.4134 | DWA 0.8228', '\\n', 'Epoch 2 |\ntrain_loss 0.3878 | val_loss 0.3695 | DWA 0.8552', '\\n', 'Epoch 3 | train_loss\n0.3564 | val_loss 0.3475 | DWA 0.8738', '\\n', 'Epoch 4 | train_loss 0.3266 |\nval_loss 0.3118 | DWA 0.8862', '\\n', 'Epoch 5 | train_loss 0.3028 | val_loss\n0.3006 | DWA 0.8911', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 51 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n654899.52 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 771948.32\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 912837.12\nexamples/s]', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n=== Training with weight_decay=0.0 ===', '\\n',\n'Epoch 1: train_loss=0.4715, val_loss=0.3981, DWA=0.8234', '\\n', 'Epoch 2:\ntrain_loss=0.3772, val_loss=0.3566, DWA=0.8650', '\\n', 'Epoch 3:\ntrain_loss=0.3344, val_loss=0.3180, DWA=0.8787', '\\n', 'Epoch 4:\ntrain_loss=0.2958, val_loss=0.2859, DWA=0.8938', '\\n', 'Epoch 5:\ntrain_loss=0.2684, val_loss=0.2570, DWA=0.9179', '\\n', '\\n=== Training with\nweight_decay=1e-06 ===', '\\n', 'Epoch 1: train_loss=0.4649, val_loss=0.4059,\nDWA=0.8318', '\\n', 'Epoch 2: train_loss=0.3681, val_loss=0.3561, DWA=0.8527',\n'\\n', 'Epoch 3: train_loss=0.3217, val_loss=0.3171, DWA=0.8786', '\\n', 'Epoch 4:\ntrain_loss=0.2860, val_loss=0.2931, DWA=0.8913', '\\n', 'Epoch 5:\ntrain_loss=0.2602, val_loss=0.2559, DWA=0.9130', '\\n', '\\n=== Training with\nweight_decay=1e-05 ===', '\\n', 'Epoch 1: train_loss=0.4697, val_loss=0.3940,\nDWA=0.8390', '\\n', 'Epoch 2: train_loss=0.3735, val_loss=0.3498, DWA=0.8656',\n'\\n', 'Epoch 3: train_loss=0.3294, val_loss=0.3097, DWA=0.8873', '\\n', 'Epoch 4:\ntrain_loss=0.2930, val_loss=0.3012, DWA=0.8953', '\\n', 'Epoch 5:\ntrain_loss=0.2700, val_loss=0.2659, DWA=0.9050', '\\n', '\\n=== Training with\nweight_decay=0.0001 ===', '\\n', 'Epoch 1: train_loss=0.4731, val_loss=0.4077,\nDWA=0.8332', '\\n', 'Epoch 2: train_loss=0.3809, val_loss=0.3637, DWA=0.8624',\n'\\n', 'Epoch 3: train_loss=0.3443, val_loss=0.3427, DWA=0.8862', '\\n', 'Epoch 4:\ntrain_loss=0.3152, val_loss=0.3082, DWA=0.8941', '\\n', 'Epoch 5:\ntrain_loss=0.2905, val_loss=0.2805, DWA=0.9062', '\\n', '\\n=== Training with\nweight_decay=0.001 ===', '\\n', 'Epoch 1: train_loss=0.4951, val_loss=0.4348,\nDWA=0.7989', '\\n', 'Epoch 2: train_loss=0.4015, val_loss=0.3777, DWA=0.8511',\n'\\n', 'Epoch 3: train_loss=0.3707, val_loss=0.3532, DWA=0.8652', '\\n', 'Epoch 4:\ntrain_loss=0.3431, val_loss=0.3469, DWA=0.8733', '\\n', 'Epoch 5:\ntrain_loss=0.3262, val_loss=0.3340, DWA=0.8848', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n======== Training with embedding_dim = 32\n========', '\\n', 'Epoch 01 | train_loss 0.4790 | val_loss 0.4066 | DWA 0.8222',\n'\\n', 'Epoch 02 | train_loss 0.3841 | val_loss 0.3715 | DWA 0.8500', '\\n',\n'Epoch 03 | train_loss 0.3509 | val_loss 0.3443 | DWA 0.8639', '\\n', 'Epoch 04 |\ntrain_loss 0.3238 | val_loss 0.3117 | DWA 0.8902', '\\n', 'Epoch 05 | train_loss\n0.3001 | val_loss 0.2922 | DWA 0.8971', '\\n', '\\n======== Training with\nembedding_dim = 64 ========', '\\n', 'Epoch 01 | train_loss 0.4631 | val_loss\n0.3924 | DWA 0.8462', '\\n', 'Epoch 02 | train_loss 0.3587 | val_loss 0.3369 |\nDWA 0.8768', '\\n', 'Epoch 03 | train_loss 0.3170 | val_loss 0.3021 | DWA\n0.8975', '\\n', 'Epoch 04 | train_loss 0.2883 | val_loss 0.2706 | DWA 0.9067',\n'\\n', 'Epoch 05 | train_loss 0.2666 | val_loss 0.2693 | DWA 0.8959', '\\n',\n'\\n======== Training with embedding_dim = 128 ========', '\\n', 'Epoch 01 |\ntrain_loss 0.4519 | val_loss 0.3938 | DWA 0.8376', '\\n', 'Epoch 02 | train_loss\n0.3479 | val_loss 0.3171 | DWA 0.8924', '\\n', 'Epoch 03 | train_loss 0.3009 |\nval_loss 0.2920 | DWA 0.8961', '\\n', 'Epoch 04 | train_loss 0.2676 | val_loss\n0.2482 | DWA 0.9178', '\\n', 'Epoch 05 | train_loss 0.2420 | val_loss 0.2456 |\nDWA 0.9178', '\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scient\nist-v2/experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time:\n41 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH/', '\\n', '\\n=== Training with dropout=0.0 ===', '\\n', 'Ep\n1 | tr_loss 0.4599 | val_loss 0.4034 | DWA 0.8366', '\\n', 'Ep 2 | tr_loss 0.3644\n| val_loss 0.3433 | DWA 0.8737', '\\n', 'Ep 3 | tr_loss 0.3205 | val_loss 0.2975\n| DWA 0.8962', '\\n', 'Ep 4 | tr_loss 0.2832 | val_loss 0.2826 | DWA 0.9085',\n'\\n', 'Ep 5 | tr_loss 0.2560 | val_loss 0.2470 | DWA 0.9153', '\\n', '\\n===\nTraining with dropout=0.1 ===', '\\n', 'Ep 1 | tr_loss 0.4696 | val_loss 0.4064 |\nDWA 0.8187', '\\n', 'Ep 2 | tr_loss 0.3786 | val_loss 0.3575 | DWA 0.8682', '\\n',\n'Ep 3 | tr_loss 0.3418 | val_loss 0.3237 | DWA 0.8801', '\\n', 'Ep 4 | tr_loss\n0.3152 | val_loss 0.3036 | DWA 0.8833', '\\n', 'Ep 5 | tr_loss 0.2870 | val_loss\n0.2719 | DWA 0.9077', '\\n', '\\n=== Training with dropout=0.2 ===', '\\n', 'Ep 1 |\ntr_loss 0.4812 | val_loss 0.4047 | DWA 0.8297', '\\n', 'Ep 2 | tr_loss 0.3978 |\nval_loss 0.3738 | DWA 0.8619', '\\n', 'Ep 3 | tr_loss 0.3647 | val_loss 0.3357 |\nDWA 0.8813', '\\n', 'Ep 4 | tr_loss 0.3323 | val_loss 0.3139 | DWA 0.8987', '\\n',\n'Ep 5 | tr_loss 0.3112 | val_loss 0.3119 | DWA 0.8970', '\\n', '\\n=== Training\nwith dropout=0.3 ===', '\\n', 'Ep 1 | tr_loss 0.4867 | val_loss 0.4056 | DWA\n0.8298', '\\n', 'Ep 2 | tr_loss 0.4004 | val_loss 0.3682 | DWA 0.8547', '\\n', 'Ep\n3 | tr_loss 0.3714 | val_loss 0.3609 | DWA 0.8679', '\\n', 'Ep 4 | tr_loss 0.3454\n| val_loss 0.3133 | DWA 0.8934', '\\n', 'Ep 5 | tr_loss 0.3230 | val_loss 0.3002\n| DWA 0.8908', '\\n', '\\n=== Training with dropout=0.4 ===', '\\n', 'Ep 1 |\ntr_loss 0.4944 | val_loss 0.4078 | DWA 0.8323', '\\n', 'Ep 2 | tr_loss 0.4043 |\nval_loss 0.3712 | DWA 0.8593', '\\n', 'Ep 3 | tr_loss 0.3799 | val_loss 0.3557 |\nDWA 0.8686', '\\n', 'Ep 4 | tr_loss 0.3629 | val_loss 0.3357 | DWA 0.8826', '\\n',\n'Ep 5 | tr_loss 0.3430 | val_loss 0.3089 | DWA 0.8926', '\\n', '\\n=== Training\nwith dropout=0.5 ===', '\\n', 'Ep 1 | tr_loss 0.5132 | val_loss 0.4319 | DWA\n0.8070', '\\n', 'Ep 2 | tr_loss 0.4258 | val_loss 0.3837 | DWA 0.8478', '\\n', 'Ep\n3 | tr_loss 0.3935 | val_loss 0.3660 | DWA 0.8639', '\\n', 'Ep 4 | tr_loss 0.3776\n| val_loss 0.3586 | DWA 0.8713', '\\n', 'Ep 5 | tr_loss 0.3628 | val_loss 0.3336\n| DWA 0.8855', '\\n', '\\nSaved experiment_data.npy', '\\n', 'Execution time: 43\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH/', '\\n', '\\n=== Training with mean\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4599 | ValLoss 0.4033 | DWA\n0.8369', '\\n', 'Epoch 2/5 | TrainLoss 0.3642 | ValLoss 0.3427 | DWA 0.8746',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3201 | ValLoss 0.2965 | DWA 0.8968', '\\n', 'Epoch\n4/5 | TrainLoss 0.2818 | ValLoss 0.2808 | DWA 0.9092', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2539 | ValLoss 0.2450 | DWA 0.9180', '\\n', '\\n=== Training with max\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4073 | ValLoss 0.3115 | DWA\n0.8816', '\\n', 'Epoch 2/5 | TrainLoss 0.2615 | ValLoss 0.2143 | DWA 0.9270',\n'\\n', 'Epoch 3/5 | TrainLoss 0.1922 | ValLoss 0.1638 | DWA 0.9494', '\\n', 'Epoch\n4/5 | TrainLoss 0.1521 | ValLoss 0.1551 | DWA 0.9488', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1267 | ValLoss 0.1176 | DWA 0.9682', '\\n', '\\n=== Training with add\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4422 | ValLoss 0.3896 | DWA\n0.8228', '\\n', 'Epoch 2/5 | TrainLoss 0.3557 | ValLoss 0.3312 | DWA 0.8848',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3129 | ValLoss 0.2943 | DWA 0.8941', '\\n', 'Epoch\n4/5 | TrainLoss 0.2761 | ValLoss 0.2734 | DWA 0.9163', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2489 | ValLoss 0.2376 | DWA 0.9234', '\\n', '\\n=== Training with attn\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.3798 | ValLoss 0.2744 | DWA\n0.8990', '\\n', 'Epoch 2/5 | TrainLoss 0.2588 | ValLoss 0.2293 | DWA 0.9219',\n'\\n', 'Epoch 3/5 | TrainLoss 0.2284 | ValLoss 0.2205 | DWA 0.9259', '\\n', 'Epoch\n4/5 | TrainLoss 0.2078 | ValLoss 0.1897 | DWA 0.9361', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1883 | ValLoss 0.1745 | DWA 0.9445', '\\n', '\\nSaved results to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 32 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH from', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH/', '\\n', '\\n--- Training with hidden_dim = 64 ---', '\\n',\n'Epoch 1: train_loss=0.4876, val_loss=0.4118, DWA=0.8221', '\\n', 'Epoch 2:\ntrain_loss=0.3918, val_loss=0.4004, DWA=0.8447', '\\n', 'Epoch 3:\ntrain_loss=0.3606, val_loss=0.3527, DWA=0.8666', '\\n', 'Epoch 4:\ntrain_loss=0.3329, val_loss=0.3248, DWA=0.8800', '\\n', 'Epoch 5:\ntrain_loss=0.3068, val_loss=0.3003, DWA=0.8904', '\\n', '\\n--- Training with\nhidden_dim = 128 ---', '\\n', 'Epoch 1: train_loss=0.4770, val_loss=0.4033,\nDWA=0.8282', '\\n', 'Epoch 2: train_loss=0.3792, val_loss=0.3591, DWA=0.8573',\n'\\n', 'Epoch 3: train_loss=0.3355, val_loss=0.3218, DWA=0.8840', '\\n', 'Epoch 4:\ntrain_loss=0.3011, val_loss=0.3009, DWA=0.8939', '\\n', 'Epoch 5:\ntrain_loss=0.2720, val_loss=0.2750, DWA=0.9098', '\\n', '\\n--- Training with\nhidden_dim = 256 ---', '\\n', 'Epoch 1: train_loss=0.4545, val_loss=0.3840,\nDWA=0.8484', '\\n', 'Epoch 2: train_loss=0.3523, val_loss=0.3183, DWA=0.8799',\n'\\n', 'Epoch 3: train_loss=0.3006, val_loss=0.2760, DWA=0.9006', '\\n', 'Epoch 4:\ntrain_loss=0.2642, val_loss=0.2556, DWA=0.9198', '\\n', 'Epoch 5:\ntrain_loss=0.2358, val_loss=0.2235, DWA=0.9224', '\\n', 'Saved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH/', '\\n', '\\n=== Training with mean\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4621 | ValLoss 0.3922 | DWA\n0.8410', '\\n', 'Epoch 2/5 | TrainLoss 0.3728 | ValLoss 0.3604 | DWA 0.8623',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3310 | ValLoss 0.3141 | DWA 0.8876', '\\n', 'Epoch\n4/5 | TrainLoss 0.2974 | ValLoss 0.2871 | DWA 0.9047', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2719 | ValLoss 0.2567 | DWA 0.9136', '\\n', '\\n=== Training with max\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4203 | ValLoss 0.3203 | DWA\n0.8754', '\\n', 'Epoch 2/5 | TrainLoss 0.2780 | ValLoss 0.2281 | DWA 0.9273',\n'\\n', 'Epoch 3/5 | TrainLoss 0.2069 | ValLoss 0.1843 | DWA 0.9375', '\\n', 'Epoch\n4/5 | TrainLoss 0.1527 | ValLoss 0.1329 | DWA 0.9617', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1237 | ValLoss 0.1105 | DWA 0.9683', '\\n', '\\n=== Training with add\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4413 | ValLoss 0.3751 | DWA\n0.8557', '\\n', 'Epoch 2/5 | TrainLoss 0.3520 | ValLoss 0.3257 | DWA 0.8815',\n'\\n', 'Epoch 3/5 | TrainLoss 0.2999 | ValLoss 0.2918 | DWA 0.8989', '\\n', 'Epoch\n4/5 | TrainLoss 0.2732 | ValLoss 0.2638 | DWA 0.9129', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2480 | ValLoss 0.2397 | DWA 0.9204', '\\n', '\\n=== Training with attn\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.3888 | ValLoss 0.2655 | DWA\n0.9009', '\\n', 'Epoch 2/5 | TrainLoss 0.2564 | ValLoss 0.2311 | DWA 0.9186',\n'\\n', 'Epoch 3/5 | TrainLoss 0.2191 | ValLoss 0.1920 | DWA 0.9367', '\\n', 'Epoch\n4/5 | TrainLoss 0.1961 | ValLoss 0.1942 | DWA 0.9374', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1827 | ValLoss 0.1636 | DWA 0.9448', '\\n', '\\nSaved results to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH/', '\\n', '\\n=== Training with mean\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4629 | ValLoss 0.3917 | DWA\n0.8397', '\\n', 'Epoch 2/5 | TrainLoss 0.3710 | ValLoss 0.3467 | DWA 0.8711',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3265 | ValLoss 0.3076 | DWA 0.8894', '\\n', 'Epoch\n4/5 | TrainLoss 0.2928 | ValLoss 0.2954 | DWA 0.8975', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2628 | ValLoss 0.2659 | DWA 0.9148', '\\n', '\\n=== Training with max\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4080 | ValLoss 0.3097 | DWA\n0.8770', '\\n', 'Epoch 2/5 | TrainLoss 0.2583 | ValLoss 0.2205 | DWA 0.9273',\n'\\n', 'Epoch 3/5 | TrainLoss 0.1852 | ValLoss 0.1636 | DWA 0.9521', '\\n', 'Epoch\n4/5 | TrainLoss 0.1407 | ValLoss 0.1371 | DWA 0.9589', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1174 | ValLoss 0.1178 | DWA 0.9650', '\\n', '\\n=== Training with add\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4350 | ValLoss 0.3807 | DWA\n0.8540', '\\n', 'Epoch 2/5 | TrainLoss 0.3485 | ValLoss 0.3330 | DWA 0.8576',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3023 | ValLoss 0.2932 | DWA 0.9032', '\\n', 'Epoch\n4/5 | TrainLoss 0.2698 | ValLoss 0.2638 | DWA 0.9102', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2470 | ValLoss 0.3044 | DWA 0.9045', '\\n', '\\n=== Training with attn\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.3730 | ValLoss 0.2681 | DWA\n0.9007', '\\n', 'Epoch 2/5 | TrainLoss 0.2532 | ValLoss 0.2169 | DWA 0.9216',\n'\\n', 'Epoch 3/5 | TrainLoss 0.2215 | ValLoss 0.1993 | DWA 0.9301', '\\n', 'Epoch\n4/5 | TrainLoss 0.1928 | ValLoss 0.1922 | DWA 0.9326', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1919 | ValLoss 0.1954 | DWA 0.9347', '\\n', '\\nSaved results to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 38 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Loaded SPR_BENCH from', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH/', '\\n', '\\n=== Training with mean\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4685 | ValLoss 0.4040 | DWA\n0.8292', '\\n', 'Epoch 2/5 | TrainLoss 0.3794 | ValLoss 0.3604 | DWA 0.8558',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3382 | ValLoss 0.3234 | DWA 0.8754', '\\n', 'Epoch\n4/5 | TrainLoss 0.2995 | ValLoss 0.2894 | DWA 0.8930', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2748 | ValLoss 0.2695 | DWA 0.9125', '\\n', '\\n=== Training with max\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4092 | ValLoss 0.3030 | DWA\n0.8881', '\\n', 'Epoch 2/5 | TrainLoss 0.2615 | ValLoss 0.2087 | DWA 0.9295',\n'\\n', 'Epoch 3/5 | TrainLoss 0.1872 | ValLoss 0.1612 | DWA 0.9422', '\\n', 'Epoch\n4/5 | TrainLoss 0.1358 | ValLoss 0.1349 | DWA 0.9522', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1124 | ValLoss 0.1043 | DWA 0.9713', '\\n', '\\n=== Training with add\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.4469 | ValLoss 0.3824 | DWA\n0.8437', '\\n', 'Epoch 2/5 | TrainLoss 0.3525 | ValLoss 0.3382 | DWA 0.8757',\n'\\n', 'Epoch 3/5 | TrainLoss 0.3066 | ValLoss 0.2995 | DWA 0.8833', '\\n', 'Epoch\n4/5 | TrainLoss 0.2714 | ValLoss 0.2543 | DWA 0.9129', '\\n', 'Epoch 5/5 |\nTrainLoss 0.2422 | ValLoss 0.2358 | DWA 0.9251', '\\n', '\\n=== Training with attn\npooling ===', '\\n', 'Epoch 1/5 | TrainLoss 0.3682 | ValLoss 0.2789 | DWA\n0.8990', '\\n', 'Epoch 2/5 | TrainLoss 0.2416 | ValLoss 0.2165 | DWA 0.9221',\n'\\n', 'Epoch 3/5 | TrainLoss 0.2103 | ValLoss 0.2213 | DWA 0.9216', '\\n', 'Epoch\n4/5 | TrainLoss 0.1874 | ValLoss 0.1962 | DWA 0.9328', '\\n', 'Epoch 5/5 |\nTrainLoss 0.1847 | ValLoss 0.1774 | DWA 0.9438', '\\n', '\\nSaved results to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n38_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 39 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["", "", "", "", "", "The training script executed successfully without any errors or bugs. The model\ntrained on the SPR_BENCH dataset with three different embedding dimensions (32,\n64, and 128), and the Dual Weighted Accuracy (DWA) metric improved consistently\nacross epochs for each configuration. The results were saved to the specified\ndirectory. The execution time was well within the limit, and the code appears to\nfunction as intended.", "", "The execution of the training script was successful. The training and validation\nlosses decreased over epochs for all pooling types, and the Dual Weighted\nAccuracy (DWA) consistently improved. The results were saved correctly to the\nspecified path. The script effectively explored different pooling strategies and\nlogged results without any apparent issues.", "The execution of the training script was successful. The training process was\nconducted for three different hidden dimensions (64, 128, 256), and the model\nshowed consistent improvement in both training and validation losses, as well as\nDual-Weighted Accuracy (DWA) over epochs. The results indicate that the model is\nlearning effectively, and the code is functioning as intended. Experiment data\nwas successfully saved for further analysis.", "The execution of the training script was successful, and there were no apparent\nbugs. The script tested four different pooling methods (mean, max, add, and\nattention) for the GCN model and evaluated their performance using the dual-\nweighted accuracy (DWA) metric. Results indicate that all pooling methods\nimproved over epochs, with max pooling achieving the highest final DWA score at\n0.9683. The results were saved successfully to the specified path. The script\nperformed as expected and adhered to the goals of the baseline tuning stage.", "The execution output shows that the training script ran successfully without any\nerrors or bugs. The model was trained with four different pooling strategies\n(mean, max, add, and attention), and the results for each were logged. The Dual\nWeighted Accuracy (DWA) improved consistently across epochs for all pooling\ntypes, indicating that the model was learning effectively. The results were\nsaved to a file for further analysis. Overall, the experiment appears to have\nbeen executed as intended.", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Represents the final loss value during the training process, where lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2564, "best_value": 0.2564}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Represents the final loss value during the validation process, where lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2475, "best_value": 0.2475}]}, {"metric_name": "dual weighted accuracy (validation)", "lower_is_better": false, "description": "Represents the accuracy metric during validation, where higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.916, "best_value": 0.916}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (configuration: 10 epochs)", "final_value": 0.195044, "best_value": 0.195044}, {"dataset_name": "SPR_BENCH (configuration: 20 epochs)", "final_value": 0.220119, "best_value": 0.220119}, {"dataset_name": "SPR_BENCH (configuration: 30 epochs)", "final_value": 0.176026, "best_value": 0.176026}, {"dataset_name": "SPR_BENCH (configuration: 50 epochs)", "final_value": 0.174817, "best_value": 0.174817}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (configuration: 10 epochs)", "final_value": 0.194936, "best_value": 0.194936}, {"dataset_name": "SPR_BENCH (configuration: 20 epochs)", "final_value": 0.261776, "best_value": 0.261776}, {"dataset_name": "SPR_BENCH (configuration: 30 epochs)", "final_value": 0.191965, "best_value": 0.191965}, {"dataset_name": "SPR_BENCH (configuration: 50 epochs)", "final_value": 0.189629, "best_value": 0.189629}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The dual weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (configuration: 10 epochs)", "final_value": 0.93949, "best_value": 0.93949}, {"dataset_name": "SPR_BENCH (configuration: 20 epochs)", "final_value": 0.92044, "best_value": 0.92044}, {"dataset_name": "SPR_BENCH (configuration: 30 epochs)", "final_value": 0.945478, "best_value": 0.945478}, {"dataset_name": "SPR_BENCH (configuration: 50 epochs)", "final_value": 0.947407, "best_value": 0.947407}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures how well the model is fitting the training data. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2193, "best_value": 0.2193}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model is performing on validation data. Lower values indicate better generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1997, "best_value": 0.1997}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "Represents the accuracy of the model on validation data, weighted by dual factors. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9419, "best_value": 0.9419}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (batch size 16)", "final_value": 0.2199, "best_value": 0.2199}, {"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.2493, "best_value": 0.2493}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.2773, "best_value": 0.2773}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.3028, "best_value": 0.3028}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (batch size 16)", "final_value": 0.2184, "best_value": 0.2184}, {"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.2414, "best_value": 0.2414}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.2726, "best_value": 0.2726}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.3006, "best_value": 0.3006}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The dual weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (batch size 16)", "final_value": 0.9279, "best_value": 0.9279}, {"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.9213, "best_value": 0.9213}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.9148, "best_value": 0.9148}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.8911, "best_value": 0.8911}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3262, "best_value": 0.2602}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.334, "best_value": 0.2559}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The dual weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8848, "best_value": 0.9179}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.242, "best_value": 0.242}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating how well the model is generalizing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2456, "best_value": 0.2456}]}, {"metric_name": "dual weighted accuracy", "lower_is_better": false, "description": "The accuracy metric weighted by dual criteria, representing the correctness of predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9178, "best_value": 0.9178}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value on the training dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.256032, "best_value": 0.256032}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.246976, "best_value": 0.246976}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The dual weighted accuracy on the validation dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.915333, "best_value": 0.915333}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2539, "best_value": 0.2539}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1267, "best_value": 0.1267}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.2489, "best_value": 0.2489}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1883, "best_value": 0.1883}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.245, "best_value": 0.245}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1176, "best_value": 0.1176}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.2376, "best_value": 0.2376}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1745, "best_value": 0.1745}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy during validation using dual weighted metrics. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.918, "best_value": 0.918}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.9682, "best_value": 0.9682}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.9234, "best_value": 0.9234}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.9445, "best_value": 0.9445}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (hidden_dim=64)", "final_value": 0.306768, "best_value": 0.306768}, {"dataset_name": "SPR_BENCH (hidden_dim=128)", "final_value": 0.271951, "best_value": 0.271951}, {"dataset_name": "SPR_BENCH (hidden_dim=256)", "final_value": 0.23576, "best_value": 0.23576}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (hidden_dim=64)", "final_value": 0.300321, "best_value": 0.300321}, {"dataset_name": "SPR_BENCH (hidden_dim=128)", "final_value": 0.274997, "best_value": 0.274997}, {"dataset_name": "SPR_BENCH (hidden_dim=256)", "final_value": 0.223457, "best_value": 0.223457}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "Measures the weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (hidden_dim=64)", "final_value": 0.890408, "best_value": 0.890408}, {"dataset_name": "SPR_BENCH (hidden_dim=128)", "final_value": 0.909787, "best_value": 0.909787}, {"dataset_name": "SPR_BENCH (hidden_dim=256)", "final_value": 0.922419, "best_value": 0.922419}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset during training.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2719, "best_value": 0.2719}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1237, "best_value": 0.1237}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.248, "best_value": 0.248}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1827, "best_value": 0.1827}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2567, "best_value": 0.2567}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1105, "best_value": 0.1105}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.2397, "best_value": 0.2397}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1636, "best_value": 0.1636}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The accuracy metric computed on the validation dataset, weighted by dual criteria.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.9136, "best_value": 0.9136}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.9683, "best_value": 0.9683}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.9204, "best_value": 0.9204}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.9448, "best_value": 0.9448}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2628, "best_value": 0.2628}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1174, "best_value": 0.1174}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.247, "best_value": 0.247}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1919, "best_value": 0.1919}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2659, "best_value": 0.2659}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1178, "best_value": 0.1178}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.3044, "best_value": 0.2638}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1954, "best_value": 0.1922}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The dual weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.9148, "best_value": 0.9148}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.965, "best_value": 0.965}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.9045, "best_value": 0.9102}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.9347, "best_value": 0.9347}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training data.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2748, "best_value": 0.2748}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1124, "best_value": 0.1124}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.2422, "best_value": 0.2422}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1847, "best_value": 0.1847}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation data.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.2695, "best_value": 0.2695}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.1043, "best_value": 0.1043}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.2358, "best_value": 0.2358}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.1774, "best_value": 0.1774}]}, {"metric_name": "validation dual weighted accuracy", "lower_is_better": false, "description": "The dual weighted accuracy calculated on the validation data.", "data": [{"dataset_name": "SPR_BENCH (pooling_type = mean)", "final_value": 0.9125, "best_value": 0.9125}, {"dataset_name": "SPR_BENCH (pooling_type = max)", "final_value": 0.9713, "best_value": 0.9713}, {"dataset_name": "SPR_BENCH (pooling_type = add)", "final_value": 0.9251, "best_value": 0.9251}, {"dataset_name": "SPR_BENCH (pooling_type = attn)", "final_value": 0.9438, "best_value": 0.9438}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_DWA_curve.png", "../../logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_10_epochs.png", "../../logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_20_epochs.png", "../../logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_30_epochs.png", "../../logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_50_epochs.png", "../../logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_DWA_curves.png"], ["../../logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_validation_DWA.png", "../../logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_final_DWA_bar.png"], ["../../logs/0-run/experiment_results/experiment_d64d8068a8804dc5b32120e66983e974_proc_1441386/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d64d8068a8804dc5b32120e66983e974_proc_1441386/spr_bench_validation_dwa.png"], ["../../logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_DWA_curves.png", "../../logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_final_DWA_bar.png", "../../logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_scatter_best_wd_0.0.png"], ["../../logs/0-run/experiment_results/experiment_3682d5bade2643a484012e40f32dd022_proc_1441385/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3682d5bade2643a484012e40f32dd022_proc_1441385/SPR_BENCH_DWA_curves.png"], ["../../logs/0-run/experiment_results/experiment_dcf65c15ac2147c2bdb22159f9b8a0cd_proc_1441386/SPR_BENCH_val_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dcf65c15ac2147c2bdb22159f9b8a0cd_proc_1441386/SPR_BENCH_final_DWA_vs_dropout.png"], ["../../logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_dwa_curves.png", "../../logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_final_dwa_bar.png"], ["../../logs/0-run/experiment_results/experiment_fec6a1c56ab34522947c097266dbcc1b_proc_1441384/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fec6a1c56ab34522947c097266dbcc1b_proc_1441384/SPR_BENCH_DWA_curves.png"], ["../../logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_dwa_curves.png", "../../logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_final_dwa_bar.png"], ["../../logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_dwa_curves.png", "../../logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_final_dwa_bar.png"], ["../../logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_dwa_curves.png", "../../logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_final_dwa_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_3898931d2a374b73af23ed1c2df593b7/SPR_BENCH_loss_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_3898931d2a374b73af23ed1c2df593b7/SPR_BENCH_dwa_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_3898931d2a374b73af23ed1c2df593b7/SPR_BENCH_final_dwa_bar_mean_se.png"]], "plot_paths": [["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_DWA_curve.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_10_epochs.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_20_epochs.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_30_epochs.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_50_epochs.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_DWA_curves.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_train_val_loss.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_validation_DWA.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_final_DWA_bar.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d64d8068a8804dc5b32120e66983e974_proc_1441386/spr_bench_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d64d8068a8804dc5b32120e66983e974_proc_1441386/spr_bench_validation_dwa.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_DWA_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_final_DWA_bar.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_scatter_best_wd_0.0.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3682d5bade2643a484012e40f32dd022_proc_1441385/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3682d5bade2643a484012e40f32dd022_proc_1441385/SPR_BENCH_DWA_curves.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_dcf65c15ac2147c2bdb22159f9b8a0cd_proc_1441386/SPR_BENCH_val_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_dcf65c15ac2147c2bdb22159f9b8a0cd_proc_1441386/SPR_BENCH_final_DWA_vs_dropout.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_dwa_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_final_dwa_bar.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fec6a1c56ab34522947c097266dbcc1b_proc_1441384/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fec6a1c56ab34522947c097266dbcc1b_proc_1441384/SPR_BENCH_DWA_curves.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_dwa_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_final_dwa_bar.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_dwa_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_final_dwa_bar.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_dwa_curves.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_final_dwa_bar.png"], ["experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_3898931d2a374b73af23ed1c2df593b7/SPR_BENCH_loss_curves_mean_se.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_3898931d2a374b73af23ed1c2df593b7/SPR_BENCH_dwa_curves_mean_se.png", "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_3898931d2a374b73af23ed1c2df593b7/SPR_BENCH_final_dwa_bar_mean_se.png"]], "plot_analyses": [[{"analysis": "The plot shows the training and validation loss trends over five epochs. Both losses decrease steadily, indicating that the model is learning effectively. The validation loss closely follows the training loss, suggesting that the model generalizes well without significant overfitting. By the fifth epoch, the losses converge, which is a positive sign of stable training.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot demonstrates the improvement of the model\u2019s validation dual-weighted accuracy over five epochs. The accuracy increases consistently, starting from approximately 84% and reaching over 91%. This steady improvement indicates that the model is effectively learning and improving its performance on the validation set.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_DWA_curve.png"}, {"analysis": "The confusion matrix shows the model's performance in terms of true positives, true negatives, false positives, and false negatives. The model correctly predicted 2196 true negatives and 2387 true positives while misclassifying 304 false positives and 113 false negatives. This indicates that the model performs well, with a relatively low number of misclassifications, particularly in terms of false negatives.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fd5d2b64063e48e8bb5a73aebc6b7a8e_proc_1437147/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves for 10 epochs show a steady decrease in both training and validation loss, indicating that the model is learning effectively. The validation loss closely follows the training loss, suggesting minimal overfitting at this stage. However, more epochs may be needed to fully evaluate the model's performance.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_10_epochs.png"}, {"analysis": "The loss curves for 20 epochs demonstrate continued improvement in training loss, but the validation loss begins to plateau and even shows a slight increase after epoch 7. This suggests potential overfitting as the model trains longer. Early stopping might be beneficial to avoid overfitting.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_20_epochs.png"}, {"analysis": "The loss curves for 30 epochs reveal a similar trend to the 20-epoch plot, with a steady decrease in training loss but some fluctuations in validation loss after epoch 10. These fluctuations might indicate overfitting or instability in the model's performance on the validation set.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_30_epochs.png"}, {"analysis": "The loss curves for 50 epochs show a consistent decrease in training loss, but the validation loss exhibits more pronounced fluctuations after epoch 10. This suggests that overfitting becomes more significant as the model trains for extended periods. Regularization techniques or early stopping should be considered to mitigate this issue.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_loss_curves_50_epochs.png"}, {"analysis": "The validation dual weighted accuracy (DWA) plot shows that all configurations achieve significant performance improvement within the first few epochs, with diminishing returns after approximately 8 epochs. The 50-epoch configuration achieves slightly higher peak accuracy compared to others, but the differences are marginal. This indicates that training beyond 10-20 epochs may not yield substantial performance gains and could lead to overfitting.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c71425718275418eb2b1a2cd58938161_proc_1441384/SPR_BENCH_DWA_curves.png"}], [{"analysis": "This plot shows the training and validation loss for different learning rates (0.0001, 0.0003, 0.003) over 5 epochs. A higher learning rate (0.003) leads to the fastest convergence in terms of loss reduction, both for training and validation. However, the validation loss for lr=0.003 appears slightly higher than the training loss, indicating potential overfitting. The learning rates of 0.0003 and 0.0001 show slower convergence but better alignment between training and validation loss, suggesting more stable training. Overall, lr=0.003 seems the most effective for rapid optimization, but its generalization ability should be monitored closely.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_train_val_loss.png"}, {"analysis": "This plot compares the Dual-Weighted Accuracy (DWA) on the validation set for different learning rates over 5 epochs. The learning rate of 0.003 achieves the highest DWA, reaching approximately 0.95 by the fifth epoch. The learning rate of 0.0003 also shows consistent improvement, reaching around 0.88, while 0.0001 lags behind at approximately 0.83. The results suggest that a higher learning rate (0.003) enables faster and better optimization for this metric, though stability and overfitting risks should be considered.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_validation_DWA.png"}, {"analysis": "This bar chart presents the final Dual-Weighted Accuracy (DWA) achieved by models trained with different learning rates. The learning rate of 0.003 achieves the highest final DWA, followed by 0.0003 and 0.0001. This confirms the trend observed in the previous plots, where a higher learning rate leads to better performance on this metric. The results suggest that 0.003 is the most effective learning rate among the tested values for achieving high DWA.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_4f10ef4efbb84584b08ab334f93e4934_proc_1441385/SPR_BENCH_final_DWA_bar.png"}], [{"analysis": "This plot illustrates the training and validation loss across different batch sizes (16, 32, 64, and 128) over five epochs. Smaller batch sizes (16 and 32) show consistently lower training and validation losses, indicating better convergence and generalization. The validation loss for batch size 16 is the lowest by the end of training, suggesting that smaller batch sizes may help the model generalize better. Larger batch sizes (64 and 128) exhibit slower convergence and higher final losses, potentially due to reduced gradient noise, which may hinder exploration of the loss landscape. The gap between training and validation loss for all batch sizes is relatively small, indicating that overfitting is not a significant concern in this scenario.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d64d8068a8804dc5b32120e66983e974_proc_1441386/spr_bench_loss_curves.png"}, {"analysis": "This plot shows the Dual-Weighted Accuracy (DWA) on the validation set across different batch sizes (16, 32, 64, and 128) over five epochs. Smaller batch sizes (16 and 32) achieve higher DWA earlier in training and maintain a higher final accuracy compared to larger batch sizes. Batch size 16 reaches the highest DWA, followed by 32, 64, and then 128. This suggests that smaller batch sizes are more effective at optimizing the model for the SPR task, likely due to better gradient updates and exploration of the parameter space. The trend indicates that larger batch sizes may require more epochs to achieve comparable performance, but they still lag behind smaller batch sizes in this training setup.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d64d8068a8804dc5b32120e66983e974_proc_1441386/spr_bench_validation_dwa.png"}], [{"analysis": "The training and validation loss curves across different weight decay values suggest that lower weight decay values (e.g., wd_0.0 and wd_1e-06) result in better convergence rates. These configurations lead to lower final loss values, indicating better model generalization. In contrast, higher weight decay values (e.g., wd_0.001) show slower convergence and higher final loss values, which may indicate underfitting or insufficient learning capacity.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_loss_curves.png"}, {"analysis": "The Dual-Weighted Accuracy (DWA) over epochs indicates that lower weight decay values (e.g., wd_0.0 and wd_1e-06) consistently outperform higher weight decay values (e.g., wd_0.001). The curves for wd_0.0 and wd_1e-06 show steady improvement and reach the highest DWA scores, suggesting that these configurations are better suited for the task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_DWA_curves.png"}, {"analysis": "The final DWA bar plot shows that the configurations with lower weight decay values (e.g., wd_0.0 and wd_1e-06) achieve the highest accuracy, while higher weight decay values (e.g., wd_0.001) result in relatively lower performance. This reinforces the observation that lower weight decay values are more effective for this task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_final_DWA_bar.png"}, {"analysis": "The scatter plot comparing predictions with ground truth shows that the model with wd_0.0 produces predictions that align closely with the true labels. The clusters are well-separated and aligned along the diagonal, indicating that the model effectively captures the underlying relationships in the data for this configuration.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_52ca552a327d40b3b00be6e9ac68b023_proc_1441387/SPR_BENCH_scatter_best_wd_0.0.png"}], [{"analysis": "This plot shows the training and validation loss curves for different embedding sizes (32, 64, and 128 dimensions). The loss decreases consistently across all configurations, indicating effective training. However, the gap between training and validation loss for smaller embedding sizes (e.g., dim_32) suggests potential overfitting. The larger embedding size (dim_128) achieves the lowest validation loss, suggesting better generalization. This indicates that increasing the embedding size improves the model's ability to capture the structural relationships in the data. Further hyperparameter tuning may be needed to reduce overfitting for smaller embedding sizes.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3682d5bade2643a484012e40f32dd022_proc_1441385/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the Dual Weighted Accuracy (DWA) on the validation set across different embedding sizes over epochs. The larger embedding size (dim_128) consistently outperforms the smaller sizes (dim_64 and dim_32), achieving the highest DWA by epoch 5. The smaller embedding sizes show slower improvement and plateau at lower accuracy levels. This suggests that larger embeddings enable the model to better capture the complex relationships in the SPR task, leading to superior performance. However, the diminishing returns in DWA improvement for dim_128 after epoch 4 suggest that further increases in training duration may not yield significant gains.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3682d5bade2643a484012e40f32dd022_proc_1441385/SPR_BENCH_DWA_curves.png"}], [{"analysis": "This plot shows the validation loss over epochs for different dropout rates. Lower dropout rates (e.g., 0.0 and 0.1) result in faster convergence and lower final validation loss, indicating that the model performs better with minimal regularization in this scenario. Higher dropout rates (e.g., 0.4 and 0.5) lead to slower convergence and higher final validation loss, suggesting that excessive regularization impairs the model's ability to learn effectively. Dropout rates between 0.2 and 0.3 offer a balance between regularization and performance, as they achieve relatively low validation loss while maintaining some regularization benefits.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_dcf65c15ac2147c2bdb22159f9b8a0cd_proc_1441386/SPR_BENCH_val_loss_curves.png"}, {"analysis": "This plot displays the final Dual-Weighted Accuracy (DWA) across different dropout rates. The DWA remains consistently high across all dropout rates, indicating that the model's overall performance on the SPR task is robust to changes in dropout. This suggests that while dropout impacts the training dynamics (as seen in the previous plot), it does not significantly affect the final accuracy metrics. This robustness is a positive outcome, as it implies that the model can generalize well across varying levels of regularization.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_dcf65c15ac2147c2bdb22159f9b8a0cd_proc_1441386/SPR_BENCH_final_DWA_vs_dropout.png"}], [{"analysis": "The training and validation loss curves indicate a consistent decrease in loss across all pooling types (mean, max, add, attn) as training progresses. This suggests that the model is learning effectively and there is no evidence of overfitting, as the validation loss follows a similar trend to the training loss. The max pooling approach appears to achieve the lowest loss values, particularly in validation, suggesting it might be the most effective pooling method for this task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation dual-weighted accuracy (DWA) curves show steady improvement across all pooling types over the epochs. The max pooling method consistently outperforms other pooling types, achieving the highest validation DWA by the end of training. The add and attn methods also perform well, with attn showing a slightly slower improvement rate initially but catching up towards the later epochs. The mean pooling method lags behind the others, indicating it might not be as effective for this task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_dwa_curves.png"}, {"analysis": "The final DWA bar plot confirms that the max pooling method achieves the highest accuracy among the pooling types, followed by add and attn, which are very close in performance. The mean pooling method has the lowest final DWA, though it is still relatively high. This reinforces the observation that max pooling is the most effective for this task, while mean pooling is less suitable.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_793aca35999d4c11bf783c261a3c60d5_proc_1441387/SPR_BENCH_final_dwa_bar.png"}], [{"analysis": "The first plot illustrates the cross-entropy loss curves for both training and validation sets across different hidden layer sizes (h=64, 128, 256). As the number of epochs increases, the loss consistently decreases for all configurations, indicating that the model is learning effectively. Larger hidden layer sizes (h=256) lead to faster and lower final loss values for both training and validation sets, suggesting that increasing model capacity improves performance. However, the validation loss for h=256 converges more closely to the training loss compared to smaller configurations, indicating reduced overfitting.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fec6a1c56ab34522947c097266dbcc1b_proc_1441384/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot shows the Dual Weighted Accuracy (DWA) on the validation set across epochs for different hidden layer sizes. The accuracy improves steadily with training for all configurations, with larger hidden layer sizes (h=256) achieving the highest final accuracy. This trend further supports the observation that increasing the model's capacity enhances its ability to capture the underlying patterns in the data. The consistent improvement without visible saturation or decline suggests the model is not overfitting within the observed epochs.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fec6a1c56ab34522947c097266dbcc1b_proc_1441384/SPR_BENCH_DWA_curves.png"}], [{"analysis": "The loss curves illustrate the training and validation loss trends for different pooling mechanisms ('mean', 'max', 'add', 'attn') over five epochs. All configurations show a steady decrease in loss, indicating effective learning. Notably, the attention-based pooling ('attn') achieves the lowest validation loss, suggesting it generalizes better to unseen data. The 'max' pooling also performs well, with slightly higher validation loss compared to 'attn'. The 'mean' pooling shows the slowest improvement, suggesting it may not capture the relational structure as effectively.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_loss_curves.png"}, {"analysis": "The accuracy curves demonstrate the progression of the Dual Weighted Accuracy (DWA) metric for different pooling mechanisms over epochs. The 'max' pooling consistently achieves the highest validation DWA, followed by 'attn'. The 'add' and 'mean' pooling mechanisms lag behind, with 'mean' showing the slowest improvement. This indicates that 'max' pooling is particularly effective in capturing the structural and relational dependencies in the data, leading to superior performance.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_dwa_curves.png"}, {"analysis": "The final DWA bar chart summarizes the performance of different pooling mechanisms. 'Max' pooling achieves the highest final DWA, closely followed by 'attn'. 'Add' and 'mean' pooling mechanisms perform slightly worse, with 'mean' showing the lowest final accuracy. This confirms that 'max' pooling is the most effective strategy for this task, while 'attn' also provides competitive results.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/SPR_BENCH_final_dwa_bar.png"}], [{"analysis": "The training and validation loss curves show a consistent decrease across all pooling types (mean, max, add, and attention) over the epochs. The validation losses closely follow the training losses, indicating that the model is not overfitting. Among the pooling types, the attention-based pooling seems to achieve the lowest validation loss by the end of training, suggesting its effectiveness in capturing the relational structures in the SPR task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation dual weighted accuracy (DWA) curves indicate that the max pooling strategy consistently outperforms the others throughout training, achieving the highest DWA by the last epoch. The add and attention pooling methods also show strong performance, with attention slightly lagging behind max pooling. Mean pooling performs the worst, showing slower convergence and lower final accuracy. This suggests that max pooling is the most effective for this task, followed closely by add and attention pooling.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_dwa_curves.png"}, {"analysis": "The bar chart highlights the final dual weighted accuracy for each pooling type. Max pooling achieves the highest accuracy, followed by attention and add pooling, with mean pooling trailing behind. The differences in performance are relatively small but consistent, reinforcing the observation that max pooling is the most effective method for this task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/SPR_BENCH_final_dwa_bar.png"}], [{"analysis": "This plot shows the training and validation loss curves for different pooling mechanisms (mean, max, add, attn) over 5 epochs. All pooling mechanisms demonstrate a consistent decrease in loss for both training and validation sets, indicating effective learning. The max pooling mechanism achieves the lowest validation loss, suggesting it is the most effective in this setup. The attn pooling also performs well, with a marginally higher validation loss compared to max pooling. The gaps between training and validation losses remain small, indicating limited overfitting.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot presents the validation dual weighted accuracy (DWA) curves for different pooling mechanisms over 5 epochs. The max pooling mechanism consistently outperforms the others, achieving the highest DWA. The attn and add pooling mechanisms also show good performance, with attn slightly outperforming add. The mean pooling mechanism lags behind the others but still demonstrates a steady improvement over epochs. This suggests that max pooling captures the relationships in the data more effectively for the SPR task.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_dwa_curves.png"}, {"analysis": "This plot summarizes the final dual weighted accuracy (DWA) achieved by each pooling mechanism. Max pooling achieves the highest DWA, followed closely by attn and add pooling, with mean pooling achieving the lowest DWA. This reinforces the observation that max pooling is the most effective mechanism for this task, while attn and add pooling are also viable options. Mean pooling, while performing adequately, is less effective compared to the other methods.", "plot_path": "experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/SPR_BENCH_final_dwa_bar.png"}], []], "vlm_feedback_summary": ["The plots indicate that the model training is stable, with decreasing losses and\nincreasing accuracy. The confusion matrix highlights good classification\nperformance with a low error rate.", "The plots show that the model effectively learns the task, with diminishing\nreturns on performance after 10-20 epochs. Overfitting becomes apparent with\nlonger training durations, as seen in the fluctuations of validation loss.\nValidation dual weighted accuracy peaks early, suggesting the need for careful\nhyperparameter tuning or regularization to optimize performance.", "The analysis highlights that a higher learning rate (0.003) leads to faster\nconvergence and better performance on the Dual-Weighted Accuracy (DWA) metric.\nHowever, potential overfitting with this learning rate should be monitored.\nLearning rates of 0.0003 and 0.0001 show more stable training but lower final\nperformance. The results suggest that 0.003 is the most promising learning rate,\nbalancing rapid optimization and high final accuracy.", "The analysis highlights that smaller batch sizes (16 and 32) outperform larger\nones in terms of both loss reduction and accuracy improvement, suggesting their\nsuitability for the SPR task. The results emphasize the impact of batch size on\nconvergence and generalization.", "The analysis highlights that lower weight decay values (wd_0.0 and wd_1e-06)\nlead to better performance across all metrics and visualizations. These\nconfigurations show faster convergence, higher accuracy, and better alignment of\npredictions with ground truth, making them the most promising candidates for\nfurther experimentation.", "The plots provide valuable insights into the model's performance. Larger\nembedding sizes (dim_128) consistently outperform smaller ones in both loss\nreduction and accuracy metrics, highlighting their effectiveness in capturing\ncomplex data relationships. However, overfitting and diminishing returns on\nperformance improvement warrant further hyperparameter tuning and careful\nanalysis of training dynamics.", "The analysis highlights the impact of dropout rates on validation loss and final\naccuracy. Lower dropout rates lead to faster convergence and lower validation\nloss, whereas higher dropout rates slow down learning. Despite these\ndifferences, the final accuracy remains consistently high across all dropout\nrates, showcasing the model's robustness.", "The results indicate that max pooling consistently outperforms other pooling\nmethods in terms of both loss reduction and accuracy improvement. Add and attn\npooling methods also demonstrate strong performance, while mean pooling is less\neffective. The model shows steady learning progress without signs of\noverfitting, and the evaluation metrics validate the effectiveness of max\npooling for the SPR task.", "The results demonstrate that increasing the hidden layer size improves both loss\nreduction and accuracy metrics. The model shows effective learning, and the\nlarger capacity (h=256) achieves the best performance. There is no evidence of\noverfitting within the observed epochs.", "The provided plots effectively demonstrate the training dynamics and final\nperformance of different pooling mechanisms for the SPR task. The results\nhighlight the superiority of 'max' and 'attn' pooling strategies in achieving\nhigh accuracy and low loss, validating their efficacy for this task.", "The analysis of the plots indicates that the model's performance improves\nsteadily across epochs, with max pooling consistently achieving the best\nresults. Attention and add pooling also perform well, but mean pooling lags\nbehind. The results suggest that max pooling effectively captures the relational\nand structural information in the sequences, making it the most suitable choice\nfor the SPR task.", "The results indicate that max pooling is the most effective pooling mechanism\nfor the SPR task, consistently achieving the lowest loss and highest dual\nweighted accuracy. Attention-based pooling also performs well, followed by\nadditive pooling. Mean pooling lags behind but still shows steady improvement.\nThese findings suggest that max pooling captures the structural dependencies in\nthe data most effectively, making it a strong candidate for further\nexperimentation.", "[]"], "exec_time": [27.51313853263855, 240.05519676208496, 23.860662698745728, 51.359869956970215, 105.91934490203857, 41.82930040359497, 43.92392349243164, 32.10449171066284, 86.22403168678284, 103.5722508430481, 38.61158037185669, 39.01808190345764, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = None\n\nif data:\n    # ---- unpack helpers ----\n    def unpack(pairs):  # list of (ts,val) or None\n        return [v for ts, v in pairs if v is not None] if pairs else []\n\n    epochs = range(1, len(data[\"losses\"][\"train\"]) + 1)\n    train_loss = unpack(data[\"losses\"][\"train\"])\n    val_loss = unpack(data[\"losses\"][\"val\"])\n    val_dwa = unpack(data[\"metrics\"][\"val\"])\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n    classes = sorted(set(gts) | set(preds))\n    cm = np.zeros((len(classes), len(classes)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[classes.index(t), classes.index(p)] += 1\n\n    # -------------- plots --------------\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"-o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, \"-o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. DWA curve\n    try:\n        plt.figure()\n        plt.plot(epochs, val_dwa, \"-o\", color=\"green\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dual-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Dual-Weighted Accuracy\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_DWA_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"SPR_BENCH: Confusion Matrix\\nLeft axis: Ground Truth, Bottom axis: Predicted\"\n        )\n        plt.xticks(range(len(classes)), classes)\n        plt.yticks(range(len(classes)), classes)\n        # annotate cells\n        for i in range(len(classes)):\n            for j in range(len(classes)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"num_epochs\", {})\n\n# ---------- per-run loss curves ----------\nfor run_key, run_val in runs.items():\n    try:\n        data = run_val.get(\"SPR_BENCH\", {})\n        train_loss = [v for _, v in data.get(\"losses\", {}).get(\"train\", [])]\n        val_loss = [v for _, v in data.get(\"losses\", {}).get(\"val\", [])]\n        epochs = range(1, len(train_loss) + 1)\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.title(f\"SPR_BENCH Loss Curves \u2013 {run_key} Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = f\"SPR_BENCH_loss_curves_{run_key}_epochs.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {run_key} epochs: {e}\")\n        plt.close()\n\n# ---------- aggregated DWA curves ----------\ntry:\n    plt.figure()\n    for run_key, run_val in runs.items():\n        data = run_val.get(\"SPR_BENCH\", {})\n        dwa_vals = [v for _, v in data.get(\"metrics\", {}).get(\"val\", [])]\n        epochs = range(1, len(dwa_vals) + 1)\n        plt.plot(epochs, dwa_vals, label=f\"{run_key} Epochs\")\n    plt.title(\"SPR_BENCH Validation Dual Weighted Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"DWA\")\n    plt.legend()\n    fname = \"SPR_BENCH_DWA_curves.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    lr_section = experiment_data.get(\"learning_rate\", {}).get(\"SPR_BENCH\", {})\n    final_dwa = {}\n\n    # ---------- Plot 1: Train/Val Loss ----------\n    try:\n        plt.figure()\n        for lr_key, run in lr_section.items():\n            train_losses = [v for _, v in run[\"losses\"][\"train\"]]\n            val_losses = [v for _, v in run[\"losses\"][\"val\"]]\n            epochs = range(1, len(train_losses) + 1)\n            plt.plot(epochs, train_losses, label=f\"{lr_key} train\")\n            plt.plot(epochs, val_losses, \"--\", label=f\"{lr_key} val\")\n        plt.title(\"SPR_BENCH: Training & Validation Loss vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_train_val_loss.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 2: Validation DWA ----------\n    try:\n        plt.figure()\n        for lr_key, run in lr_section.items():\n            dwa_vals = [v for _, v in run[\"metrics\"][\"val\"]]\n            epochs = range(1, len(dwa_vals) + 1)\n            plt.plot(epochs, dwa_vals, label=lr_key)\n            final_dwa[lr_key] = dwa_vals[-1] if dwa_vals else np.nan\n        plt.title(\"SPR_BENCH: Dual-Weighted Accuracy (Validation)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"DWA\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_validation_DWA.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # ---------- Plot 3: Final DWA Bar ----------\n    try:\n        plt.figure()\n        keys = list(final_dwa.keys())\n        vals = [final_dwa[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        plt.title(\"SPR_BENCH: Final Dual-Weighted Accuracy by Learning Rate\")\n        plt.ylabel(\"Final DWA\")\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_DWA_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n    # ---------- Print evaluation summary ----------\n    print(\"\\nFinal Validation DWA per learning rate:\")\n    for k, v in final_dwa.items():\n        print(f\"  {k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata_root = experiment_data.get(\"batch_size_tuning\", {}).get(\"SPR_BENCH\", {})\n\n# ---- Plot 1: Loss curves -----------------------------------------------------\ntry:\n    plt.figure()\n    for bs, entry in data_root.items():\n        epochs = range(1, len(entry[\"losses\"][\"train\"]) + 1)\n        train_losses = [l for _, l in entry[\"losses\"][\"train\"]]\n        val_losses = [l for _, l in entry[\"losses\"][\"val\"]]\n        plt.plot(epochs, train_losses, label=f\"train bs={bs}\", linestyle=\"--\")\n        plt.plot(epochs, val_losses, label=f\"val bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs. Validation Loss across Batch Sizes\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---- Plot 2: Validation DWA ---------------------------------------------------\ntry:\n    plt.figure()\n    for bs, entry in data_root.items():\n        epochs = range(1, len(entry[\"metrics\"][\"val\"]) + 1)\n        dwa_vals = [m for _, m in entry[\"metrics\"][\"val\"]]\n        plt.plot(epochs, dwa_vals, label=f\"bs={bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual-Weighted Accuracy\")\n    plt.title(\"SPR_BENCH: Validation DWA across Batch Sizes\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_validation_dwa.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n\n# Helper to extract per-epoch arrays -------------------------------------------------\ndef extract_series(run, key):  # key in {'train','val'}\n    \"\"\"Return list of values for losses[key] or metrics[key]\"\"\"\n    if key == \"dwa\":\n        return [v for _, v in run[\"metrics\"][\"val\"]]\n    if key == \"train_loss\":\n        return [v for _, v in run[\"losses\"][\"train\"]]\n    if key == \"val_loss\":\n        return [v for _, v in run[\"losses\"][\"val\"]]\n\n\n# ---------- Plot 1: loss curves -----------------------------------------------------\ntry:\n    plt.figure(figsize=(10, 4))\n    # two subplots\n    ax1 = plt.subplot(1, 2, 1)\n    ax2 = plt.subplot(1, 2, 2)\n    for wd_key, run in runs.items():\n        epochs = np.arange(1, len(run[\"losses\"][\"train\"]) + 1)\n        ax1.plot(epochs, extract_series(run, \"train_loss\"), label=wd_key)\n        ax2.plot(epochs, extract_series(run, \"val_loss\"), label=wd_key)\n    ax1.set_title(\"Train Loss\")\n    ax2.set_title(\"Validation Loss\")\n    for ax in (ax1, ax2):\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Cross-Entropy\")\n        ax.legend()\n    plt.suptitle(\"SPR_BENCH Loss Curves across weight_decay\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: DWA curves ------------------------------------------------------\ntry:\n    plt.figure()\n    for wd_key, run in runs.items():\n        epochs = np.arange(1, len(run[\"metrics\"][\"val\"]) + 1)\n        plt.plot(epochs, extract_series(run, \"dwa\"), label=wd_key)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual-Weighted-Accuracy\")\n    plt.title(\"SPR_BENCH Validation DWA over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_DWA_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA curves plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: final DWA bar chart --------------------------------------------\ntry:\n    final_dwa = {wd_key: extract_series(run, \"dwa\")[-1] for wd_key, run in runs.items()}\n    plt.figure()\n    plt.bar(\n        range(len(final_dwa)),\n        list(final_dwa.values()),\n        tick_label=list(final_dwa.keys()),\n    )\n    plt.ylabel(\"Final Dual-Weighted-Accuracy\")\n    plt.title(\"SPR_BENCH Final DWA per weight_decay\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_DWA_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final DWA bar plot: {e}\")\n    plt.close()\n\n# ---------- Plot 4: confusion scatter for best run ---------------------------------\ntry:\n    # pick best by final DWA\n    best_wd = max(runs, key=lambda k: extract_series(runs[k], \"dwa\")[-1])\n    best_run = runs[best_wd]\n    y_true = np.array(best_run[\"ground_truth\"])\n    y_pred = np.array(best_run[\"predictions\"])\n    plt.figure()\n    plt.scatter(\n        y_true + 0.05 * np.random.randn(len(y_true)),\n        y_pred + 0.05 * np.random.randn(len(y_pred)),\n        alpha=0.6,\n        s=10,\n    )\n    max_cls = max(y_true.max(), y_pred.max())\n    plt.plot([0, max_cls], [0, max_cls], \"k--\", linewidth=1)\n    plt.xlabel(\"True Label\")\n    plt.ylabel(\"Predicted Label\")\n    plt.title(f\"SPR_BENCH Predictions vs Ground Truth (Best: {best_wd})\")\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_scatter_best_{best_wd}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating scatter plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"embedding_dim\", {}).get(\"SPR_BENCH\", {})\nif not spr_data:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n\n# helper to collect series\ndef collect_series(key):\n    losses_train, losses_val, dwa_val = {}, {}, {}\n    for dim_key, d in spr_data.items():\n        ep = np.arange(1, len(d[\"losses\"][\"train\"]) + 1)\n        lt = np.array([v for _, v in d[\"losses\"][\"train\"]])\n        lv = np.array([v for _, v in d[\"losses\"][\"val\"]])\n        dv = np.array([v for _, v in d[\"metrics\"][\"val\"]])\n        losses_train[dim_key] = (ep, lt)\n        losses_val[dim_key] = (ep, lv)\n        dwa_val[dim_key] = (ep, dv)\n    return losses_train, losses_val, dwa_val\n\n\nloss_tr, loss_val, dwa_val = collect_series(spr_data)\n\n# -------- Figure 1: Loss curves --------\ntry:\n    plt.figure()\n    for dim_key in loss_tr:\n        ep, lt = loss_tr[dim_key]\n        _, lv = loss_val[dim_key]\n        plt.plot(ep, lt, label=f\"{dim_key}-train\")\n        plt.plot(ep, lv, linestyle=\"--\", label=f\"{dim_key}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Training, Right: Validation (dashed)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss figure: {e}\")\n    plt.close()\n\n# -------- Figure 2: Validation Dual Weighted Accuracy --------\ntry:\n    plt.figure()\n    for dim_key in dwa_val:\n        ep, dv = dwa_val[dim_key]\n        plt.plot(ep, dv, marker=\"o\", label=f\"{dim_key}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual Weighted Accuracy\")\n    plt.title(\"SPR_BENCH Validation DWA across Embedding Sizes\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_DWA_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA figure: {e}\")\n    plt.close()\n\n# -------- Print final DWA --------\nfor dim_key, (_, dv) in dwa_val.items():\n    print(f\"Final DWA ({dim_key}): {dv[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    data_root = experiment_data.get(\"dropout_rate_tuning\", {}).get(\"SPR_BENCH\", {})\n    rates = sorted(data_root.keys(), key=lambda r: float(r))\n    final_dwa = {}\n\n    # --------- Fig 1: val-loss curves ----------\n    try:\n        plt.figure()\n        for r in rates:\n            epochs = list(range(1, len(data_root[r][\"losses\"][\"val\"]) + 1))\n            vloss = [v for _, v in data_root[r][\"losses\"][\"val\"]]\n            plt.plot(epochs, vloss, marker=\"o\", label=f\"dropout={r}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Loss\")\n        plt.title(\"SPR_BENCH: Validation Loss vs Epoch for Different Dropout Rates\")\n        plt.legend()\n        fname1 = os.path.join(working_dir, \"SPR_BENCH_val_loss_curves.png\")\n        plt.savefig(fname1)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation-loss plot: {e}\")\n        plt.close()\n\n    # --------- Fig 2: final DWA per rate ----------\n    try:\n        for r in rates:\n            dwa_vals = [d for _, d in data_root[r][\"metrics\"][\"val\"] if d is not None]\n            final_dwa[r] = dwa_vals[-1] if dwa_vals else 0.0\n        plt.figure()\n        plt.bar(list(map(float, rates)), [final_dwa[r] for r in rates], width=0.04)\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Final Dual-Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Final Validation DWA by Dropout Rate\")\n        fname2 = os.path.join(working_dir, \"SPR_BENCH_final_DWA_vs_dropout.png\")\n        plt.savefig(fname2)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating DWA plot: {e}\")\n        plt.close()\n\n    # ---------- print best setting ----------\n    if final_dwa:\n        best_rate = max(final_dwa, key=final_dwa.get)\n        print(f\"Best dropout rate: {best_rate} | DWA={final_dwa[best_rate]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\npoolings = list(experiment_data.get(\"pooling_type\", {}).keys())\nepochs_dict = {}\n\n# Pre-extract data for ease\nfor p in poolings:\n    log = experiment_data[\"pooling_type\"][p][\"SPR_BENCH\"]\n    epochs_dict[p] = {\n        \"train_loss\": [v for _, v in log[\"losses\"][\"train\"]],\n        \"val_loss\": [v for _, v in log[\"losses\"][\"val\"]],\n        \"dwa\": [v for _, v in log[\"metrics\"][\"val\"]],\n    }\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"train_loss\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"train_loss\"], linestyle=\"--\", label=f\"{p}-train\")\n        plt.plot(ep, epochs_dict[p][\"val_loss\"], linestyle=\"-\", label=f\"{p}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# 2) DWA curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"dwa\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"dwa\"], label=p)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual Weighted Accuracy\")\n    plt.title(\"Validation Dual Weighted Accuracy Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_dwa_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA curves plot: {e}\")\n    plt.close()\n\n# 3) Final DWA bar chart\ntry:\n    plt.figure(figsize=(6, 4))\n    final_scores = [epochs_dict[p][\"dwa\"][-1] for p in poolings]\n    plt.bar(poolings, final_scores, color=\"skyblue\")\n    plt.ylabel(\"Final Dual Weighted Accuracy\")\n    plt.title(\"Final DWA by Pooling Type - SPR_BENCH\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_dwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final DWA bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- io ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nhidden_dims = [\"64\", \"128\", \"256\"]\nloss_dict = {}\ndwa_dict = {}\n\n# ---------- gather ----------\nfor h in hidden_dims:\n    try:\n        entry = experiment_data[\"gcn_hidden_dim\"][\"SPR_BENCH\"][h]\n        tr_losses = [v for _, v in entry[\"losses\"][\"train\"]]\n        val_losses = [v for _, v in entry[\"losses\"][\"val\"]]\n        val_dwa = [v for _, v in entry[\"metrics\"][\"val\"]]\n        loss_dict[h] = (tr_losses, val_losses)\n        dwa_dict[h] = val_dwa\n    except Exception as e:\n        print(f\"Hidden dim {h} missing: {e}\")\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    for h, (tr, val) in loss_dict.items():\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, \"--\", label=f\"train h={h}\")\n        plt.plot(epochs, val, \"-\", label=f\"val h={h}\")\n    plt.title(\"SPR_BENCH Loss Curves (GCN)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: validation DWA ----------\ntry:\n    plt.figure()\n    for h, vals in dwa_dict.items():\n        epochs = range(1, len(vals) + 1)\n        plt.plot(epochs, vals, label=f\"h={h}\")\n    plt.title(\"SPR_BENCH Validation Dual Weighted Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"DWA\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_DWA_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nfinal_scores = []\nfor h, vals in dwa_dict.items():\n    if vals:\n        final_scores.append((h, vals[-1]))\n        print(f\"Final DWA for hidden_dim={h}: {vals[-1]:.4f}\")\nif final_scores:\n    best_h, best_score = max(final_scores, key=lambda x: x[1])\n    print(f\"Best hidden_dim is {best_h} with DWA={best_score:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\npoolings = list(experiment_data.get(\"pooling_type\", {}).keys())\nepochs_dict = {}\n\n# Pre-extract data for ease\nfor p in poolings:\n    log = experiment_data[\"pooling_type\"][p][\"SPR_BENCH\"]\n    epochs_dict[p] = {\n        \"train_loss\": [v for _, v in log[\"losses\"][\"train\"]],\n        \"val_loss\": [v for _, v in log[\"losses\"][\"val\"]],\n        \"dwa\": [v for _, v in log[\"metrics\"][\"val\"]],\n    }\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"train_loss\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"train_loss\"], linestyle=\"--\", label=f\"{p}-train\")\n        plt.plot(ep, epochs_dict[p][\"val_loss\"], linestyle=\"-\", label=f\"{p}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# 2) DWA curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"dwa\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"dwa\"], label=p)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual Weighted Accuracy\")\n    plt.title(\"Validation Dual Weighted Accuracy Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_dwa_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA curves plot: {e}\")\n    plt.close()\n\n# 3) Final DWA bar chart\ntry:\n    plt.figure(figsize=(6, 4))\n    final_scores = [epochs_dict[p][\"dwa\"][-1] for p in poolings]\n    plt.bar(poolings, final_scores, color=\"skyblue\")\n    plt.ylabel(\"Final Dual Weighted Accuracy\")\n    plt.title(\"Final DWA by Pooling Type - SPR_BENCH\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_dwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final DWA bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\npoolings = list(experiment_data.get(\"pooling_type\", {}).keys())\nepochs_dict = {}\n\n# Pre-extract data for ease\nfor p in poolings:\n    log = experiment_data[\"pooling_type\"][p][\"SPR_BENCH\"]\n    epochs_dict[p] = {\n        \"train_loss\": [v for _, v in log[\"losses\"][\"train\"]],\n        \"val_loss\": [v for _, v in log[\"losses\"][\"val\"]],\n        \"dwa\": [v for _, v in log[\"metrics\"][\"val\"]],\n    }\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"train_loss\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"train_loss\"], linestyle=\"--\", label=f\"{p}-train\")\n        plt.plot(ep, epochs_dict[p][\"val_loss\"], linestyle=\"-\", label=f\"{p}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# 2) DWA curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"dwa\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"dwa\"], label=p)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual Weighted Accuracy\")\n    plt.title(\"Validation Dual Weighted Accuracy Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_dwa_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA curves plot: {e}\")\n    plt.close()\n\n# 3) Final DWA bar chart\ntry:\n    plt.figure(figsize=(6, 4))\n    final_scores = [epochs_dict[p][\"dwa\"][-1] for p in poolings]\n    plt.bar(poolings, final_scores, color=\"skyblue\")\n    plt.ylabel(\"Final Dual Weighted Accuracy\")\n    plt.title(\"Final DWA by Pooling Type - SPR_BENCH\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_dwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final DWA bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\npoolings = list(experiment_data.get(\"pooling_type\", {}).keys())\nepochs_dict = {}\n\n# Pre-extract data for ease\nfor p in poolings:\n    log = experiment_data[\"pooling_type\"][p][\"SPR_BENCH\"]\n    epochs_dict[p] = {\n        \"train_loss\": [v for _, v in log[\"losses\"][\"train\"]],\n        \"val_loss\": [v for _, v in log[\"losses\"][\"val\"]],\n        \"dwa\": [v for _, v in log[\"metrics\"][\"val\"]],\n    }\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"train_loss\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"train_loss\"], linestyle=\"--\", label=f\"{p}-train\")\n        plt.plot(ep, epochs_dict[p][\"val_loss\"], linestyle=\"-\", label=f\"{p}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# 2) DWA curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for p in poolings:\n        ep = np.arange(1, len(epochs_dict[p][\"dwa\"]) + 1)\n        plt.plot(ep, epochs_dict[p][\"dwa\"], label=p)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual Weighted Accuracy\")\n    plt.title(\"Validation Dual Weighted Accuracy Curves - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_dwa_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating DWA curves plot: {e}\")\n    plt.close()\n\n# 3) Final DWA bar chart\ntry:\n    plt.figure(figsize=(6, 4))\n    final_scores = [epochs_dict[p][\"dwa\"][-1] for p in poolings]\n    plt.bar(poolings, final_scores, color=\"skyblue\")\n    plt.ylabel(\"Final Dual Weighted Accuracy\")\n    plt.title(\"Final DWA by Pooling Type - SPR_BENCH\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_dwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final DWA bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# Basic I/O setup\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load all experiment_data dictionaries\n# ------------------------------------------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d470518d7b4bc6be574a2b8a51d291_proc_1441385/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_71b534c25dc3465ba38a6d3287a9cd5d_proc_1441387/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-38_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_066a13ae404c43ad9fcdc4423c6d4ab5_proc_1441386/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp_dict = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dict)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# ------------------------------------------------------------------\n# Aggregate per-epoch metrics across runs\n# ------------------------------------------------------------------\nagg = {}  # {pooling_type: {metric: list_of_arrays_from_runs}}\nfor exp in all_experiment_data:\n    for pooling_type, datasets in exp.get(\"pooling_type\", {}).items():\n        log = datasets.get(\"SPR_BENCH\", {})\n        tl = np.array([v for _, v in log.get(\"losses\", {}).get(\"train\", [])])\n        vl = np.array([v for _, v in log.get(\"losses\", {}).get(\"val\", [])])\n        dwa = np.array([v for _, v in log.get(\"metrics\", {}).get(\"val\", [])])\n\n        if pooling_type not in agg:\n            agg[pooling_type] = {\"train_loss\": [], \"val_loss\": [], \"dwa\": []}\n\n        # Only add if we actually have data\n        if tl.size:\n            agg[pooling_type][\"train_loss\"].append(tl)\n        if vl.size:\n            agg[pooling_type][\"val_loss\"].append(vl)\n        if dwa.size:\n            agg[pooling_type][\"dwa\"].append(dwa)\n\n\n# Helper to stack runs to same length (min length across runs)\ndef stack_and_crop(list_of_1d_arrays):\n    if not list_of_1d_arrays:\n        return np.empty((0, 0))\n    min_len = min(arr.shape[0] for arr in list_of_1d_arrays)\n    cropped = np.stack([arr[:min_len] for arr in list_of_1d_arrays], axis=0)\n    return cropped  # shape (n_runs, min_len)\n\n\n# Prepare summary stats\nsummary = {}  # {pooling_type: {metric: {\"mean\":1d, \"se\":1d}}}\nfor p, metrics in agg.items():\n    summary[p] = {}\n    for m, runs in metrics.items():\n        data = stack_and_crop(runs)\n        if data.size == 0:\n            continue\n        mean = data.mean(axis=0)\n        se = (\n            data.std(axis=0, ddof=1) / np.sqrt(data.shape[0])\n            if data.shape[0] > 1\n            else np.zeros_like(mean)\n        )\n        summary[p][m] = {\"mean\": mean, \"se\": se}\n\n# ------------------------------------------------------------------\n# 1) Training & Validation Loss curves with standard error\n# ------------------------------------------------------------------\ntry:\n    plt.figure(figsize=(8, 5))\n    for p, metrics in summary.items():\n        if \"train_loss\" not in metrics or \"val_loss\" not in metrics:\n            continue\n        ep = np.arange(1, len(metrics[\"train_loss\"][\"mean\"]) + 1)\n        # Train\n        plt.plot(\n            ep, metrics[\"train_loss\"][\"mean\"], linestyle=\"--\", label=f\"{p}-train mean\"\n        )\n        plt.fill_between(\n            ep,\n            metrics[\"train_loss\"][\"mean\"] - metrics[\"train_loss\"][\"se\"],\n            metrics[\"train_loss\"][\"mean\"] + metrics[\"train_loss\"][\"se\"],\n            alpha=0.25,\n        )\n        # Val\n        plt.plot(ep, metrics[\"val_loss\"][\"mean\"], linestyle=\"-\", label=f\"{p}-val mean\")\n        plt.fill_between(\n            ep,\n            metrics[\"val_loss\"][\"mean\"] - metrics[\"val_loss\"][\"se\"],\n            metrics[\"val_loss\"][\"mean\"] + metrics[\"val_loss\"][\"se\"],\n            alpha=0.25,\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training & Validation Loss (mean \u00b1 SE) - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curves plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Validation DWA curves with standard error\n# ------------------------------------------------------------------\ntry:\n    plt.figure(figsize=(8, 5))\n    for p, metrics in summary.items():\n        if \"dwa\" not in metrics:\n            continue\n        ep = np.arange(1, len(metrics[\"dwa\"][\"mean\"]) + 1)\n        plt.plot(ep, metrics[\"dwa\"][\"mean\"], label=f\"{p} mean\")\n        plt.fill_between(\n            ep,\n            metrics[\"dwa\"][\"mean\"] - metrics[\"dwa\"][\"se\"],\n            metrics[\"dwa\"][\"mean\"] + metrics[\"dwa\"][\"se\"],\n            alpha=0.25,\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Dual Weighted Accuracy\")\n    plt.title(\"Validation DWA (mean \u00b1 SE) - SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_dwa_curves_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated DWA curves plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Final-epoch DWA bar chart with error bars\n# ------------------------------------------------------------------\ntry:\n    plt.figure(figsize=(6, 4))\n    poolings, means, ses = [], [], []\n    for p, metrics in summary.items():\n        if \"dwa\" in metrics:\n            poolings.append(p)\n            means.append(metrics[\"dwa\"][\"mean\"][-1])\n            ses.append(metrics[\"dwa\"][\"se\"][-1])\n    if poolings:\n        x = np.arange(len(poolings))\n        plt.bar(x, means, yerr=ses, capsize=5, color=\"skyblue\")\n        plt.xticks(x, poolings)\n        plt.ylabel(\"Final Dual Weighted Accuracy\")\n        plt.title(\"Final DWA by Pooling Type (mean \u00b1 SE) - SPR_BENCH\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_dwa_bar_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated final DWA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print aggregated final metrics for quick inspection\n# ------------------------------------------------------------------\nfor p, metrics in summary.items():\n    if \"dwa\" in metrics:\n        print(\n            f\"{p}: final DWA mean={metrics['dwa']['mean'][-1]:.4f} \u00b1 SE={metrics['dwa']['se'][-1]:.4f}\"\n        )\n"], "plot_plan": ["We first load the saved experiment_data.npy, then pull out timestamps, training/validation losses, and validation dual-weighted accuracy (DWA) recorded for the SPR_BENCH run. Three separate figures are produced: (1) a line plot comparing training and validation losses versus epoch index, (2) a line plot of validation DWA versus epoch index, and (3) a confusion-matrix heat-map of ground-truth versus predicted class labels on the dev set from the final epoch. Each figure is created inside its own try-except block, saved to the working directory with descriptive names that include the dataset and plot type, and then closed to free resources. If more than five epochs were present we would only plot at evenly spaced intervals, but the current data already contains \u22645 epochs. All plots have clear titles and, where relevant, subtitles that clarify what the left/right axes or panels represent. No values are fabricated\u2014everything comes directly from experiment_data.npy. The code follows the mandated import/working_dir boilerplate and relies only on matplotlib/numpy (plus the tiny helper from numpy for the confusion matrix).", "We first load experiment_data.npy from the working directory and inspect the nested dictionary. For every training\u2010length setting (10, 20, 30, 50 epochs) we extract the stored lists of (timestamp, value) tuples for train/val losses and the validation dual weighted accuracy (DWA). Using enumerate, we convert these lists into epoch-indexed arrays, ignoring timestamps because epoch order is clearer. For each run we create one figure comparing train and validation losses, saving it with a descriptive file name (e.g., \u201cSPR_BENCH_loss_curves_10_epochs.png\u201d). After those four figures, we build a fifth figure that overlays the validation DWA curves for all runs, placing a legend that shows the epoch budget of each curve. Each plotting block is wrapped in a try-except to guarantee graceful failure and that figures are always closed. All plots are saved into the predefined working_dir and explicitly closed to avoid memory leaks. No data are fabricated; everything comes directly from experiment_data.npy. The code below implements these steps concisely.", "We load the saved experiment_data.npy, parse the \u201clearning_rate/SPR_BENCH\u201d section, and iterate through every lr run. For each run we extract epoch-wise train losses, validation losses, and the dual-weighted accuracies (DWA). We then create three visuals: (1) a combined line plot of train/val loss versus epoch for all lrs, (2) a combined line plot of validation DWA versus epoch for all lrs, and (3) a bar plot comparing the final-epoch DWA of each lr. Each figure is wrapped in its own try-except, given a descriptive title, saved to working_dir with an informative filename, and closed afterwards. We restrict ourselves to these three aggregate plots (\u22645 total) to avoid redundancy. Finally, we print the final DWA for every learning rate so the user can immediately see which setting performed best.", "We will first load the serialized experiment_data.npy, navigate to the \u201cbatch_size_tuning \u2192 SPR_BENCH\u201d dictionary and retrieve the per-epoch losses and validation dual-weighted accuracy for every examined batch size. Two figures will be produced: (1) training vs. validation loss curves, and (2) validation DWA curves; each line corresponds to a different batch size so that we keep the number of figures \u2264 5. Every figure is wrapped in its own try-except to satisfy robustness requirements.  All plots include titles that explicitly mention the dataset and plot type, legends that list the batch sizes, and axis labels for clarity. Figures are saved inside working_dir with descriptive filenames that include the dataset name and plot description.  We always close figures, create working_dir if absent, and never fabricate data\u2014everything is taken directly from the loaded numpy dictionary.  The code is concise, starts with the mandated imports, and ends by printing the paths of the saved plots for quick confirmation.", "We first load experiment_data.npy, focus on the \u201cSPR_BENCH\u201d runs and gather, for every weight-decay value, the epoch-wise training/validation losses and the validation Dual-Weighted-Accuracy (DWA).  \nPlot 1 contains two subplots: train and validation loss curves, each curve colour-coded by weight-decay so one can compare convergence behaviour.  \nPlot 2 visualises validation DWA over epochs for each weight-decay, revealing which regularisation level generalises best during training.  \nPlot 3 is a simple bar chart of the final-epoch DWA for all weight-decays to give a quick at-a-glance ranking.  \nWe then pick the best run (highest final DWA) and, in Plot 4, scatter the ground-truth labels against the model\u2019s predictions to highlight any systematic confusions.  \nAll figures are created inside individual try-except blocks, saved in the \u201cworking\u201d directory with descriptive names, and closed immediately after saving.  \nOnly data present in experiment_data.npy are used\u2014no synthetic values are introduced\u2014and at most four figures (\u22645 allowed) are produced.  \nThe code follows the required boilerplate, uses standard matplotlib without custom styles, limits itself to numpy/matplotlib/os, and respects the figure-naming, titling, and interval rules.", "We load the saved experiment_data.npy, extract per-epoch losses and Dual Weighted Accuracy (DWA) for each embedding dimension, and build two concise visualizations. Figure 1 overlays training and validation loss curves for all embedding sizes so differences are directly comparable; Figure 2 overlays the corresponding validation DWA curves. Each figure is wrapped in its own try-except to avoid crashing and is saved to the working directory with descriptive names that include the dataset (SPR_BENCH) and content. After plotting, figures are always closed. Finally, the script prints the last recorded validation DWA for each embedding dimension so the user can immediately see the best metric values without opening the plots. All plotting strictly uses data contained in experiment_data.npy; no synthetic data are generated. The code is kept short and readable, following the provided guidelines.", "We will load the saved experiment dictionary, iterate over the dropout-rate keys, and extract per-epoch validation loss as well as the final dual-weighted accuracy (DWA).  \nFigure 1 plots the validation-loss curves for every dropout value so we can see which setting converges best; all rates share the same x-axis (epochs) and appear as separate labeled lines.  \nFigure 2 shows a bar chart of the final-epoch DWA for each rate, allowing quick comparison of predictive quality.  \nBoth plots are wrapped in independent try-except blocks, saved into the working directory with descriptive names, and the figures are closed afterward.  \nWe also compute the best dropout rate according to highest DWA and print it alongside its score.  \nNo synthetic data are generated\u2014only arrays already present in experiment_data.npy are used.  \nAt most two figures are produced (well below the five-figure limit), each titled clearly and including the dataset name.  \nThe code begins with the required imports and working_dir creation and is kept concise.", "We will load the stored dictionary from experiment_data.npy, iterate over the four pooling configurations and extract, for the SPR_BENCH split, the epoch-wise training/validation losses and validation Dual-Weighted Accuracy (DWA). Three standard visualizations will be created: (1) a combined line plot of the training and validation loss curves for all pooling types, (2) a combined line plot of the DWA curves for all pooling types, and (3) a bar chart that summarizes the final-epoch DWA for each pooling type. Each figure is wrapped in its own try-except block to ensure robust execution and is saved to working_dir with descriptive names. Epoch indices (1..n) are used on the x-axis because absolute timestamps are not informative for readers. Every figure is titled and legends are provided to clarify pooling types and curve meanings, and figures are always closed afterward. No data is fabricated; everything comes directly from experiment_data.npy, and no more than three figures are produced, keeping within the five-figure limit.", "We first load the serialized experiment_data.npy, then extract the recorded per-epoch losses and Dual Weighted Accuracy (DWA) for each hidden dimension (64, 128, 256). We plot two figures: (1) training vs. validation losses and (2) validation DWA, each containing one line per hidden dimension so we stay within the 5-figure limit. Each plot is wrapped in its own try-except block to guarantee graceful failure and closed after saving. Clear titles and axis labels reference the SPR_BENCH dataset, and filenames include the dataset name plus plot type for easy identification. After plotting, the script prints the final-epoch DWA for every hidden size and points out the best model according to this metric. All outputs are written to the pre-defined working_dir. The code adheres strictly to the plotting guidelines, uses only the data present in experiment_data.npy, and relies solely on base matplotlib calls.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "batch_size", "weight_decay", "embedding_dim", "dropout_rate", "pooling_type", "gcn_hidden_dim", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["We will load the numpy file from the working directory, convert the stored\nobject back to a Python dict, and iterate over its top-level keys (datasets).\nFor every dataset we will pull the last entry of each metric/loss list, because\nthe training script always appends the newest result to the end; this serves as\nthe \u201cfinal\u201d value.   Only numeric tuples are considered\u2014`None` placeholders\n(e.g., train metric) are ignored.   Each value is printed with an explicit,\ndescriptive label so that the output is unambiguous.", "The script first locates the saved experiment_data.npy file in the working\ndirectory, loads it into memory, and iterates through every hyper-parameter\nsetting stored under the \"num_epochs\" key. For each run it drills down to the\nsingle dataset (\u201cSPR_BENCH\u201d) and extracts the latest training loss, the latest\nvalidation loss, the best validation dual weighted accuracy, and the epoch at\nwhich that best accuracy was achieved. Each metric is printed with an explicit,\ndescriptive name so the output is self-explanatory and meets the formatting\nconstraints. No figures are created and the code runs immediately at import\ntime.", "Below is a short outline followed by fully-executable Python code.  The script\nfirst locates and loads the saved numpy file from the \u201cworking\u201d directory.   It\nthen iterates through every hyper-parameter setting stored for each dataset\n(here only \u201cSPR_BENCH\u201d).   For every setting it extracts the recorded series of\ntraining losses, validation losses and validation Dual-Weighted Accuracy (DWA),\nselects the best value for each metric across all epochs, and prints them with\nexplicit metric names.", "The script will load experiment_data.npy from the working directory, iterate\nthrough every sweep, dataset and batch-size configuration, and then compute the\nfinal (last epoch) training loss, the best (minimum) validation loss, and the\nbest (maximum) validation dual weighted accuracy. For each dataset it prints\nthese metrics clearly, including the batch-size used, exactly once without\ngenerating any plots.", "Below is a small utility that loads the saved numpy file, walks through the\nnested dictionary, and prints the best (minimum for losses, maximum for\naccuracy) values recorded during training.   The script executes immediately,\nprinting the dataset name once and then, for every weight-decay setting, the\nbest train loss, best validation loss, and best validation dual-weighted\naccuracy found in the log.", "The script will load the saved numpy file from the working directory, walk\nthrough the nested dictionary structure (hyper-parameter group \u2192 dataset \u2192\nindividual experiment), and print the final values recorded for training loss,\nvalidation loss, and dual weighted accuracy. The dataset name and experiment\nidentifier (e.g., the embedding dimension) are printed first to keep the output\nclear and hierarchical. Only the last (i.e., final) value stored for each metric\nis reported.", "We will load experiment_data.npy from the working directory, iterate through\nevery dataset contained in the hyper-parameter sweep, and for each dataset find\nthe run (dropout rate) that yields the highest final validation dual-weighted\naccuracy.  For this best run we will report its final training loss, final\nvalidation loss, and final validation dual-weighted accuracy, always printing\nexplicit metric names.  The code executes immediately at global scope without\nany __main__ guard.", "The script will (1) locate the working directory, (2) load experiment_data.npy,\n(3) iterate through every pooling-type run stored for the single dataset\n\u201cSPR_BENCH,\u201d and (4) print the final training loss, the final validation loss,\nand the best (highest) validation dual-weighted accuracy for each run.  All\nprinting is done immediately at the global scope, with clear, explicit metric\nnames.", "The script will locate the working directory, load the stored NumPy dictionary,\nand then iterate through every experiment configuration in it.   For each\ndataset (e.g., SPR_BENCH) and each configuration (hidden-dim), it extracts the\nlast recorded training loss, validation loss, and validation dual-weighted\naccuracy, and also computes the best validation dual-weighted accuracy across\nepochs.   The code then prints the dataset name followed by clearly labelled\nmetric values for every configuration.", "The script will (1) locate the working directory, (2) load experiment_data.npy,\n(3) iterate through every pooling-type run stored for the single dataset\n\u201cSPR_BENCH,\u201d and (4) print the final training loss, the final validation loss,\nand the best (highest) validation dual-weighted accuracy for each run.  All\nprinting is done immediately at the global scope, with clear, explicit metric\nnames.", "The script will (1) locate the working directory, (2) load experiment_data.npy,\n(3) iterate through every pooling-type run stored for the single dataset\n\u201cSPR_BENCH,\u201d and (4) print the final training loss, the final validation loss,\nand the best (highest) validation dual-weighted accuracy for each run.  All\nprinting is done immediately at the global scope, with clear, explicit metric\nnames.", "The script will (1) locate the working directory, (2) load experiment_data.npy,\n(3) iterate through every pooling-type run stored for the single dataset\n\u201cSPR_BENCH,\u201d and (4) print the final training loss, the final validation loss,\nand the best (highest) validation dual-weighted accuracy for each run.  All\nprinting is done immediately at the global scope, with clear, explicit metric\nnames.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef get_last_numeric_entry(lst):\n    \"\"\"Return the last element whose second field is a number.\"\"\"\n    for entry in reversed(lst):\n        # skip `None` placeholders\n        if entry is None:\n            continue\n        # entry is expected to be (timestamp, value)\n        if isinstance(entry, (list, tuple)) and len(entry) == 2:\n            return entry[1]\n    return None\n\n\n# ---------- iterate and print ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}\")  # dataset header\n\n    # losses\n    train_loss = get_last_numeric_entry(content[\"losses\"][\"train\"])\n    if train_loss is not None:\n        print(f\"final training loss: {train_loss:.4f}\")\n\n    val_loss = get_last_numeric_entry(content[\"losses\"][\"val\"])\n    if val_loss is not None:\n        print(f\"final validation loss: {val_loss:.4f}\")\n\n    # metrics\n    val_dwa = get_last_numeric_entry(content[\"metrics\"][\"val\"])\n    if val_dwa is not None:\n        print(f\"final dual weighted accuracy (validation): {val_dwa:.4f}\")\n\n    # add more metrics here if they appear in future experiment_data structures\n", "import os\nimport numpy as np\n\n# ---------- locate and load the experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper for safe metric extraction ----------\ndef get_last_value(lst):\n    \"\"\"Return the second element (value) of the last tuple in a list or None.\"\"\"\n    if lst:\n        return lst[-1][1]\n    return None\n\n\n# ---------- iterate through stored runs and print metrics ----------\nfor num_epoch_str, dataset_dict in experiment_data.get(\"num_epochs\", {}).items():\n    for dataset_name, run_data in dataset_dict.items():\n        print(f\"Dataset: {dataset_name} (configuration: {num_epoch_str} epochs)\")\n\n        final_train_loss = get_last_value(run_data[\"losses\"][\"train\"])\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.6f}\")\n\n        final_val_loss = get_last_value(run_data[\"losses\"][\"val\"])\n        if final_val_loss is not None:\n            print(f\"final validation loss: {final_val_loss:.6f}\")\n\n        best_dwa = run_data.get(\"best_dwa\")\n        if best_dwa is not None and best_dwa >= 0:\n            print(f\"best validation dual weighted accuracy: {best_dwa:.6f}\")\n\n        best_epoch = run_data.get(\"best_epoch\")\n        if best_epoch is not None:\n            print(f\"best epoch: {best_epoch}\")\n\n        print()  # blank line for readability between configurations\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- parse and print ----------\nlr_section = experiment_data.get(\"learning_rate\", {})\nfor dataset_name, configs in lr_section.items():\n    print(f\"Dataset: {dataset_name}\")\n    for config_name, results in configs.items():\n        train_losses = results[\"losses\"][\"train\"]  # list[(timestamp, value)]\n        val_losses = results[\"losses\"][\"val\"]  # list[(timestamp, value)]\n        val_metrics = results[\"metrics\"][\"val\"]  # list[(timestamp, DWA)]\n\n        # Guard against empty lists\n        if not (train_losses and val_losses and val_metrics):\n            continue\n\n        # best values across all epochs\n        best_train_loss = min(item[1] for item in train_losses)\n        best_val_loss = min(item[1] for item in val_losses)\n        best_dwa = max(item[1] for item in val_metrics)\n\n        print(f\"  Hyper-parameters: {config_name}\")\n        print(f\"    best training loss: {best_train_loss:.4f}\")\n        print(f\"    best validation loss: {best_val_loss:.4f}\")\n        print(f\"    best validation dual weighted accuracy: {best_dwa:.4f}\")\n", "import os\nimport numpy as np\n\n# --- locate and load ----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --- helper to pretty-print floats -------------------------------------------\ndef fmt(v):\n    return f\"{v:.4f}\" if isinstance(v, (float, int)) else str(v)\n\n\n# --- iterate and report -------------------------------------------------------\nfor sweep_name, sweep_content in experiment_data.items():  # e.g. 'batch_size_tuning'\n    for dataset_name, dataset_content in sweep_content.items():  # e.g. 'SPR_BENCH'\n        print(dataset_name)  # requirement (3)\n\n        for bs, run in dataset_content.items():  # hyper-parameter value\n            # extract lists\n            train_losses = run[\"losses\"][\"train\"]  # list of (ts, value)\n            val_losses = run[\"losses\"][\"val\"]\n            val_metrics = run[\"metrics\"][\"val\"]  # list of (ts, DWA)\n\n            # final / best values\n            final_train_loss = train_losses[-1][1] if train_losses else None\n            best_val_loss = min(v for _, v in val_losses) if val_losses else None\n            best_val_dwa = max(m for _, m in val_metrics) if val_metrics else None\n\n            # print, labelling each metric clearly (4)\n            print(f\"  batch size {bs}\")\n            print(f\"    final training loss: {fmt(final_train_loss)}\")\n            print(f\"    best validation loss: {fmt(best_val_loss)}\")\n            print(f\"    best validation dual weighted accuracy: {fmt(best_val_dwa)}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------\n# locate and load the serialized experiment dictionary\n# ---------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------\n# helper to get best (min for loss, max for accuracy)\n# ---------------------------------------------------------\ndef best_loss(loss_list):\n    # loss_list contains tuples (timestamp, value)\n    return min(val for _, val in loss_list) if loss_list else None\n\n\ndef best_metric(metric_list):\n    # metric_list contains tuples (timestamp, value)\n    # Some entries may be None (e.g. train metrics), so filter them out\n    cleaned = [val for item in metric_list if item is not None for _, val in [item]]\n    return max(cleaned) if cleaned else None\n\n\n# ---------------------------------------------------------\n# iterate through datasets and runs\n# ---------------------------------------------------------\nfor hp_family, datasets in experiment_data.items():  # e.g. \"weight_decay\"\n    for dataset_name, runs in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"{dataset_name}\")  # dataset header\n        for wd_key, run_data in runs.items():  # e.g. \"wd_0.0\"\n            train_losses = run_data[\"losses\"][\"train\"]\n            val_losses = run_data[\"losses\"][\"val\"]\n            val_metrics = run_data[\"metrics\"][\"val\"]\n\n            print(f\"  {wd_key}\")  # hyper-parameter value\n            print(f\"    best train loss: {best_loss(train_losses):.4f}\")\n            print(f\"    best validation loss: {best_loss(val_losses):.4f}\")\n            print(\n                f\"    best validation dual weighted accuracy: {best_metric(val_metrics):.4f}\"\n            )\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate & report ----------\nfor hp_group, hp_dict in experiment_data.items():  # e.g. \"embedding_dim\"\n    for dataset_name, exp_dict in hp_dict.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\nDataset: {dataset_name}\")\n        for exp_id, results in exp_dict.items():  # e.g. \"dim_32\", \"dim_64\", ...\n            print(f\"  Experiment: {exp_id}\")\n\n            # --- losses ---\n            train_losses = results[\"losses\"][\"train\"]\n            val_losses = results[\"losses\"][\"val\"]\n            if train_losses:\n                final_train_loss = train_losses[-1][1]  # (timestamp, value)\n                print(f\"    training loss: {final_train_loss:.4f}\")\n            if val_losses:\n                final_val_loss = val_losses[-1][1]\n                print(f\"    validation loss: {final_val_loss:.4f}\")\n\n            # --- dual weighted accuracy ---\n            val_metrics = results[\"metrics\"][\"val\"]  # list of (timestamp, metric_val)\n            final_dwa = None\n            for ts, val in reversed(val_metrics):\n                if val is not None:\n                    final_dwa = val\n                    break\n            if final_dwa is not None:\n                print(f\"    dual weighted accuracy: {final_dwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment file ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate and extract best metrics ----------\nfor sweep_name, datasets in experiment_data.items():  # e.g. \"dropout_rate_tuning\"\n    for dataset_name, runs in datasets.items():  # e.g. \"SPR_BENCH\"\n        best_rec, best_rate, best_dwa = None, None, -float(\"inf\")\n\n        for rate_str, record in runs.items():  # each dropout rate run\n            val_metrics = record[\"metrics\"][\"val\"]\n            if not val_metrics:  # safety-check\n                continue\n            # final dual-weighted accuracy is the last stored value\n            final_dwa = (\n                val_metrics[-1][1]\n                if isinstance(val_metrics[-1], tuple)\n                else val_metrics[-1]\n            )\n            if final_dwa > best_dwa:\n                best_dwa = final_dwa\n                best_rate = rate_str\n                best_rec = record\n\n        if best_rec is None:  # no valid runs found\n            print(f\"{dataset_name}\\n  (no metrics found)\")\n            continue\n\n        # fetch final losses\n        final_train_loss = best_rec[\"losses\"][\"train\"][-1][1]\n        final_val_loss = best_rec[\"losses\"][\"val\"][-1][1]\n\n        # ---------- print results ----------\n        print(dataset_name)\n        print(f\"best dropout rate: {best_rate}\")\n        print(f\"training loss: {final_train_loss:.6f}\")\n        print(f\"validation loss: {final_val_loss:.6f}\")\n        print(f\"validation dual weighted accuracy: {best_dwa:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate & load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate & report ----------\nfor search_space, runs in experiment_data.items():  # e.g., \"pooling_type\"\n    for run_name, datasets in runs.items():  # e.g., \"mean\", \"max\"...\n        for dataset_name, log in datasets.items():  # e.g., \"SPR_BENCH\"\n            print(f\"\\nDataset: {dataset_name}  ( {search_space} = {run_name} )\")\n\n            # losses\n            train_losses = log[\"losses\"][\"train\"]\n            val_losses = log[\"losses\"][\"val\"]\n\n            final_train_loss = train_losses[-1][1] if train_losses else None\n            final_val_loss = val_losses[-1][1] if val_losses else None\n            best_val_loss = min(v[1] for v in val_losses) if val_losses else None\n\n            # metrics (dual-weighted accuracy stored in validation list)\n            val_metrics = log[\"metrics\"][\"val\"]\n            final_val_dwa = val_metrics[-1][1] if val_metrics else None\n            best_val_dwa = max(v[1] for v in val_metrics) if val_metrics else None\n\n            # ----- printing -----\n            if final_train_loss is not None:\n                print(f\"final training loss: {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"final validation loss: {final_val_loss:.4f}\")\n                print(f\"best (lowest) validation loss: {best_val_loss:.4f}\")\n            if final_val_dwa is not None:\n                print(f\"final validation dual weighted accuracy: {final_val_dwa:.4f}\")\n                print(\n                    f\"best (highest) validation dual weighted accuracy: {best_val_dwa:.4f}\"\n                )\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to fetch final / best ----------\ndef last_value(list_of_tuples):\n    \"\"\"Return the value (index 1) from the last (timestamp, value) tuple.\"\"\"\n    return list_of_tuples[-1][1] if list_of_tuples else None\n\n\ndef best_value(list_of_tuples, mode=\"max\"):\n    \"\"\"Return best value depending on mode ('max' or 'min').\"\"\"\n    if not list_of_tuples:\n        return None\n    values = [v for _, v in list_of_tuples]\n    return max(values) if mode == \"max\" else min(values)\n\n\n# ---------- iterate and print ----------\nfor exp_name, datasets in experiment_data.items():  # e.g., 'gcn_hidden_dim'\n    for dataset_name, configs in datasets.items():  # e.g., 'SPR_BENCH'\n        print(dataset_name)  # requirement (3)\n        for config_name, entry in configs.items():  # e.g., '64', '128', ...\n            # fetch final metrics\n            final_train_loss = last_value(entry[\"losses\"][\"train\"])\n            final_val_loss = last_value(entry[\"losses\"][\"val\"])\n            final_val_dwa = last_value(entry[\"metrics\"][\"val\"])\n            best_val_dwa = best_value(entry[\"metrics\"][\"val\"], mode=\"max\")\n\n            # print results\n            print(f\"  configuration (hidden_dim={config_name})\")\n            print(f\"    final training loss: {final_train_loss:.6f}\")\n            print(f\"    final validation loss: {final_val_loss:.6f}\")\n            print(f\"    final validation dual weighted accuracy: {final_val_dwa:.6f}\")\n            print(f\"    best validation dual weighted accuracy: {best_val_dwa:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate & load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate & report ----------\nfor search_space, runs in experiment_data.items():  # e.g., \"pooling_type\"\n    for run_name, datasets in runs.items():  # e.g., \"mean\", \"max\"...\n        for dataset_name, log in datasets.items():  # e.g., \"SPR_BENCH\"\n            print(f\"\\nDataset: {dataset_name}  ( {search_space} = {run_name} )\")\n\n            # losses\n            train_losses = log[\"losses\"][\"train\"]\n            val_losses = log[\"losses\"][\"val\"]\n\n            final_train_loss = train_losses[-1][1] if train_losses else None\n            final_val_loss = val_losses[-1][1] if val_losses else None\n            best_val_loss = min(v[1] for v in val_losses) if val_losses else None\n\n            # metrics (dual-weighted accuracy stored in validation list)\n            val_metrics = log[\"metrics\"][\"val\"]\n            final_val_dwa = val_metrics[-1][1] if val_metrics else None\n            best_val_dwa = max(v[1] for v in val_metrics) if val_metrics else None\n\n            # ----- printing -----\n            if final_train_loss is not None:\n                print(f\"final training loss: {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"final validation loss: {final_val_loss:.4f}\")\n                print(f\"best (lowest) validation loss: {best_val_loss:.4f}\")\n            if final_val_dwa is not None:\n                print(f\"final validation dual weighted accuracy: {final_val_dwa:.4f}\")\n                print(\n                    f\"best (highest) validation dual weighted accuracy: {best_val_dwa:.4f}\"\n                )\n", "import os\nimport numpy as np\n\n# ---------- locate & load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate & report ----------\nfor search_space, runs in experiment_data.items():  # e.g., \"pooling_type\"\n    for run_name, datasets in runs.items():  # e.g., \"mean\", \"max\"...\n        for dataset_name, log in datasets.items():  # e.g., \"SPR_BENCH\"\n            print(f\"\\nDataset: {dataset_name}  ( {search_space} = {run_name} )\")\n\n            # losses\n            train_losses = log[\"losses\"][\"train\"]\n            val_losses = log[\"losses\"][\"val\"]\n\n            final_train_loss = train_losses[-1][1] if train_losses else None\n            final_val_loss = val_losses[-1][1] if val_losses else None\n            best_val_loss = min(v[1] for v in val_losses) if val_losses else None\n\n            # metrics (dual-weighted accuracy stored in validation list)\n            val_metrics = log[\"metrics\"][\"val\"]\n            final_val_dwa = val_metrics[-1][1] if val_metrics else None\n            best_val_dwa = max(v[1] for v in val_metrics) if val_metrics else None\n\n            # ----- printing -----\n            if final_train_loss is not None:\n                print(f\"final training loss: {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"final validation loss: {final_val_loss:.4f}\")\n                print(f\"best (lowest) validation loss: {best_val_loss:.4f}\")\n            if final_val_dwa is not None:\n                print(f\"final validation dual weighted accuracy: {final_val_dwa:.4f}\")\n                print(\n                    f\"best (highest) validation dual weighted accuracy: {best_val_dwa:.4f}\"\n                )\n", "import os\nimport numpy as np\n\n# ---------- locate & load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate & report ----------\nfor search_space, runs in experiment_data.items():  # e.g., \"pooling_type\"\n    for run_name, datasets in runs.items():  # e.g., \"mean\", \"max\"...\n        for dataset_name, log in datasets.items():  # e.g., \"SPR_BENCH\"\n            print(f\"\\nDataset: {dataset_name}  ( {search_space} = {run_name} )\")\n\n            # losses\n            train_losses = log[\"losses\"][\"train\"]\n            val_losses = log[\"losses\"][\"val\"]\n\n            final_train_loss = train_losses[-1][1] if train_losses else None\n            final_val_loss = val_losses[-1][1] if val_losses else None\n            best_val_loss = min(v[1] for v in val_losses) if val_losses else None\n\n            # metrics (dual-weighted accuracy stored in validation list)\n            val_metrics = log[\"metrics\"][\"val\"]\n            final_val_dwa = val_metrics[-1][1] if val_metrics else None\n            best_val_dwa = max(v[1] for v in val_metrics) if val_metrics else None\n\n            # ----- printing -----\n            if final_train_loss is not None:\n                print(f\"final training loss: {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"final validation loss: {final_val_loss:.4f}\")\n                print(f\"best (lowest) validation loss: {best_val_loss:.4f}\")\n            if final_val_dwa is not None:\n                print(f\"final validation dual weighted accuracy: {final_val_dwa:.4f}\")\n                print(\n                    f\"best (highest) validation dual weighted accuracy: {best_val_dwa:.4f}\"\n                )\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'final training loss: 0.2564', '\\n', 'final validation loss:\n0.2475', '\\n', 'final dual weighted accuracy (validation): 0.9160', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH (configuration: 10 epochs)', '\\n', 'final training loss:\n0.195044', '\\n', 'final validation loss: 0.194936', '\\n', 'best validation dual\nweighted accuracy: 0.939490', '\\n', 'best epoch: 9', '\\n', '\\n', 'Dataset:\nSPR_BENCH (configuration: 20 epochs)', '\\n', 'final training loss: 0.220119',\n'\\n', 'final validation loss: 0.261776', '\\n', 'best validation dual weighted\naccuracy: 0.920440', '\\n', 'best epoch: 5', '\\n', '\\n', 'Dataset: SPR_BENCH\n(configuration: 30 epochs)', '\\n', 'final training loss: 0.176026', '\\n', 'final\nvalidation loss: 0.191965', '\\n', 'best validation dual weighted accuracy:\n0.945478', '\\n', 'best epoch: 12', '\\n', '\\n', 'Dataset: SPR_BENCH\n(configuration: 50 epochs)', '\\n', 'final training loss: 0.174817', '\\n', 'final\nvalidation loss: 0.189629', '\\n', 'best validation dual weighted accuracy:\n0.947407', '\\n', 'best epoch: 13', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Hyper-parameters: lr=0.0003', '\\n', '    best\ntraining loss: 0.3518', '\\n', '    best validation loss: 0.3518', '\\n', '\nbest validation dual weighted accuracy: 0.8649', '\\n', '  Hyper-parameters:\nlr=0.0001', '\\n', '    best training loss: 0.4200', '\\n', '    best validation\nloss: 0.4181', '\\n', '    best validation dual weighted accuracy: 0.8211', '\\n',\n'  Hyper-parameters: lr=0.003', '\\n', '    best training loss: 0.2193', '\\n', '\nbest validation loss: 0.1997', '\\n', '    best validation dual weighted\naccuracy: 0.9419', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', '  batch size 16', '\\n', '    final training loss: 0.2199',\n'\\n', '    best validation loss: 0.2184', '\\n', '    best validation dual\nweighted accuracy: 0.9279', '\\n', '  batch size 32', '\\n', '    final training\nloss: 0.2493', '\\n', '    best validation loss: 0.2414', '\\n', '    best\nvalidation dual weighted accuracy: 0.9213', '\\n', '  batch size 64', '\\n', '\nfinal training loss: 0.2773', '\\n', '    best validation loss: 0.2726', '\\n', '\nbest validation dual weighted accuracy: 0.9148', '\\n', '  batch size 128', '\\n',\n'    final training loss: 0.3028', '\\n', '    best validation loss: 0.3006',\n'\\n', '    best validation dual weighted accuracy: 0.8911', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  wd_0.0', '\\n', '    best train loss: 0.2684', '\\n', '\nbest validation loss: 0.2570', '\\n', '    best validation dual weighted\naccuracy: 0.9179', '\\n', '  wd_1e-06', '\\n', '    best train loss: 0.2602',\n'\\n', '    best validation loss: 0.2559', '\\n', '    best validation dual\nweighted accuracy: 0.9130', '\\n', '  wd_1e-05', '\\n', '    best train loss:\n0.2700', '\\n', '    best validation loss: 0.2659', '\\n', '    best validation\ndual weighted accuracy: 0.9050', '\\n', '  wd_0.0001', '\\n', '    best train\nloss: 0.2905', '\\n', '    best validation loss: 0.2805', '\\n', '    best\nvalidation dual weighted accuracy: 0.9062', '\\n', '  wd_0.001', '\\n', '    best\ntrain loss: 0.3262', '\\n', '    best validation loss: 0.3340', '\\n', '    best\nvalidation dual weighted accuracy: 0.8848', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Experiment: dim_32', '\\n', '    training loss:\n0.3001', '\\n', '    validation loss: 0.2922', '\\n', '    dual weighted accuracy:\n0.8971', '\\n', '  Experiment: dim_64', '\\n', '    training loss: 0.2666', '\\n',\n'    validation loss: 0.2693', '\\n', '    dual weighted accuracy: 0.8959', '\\n',\n'  Experiment: dim_128', '\\n', '    training loss: 0.2420', '\\n', '\nvalidation loss: 0.2456', '\\n', '    dual weighted accuracy: 0.9178', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best dropout rate: 0.0', '\\n', 'training loss: 0.256032',\n'\\n', 'validation loss: 0.246976', '\\n', 'validation dual weighted accuracy:\n0.915333', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  ( pooling_type = mean )', '\\n', 'final training loss:\n0.2539', '\\n', 'final validation loss: 0.2450', '\\n', 'best (lowest) validation\nloss: 0.2450', '\\n', 'final validation dual weighted accuracy: 0.9180', '\\n',\n'best (highest) validation dual weighted accuracy: 0.9180', '\\n', '\\nDataset:\nSPR_BENCH  ( pooling_type = max )', '\\n', 'final training loss: 0.1267', '\\n',\n'final validation loss: 0.1176', '\\n', 'best (lowest) validation loss: 0.1176',\n'\\n', 'final validation dual weighted accuracy: 0.9682', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9682', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = add )', '\\n', 'final training loss: 0.2489', '\\n', 'final\nvalidation loss: 0.2376', '\\n', 'best (lowest) validation loss: 0.2376', '\\n',\n'final validation dual weighted accuracy: 0.9234', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9234', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = attn )', '\\n', 'final training loss: 0.1883', '\\n', 'final\nvalidation loss: 0.1745', '\\n', 'best (lowest) validation loss: 0.1745', '\\n',\n'final validation dual weighted accuracy: 0.9445', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9445', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  configuration (hidden_dim=64)', '\\n', '    final training\nloss: 0.306768', '\\n', '    final validation loss: 0.300321', '\\n', '    final\nvalidation dual weighted accuracy: 0.890408', '\\n', '    best validation dual\nweighted accuracy: 0.890408', '\\n', '  configuration (hidden_dim=128)', '\\n', '\nfinal training loss: 0.271951', '\\n', '    final validation loss: 0.274997',\n'\\n', '    final validation dual weighted accuracy: 0.909787', '\\n', '    best\nvalidation dual weighted accuracy: 0.909787', '\\n', '  configuration\n(hidden_dim=256)', '\\n', '    final training loss: 0.235760', '\\n', '    final\nvalidation loss: 0.223457', '\\n', '    final validation dual weighted accuracy:\n0.922419', '\\n', '    best validation dual weighted accuracy: 0.922419', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  ( pooling_type = mean )', '\\n', 'final training loss:\n0.2719', '\\n', 'final validation loss: 0.2567', '\\n', 'best (lowest) validation\nloss: 0.2567', '\\n', 'final validation dual weighted accuracy: 0.9136', '\\n',\n'best (highest) validation dual weighted accuracy: 0.9136', '\\n', '\\nDataset:\nSPR_BENCH  ( pooling_type = max )', '\\n', 'final training loss: 0.1237', '\\n',\n'final validation loss: 0.1105', '\\n', 'best (lowest) validation loss: 0.1105',\n'\\n', 'final validation dual weighted accuracy: 0.9683', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9683', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = add )', '\\n', 'final training loss: 0.2480', '\\n', 'final\nvalidation loss: 0.2397', '\\n', 'best (lowest) validation loss: 0.2397', '\\n',\n'final validation dual weighted accuracy: 0.9204', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9204', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = attn )', '\\n', 'final training loss: 0.1827', '\\n', 'final\nvalidation loss: 0.1636', '\\n', 'best (lowest) validation loss: 0.1636', '\\n',\n'final validation dual weighted accuracy: 0.9448', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9448', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  ( pooling_type = mean )', '\\n', 'final training loss:\n0.2628', '\\n', 'final validation loss: 0.2659', '\\n', 'best (lowest) validation\nloss: 0.2659', '\\n', 'final validation dual weighted accuracy: 0.9148', '\\n',\n'best (highest) validation dual weighted accuracy: 0.9148', '\\n', '\\nDataset:\nSPR_BENCH  ( pooling_type = max )', '\\n', 'final training loss: 0.1174', '\\n',\n'final validation loss: 0.1178', '\\n', 'best (lowest) validation loss: 0.1178',\n'\\n', 'final validation dual weighted accuracy: 0.9650', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9650', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = add )', '\\n', 'final training loss: 0.2470', '\\n', 'final\nvalidation loss: 0.3044', '\\n', 'best (lowest) validation loss: 0.2638', '\\n',\n'final validation dual weighted accuracy: 0.9045', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9102', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = attn )', '\\n', 'final training loss: 0.1919', '\\n', 'final\nvalidation loss: 0.1954', '\\n', 'best (lowest) validation loss: 0.1922', '\\n',\n'final validation dual weighted accuracy: 0.9347', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9347', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  ( pooling_type = mean )', '\\n', 'final training loss:\n0.2748', '\\n', 'final validation loss: 0.2695', '\\n', 'best (lowest) validation\nloss: 0.2695', '\\n', 'final validation dual weighted accuracy: 0.9125', '\\n',\n'best (highest) validation dual weighted accuracy: 0.9125', '\\n', '\\nDataset:\nSPR_BENCH  ( pooling_type = max )', '\\n', 'final training loss: 0.1124', '\\n',\n'final validation loss: 0.1043', '\\n', 'best (lowest) validation loss: 0.1043',\n'\\n', 'final validation dual weighted accuracy: 0.9713', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9713', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = add )', '\\n', 'final training loss: 0.2422', '\\n', 'final\nvalidation loss: 0.2358', '\\n', 'best (lowest) validation loss: 0.2358', '\\n',\n'final validation dual weighted accuracy: 0.9251', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9251', '\\n', '\\nDataset: SPR_BENCH  (\npooling_type = attn )', '\\n', 'final training loss: 0.1847', '\\n', 'final\nvalidation loss: 0.1774', '\\n', 'best (lowest) validation loss: 0.1774', '\\n',\n'final validation dual weighted accuracy: 0.9438', '\\n', 'best (highest)\nvalidation dual weighted accuracy: 0.9438', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}