
% Paper 0 ('How Powerful are Graph Neural Networks?') provides a theoretical framework to analyze the expressive power of GNNs, which is crucial to justify their use in capturing relational dependencies in the SPR task. Paper 1 ('A Comprehensive Survey on Graph Neural Networks') surveys GNN architectures and applications, offering a broader context and situating the proposed approach within the state-of-the-art. These citations are foundational and should be included in the introduction and related work sections.
@article{xu2018howpa,
 author = {Keyulu Xu and Weihua Hu and J. Leskovec and S. Jegelka},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {How Powerful are Graph Neural Networks?},
 volume = {abs/1810.00826},
 year = {2018}
}

@article{wu2019acs,
 author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
 booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 pages = {4-24},
 title = {A Comprehensive Survey on Graph Neural Networks},
 volume = {32},
 year = {2019}
}

% The paper 'A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts' introduces rsbench, a synthetic benchmark suite for evaluating reasoning tasks affected by reasoning shortcuts. This work provides a broader context for benchmarks in reasoning tasks and situates the SPR_BENCH dataset within the landscape of synthetic benchmarks. It should be cited in the related work section when discussing datasets and benchmarks for reasoning tasks.
@article{bortolotti2024anb,
 author = {Samuele Bortolotti and Emanuele Marconato and Tommaso Carraro and Paolo Morettin and Emile van Krieken and Antonio Vergari and Stefano Teso and Andrea Passerini},
 booktitle = {Neural Information Processing Systems},
 title = {A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts},
 year = {2024}
}

% Paper 1 ('Graph Neural Networks with Haar Transform-Based Convolution and Pooling: A Complete Guide') provides a comprehensive overview of pooling strategies used in GNNs and validates their impact on graph-level applications, which supports the methodological choices in the SPR task. Paper 4 ('Pooling Strategies for Simplicial Convolutional Networks') introduces and evaluates multiple pooling strategies (e.g., max, top-k, self-attention) in hierarchical architectures, making it highly relevant for discussing the pooling methods explored in the proposed approach. These citations should be integrated into the related work and methodology sections to highlight the role of pooling strategies in GNN performance.
@article{zheng2020graphnn,
 author = {Xuebin Zheng and Bingxin Zhou and Ming Li and Yu Guang Wang and Junbin Gao},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Graph Neural Networks with Haar Transform-Based Convolution and Pooling: A Complete Guide},
 volume = {abs/2007.11202},
 year = {2020}
}

@article{cinque2022poolingsf,
 author = {Domenico Mattia Cinque and Claudio Battiloro and P. Lorenzo},
 booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
 journal = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 pages = {1-5},
 title = {Pooling Strategies for Simplicial Convolutional Networks},
 year = {2022}
}

% Paper 0 ('Evaluating Step-by-Step Reasoning through Symbolic Verification') curates synthetic datasets with symbolic and natural data pairs, focusing on reasoning with logic rules. This paper provides context for synthetic reasoning datasets and is relevant for situating SPR_BENCH within the broader field of reasoning benchmarks. Paper 3 ('VCR: A Cone of Experience Driven Synthetic Data Generation Framework for Mathematical Reasoning') introduces a novel framework for generating high-quality synthetic data for mathematical reasoning, emphasizing diversity and quality in synthetic datasets. This is relevant to methodologies for constructing synthetic datasets like SPR_BENCH and should be cited in the related work discussion on datasets and data generation methods.
@article{zhang2022evaluatingsr,
 author = {Hanlin Zhang and Yi-Fan Zhang and Li Erran Li and Eric P. Xing},
 booktitle = {NAACL-HLT},
 pages = {2984-3002},
 title = {Evaluating Step-by-Step Reasoning through Symbolic Verification},
 year = {2022}
}

@article{liu2025vcra,
 author = {Sannyuya Liu and Jintian Feng and Xiaoxuan Shen and Shengyingjie Liu and Qian Wan and Jianwen Sun},
 booktitle = {AAAI Conference on Artificial Intelligence},
 pages = {24650-24658},
 title = {VCR: A "Cone of Experience" Driven Synthetic Data Generation Framework for Mathematical Reasoning},
 year = {2025}
}

% The paper 'Relational Attention: Generalizing Transformers for Graph-Structured Tasks' evaluates the performance of relational transformers against graph neural networks (GNNs) on graph-structured tasks, demonstrating the advantages of relational attention mechanisms. This citation is relevant for discussing the comparative strengths of GNNs and Transformers in capturing relational dependencies within symbolic data and should be included in the related work section.
@article{diao2022relationalag,
 author = {Cameron Diao and Ricky Loynd},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Relational Attention: Generalizing Transformers for Graph-Structured Tasks},
 volume = {abs/2210.05062},
 year = {2022}
}

% The paper 'CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph Neural Network Training with Communication Reduction' presents methods to enhance efficiency in training GNNs by reducing communication overhead and computational complexity through adaptive caching mechanisms. This citation is relevant for the methodology section when discussing computational efficiency and optimization strategies for GNN training in the SPR task.
@article{zhang2024cdfgnnas,
 author = {Shuai Zhang and Zite Jiang and Haihang You},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph Neural Network Training with Communication Reduction},
 volume = {abs/2408.00232},
 year = {2024}
}
