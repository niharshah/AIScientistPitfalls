{"edges": [[0, 5], [0, 12], [0, 11], [0, 2], [0, 4], [0, 8], [0, 9], [0, 1], [0, 10], [0, 3], [4, 7], [4, 6]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.1111111111111111, 0.5], [0.2222222222222222, 0.5], [0.3333333333333333, 0.5], [0.4444444444444444, 0.5], [0.2777777777777778, 1.0], [0.3888888888888889, 1.0], [0.5555555555555556, 0.5], [0.6666666666666666, 0.5], [0.7777777777777778, 0.5], [0.8888888888888888, 0.5], [1.0, 0.5]], "plan": ["We will treat each SPR sequence as a graph whose nodes are the symbols (token-\nstrings).  Each node is described by a one-hot encoding of its \u201cshape\u201d (first\ncharacter) and \u201ccolor\u201d (remaining characters).  Undirected edges are added\nbetween consecutive tokens, giving minimal yet meaningful relational structure.\nWe pre-build dictionaries of all shapes and colours found in the training split\nso the same encodings are reused on dev/test.  Graphs are wrapped in torch-\ngeometric Data objects and fed to a tiny 2-layer GCN with global-mean pooling\nfollowed by a linear classifier.  During training we track Cross-Entropy loss,\nplain accuracy and the requested Complexity-Weighted Accuracy (CompWA = weight-\nsum(correct)/(weight-sum(all))).  All tensors and the model are moved to GPU\nwhen available.  At the end we evaluate on the dev set, plot the learning\ncurves, save metrics/predictions in \u201c./working\u201d, and print the final CompWA.", "We enrich the baseline by (1) treating each sequence as a graph that contains\nthree kinds of relations: token-order (i\u2194i+1), same-shape clique and same-color\nclique; (2) encoding every node with one-hot shape, one-hot color and a\nnormalised positional scalar; (3) using a 3-layer GCN with dropout and weight-\ndecay to fight over-fitting.  At every epoch we evaluate on the dev split and\nlog cross-entropy, plain accuracy, Color-Weighted Accuracy, Shape-Weighted\nAccuracy and the corrected Complexity-Weighted Accuracy (|shapes|\u00d7|colors|).\nAll metrics are stored in experiment_data and saved to ./working for later\nanalysis.  Finally the trained model is evaluated on the held-out test split and\nthe three weighted accuracies are printed.  The code below is self-contained,\nhandles GPU/CPU correctly, and falls back to a small synthetic dataset if\nSPR_BENCH is absent.", "To combat over-fitting and make better use of the symbolic structure, we extend\nthe previous GCN in three ways:   1) build richer graphs that include not only\nsequential edges but also \u201csame-shape\u201d and \u201csame-color\u201d edges, allowing direct\nmessage passing among related tokens;   2) switch to GraphSAGE layers with\ndropout and layer-norm which typically generalise better;   3) track and print\nthe required Complexity-Weighted Accuracy (CXA = #colors \u00d7 #shapes) every epoch\nin addition to the loss.   The code below constructs graphs on-the-fly, trains\nfor a few epochs (so it finishes quickly), logs loss, plain accuracy and CXA for\ntrain/val, then evaluates CWA, SWA and CXA on the test set. The whole run is\nself-contained, GPU-aware, and stores curves and raw data under ./working.", "We strengthen the previous prototype by (1) enriching the graph: besides order-\nedges we now connect every pair of tokens that share the same color or the same\nshape, letting the GNN explicitly reason over these latent relations; (2) adding\na positional scalar feature to each node; (3) switching to a 3-layer GCN with\nReLU + Dropout to mitigate over-fitting; (4) tracking and storing every required\nmetric\u2014including the corrected Complexity-Weighted Accuracy CXA that multiplies\nthe numbers of distinct colors and shapes\u2014at every epoch; (5) cleaning metric\ncomputation so that sequences, labels and predictions are aligned; and (6)\nfixing data-parallel issues and obeying all device-handling rules.  Ten epochs\nare run and all intermediate artefacts (loss curves, metric arrays) are saved\ninto ./working for later analysis.", "To mitigate the over-fitting and class-bias observed previously, we enrich each\nsequence graph with three explicit relation types (next-position, same-shape,\nsame-color) and train a two-layer Relational GCN that includes dropout and\nweight-decay regularisation. Per-graph we pre-compute its complexity weight w =\n(#shapes)\u00d7(#colors) and store it in the Data object so that Complexity-Weighted\nAccuracy (CXA) can be tracked every epoch together with the usual loss/accuracy.\nClass imbalance is countered by using inverse-frequency class weights in the\ncross-entropy loss. The training/validation loops now accumulate plain accuracy\nand CXA, print validation loss each epoch and save all metrics in\nexperiment_data; over-fitting can be monitored from the saved curves. Finally,\nthe script evaluates CWA, SWA and CXA on the test split and stores predictions,\nenabling later error analysis.", "We switch to a richer graph that links tokens not only by position but also by\nshared-color and shared-shape edges, allowing the GNN to propagate rule signals\nalong these latent relations.   A 3-layer GraphSAGE with LayerNorm and dropout\nis trained, and at each epoch we compute and store Loss, plain Accuracy, Color-\nWeighted Accuracy (CWA), Shape-Weighted Accuracy (SWA) and the corrected\nComplexity-Weighted Accuracy (CXA = #distinct-colors \u00d7 #distinct-shapes).\nEarly stopping (patience = 5 on CXA) avoids over-fitting.   All tensors, model\nand batches are placed on GPU when available, and every metric history,\nprediction list and loss curve is saved to the ./working folder.   The script\nworks even when the official SPR_BENCH folder is missing by generating a small\nsynthetic fallback dataset so it can always run end-to-end inside 30 min.", "The original script stops when it cannot locate the SPR_BENCH folder and, during\ntest-time evaluation, pairs each batch with only one row from the raw\nHuggingFace dataset, so \u224896 % of the test examples are silently skipped.   The\nfix therefore:   1. Look for the dataset in several common places (env-var\nSPR_BENCH, current/parent folder, original absolute path) and abort only if\nevery candidate fails.   2. Replace the zip-based test loop with an index\ntracker so that every prediction is matched with its correct raw sequence/label.\nEverything else (device handling, metric tracking, saving) is kept unchanged and\nstill satisfies the execution-time guidelines.", "The script aborted because it looked for the dataset only in \u201c./SPR_BENCH\u201d and\nexited when it was missing; additionally, the final evaluation zipped whole-\nbatch DataLoader objects with item-wise raw examples, producing incomplete test\nmetrics.   The fix tries several plausible locations (environment variable, cwd,\nauthor\u2019s absolute path) before failing, and removes the early exit.   At test\ntime (and during CXA calculation each epoch) predictions are now aligned with\nevery graph by advancing an index counter instead of zipping batches with\nindividual rows.   Minor improvements include printing / logging CXA each epoch,\nstoring all metrics in the required experiment_data dict, and slightly\nrefactoring the Data object to keep its complexity weight.", "We will enhance the previous baseline by \u2460 enriching every graph with\ncolour/shape-homophily edges and a positional feature, \u2461 switching to a two-\nlayer Graph-Attention Network (GAT) that better exploits those richer relations,\nand \u2462 tracking the required Color-, Shape- and Complexity-Weighted Accuracies at\nevery epoch. Five fast epochs are run for a first-pass comparison; the script\nfalls back to a tiny synthetic dataset if SPR_BENCH is absent, so it is fully\nself-contained. All metrics, losses and predictions are stored in\n./working/experiment_data.npy for later analysis, and a loss curve is plotted.\nThe code follows every GPU, logging and saving requirement stated.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\n# ------------- required working dir -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------- GPU/CPU handling -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------- helper: load SPR_BENCH -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if necessary\nif not DATA_PATH.exists():  # fallback tiny synthetic data\n    print(\"SPR_BENCH not found \u2013 creating tiny synthetic data.\")\n    os.makedirs(DATA_PATH, exist_ok=True)\n    for split, s in [(\"train\", 200), (\"dev\", 40), (\"test\", 40)]:\n        seqs, labels = [], []\n        shapes = [\"A\", \"B\", \"C\"]\n        colors = [\"1\", \"2\", \"3\"]\n        rng = np.random.default_rng(0)\n        for _ in range(s):\n            n = rng.integers(3, 7)\n            seq = \" \".join(rng.choice(shapes) + rng.choice(colors) for _ in range(n))\n            label = rng.choice([\"yes\", \"no\"])\n            seqs.append(seq)\n            labels.append(label)\n        import csv\n\n        with open(DATA_PATH / f\"{split}.csv\", \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i, (seq, lbl) in enumerate(zip(seqs, labels)):\n                w.writerow([f\"{split}_{i}\", seq, lbl])\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ------------- preprocessing: build vocab -------------\ndef parse_token(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshapes, colours = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colours.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncol2id = {c: i for i, c in enumerate(sorted(colours))}\nprint(\"Shapes:\", shape2id)\nprint(\"Colours:\", col2id)\n\n# label mapping\nall_labels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(all_labels)}\n\n\n# ------------- sequence -> graph -------------\ndef seq_to_graph(sequence, lbl):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = []\n    for tok in tokens:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(col2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + col2id[c]] = 1.0\n        x.append(vec)\n    x = torch.tensor(np.stack(x))\n    # edges between consecutive positions (undirected)\n    if n > 1:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2id[lbl]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef build_graph_dataset(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\n# ------------- Dataloaders -------------\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n# ------------- model -------------\nclass GCN(torch.nn.Module):\n    def __init__(self, in_dim, hid=64, num_classes=len(label2id)):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, hid)\n        self.conv2 = GCNConv(hid, hid)\n        self.lin = torch.nn.Linear(hid, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GCN(in_dim=len(shape2id) + len(col2id)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------- Complexity Weighted Accuracy -------------\ndef complexity_weight(seq):\n    toks = seq.split()\n    shapes = {t[0] for t in toks}\n    cols = {t[1:] if len(t) > 1 else \"0\" for t in toks}\n    return len(shapes) + len(cols)\n\n\ndef comp_weighted_accuracy(seqs, y_true, y_pred):\n    w = [complexity_weight(s) for s in seqs]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------- tracking dict -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------- training loop -------------\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tot_loss, tot_corr, tot_ex = 0.0, 0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        pred = out.argmax(dim=-1)\n        tot_corr += int((pred == batch.y).sum().item())\n        tot_ex += batch.num_graphs\n    tr_loss = tot_loss / tot_ex\n    tr_acc = tot_corr / tot_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_acc)\n\n    # ---- validation ----\n    model.eval()\n    v_loss, v_corr, v_ex = 0.0, 0, 0\n    all_pred, all_gt, all_seq = [], [], []\n    with torch.no_grad():\n        for batch, raw in zip(dev_loader, dsets[\"dev\"]):\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            pred = out.argmax(dim=-1).cpu()\n            v_corr += int((pred == batch.y.cpu()).sum().item())\n            v_ex += batch.num_graphs\n            all_pred.extend(pred.tolist())\n            all_gt.extend(batch.y.cpu().tolist())\n            all_seq.append(raw[\"sequence\"])\n    val_loss = v_loss / v_ex\n    val_acc = v_corr / v_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# ------------- final evaluation on dev for CompWA -------------\nseqs = [row[\"sequence\"] for row in dsets[\"dev\"]]\nmodel.eval()\npreds = []\nwith torch.no_grad():\n    for batch in dev_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.batch)\n        preds.extend(out.argmax(dim=-1).cpu().tolist())\ncompwa = comp_weighted_accuracy(\n    seqs, [label2id[r[\"label\"]] for r in dsets[\"dev\"]], preds\n)\nprint(f\"Complexity-Weighted Accuracy (dev): {compwa:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = [\n    label2id[r[\"label\"]] for r in dsets[\"dev\"]\n]\n\n# ------------- plot and save -------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Data & plot saved to ./working\")\n", "import os, pathlib, time, itertools, numpy as np, torch, torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\n# ------------------------------------------------------------------\n# compulsory working dir & device setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# data helpers (taken from original utility, extended)\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len({tok[1:] if len(tok) > 1 else \"0\" for tok in sequence.split()})\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len({tok[0] for tok in sequence.split()})\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef complexity_weight(sequence: str) -> int:\n    return count_shape_variety(sequence) * count_color_variety(sequence)\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [complexity_weight(s) for s in seqs]\n    corr = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# ------------------------------------------------------------------\n# try to load real benchmark else make tiny synthetic one\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nif not DATA_PATH.exists():\n    print(\"SPR_BENCH folder not found, creating tiny synthetic data for demo.\")\n    DATA_PATH.mkdir(exist_ok=True)\n    rng = np.random.default_rng(0)\n    shapes, colors = list(\"ABC\"), [\"1\", \"2\", \"3\"]\n    for split, size in [(\"train\", 300), (\"dev\", 60), (\"test\", 60)]:\n        with open(DATA_PATH / f\"{split}.csv\", \"w\") as f:\n            f.write(\"id,sequence,label\\n\")\n            for i in range(size):\n                ln = rng.integers(3, 8)\n                seq = \" \".join(\n                    rng.choice(shapes) + rng.choice(colors) for _ in range(ln)\n                )\n                lbl = rng.choice([\"yes\", \"no\"])\n                f.write(f\"{split}_{i},{seq},{lbl}\\n\")\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ------------------------------------------------------------------\n# build vocabularies\ndef parse_token(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshape_set, color_set = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shape_set.add(s)\n        color_set.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shape_set))}\ncolor2id = {c: i for i, c in enumerate(sorted(color_set))}\nlabel2id = {l: i for i, l in enumerate(sorted({r[\"label\"] for r in dsets[\"train\"]}))}\nprint(\"Shapes\", shape2id)\nprint(\"Colors\", color2id)\n\nin_dim = len(shape2id) + len(color2id) + 1  # +1 for position\n\n\n# ------------------------------------------------------------------\n# graph construction with richer edges\ndef seq_to_graph(sequence: str, lbl: str):\n    toks = sequence.split()\n    n = len(toks)\n    # node features\n    x = np.zeros((n, in_dim), dtype=np.float32)\n    for idx, tok in enumerate(toks):\n        s, c = parse_token(tok)\n        x[idx, shape2id[s]] = 1.0\n        x[idx, len(shape2id) + color2id[c]] = 1.0\n        x[idx, -1] = idx / max(1, n - 1)  # normalised position\n    # build edges: order + same-shape + same-color\n    edges = set()\n    # order\n    for i in range(n - 1):\n        edges.add((i, i + 1))\n        edges.add((i + 1, i))\n    # shape clique\n    for shp in shape2id:\n        idxs = [i for i, tok in enumerate(toks) if parse_token(tok)[0] == shp]\n        for i, j in itertools.permutations(idxs, 2):\n            edges.add((i, j))\n    # color clique\n    for col in color2id:\n        idxs = [i for i, tok in enumerate(toks) if parse_token(tok)[1] == col]\n        for i, j in itertools.permutations(idxs, 2):\n            edges.add((i, j))\n    if len(edges) == 0:  # single node\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    else:\n        edge_index = torch.tensor(list(zip(*edges)), dtype=torch.long)\n    data = Data(\n        x=torch.tensor(x),\n        edge_index=edge_index,\n        y=torch.tensor([label2id[lbl]], dtype=torch.long),\n        seq=sequence,\n    )\n    return data\n\n\ndef build_graph_list(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_list(\"train\")\ngraph_dev = build_graph_list(\"dev\")\ngraph_test = build_graph_list(\"test\")\n\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n# ------------------------------------------------------------------\n# GCN model\nclass SPR_GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, 96)\n        self.conv2 = GCNConv(96, 96)\n        self.conv3 = GCNConv(96, 96)\n        self.lin = torch.nn.Linear(96, len(label2id))\n        self.dropout = torch.nn.Dropout(0.25)\n\n    def forward(self, x, edge_index, batch):\n        x = self.dropout(self.conv1(x, edge_index).relu())\n        x = self.dropout(self.conv2(x, edge_index).relu())\n        x = self.conv3(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = SPR_GCN().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=1e-4)\n\n# ------------------------------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\ndef evaluate(loader, split_name):\n    model.eval()\n    total_loss, correct, n_graphs = 0.0, 0, 0\n    preds, gts, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            total_loss += loss.item() * batch.num_graphs\n            pred = out.argmax(dim=-1)\n            correct += int((pred == batch.y).sum().item())\n            n_graphs += batch.num_graphs\n            preds.extend(pred.cpu().tolist())\n            gts.extend(batch.y.cpu().tolist())\n            seqs.extend(batch.seq)  # plain python list\n    loss = total_loss / n_graphs\n    acc = correct / n_graphs\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    cxa = complexity_weighted_accuracy(seqs, gts, preds)\n    print(\n        f\"  {split_name}: loss={loss:.4f} acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CXA={cxa:.3f}\"\n    )\n    return loss, acc, cwa, swa, cxa, preds, gts, seqs\n\n\n# ------------------------------------------------------------------\nEPOCHS = 15\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    t0 = time.time()\n    tot_loss, tot_corr, tot_ex = 0.0, 0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        tot_corr += int((out.argmax(dim=-1) == batch.y).sum().item())\n        tot_ex += batch.num_graphs\n    train_loss = tot_loss / tot_ex\n    train_acc = tot_corr / tot_ex\n    # dev evaluation\n    val_loss, val_acc, val_cwa, val_swa, val_cxa, _, _, _ = evaluate(dev_loader, \"dev\")\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} (time {time.time()-t0:.1f}s)\"\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n\n# ------------------------------------------------------------------\nprint(\"\\n==== Final evaluation on test split ====\")\ntest_loss, test_acc, test_cwa, test_swa, test_cxa, preds, gts, seqs = evaluate(\n    test_loader, \"test\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All metrics saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, numpy as np, torch, torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool, LayerNorm\n\n# ---------------- working dir ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- dataset loader ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nif not DATA_PATH.exists():\n    # tiny synthetic back-up so script is runnable\n    print(\"SPR_BENCH not found \u2013 writing tiny synthetic data.\")\n    DATA_PATH.mkdir(exist_ok=True)\n    rng = np.random.default_rng(0)\n    for split, n in [(\"train\", 300), (\"dev\", 60), (\"test\", 60)]:\n        with open(DATA_PATH / f\"{split}.csv\", \"w\") as f:\n            f.write(\"id,sequence,label\\n\")\n            for i in range(n):\n                L = rng.integers(3, 8)\n                seq = \" \".join(\n                    rng.choice(list(\"ABC\")) + str(rng.integers(1, 4)) for _ in range(L)\n                )\n                label = rng.choice([\"yes\", \"no\"])\n                f.write(f\"{split}_{i},{seq},{label}\\n\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ---------------- vocab ----------------\ndef token_parts(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshapes, colors = set(), set()\nfor ex in dsets[\"train\"]:\n    for t in ex[\"sequence\"].split():\n        s, c = token_parts(t)\n        shapes.add(s)\n        colors.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncolor2id = {c: i for i, c in enumerate(sorted(colors))}\nlabel2id = {l: i for i, l in enumerate(sorted({ex[\"label\"] for ex in dsets[\"train\"]}))}\n\n\n# ---------------- utils for metrics ----------------\ndef count_color_variety(seq):\n    return len({t[1:] if len(t) > 1 else \"0\" for t in seq.split()})\n\n\ndef count_shape_variety(seq):\n    return len({t[0] for t in seq.split()})\n\n\ndef complexity_weight(seq):\n    return count_color_variety(seq) * count_shape_variety(seq)\n\n\ndef weighted_accuracy(weights, ytrue, ypred):\n    corr = [w if a == b else 0 for w, a, b in zip(weights, ytrue, ypred)]\n    return sum(corr) / sum(weights) if weights else 0.0\n\n\n# ---------------- graph construction ----------------\ndef sequence_to_graph(sequence, label):\n    toks = sequence.split()\n    n = len(toks)\n    # node features\n    feats = []\n    for t in toks:\n        s, c = token_parts(t)\n        v = np.zeros(len(shape2id) + len(color2id), dtype=np.float32)\n        v[shape2id[s]] = 1.0\n        v[len(shape2id) + color2id[c]] = 1.0\n        feats.append(v)\n    x = torch.tensor(np.stack(feats))\n    # edges: sequential + same shape + same color\n    src, dst = [], []\n    # sequential\n    for i in range(n - 1):\n        src += [i, i + 1]\n        dst += [i + 1, i]\n    # same shape/color\n    by_shape, by_color = {}, {}\n    for i, t in enumerate(toks):\n        s, c = token_parts(t)\n        by_shape.setdefault(s, []).append(i)\n        by_color.setdefault(c, []).append(i)\n    for group in list(by_shape.values()) + list(by_color.values()):\n        for i in range(len(group)):\n            for j in range(i + 1, len(group)):\n                src += [group[i], group[j]]\n                dst += [group[j], group[i]]\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    y = torch.tensor([label2id[label]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y, seq=sequence)\n\n\ndef build_graph_dataset(split):\n    return [sequence_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ntrain_graphs = build_graph_dataset(\"train\")\ndev_graphs = build_graph_dataset(\"dev\")\ntest_graphs = build_graph_dataset(\"test\")\n\ntrain_loader = DataLoader(train_graphs, batch_size=128, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=256, shuffle=False)\n\n\n# ---------------- model ----------------\nclass GraphModel(torch.nn.Module):\n    def __init__(self, in_dim, hid=64, num_classes=2):\n        super().__init__()\n        self.conv1 = SAGEConv(in_dim, hid)\n        self.ln1 = LayerNorm(hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.ln2 = LayerNorm(hid)\n        self.lin = torch.nn.Linear(hid, num_classes)\n        self.drop = torch.nn.Dropout(0.3)\n\n    def forward(self, x, edge_index, batch):\n        x = self.drop(self.ln1(self.conv1(x, edge_index).relu()))\n        x = self.drop(self.ln2(self.conv2(x, edge_index).relu()))\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GraphModel(len(shape2id) + len(color2id), hid=96, num_classes=len(label2id)).to(\n    device\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------------- tracking dict ----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_cxa\": [], \"val_cxa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ---------------- training loop ----------------\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    # ---- train ----\n    model.train()\n    tot_loss = tot_corr = tot_graphs = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        pred = out.argmax(-1)\n        tot_loss += loss.item() * batch.num_graphs\n        tot_corr += int((pred == batch.y).sum())\n        tot_graphs += batch.num_graphs\n    tr_loss = tot_loss / tot_graphs\n    tr_acc = tot_corr / tot_graphs\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n\n    # ---- validation ----\n    model.eval()\n    v_loss = v_corr = v_graphs = 0\n    all_pred, all_gt, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            pred = out.argmax(-1).cpu()\n            v_loss += loss.item() * batch.num_graphs\n            v_corr += int((pred == batch.y.cpu()).sum())\n            v_graphs += batch.num_graphs\n            all_pred.extend(pred.tolist())\n            all_gt.extend(batch.y.cpu().tolist())\n            all_seq.extend(batch.seq)\n    val_loss = v_loss / v_graphs\n    val_acc = v_corr / v_graphs\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n\n    # ---- CXA ----\n    tr_cxa = 0\n    with torch.no_grad():\n        for g in train_graphs:\n            out = model(\n                g.x.to(device),\n                g.edge_index.to(device),\n                torch.zeros(g.x.size(0), dtype=torch.long, device=device),\n            )\n            tr_cxa += (\n                complexity_weight(g.seq) if out.argmax().item() == g.y.item() else 0\n            )\n    tr_cxa /= sum(complexity_weight(g.seq) for g in train_graphs)\n    val_cxa = weighted_accuracy(\n        [complexity_weight(s) for s in all_seq], all_gt, all_pred\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_cxa\"].append(tr_cxa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cxa\"].append(val_cxa)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_acc={val_acc:.3f} | val_CXA={val_cxa:.3f}\"\n    )\n\n# ---------------- test evaluation ----------------\nmodel.eval()\ntest_pred, test_gt, test_seq = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.batch)\n        p = out.argmax(-1).cpu()\n        test_pred.extend(p.tolist())\n        test_gt.extend(batch.y.cpu().tolist())\n        test_seq.extend(batch.seq)\nCWA = weighted_accuracy([count_color_variety(s) for s in test_seq], test_gt, test_pred)\nSWA = weighted_accuracy([count_shape_variety(s) for s in test_seq], test_gt, test_pred)\nCXA = weighted_accuracy([complexity_weight(s) for s in test_seq], test_gt, test_pred)\nprint(f\"Test  CWA={CWA:.3f}  SWA={SWA:.3f}  CXA={CXA:.3f}\")\n\n# save overall data\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gt\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to ./working\")\n", "import os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool, Dropout\n\n# ---------------- obligatory working dir ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- GPU/CPU handling ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- dataset loader ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # treat each csv as a split\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nif not DATA_PATH.exists():  # tiny synthetic fallback\n    print(\"No SPR_BENCH found. Creating synthetic toy data.\")\n    import csv, random, string\n\n    os.makedirs(DATA_PATH, exist_ok=True)\n    rng = random.Random(0)\n    shapes = list(\"ABCDEF\")\n    colors = list(\"123456\")\n    for split, n in [(\"train\", 400), (\"dev\", 100), (\"test\", 100)]:\n        with open(DATA_PATH / f\"{split}.csv\", \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                ln = rng.randint(4, 10)\n                seq = \" \".join(\n                    rng.choice(shapes) + rng.choice(colors) for _ in range(ln)\n                )\n                lbl = rng.choice([\"yes\", \"no\"])\n                w.writerow([f\"{split}_{i}\", seq, lbl])\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ---------------- vocab build ----------------\ndef parse_tok(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshapes = set()\ncolors = set()\nfor r in dsets[\"train\"]:\n    for t in r[\"sequence\"].split():\n        s, c = parse_tok(t)\n        shapes.add(s)\n        colors.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncolor2id = {c: i for i, c in enumerate(sorted(colors))}\nlabel2id = {l: i for i, l in enumerate(sorted({r[\"label\"] for r in dsets[\"train\"]}))}\n\ninput_dim = len(shape2id) + len(color2id) + 1  # +1 for position scalar\n\n\n# ---------------- helpers for metrics ----------------\ndef count_colors(seq: str):\n    return len({t[1:] if len(t) > 1 else \"0\" for t in seq.split()})\n\n\ndef count_shapes(seq: str):\n    return len({t[0] for t in seq.split()})\n\n\ndef CWA(seq, y, yhat):  # per sequence\n    w = count_colors(seq)\n    return w if y == yhat else 0, w\n\n\ndef SWA(seq, y, yhat):\n    w = count_shapes(seq)\n    return w if y == yhat else 0, w\n\n\ndef CXA(seq, y, yhat):\n    w = count_shapes(seq) * count_colors(seq)\n    return w if y == yhat else 0, w\n\n\n# ---------------- graph construction ----------------\ndef seq_to_graph(sequence, label):\n    toks = sequence.split()\n    n = len(toks)\n    x = []\n    for idx, t in enumerate(toks):\n        s, c = parse_tok(t)\n        feat = np.zeros(input_dim, dtype=np.float32)\n        feat[shape2id[s]] = 1.0\n        feat[len(shape2id) + color2id[c]] = 1.0\n        feat[-1] = idx / (n - 1) if n > 1 else 0.0  # position scalar\n        x.append(feat)\n    x = torch.tensor(np.stack(x))\n    # order edges\n    src = torch.arange(0, n - 1, dtype=torch.long)\n    dst = src + 1\n    edges = [(src, dst)]\n    # same-shape edges\n    same_shape = []\n    for i in range(n):\n        for j in range(i + 1, n):\n            if toks[i][0] == toks[j][0]:\n                same_shape.append((i, j))\n    if same_shape:\n        ss_src, ss_dst = zip(*same_shape)\n        edges.append((torch.tensor(ss_src), torch.tensor(ss_dst)))\n    # same-color edges\n    same_col = []\n    for i in range(n):\n        for j in range(i + 1, n):\n            if parse_tok(toks[i])[1] == parse_tok(toks[j])[1]:\n                same_col.append((i, j))\n    if same_col:\n        sc_src, sc_dst = zip(*same_col)\n        edges.append((torch.tensor(sc_src), torch.tensor(sc_dst)))\n    # assemble undirected edge_index\n    if edges:\n        src_cat = torch.cat([e[0] for e in edges])\n        dst_cat = torch.cat([e[1] for e in edges])\n        edge_index = torch.stack(\n            [torch.cat([src_cat, dst_cat]), torch.cat([dst_cat, src_cat])], dim=0\n        )\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2id[label]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y, seq=sequence)\n    return data\n\n\ndef build_graph_split(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train, graph_dev, graph_test = map(build_graph_split, [\"train\", \"dev\", \"test\"])\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n# ---------------- model ----------------\nclass GCNModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(input_dim, 128)\n        self.conv2 = GCNConv(128, 128)\n        self.conv3 = GCNConv(128, 128)\n        self.drop = Dropout(p=0.3)\n        self.lin = torch.nn.Linear(128, len(label2id))\n\n    def forward(self, x, edge_index, batch):\n        x = F.relu(self.conv1(x, edge_index))\n        x = self.drop(x)\n        x = F.relu(self.conv2(x, edge_index))\n        x = self.drop(x)\n        x = F.relu(self.conv3(x, edge_index))\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GCNModel().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n# ---------------- experiment tracker ----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"val_cwa\": [],\n            \"val_swa\": [],\n            \"val_cxa\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ---------------- training loop ----------------\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    # ----- train -----\n    model.train()\n    tot_loss = tot_corr = tot_graphs = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        tot_corr += int((out.argmax(dim=-1) == batch.y).sum())\n        tot_graphs += batch.num_graphs\n    tr_loss = tot_loss / tot_graphs\n    tr_acc = tot_corr / tot_graphs\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n\n    # ----- validation -----\n    model.eval()\n    v_loss = v_corr = v_graphs = 0\n    seqs = []\n    gts = []\n    preds = []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            v_corr += int((out.argmax(dim=-1) == batch.y).sum())\n            v_graphs += batch.num_graphs\n            preds.extend(out.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch.y.cpu().tolist())\n            seqs.extend(batch.seq)  # stored in Data object\n    val_loss = v_loss / v_graphs\n    val_acc = v_corr / v_graphs\n    # compute weighted metrics\n    cwa_num = cwa_den = swa_num = swa_den = cxa_num = cxa_den = 0\n    for s, y, yh in zip(seqs, gts, preds):\n        a, b = CWA(s, y, yh)\n        cwa_num += a\n        cwa_den += b\n        a, b = SWA(s, y, yh)\n        swa_num += a\n        swa_den += b\n        a, b = CXA(s, y, yh)\n        cxa_num += a\n        cxa_den += b\n    val_cwa = cwa_num / cwa_den if cwa_den else 0.0\n    val_swa = swa_num / swa_den if swa_den else 0.0\n    val_cxa = cxa_num / cxa_den if cxa_den else 0.0\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cwa\"].append(val_cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cxa\"].append(val_cxa)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | \"\n        f\"Acc {val_acc:.3f} | CWA {val_cwa:.3f} | SWA {val_swa:.3f} | CXA {val_cxa:.3f}\"\n    )\n\n# store final preds on dev\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\n\n# ---------------- plot & save ----------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Artifacts saved in ./working\")\n", "import os, pathlib, time, numpy as np, torch, torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# --- mandatory working dir ----------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --- GPU / CPU ----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n#                              DATA UTILITIES                                  #\n# -----------------------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    out = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        out[split] = _load(f\"{split}.csv\")\n    return out\n\n\ndef parse_token(tok):\n    tok = tok.strip()\n    shape = tok[0]\n    color = tok[1:] if len(tok) > 1 else \"0\"\n    return shape, color\n\n\ndef color_variety(seq):  # |colors|\n    return len({parse_token(t)[1] for t in seq.split()})\n\n\ndef shape_variety(seq):  # |shapes|\n    return len({parse_token(t)[0] for t in seq.split()})\n\n\ndef complexity_weight(seq):  # shapes * colors\n    return max(1, color_variety(seq) * shape_variety(seq))\n\n\n# -----------------------------------------------------------------------------#\n#                             DATA PREPARATION                                 #\n# -----------------------------------------------------------------------------#\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nif not DATA_PATH.exists():\n    print(\"Dataset folder SPR_BENCH not found \u2013 abort.\")\n    exit(0)\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n# Build vocabularies\nshapes, colors = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colors.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncol2id = {c: i for i, c in enumerate(sorted(colors))}\n\nlabels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(labels)}\n\n\n# -----------------------------------------------------------------------------#\n#                       GRAPH CONSTRUCTION WITH RELATIONS                      #\n# -----------------------------------------------------------------------------#\n# relation ids: 0=next, 1=same_shape, 2=same_color\ndef seq_to_graph(sequence, label):\n    toks = sequence.split()\n    n = len(toks)\n    # node features: one-hot(shape) | one-hot(color)\n    feat = []\n    shapes_seen, colors_seen = [], []\n    for tok in toks:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(col2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + col2id[c]] = 1.0\n        feat.append(vec)\n        shapes_seen.append(s)\n        colors_seen.append(c)\n    x = torch.tensor(np.stack(feat))\n    edge_src, edge_dst, edge_type = [], [], []\n    # consecutive\n    for i in range(n - 1):\n        edge_src += [i, i + 1]\n        edge_dst += [i + 1, i]\n        edge_type += [0, 0]\n    # same shape & same color\n    for i in range(n):\n        for j in range(i + 1, n):\n            if shapes_seen[i] == shapes_seen[j]:\n                edge_src += [i, j]\n                edge_dst += [j, i]\n                edge_type += [1, 1]\n            if colors_seen[i] == colors_seen[j]:\n                edge_src += [i, j]\n                edge_dst += [j, i]\n                edge_type += [2, 2]\n    edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n    edge_type = torch.tensor(edge_type, dtype=torch.long)\n    y = torch.tensor([label2id[label]], dtype=torch.long)\n    w = torch.tensor([complexity_weight(sequence)], dtype=torch.float32)\n    return Data(x=x, edge_index=edge_index, edge_type=edge_type, y=y, w=w)\n\n\ndef build_graph_dataset(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\ntrain_loader = DataLoader(graph_train, batch_size=128, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=256, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=256, shuffle=False)\n\n\n# -----------------------------------------------------------------------------#\n#                                   MODEL                                      #\n# -----------------------------------------------------------------------------#\nclass RGNN(torch.nn.Module):\n    def __init__(self, in_dim, hid_dim=96, num_classes=2, num_rels=3, dropout=0.3):\n        super().__init__()\n        self.conv1 = RGCNConv(in_dim, hid_dim, num_rels)\n        self.conv2 = RGCNConv(hid_dim, hid_dim, num_rels)\n        self.lin = torch.nn.Linear(hid_dim, num_classes)\n        self.dp = torch.nn.Dropout(dropout)\n\n    def forward(self, x, edge_index, edge_type, batch):\n        x = self.conv1(x, edge_index, edge_type).relu()\n        x = self.dp(x)\n        x = self.conv2(x, edge_index, edge_type).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = RGNN(in_dim=len(shape2id) + len(col2id), num_classes=len(label2id)).to(device)\n\n# class-balanced loss\ntrain_labels = [d.y.item() for d in graph_train]\ncounts = np.bincount(train_labels, minlength=len(label2id))\nclass_weight = torch.tensor(1.0 / (counts + 1e-6), dtype=torch.float32, device=device)\ncriterion = torch.nn.CrossEntropyLoss(weight=class_weight)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n\n\n# -----------------------------------------------------------------------------#\n#                             METRIC FUNCTIONS                                 #\n# -----------------------------------------------------------------------------#\ndef weighted_acc(pred, target, weights):\n    correct = (pred == target).float()\n    return (correct * weights).sum().item(), weights.sum().item()\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [color_variety(s) for s in seqs]\n    return sum(w for w, t, p in zip(ws, y_true, y_pred) if t == p) / max(1, sum(ws))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [shape_variety(s) for s in seqs]\n    return sum(w for w, t, p in zip(ws, y_true, y_pred) if t == p) / max(1, sum(ws))\n\n\n# -----------------------------------------------------------------------------#\n#                              EXPERIMENT TRACKER                              #\n# -----------------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_cxa\": [], \"val_cxa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# -----------------------------------------------------------------------------#\n#                                 TRAINING                                     #\n# -----------------------------------------------------------------------------#\nEPOCHS = 15\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tr_loss = tr_correct = 0.0\n    tr_w_correct = tr_w_total = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * batch.num_graphs\n        preds = out.argmax(dim=-1)\n        tr_correct += (preds == batch.y).sum().item()\n        wc, wt = weighted_acc(preds, batch.y, batch.w)\n        tr_w_correct += wc\n        tr_w_total += wt\n    train_acc = tr_correct / len(graph_train)\n    train_cxa = tr_w_correct / tr_w_total\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss / len(graph_train))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_cxa\"].append(train_cxa)\n\n    # ---------- validation ----------\n    model.eval()\n    val_loss = val_correct = 0.0\n    val_w_correct = val_w_total = 0.0\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n            loss = criterion(out, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1)\n            val_correct += (preds == batch.y).sum().item()\n            wc, wt = weighted_acc(preds, batch.y, batch.w)\n            val_w_correct += wc\n            val_w_total += wt\n    val_acc = val_correct / len(graph_dev)\n    val_cxa = val_w_correct / val_w_total\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss / len(graph_dev))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cxa\"].append(val_cxa)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss/len(graph_dev):.4f}\")\n\n# -----------------------------------------------------------------------------#\n#                               FINAL TEST                                     #\n# -----------------------------------------------------------------------------#\nmodel.eval()\ntest_preds, test_seqs, test_labels = [], [], []\nwith torch.no_grad():\n    for batch, raw in zip(test_loader, dsets[\"test\"]):\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n        test_preds.extend(out.argmax(dim=-1).cpu().tolist())\n        test_seqs.append(raw[\"sequence\"])\n        test_labels.append(label2id[raw[\"label\"]])\n\ncwa = color_weighted_accuracy(test_seqs, test_labels, test_preds)\nswa = shape_weighted_accuracy(test_seqs, test_labels, test_preds)\ncxa = sum(\n    complexity_weight(s)\n    for s, t, p in zip(test_seqs, test_labels, test_preds)\n    if t == p\n) / max(1, sum(complexity_weight(s) for s in test_seqs))\nprint(f\"TEST  CWA={cwa:.4f}  SWA={swa:.4f}  CXA={cxa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All metrics saved to\", working_dir)\n", "import os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\nfrom collections import defaultdict\n\n#####################################################################\n# working dir + device\n#####################################################################\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n#####################################################################\n# load / fallback SPR_BENCH\n#####################################################################\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for spl in (\"train\", \"dev\", \"test\"):\n        d[spl] = _load(f\"{spl}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nif not DATA_PATH.exists():\n    print(\"SPR_BENCH not found \u2013 creating tiny synthetic data so the script can run.\")\n    os.makedirs(DATA_PATH, exist_ok=True)\n    rng = np.random.default_rng(0)\n    shapes = list(\"ABCDE\")\n    colours = list(\"12345\")\n    for split, n in ((\"train\", 500), (\"dev\", 100), (\"test\", 100)):\n        with open(DATA_PATH / f\"{split}.csv\", \"w\") as f:\n            f.write(\"id,sequence,label\\n\")\n            for i in range(n):\n                length = rng.integers(4, 9)\n                seq = \" \".join(\n                    rng.choice(shapes) + rng.choice(colours) for _ in range(length)\n                )\n                lbl = rng.choice([\"yes\", \"no\"])\n                f.write(f\"{split}_{i},{seq},{lbl}\\n\")\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n#####################################################################\n# utility functions for metrics\n#####################################################################\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1:] if len(t) > 1 else \"0\" for t in sequence.strip().split()))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) else 0.0\n\n\ndef complexity_weight(sequence: str) -> int:\n    return count_color_variety(sequence) * count_shape_variety(sequence)\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [complexity_weight(s) for s in seqs]\n    correct = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) else 0.0\n\n\n#####################################################################\n# build vocabularies\n#####################################################################\ndef parse_token(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshape_set, colour_set = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shape_set.add(s)\n        colour_set.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shape_set))}\ncol2id = {c: i for i, c in enumerate(sorted(colour_set))}\nlabel2id = {l: i for i, l in enumerate(sorted({r[\"label\"] for r in dsets[\"train\"]}))}\nprint(\"Shapes:\", shape2id)\nprint(\"Colours:\", col2id)\nprint(\"Labels:\", label2id)\n\nfeat_dim = len(shape2id) + len(col2id)\n\n\n#####################################################################\n# sequence -> graph\n#####################################################################\ndef seq_to_graph(sequence: str, label: str):\n    toks = sequence.split()\n    n = len(toks)\n    # node features (one-hot shape | one-hot colour)\n    x = torch.zeros((n, feat_dim), dtype=torch.float32)\n    for i, tok in enumerate(toks):\n        s, c = parse_token(tok)\n        x[i, shape2id[s]] = 1.0\n        x[i, len(shape2id) + col2id[c]] = 1.0\n    # edges: consecutive + same-shape + same-color (bidirectional)\n    src, dst = [], []\n    # positional edges\n    for i in range(n - 1):\n        src += [i, i + 1]\n        dst += [i + 1, i]\n    # shared attributes\n    shape_groups = defaultdict(list)\n    colour_groups = defaultdict(list)\n    for i, tok in enumerate(toks):\n        s, c = parse_token(tok)\n        shape_groups[s].append(i)\n        colour_groups[c].append(i)\n    for group in list(shape_groups.values()) + list(colour_groups.values()):\n        for i in group:\n            for j in group:\n                if i != j:\n                    src.append(i)\n                    dst.append(j)\n    edge_index = (\n        torch.tensor([src, dst], dtype=torch.long)\n        if src\n        else torch.zeros((2, 0), dtype=torch.long)\n    )\n    y = torch.tensor([label2id[label]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef build_graph_dataset(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n#####################################################################\n# model\n#####################################################################\nclass GraphModel(torch.nn.Module):\n    def __init__(self, in_dim, hid=128, num_classes=len(label2id)):\n        super().__init__()\n        self.conv1 = SAGEConv(in_dim, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.conv3 = SAGEConv(hid, hid)\n        self.norm1 = torch.nn.LayerNorm(hid)\n        self.norm2 = torch.nn.LayerNorm(hid)\n        self.drop = torch.nn.Dropout(0.3)\n        self.lin = torch.nn.Linear(hid, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = self.drop(self.norm1(self.conv1(x, edge_index).relu()))\n        x = self.drop(self.norm2(self.conv2(x, edge_index).relu()))\n        x = self.conv3(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GraphModel(feat_dim).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n#####################################################################\n# experiment tracking dict\n#####################################################################\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},  # will store dict per epoch\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n#####################################################################\n# training loop with early stopping on CXA\n#####################################################################\nMAX_EPOCH = 30\npatience = 5\nbest_cxa = -1.0\nstagnant = 0\n\n\ndef evaluate(loader, seqs):\n    model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            total_loss += loss.item() * batch.num_graphs\n            preds.extend(out.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch.y.cpu().tolist())\n    avg_loss = total_loss / len(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    cxa = complexity_weighted_accuracy(seqs, gts, preds)\n    return avg_loss, acc, cwa, swa, cxa, preds\n\n\nfor epoch in range(1, MAX_EPOCH + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item() * batch.num_graphs\n    train_loss = epoch_loss / len(graph_train)\n\n    # quick train accuracy for logging\n    tr_loss, tr_acc, tr_cwa, tr_swa, tr_cxa, _ = evaluate(\n        train_loader, [r[\"sequence\"] for r in dsets[\"train\"]]\n    )\n\n    # validation\n    val_loss, val_acc, val_cwa, val_swa, val_cxa, val_preds = evaluate(\n        dev_loader, [r[\"sequence\"] for r in dsets[\"dev\"]]\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"acc\": tr_acc, \"cwa\": tr_cwa, \"swa\": tr_swa, \"cxa\": tr_cxa}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"acc\": val_acc, \"cwa\": val_cwa, \"swa\": val_swa, \"cxa\": val_cxa}\n    )\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | CXA = {val_cxa:.4f}\")\n\n    # early stopping\n    if val_cxa > best_cxa + 1e-4:\n        best_cxa = val_cxa\n        stagnant = 0\n        # keep best predictions for later analysis\n        experiment_data[\"SPR_BENCH\"][\"predictions\"] = val_preds\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = [\n            label2id[r[\"label\"]] for r in dsets[\"dev\"]\n        ]\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        stagnant += 1\n        if stagnant >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n#####################################################################\n# plot loss & CXA\n#####################################################################\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nplt.figure()\nplt.plot(\n    [d[\"cxa\"] for d in experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"]],\n    label=\"train_CXA\",\n)\nplt.plot(\n    [d[\"cxa\"] for d in experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"]], label=\"val_CXA\"\n)\nplt.title(\"Complexity-Weighted Accuracy\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"cxa_curve.png\"))\n\n#####################################################################\n# save all experiment data\n#####################################################################\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All artefacts saved to ./working\")\n", "import os, sys, pathlib, time, numpy as np, torch, torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n#                                WORKING DIR                                   #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------------------------#\n#                              DEVICE HANDLING                                 #\n# -----------------------------------------------------------------------------#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n#                          OPTIONAL PYG INSTALL (IF NEEDED)                    #\n# -----------------------------------------------------------------------------#\ntry:\n    from torch_geometric.data import Data, DataLoader\n    from torch_geometric.nn import RGCNConv, global_mean_pool\nexcept ImportError as e:\n    print(\"torch_geometric not found, attempting lightweight install \u2026\")\n    import subprocess\n\n    # CPU wheels are fast to grab; adjust torch version tag if necessary\n    url = \"https://data.pyg.org/whl/torch-2.1.0+cpu.html\"\n    subprocess.check_call(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"torch-geometric\", \"-f\", url]\n    )\n    from torch_geometric.data import Data, DataLoader\n    from torch_geometric.nn import RGCNConv, global_mean_pool\n\n\n# -----------------------------------------------------------------------------#\n#                              DATA UTILITIES                                  #\n# -----------------------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    out = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        out[split] = _load(f\"{split}.csv\")\n    return out\n\n\ndef locate_dataset() -> pathlib.Path:\n    \"\"\"Search a few likely locations for the SPR_BENCH folder.\"\"\"\n    candidates = [\n        pathlib.Path(os.environ.get(\"SPR_BENCH_DIR\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for c in candidates:\n        if c.is_dir():\n            print(f\"Found SPR_BENCH at: {c.resolve()}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH directory not found. Please set $SPR_BENCH_DIR or place the \"\n        \"dataset in one of the default search paths.\"\n    )\n\n\ndef parse_token(tok: str):\n    tok = tok.strip()\n    shape = tok[0]\n    color = tok[1:] if len(tok) > 1 else \"0\"\n    return shape, color\n\n\ndef color_variety(seq: str) -> int:\n    return len({parse_token(t)[1] for t in seq.split()})\n\n\ndef shape_variety(seq: str) -> int:\n    return len({parse_token(t)[0] for t in seq.split()})\n\n\ndef complexity_weight(seq: str) -> int:\n    return max(1, color_variety(seq) * shape_variety(seq))\n\n\n# -----------------------------------------------------------------------------#\n#                             LOAD DATASETS                                    #\n# -----------------------------------------------------------------------------#\nDATA_PATH = locate_dataset()\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n# Build vocabularies\nshapes, colors = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colors.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncol2id = {c: i for i, c in enumerate(sorted(colors))}\n\nlabels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(labels)}\n\n\n# -----------------------------------------------------------------------------#\n#                           GRAPH CONSTRUCTION                                 #\n# -----------------------------------------------------------------------------#\n# relation ids: 0=next, 1=same_shape, 2=same_color\ndef seq_to_graph(sequence, label):\n    toks = sequence.split()\n    n = len(toks)\n    # node feature: one-hot(shape) + one-hot(color)\n    feat = []\n    s_list, c_list = [], []\n    for tok in toks:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(col2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + col2id[c]] = 1.0\n        feat.append(vec)\n        s_list.append(s)\n        c_list.append(c)\n    x = torch.tensor(np.stack(feat), dtype=torch.float32)\n\n    edge_src, edge_dst, edge_type = [], [], []\n    # next-token edges\n    for i in range(n - 1):\n        edge_src += [i, i + 1]\n        edge_dst += [i + 1, i]\n        edge_type += [0, 0]\n    # same shape / color\n    for i in range(n):\n        for j in range(i + 1, n):\n            if s_list[i] == s_list[j]:\n                edge_src += [i, j]\n                edge_dst += [j, i]\n                edge_type += [1, 1]\n            if c_list[i] == c_list[j]:\n                edge_src += [i, j]\n                edge_dst += [j, i]\n                edge_type += [2, 2]\n    edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n    edge_type = torch.tensor(edge_type, dtype=torch.long)\n    y = torch.tensor([label2id[label]], dtype=torch.long)\n    w = torch.tensor([complexity_weight(sequence)], dtype=torch.float32)\n    return Data(x=x, edge_index=edge_index, edge_type=edge_type, y=y, w=w)\n\n\ndef build_graph_dataset(split_name):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split_name]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\ntrain_loader = DataLoader(graph_train, batch_size=128, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=256, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=256, shuffle=False)\n\n\n# -----------------------------------------------------------------------------#\n#                                MODEL                                         #\n# -----------------------------------------------------------------------------#\nclass RGNN(torch.nn.Module):\n    def __init__(self, in_dim, hid_dim=96, num_classes=2, num_rels=3, dropout=0.3):\n        super().__init__()\n        self.conv1 = RGCNConv(in_dim, hid_dim, num_rels)\n        self.conv2 = RGCNConv(hid_dim, hid_dim, num_rels)\n        self.lin = torch.nn.Linear(hid_dim, num_classes)\n        self.dp = torch.nn.Dropout(dropout)\n\n    def forward(self, x, edge_index, edge_type, batch):\n        x = self.conv1(x, edge_index, edge_type).relu()\n        x = self.dp(x)\n        x = self.conv2(x, edge_index, edge_type).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = RGNN(in_dim=len(shape2id) + len(col2id), num_classes=len(label2id)).to(device)\n\n# class-balanced loss\ntrain_labels = [d.y.item() for d in graph_train]\ncounts = np.bincount(train_labels, minlength=len(label2id))\nclass_weight = torch.tensor(1.0 / (counts + 1e-6), dtype=torch.float32, device=device)\ncriterion = torch.nn.CrossEntropyLoss(weight=class_weight)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n\n\n# -----------------------------------------------------------------------------#\n#                          METRICS & TRACKING                                  #\n# -----------------------------------------------------------------------------#\ndef weighted_acc(pred, target, weights):\n    correct = (pred == target).float()\n    return (correct * weights).sum().item(), weights.sum().item()\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [color_variety(s) for s in seqs]\n    return sum(w for w, t, p in zip(ws, y_true, y_pred) if t == p) / max(1, sum(ws))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [shape_variety(s) for s in seqs]\n    return sum(w for w, t, p in zip(ws, y_true, y_pred) if t == p) / max(1, sum(ws))\n\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"train_cxa\": [],\n            \"val_cxa\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# -----------------------------------------------------------------------------#\n#                                 TRAINING                                     #\n# -----------------------------------------------------------------------------#\nEPOCHS = 15\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tr_loss = tr_correct = tr_wc = tr_wt = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n\n        tr_loss += loss.item() * batch.num_graphs\n        preds = out.argmax(dim=-1)\n        tr_correct += (preds == batch.y).sum().item()\n        wc, wt = weighted_acc(preds, batch.y, batch.w)\n        tr_wc += wc\n        tr_wt += wt\n\n    train_acc = tr_correct / len(graph_train)\n    train_cxa = tr_wc / tr_wt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss / len(graph_train))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_cxa\"].append(train_cxa)\n\n    # ------------------ validation ------------------\n    model.eval()\n    val_loss = val_correct = val_wc = val_wt = 0.0\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n            loss = criterion(out, batch.y)\n\n            val_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1)\n            val_correct += (preds == batch.y).sum().item()\n            wc, wt = weighted_acc(preds, batch.y, batch.w)\n            val_wc += wc\n            val_wt += wt\n\n    val_acc = val_correct / len(graph_dev)\n    val_cxa = val_wc / val_wt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss / len(graph_dev))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cxa\"].append(val_cxa)\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss/len(graph_dev):.4f}\")\n\n# -----------------------------------------------------------------------------#\n#                                   TEST                                       #\n# -----------------------------------------------------------------------------#\nmodel.eval()\ntest_sequences = [d[\"sequence\"] for d in dsets[\"test\"]]\ntest_labels_raw = [label2id[d[\"label\"]] for d in dsets[\"test\"]]\n\ntest_preds = []\nidx = 0\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n        preds = out.argmax(dim=-1).cpu().tolist()\n        test_preds.extend(preds)\n        idx += batch.num_graphs\n\nassert idx == len(test_sequences), \"Mismatch in test sample count!\"\n\ncwa = color_weighted_accuracy(test_sequences, test_labels_raw, test_preds)\nswa = shape_weighted_accuracy(test_sequences, test_labels_raw, test_preds)\ncxa = sum(\n    complexity_weight(s)\n    for s, t, p in zip(test_sequences, test_labels_raw, test_preds)\n    if t == p\n) / max(1, sum(complexity_weight(s) for s in test_sequences))\n\nprint(f\"TEST  CWA={cwa:.4f}  SWA={swa:.4f}  CXA={cxa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels_raw\n\n# -----------------------------------------------------------------------------#\n#                             SAVE EXPERIMENT DATA                             #\n# -----------------------------------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All metrics saved to\", working_dir)\n", "import os, pathlib, time, random, numpy as np, torch, torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# -----------------------------------------------------------------------------#\n#                          housekeeping & reproducibility                      #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n#                               load SPR_BENCH                                 #\n# -----------------------------------------------------------------------------#\ndef locate_spr_bench() -> pathlib.Path:\n    \"\"\"Try multiple candidate paths and return the first hit.\"\"\"\n    candidates = [\n        os.environ.get(\"SPR_BENCH_PATH\", \"\"),  # env-var\n        \"./SPR_BENCH\",  # local folder\n        \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\",  # author\u2019s absolute path\n    ]\n    for c in candidates:\n        if c and pathlib.Path(c).exists():\n            return pathlib.Path(c)\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset was not found in any default location. \"\n        \"Set SPR_BENCH_PATH environment variable to its folder.\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    ds = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        ds[split] = _load(f\"{split}.csv\")\n    return ds\n\n\nDATA_PATH = locate_spr_bench()\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# -----------------------------------------------------------------------------#\n#                         sequence parsing & helpers                           #\n# -----------------------------------------------------------------------------#\ndef parse_token(tok: str):\n    tok = tok.strip()\n    shape = tok[0]\n    color = tok[1:] if len(tok) > 1 else \"0\"\n    return shape, color\n\n\ndef color_variety(seq: str) -> int:\n    return len({parse_token(t)[1] for t in seq.split()})\n\n\ndef shape_variety(seq: str) -> int:\n    return len({parse_token(t)[0] for t in seq.split()})\n\n\ndef complexity_weight(seq: str) -> int:\n    return max(1, color_variety(seq) * shape_variety(seq))\n\n\n# -----------------------------------------------------------------------------#\n#                        vocabularies: shapes, colors, labels                  #\n# -----------------------------------------------------------------------------#\nshapes, colors = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colors.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncolor2id = {c: i for i, c in enumerate(sorted(colors))}\n\nlabels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(labels)}\n\n\n# -----------------------------------------------------------------------------#\n#                     graph construction with relation edges                   #\n# -----------------------------------------------------------------------------#\n# relations: 0 \u2192 next/prev  1 \u2192 same_shape  2 \u2192 same_color\ndef seq_to_graph(sequence: str, label: str):\n    tokens = sequence.split()\n    n = len(tokens)\n    feats, s_list, c_list = [], [], []\n    for tok in tokens:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(color2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + color2id[c]] = 1.0\n        feats.append(vec)\n        s_list.append(s)\n        c_list.append(c)\n\n    edge_src, edge_dst, edge_type = [], [], []\n\n    # next / prev edges\n    for i in range(n - 1):\n        edge_src += [i, i + 1]\n        edge_dst += [i + 1, i]\n        edge_type += [0, 0]\n\n    # same shape & same color edges\n    for i in range(n):\n        for j in range(i + 1, n):\n            if s_list[i] == s_list[j]:\n                edge_src += [i, j]\n                edge_dst += [j, i]\n                edge_type += [1, 1]\n            if c_list[i] == c_list[j]:\n                edge_src += [i, j]\n                edge_dst += [j, i]\n                edge_type += [2, 2]\n\n    data = Data(\n        x=torch.tensor(np.stack(feats), dtype=torch.float32),\n        edge_index=torch.tensor([edge_src, edge_dst], dtype=torch.long),\n        edge_type=torch.tensor(edge_type, dtype=torch.long),\n        y=torch.tensor([label2id[label]], dtype=torch.long),\n        w=torch.tensor([complexity_weight(sequence)], dtype=torch.float32),\n    )\n    return data\n\n\ndef build_graph_dataset(split: str):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\ntrain_loader = DataLoader(graph_train, batch_size=128, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=256, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=256, shuffle=False)\n\n\n# -----------------------------------------------------------------------------#\n#                                  model                                       #\n# -----------------------------------------------------------------------------#\nclass RGNN(torch.nn.Module):\n    def __init__(self, in_dim, hidden=96, classes=2, rels=3, dropout=0.3):\n        super().__init__()\n        self.conv1 = RGCNConv(in_dim, hidden, rels)\n        self.conv2 = RGCNConv(hidden, hidden, rels)\n        self.lin = torch.nn.Linear(hidden, classes)\n        self.dp = torch.nn.Dropout(dropout)\n\n    def forward(self, x, edge_index, edge_type, batch):\n        x = self.conv1(x, edge_index, edge_type).relu()\n        x = self.dp(x)\n        x = self.conv2(x, edge_index, edge_type).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = RGNN(len(shape2id) + len(color2id), classes=len(label2id)).to(device)\n\n# class-balanced loss\ntrain_labels = [d.y.item() for d in graph_train]\ncounts = np.bincount(train_labels, minlength=len(label2id))\nclass_weight = torch.tensor(1.0 / (counts + 1e-6), dtype=torch.float32, device=device)\ncriterion = torch.nn.CrossEntropyLoss(weight=class_weight)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n\n\n# -----------------------------------------------------------------------------#\n#                            metric utilities                                  #\n# -----------------------------------------------------------------------------#\ndef accumulate_weighted(correct_mask: torch.Tensor, weights: torch.Tensor):\n    \"\"\"Return numerator & denominator for any weighted accuracy.\"\"\"\n    return (correct_mask * weights).sum().item(), weights.sum().item()\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [color_variety(s) for s in seqs]\n    return sum(w for w, t, p in zip(ws, y_true, y_pred) if t == p) / max(1, sum(ws))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    ws = [shape_variety(s) for s in seqs]\n    return sum(w for w, t, p in zip(ws, y_true, y_pred) if t == p) / max(1, sum(ws))\n\n\ndef complexity_weighted_accuracy(weights, correct_mask):\n    num, den = accumulate_weighted(correct_mask, weights)\n    return num / den if den > 0 else 0.0\n\n\n# -----------------------------------------------------------------------------#\n#                       experiment_data container                              #\n# -----------------------------------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"train_cxa\": [],\n            \"val_cxa\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# -----------------------------------------------------------------------------#\n#                               training loop                                  #\n# -----------------------------------------------------------------------------#\nEPOCHS = 15\nfor epoch in range(1, EPOCHS + 1):\n    # --- training ---\n    model.train()\n    tr_loss = tr_correct = 0.0\n    tr_num, tr_den = 0.0, 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        logits = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n        loss = criterion(logits, batch.y)\n        loss.backward()\n        optimizer.step()\n\n        # stats\n        tr_loss += loss.item() * batch.num_graphs\n        preds = logits.argmax(dim=-1)\n        correct_mask = (preds == batch.y).float()\n        tr_correct += correct_mask.sum().item()\n        num, den = accumulate_weighted(correct_mask, batch.w.to(device))\n        tr_num += num\n        tr_den += den\n\n    train_acc = tr_correct / len(graph_train)\n    train_cxa = tr_num / tr_den\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss / len(graph_train))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_cxa\"].append(train_cxa)\n\n    # --- validation ---\n    model.eval()\n    val_loss = val_correct = 0.0\n    val_num, val_den = 0.0, 0.0\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n            loss = criterion(logits, batch.y)\n\n            val_loss += loss.item() * batch.num_graphs\n            preds = logits.argmax(dim=-1)\n            correct_mask = (preds == batch.y).float()\n            val_correct += correct_mask.sum().item()\n            num, den = accumulate_weighted(correct_mask, batch.w.to(device))\n            val_num += num\n            val_den += den\n\n    val_acc = val_correct / len(graph_dev)\n    val_cxa = val_num / val_den\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss / len(graph_dev))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cxa\"].append(val_cxa)\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"val_loss={val_loss/len(graph_dev):.4f}  \"\n        f\"val_acc={val_acc:.4f}  val_CXA={val_cxa:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n#                                 testing                                      #\n# -----------------------------------------------------------------------------#\nmodel.eval()\ntest_preds, test_seqs, test_labels = [], [], []\nidx_pointer = 0\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch.x, batch.edge_index, batch.edge_type, batch.batch)\n        preds = logits.argmax(dim=-1).cpu().tolist()\n\n        num_graphs = batch.num_graphs\n        seqs_batch = [\n            dsets[\"test\"][i + idx_pointer][\"sequence\"] for i in range(num_graphs)\n        ]\n        labels_batch = [\n            label2id[dsets[\"test\"][i + idx_pointer][\"label\"]] for i in range(num_graphs)\n        ]\n        idx_pointer += num_graphs\n\n        test_preds.extend(preds)\n        test_seqs.extend(seqs_batch)\n        test_labels.extend(labels_batch)\n\nassert idx_pointer == len(\n    dsets[\"test\"]\n), \"Mismatch between predictions and test set size.\"\n\ncwa = color_weighted_accuracy(test_seqs, test_labels, test_preds)\nswa = shape_weighted_accuracy(test_seqs, test_labels, test_preds)\ncxa_num = sum(\n    complexity_weight(s)\n    for s, t, p in zip(test_seqs, test_labels, test_preds)\n    if t == p\n)\ncxa_den = sum(complexity_weight(s) for s in test_seqs)\ncxa = cxa_num / cxa_den if cxa_den else 0.0\n\nprint(f\"TEST  CWA={cwa:.4f}  SWA={swa:.4f}  CXA={cxa:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All metrics saved to\", working_dir)\n", "import os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool\n\n# ---------- mandatory working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        dd[split] = _load(f\"{split}.csv\")\n    return DatasetDict(dd)\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nif not DATA_PATH.exists():\n    # create tiny synthetic fallback\n    print(\"SPR_BENCH folder not found \u2013 generating tiny toy data.\")\n    DATA_PATH.mkdir(exist_ok=True)\n    rng = np.random.default_rng(0)\n    shapes, colors = [\"A\", \"B\", \"C\"], [\"1\", \"2\", \"3\", \"4\"]\n    for split, n in [(\"train\", 300), (\"dev\", 60), (\"test\", 60)]:\n        rows = [(\"id\", \"sequence\", \"label\")]\n        for i in range(n):\n            length = rng.integers(3, 8)\n            seq = \" \".join(\n                rng.choice(shapes) + rng.choice(colors) for _ in range(length)\n            )\n            lbl = rng.choice([\"yes\", \"no\"])\n            rows.append((f\"{split}_{i}\", seq, lbl))\n        with open(DATA_PATH / f\"{split}.csv\", \"w\") as f:\n            for r in rows:\n                f.write(\",\".join(r) + \"\\n\")\n\ndsets = load_spr_bench(DATA_PATH)\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dsets.items()})\n\n\n# ---------- vocab build ----------\ndef parse_tok(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshape_set, color_set = set(), set()\nfor r in dsets[\"train\"]:\n    for t in r[\"sequence\"].split():\n        s, c = parse_tok(t)\n        shape_set.add(s)\n        color_set.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shape_set))}\ncolor2id = {c: i for i, c in enumerate(sorted(color_set))}\nlabel2id = {l: i for i, l in enumerate(sorted({r[\"label\"] for r in dsets[\"train\"]}))}\n\n\n# ---------- seq\u2192graph ----------\ndef seq_to_graph(sequence, lbl):\n    toks = sequence.split()\n    n = len(toks)\n    # node features: one-hot shape | one-hot colour | scalar position index/n\n    feat_len = len(shape2id) + len(color2id) + 1\n    feats = []\n    shapes, colours = [], []\n    for idx, tok in enumerate(toks):\n        s, c = parse_tok(tok)\n        vec = np.zeros(feat_len, dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + color2id[c]] = 1.0\n        vec[-1] = idx / max(1, n - 1)\n        feats.append(vec)\n        shapes.append(s)\n        colours.append(c)\n    x = torch.tensor(np.stack(feats), dtype=torch.float)\n\n    # edges: sequential + same-shape + same-colour (undirected)\n    src, dst = [], []\n    for i in range(n - 1):\n        src.extend([i, i + 1])\n        dst.extend([i + 1, i])  # consecutive\n    for i in range(n):\n        for j in range(i + 1, n):\n            if shapes[i] == shapes[j] or colours[i] == colours[j]:\n                src.extend([i, j])\n                dst.extend([j, i])\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    y = torch.tensor([label2id[lbl]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = sequence\n    return data\n\n\ndef build_graph_split(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train, graph_dev, graph_test = map(build_graph_split, [\"train\", \"dev\", \"test\"])\n\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\n\n\n# ---------- model ----------\nclass GATModel(torch.nn.Module):\n    def __init__(self, in_dim, hid=64, num_classes=len(label2id)):\n        super().__init__()\n        self.g1 = GATConv(in_dim, hid, heads=4, concat=True, dropout=0.2)\n        self.g2 = GATConv(hid * 4, hid, heads=4, concat=False, dropout=0.2)\n        self.lin = torch.nn.Linear(hid, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = F.relu(self.g1(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.relu(self.g2(x, edge_index))\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GATModel(in_dim=len(shape2id) + len(color2id) + 1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- metrics ----------\ndef count_color_var(seq):\n    return len({tok[1:] if len(tok) > 1 else \"0\" for tok in seq.split()})\n\n\ndef count_shape_var(seq):\n    return len({tok[0] for tok in seq.split()})\n\n\ndef color_weighted_accuracy(seq_list, yt, yp):\n    w = [count_color_var(s) for s in seq_list]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, yt, yp)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef shape_weighted_accuracy(seq_list, yt, yp):\n    w = [count_shape_var(s) for s in seq_list]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, yt, yp)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seq_list, yt, yp):\n    w = [count_color_var(s) * count_shape_var(s) for s in seq_list]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, yt, yp)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------- tracking dict ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ---------- train loop ----------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    # ---- train ----\n    model.train()\n    tr_loss, tr_corr, tr_tot = 0.0, 0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * batch.num_graphs\n        tr_corr += (out.argmax(1) == batch.y).sum().item()\n        tr_tot += batch.num_graphs\n    tr_loss /= tr_tot\n    tr_acc = tr_corr / tr_tot\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_acc)\n\n    # ---- validation ----\n    model.eval()\n    v_loss, v_corr, v_tot = 0.0, 0, 0\n    seqs, gts, preds = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            pred = out.argmax(1)\n            v_corr += (pred == batch.y).sum().item()\n            v_tot += batch.num_graphs\n            preds.extend(pred.cpu().tolist())\n            gts.extend(batch.y.cpu().tolist())\n            # gather sequences per graph in batch\n            seqs.extend(batch.seq)\n    v_loss /= v_tot\n    v_acc = v_corr / v_tot\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    cxa = complexity_weighted_accuracy(seqs, gts, preds)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append((v_acc, cwa, swa, cxa))\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | acc={v_acc:.3f} cwa={cwa:.3f} swa={swa:.3f} cxa={cxa:.3f}\"\n    )\n\n# ---------- final save ----------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- plot ----------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nprint(\"Experiment finished \u2013 results saved in ./working\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\n# ------------- required working dir -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------- GPU/CPU handling -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------- helper: load SPR_BENCH -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if necessary\nif not DATA_PATH.exists():  # fallback tiny synthetic data\n    print(\"SPR_BENCH not found \u2013 creating tiny synthetic data.\")\n    os.makedirs(DATA_PATH, exist_ok=True)\n    for split, s in [(\"train\", 200), (\"dev\", 40), (\"test\", 40)]:\n        seqs, labels = [], []\n        shapes = [\"A\", \"B\", \"C\"]\n        colors = [\"1\", \"2\", \"3\"]\n        rng = np.random.default_rng(0)\n        for _ in range(s):\n            n = rng.integers(3, 7)\n            seq = \" \".join(rng.choice(shapes) + rng.choice(colors) for _ in range(n))\n            label = rng.choice([\"yes\", \"no\"])\n            seqs.append(seq)\n            labels.append(label)\n        import csv\n\n        with open(DATA_PATH / f\"{split}.csv\", \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i, (seq, lbl) in enumerate(zip(seqs, labels)):\n                w.writerow([f\"{split}_{i}\", seq, lbl])\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ------------- preprocessing: build vocab -------------\ndef parse_token(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshapes, colours = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colours.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncol2id = {c: i for i, c in enumerate(sorted(colours))}\nprint(\"Shapes:\", shape2id)\nprint(\"Colours:\", col2id)\n\n# label mapping\nall_labels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(all_labels)}\n\n\n# ------------- sequence -> graph -------------\ndef seq_to_graph(sequence, lbl):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = []\n    for tok in tokens:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(col2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + col2id[c]] = 1.0\n        x.append(vec)\n    x = torch.tensor(np.stack(x))\n    # edges between consecutive positions (undirected)\n    if n > 1:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2id[lbl]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef build_graph_dataset(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\n# ------------- Dataloaders -------------\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n# ------------- model -------------\nclass GCN(torch.nn.Module):\n    def __init__(self, in_dim, hid=64, num_classes=len(label2id)):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, hid)\n        self.conv2 = GCNConv(hid, hid)\n        self.lin = torch.nn.Linear(hid, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GCN(in_dim=len(shape2id) + len(col2id)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------- Complexity Weighted Accuracy -------------\ndef complexity_weight(seq):\n    toks = seq.split()\n    shapes = {t[0] for t in toks}\n    cols = {t[1:] if len(t) > 1 else \"0\" for t in toks}\n    return len(shapes) + len(cols)\n\n\ndef comp_weighted_accuracy(seqs, y_true, y_pred):\n    w = [complexity_weight(s) for s in seqs]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------- tracking dict -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------- training loop -------------\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tot_loss, tot_corr, tot_ex = 0.0, 0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        pred = out.argmax(dim=-1)\n        tot_corr += int((pred == batch.y).sum().item())\n        tot_ex += batch.num_graphs\n    tr_loss = tot_loss / tot_ex\n    tr_acc = tot_corr / tot_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_acc)\n\n    # ---- validation ----\n    model.eval()\n    v_loss, v_corr, v_ex = 0.0, 0, 0\n    all_pred, all_gt, all_seq = [], [], []\n    with torch.no_grad():\n        for batch, raw in zip(dev_loader, dsets[\"dev\"]):\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            pred = out.argmax(dim=-1).cpu()\n            v_corr += int((pred == batch.y.cpu()).sum().item())\n            v_ex += batch.num_graphs\n            all_pred.extend(pred.tolist())\n            all_gt.extend(batch.y.cpu().tolist())\n            all_seq.append(raw[\"sequence\"])\n    val_loss = v_loss / v_ex\n    val_acc = v_corr / v_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# ------------- final evaluation on dev for CompWA -------------\nseqs = [row[\"sequence\"] for row in dsets[\"dev\"]]\nmodel.eval()\npreds = []\nwith torch.no_grad():\n    for batch in dev_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.batch)\n        preds.extend(out.argmax(dim=-1).cpu().tolist())\ncompwa = comp_weighted_accuracy(\n    seqs, [label2id[r[\"label\"]] for r in dsets[\"dev\"]], preds\n)\nprint(f\"Complexity-Weighted Accuracy (dev): {compwa:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = [\n    label2id[r[\"label\"]] for r in dsets[\"dev\"]\n]\n\n# ------------- plot and save -------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Data & plot saved to ./working\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\n# ------------- required working dir -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------- GPU/CPU handling -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------- helper: load SPR_BENCH -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if necessary\nif not DATA_PATH.exists():  # fallback tiny synthetic data\n    print(\"SPR_BENCH not found \u2013 creating tiny synthetic data.\")\n    os.makedirs(DATA_PATH, exist_ok=True)\n    for split, s in [(\"train\", 200), (\"dev\", 40), (\"test\", 40)]:\n        seqs, labels = [], []\n        shapes = [\"A\", \"B\", \"C\"]\n        colors = [\"1\", \"2\", \"3\"]\n        rng = np.random.default_rng(0)\n        for _ in range(s):\n            n = rng.integers(3, 7)\n            seq = \" \".join(rng.choice(shapes) + rng.choice(colors) for _ in range(n))\n            label = rng.choice([\"yes\", \"no\"])\n            seqs.append(seq)\n            labels.append(label)\n        import csv\n\n        with open(DATA_PATH / f\"{split}.csv\", \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i, (seq, lbl) in enumerate(zip(seqs, labels)):\n                w.writerow([f\"{split}_{i}\", seq, lbl])\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ------------- preprocessing: build vocab -------------\ndef parse_token(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshapes, colours = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colours.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncol2id = {c: i for i, c in enumerate(sorted(colours))}\nprint(\"Shapes:\", shape2id)\nprint(\"Colours:\", col2id)\n\n# label mapping\nall_labels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(all_labels)}\n\n\n# ------------- sequence -> graph -------------\ndef seq_to_graph(sequence, lbl):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = []\n    for tok in tokens:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(col2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + col2id[c]] = 1.0\n        x.append(vec)\n    x = torch.tensor(np.stack(x))\n    # edges between consecutive positions (undirected)\n    if n > 1:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2id[lbl]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef build_graph_dataset(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\n# ------------- Dataloaders -------------\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n# ------------- model -------------\nclass GCN(torch.nn.Module):\n    def __init__(self, in_dim, hid=64, num_classes=len(label2id)):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, hid)\n        self.conv2 = GCNConv(hid, hid)\n        self.lin = torch.nn.Linear(hid, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GCN(in_dim=len(shape2id) + len(col2id)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------- Complexity Weighted Accuracy -------------\ndef complexity_weight(seq):\n    toks = seq.split()\n    shapes = {t[0] for t in toks}\n    cols = {t[1:] if len(t) > 1 else \"0\" for t in toks}\n    return len(shapes) + len(cols)\n\n\ndef comp_weighted_accuracy(seqs, y_true, y_pred):\n    w = [complexity_weight(s) for s in seqs]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------- tracking dict -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------- training loop -------------\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tot_loss, tot_corr, tot_ex = 0.0, 0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        pred = out.argmax(dim=-1)\n        tot_corr += int((pred == batch.y).sum().item())\n        tot_ex += batch.num_graphs\n    tr_loss = tot_loss / tot_ex\n    tr_acc = tot_corr / tot_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_acc)\n\n    # ---- validation ----\n    model.eval()\n    v_loss, v_corr, v_ex = 0.0, 0, 0\n    all_pred, all_gt, all_seq = [], [], []\n    with torch.no_grad():\n        for batch, raw in zip(dev_loader, dsets[\"dev\"]):\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            pred = out.argmax(dim=-1).cpu()\n            v_corr += int((pred == batch.y.cpu()).sum().item())\n            v_ex += batch.num_graphs\n            all_pred.extend(pred.tolist())\n            all_gt.extend(batch.y.cpu().tolist())\n            all_seq.append(raw[\"sequence\"])\n    val_loss = v_loss / v_ex\n    val_acc = v_corr / v_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# ------------- final evaluation on dev for CompWA -------------\nseqs = [row[\"sequence\"] for row in dsets[\"dev\"]]\nmodel.eval()\npreds = []\nwith torch.no_grad():\n    for batch in dev_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.batch)\n        preds.extend(out.argmax(dim=-1).cpu().tolist())\ncompwa = comp_weighted_accuracy(\n    seqs, [label2id[r[\"label\"]] for r in dsets[\"dev\"]], preds\n)\nprint(f\"Complexity-Weighted Accuracy (dev): {compwa:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = [\n    label2id[r[\"label\"]] for r in dsets[\"dev\"]\n]\n\n# ------------- plot and save -------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Data & plot saved to ./working\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, torch.nn.functional as F, matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\n# ------------- required working dir -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------- GPU/CPU handling -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------- helper: load SPR_BENCH -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if necessary\nif not DATA_PATH.exists():  # fallback tiny synthetic data\n    print(\"SPR_BENCH not found \u2013 creating tiny synthetic data.\")\n    os.makedirs(DATA_PATH, exist_ok=True)\n    for split, s in [(\"train\", 200), (\"dev\", 40), (\"test\", 40)]:\n        seqs, labels = [], []\n        shapes = [\"A\", \"B\", \"C\"]\n        colors = [\"1\", \"2\", \"3\"]\n        rng = np.random.default_rng(0)\n        for _ in range(s):\n            n = rng.integers(3, 7)\n            seq = \" \".join(rng.choice(shapes) + rng.choice(colors) for _ in range(n))\n            label = rng.choice([\"yes\", \"no\"])\n            seqs.append(seq)\n            labels.append(label)\n        import csv\n\n        with open(DATA_PATH / f\"{split}.csv\", \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i, (seq, lbl) in enumerate(zip(seqs, labels)):\n                w.writerow([f\"{split}_{i}\", seq, lbl])\n\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ------------- preprocessing: build vocab -------------\ndef parse_token(tok):\n    return tok[0], tok[1:] if len(tok) > 1 else \"0\"\n\n\nshapes, colours = set(), set()\nfor row in dsets[\"train\"]:\n    for tok in row[\"sequence\"].split():\n        s, c = parse_token(tok)\n        shapes.add(s)\n        colours.add(c)\nshape2id = {s: i for i, s in enumerate(sorted(shapes))}\ncol2id = {c: i for i, c in enumerate(sorted(colours))}\nprint(\"Shapes:\", shape2id)\nprint(\"Colours:\", col2id)\n\n# label mapping\nall_labels = sorted({row[\"label\"] for row in dsets[\"train\"]})\nlabel2id = {l: i for i, l in enumerate(all_labels)}\n\n\n# ------------- sequence -> graph -------------\ndef seq_to_graph(sequence, lbl):\n    tokens = sequence.split()\n    n = len(tokens)\n    x = []\n    for tok in tokens:\n        s, c = parse_token(tok)\n        vec = np.zeros(len(shape2id) + len(col2id), dtype=np.float32)\n        vec[shape2id[s]] = 1.0\n        vec[len(shape2id) + col2id[c]] = 1.0\n        x.append(vec)\n    x = torch.tensor(np.stack(x))\n    # edges between consecutive positions (undirected)\n    if n > 1:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2id[lbl]], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef build_graph_dataset(split):\n    return [seq_to_graph(r[\"sequence\"], r[\"label\"]) for r in dsets[split]]\n\n\ngraph_train = build_graph_dataset(\"train\")\ngraph_dev = build_graph_dataset(\"dev\")\ngraph_test = build_graph_dataset(\"test\")\n\n# ------------- Dataloaders -------------\ntrain_loader = DataLoader(graph_train, batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_dev, batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_test, batch_size=128, shuffle=False)\n\n\n# ------------- model -------------\nclass GCN(torch.nn.Module):\n    def __init__(self, in_dim, hid=64, num_classes=len(label2id)):\n        super().__init__()\n        self.conv1 = GCNConv(in_dim, hid)\n        self.conv2 = GCNConv(hid, hid)\n        self.lin = torch.nn.Linear(hid, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\n\nmodel = GCN(in_dim=len(shape2id) + len(col2id)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------- Complexity Weighted Accuracy -------------\ndef complexity_weight(seq):\n    toks = seq.split()\n    shapes = {t[0] for t in toks}\n    cols = {t[1:] if len(t) > 1 else \"0\" for t in toks}\n    return len(shapes) + len(cols)\n\n\ndef comp_weighted_accuracy(seqs, y_true, y_pred):\n    w = [complexity_weight(s) for s in seqs]\n    good = [wt if a == b else 0 for wt, a, b in zip(w, y_true, y_pred)]\n    return sum(good) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------- tracking dict -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------- training loop -------------\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    tot_loss, tot_corr, tot_ex = 0.0, 0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch)\n        loss = F.cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        pred = out.argmax(dim=-1)\n        tot_corr += int((pred == batch.y).sum().item())\n        tot_ex += batch.num_graphs\n    tr_loss = tot_loss / tot_ex\n    tr_acc = tot_corr / tot_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_acc)\n\n    # ---- validation ----\n    model.eval()\n    v_loss, v_corr, v_ex = 0.0, 0, 0\n    all_pred, all_gt, all_seq = [], [], []\n    with torch.no_grad():\n        for batch, raw in zip(dev_loader, dsets[\"dev\"]):\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = F.cross_entropy(out, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            pred = out.argmax(dim=-1).cpu()\n            v_corr += int((pred == batch.y.cpu()).sum().item())\n            v_ex += batch.num_graphs\n            all_pred.extend(pred.tolist())\n            all_gt.extend(batch.y.cpu().tolist())\n            all_seq.append(raw[\"sequence\"])\n    val_loss = v_loss / v_ex\n    val_acc = v_corr / v_ex\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# ------------- final evaluation on dev for CompWA -------------\nseqs = [row[\"sequence\"] for row in dsets[\"dev\"]]\nmodel.eval()\npreds = []\nwith torch.no_grad():\n    for batch in dev_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.batch)\n        preds.extend(out.argmax(dim=-1).cpu().tolist())\ncompwa = comp_weighted_accuracy(\n    seqs, [label2id[r[\"label\"]] for r in dsets[\"dev\"]], preds\n)\nprint(f\"Complexity-Weighted Accuracy (dev): {compwa:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = [\n    label2id[r[\"label\"]] for r in dsets[\"dev\"]\n]\n\n# ------------- plot and save -------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Cross-Entropy loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Data & plot saved to ./working\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 creating tiny synthetic\ndata.', '\\n', '\\rGenerating train split: 0 examples [00:00, ? examples/s]', '',\n'\\rGenerating train split: 200 examples [00:00, 50870.88 examples/s]', '\\n',\n'\\rGenerating train split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating\ntrain split: 40 examples [00:00, 24683.27 examples/s]', '\\n', '\\rGenerating\ntrain split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split:\n40 examples [00:00, 24925.29 examples/s]', '\\n', \"{'train': 200, 'dev': 40,\n'test': 40}\", '\\n', 'Shapes:', ' ', \"{'A': 0, 'B': 1, 'C': 2}\", '\\n',\n'Colours:', ' ', \"{'1': 0, '2': 1, '3': 2}\", '\\n', 'Epoch 1: validation_loss =\n0.6919', '\\n', 'Epoch 2: validation_loss = 0.6906', '\\n', 'Epoch 3:\nvalidation_loss = 0.6906', '\\n', 'Epoch 4: validation_loss = 0.6924', '\\n',\n'Epoch 5: validation_loss = 0.6932', '\\n', 'Epoch 6: validation_loss = 0.6943',\n'\\n', 'Epoch 7: validation_loss = 0.6950', '\\n', 'Epoch 8: validation_loss =\n0.6960', '\\n', 'Epoch 9: validation_loss = 0.6936', '\\n', 'Epoch 10:\nvalidation_loss = 0.6919', '\\n', 'Complexity-Weighted Accuracy (dev): 0.4925',\n'\\n', 'Data & plot saved to ./working', '\\n', 'Execution time: 3 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH folder not found, creating tiny\nsynthetic data for demo.', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 300 examples [00:00, 50990.44\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 60 examples [00:00, 24789.03\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 60 examples [00:00, 28365.45\nexamples/s]', '\\n', \"{'train': 300, 'dev': 60, 'test': 60}\", '\\n', 'Shapes', '\n', \"{'A': 0, 'B': 1, 'C': 2}\", '\\n', 'Colors', ' ', \"{'1': 0, '2': 1, '3': 2}\",\n'\\n', '  dev: loss=0.7035 acc=0.450 CWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch\n1: validation_loss = 0.7035 (time 0.3s)', '\\n', '  dev: loss=0.7021 acc=0.450\nCWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 2: validation_loss = 0.7021 (time\n0.0s)', '\\n', '  dev: loss=0.7082 acc=0.450 CWA=0.444 SWA=0.456 CXA=0.449',\n'\\n', 'Epoch 3: validation_loss = 0.7082 (time 0.0s)', '\\n', '  dev: loss=0.7052\nacc=0.450 CWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 4: validation_loss =\n0.7052 (time 0.0s)', '\\n', '  dev: loss=0.7065 acc=0.450 CWA=0.444 SWA=0.456\nCXA=0.449', '\\n', 'Epoch 5: validation_loss = 0.7065 (time 0.1s)', '\\n', '  dev:\nloss=0.7044 acc=0.450 CWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 6:\nvalidation_loss = 0.7044 (time 0.2s)', '\\n', '  dev: loss=0.7003 acc=0.450\nCWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 7: validation_loss = 0.7003 (time\n0.1s)', '\\n', '  dev: loss=0.7093 acc=0.450 CWA=0.444 SWA=0.456 CXA=0.449',\n'\\n', 'Epoch 8: validation_loss = 0.7093 (time 0.2s)', '\\n', '  dev: loss=0.7147\nacc=0.450 CWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 9: validation_loss =\n0.7147 (time 0.2s)', '\\n', '  dev: loss=0.6997 acc=0.433 CWA=0.424 SWA=0.435\nCXA=0.425', '\\n', 'Epoch 10: validation_loss = 0.6997 (time 0.2s)', '\\n', '\ndev: loss=0.6982 acc=0.450 CWA=0.437 SWA=0.449 CXA=0.436', '\\n', 'Epoch 11:\nvalidation_loss = 0.6982 (time 0.1s)', '\\n', '  dev: loss=0.7017 acc=0.450\nCWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 12: validation_loss = 0.7017 (time\n0.0s)', '\\n', '  dev: loss=0.7028 acc=0.450 CWA=0.444 SWA=0.456 CXA=0.449',\n'\\n', 'Epoch 13: validation_loss = 0.7028 (time 0.0s)', '\\n', '  dev:\nloss=0.7028 acc=0.450 CWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 14:\nvalidation_loss = 0.7028 (time 0.0s)', '\\n', '  dev: loss=0.7089 acc=0.450\nCWA=0.444 SWA=0.456 CXA=0.449', '\\n', 'Epoch 15: validation_loss = 0.7089 (time\n0.0s)', '\\n', '\\n==== Final evaluation on test split ====', '\\n', '  test:\nloss=0.7097 acc=0.467 CWA=0.465 SWA=0.468 CXA=0.465', '\\n', 'All metrics saved\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_21-49-\n50_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 writing tiny synthetic\ndata.', '\\n', '\\rGenerating train split: 0 examples [00:00, ? examples/s]', '',\n'\\rGenerating train split: 300 examples [00:00, 60602.57 examples/s]', '\\n',\n'\\rGenerating train split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating\ntrain split: 60 examples [00:00, 17505.44 examples/s]', '\\n', '\\rGenerating\ntrain split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split:\n60 examples [00:00, 26296.58 examples/s]', '\\n', \"{'train': 300, 'dev': 60,\n'test': 60}\", '\\n', 'Epoch 1: validation_loss = 0.7246 | val_acc=0.400 |\nval_CXA=0.398', '\\n', 'Epoch 2: validation_loss = 0.7135 | val_acc=0.433 |\nval_CXA=0.412', '\\n', 'Epoch 3: validation_loss = 0.7030 | val_acc=0.500 |\nval_CXA=0.473', '\\n', 'Epoch 4: validation_loss = 0.7207 | val_acc=0.417 |\nval_CXA=0.406', '\\n', 'Epoch 5: validation_loss = 0.7608 | val_acc=0.450 |\nval_CXA=0.449', '\\n', 'Epoch 6: validation_loss = 0.7369 | val_acc=0.450 |\nval_CXA=0.449', '\\n', 'Test  CWA=0.458  SWA=0.468  CXA=0.460', '\\n', 'Saved\nexperiment data to ./working', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 4, in <module>\\n\nfrom torch_geometric.nn import GCNConv, global_mean_pool, Dropout\\nImportError:\ncannot import name \\'Dropout\\' from \\'torch_geometric.nn\\'\n(/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch_geometric/nn/__init__.py)\\n', 'Execution time: 2 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset folder SPR_BENCH not found \u2013 abort.',\n'\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line 58, in\n<module>\\n    exit(0)\\n  File \"<frozen _sitebuiltins>\", line 26, in\n__call__\\nSystemExit: 0\\n', 'Execution time: 2 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 creating tiny synthetic data\nso the script can run.', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 101655.45\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 100 examples [00:00, 56081.08\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 100 examples [00:00, 60133.39\nexamples/s]', '\\n', \"{'train': 500, 'dev': 100, 'test': 100}\", '\\n', 'Shapes:',\n' ', \"{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\", '\\n', 'Colours:', ' ', \"{'1':\n0, '2': 1, '3': 2, '4': 3, '5': 4}\", '\\n', 'Labels:', ' ', \"{'no': 0, 'yes':\n1}\", '\\n', 'Epoch 1: validation_loss = 0.7144 | CXA = 0.4321', '\\n', 'Epoch 2:\nvalidation_loss = 0.6979 | CXA = 0.5142', '\\n', 'Epoch 3: validation_loss =\n0.6965 | CXA = 0.5352', '\\n', 'Epoch 4: validation_loss = 0.7356 | CXA =\n0.4742', '\\n', 'Epoch 5: validation_loss = 0.7020 | CXA = 0.5120', '\\n', 'Epoch\n6: validation_loss = 0.7375 | CXA = 0.4365', '\\n', 'Epoch 7: validation_loss =\n0.7230 | CXA = 0.5134', '\\n', 'Epoch 8: validation_loss = 0.7635 | CXA =\n0.4611', '\\n', 'Early stopping triggered.', '\\n', 'All artefacts saved to\n./working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-14', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 94, in <module>\\n    dsets = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 49, in load_spr_bench\\n\nout[split] = _load(f\"{split}.csv\")\\n                 ^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"runfile.py\", line 40, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_21-49-\n50_gnn_for_spr_attempt_0/0-run/process_ForkProcess-14/train.csv\\'\\n', 'Execution\ntime: 2 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 300, 'dev': 60, 'test': 60}\", '\\n',\n'Epoch 01: val_loss=0.7039  val_acc=0.3833  val_CXA=0.3984', '\\n', 'Epoch 02:\nval_loss=0.7112  val_acc=0.4333  val_CXA=0.4171', '\\n', 'Epoch 03:\nval_loss=0.7146  val_acc=0.4167  val_CXA=0.4091', '\\n', 'Epoch 04:\nval_loss=0.7063  val_acc=0.4500  val_CXA=0.5000', '\\n', 'Epoch 05:\nval_loss=0.7039  val_acc=0.4500  val_CXA=0.5000', '\\n', 'Epoch 06:\nval_loss=0.7021  val_acc=0.4833  val_CXA=0.5428', '\\n', 'Epoch 07:\nval_loss=0.7017  val_acc=0.5000  val_CXA=0.5588', '\\n', 'Epoch 08:\nval_loss=0.7076  val_acc=0.4667  val_CXA=0.5053', '\\n', 'Epoch 09:\nval_loss=0.7130  val_acc=0.4333  val_CXA=0.5000', '\\n', 'Epoch 10:\nval_loss=0.7097  val_acc=0.4500  val_CXA=0.4893', '\\n', 'Epoch 11:\nval_loss=0.7090  val_acc=0.5000  val_CXA=0.5535', '\\n', 'Epoch 12:\nval_loss=0.7107  val_acc=0.4667  val_CXA=0.5214', '\\n', 'Epoch 13:\nval_loss=0.7118  val_acc=0.4833  val_CXA=0.5374', '\\n', 'Epoch 14:\nval_loss=0.7137  val_acc=0.5167  val_CXA=0.5642', '\\n', 'Epoch 15:\nval_loss=0.7133  val_acc=0.5000  val_CXA=0.5615', '\\n', 'TEST  CWA=0.4065\nSWA=0.4359  CXA=0.4083', '\\n', 'All metrics saved to', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working', '\\n', 'Execution time: 3 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 300, 'dev': 60,\n'test': 60}\", '\\n', 'Epoch 1: validation_loss = 0.7028 | acc=0.450 cwa=0.444\nswa=0.456 cxa=0.449', '\\n', 'Epoch 2: validation_loss = 0.7037 | acc=0.450\ncwa=0.444 swa=0.456 cxa=0.449', '\\n', 'Epoch 3: validation_loss = 0.7034 |\nacc=0.450 cwa=0.444 swa=0.456 cxa=0.449', '\\n', 'Epoch 4: validation_loss =\n0.7036 | acc=0.450 cwa=0.444 swa=0.456 cxa=0.449', '\\n', 'Epoch 5:\nvalidation_loss = 0.7047 | acc=0.450 cwa=0.444 swa=0.456 cxa=0.449', '\\n',\n'Experiment finished \u2013 results saved in ./working', '\\n', 'Execution time: 3\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 300, 'dev': 60, 'test': 60}\", '\\n',\n'Shapes:', ' ', \"{'A': 0, 'B': 1, 'C': 2}\", '\\n', 'Colours:', ' ', \"{'1': 0,\n'2': 1, '3': 2}\", '\\n', 'Epoch 1: validation_loss = 0.6921', '\\n', 'Epoch 2:\nvalidation_loss = 0.6973', '\\n', 'Epoch 3: validation_loss = 0.7038', '\\n',\n'Epoch 4: validation_loss = 0.7092', '\\n', 'Epoch 5: validation_loss = 0.7098',\n'\\n', 'Epoch 6: validation_loss = 0.7104', '\\n', 'Epoch 7: validation_loss =\n0.7093', '\\n', 'Epoch 8: validation_loss = 0.7085', '\\n', 'Epoch 9:\nvalidation_loss = 0.7074', '\\n', 'Epoch 10: validation_loss = 0.7072', '\\n',\n'Complexity-Weighted Accuracy (dev): 0.4497', '\\n', 'Data & plot saved to\n./working', '\\n', 'Execution time: 5 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 creating tiny synthetic\ndata.', '\\n', '\\rGenerating train split: 0 examples [00:00, ? examples/s]', '',\n'\\rGenerating train split: 200 examples [00:00, 48246.44 examples/s]', '\\n',\n'\\rGenerating train split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating\ntrain split: 40 examples [00:00, 20942.72 examples/s]', '\\n', '\\rGenerating\ntrain split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split:\n40 examples [00:00, 22209.71 examples/s]', '\\n', \"{'train': 200, 'dev': 40,\n'test': 40}\", '\\n', 'Shapes:', ' ', \"{'A': 0, 'B': 1, 'C': 2}\", '\\n',\n'Colours:', ' ', \"{'1': 0, '2': 1, '3': 2}\", '\\n', 'Epoch 1: validation_loss =\n0.6983', '\\n', 'Epoch 2: validation_loss = 0.6973', '\\n', 'Epoch 3:\nvalidation_loss = 0.6953', '\\n', 'Epoch 4: validation_loss = 0.6935', '\\n',\n'Epoch 5: validation_loss = 0.6910', '\\n', 'Epoch 6: validation_loss = 0.6900',\n'\\n', 'Epoch 7: validation_loss = 0.6900', '\\n', 'Epoch 8: validation_loss =\n0.6900', '\\n', 'Epoch 9: validation_loss = 0.6878', '\\n', 'Epoch 10:\nvalidation_loss = 0.6859', '\\n', 'Complexity-Weighted Accuracy (dev): 0.4925',\n'\\n', 'Data & plot saved to ./working', '\\n', 'Execution time: 4 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 300, 'dev': 60, 'test': 60}\", '\\n',\n'Shapes:', ' ', \"{'A': 0, 'B': 1, 'C': 2}\", '\\n', 'Colours:', ' ', \"{'1': 0,\n'2': 1, '3': 2}\", '\\n', 'Epoch 1: validation_loss = 0.6909', '\\n', 'Epoch 2:\nvalidation_loss = 0.6936', '\\n', 'Epoch 3: validation_loss = 0.6985', '\\n',\n'Epoch 4: validation_loss = 0.7052', '\\n', 'Epoch 5: validation_loss = 0.7093',\n'\\n', 'Epoch 6: validation_loss = 0.7106', '\\n', 'Epoch 7: validation_loss =\n0.7108', '\\n', 'Epoch 8: validation_loss = 0.7094', '\\n', 'Epoch 9:\nvalidation_loss = 0.7087', '\\n', 'Epoch 10: validation_loss = 0.7085', '\\n',\n'Complexity-Weighted Accuracy (dev): 0.4497', '\\n', 'Data & plot saved to\n./working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", ""], "analysis": ["", "The model's performance on the validation and test datasets did not improve over\nthe course of training. The accuracy, Color-Weighted Accuracy (CWA), and Shape-\nWeighted Accuracy (SWA) remained stagnant or showed minimal fluctuations,\nhovering around 45% for accuracy and similar values for CWA and SWA. This\nindicates that the model has failed to learn meaningful patterns from the data.\nPossible reasons for this could include: 1. The synthetic dataset generated for\nthe experiment might not be complex enough to allow the model to learn\neffectively. 2. The model architecture or hyperparameters may not be suitable\nfor the task.  Proposed fixes: 1. Use the original SPR_BENCH dataset instead of\nthe synthetic dataset to ensure the model is trained on data with meaningful\npatterns. 2. Experiment with different hyperparameters, such as learning rate,\nnumber of layers, and hidden units. 3. Consider alternative GNN architectures,\nsuch as Graph Attention Networks (GAT) or more advanced variants. 4. Ensure the\nsynthetic dataset generation process creates more complex and diverse sequences\nto mimic the real-world data distribution.", "", "The script encountered an ImportError because 'Dropout' cannot be imported from\n'torch_geometric.nn'. This is likely due to the fact that 'Dropout' is not a\nvalid module within 'torch_geometric.nn'. To fix this, replace 'Dropout' with\n'torch.nn.Dropout', which is the correct dropout module for PyTorch-based\nmodels. Update the import statement and the model definition accordingly.", "The execution failed because the dataset folder 'SPR_BENCH' was not found in the\nexpected location. This caused the script to abort early without performing any\ntraining or evaluation.   Proposed Fix: 1. Ensure the dataset 'SPR_BENCH' is\ncorrectly downloaded and placed in the expected directory ('./SPR_BENCH'). 2.\nVerify the directory path in the code to ensure it matches the actual location\nof the dataset. 3. Add a more descriptive error message or a fallback mechanism\nto allow users to specify the dataset path dynamically if it is not found in the\ndefault location.", "", "The execution failed due to a FileNotFoundError. The script could not locate the\nrequired dataset file 'train.csv' in the specified directory\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_21-49-\n50_gnn_for_spr_attempt_0/0-run/process_ForkProcess-14/'.  Proposed Fix: 1.\nVerify that the dataset files (train.csv, dev.csv, test.csv) are placed in the\ncorrect directory. 2. Ensure the DATA_PATH variable is correctly pointing to the\ndataset directory. 3. If the dataset directory is dynamically set, confirm that\nthe environment variable $SPR_BENCH_DIR or other search paths contain the\ncorrect dataset location. 4. Add additional logging to output the resolved\ndataset path for easier debugging.", "The execution output indicates that the model's performance on the test dataset\nis significantly below the SOTA benchmarks for the SPR_BENCH dataset.\nSpecifically, the Color-Weighted Accuracy (CWA) is 40.65% compared to the SOTA\nof 65.0%, and the Shape-Weighted Accuracy (SWA) is 43.59% compared to the SOTA\nof 70.0%. This performance gap suggests that the model architecture, training\nprocess, or dataset preprocessing may have issues. Potential fixes include: 1)\nReviewing the GNN architecture to ensure it is capable of capturing the\nnecessary relational information, 2) Experimenting with hyperparameter tuning,\nincluding learning rate, dropout, and hidden layer dimensions, 3) Investigating\nwhether the dataset preprocessing or graph construction introduces errors or\ninadequately represents the sequences, and 4) Extending the training period or\nusing a more robust optimization strategy to improve convergence.", "The model's performance metrics (accuracy, CWA, SWA, and CXA) remain stagnant at\nlow values (around 0.450) throughout all epochs, indicating that the model is\nnot learning. This suggests a potential bug or issue in the training process.\nPossible reasons include insufficient training data (only 300 samples in the\ntraining set), inadequate model complexity, or a learning rate that is too low\nor high. To fix this, consider increasing the size of the training dataset,\nexperimenting with different learning rates, adding more layers or neurons to\nthe model, or using a pre-trained model as a starting point.", "The model's validation loss does not improve significantly over the epochs, and\nthe final Complexity-Weighted Accuracy (CWA) on the development set is only\n44.97%, which is much lower than the SOTA benchmark of 65.0% for CWA. This\nindicates that the model is not learning effectively. Potential fixes include:\n1. **Hyperparameter Tuning**: Experiment with learning rates, hidden dimensions,\nor batch sizes to improve learning. 2. **Model Architecture**: Add more layers\nor use advanced GNN variants like Graph Attention Networks (GATs) to better\ncapture relationships. 3. **Data Augmentation**: Increase the size and variety\nof the dataset to improve generalization. 4. **Edge Features**: Incorporate edge\nfeatures to better represent relationships between nodes. 5. **Training\nEpochs**: Train for more epochs to allow the model to converge.", "The execution output indicates that the training script ran successfully without\nany bugs or errors. The model was trained for 10 epochs, and the validation loss\ndecreased steadily over the epochs. However, the Complexity-Weighted Accuracy\n(CompWA) on the development set was 0.4925, which is below the SOTA benchmarks\n(CWA: 65.0%, SWA: 70.0%). This suggests that while the script is functioning\ncorrectly, the model's performance is suboptimal and further experimentation or\noptimization is needed to improve accuracy.", "The validation loss increases over epochs, indicating that the model is not\nlearning effectively and may be overfitting to the training data. Additionally,\nthe achieved Complexity-Weighted Accuracy (CWA) on the dev set is significantly\nlower (0.4497) than the SOTA benchmark (CWA: 65.0%). This suggests that the\nmodel design or training process is not adequately capturing the patterns in the\ndata.  Proposed Fix: 1. Investigate the model architecture: Experiment with\ndifferent GNN architectures or add more layers to improve the model's capacity.\n2. Adjust hyperparameters: Tune learning rate, hidden dimensions, and batch size\nto improve optimization. 3. Data augmentation: Generate additional training\nsamples or apply transformations to improve generalization. 4. Regularization\ntechniques: Apply dropout or L2 regularization to prevent overfitting. 5. Edge\nfeatures: Include edge features in the graph representation to better capture\nrelationships between nodes. 6. Curriculum learning: Start training on simpler\nexamples and gradually move to more complex ones.", ""], "exc_type": [null, null, null, "ImportError", "SystemExit", null, "FileNotFoundError", null, null, null, null, null, null], "exc_info": [null, null, null, {"args": ["cannot import name 'Dropout' from 'torch_geometric.nn' (/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch_geometric/nn/__init__.py)"], "name": "torch_geometric.nn", "msg": "cannot import name 'Dropout' from 'torch_geometric.nn' (/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch_geometric/nn/__init__.py)"}, {"args": ["0"]}, null, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/0-run/process_ForkProcess-14/train.csv'"]}, null, null, null, null, null, null], "exc_stack": [null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 4, "<module>", "from torch_geometric.nn import GCNConv, global_mean_pool, Dropout"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 58, "<module>", "exit(0)"], ["<frozen _sitebuiltins>", 26, "__call__", ""]], null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 94, "<module>", "dsets = load_spr_bench(DATA_PATH)"], ["runfile.py", 49, "load_spr_bench", "out[split] = _load(f\"{split}.csv\")"], ["runfile.py", 40, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.58, "best_value": 0.58}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.65, "best_value": 0.65}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.678, "best_value": 0.678}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6906, "best_value": 0.6906}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.55, "best_value": 0.55}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.45, "best_value": 0.45}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6847, "best_value": 0.6847}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6982, "best_value": 0.6982}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4667, "best_value": 0.4667}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5467, "best_value": 0.5467}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6946, "best_value": 0.6946}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.703, "best_value": 0.703}]}, {"metric_name": "training complexity-weighted accuracy", "lower_is_better": false, "description": "Complexity-weighted accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5451, "best_value": 0.5451}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "Complexity-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4733, "best_value": 0.4733}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6213, "best_value": 0.6213}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7635, "best_value": 0.7635}]}, {"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.634, "best_value": 0.634}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.47, "best_value": 0.47}]}, {"metric_name": "training color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6405, "best_value": 0.6405}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.469, "best_value": 0.469}]}, {"metric_name": "training shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6306, "best_value": 0.6306}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4617, "best_value": 0.4617}]}, {"metric_name": "training complexity-weighted accuracy", "lower_is_better": false, "description": "The complexity-weighted accuracy value on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6356, "best_value": 0.6356}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "The complexity-weighted accuracy value on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4611, "best_value": 0.4611}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified samples.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4333, "best_value": 0.4333}]}, {"metric_name": "Cxa", "lower_is_better": false, "description": "A custom metric provided in the output.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5642, "best_value": 0.5642}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or difference between predicted and true values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7017, "best_value": 0.7017}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5367, "best_value": 0.5367}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.45, "best_value": 0.45}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4437, "best_value": 0.4437}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4558, "best_value": 0.4558}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "The complexity-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4492, "best_value": 0.4492}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6894, "best_value": 0.6894}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7028, "best_value": 0.7028}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5533, "best_value": 0.5533}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5667, "best_value": 0.5667}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6877, "best_value": 0.6877}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6921, "best_value": 0.6921}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.565, "best_value": 0.565}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6756, "best_value": 0.6756}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6859, "best_value": 0.6859}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.54, "best_value": 0.54}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.55, "best_value": 0.55}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6883, "best_value": 0.6883}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6909, "best_value": 0.6909}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/loss_curve.png", "../../logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_CXA_curve.png", "../../logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_conf_matrix.png"], [], [], ["../../logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/loss_curve.png", "../../logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/cxa_curve.png", "../../logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_cxa_curve.png", "../../logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_confusion_matrix.png"], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/loss_curve.png", "../../logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/seed_aggregation_6094c9dd92ed4c53b6e3f41ef7b1862c/SPR_BENCH_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_6094c9dd92ed4c53b6e3f41ef7b1862c/SPR_BENCH_aggregated_accuracy_curve.png"]], "plot_paths": [["experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_CXA_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_conf_matrix.png"], [], [], ["experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/cxa_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_cxa_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_confusion_matrix.png"], [], [], [], [], ["experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_accuracy_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_6094c9dd92ed4c53b6e3f41ef7b1862c/SPR_BENCH_aggregated_loss_curve.png", "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_6094c9dd92ed4c53b6e3f41ef7b1862c/SPR_BENCH_aggregated_accuracy_curve.png"]], "plot_analyses": [[{"analysis": "This plot shows the Cross-Entropy loss for both the training and validation datasets over several epochs. The training loss decreases steadily, indicating that the model is learning from the training data. However, the validation loss plateaus and even increases slightly after a few epochs, which may suggest overfitting. The model might be learning the training data too well at the expense of generalizability to unseen data.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/loss_curve.png"}, {"analysis": "This plot reiterates the Cross-Entropy loss trends for the training and validation datasets. The training loss decreases continuously, while the validation loss initially decreases but then shows signs of stagnation or increase. This observation highlights potential overfitting and suggests the need for regularization techniques or adjustments to the model architecture or training process.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot depicts the accuracy for both the training and validation datasets over epochs. The training accuracy stabilizes at a moderate level, while the validation accuracy peaks early and then drops significantly, remaining constant afterward. This behavior is a clear sign of overfitting, as the model fails to maintain its performance on validation data after initial improvements.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_accuracy_curve.png"}, {"analysis": "This confusion matrix indicates that the model's predictions are highly skewed. It predominantly predicts one class (c0) while neglecting the other class (c1). This imbalance in predictions suggests that the model is biased towards the majority class or has not learned to distinguish between the classes effectively. Addressing class imbalance in the training data or modifying the loss function might be necessary to improve performance.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2e530e6554fd432c9557ba4fa368902d_proc_1509391/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The loss curve shows that the training loss decreases slightly, indicating some learning progress. However, the validation loss fluctuates significantly, suggesting potential overfitting or instability in the model's learning process. The divergence between training and validation loss at later epochs requires further investigation, such as hyperparameter tuning or regularization.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_loss_curve.png"}, {"analysis": "The accuracy curve indicates that the training accuracy improves steadily, demonstrating that the model is learning from the training data. In contrast, the validation accuracy shows a sharp decline after an initial improvement, pointing to overfitting. This trend highlights the need for better generalization strategies, such as dropout or early stopping.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_accuracy_curve.png"}, {"analysis": "The complexity-weighted accuracy curve (CXA) reveals a similar pattern to the standard accuracy curve. Training CXA remains relatively stable with slight improvements, while validation CXA exhibits a sharp drop after an initial rise. This suggests that the model struggles with generalizing to more complex sequences in the validation set.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_CXA_curve.png"}, {"analysis": "The confusion matrix shows that the model performs better on one class compared to the other, as indicated by the imbalance in correct predictions. This could suggest a bias in the model or class imbalance in the dataset. Addressing this might involve rebalancing the dataset or modifying the loss function to account for class imbalance.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8e26800dcb4040669a85671b529474a4_proc_1515241/SPR_BENCH_conf_matrix.png"}], [], [], [{"analysis": "The cross-entropy loss plot shows a steady decrease in training loss over the epochs, indicating that the model is learning effectively on the training data. However, the validation loss exhibits fluctuations and a general upward trend, suggesting potential overfitting or issues with generalization to unseen data.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/loss_curve.png"}, {"analysis": "The complexity-weighted accuracy (CWA) plot indicates that training accuracy steadily improves over epochs, while validation accuracy fluctuates significantly without a clear upward trend. This discrepancy suggests that the model struggles to generalize and may not effectively capture the complexity of the validation sequences.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/cxa_curve.png"}, {"analysis": "The cross-entropy loss plot for the SPR_BENCH dataset corroborates the earlier observation of effective learning on the training set but poor generalization to the validation set. The validation loss increases over time, which may indicate overfitting.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_loss_curve.png"}, {"analysis": "The accuracy plot for the SPR_BENCH dataset shows an improvement in training accuracy over epochs, but the validation accuracy remains low and inconsistent. This reinforces the conclusion that the model is overfitting and not generalizing well to the validation data.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_accuracy_curve.png"}, {"analysis": "The complexity-weighted accuracy (CWA) plot for SPR_BENCH reflects a similar trend as the accuracy plot, with training CWA improving while validation CWA remains low and erratic. This suggests that the model may not be capturing the nuanced relationships required for high CWA on the validation set.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_cxa_curve.png"}, {"analysis": "The confusion matrix reveals that the model performs better at predicting one class (c0) compared to the other (c1). This imbalance may be contributing to the low validation performance and suggests a need for further investigation into class distribution and model bias.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6f8c73189a114025b94de74155908713_proc_1515242/SPR_BENCH_confusion_matrix.png"}], [], [], [], [], [{"analysis": "This plot shows the cross-entropy loss for both the training and validation sets over 10 epochs. The training loss decreases consistently, indicating that the model is learning from the training data. However, the validation loss also decreases but at a slower rate, suggesting potential underfitting or insufficient model complexity to capture the validation data's structure fully.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/loss_curve.png"}, {"analysis": "This plot also depicts the cross-entropy loss for training and validation sets, similar to the earlier one. The trend remains consistent, with the training loss decreasing more rapidly than the validation loss. This may indicate a gap between training and validation performance, warranting further investigation into regularization or model architecture adjustments.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_loss_curve.png"}, {"analysis": "This accuracy curve shows that the training accuracy remains constant at approximately 56%, while the validation accuracy stays fixed at 50% across all epochs. This lack of improvement in accuracy suggests that the model is not generalizing well to the validation set and may be stuck at a baseline performance level, such as random guessing.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_accuracy_curve.png"}, {"analysis": "The confusion matrix indicates that the model predominantly predicts one class (c0) while ignoring the other (c1), leading to a severe class imbalance in predictions. This suggests that the model is biased towards the majority class and fails to capture meaningful patterns for the minority class. Addressing this imbalance is critical for improving model performance.", "plot_path": "experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/SPR_BENCH_confusion_matrix.png"}], [], []], "vlm_feedback_summary": ["The plots indicate that the model is overfitting, as evidenced by the divergence\nbetween training and validation performance. Additionally, the confusion matrix\nhighlights a significant bias in predictions, suggesting the need for strategies\nto address class imbalance and improve generalization.", "[]", "The provided plots highlight issues with overfitting, generalization, and\npotential class imbalance in the model. While the training metrics improve, the\nvalidation metrics decline or fluctuate, suggesting the need for strategies to\nimprove validation performance and generalization. The confusion matrix points\nto class-specific performance disparities that require attention.", "[]", "[]", "The provided plots indicate that the model is learning effectively on the\ntraining data but faces challenges in generalization, as evidenced by\nfluctuating and low validation performance across metrics. Overfitting and\npotential class imbalance are key issues that need to be addressed.", "[]", "[]", "[]", "[]", "The provided plots reveal that the model struggles to generalize effectively,\nwith training and validation losses decreasing but accuracy metrics showing no\nimprovement. The confusion matrix highlights a significant class imbalance\nissue, suggesting the need for rebalancing techniques or architectural changes\nto achieve better performance.", "[]", "[]"], "exec_time": [3.6375837326049805, 4.319355010986328, 4.503755569458008, 2.183274030685425, 2.158717632293701, 4.212554216384888, 2.500582218170166, 3.737630605697632, 3.6157102584838867, 5.8471574783325195, 4.2553627490997314, 4.026163101196289, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], [], ["[]"], [], [], ["[]"], [], [], [], [], ["[]"], [], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_content in experiment_data.items():\n    losses = ds_content.get(\"losses\", {})\n    metrics = ds_content.get(\"metrics\", {})\n    preds = np.array(ds_content.get(\"predictions\", []))\n    gts = np.array(ds_content.get(\"ground_truth\", []))\n\n    # --------- plot 1: loss curves ---------\n    try:\n        plt.figure()\n        if \"train\" in losses and losses[\"train\"]:\n            plt.plot(losses[\"train\"], label=\"Train\")\n        if \"val\" in losses and losses[\"val\"]:\n            plt.plot(losses[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = f\"{ds_name}_loss_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 2: accuracy curves ---------\n    try:\n        plt.figure()\n        if \"train\" in metrics and metrics[\"train\"]:\n            plt.plot(metrics[\"train\"], label=\"Train\")\n        if \"val\" in metrics and metrics[\"val\"]:\n            plt.plot(metrics[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 3: confusion matrix ---------\n    try:\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for g, p in zip(gts, preds):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks, [f\"c{i}\" for i in ticks])\n            plt.yticks(ticks, [f\"c{i}\" for i in ticks])\n            fname = f\"{ds_name}_confusion_matrix.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\n                f\"Skipping confusion matrix for {ds_name}: empty predictions or ground truth.\"\n            )\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- print summary metric ----------\n    if metrics.get(\"val\"):\n        print(f'{ds_name} final validation accuracy: {metrics[\"val\"][-1]:.4f}')\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# store final val accuracies for optional comparison\nval_acc_summary = {}\n\nfor ds_name, ds in experiment_data.items():\n    losses = ds.get(\"losses\", {})\n    metrics = ds.get(\"metrics\", {})\n    preds = np.asarray(ds.get(\"predictions\", []))\n    gts = np.asarray(ds.get(\"ground_truth\", []))\n\n    # --------- plot 1: loss curves ---------\n    try:\n        plt.figure()\n        if losses.get(\"train\"):\n            plt.plot(losses[\"train\"], label=\"Train\")\n        if losses.get(\"val\"):\n            plt.plot(losses[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 2: accuracy curves ---------\n    try:\n        plt.figure()\n        if metrics.get(\"train_acc\"):\n            plt.plot(metrics[\"train_acc\"], label=\"Train\")\n        if metrics.get(\"val_acc\"):\n            plt.plot(metrics[\"val_acc\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_accuracy_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 3: CXA curves ---------\n    try:\n        if metrics.get(\"train_cxa\") or metrics.get(\"val_cxa\"):\n            plt.figure()\n            if metrics.get(\"train_cxa\"):\n                plt.plot(metrics[\"train_cxa\"], label=\"Train CXA\")\n            if metrics.get(\"val_cxa\"):\n                plt.plot(metrics[\"val_cxa\"], label=\"Validation CXA\")\n            plt.title(f\"{ds_name} CXA Curve\\nLeft: Train, Right: Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Complexity-weighted Accuracy\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_CXA_curve.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CXA curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 4: confusion matrix ---------\n    try:\n        if preds.size and gts.size:\n            num_c = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_c, num_c), int)\n            for g, p in zip(gts, preds):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            ticks = np.arange(num_c)\n            plt.xticks(ticks, ticks)\n            plt.yticks(ticks, ticks)\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n        else:\n            print(\n                f\"Skipping confusion matrix for {ds_name}: empty predictions or labels.\"\n            )\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # record final val accuracy for comparison\n    if metrics.get(\"val_acc\"):\n        val_acc_summary[ds_name] = metrics[\"val_acc\"][-1]\n\n# --------- comparison bar plot across datasets ---------\ntry:\n    if len(val_acc_summary) > 1:\n        plt.figure()\n        names, vals = zip(*val_acc_summary.items())\n        plt.bar(names, vals)\n        plt.ylabel(\"Final Validation Accuracy\")\n        plt.title(\"Dataset Comparison of Final Validation Accuracy\")\n        plt.savefig(os.path.join(working_dir, \"datasets_val_accuracy_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating dataset comparison plot: {e}\")\n    plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_cxa = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_content in experiment_data.items():\n    losses = ds_content.get(\"losses\", {})\n    metrics = ds_content.get(\"metrics\", {})\n    preds = np.array(ds_content.get(\"predictions\", []))\n    gts = np.array(ds_content.get(\"ground_truth\", []))\n\n    # --------- plot 1: loss curves ---------\n    try:\n        plt.figure()\n        if losses.get(\"train\"):\n            plt.plot(losses[\"train\"], label=\"Train\")\n        if losses.get(\"val\"):\n            plt.plot(losses[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Cross-Entropy Loss\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = f\"{ds_name}_loss_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- helper: extract metric array ---------\n    def m_arr(split, key):\n        return [d[key] for d in metrics.get(split, [])]\n\n    # --------- plot 2: accuracy curves ---------\n    try:\n        plt.figure()\n        if metrics.get(\"train\"):\n            plt.plot(m_arr(\"train\", \"acc\"), label=\"Train\")\n        if metrics.get(\"val\"):\n            plt.plot(m_arr(\"val\", \"acc\"), label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 3: CXA curves ---------\n    try:\n        plt.figure()\n        if metrics.get(\"train\"):\n            plt.plot(m_arr(\"train\", \"cxa\"), label=\"Train\")\n        if metrics.get(\"val\"):\n            plt.plot(m_arr(\"val\", \"cxa\"), label=\"Validation\")\n        plt.title(\n            f\"{ds_name} Complexity-Weighted Accuracy\\nLeft: Train, Right: Validation\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CXA\")\n        plt.legend()\n        fname = f\"{ds_name}_cxa_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CXA curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 4: confusion matrix ---------\n    try:\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for g, p in zip(gts, preds):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks, [f\"c{i}\" for i in ticks])\n            plt.yticks(ticks, [f\"c{i}\" for i in ticks])\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\"))\n            plt.close()\n        else:\n            print(\n                f\"Skipping confusion matrix for {ds_name}: empty predictions or ground truth.\"\n            )\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- print summary metrics ----------\n    if metrics.get(\"val\"):\n        final_acc = m_arr(\"val\", \"acc\")[-1]\n        final_cxa_val = m_arr(\"val\", \"cxa\")[-1]\n        final_cxa[ds_name] = final_cxa_val\n        print(\n            f\"{ds_name} final validation accuracy: {final_acc:.4f}, CXA: {final_cxa_val:.4f}\"\n        )\n\n# ---------- comparison plot across datasets ----------\nif len(final_cxa) > 1:\n    try:\n        plt.figure()\n        names, cxa_vals = zip(*final_cxa.items())\n        plt.bar(names, cxa_vals, color=\"steelblue\")\n        plt.title(\"Final Validation CXA Comparison\\nDataset-wise Performance\")\n        plt.ylabel(\"CXA\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"dataset_cxa_comparison.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating dataset comparison plot: {e}\")\n        plt.close()\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_content in experiment_data.items():\n    losses = ds_content.get(\"losses\", {})\n    metrics = ds_content.get(\"metrics\", {})\n    preds = np.array(ds_content.get(\"predictions\", []))\n    gts = np.array(ds_content.get(\"ground_truth\", []))\n\n    # --------- plot 1: loss curves ---------\n    try:\n        plt.figure()\n        if \"train\" in losses and losses[\"train\"]:\n            plt.plot(losses[\"train\"], label=\"Train\")\n        if \"val\" in losses and losses[\"val\"]:\n            plt.plot(losses[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = f\"{ds_name}_loss_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 2: accuracy curves ---------\n    try:\n        plt.figure()\n        if \"train\" in metrics and metrics[\"train\"]:\n            plt.plot(metrics[\"train\"], label=\"Train\")\n        if \"val\" in metrics and metrics[\"val\"]:\n            plt.plot(metrics[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 3: confusion matrix ---------\n    try:\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for g, p in zip(gts, preds):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks, [f\"c{i}\" for i in ticks])\n            plt.yticks(ticks, [f\"c{i}\" for i in ticks])\n            fname = f\"{ds_name}_confusion_matrix.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\n                f\"Skipping confusion matrix for {ds_name}: empty predictions or ground truth.\"\n            )\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- print summary metric ----------\n    if metrics.get(\"val\"):\n        print(f'{ds_name} final validation accuracy: {metrics[\"val\"][-1]:.4f}')\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_content in experiment_data.items():\n    losses = ds_content.get(\"losses\", {})\n    metrics = ds_content.get(\"metrics\", {})\n    preds = np.array(ds_content.get(\"predictions\", []))\n    gts = np.array(ds_content.get(\"ground_truth\", []))\n\n    # --------- plot 1: loss curves ---------\n    try:\n        plt.figure()\n        if \"train\" in losses and losses[\"train\"]:\n            plt.plot(losses[\"train\"], label=\"Train\")\n        if \"val\" in losses and losses[\"val\"]:\n            plt.plot(losses[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = f\"{ds_name}_loss_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 2: accuracy curves ---------\n    try:\n        plt.figure()\n        if \"train\" in metrics and metrics[\"train\"]:\n            plt.plot(metrics[\"train\"], label=\"Train\")\n        if \"val\" in metrics and metrics[\"val\"]:\n            plt.plot(metrics[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 3: confusion matrix ---------\n    try:\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for g, p in zip(gts, preds):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks, [f\"c{i}\" for i in ticks])\n            plt.yticks(ticks, [f\"c{i}\" for i in ticks])\n            fname = f\"{ds_name}_confusion_matrix.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\n                f\"Skipping confusion matrix for {ds_name}: empty predictions or ground truth.\"\n            )\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- print summary metric ----------\n    if metrics.get(\"val\"):\n        print(f'{ds_name} final validation accuracy: {metrics[\"val\"][-1]:.4f}')\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_content in experiment_data.items():\n    losses = ds_content.get(\"losses\", {})\n    metrics = ds_content.get(\"metrics\", {})\n    preds = np.array(ds_content.get(\"predictions\", []))\n    gts = np.array(ds_content.get(\"ground_truth\", []))\n\n    # --------- plot 1: loss curves ---------\n    try:\n        plt.figure()\n        if \"train\" in losses and losses[\"train\"]:\n            plt.plot(losses[\"train\"], label=\"Train\")\n        if \"val\" in losses and losses[\"val\"]:\n            plt.plot(losses[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Loss Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = f\"{ds_name}_loss_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 2: accuracy curves ---------\n    try:\n        plt.figure()\n        if \"train\" in metrics and metrics[\"train\"]:\n            plt.plot(metrics[\"train\"], label=\"Train\")\n        if \"val\" in metrics and metrics[\"val\"]:\n            plt.plot(metrics[\"val\"], label=\"Validation\")\n        plt.title(f\"{ds_name} Accuracy Curve\\nLeft: Train, Right: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- plot 3: confusion matrix ---------\n    try:\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for g, p in zip(gts, preds):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks, [f\"c{i}\" for i in ticks])\n            plt.yticks(ticks, [f\"c{i}\" for i in ticks])\n            fname = f\"{ds_name}_confusion_matrix.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\n                f\"Skipping confusion matrix for {ds_name}: empty predictions or ground truth.\"\n            )\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- print summary metric ----------\n    if metrics.get(\"val\"):\n        print(f'{ds_name} final validation accuracy: {metrics[\"val\"][-1]:.4f}')\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- list all experiment_data paths ----------\nexperiment_data_path_list = [\n    \"None/experiment_data.npy\",\n    \"experiments/2025-08-30_21-49-50_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_739457ffbfdd47968185b51791bbe34a_proc_1515243/experiment_data.npy\",\n    \"None/experiment_data.npy\",\n]\n\n# ---------- load data ----------\nall_experiment_data = []\nfor path in experiment_data_path_list:\n    try:\n        # If AI_SCIENTIST_ROOT is set prepend it, else use path as is\n        root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n        full_path = os.path.join(root, path) if root else path\n        edata = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(edata)\n    except Exception as e:\n        print(f\"Error loading experiment data from {path}: {e}\")\n\n\n# ---------- aggregate helper ----------\ndef stack_and_aggregate(list_of_arrays):\n    \"\"\"Return mean and stderr along axis 0 after matching min length.\"\"\"\n    min_len = min(len(a) for a in list_of_arrays)\n    trimmed = np.stack([a[:min_len] for a in list_of_arrays], axis=0)  # (runs, epochs)\n    mean = trimmed.mean(axis=0)\n    stderr = trimmed.std(axis=0) / np.sqrt(trimmed.shape[0])\n    return mean, stderr\n\n\n# ---------- union of dataset names ----------\ndataset_names = set()\nfor run_data in all_experiment_data:\n    dataset_names.update(run_data.keys())\n\nresults_summary = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name in sorted(dataset_names):\n    # gather metric lists per run\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for run_data in all_experiment_data:\n        if ds_name not in run_data:\n            continue\n        ds_content = run_data[ds_name]\n        losses = ds_content.get(\"losses\", {})\n        metrics = ds_content.get(\"metrics\", {})\n        if losses.get(\"train\"):\n            train_losses.append(np.array(losses[\"train\"]))\n        if losses.get(\"val\"):\n            val_losses.append(np.array(losses[\"val\"]))\n        if metrics.get(\"train\"):\n            train_accs.append(np.array(metrics[\"train\"]))\n        if metrics.get(\"val\"):\n            val_accs.append(np.array(metrics[\"val\"]))\n\n    # --------- aggregated loss curves ---------\n    try:\n        if train_losses or val_losses:\n            plt.figure()\n            if train_losses:\n                m, se = stack_and_aggregate(train_losses)\n                x = np.arange(len(m))\n                plt.plot(x, m, label=\"Train mean\")\n                plt.fill_between(x, m - se, m + se, alpha=0.3, label=\"Train \u00b11 s.e.\")\n            if val_losses:\n                m, se = stack_and_aggregate(val_losses)\n                x = np.arange(len(m))\n                plt.plot(x, m, label=\"Val mean\")\n                plt.fill_between(x, m - se, m + se, alpha=0.3, label=\"Val \u00b11 s.e.\")\n            plt.title(f\"{ds_name} Aggregated Loss Curve\\nMean \u00b11 Standard Error\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = f\"{ds_name}_aggregated_loss_curve.png\".replace(\" \", \"_\")\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss curve for {ds_name}: {e}\")\n        plt.close()\n\n    # --------- aggregated accuracy curves ---------\n    try:\n        if train_accs or val_accs:\n            plt.figure()\n            if train_accs:\n                m, se = stack_and_aggregate(train_accs)\n                x = np.arange(len(m))\n                plt.plot(x, m, label=\"Train mean\")\n                plt.fill_between(x, m - se, m + se, alpha=0.3, label=\"Train \u00b11 s.e.\")\n            if val_accs:\n                m, se = stack_and_aggregate(val_accs)\n                x = np.arange(len(m))\n                plt.plot(x, m, label=\"Val mean\")\n                plt.fill_between(x, m - se, m + se, alpha=0.3, label=\"Val \u00b11 s.e.\")\n                results_summary[ds_name] = (m[-1], se[-1])\n            plt.title(f\"{ds_name} Aggregated Accuracy Curve\\nMean \u00b11 Standard Error\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend()\n            fname = f\"{ds_name}_aggregated_accuracy_curve.png\".replace(\" \", \"_\")\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated accuracy curve for {ds_name}: {e}\")\n        plt.close()\n\n# ---------- print summary ----------\nif results_summary:\n    print(\"\\nFinal Validation Accuracy (mean \u00b1 s.e.):\")\n    for ds_name, (mean_val, se_val) in results_summary.items():\n        print(f\"  {ds_name}: {mean_val:.4f} \u00b1 {se_val:.4f}\")\n"], "plot_plan": ["We load experiment_data.npy, iterate over available datasets, and for each one generate three visualizations: (1) training and validation loss curves, (2) training and validation accuracy curves, and (3) a confusion-matrix heat-map comparing stored predictions with ground-truth.  Each figure is wrapped in its own try-except block, saved to the working directory with descriptive names, provided with clear titles/subtitles, and properly closed afterwards.  No synthetic data are fabricated; everything is taken directly from the NumPy file.  The script also prints the final validation accuracy (last epoch) so the user can quickly see headline performance.  We cap the number of figures at three per dataset, well under the five-figure guideline.  Basic matplotlib is used without external styles, and the code structure follows the required imports, directory handling, and plotting best practices.  All plots are saved in PNG format and include the dataset name and plot type in their filenames.  Error handling ensures that failures in one plot do not prevent the others from being created, and all figures are always closed.", null, "We will first load experiment_data.npy and iterate over each dataset entry, extracting losses, accuracy metrics (train_acc/val_acc) and complexity\u2010weighted metrics (train_cxa/val_cxa) if present. For every dataset we create three standard plots\u2014loss curve, accuracy curve and CXA curve\u2014each wrapped in its own try-except to ensure robustness, titled clearly and saved into working_dir with descriptive filenames. A fourth plot visualises the test confusion matrix when predictions and ground-truth are available. After looping through individual datasets we gather final validation accuracies across datasets and, only if more than one dataset is found, generate a bar chart for comparison. All figures are explicitly closed after saving. We rely solely on the values already stored in experiment_data.npy; no artificial data are generated. The plotting code begins with the prescribed imports and directory setup and follows the naming/format conventions requested.", null, null, "We load the saved experiment_data.npy, iterate over each dataset and in turn create four visualizations: (1) train/validation cross-entropy loss curves, (2) train/validation regular accuracy curves, (3) train/validation complexity-weighted accuracy (CXA) curves, and (4) a confusion matrix using the best validation predictions. Every plot is wrapped in its own try-except to avoid cascading errors, given a clear title/subtitle, saved to working_dir with descriptive filenames, and the figure is always closed. After processing individual datasets we build a comparison bar plot that shows the final validation CXA for each dataset (only if more than one is present). While generating plots we also print the final validation accuracy and CXA so that metrics are visible in stdout. The script strictly works with data that exists inside experiment_data.npy, creates at most five figures per dataset, and follows the import/working_dir template required by the guidelines.", null, null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script needs to (1) locate the \u201cworking\u201d directory, (2) load\nexperiment_data.npy, (3) iterate over every dataset contained inside, (4)\ndetermine the best value for each stored metric list (max for accuracy\u2010type\nlists, min for loss\u2010type lists), and (5) print them with fully-qualified names\nsuch as \u201ctrain accuracy\u201d or \u201cvalidation loss\u201d. The code below follows those\nrules and runs immediately when executed.", "The solution loads the saved numpy dictionary, iterates over each dataset, and\nextracts the stored accuracy and loss histories. It then determines the \u201cbest\u201d\n(maximum accuracy, minimum loss) and \u201cfinal\u201d (last epoch) values, as well as the\ntest-set accuracy computed from the saved predictions. Each metric is printed\nwith an explicit, self-describing label, preceded by the dataset name.", "We will load the experiment data from the \u201cworking\u201d directory, iterate over each\nstored dataset, and compute the best (i.e., highest for accuracies/CXA and\nlowest for losses) value found across epochs. Then we print the dataset name\nfollowed by clearly-labelled metrics. No code is wrapped in a special entry-\npoint so the script runs immediately.", "", "", "The script below loads the saved NumPy file from the working directory, extracts\nthe final epoch\u2019s metrics and losses for every dataset contained in the file,\nand prints them with explicit, descriptive labels. It immediately executes on\nimport and satisfies all structural constraints.", "", "The script will immediately load the saved NumPy dictionary from the working\ndirectory, iterate over every dataset stored inside, and then inspect the lists\nthat hold the logged values.   For accuracy\u2010type metrics (those whose names\ncontain \u201cacc\u201d or \u201ccxa\u201d) the script reports the maximum value obtained; for\nloss\u2010type metrics it reports the minimum value.   Using the saved test\npredictions and ground-truth labels, the script additionally computes and prints\nthe final test accuracy.   Each printed line starts with the dataset name\nfollowed by a clearly labelled metric name and its corresponding value.", "The script loads the saved NumPy dictionary, navigates through its nested\nstructure, and pulls out the recorded metric/loss histories. It then computes\nthe \u201cbest\u201d value for each metric\u2014maximum for accuracies, minimum for\nlosses\u2014across all epochs. Finally, it prints the dataset name followed by each\nmetric name and its best value using clear, explicit labels. The code executes\nimmediately and respects the required structure (no `if __name__ ==\n\"__main__\":`, no plotting).", "The script needs to (1) locate the \u201cworking\u201d directory, (2) load\nexperiment_data.npy, (3) iterate over every dataset contained inside, (4)\ndetermine the best value for each stored metric list (max for accuracy\u2010type\nlists, min for loss\u2010type lists), and (5) print them with fully-qualified names\nsuch as \u201ctrain accuracy\u201d or \u201cvalidation loss\u201d. The code below follows those\nrules and runs immediately when executed.", "The script needs to (1) locate the \u201cworking\u201d directory, (2) load\nexperiment_data.npy, (3) iterate over every dataset contained inside, (4)\ndetermine the best value for each stored metric list (max for accuracy\u2010type\nlists, min for loss\u2010type lists), and (5) print them with fully-qualified names\nsuch as \u201ctrain accuracy\u201d or \u201cvalidation loss\u201d. The code below follows those\nrules and runs immediately when executed.", "The script needs to (1) locate the \u201cworking\u201d directory, (2) load\nexperiment_data.npy, (3) iterate over every dataset contained inside, (4)\ndetermine the best value for each stored metric list (max for accuracy\u2010type\nlists, min for loss\u2010type lists), and (5) print them with fully-qualified names\nsuch as \u201ctrain accuracy\u201d or \u201cvalidation loss\u201d. The code below follows those\nrules and runs immediately when executed.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ----------------------------------------------------------------------\n# locate and load experiment data\n# ----------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ----------------------------------------------------------------------\n# helper: decide whether higher or lower is better\n# ----------------------------------------------------------------------\ndef best_value(values, kind):\n    \"\"\"\n    Return the best (max or min) value depending on metric type.\n    Accuracy-like lists use max; loss-like lists use min.\n    \"\"\"\n    if kind == \"metrics\":  # accuracy\n        return max(values)\n    else:  # losses\n        return min(values)\n\n\n# ----------------------------------------------------------------------\n# print results\n# ----------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # accuracy and loss sections\n    for section_key in (\"metrics\", \"losses\"):  # iterate over stored dicts\n        section = data.get(section_key, {})\n        for split_key, values in section.items():\n            if not values:  # skip empty lists\n                continue\n\n            # build human-readable metric name\n            split_label = \"train\" if split_key == \"train\" else \"validation\"\n            metric_label = \"accuracy\" if section_key == \"metrics\" else \"loss\"\n            full_name = f\"{split_label} {metric_label}\"\n\n            # compute best value\n            value = best_value(values, section_key)\n            print(f\"{full_name}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef safe_get(lst, fn):\n    \"\"\"Return fn(lst) if list is non-empty, else None\"\"\"\n    return fn(lst) if lst else None\n\n\nfor ds_name, ds_dict in experiment_data.items():\n    print(ds_name)  # dataset header\n\n    # ----- accuracy and loss histories -----\n    train_acc_hist = ds_dict[\"metrics\"][\"train\"]\n    val_acc_hist = ds_dict[\"metrics\"][\"val\"]\n    train_loss_hist = ds_dict[\"losses\"][\"train\"]\n    val_loss_hist = ds_dict[\"losses\"][\"val\"]\n\n    final_train_acc = safe_get(train_acc_hist, lambda x: x[-1])\n    best_val_acc = safe_get(val_acc_hist, max)\n    final_train_loss = safe_get(train_loss_hist, lambda x: x[-1])\n    best_val_loss = safe_get(val_loss_hist, min)\n\n    if final_train_acc is not None:\n        print(f\"  final train accuracy: {final_train_acc:.4f}\")\n    if best_val_acc is not None:\n        print(f\"  best validation accuracy: {best_val_acc:.4f}\")\n    if final_train_loss is not None:\n        print(f\"  final train loss: {final_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"  best validation loss: {best_val_loss:.4f}\")\n\n    # ----- test-set metrics (if predictions and ground truth are stored) -----\n    preds = ds_dict.get(\"predictions\")\n    gts = ds_dict.get(\"ground_truth\")\n    if preds is not None and gts is not None and len(preds) == len(gts):\n        correct = sum(int(p == g) for p, g in zip(preds, gts))\n        test_acc = correct / len(gts) if gts else 0.0\n        print(f\"  test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Locate the file and load it\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\ndef safe_best(values, higher_is_better=True):\n    \"\"\"Return the best value in a list or None if the list is empty.\"\"\"\n    if not values:\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# 2\u20135. Extract and print best metrics for each dataset\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}:\")  # 3. dataset name first\n\n    metrics = content.get(\"metrics\", {})\n    losses = content.get(\"losses\", {})\n\n    # Accuracies & CXA \u2013 higher is better\n    best_train_acc = safe_best(metrics.get(\"train_acc\", []), higher_is_better=True)\n    best_val_acc = safe_best(metrics.get(\"val_acc\", []), higher_is_better=True)\n    best_train_cxa = safe_best(metrics.get(\"train_cxa\", []), higher_is_better=True)\n    best_val_cxa = safe_best(metrics.get(\"val_cxa\", []), higher_is_better=True)\n\n    # Losses \u2013 lower is better\n    best_train_loss = safe_best(losses.get(\"train\", []), higher_is_better=False)\n    best_val_loss = safe_best(losses.get(\"val\", []), higher_is_better=False)\n\n    # 4. Print metric names explicitly\n    if best_train_acc is not None:\n        print(f\"  Best training accuracy: {best_train_acc:.4f}\")\n    if best_val_acc is not None:\n        print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n    if best_train_loss is not None:\n        print(f\"  Best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"  Best validation loss: {best_val_loss:.4f}\")\n    if best_train_cxa is not None:\n        print(f\"  Best training complexity-weighted accuracy: {best_train_cxa:.4f}\")\n    if best_val_cxa is not None:\n        print(f\"  Best validation complexity-weighted accuracy: {best_val_cxa:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper: nicely formatted float printing\n# ------------------------------------------------------------------\ndef fmt(v):\n    return f\"{v:.4f}\" if isinstance(v, (float, np.floating)) else str(v)\n\n\n# ------------------------------------------------------------------\n# Iterate over datasets and report final metrics\n# ------------------------------------------------------------------\nfor dataset_name, results in experiment_data.items():\n    # Retrieve metric/loss lists\n    train_metrics = results[\"metrics\"][\"train\"]\n    val_metrics = results[\"metrics\"][\"val\"]\n    train_losses = results[\"losses\"][\"train\"]\n    val_losses = results[\"losses\"][\"val\"]\n\n    # Use the last (final) epoch\n    final_train = train_metrics[-1]\n    final_val = val_metrics[-1]\n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n\n    # ------------------------------------------------------------------\n    # Print section header (dataset name)\n    # ------------------------------------------------------------------\n    print(f\"\\n=== {dataset_name} ===\")\n\n    # Losses\n    print(f\"final training loss: {fmt(final_train_loss)}\")\n    print(f\"final validation loss: {fmt(final_val_loss)}\")\n\n    # Accuracies\n    print(f\"final training accuracy: {fmt(final_train['acc'])}\")\n    print(f\"final validation accuracy: {fmt(final_val['acc'])}\")\n\n    # Color-Weighted Accuracy (CWA)\n    print(f\"final training color-weighted accuracy: {fmt(final_train['cwa'])}\")\n    print(f\"final validation color-weighted accuracy: {fmt(final_val['cwa'])}\")\n\n    # Shape-Weighted Accuracy (SWA)\n    print(f\"final training shape-weighted accuracy: {fmt(final_train['swa'])}\")\n    print(f\"final validation shape-weighted accuracy: {fmt(final_val['swa'])}\")\n\n    # Complexity-Weighted Accuracy (CXA)\n    print(f\"final training complexity-weighted accuracy: {fmt(final_train['cxa'])}\")\n    print(f\"final validation complexity-weighted accuracy: {fmt(final_val['cxa'])}\")\n", "", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n#                       locate and load experiment data                         #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n#                       helper to classify metric type                         #\n# -----------------------------------------------------------------------------#\ndef is_loss_key(key: str) -> bool:\n    \"\"\"Return True if the key refers to a loss curve.\"\"\"\n    return \"loss\" in key.lower()\n\n\ndef best_value(key: str, values):\n    \"\"\"Pick best value depending on metric type.\"\"\"\n    if not values:  # empty list guard\n        return None\n    if is_loss_key(key):\n        return min(values)  # lower is better for losses\n    return max(values)  # higher is better for accuracies / cxa\n\n\n# -----------------------------------------------------------------------------#\n#                               metric printing                                #\n# -----------------------------------------------------------------------------#\nfor dataset_name, content in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # --- metric curves (accuracies, CXA, etc.) ---\n    for metric_key, metric_values in content.get(\"metrics\", {}).items():\n        val = best_value(metric_key, metric_values)\n        if val is not None:\n            pretty_name = metric_key.replace(\"_\", \" \").title()\n            print(f\"  {pretty_name}: {val:.4f}\")\n\n    # --- loss curves ---\n    for split, loss_values in content.get(\"losses\", {}).items():\n        val = best_value(f\"{split}_loss\", loss_values)\n        if val is not None:\n            pretty_name = f\"{split.capitalize()} Loss\"\n            print(f\"  {pretty_name}: {val:.4f}\")\n\n    # --- test accuracy from saved predictions ---\n    preds = content.get(\"predictions\", [])\n    gts = content.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = sum(int(p == t) for p, t in zip(preds, gts)) / len(gts)\n        print(f\"  Test Accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper functions ----------\ndef best_accuracy(values):\n    # values is a list of floats\n    return max(values) if values else None\n\n\ndef best_loss(values):\n    # values is a list of floats\n    return min(values) if values else None\n\n\ndef best_val_tuple(list_of_tuples, idx):\n    # pick the best value (max) for a specific metric contained in tuples\n    if not list_of_tuples:\n        return None\n    return max(t[idx] for t in list_of_tuples)\n\n\n# ---------- iterate over datasets ----------\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # ----- histories -----\n    train_acc_hist = data[\"metrics\"][\"train\"]  # list[float]\n    val_tuple_hist = data[\"metrics\"][\"val\"]  # list[tuple]\n    train_loss_hist = data[\"losses\"][\"train\"]  # list[float]\n    val_loss_hist = data[\"losses\"][\"val\"]  # list[float]\n\n    # ----- compute bests -----\n    best_train_acc = best_accuracy(train_acc_hist)\n    best_val_acc = best_val_tuple(val_tuple_hist, 0)  # index 0 is plain val accuracy\n    best_val_cwa = best_val_tuple(val_tuple_hist, 1)  # color-weighted accuracy\n    best_val_swa = best_val_tuple(val_tuple_hist, 2)  # shape-weighted accuracy\n    best_val_cxa = best_val_tuple(val_tuple_hist, 3)  # complexity-weighted accuracy\n    best_train_loss = best_loss(train_loss_hist)\n    best_val_loss = best_loss(val_loss_hist)\n\n    # ----- print results -----\n    print(f\"Best train accuracy: {best_train_acc:.4f}\")\n    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n    print(f\"Best validation color weighted accuracy: {best_val_cwa:.4f}\")\n    print(f\"Best validation shape weighted accuracy: {best_val_swa:.4f}\")\n    print(f\"Best validation complexity weighted accuracy: {best_val_cxa:.4f}\")\n    print(f\"Best train loss: {best_train_loss:.4f}\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------------------------\n# locate and load experiment data\n# ----------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ----------------------------------------------------------------------\n# helper: decide whether higher or lower is better\n# ----------------------------------------------------------------------\ndef best_value(values, kind):\n    \"\"\"\n    Return the best (max or min) value depending on metric type.\n    Accuracy-like lists use max; loss-like lists use min.\n    \"\"\"\n    if kind == \"metrics\":  # accuracy\n        return max(values)\n    else:  # losses\n        return min(values)\n\n\n# ----------------------------------------------------------------------\n# print results\n# ----------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # accuracy and loss sections\n    for section_key in (\"metrics\", \"losses\"):  # iterate over stored dicts\n        section = data.get(section_key, {})\n        for split_key, values in section.items():\n            if not values:  # skip empty lists\n                continue\n\n            # build human-readable metric name\n            split_label = \"train\" if split_key == \"train\" else \"validation\"\n            metric_label = \"accuracy\" if section_key == \"metrics\" else \"loss\"\n            full_name = f\"{split_label} {metric_label}\"\n\n            # compute best value\n            value = best_value(values, section_key)\n            print(f\"{full_name}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------------------------\n# locate and load experiment data\n# ----------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ----------------------------------------------------------------------\n# helper: decide whether higher or lower is better\n# ----------------------------------------------------------------------\ndef best_value(values, kind):\n    \"\"\"\n    Return the best (max or min) value depending on metric type.\n    Accuracy-like lists use max; loss-like lists use min.\n    \"\"\"\n    if kind == \"metrics\":  # accuracy\n        return max(values)\n    else:  # losses\n        return min(values)\n\n\n# ----------------------------------------------------------------------\n# print results\n# ----------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # accuracy and loss sections\n    for section_key in (\"metrics\", \"losses\"):  # iterate over stored dicts\n        section = data.get(section_key, {})\n        for split_key, values in section.items():\n            if not values:  # skip empty lists\n                continue\n\n            # build human-readable metric name\n            split_label = \"train\" if split_key == \"train\" else \"validation\"\n            metric_label = \"accuracy\" if section_key == \"metrics\" else \"loss\"\n            full_name = f\"{split_label} {metric_label}\"\n\n            # compute best value\n            value = best_value(values, section_key)\n            print(f\"{full_name}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------------------------\n# locate and load experiment data\n# ----------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ----------------------------------------------------------------------\n# helper: decide whether higher or lower is better\n# ----------------------------------------------------------------------\ndef best_value(values, kind):\n    \"\"\"\n    Return the best (max or min) value depending on metric type.\n    Accuracy-like lists use max; loss-like lists use min.\n    \"\"\"\n    if kind == \"metrics\":  # accuracy\n        return max(values)\n    else:  # losses\n        return min(values)\n\n\n# ----------------------------------------------------------------------\n# print results\n# ----------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # accuracy and loss sections\n    for section_key in (\"metrics\", \"losses\"):  # iterate over stored dicts\n        section = data.get(section_key, {})\n        for split_key, values in section.items():\n            if not values:  # skip empty lists\n                continue\n\n            # build human-readable metric name\n            split_label = \"train\" if split_key == \"train\" else \"validation\"\n            metric_label = \"accuracy\" if section_key == \"metrics\" else \"loss\"\n            full_name = f\"{split_label} {metric_label}\"\n\n            # compute best value\n            value = best_value(values, section_key)\n            print(f\"{full_name}: {value:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'train accuracy: 0.5800', '\\n', 'validation accuracy:\n0.6500', '\\n', 'train loss: 0.6780', '\\n', 'validation loss: 0.6906', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  final train accuracy: 0.5500', '\\n', '  best validation\naccuracy: 0.4500', '\\n', '  final train loss: 0.6847', '\\n', '  best validation\nloss: 0.6982', '\\n', '  test accuracy: 0.4667', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  Best training accuracy: 0.5467', '\\n', '  Best\nvalidation accuracy: 0.5000', '\\n', '  Best training loss: 0.6946', '\\n', '\nBest validation loss: 0.7030', '\\n', '  Best training complexity-weighted\naccuracy: 0.5451', '\\n', '  Best validation complexity-weighted accuracy:\n0.4733', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "", "['\\n=== SPR_BENCH ===', '\\n', 'final training loss: 0.6213', '\\n', 'final\nvalidation loss: 0.7635', '\\n', 'final training accuracy: 0.6340', '\\n', 'final\nvalidation accuracy: 0.4700', '\\n', 'final training color-weighted accuracy:\n0.6405', '\\n', 'final validation color-weighted accuracy: 0.4690', '\\n', 'final\ntraining shape-weighted accuracy: 0.6306', '\\n', 'final validation shape-\nweighted accuracy: 0.4617', '\\n', 'final training complexity-weighted accuracy:\n0.6356', '\\n', 'final validation complexity-weighted accuracy: 0.4611', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH:', '\\n', '  Train Acc: 0.6067', '\\n', '  Val Acc: 0.5167', '\\n', '\nTrain Cxa: 0.6040', '\\n', '  Val Cxa: 0.5642', '\\n', '  Train Loss: 0.6672',\n'\\n', '  Val Loss: 0.7017', '\\n', '  Test Accuracy: 0.4333', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best train accuracy: 0.5367', '\\n', 'Best\nvalidation accuracy: 0.4500', '\\n', 'Best validation color weighted accuracy:\n0.4437', '\\n', 'Best validation shape weighted accuracy: 0.4558', '\\n', 'Best\nvalidation complexity weighted accuracy: 0.4492', '\\n', 'Best train loss:\n0.6894', '\\n', 'Best validation loss: 0.7028', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.5533', '\\n', 'validation accuracy:\n0.5667', '\\n', 'train loss: 0.6877', '\\n', 'validation loss: 0.6921', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.5650', '\\n', 'validation accuracy:\n0.5000', '\\n', 'train loss: 0.6756', '\\n', 'validation loss: 0.6859', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.5400', '\\n', 'validation accuracy:\n0.5500', '\\n', 'train loss: 0.6883', '\\n', 'validation loss: 0.6909', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}