{
    "Name": "gnn_for_spr",
    "Title": "Leveraging Graph Neural Networks for Enhanced Synthetic PolyRule Reasoning",
    "Short Hypothesis": "Graph Neural Networks (GNNs) can effectively capture the inherent structure and relationships within sequences of symbolic data in the Synthetic PolyRule Reasoning (SPR) task, leading to superior performance on both Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) metrics compared to current State-of-the-Art (SOTA) models.",
    "Related Work": "Current approaches for symbolic pattern recognition often rely on sequence models like RNNs, LSTMs, and Transformers. These models primarily focus on the sequential nature of the data but may not fully exploit the relational and structural information present in the sequences. While some works have explored GNNs for related tasks, they have not been specifically applied to the SPR task involving complex poly-factor rules. This proposal aims to fill this gap by leveraging the unique strengths of GNNs in capturing structural dependencies and relationships within the sequences.",
    "Abstract": "This research proposes the use of Graph Neural Networks (GNNs) for the Synthetic PolyRule Reasoning (SPR) task. The SPR task involves classifying sequences of symbolic data according to hidden poly-factor rules. Current SOTA models primarily utilize sequence-based architectures that may not fully capture the relational and structural information inherent in the sequences. We hypothesize that GNNs, designed to model relational data, can better capture these dependencies, leading to improved performance on both Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) metrics. We will design a GNN-based model that represents each sequence as a graph, with nodes corresponding to tokens and edges representing relationships based on position, color, and shape. The model will be evaluated on the SPR_BENCH benchmark, and we aim to surpass the current SOTA performance on the chosen metric.",
    "Experiments": [
        {
            "Name": "Model Design and Training",
            "Description": "Design a GNN-based model where each token in the sequence is represented as a node, and edges capture relationships based on color, shape, position, and order. Train the model on the training split of the SPR_BENCH benchmark. Tune hyperparameters using the development split."
        },
        {
            "Name": "Evaluation",
            "Description": "Evaluate the model's performance on the test split using Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA). Compare the performance against the current SOTA benchmarks (CWA: 65.0%, SWA: 70.0%)."
        },
        {
            "Name": "Ablation Studies",
            "Description": "Investigate the impact of different types of edges (e.g., color-based, shape-based, position-based) on the model's performance. Compare the performance of the GNN-based model to traditional sequence-based models (e.g., LSTMs, Transformers)."
        },
        {
            "Name": "Visualization and Analysis",
            "Description": "Visualize the learned graph embeddings to understand how the model captures the relational structure of the sequences. Analyze the types of rules that the model learns to recognize, and identify any patterns or commonalities among misclassified instances."
        }
    ],
    "Risk Factors and Limitations": [
        "Data Representation: Representing sequences as graphs introduces complexity in data preprocessing. Ensuring that the graph representation accurately captures all relevant relationships is crucial.",
        "Scalability: GNNs can be computationally intensive, especially for longer sequences with many nodes and edges. Efficient implementation and optimization will be necessary to handle large datasets.",
        "Generalization: While GNNs are powerful, there is a risk that the model may overfit to specific patterns in the training data. Careful regularization and validation will be required to ensure robust generalization."
    ],
    "Code": "\"\"\"\nSPR.py\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nUtility to load the SPR_BENCH benchmark datasets\nUsing HuggingFace\u2019s `datasets` library.\nDefinition of two evaluation metrics:\n1. Color-Weighted Accuracy (CWA)\n2. Shape-Weighted Accuracy (SWA)\nDirectory layout expected\nSPR_BENCH/\n \u251c\u2500 train.csv   (20000 rows)\n \u251c\u2500 dev.csv     (5000 rows)\n \u2514\u2500 test.csv    (10000 rows)\n\nEach CSV has header:  id,sequence,label\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n$ pip install datasets   # once\n\"\"\"\nimport pathlib\nfrom typing import Dict\n\nfrom datasets import load_dataset, DatasetDict                                         # <- no pandas import\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"\n    Return a DatasetDict {'train':\u2026, 'dev':\u2026, 'test':\u2026} for one SPR ID folder.\n    \"\"\"\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",           # treat csv as a single split\n            cache_dir=\".cache_dsets\" # optional; keeps HF cache tidy\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"]   = _load(\"dev.csv\")\n    dset[\"test\"]  = _load(\"test.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    \"\"\"Count the number of unique color types in the sequence\"\"\"\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    \"\"\"Count the number of unique shape types in the sequence\"\"\"\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    \"\"\"Color-Weighted Accuracy (CWA)\"\"\"\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    \"\"\"Shape-Weighted Accuracy (SWA)\"\"\"\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef main():\n\n    ## Absolute path of the datasets\n    DATA_PATH = pathlib.Path('/home/zxl240011/AI-Scientist-v2/SPR_BENCH/')\n    spr_bench = load_spr_bench(DATA_PATH)\n\n    print(\"Benchmarks split:\", spr_bench.keys())\n\n    # Demo: show first example from SPR_BENCH\u2011train\n    ex = spr_bench[\"train\"][0]\n    print(\"\\nExample row:\")\n    print(ex)          \n\n\nif __name__ == \"__main__\":\n    main()\n"
}