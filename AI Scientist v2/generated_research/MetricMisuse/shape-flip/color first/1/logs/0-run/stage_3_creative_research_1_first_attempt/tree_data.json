{"edges": [[0, 3], [0, 2], [0, 1], [0, 4], [1, 8], [1, 5], [1, 6], [1, 7], [8, 11], [8, 9], [8, 10], [8, 12]], "layout": [[0.6666666666666666, 0.0], [0.3333333333333333, 0.33333333333333337], [0.5555555555555556, 0.33333333333333337], [0.7777777777777778, 0.33333333333333337], [1.0, 0.33333333333333337], [0.0, 0.6666666666666667], [0.2222222222222222, 0.6666666666666667], [0.4444444444444444, 0.6666666666666667], [0.6666666666666666, 0.6666666666666667], [0.3333333333333333, 1.0], [0.5555555555555556, 1.0], [0.7777777777777778, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: learning_rate. We keep the original pipeline (data\npreparation, model, metrics, training loop) and wrap it inside a grid-search\nloop that re-initializes the model for each candidate learning rate (3e-4, 1e-3,\n3e-3). Validation scores, losses and predictions for every epoch of every run\nare stored under experiment_data['learning_rate']['SPR'][<lr>]. Everything is\nfinally saved to \u2018experiment_data.npy\u2019. The rest of the code remains unchanged\nso the script is fully self-contained and executable.", "Our enhanced approach encodes each symbol as two categorical features (shape,\ncolor), embeds them separately, and augments the graph with rich relational\nedges: sequential order, same-shape, and same-color links.  This richer\nstructure lets a lightweight 2-layer GraphSAGE aggregate information along\nmultiple rule-relevant paths.  We train with early-stopping\u2013style evaluation on\nthe dev split and track CWA, SWA and the required Harmonic Poly Accuracy (HPA)\nevery epoch, logging everything to experiment_data and saving it to ./working.\nA modest learning-rate sweep (1e-3, 3e-3) and 8 epochs keeps runtime low while\nstill exploring optimisation space.  The script automatically falls back to a\nsmall synthetic dataset if SPR_BENCH is absent so it always runs end-to-end.", "We enrich the graph structure by adding edges between tokens that share the same\nshape or colour in addition to the sequential order edges, and supply node\nfeatures that sum three embeddings (shape, colour, position).   This lets the\nGNN reason explicitly over the latent poly-factors.   We train a simple two-\nlayer GraphSAGE on the benchmark (or synthetic fallback), compute CWA, SWA and\ntheir harmonic mean HPA each epoch, and log everything for later analysis.   The\nimplementation remains compact, single-file and self-contained, follows all\ndevice / logging requirements, and stores the full experiment data to\n./working/experiment_data.npy.", "We enrich the previous GNN by explicitly wiring relational edges: besides the\nnatural left-to-right order we add (1) shape-edges that connect all tokens\nsharing the same shape letter and (2) color-edges that connect all tokens with\nthe same color letter.  Each node still carries an embedding of the full token\nstring, so we keep the vocabulary simple while the graph structure now exposes\nthe hidden factors the benchmark cares about.  We train the model for a few\nepochs, evaluate after every epoch on the dev set, and compute the new Harmonic\nPoly Accuracy (HPA) together with the original CWA/SWA.  The script falls back\nto a small synthetic SPR-like dataset if the real benchmark folder is absent, so\nit remains fully self-contained.  All required GPU handling, metric logging, and\nexperiment data saving are included.  This first attempt focuses on showing the\nbenefit of richer graph connectivity; follow-up runs can refine hyper-\nparameters, deeper stacks, or edge-type embeddings.", "We turn each SPR sequence into a small graph whose nodes are token\u2013embeddings\nand whose edges connect adjacent tokens (order information).  A lightweight\nGraph-SAGE encoder followed by mean-pooling is trained with cross-entropy to\npredict the hidden rule class.  At every epoch we evaluate on the dev split and\ncompute Color-Weighted Accuracy (CWA), Shape-Weighted Accuracy (SWA) and their\nharmonic mean HPA, printing and logging them.  If the official SPR_BENCH folder\nis absent the script automatically fabricates a tiny synthetic dataset so that\nthe code remains executable everywhere.  A three-value learning-rate sweep\n(3e-4, 1e-3, 3e-3) runs in a loop; for each run losses and metrics are stored in\na nested experiment_data dictionary that is finally saved to\n./working/experiment_data.npy for later analysis.  All tensors and models are\nexplicitly moved to GPU when available, and DataLoader batches are transferred\ninside the training loop to respect the hardware guidelines.  The whole script\nis self-contained, launches immediately on execution, finishes in well under 30\nminutes, and leaves a full log of results.", "We enrich the graphs with three explicit relation types\u2014sequential-order, same-\nshape and same-color\u2014and let a lightweight multi\u2013relation GraphSAGE aggregate\nmessages separately over each edge type before merging them, giving the network\nclearer inductive bias for poly-factor rules.  Shape and colour tokens obtain\nindependent embeddings that are concatenated, projected and passed through two\nstacked MultiRel-SAGE layers followed by global mean pooling for graph\nclassification.  A tiny grid search over learning rate is performed, metrics are\ntracked every epoch (loss, CWA, SWA, HPA) and everything is logged to NumPy for\nlater analysis.  The script auto-falls back to a synthetic dataset if the\nbenchmark folder is absent, is fully self-contained, GPU-aware and respects all\nlogging and saving requirements.  It should finish within minutes on commodity\nhardware while still demonstrating the proposed architectural idea and\nevaluation pipeline.  The code below executes immediately on load.", "We boost relational reasoning by explicitly separating edge types and using a\nRelational Graph Convolutional Network.   Each sequence is turned into a\nheterogeneous graph with three relations: consecutive-order, same-shape, and\nsame-color.   Tokens are encoded through independent shape and color embeddings,\nconcatenated and projected before three stacked RGCN layers propagate\ninformation by relation type.   Validation Harmonic-Poly Accuracy (HPA) is\nmonitored every epoch and the best model on the dev split is kept; a simple\nlearning-rate sweep selects the strongest run.   If the official SPR_BENCH data\nare absent the script falls back to a small synthetic generator, ensuring self-\ncontained execution.   All losses, CWA, SWA and HPA curves are logged in the\n`experiment_data` object and saved to disk, enabling later analysis and\nplotting.   The code follows the GPU-handling, metric-reporting and data-saving\nrequirements exactly, executes immediately, and finishes in <30 minutes on a\nsingle GPU/CPU.", "We add richer relational bias by encoding three edge types\u2014next-token order,\nsame-shape, and same-color\u2014then train a two-layer R-GCN that can learn separate\ntransformation matrices per relation.  Nodes now also carry a small positional\nembedding, giving the network explicit access to absolute order while still\nreasoning relationally.  We keep embeddings for shape and color separate,\nconcatenate them, and project to the hidden size.  A light grid-search over two\nlearning-rates trains for up to 20 epochs with early stopping on dev-set\nHarmonic Poly Accuracy (HPA); the best checkpoint of every run is finally\nevaluated on the test split.  All losses and metrics (ACC, CWA, SWA, HPA) are\nlogged each epoch and stored to \u201cworking/experiment_data.npy\u201d for later\nanalysis.  The script is self-contained: it tries to load the real SPR_BENCH;\nwhen absent it falls back to a synthetic generator so it can always run.\nEverything is GPU-aware, moving data, model, and optimizer to the detected\ndevice.  Execution finishes well within the 30-minute budget on a modern GPU.", "We enrich the previous GNN by turning each relation (consecutive-order, same-\nshape, same-color) into a distinct edge type and apply a 2-layer Relational\nGraph Convolution Network (RGCN).  Distinguishing relations lets the model learn\ndifferent message-passing channels, alleviating the previous imbalance that\ncaused flat accuracies.  We also introduce class-weighted cross-entropy\n(computed from the training set) and a cosine-annealing learning-rate scheduler\nfor stabler convergence.  All metrics\u2014Color-Weighted Accuracy (CWA), Shape-\nWeighted Accuracy (SWA), and their Harmonic Poly Accuracy (HPA)\u2014are tracked\nevery epoch, together with losses, and saved to ./working/experiment_data.npy\nfor later analysis.  If the official SPR_BENCH data are missing, a small\nsynthetic dataset is generated so the script always runs.  The code respects all\nGPU/CPU handling rules, keeps everything self-contained, and finishes well\ninside the 30-minute limit.  Running it will print per-epoch validation loss and\nthe three metrics, then store the full log.  You can later reload\nexperiment_data.npy to visualise training curves or inspect predictions.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, string, time, math, json, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------- helper: metrics ------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\n# --------------------- data loading ---------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef generate_synthetic(num_train=200, num_dev=60, num_test=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            tokens = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq = \" \".join(tokens)\n            label = int(tokens[0][0] == tokens[-1][0])\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(num_train)),\n            \"dev\": Dataset.from_dict(_gen(num_dev)),\n            \"test\": Dataset.from_dict(_gen(num_test)),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Loaded dataset with {len(dataset['train'])} train, {len(dataset['dev'])} dev samples, classes={num_classes}\"\n)\n\n# ------------------ vocabulary build --------------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n\n# -------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# -------------------------- model -----------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------ experiment tracking -----------------------\nexperiment_data = {\"learning_rate\": {\"SPR\": {}}}\n\n# ------------------ hyperparameter sweep ----------------------\nlearning_rates = [3e-4, 1e-3, 3e-3]\nepochs = 5\nbatch_size_train = 32\nbatch_size_eval = 64\n\nfor lr in learning_rates:\n    lr_key = f\"{lr:.0e}\" if lr < 1 else str(lr)\n    print(f\"\\n=== Training with learning rate {lr_key} ===\")\n    model = GNNClassifier(vocab, num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(train_graphs, batch_size=batch_size_train, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=batch_size_eval)\n\n    run_log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # ---- train\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n        run_log[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation\n        model.eval()\n        dev_loss, preds, gts, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                pred = logits.argmax(dim=-1).cpu().tolist()\n                gt = batch.y.cpu().tolist()\n                seqs.extend(batch.seq_raw)\n                preds.extend(pred)\n                gts.extend(gt)\n        dev_loss /= len(dev_loader.dataset)\n        run_log[\"losses\"][\"val\"].append(dev_loss)\n\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        run_log[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n        run_log[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={dev_loss:.4f}, \"\n            f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n        )\n\n    run_log[\"predictions\"] = preds\n    run_log[\"ground_truth\"] = gts\n    experiment_data[\"learning_rate\"][\"SPR\"][lr_key] = run_log\n\n# --------------- save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All runs finished. Data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "import os, pathlib, random, string, time, math, json, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader, Batch\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------- metric helpers --------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ------------------------ data utils --------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _ld(\"train.csv\"),\n        _ld(\"dev.csv\"),\n        _ld(\"test.csv\"),\n    )\n    return dset\n\n\ndef make_synthetic(n_tr=300, n_dev=100, n_te=120):\n    shapes = list(string.ascii_uppercase[:8])\n    colors = list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(toks)\n            # simple parity rule: label 1 if majority shape==first shape\n            maj = sum(t[0] == toks[0][0] for t in toks) > L / 2\n            labels.append(int(maj))\n            seqs.append(seq)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(n_tr)),\n            \"dev\": Dataset.from_dict(gen(n_dev)),\n            \"test\": Dataset.from_dict(gen(n_te)),\n        }\n    )\n\n\ndataset = try_load_benchmark() or make_synthetic()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n\n# ---------------------- vocabularies --------------------------\nshape2idx, color2idx = {}, {}\n\n\ndef add_vocab(tok):\n    sh, co = tok[0], tok[1]\n    if sh not in shape2idx:\n        shape2idx[sh] = len(shape2idx)\n    if co not in color2idx:\n        color2idx[co] = len(color2idx)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_vocab(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"ShapeVocab={n_shapes}, ColorVocab={n_colors}, Classes={num_classes}\")\n\n\n# ------------------- seq -> graph -----------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    shape_ids = [shape2idx[t[0]] for t in toks]\n    color_ids = [color2idx[t[1]] for t in toks]\n    pos_ids = list(range(len(toks)))\n    # edges: consecutive\n    src = list(range(len(toks) - 1))\n    dst = list(range(1, len(toks)))\n    # same-shape\n    sh_dict = {}\n    for i, sh in enumerate(shape_ids):\n        sh_dict.setdefault(sh, []).append(i)\n    for ids in sh_dict.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src.append(i)\n                    dst.append(j)\n    # same-color\n    co_dict = {}\n    for i, co in enumerate(color_ids):\n        co_dict.setdefault(co, []).append(i)\n    for ids in co_dict.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src.append(i)\n                    dst.append(j)\n    edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    x = torch.tensor(list(zip(shape_ids, color_ids)), dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long))\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------------------- model ---------------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, n_shapes, n_colors, emb_dim=32, hid=64, n_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb_dim)\n        self.color_emb = nn.Embedding(n_colors, emb_dim)\n        self.lin_in = nn.Linear(emb_dim * 2, hid)\n        self.conv1 = SAGEConv(hid, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, data):\n        sh = self.shape_emb(data.x[:, 0])\n        co = self.color_emb(data.x[:, 1])\n        x = torch.cat([sh, co], dim=-1)\n        x = F.relu(self.lin_in(x))\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# ----------------- experiment logging -------------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- training loop ---------------------------\ndef run_one(lr):\n    model = GNNClassifier(n_shapes, n_colors, n_cls=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    crit = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=64)\n    for epoch in range(1, 9):\n        # train\n        model.train()\n        tot_loss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch.num_graphs\n        tr_loss = tot_loss / len(train_loader.dataset)\n        # eval\n        model.eval()\n        dev_loss = 0\n        preds = []\n        gts = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = crit(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                p = logits.argmax(-1).cpu().tolist()\n                g = batch.y.cpu().tolist()\n                s = batch.seq_raw\n                preds.extend(p)\n                gts.extend(g)\n                seqs.extend(s)\n        dev_loss /= len(dev_loader.dataset)\n        acc = np.mean([p == g for p, g in zip(preds, gts)])\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        hpa = harmonic_poly_accuracy(cwa, swa)\n        print(\n            f\"Epoch {epoch}: validation_loss = {dev_loss:.4f} | acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n        )\n        # log\n        experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR\"][\"losses\"][\"val\"].append(dev_loss)\n        experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n        )\n        experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR\"][\"predictions\"] = preds\n    experiment_data[\"SPR\"][\"ground_truth\"] = gts\n\n\nfor lr in [1e-3, 3e-3]:\n    print(f\"\\n=== Running LR={lr} ===\")\n    run_one(lr)\n\n# ---------------------- save logs -----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ---------- required boiler-plate ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metric helpers ----------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 0 if (cwa + swa) == 0 else 2 * cwa * swa / (cwa + swa)\n\n\n# ---------- load SPR_BENCH or fallback ----------\ndef load_dataset_spr():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if root.exists():\n\n        def _ld(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(root / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        d = DatasetDict(\n            {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n        )\n        return d\n    # synthetic tiny fallback\n    shapes, colours = string.ascii_uppercase[:6], string.ascii_lowercase[:6]\n\n    def _gen(n):\n        seqs, lbl = [], []\n        for _ in range(n):\n            L = random.randint(4, 12)\n            toks = [random.choice(shapes) + random.choice(colours) for _ in range(L)]\n            seqs.append(\" \".join(toks))\n            lbl.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seqs, \"label\": lbl}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(400)),\n            \"dev\": Dataset.from_dict(_gen(120)),\n            \"test\": Dataset.from_dict(_gen(200)),\n        }\n    )\n\n\ndataset = load_dataset_spr()\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f'Dataset sizes: train={len(dataset[\"train\"])}, dev={len(dataset[\"dev\"])}, test={len(dataset[\"test\"])}'\n)\n\n# ---------- build vocabularies ----------\nshape_vocab, colour_vocab = {}, {}\n\n\ndef idx(d, key):\n    if key not in d:\n        d[key] = len(d)\n    return d[key]\n\n\nmax_len = 0\nfor seq in dataset[\"train\"][\"sequence\"]:\n    toks = seq.split()\n    max_len = max(max_len, len(toks))\n    for t in toks:\n        idx(shape_vocab, t[0])\n        idx(colour_vocab, t[1])\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    n = len(toks)\n    shape_ids = torch.tensor([shape_vocab[t[0]] for t in toks], dtype=torch.long)\n    colour_ids = torch.tensor([colour_vocab[t[1]] for t in toks], dtype=torch.long)\n    pos_ids = torch.tensor(list(range(n)), dtype=torch.long)\n\n    # sequential edges\n    src = list(range(n - 1))\n    dst = list(range(1, n))\n    edges_s = [(i, j) for i, j in zip(src, dst)] + [(j, i) for i, j in zip(src, dst)]\n    # same shape edges\n    for i in range(n):\n        for j in range(i + 1, n):\n            if toks[i][0] == toks[j][0]:\n                edges_s.append((i, j))\n                edges_s.append((j, i))\n            if toks[i][1] == toks[j][1]:\n                edges_s.append((i, j))\n                edges_s.append((j, i))\n    if edges_s:\n        edge_index = torch.tensor(list(map(list, zip(*edges_s))), dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    data = Data(\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        shape_id=shape_ids,\n        colour_id=colour_ids,\n        pos_id=pos_ids,\n        seq_raw=seq,\n    )\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------- GNN model ----------\nclass GNNClassifier(nn.Module):\n    def __init__(self, s_vocab, c_vocab, max_pos, emb=32, hidden=64, num_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(len(s_vocab), emb)\n        self.col_emb = nn.Embedding(len(c_vocab), emb)\n        self.pos_emb = nn.Embedding(max_pos + 1, emb)\n        self.conv1 = SAGEConv(emb, hidden)\n        self.conv2 = SAGEConv(hidden, hidden)\n        self.lin = nn.Linear(hidden, num_cls)\n\n    def forward(self, data):\n        x = (\n            self.shape_emb(data.shape_id)\n            + self.col_emb(data.colour_id)\n            + self.pos_emb(data.pos_id)\n        )\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ---------- experiment tracking ----------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------- training ----------\nlr, epochs, bs_train, bs_eval = 1e-3, 8, 64, 128\nmodel = GNNClassifier(\n    shape_vocab, colour_vocab, max_len, emb=48, hidden=96, num_cls=num_classes\n).to(device)\noptimizer = Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_loader = DataLoader(train_graphs, batch_size=bs_train, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=bs_eval)\n\nfor epoch in range(1, epochs + 1):\n    # train\n    model.train()\n    tloss, tcorrect = 0.0, 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tloss += loss.item() * batch.num_graphs\n        tcorrect += (out.argmax(dim=-1) == batch.y).sum().item()\n    tloss /= len(train_loader.dataset)\n    tacc = tcorrect / len(train_loader.dataset)\n\n    # validation\n    model.eval()\n    vloss, seqs, preds, gts = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            vloss += loss.item() * batch.num_graphs\n            p = logits.argmax(dim=-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(batch.seq_raw)\n    vloss /= len(dev_loader.dataset)\n    vacc = float(np.mean([p == g for p, g in zip(preds, gts)]))\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n\n    # log\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tloss)\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(vloss)\n    experiment_data[\"SPR\"][\"metrics\"][\"train\"].append({\"acc\": tacc})\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"acc\": vacc, \"cwa\": cwa, \"swa\": swa, \"hpa\": hpa}\n    )\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: validation_loss = {vloss:.4f} | acc={vacc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n    )\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment complete \u2013 data saved.\")\n", "import os, pathlib, random, string, time, warnings, math, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader, Batch\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ---------- working dir & device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metric helpers ----------\ndef count_color_variety(seq):\n    return len({t[1] for t in seq.split() if len(t) > 1})\n\n\ndef count_shape_variety(seq):\n    return len({t[0] for t in seq.split() if t})\n\n\ndef CWA(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [w_ if t == p else 0 for w_, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef SWA(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [w_ if t == p else 0 for w_, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef HPA(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-9)\n\n\n# ---------- dataset loading ----------\ndef load_or_synth():\n    try:\n        from datasets import load_dataset, DatasetDict\n\n        root = pathlib.Path(\"./SPR_BENCH\")\n        if root.exists():\n\n            def _ld(name):\n                return load_dataset(\n                    \"csv\",\n                    data_files=str(root / name),\n                    split=\"train\",\n                    cache_dir=\".cache_dsets\",\n                )\n\n            return DatasetDict(\n                {\n                    \"train\": _ld(\"train.csv\"),\n                    \"dev\": _ld(\"dev.csv\"),\n                    \"test\": _ld(\"test.csv\"),\n                }\n            )\n    except Exception as e:\n        warnings.warn(str(e))\n    # synthetic fallback\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def make(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(toks)\n            labels.append(int(toks[0][0] == toks[-1][0]))  # toy rule\n            seqs.append(seq)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(make(600)),\n            \"dev\": Dataset.from_dict(make(200)),\n            \"test\": Dataset.from_dict(make(300)),\n        }\n    )\n\n\nds = load_or_synth()\nnum_classes = len(set(ds[\"train\"][\"label\"]))\nprint(\n    f\"dataset: {len(ds['train'])}/{len(ds['dev'])}/{len(ds['test'])}, classes={num_classes}\"\n)\n\n# ---------- vocabulary ----------\nvocab = {}\n\n\ndef add(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in ds[\"train\"][\"sequence\"]:\n    for t in seq.split():\n        add(t)\nvocab_size = len(vocab)\nprint(\"vocab:\", vocab_size)\n\n\n# ---------- graph builder ----------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    n = len(toks)\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    edges = set()\n    # sequential\n    for i in range(n - 1):\n        edges.add((i, i + 1))\n        edges.add((i + 1, i))\n    # shape & color relations\n    shape_map, color_map = {}, {}\n    for i, t in enumerate(toks):\n        shape_map.setdefault(t[0], []).append(i)\n        color_map.setdefault(t[1], []).append(i)\n    for lst in list(shape_map.values()) + list(color_map.values()):\n        for i in lst:\n            for j in lst:\n                if i != j:\n                    edges.add((i, j))\n    if edges:\n        ei = torch.tensor(list(zip(*edges)), dtype=torch.long)\n    else:\n        ei = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=ei,\n        y=torch.tensor([label], dtype=torch.long),\n        seq_raw=seq,\n    )\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l) for s, l in zip(ds[\"train\"][\"sequence\"], ds[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l) for s, l in zip(ds[\"dev\"][\"sequence\"], ds[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l) for s, l in zip(ds[\"test\"][\"sequence\"], ds[\"test\"][\"label\"])\n]\n\n\n# ---------- model ----------\nclass GNN(nn.Module):\n    def __init__(self, vs, emb=32, hid=64, cls=2):\n        super().__init__()\n        self.emb = nn.Embedding(vs, emb)\n        self.conv1 = SAGEConv(emb, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.lin = nn.Linear(hid, cls)\n\n    def forward(self, data):\n        x = self.emb(data.x.squeeze()).to(device)\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ---------- experiment tracking ----------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ---------- training ----------\nlr = 1e-3\nepochs = 10\nbs_train = 32\nbs_eval = 64\nmodel = GNN(vocab_size, cls=num_classes).to(device)\nopt = Adam(model.parameters(), lr=lr)\ncrit = nn.CrossEntropyLoss()\n\ntrain_loader = DataLoader(train_graphs, batch_size=bs_train, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=bs_eval)\nprint(\"\\nTraining...\")\nfor epoch in range(1, epochs + 1):\n    # --- train ---\n    model.train()\n    tloss = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        opt.zero_grad()\n        out = model(batch)\n        loss = crit(out, batch.y)\n        loss.backward()\n        opt.step()\n        tloss += loss.item() * batch.num_graphs\n    tloss /= len(train_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tloss)\n    # --- val ---\n    model.eval()\n    vloss = 0.0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = crit(logits, batch.y)\n            vloss += loss.item() * batch.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(batch.seq_raw)\n    vloss /= len(dev_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(vloss)\n    acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n    cwa = CWA(seqs, gts, preds)\n    swa = SWA(seqs, gts, preds)\n    hpa = HPA(cwa, swa)\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"hpa\": hpa}\n    )\n    print(\n        f\"Epoch {epoch}: val_loss={vloss:.4f}, acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, HPA={hpa:.3f}\"\n    )\n\n# ---------- final test evaluation ----------\ntest_loader = DataLoader(test_graphs, batch_size=bs_eval)\nmodel.eval()\npreds = []\ngts = []\nseqs = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch)\n        preds.extend(logits.argmax(-1).cpu().tolist())\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq_raw)\ncwa = CWA(seqs, gts, preds)\nswa = SWA(seqs, gts, preds)\nhpa = HPA(cwa, swa)\nprint(f\"\\nTEST  CWA={cwa:.3f}, SWA={swa:.3f}, HPA={hpa:.3f}\")\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data.\")\n", "import os, pathlib, random, string, time, json, warnings, math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ---------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------- metric helpers -------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# -------------------- load or fake data ----------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return {\n        \"train\": _load(\"train.csv\"),\n        \"dev\": _load(\"dev.csv\"),\n        \"test\": _load(\"test.csv\"),\n    }\n\n\ndef make_tiny_synthetic(n_tr=300, n_dev=100, n_te=150):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 12)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(toks)\n            labels.append(int(toks[0][0] == toks[-1][0]))\n            seqs.append(seq)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset\n\n    return {\n        k: Dataset.from_dict(gen(v))\n        for k, v in [(\"train\", n_tr), (\"dev\", n_dev), (\"test\", n_te)]\n    }\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"SPR_BENCH not found, using synthetic toy data.\")\n    dataset = make_tiny_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Train={len(dataset['train'])}, Dev={len(dataset['dev'])}, Test={len(dataset['test'])}, Classes={num_classes}\"\n)\n\n# --------------------- vocabulary ----------------------------\nvocab = {}\n\n\ndef add(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ------------------ graph construction -----------------------\ndef seq_to_graph(seq, label, idx):\n    toks = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    if len(toks) > 1:\n        src = list(range(len(toks) - 1))\n        dst = list(range(1, len(toks)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    d = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq_idx=torch.tensor([idx], dtype=torch.long),\n    )\n    return d\n\n\ntrain_seq = dataset[\"train\"][\"sequence\"]\ndev_seq = dataset[\"dev\"][\"sequence\"]\ntest_seq = dataset[\"test\"][\"sequence\"]\n\ntrain_graphs = [\n    seq_to_graph(s, l, i)\n    for i, (s, l) in enumerate(zip(train_seq, dataset[\"train\"][\"label\"]))\n]\ndev_graphs = [\n    seq_to_graph(s, l, i)\n    for i, (s, l) in enumerate(zip(dev_seq, dataset[\"dev\"][\"label\"]))\n]\ntest_graphs = [\n    seq_to_graph(s, l, i)\n    for i, (s, l) in enumerate(zip(test_seq, dataset[\"test\"][\"label\"]))\n]\n\n\n# ------------------------- model -----------------------------\nclass SPRGraphNet(nn.Module):\n    def __init__(self, vocab, emb=32, hid=64, n_class=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), emb)\n        self.conv1 = SAGEConv(emb, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.lin = nn.Linear(hid, n_class)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze()).to(device)\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------- experiment tracking ---------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------------- training loop -------------------------\nlrs = [3e-4, 1e-3, 3e-3]\nepochs = 6\nbatch_train = 32\nbatch_eval = 64\n\nfor lr in lrs:\n    print(f\"\\n=== LR {lr:.0e} ===\")\n    model = SPRGraphNet(vocab, n_class=num_classes).to(device)\n    optim = Adam(model.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=batch_train, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=batch_eval)\n    for ep in range(1, epochs + 1):\n        # --- train ---\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optim.zero_grad()\n            out = model(batch)\n            loss = loss_fn(out, batch.y)\n            loss.backward()\n            optim.step()\n            tot_loss += loss.item() * batch.num_graphs\n        tr_loss = tot_loss / len(train_loader.dataset)\n        experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n        # --- eval ---\n        model.eval()\n        dev_loss = 0.0\n        preds = []\n        gts = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = loss_fn(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                pr = logits.argmax(-1).cpu().tolist()\n                gt = batch.y.cpu().tolist()\n                idxs = batch.seq_idx.cpu().tolist()\n                preds.extend(pr)\n                gts.extend(gt)\n                seqs.extend([dev_seq[i] for i in idxs])\n        dev_loss /= len(dev_loader.dataset)\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        hpa = harmonic_poly_accuracy(cwa, swa)\n        experiment_data[\"SPR\"][\"losses\"][\"val\"].append(dev_loss)\n        experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"hpa\": hpa}\n        )\n        print(\n            f\"Epoch {ep}: validation_loss = {dev_loss:.4f}  Acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n        )\n\n# --------------- save everything -----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, string, math, time, json, warnings, pathlib\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ---------- mandatory working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ----------\ndef count_color_variety(seq):  # token[1] = colour\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):  # token[0] = shape\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y, yhat):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y, yhat)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y, yhat):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y, yhat)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------- dataset ----------\ndef load_spr_bench_if_present():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    from datasets import load_dataset, DatasetDict\n\n    def ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = {\"train\": ld(\"train.csv\"), \"dev\": ld(\"dev.csv\"), \"test\": ld(\"test.csv\")}\n    from datasets import DatasetDict\n\n    return DatasetDict(d)\n\n\ndef make_synthetic(n_tr=400, n_dev=120, n_te=150):\n    shapes = list(string.ascii_uppercase[:8])\n    colors = list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(5, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(toks))\n            # simple majority-shape rule\n            labels.append(int(sum(t[0] == toks[0][0] for t in toks) > L / 2))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(n_tr)),\n            \"dev\": Dataset.from_dict(gen(n_dev)),\n            \"test\": Dataset.from_dict(gen(n_te)),\n        }\n    )\n\n\ndataset = load_spr_bench_if_present() or make_synthetic()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n\n# ---------- vocab ----------\nshape2idx, color2idx = {}, {}\n\n\ndef add_tok(tok):\n    if tok[0] not in shape2idx:\n        shape2idx[tok[0]] = len(shape2idx)\n    if tok[1] not in color2idx:\n        color2idx[tok[1]] = len(color2idx)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"Vocab: shapes={n_shapes} colours={n_colors}  classes={num_classes}\")\n\n\n# ---------- graph construction ----------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    n = len(toks)\n    shape_ids = [shape2idx[t[0]] for t in toks]\n    color_ids = [color2idx[t[1]] for t in toks]\n\n    # relation 0: sequential\n    src0 = list(range(n - 1))\n    dst0 = list(range(1, n))\n    # relation 1: same-shape\n    src1, dst1 = [], []\n    bucket = {}\n    for i, s in enumerate(shape_ids):\n        bucket.setdefault(s, []).append(i)\n    for ids in bucket.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src1.append(i)\n                    dst1.append(j)\n    # relation 2: same-colour\n    src2, dst2 = [], []\n    bucket = {}\n    for i, c in enumerate(color_ids):\n        bucket.setdefault(c, []).append(i)\n    for ids in bucket.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src2.append(i)\n                    dst2.append(j)\n\n    def mk(src, dst):\n        if not src:\n            return torch.empty((2, 0), dtype=torch.long)\n        ei = torch.tensor([src + dst, dst + src], dtype=torch.long)\n        return ei\n\n    data = Data(\n        x=torch.tensor(list(zip(shape_ids, color_ids)), dtype=torch.long),\n        y=torch.tensor([label], dtype=torch.long),\n        edge_index_seq=mk(src0, dst0),\n        edge_index_shape=mk(src1, dst1),\n        edge_index_color=mk(src2, dst2),\n    )\n    data.seq_raw = seq\n    return data\n\n\ngraphs = {\n    split: [\n        seq_to_graph(s, l)\n        for s, l in zip(dataset[split][\"sequence\"], dataset[split][\"label\"])\n    ]\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n\n# ---------- model ----------\nclass MultiRelSAGE(nn.Module):\n    def __init__(self, in_dim, out_dim, num_rel=3):\n        super().__init__()\n        self.convs = nn.ModuleList([SAGEConv(in_dim, out_dim) for _ in range(num_rel)])\n\n    def forward(self, x, edge_indices):\n        out = 0\n        for conv, ei in zip(self.convs, edge_indices):\n            if ei.numel():\n                out = out + conv(x, ei)\n        return out\n\n\nclass GNN(nn.Module):\n    def __init__(self, n_shapes, n_colors, emb_dim=32, hid=64, n_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb_dim)\n        self.color_emb = nn.Embedding(n_colors, emb_dim)\n        self.lin0 = nn.Linear(emb_dim * 2, hid)\n        self.rel1 = MultiRelSAGE(hid, hid)\n        self.rel2 = MultiRelSAGE(hid, hid)\n        self.dropout = nn.Dropout(0.2)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, batch):\n        sh = self.shape_emb(batch.x[:, 0])\n        co = self.color_emb(batch.x[:, 1])\n        x = F.relu(self.lin0(torch.cat([sh, co], dim=-1)))\n        ei = [batch.edge_index_seq, batch.edge_index_shape, batch.edge_index_color]\n        x = F.relu(self.rel1(x, ei))\n        x = self.dropout(F.relu(self.rel2(x, ei)))\n        x = global_mean_pool(x, batch.batch)\n        return self.out(x)\n\n\n# ---------- logging dict ----------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- training / evaluation ----------\ndef evaluate(model, loader, crit):\n    model.eval()\n    tot_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for b in loader:\n            b = b.to(device)\n            logits = model(b)\n            loss = crit(logits, b.y)\n            tot_loss += loss.item() * b.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = b.y.cpu().tolist()\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(b.seq_raw)\n    tot_loss /= len(loader.dataset)\n    acc = np.mean([p == g for p, g in zip(preds, gts)])\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n    return tot_loss, acc, cwa, swa, hpa, preds, gts, seqs\n\n\ndef train_one(lr, emb=32, hid=64, epochs=10):\n    print(f\"\\n=== LR={lr} emb={emb} hid={hid} ===\")\n    model = GNN(n_shapes, n_colors, emb_dim=emb, hid=hid, n_cls=num_classes).to(device)\n    optim = Adam(model.parameters(), lr=lr)\n    crit = nn.CrossEntropyLoss()\n    tl = DataLoader(graphs[\"train\"], batch_size=32, shuffle=True)\n    dl = DataLoader(graphs[\"dev\"], batch_size=64)\n    best_hpa, best_state = -1, None\n    for ep in range(1, epochs + 1):\n        model.train()\n        tot = 0\n        for b in tl:\n            b = b.to(device)\n            optim.zero_grad()\n            loss = crit(model(b), b.y)\n            loss.backward()\n            optim.step()\n            tot += loss.item() * b.num_graphs\n        tr_loss = tot / len(tl.dataset)\n        val_loss, acc, cwa, swa, hpa, *_ = evaluate(model, dl, crit)\n        print(\n            f\"Epoch {ep}: validation_loss = {val_loss:.4f} | acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n        )\n        # log\n        experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n        )\n        experiment_data[\"SPR\"][\"epochs\"].append(ep)\n        if hpa > best_hpa:\n            best_hpa, best_state = hpa, model.state_dict()\n    # restore best\n    model.load_state_dict(best_state)\n    return model\n\n\nbest_model = None\nbest_hpa = -1\nfor lr in [1e-3, 3e-3]:\n    mdl = train_one(lr)\n    hpa = experiment_data[\"SPR\"][\"metrics\"][\"val\"][-1][\"HPA\"]\n    if hpa > best_hpa:\n        best_hpa, best_model = hpa, mdl\n\n# ---------- final test evaluation ----------\ntest_loader = DataLoader(graphs[\"test\"], batch_size=64)\ncrit = nn.CrossEntropyLoss()\ntest_loss, acc, cwa, swa, hpa, preds, gts, seqs = evaluate(\n    best_model, test_loader, crit\n)\nprint(\n    f\"\\nTEST  loss={test_loss:.4f} acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n)\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, warnings, gc\nfrom typing import List, Tuple\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# -------------------- mandatory working dir -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device handling -------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------- metrics ----------------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# -------------------- dataset loading -------------------------\ndef load_spr_bench() -> \"datasets.DatasetDict|None\":  # type: ignore\n    try:\n        from datasets import load_dataset, DatasetDict\n    except ImportError:\n        return None\n    root = pathlib.Path(\"SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    from datasets import DatasetDict\n\n    return DatasetDict(\n        {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    )\n\n\ndef make_synthetic(nt=500, nd=150, nte=200):\n    shapes = list(string.ascii_uppercase[:8])\n    colors = list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 18)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(toks)\n            label = int(sum(t[0] == toks[0][0] for t in toks) > L / 2)\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(nt)),\n            \"dev\": Dataset.from_dict(gen(nd)),\n            \"test\": Dataset.from_dict(gen(nte)),\n        }\n    )\n\n\ndset = load_spr_bench() or make_synthetic()\nprint(\"Loaded dataset sizes:\", {k: len(v) for k, v in dset.items()})\n\n# ------------------- vocab construction -----------------------\nshape2idx, color2idx = {}, {}\n\n\ndef add_tok(tok):\n    if tok[0] not in shape2idx:\n        shape2idx[tok[0]] = len(shape2idx)\n    if tok[1] not in color2idx:\n        color2idx[tok[1]] = len(color2idx)\n\n\nfor seq in dset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nprint(f\"Vocab sizes -> shapes:{n_shapes} colors:{n_colors} classes:{num_classes}\")\n\n# ------------------- seq -> graph -----------------------------\nREL_CONSEC, REL_SAME_SHAPE, REL_SAME_COLOR = 0, 1, 2\n\n\ndef seq_to_graph(seq: str, label: int) -> Data:\n    toks = seq.split()\n    n = len(toks)\n    shp_ids = [shape2idx[t[0]] for t in toks]\n    col_ids = [color2idx[t[1]] for t in toks]\n    x = torch.tensor(list(zip(shp_ids, col_ids)), dtype=torch.long)\n\n    src, dst, etype = [], [], []\n    # consecutive edges (bidirectional)\n    for i in range(n - 1):\n        src.extend([i, i + 1])\n        dst.extend([i + 1, i])\n        etype.extend([REL_CONSEC] * 2)\n    # same shape\n    by_shape = {}\n    for i, s in enumerate(shp_ids):\n        by_shape.setdefault(s, []).append(i)\n    for ids in by_shape.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src.extend([i, j])\n                    dst.extend([j, i])\n                    etype.extend([REL_SAME_SHAPE] * 2)\n    # same color\n    by_color = {}\n    for i, c in enumerate(col_ids):\n        by_color.setdefault(c, []).append(i)\n    for ids in by_color.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src.extend([i, j])\n                    dst.extend([j, i])\n                    etype.extend([REL_SAME_COLOR] * 2)\n\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    edge_type = torch.tensor(etype, dtype=torch.long)\n    data = Data(\n        x=x, edge_index=edge_index, edge_type=edge_type, y=torch.tensor([label])\n    )\n    data.seq_raw = seq  # keep for metric calc\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dset[\"train\"][\"sequence\"], dset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l) for s, l in zip(dset[\"dev\"][\"sequence\"], dset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l) for s, l in zip(dset[\"test\"][\"sequence\"], dset[\"test\"][\"label\"])\n]\n\n\n# ---------------------- model ---------------------------------\nclass RGCNClassifier(nn.Module):\n    def __init__(\n        self,\n        n_shapes: int,\n        n_colors: int,\n        emb: int = 32,\n        hid: int = 64,\n        n_cls: int = 2,\n        num_rels: int = 3,\n    ):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb)\n        self.color_emb = nn.Embedding(n_colors, emb)\n        self.lin_in = nn.Linear(emb * 2, hid)\n        self.rgcn1 = RGCNConv(hid, hid, num_rels)\n        self.rgcn2 = RGCNConv(hid, hid, num_rels)\n        self.rgcn3 = RGCNConv(hid, hid, num_rels)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, data):\n        sh = self.shape_emb(data.x[:, 0])\n        co = self.color_emb(data.x[:, 1])\n        x = F.relu(self.lin_in(torch.cat([sh, co], dim=-1)))\n        x = F.relu(self.rgcn1(x, data.edge_index, data.edge_type))\n        x = F.relu(self.rgcn2(x, data.edge_index, data.edge_type))\n        x = F.relu(self.rgcn3(x, data.edge_index, data.edge_type))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# ------------------ experiment log dict -----------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ------------------ training / evaluation ---------------------\ndef evaluate(model, loader, crit):\n    model.eval()\n    tot_loss = 0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = crit(logits, batch.y)\n            tot_loss += loss.item() * batch.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            seqs.extend(batch.seq_raw)\n            preds.extend(p)\n            gts.extend(g)\n    avg_loss = tot_loss / len(loader.dataset)\n    acc = np.mean([p == g for p, g in zip(preds, gts)])\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n    return avg_loss, acc, cwa, swa, hpa, preds, gts, seqs\n\n\ndef run_experiment(\n    lr: float, weight_decay: float = 1e-4, max_epochs: int = 15, patience: int = 3\n):\n    model = RGCNClassifier(n_shapes, n_colors, n_cls=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    crit = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=64)\n\n    best_hpa = -1\n    best_state = None\n    no_improve = 0\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        tot_loss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            logits = model(batch)\n            loss = crit(logits, batch.y)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch.num_graphs\n        tr_loss = tot_loss / len(train_loader.dataset)\n\n        dev_loss, acc, cwa, swa, hpa, *_ = evaluate(model, dev_loader, crit)\n        print(\n            f\"LR {lr} Epoch {epoch}: validation_loss = {dev_loss:.4f} | acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n        )\n\n        experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR\"][\"losses\"][\"val\"].append(dev_loss)\n        experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n        )\n        experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n\n        if hpa > best_hpa:\n            best_hpa = hpa\n            best_state = model.state_dict()\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            break\n    model.load_state_dict(best_state)\n    return model, best_hpa\n\n\nbest_model = None\nbest_dev_hpa = -1\nfor lr in [1e-3, 3e-3]:\n    print(f\"\\n========= Running LR {lr} =========\")\n    model, hpa = run_experiment(lr)\n    if hpa > best_dev_hpa:\n        best_dev_hpa = hpa\n        best_model = model\n\n# ------------------ final test evaluation ---------------------\ntest_loader = DataLoader(test_graphs, batch_size=64)\ncrit = nn.CrossEntropyLoss()\ntest_loss, acc, cwa, swa, hpa, preds, gts, seqs = evaluate(\n    best_model, test_loader, crit\n)\nprint(\n    f\"\\nTEST RESULTS -> loss:{test_loss:.4f} acc:{acc:.3f} CWA:{cwa:.3f} SWA:{swa:.3f} HPA:{hpa:.3f}\"\n)\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, string, math, time, pathlib, warnings, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# ---------- mandatory working dir & device -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- metrics ------------------------------------\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------- dataset loading (real or synthetic) ----------\ndef try_load_spr_bench():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    from datasets import load_dataset, DatasetDict\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n\n\ndef make_synth(n):\n    shapes = list(string.ascii_uppercase[:8])\n    colors = list(string.ascii_lowercase[:8])\n\n    def gen(m):\n        seqs, lbs = [], []\n        for _ in range(m):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(toks)\n            label = int(sum(t[0] == toks[0][0] for t in toks) > L / 2)\n            seqs.append(seq)\n            lbs.append(label)\n        return {\"sequence\": seqs, \"label\": lbs}\n\n    from datasets import Dataset, DatasetDict\n\n    return {\n        \"train\": Dataset.from_dict(gen(n)),\n        \"dev\": Dataset.from_dict(gen(n // 3)),\n        \"test\": Dataset.from_dict(gen(n // 3)),\n    }\n\n\ndataset = try_load_spr_bench() or make_synth(900)\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------- vocab ---------------------------------------\nshape2idx\ncolor2idx = {}, {}\n\n\ndef add(tok):\n    shape2idx.setdefault(tok[0], len(shape2idx))\n    color2idx.setdefault(tok[1], len(color2idx))\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nmax_pos = 64\n\n\n# ---------- seq -> graph ---------------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    L = len(toks)\n    shape_ids = [shape2idx[t[0]] for t in toks]\n    color_ids = [color2idx[t[1]] for t in toks]\n    pos_ids = list(range(L))\n    src, dst, etype = [], [], []\n    # type 0 : order edges\n    for i in range(L - 1):\n        src.extend([i, i + 1])\n        dst.extend([i + 1, i])\n        etype.extend([0, 0])\n    # type 1 : same-shape\n    bucket = {}\n    for i, s in enumerate(shape_ids):\n        bucket.setdefault(s, []).append(i)\n    for ids in bucket.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src.extend([i, j])\n                    dst.extend([j, i])\n                    etype.extend([1, 1])\n    # type 2 : same-color\n    bucket = {}\n    for i, c in enumerate(color_ids):\n        bucket.setdefault(c, []).append(i)\n    for ids in bucket.values():\n        for i in ids:\n            for j in ids:\n                if i < j:\n                    src.extend([i, j])\n                    dst.extend([j, i])\n                    etype.extend([2, 2])\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    edge_type = torch.tensor(etype, dtype=torch.long)\n    x = torch.tensor(list(zip(shape_ids, color_ids, pos_ids)), dtype=torch.long)\n    d = Data(\n        x=x,\n        edge_index=edge_index,\n        edge_type=edge_type,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    d.seq_raw = seq\n    return d\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------- model ----------------------------------------\nclass RGCNClassifier(nn.Module):\n    def __init__(self, shapes, colors, pos, max_rel=3, emb=32, hid=64, cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(shapes, emb)\n        self.color_emb = nn.Embedding(colors, emb)\n        self.pos_emb = nn.Embedding(pos, emb)\n        self.lin_in = nn.Linear(emb * 3, hid)\n        self.conv1 = RGCNConv(hid, hid, num_relations=max_rel)\n        self.conv2 = RGCNConv(hid, hid, num_relations=max_rel)\n        self.out = nn.Linear(hid, cls)\n\n    def forward(self, data):\n        s = self.shape_emb(data.x[:, 0])\n        c = self.color_emb(data.x[:, 1])\n        p = self.pos_emb(torch.clamp(data.x[:, 2], 0, max_pos - 1))\n        x = torch.cat([s, c, p], dim=-1)\n        x = F.relu(self.lin_in(x))\n        x = F.relu(self.conv1(x, data.edge_index, data.edge_type))\n        x = F.relu(self.conv2(x, data.edge_index, data.edge_type))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# ---------- experiment log -------------------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- training routine -----------------------------\ndef train_run(lr, epochs=20, patience=4):\n    model = RGCNClassifier(n_shapes, n_colors, max_pos).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    crit = nn.CrossEntropyLoss()\n    tr_loader = DataLoader(train_graphs, batch_size=64, shuffle=True)\n    dv_loader = DataLoader(dev_graphs, batch_size=128)\n    best_hpa = 0\n    best_state = None\n    no_improve = 0\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tot = 0\n        for batch in tr_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            tot += loss.item() * batch.num_graphs\n        tr_loss = tot / len(tr_loader.dataset)\n        # evaluate on dev\n        model.eval()\n        dv_tot = 0\n        preds = []\n        gts = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dv_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = crit(logits, batch.y)\n                dv_tot += loss.item() * batch.num_graphs\n                p = logits.argmax(-1).cpu().tolist()\n                preds.extend(p)\n                gts.extend(batch.y.cpu().tolist())\n                seqs.extend(batch.seq_raw)\n        dv_loss = dv_tot / len(dv_loader.dataset)\n        acc = float(np.mean([p == g for p, g in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        hpa = harmonic_poly_accuracy(cwa, swa)\n        print(\n            f\"Epoch {ep}: validation_loss = {dv_loss:.4f} | acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n        )\n        # log\n        experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR\"][\"losses\"][\"val\"].append(dv_loss)\n        experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n        )\n        experiment_data[\"SPR\"][\"epochs\"].append(ep)\n        # early stopping\n        if hpa > best_hpa:\n            best_hpa = hpa\n            best_state = model.state_dict()\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                break\n    model.load_state_dict(best_state)\n    return model, best_hpa\n\n\n# ---------- hyper-parameter sweep & testing ---------------\nbest_overall = 0\nbest_model = None\nfor lr in [1e-3, 2e-3]:\n    print(f\"\\n=== LR {lr} ===\")\n    model, hpa = train_run(lr)\n    if hpa > best_overall:\n        best_overall = hpa\n        best_model = model\n\n# test evaluation\nts_loader = DataLoader(test_graphs, batch_size=128)\nbest_model.eval()\npreds = []\ngts = []\nseqs = []\nwith torch.no_grad():\n    for batch in ts_loader:\n        batch = batch.to(device)\n        logits = best_model(batch)\n        preds.extend(logits.argmax(-1).cpu().tolist())\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq_raw)\nacc = float(np.mean([p == g for p, g in zip(preds, gts)]))\ncwa = color_weighted_accuracy(seqs, gts, preds)\nswa = shape_weighted_accuracy(seqs, gts, preds)\nhpa = harmonic_poly_accuracy(cwa, swa)\nprint(f\"\\nTEST \u2192 acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\")\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved logs to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, string, warnings, math, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# ---------------- working dir & device ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- metric helpers ----------------------\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------- dataset utils -----------------------\ndef try_load_benchmark():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    try:\n        from datasets import load_dataset, DatasetDict\n    except ImportError:\n        warnings.warn(\"datasets lib not installed; falling back to synthetic data\")\n        return None\n\n    def _ld(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    return d\n\n\ndef make_synthetic(n_tr=500, n_dev=150, n_te=200):\n    shapes, colors = list(string.ascii_uppercase[:8]), list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            label = int(\n                sum(t[0] == toks[0][0] for t in toks) > L / 2\n            )  # simple majority rule\n            seqs.append(\" \".join(toks))\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(n_tr)),\n            \"dev\": Dataset.from_dict(gen(n_dev)),\n            \"test\": Dataset.from_dict(gen(n_te)),\n        }\n    )\n\n\ndataset = try_load_benchmark() or make_synthetic()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n\n# ---------------- vocabularies ------------------------\nshape2idx, color2idx = {}, {}\n\n\ndef add_tok(tok):\n    s, c = tok[0], tok[1]\n    if s not in shape2idx:\n        shape2idx[s] = len(shape2idx)\n    if c not in color2idx:\n        color2idx[c] = len(color2idx)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"Shapes={n_shapes} Colors={n_colors} Classes={num_classes}\")\n\n\n# ---------------- seq -> graph ------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    s_ids = [shape2idx[t[0]] for t in toks]\n    c_ids = [color2idx[t[1]] for t in toks]\n    x = torch.tensor(list(zip(s_ids, c_ids)), dtype=torch.long)\n\n    src, dst, etype = [], [], []  # edge type: 0 consecutive, 1 same-shape, 2 same-color\n    # consecutive\n    for i in range(len(toks) - 1):\n        src += [i, i + 1]\n        dst += [i + 1, i]\n        etype += [0, 0]\n    # same-shape\n    buckets = {}\n    for i, s in enumerate(s_ids):\n        buckets.setdefault(s, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [1, 1]\n    # same-color\n    buckets = {}\n    for i, c in enumerate(c_ids):\n        buckets.setdefault(c, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [2, 2]\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    edge_type = torch.tensor(etype, dtype=torch.long)\n    data = Data(\n        x=x, edge_index=edge_index, edge_type=edge_type, y=torch.tensor([label])\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------------- model --------------------------------\nclass RGCNClassifier(nn.Module):\n    def __init__(self, n_shapes, n_colors, n_relations, emb=32, hid=64, n_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb)\n        self.color_emb = nn.Embedding(n_colors, emb)\n        self.lin_in = nn.Linear(emb * 2, hid)\n        self.conv1 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.conv2 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, data):\n        sh = self.shape_emb(data.x[:, 0])\n        co = self.color_emb(data.x[:, 1])\n        x = F.relu(self.lin_in(torch.cat([sh, co], dim=-1)))\n        x = F.relu(self.conv1(x, data.edge_index, data.edge_type))\n        x = F.relu(self.conv2(x, data.edge_index, data.edge_type))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# --------------- experiment logger --------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# --------------- helpers ------------------------------\ndef build_loader(graphs, bs, shuffle=False):\n    return DataLoader(graphs, batch_size=bs, shuffle=shuffle)\n\n\ntrain_loader = build_loader(train_graphs, 32, True)\ndev_loader = build_loader(dev_graphs, 64)\ntest_loader = build_loader(test_graphs, 64)\n\n# class weights\nlabels_tensor = torch.tensor(dataset[\"train\"][\"label\"])\nclass_weights = torch.bincount(labels_tensor).float()\nclass_weights = 1.0 / (class_weights + 1e-6)\nclass_weights = class_weights / class_weights.sum() * num_classes\nclass_weights = class_weights.to(device)\n\n# ---------------- training -----------------------------\nmodel = RGCNClassifier(n_shapes, n_colors, n_relations=3, n_cls=num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=3e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\nbest_hpa, best_state = -1, None\nEPOCHS = 15\n\nfor epoch in range(1, EPOCHS + 1):\n    # train\n    model.train()\n    tot_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n    train_loss = tot_loss / len(train_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n\n    # validate\n    model.eval()\n    v_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            s = batch.seq_raw\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(s)\n    v_loss /= len(dev_loader.dataset)\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n    )\n\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n    )\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n\n    if hpa > best_hpa:\n        best_hpa = hpa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n    scheduler.step()\n\n# ---------------- test with best model ----------------\nmodel.load_state_dict(best_state)\nmodel.to(device)\nmodel.eval()\npreds, gts, seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch)\n        preds.extend(logits.argmax(-1).cpu().tolist())\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq_raw)\ncwa = color_weighted_accuracy(seqs, gts, preds)\nswa = shape_weighted_accuracy(seqs, gts, preds)\nhpa = harmonic_poly_accuracy(cwa, swa)\nprint(f\"TEST  | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\")\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved logs ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, string, warnings, math, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# ---------------- working dir & device ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- metric helpers ----------------------\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------- dataset utils -----------------------\ndef try_load_benchmark():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    try:\n        from datasets import load_dataset, DatasetDict\n    except ImportError:\n        warnings.warn(\"datasets lib not installed; falling back to synthetic data\")\n        return None\n\n    def _ld(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    return d\n\n\ndef make_synthetic(n_tr=500, n_dev=150, n_te=200):\n    shapes, colors = list(string.ascii_uppercase[:8]), list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            label = int(\n                sum(t[0] == toks[0][0] for t in toks) > L / 2\n            )  # simple majority rule\n            seqs.append(\" \".join(toks))\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(n_tr)),\n            \"dev\": Dataset.from_dict(gen(n_dev)),\n            \"test\": Dataset.from_dict(gen(n_te)),\n        }\n    )\n\n\ndataset = try_load_benchmark() or make_synthetic()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n\n# ---------------- vocabularies ------------------------\nshape2idx, color2idx = {}, {}\n\n\ndef add_tok(tok):\n    s, c = tok[0], tok[1]\n    if s not in shape2idx:\n        shape2idx[s] = len(shape2idx)\n    if c not in color2idx:\n        color2idx[c] = len(color2idx)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"Shapes={n_shapes} Colors={n_colors} Classes={num_classes}\")\n\n\n# ---------------- seq -> graph ------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    s_ids = [shape2idx[t[0]] for t in toks]\n    c_ids = [color2idx[t[1]] for t in toks]\n    x = torch.tensor(list(zip(s_ids, c_ids)), dtype=torch.long)\n\n    src, dst, etype = [], [], []  # edge type: 0 consecutive, 1 same-shape, 2 same-color\n    # consecutive\n    for i in range(len(toks) - 1):\n        src += [i, i + 1]\n        dst += [i + 1, i]\n        etype += [0, 0]\n    # same-shape\n    buckets = {}\n    for i, s in enumerate(s_ids):\n        buckets.setdefault(s, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [1, 1]\n    # same-color\n    buckets = {}\n    for i, c in enumerate(c_ids):\n        buckets.setdefault(c, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [2, 2]\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    edge_type = torch.tensor(etype, dtype=torch.long)\n    data = Data(\n        x=x, edge_index=edge_index, edge_type=edge_type, y=torch.tensor([label])\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------------- model --------------------------------\nclass RGCNClassifier(nn.Module):\n    def __init__(self, n_shapes, n_colors, n_relations, emb=32, hid=64, n_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb)\n        self.color_emb = nn.Embedding(n_colors, emb)\n        self.lin_in = nn.Linear(emb * 2, hid)\n        self.conv1 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.conv2 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, data):\n        sh = self.shape_emb(data.x[:, 0])\n        co = self.color_emb(data.x[:, 1])\n        x = F.relu(self.lin_in(torch.cat([sh, co], dim=-1)))\n        x = F.relu(self.conv1(x, data.edge_index, data.edge_type))\n        x = F.relu(self.conv2(x, data.edge_index, data.edge_type))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# --------------- experiment logger --------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# --------------- helpers ------------------------------\ndef build_loader(graphs, bs, shuffle=False):\n    return DataLoader(graphs, batch_size=bs, shuffle=shuffle)\n\n\ntrain_loader = build_loader(train_graphs, 32, True)\ndev_loader = build_loader(dev_graphs, 64)\ntest_loader = build_loader(test_graphs, 64)\n\n# class weights\nlabels_tensor = torch.tensor(dataset[\"train\"][\"label\"])\nclass_weights = torch.bincount(labels_tensor).float()\nclass_weights = 1.0 / (class_weights + 1e-6)\nclass_weights = class_weights / class_weights.sum() * num_classes\nclass_weights = class_weights.to(device)\n\n# ---------------- training -----------------------------\nmodel = RGCNClassifier(n_shapes, n_colors, n_relations=3, n_cls=num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=3e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\nbest_hpa, best_state = -1, None\nEPOCHS = 15\n\nfor epoch in range(1, EPOCHS + 1):\n    # train\n    model.train()\n    tot_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n    train_loss = tot_loss / len(train_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n\n    # validate\n    model.eval()\n    v_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            s = batch.seq_raw\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(s)\n    v_loss /= len(dev_loader.dataset)\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n    )\n\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n    )\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n\n    if hpa > best_hpa:\n        best_hpa = hpa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n    scheduler.step()\n\n# ---------------- test with best model ----------------\nmodel.load_state_dict(best_state)\nmodel.to(device)\nmodel.eval()\npreds, gts, seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch)\n        preds.extend(logits.argmax(-1).cpu().tolist())\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq_raw)\ncwa = color_weighted_accuracy(seqs, gts, preds)\nswa = shape_weighted_accuracy(seqs, gts, preds)\nhpa = harmonic_poly_accuracy(cwa, swa)\nprint(f\"TEST  | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\")\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved logs ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, string, warnings, math, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# ---------------- working dir & device ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- metric helpers ----------------------\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------- dataset utils -----------------------\ndef try_load_benchmark():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    try:\n        from datasets import load_dataset, DatasetDict\n    except ImportError:\n        warnings.warn(\"datasets lib not installed; falling back to synthetic data\")\n        return None\n\n    def _ld(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    return d\n\n\ndef make_synthetic(n_tr=500, n_dev=150, n_te=200):\n    shapes, colors = list(string.ascii_uppercase[:8]), list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            label = int(\n                sum(t[0] == toks[0][0] for t in toks) > L / 2\n            )  # simple majority rule\n            seqs.append(\" \".join(toks))\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(n_tr)),\n            \"dev\": Dataset.from_dict(gen(n_dev)),\n            \"test\": Dataset.from_dict(gen(n_te)),\n        }\n    )\n\n\ndataset = try_load_benchmark() or make_synthetic()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n\n# ---------------- vocabularies ------------------------\nshape2idx, color2idx = {}, {}\n\n\ndef add_tok(tok):\n    s, c = tok[0], tok[1]\n    if s not in shape2idx:\n        shape2idx[s] = len(shape2idx)\n    if c not in color2idx:\n        color2idx[c] = len(color2idx)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"Shapes={n_shapes} Colors={n_colors} Classes={num_classes}\")\n\n\n# ---------------- seq -> graph ------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    s_ids = [shape2idx[t[0]] for t in toks]\n    c_ids = [color2idx[t[1]] for t in toks]\n    x = torch.tensor(list(zip(s_ids, c_ids)), dtype=torch.long)\n\n    src, dst, etype = [], [], []  # edge type: 0 consecutive, 1 same-shape, 2 same-color\n    # consecutive\n    for i in range(len(toks) - 1):\n        src += [i, i + 1]\n        dst += [i + 1, i]\n        etype += [0, 0]\n    # same-shape\n    buckets = {}\n    for i, s in enumerate(s_ids):\n        buckets.setdefault(s, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [1, 1]\n    # same-color\n    buckets = {}\n    for i, c in enumerate(c_ids):\n        buckets.setdefault(c, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [2, 2]\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    edge_type = torch.tensor(etype, dtype=torch.long)\n    data = Data(\n        x=x, edge_index=edge_index, edge_type=edge_type, y=torch.tensor([label])\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------------- model --------------------------------\nclass RGCNClassifier(nn.Module):\n    def __init__(self, n_shapes, n_colors, n_relations, emb=32, hid=64, n_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb)\n        self.color_emb = nn.Embedding(n_colors, emb)\n        self.lin_in = nn.Linear(emb * 2, hid)\n        self.conv1 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.conv2 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, data):\n        sh = self.shape_emb(data.x[:, 0])\n        co = self.color_emb(data.x[:, 1])\n        x = F.relu(self.lin_in(torch.cat([sh, co], dim=-1)))\n        x = F.relu(self.conv1(x, data.edge_index, data.edge_type))\n        x = F.relu(self.conv2(x, data.edge_index, data.edge_type))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# --------------- experiment logger --------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# --------------- helpers ------------------------------\ndef build_loader(graphs, bs, shuffle=False):\n    return DataLoader(graphs, batch_size=bs, shuffle=shuffle)\n\n\ntrain_loader = build_loader(train_graphs, 32, True)\ndev_loader = build_loader(dev_graphs, 64)\ntest_loader = build_loader(test_graphs, 64)\n\n# class weights\nlabels_tensor = torch.tensor(dataset[\"train\"][\"label\"])\nclass_weights = torch.bincount(labels_tensor).float()\nclass_weights = 1.0 / (class_weights + 1e-6)\nclass_weights = class_weights / class_weights.sum() * num_classes\nclass_weights = class_weights.to(device)\n\n# ---------------- training -----------------------------\nmodel = RGCNClassifier(n_shapes, n_colors, n_relations=3, n_cls=num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=3e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\nbest_hpa, best_state = -1, None\nEPOCHS = 15\n\nfor epoch in range(1, EPOCHS + 1):\n    # train\n    model.train()\n    tot_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n    train_loss = tot_loss / len(train_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n\n    # validate\n    model.eval()\n    v_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            s = batch.seq_raw\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(s)\n    v_loss /= len(dev_loader.dataset)\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n    )\n\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n    )\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n\n    if hpa > best_hpa:\n        best_hpa = hpa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n    scheduler.step()\n\n# ---------------- test with best model ----------------\nmodel.load_state_dict(best_state)\nmodel.to(device)\nmodel.eval()\npreds, gts, seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch)\n        preds.extend(logits.argmax(-1).cpu().tolist())\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq_raw)\ncwa = color_weighted_accuracy(seqs, gts, preds)\nswa = shape_weighted_accuracy(seqs, gts, preds)\nhpa = harmonic_poly_accuracy(cwa, swa)\nprint(f\"TEST  | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\")\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved logs ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, string, warnings, math, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# ---------------- working dir & device ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- metric helpers ----------------------\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef harmonic_poly_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------- dataset utils -----------------------\ndef try_load_benchmark():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    try:\n        from datasets import load_dataset, DatasetDict\n    except ImportError:\n        warnings.warn(\"datasets lib not installed; falling back to synthetic data\")\n        return None\n\n    def _ld(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    return d\n\n\ndef make_synthetic(n_tr=500, n_dev=150, n_te=200):\n    shapes, colors = list(string.ascii_uppercase[:8]), list(string.ascii_lowercase[:8])\n\n    def gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            L = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            label = int(\n                sum(t[0] == toks[0][0] for t in toks) > L / 2\n            )  # simple majority rule\n            seqs.append(\" \".join(toks))\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(gen(n_tr)),\n            \"dev\": Dataset.from_dict(gen(n_dev)),\n            \"test\": Dataset.from_dict(gen(n_te)),\n        }\n    )\n\n\ndataset = try_load_benchmark() or make_synthetic()\nprint(\"Dataset sizes:\", {k: len(v) for k, v in dataset.items()})\n\n# ---------------- vocabularies ------------------------\nshape2idx, color2idx = {}, {}\n\n\ndef add_tok(tok):\n    s, c = tok[0], tok[1]\n    if s not in shape2idx:\n        shape2idx[s] = len(shape2idx)\n    if c not in color2idx:\n        color2idx[c] = len(color2idx)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nn_shapes, n_colors = len(shape2idx), len(color2idx)\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"Shapes={n_shapes} Colors={n_colors} Classes={num_classes}\")\n\n\n# ---------------- seq -> graph ------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    s_ids = [shape2idx[t[0]] for t in toks]\n    c_ids = [color2idx[t[1]] for t in toks]\n    x = torch.tensor(list(zip(s_ids, c_ids)), dtype=torch.long)\n\n    src, dst, etype = [], [], []  # edge type: 0 consecutive, 1 same-shape, 2 same-color\n    # consecutive\n    for i in range(len(toks) - 1):\n        src += [i, i + 1]\n        dst += [i + 1, i]\n        etype += [0, 0]\n    # same-shape\n    buckets = {}\n    for i, s in enumerate(s_ids):\n        buckets.setdefault(s, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [1, 1]\n    # same-color\n    buckets = {}\n    for i, c in enumerate(c_ids):\n        buckets.setdefault(c, []).append(i)\n    for nodes in buckets.values():\n        for i in nodes:\n            for j in nodes:\n                if i < j:\n                    src += [i, j]\n                    dst += [j, i]\n                    etype += [2, 2]\n    edge_index = torch.tensor([src, dst], dtype=torch.long)\n    edge_type = torch.tensor(etype, dtype=torch.long)\n    data = Data(\n        x=x, edge_index=edge_index, edge_type=edge_type, y=torch.tensor([label])\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ---------------- model --------------------------------\nclass RGCNClassifier(nn.Module):\n    def __init__(self, n_shapes, n_colors, n_relations, emb=32, hid=64, n_cls=2):\n        super().__init__()\n        self.shape_emb = nn.Embedding(n_shapes, emb)\n        self.color_emb = nn.Embedding(n_colors, emb)\n        self.lin_in = nn.Linear(emb * 2, hid)\n        self.conv1 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.conv2 = RGCNConv(hid, hid, num_relations=n_relations)\n        self.out = nn.Linear(hid, n_cls)\n\n    def forward(self, data):\n        sh = self.shape_emb(data.x[:, 0])\n        co = self.color_emb(data.x[:, 1])\n        x = F.relu(self.lin_in(torch.cat([sh, co], dim=-1)))\n        x = F.relu(self.conv1(x, data.edge_index, data.edge_type))\n        x = F.relu(self.conv2(x, data.edge_index, data.edge_type))\n        x = global_mean_pool(x, data.batch)\n        return self.out(x)\n\n\n# --------------- experiment logger --------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# --------------- helpers ------------------------------\ndef build_loader(graphs, bs, shuffle=False):\n    return DataLoader(graphs, batch_size=bs, shuffle=shuffle)\n\n\ntrain_loader = build_loader(train_graphs, 32, True)\ndev_loader = build_loader(dev_graphs, 64)\ntest_loader = build_loader(test_graphs, 64)\n\n# class weights\nlabels_tensor = torch.tensor(dataset[\"train\"][\"label\"])\nclass_weights = torch.bincount(labels_tensor).float()\nclass_weights = 1.0 / (class_weights + 1e-6)\nclass_weights = class_weights / class_weights.sum() * num_classes\nclass_weights = class_weights.to(device)\n\n# ---------------- training -----------------------------\nmodel = RGCNClassifier(n_shapes, n_colors, n_relations=3, n_cls=num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=3e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\nbest_hpa, best_state = -1, None\nEPOCHS = 15\n\nfor epoch in range(1, EPOCHS + 1):\n    # train\n    model.train()\n    tot_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n    train_loss = tot_loss / len(train_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n\n    # validate\n    model.eval()\n    v_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            v_loss += loss.item() * batch.num_graphs\n            p = logits.argmax(-1).cpu().tolist()\n            g = batch.y.cpu().tolist()\n            s = batch.seq_raw\n            preds.extend(p)\n            gts.extend(g)\n            seqs.extend(s)\n    v_loss /= len(dev_loader.dataset)\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    hpa = harmonic_poly_accuracy(cwa, swa)\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\"\n    )\n\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n    )\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n\n    if hpa > best_hpa:\n        best_hpa = hpa\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n    scheduler.step()\n\n# ---------------- test with best model ----------------\nmodel.load_state_dict(best_state)\nmodel.to(device)\nmodel.eval()\npreds, gts, seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch)\n        preds.extend(logits.argmax(-1).cpu().tolist())\n        gts.extend(batch.y.cpu().tolist())\n        seqs.extend(batch.seq_raw)\ncwa = color_weighted_accuracy(seqs, gts, preds)\nswa = shape_weighted_accuracy(seqs, gts, preds)\nhpa = harmonic_poly_accuracy(cwa, swa)\nprint(f\"TEST  | CWA={cwa:.3f} SWA={swa:.3f} HPA={hpa:.3f}\")\n\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved logs ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Loaded dataset with 200 train, 60 dev samples, classes=2', '\\n',\n'Vocabulary size: 36', '\\n', '\\n=== Training with learning rate 3e-04 ===',\n'\\n', 'Epoch 1: train_loss=0.6352, val_loss=0.5775, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'Epoch 2: train_loss=0.5833, val_loss=0.5152,\nacc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 3: train_loss=0.5450,\nval_loss=0.4630, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 4:\ntrain_loss=0.5159, val_loss=0.4244, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879',\n'\\n', 'Epoch 5: train_loss=0.5023, val_loss=0.3987, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', '\\n=== Training with learning rate 1e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.6095, val_loss=0.4598, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'Epoch 2: train_loss=0.5035, val_loss=0.3763,\nacc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 3: train_loss=0.5014,\nval_loss=0.3691, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 4:\ntrain_loss=0.4946, val_loss=0.3781, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879',\n'\\n', 'Epoch 5: train_loss=0.4830, val_loss=0.3891, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', '\\n=== Training with learning rate 3e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.5605, val_loss=0.3556, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'Epoch 2: train_loss=0.5234, val_loss=0.3617,\nacc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 3: train_loss=0.4790,\nval_loss=0.4085, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 4:\ntrain_loss=0.4715, val_loss=0.3680, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879',\n'\\n', 'Epoch 5: train_loss=0.4482, val_loss=0.3505, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'All runs finished. Data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 300, 'dev': 100,\n'test': 120}\", '\\n', 'ShapeVocab=8, ColorVocab=8, Classes=2', '\\n', '\\n===\nRunning LR=0.001 ===', '\\n', 'Epoch 1: validation_loss = 0.1977 | acc=0.970\nCWA=0.972 SWA=0.979 HPA=0.975', '\\n', 'Epoch 2: validation_loss = 0.1856 |\nacc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n', 'Epoch 3: validation_loss =\n0.2003 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n', 'Epoch 4:\nvalidation_loss = 0.1498 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n',\n'Epoch 5: validation_loss = 0.1387 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975',\n'\\n', 'Epoch 6: validation_loss = 0.1427 | acc=0.970 CWA=0.972 SWA=0.979\nHPA=0.975', '\\n', 'Epoch 7: validation_loss = 0.1374 | acc=0.970 CWA=0.972\nSWA=0.979 HPA=0.975', '\\n', 'Epoch 8: validation_loss = 0.1418 | acc=0.970\nCWA=0.972 SWA=0.979 HPA=0.975', '\\n', '\\n=== Running LR=0.003 ===', '\\n', 'Epoch\n1: validation_loss = 0.2329 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n',\n'Epoch 2: validation_loss = 0.1873 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975',\n'\\n', 'Epoch 3: validation_loss = 0.1380 | acc=0.970 CWA=0.972 SWA=0.979\nHPA=0.975', '\\n', 'Epoch 4: validation_loss = 0.1493 | acc=0.970 CWA=0.972\nSWA=0.979 HPA=0.975', '\\n', 'Epoch 5: validation_loss = 0.1516 | acc=0.970\nCWA=0.972 SWA=0.979 HPA=0.975', '\\n', 'Epoch 6: validation_loss = 0.1320 |\nacc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n', 'Epoch 7: validation_loss =\n0.1463 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n', 'Epoch 8:\nvalidation_loss = 0.1465 | acc=0.970 CWA=0.972 SWA=0.979 HPA=0.975', '\\n',\n'Saved experiment data ->', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/experiment_data.npy', '\\n', 'Execution time:\n6 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes: train=400, dev=120, test=200',\n'\\n', 'Epoch 1: validation_loss = 0.4599 | acc=0.842 CWA=0.834 SWA=0.854\nHPA=0.844', '\\n', 'Epoch 2: validation_loss = 0.4378 | acc=0.842 CWA=0.834\nSWA=0.854 HPA=0.844', '\\n', 'Epoch 3: validation_loss = 0.4296 | acc=0.842\nCWA=0.834 SWA=0.854 HPA=0.844', '\\n', 'Epoch 4: validation_loss = 0.4263 |\nacc=0.842 CWA=0.834 SWA=0.854 HPA=0.844', '\\n', 'Epoch 5: validation_loss =\n0.4245 | acc=0.842 CWA=0.834 SWA=0.854 HPA=0.844', '\\n', 'Epoch 6:\nvalidation_loss = 0.4203 | acc=0.842 CWA=0.834 SWA=0.854 HPA=0.844', '\\n',\n'Epoch 7: validation_loss = 0.4210 | acc=0.842 CWA=0.834 SWA=0.854 HPA=0.844',\n'\\n', 'Epoch 8: validation_loss = 0.4218 | acc=0.842 CWA=0.834 SWA=0.854\nHPA=0.844', '\\n', 'Experiment complete \u2013 data saved.', '\\n', 'Execution time: 3\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'dataset: 600/200/300, classes=2', '\\n', 'vocab:',\n' ', '36', '\\n', '\\nTraining...', '\\n', 'Epoch 1: val_loss=0.5136, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 2: val_loss=0.4826, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 3: val_loss=0.4849, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 4: val_loss=0.4904, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 5: val_loss=0.4917, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 6: val_loss=0.5069, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 7: val_loss=0.4956, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 8: val_loss=0.5103, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 9: val_loss=0.5169, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', 'Epoch 10: val_loss=0.5110, acc=0.815,\nCWA=0.822, SWA=0.834, HPA=0.828', '\\n', '\\nTEST  CWA=0.818, SWA=0.823,\nHPA=0.820', '\\n', 'Saved experiment data.', '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, using synthetic toy data.',\n'\\n', 'Train=300, Dev=100, Test=150, Classes=2', '\\n', 'Vocab size:', ' ', '36',\n'\\n', '\\n=== LR 3e-04 ===', '\\n', 'Epoch 1: validation_loss = 0.5773  Acc=0.810\nCWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 2: validation_loss = 0.5273\nAcc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 3: validation_loss =\n0.5057  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 4:\nvalidation_loss = 0.5013  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch\n5: validation_loss = 0.5025  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n',\n'Epoch 6: validation_loss = 0.5036  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816',\n'\\n', '\\n=== LR 1e-03 ===', '\\n', 'Epoch 1: validation_loss = 0.4740  Acc=0.810\nCWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 2: validation_loss = 0.4743\nAcc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 3: validation_loss =\n0.4777  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 4:\nvalidation_loss = 0.4823  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch\n5: validation_loss = 0.4832  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n',\n'Epoch 6: validation_loss = 0.4864  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816',\n'\\n', '\\n=== LR 3e-03 ===', '\\n', 'Epoch 1: validation_loss = 0.4873  Acc=0.810\nCWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 2: validation_loss = 0.4969\nAcc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 3: validation_loss =\n0.4982  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch 4:\nvalidation_loss = 0.5048  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n', 'Epoch\n5: validation_loss = 0.5160  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816', '\\n',\n'Epoch 6: validation_loss = 0.5184  Acc=0.810 CWA=0.813 SWA=0.819 HPA=0.816',\n'\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-14/working/experiment_data.npy', '\\n', 'Execution time:\n4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 400, 'dev': 120,\n'test': 150}\", '\\n', 'Vocab: shapes=8 colours=8  classes=2', '\\n', '\\n===\nLR=0.001 emb=32 hid=64 ===', '\\n', 'Epoch 1: validation_loss = 0.1252 |\nacc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 2: validation_loss =\n0.1001 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 3:\nvalidation_loss = 0.0844 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n',\n'Epoch 4: validation_loss = 0.0859 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990',\n'\\n', 'Epoch 5: validation_loss = 0.0920 | acc=0.983 CWA=0.987 SWA=0.993\nHPA=0.990', '\\n', 'Epoch 6: validation_loss = 0.0957 | acc=0.983 CWA=0.987\nSWA=0.993 HPA=0.990', '\\n', 'Epoch 7: validation_loss = 0.1054 | acc=0.983\nCWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 8: validation_loss = 0.1215 |\nacc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 9: validation_loss =\n0.1300 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 10:\nvalidation_loss = 0.1566 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n',\n'\\n=== LR=0.003 emb=32 hid=64 ===', '\\n', 'Epoch 1: validation_loss = 0.0908 |\nacc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 2: validation_loss =\n0.1087 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 3:\nvalidation_loss = 0.1278 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n',\n'Epoch 4: validation_loss = 0.1687 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990',\n'\\n', 'Epoch 5: validation_loss = 0.2081 | acc=0.983 CWA=0.987 SWA=0.993\nHPA=0.990', '\\n', 'Epoch 6: validation_loss = 0.2653 | acc=0.983 CWA=0.987\nSWA=0.993 HPA=0.990', '\\n', 'Epoch 7: validation_loss = 0.3232 | acc=0.983\nCWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 8: validation_loss = 0.3938 |\nacc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 9: validation_loss =\n0.4682 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Epoch 10:\nvalidation_loss = 0.5671 | acc=0.983 CWA=0.987 SWA=0.993 HPA=0.990', '\\n',\n'\\nTEST  loss=0.0258 acc=0.987 CWA=0.987 SWA=0.993 HPA=0.990', '\\n', 'Saved\nexperiment data ->', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n30_17-49-30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded dataset sizes:', ' ', \"{'train': 500,\n'dev': 150, 'test': 200}\", '\\n', 'Vocab sizes -> shapes:8 colors:8 classes:2',\n'\\n', '\\n========= Running LR 0.001 =========', '\\n', 'LR 0.001 Epoch 1:\nvalidation_loss = 0.0619 | acc=0.993 CWA=0.997 SWA=0.998 HPA=0.997', '\\n', 'LR\n0.001 Epoch 2: validation_loss = 0.0580 | acc=0.993 CWA=0.997 SWA=0.998\nHPA=0.997', '\\n', 'LR 0.001 Epoch 3: validation_loss = 0.0481 | acc=0.993\nCWA=0.997 SWA=0.998 HPA=0.997', '\\n', 'LR 0.001 Epoch 4: validation_loss =\n0.0455 | acc=0.993 CWA=0.997 SWA=0.998 HPA=0.997', '\\n', 'Early stopping\ntriggered.', '\\n', '\\n========= Running LR 0.003 =========', '\\n', 'LR 0.003\nEpoch 1: validation_loss = 0.0482 | acc=0.993 CWA=0.997 SWA=0.998 HPA=0.997',\n'\\n', 'LR 0.003 Epoch 2: validation_loss = 0.0357 | acc=0.993 CWA=0.997\nSWA=0.998 HPA=0.997', '\\n', 'LR 0.003 Epoch 3: validation_loss = 0.0367 |\nacc=0.993 CWA=0.997 SWA=0.998 HPA=0.997', '\\n', 'LR 0.003 Epoch 4:\nvalidation_loss = 0.0358 | acc=0.993 CWA=0.997 SWA=0.998 HPA=0.997', '\\n',\n'Early stopping triggered.', '\\n', '\\nTEST RESULTS -> loss:0.0089 acc:1.000\nCWA:1.000 SWA:1.000 HPA:1.000', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 900, 'dev': 300, 'test': 300}\", '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 85, in <module>\\n\nshape2idx\\nNameError: name \\'shape2idx\\' is not defined\\n', 'Execution time: 2\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 500, 'dev': 150,\n'test': 200}\", '\\n', 'Shapes=8 Colors=8 Classes=2', '\\n', 'Epoch 1:\nvalidation_loss = 0.2434 | CWA=1.000 SWA=1.000 HPA=1.000', '\\n', 'Epoch 2:\nvalidation_loss = 0.2220 | CWA=0.928 SWA=0.939 HPA=0.933', '\\n', 'Epoch 3:\nvalidation_loss = 0.0869 | CWA=0.971 SWA=0.976 HPA=0.973', '\\n', 'Epoch 4:\nvalidation_loss = 0.1463 | CWA=0.930 SWA=0.939 HPA=0.934', '\\n', 'Epoch 5:\nvalidation_loss = 0.0667 | CWA=0.969 SWA=0.977 HPA=0.973', '\\n', 'Epoch 6:\nvalidation_loss = 0.1118 | CWA=0.965 SWA=0.972 HPA=0.968', '\\n', 'Epoch 7:\nvalidation_loss = 0.0429 | CWA=0.980 SWA=0.986 HPA=0.983', '\\n', 'Epoch 8:\nvalidation_loss = 0.0525 | CWA=0.974 SWA=0.982 HPA=0.978', '\\n', 'Epoch 9:\nvalidation_loss = 0.0480 | CWA=0.974 SWA=0.982 HPA=0.978', '\\n', 'Epoch 10:\nvalidation_loss = 0.0246 | CWA=0.985 SWA=0.990 HPA=0.988', '\\n', 'Epoch 11:\nvalidation_loss = 0.0333 | CWA=0.979 SWA=0.986 HPA=0.983', '\\n', 'Epoch 12:\nvalidation_loss = 0.0318 | CWA=0.979 SWA=0.986 HPA=0.983', '\\n', 'Epoch 13:\nvalidation_loss = 0.0277 | CWA=0.985 SWA=0.990 HPA=0.988', '\\n', 'Epoch 14:\nvalidation_loss = 0.0247 | CWA=0.985 SWA=0.990 HPA=0.988', '\\n', 'Epoch 15:\nvalidation_loss = 0.0225 | CWA=0.985 SWA=0.990 HPA=0.988', '\\n', 'TEST  |\nCWA=0.980 SWA=0.984 HPA=0.982', '\\n', 'Saved logs ->', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-14/working/experiment_data.npy', '\\n', 'Execution time:\n5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 500, 'dev': 150,\n'test': 200}\", '\\n', 'Shapes=8 Colors=8 Classes=2', '\\n', 'Epoch 1:\nvalidation_loss = 0.6005 | CWA=0.987 SWA=0.990 HPA=0.988', '\\n', 'Epoch 2:\nvalidation_loss = 0.6296 | CWA=0.987 SWA=0.990 HPA=0.988', '\\n', 'Epoch 3:\nvalidation_loss = 0.5786 | CWA=0.987 SWA=0.990 HPA=0.988', '\\n', 'Epoch 4:\nvalidation_loss = 0.5623 | CWA=0.964 SWA=0.977 HPA=0.970', '\\n', 'Epoch 5:\nvalidation_loss = 0.6969 | CWA=0.964 SWA=0.977 HPA=0.970', '\\n', 'Epoch 6:\nvalidation_loss = 0.6891 | CWA=0.921 SWA=0.939 HPA=0.930', '\\n', 'Epoch 7:\nvalidation_loss = 1.0619 | CWA=0.950 SWA=0.967 HPA=0.959', '\\n', 'Epoch 8:\nvalidation_loss = 1.3149 | CWA=0.963 SWA=0.976 HPA=0.969', '\\n', 'Epoch 9:\nvalidation_loss = 1.7635 | CWA=0.973 SWA=0.983 HPA=0.978', '\\n', 'Epoch 10:\nvalidation_loss = 1.9507 | CWA=0.973 SWA=0.983 HPA=0.978', '\\n', 'Epoch 11:\nvalidation_loss = 2.2260 | CWA=0.982 SWA=0.987 HPA=0.984', '\\n', 'Epoch 12:\nvalidation_loss = 2.1961 | CWA=0.982 SWA=0.987 HPA=0.984', '\\n', 'Epoch 13:\nvalidation_loss = 2.2897 | CWA=0.982 SWA=0.987 HPA=0.984', '\\n', 'Epoch 14:\nvalidation_loss = 2.3889 | CWA=0.982 SWA=0.987 HPA=0.984', '\\n', 'Epoch 15:\nvalidation_loss = 2.4787 | CWA=0.982 SWA=0.987 HPA=0.984', '\\n', 'TEST  |\nCWA=0.992 SWA=0.993 HPA=0.992', '\\n', 'Saved logs ->', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/experiment_data.npy', '\\n', 'Execution time:\n5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 500, 'dev': 150,\n'test': 200}\", '\\n', 'Shapes=8 Colors=8 Classes=2', '\\n', 'Epoch 1:\nvalidation_loss = 1.7093 | CWA=0.974 SWA=0.982 HPA=0.978', '\\n', 'Epoch 2:\nvalidation_loss = 0.6571 | CWA=0.907 SWA=0.928 HPA=0.917', '\\n', 'Epoch 3:\nvalidation_loss = 0.9917 | CWA=0.963 SWA=0.973 HPA=0.968', '\\n', 'Epoch 4:\nvalidation_loss = 0.9605 | CWA=0.849 SWA=0.871 HPA=0.860', '\\n', 'Epoch 5:\nvalidation_loss = 2.2924 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 6:\nvalidation_loss = 2.1680 | CWA=0.939 SWA=0.955 HPA=0.947', '\\n', 'Epoch 7:\nvalidation_loss = 3.3968 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 8:\nvalidation_loss = 3.9309 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 9:\nvalidation_loss = 4.4456 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 10:\nvalidation_loss = 4.8998 | CWA=0.964 SWA=0.974 HPA=0.969', '\\n', 'Epoch 11:\nvalidation_loss = 5.4250 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 12:\nvalidation_loss = 5.6015 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 13:\nvalidation_loss = 5.9957 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 14:\nvalidation_loss = 6.2467 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 15:\nvalidation_loss = 6.3514 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'TEST  |\nCWA=0.987 SWA=0.991 HPA=0.989', '\\n', 'Saved logs ->', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-12/working/experiment_data.npy', '\\n', 'Execution time:\n4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset sizes:', ' ', \"{'train': 500, 'dev': 150,\n'test': 200}\", '\\n', 'Shapes=8 Colors=8 Classes=2', '\\n', 'Epoch 1:\nvalidation_loss = 1.7093 | CWA=0.974 SWA=0.982 HPA=0.978', '\\n', 'Epoch 2:\nvalidation_loss = 0.6571 | CWA=0.907 SWA=0.928 HPA=0.917', '\\n', 'Epoch 3:\nvalidation_loss = 0.9917 | CWA=0.963 SWA=0.973 HPA=0.968', '\\n', 'Epoch 4:\nvalidation_loss = 0.9605 | CWA=0.849 SWA=0.871 HPA=0.860', '\\n', 'Epoch 5:\nvalidation_loss = 2.2924 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 6:\nvalidation_loss = 2.1680 | CWA=0.939 SWA=0.955 HPA=0.947', '\\n', 'Epoch 7:\nvalidation_loss = 3.3968 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 8:\nvalidation_loss = 3.9309 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 9:\nvalidation_loss = 4.4456 | CWA=0.958 SWA=0.971 HPA=0.964', '\\n', 'Epoch 10:\nvalidation_loss = 4.8998 | CWA=0.964 SWA=0.974 HPA=0.969', '\\n', 'Epoch 11:\nvalidation_loss = 5.4250 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 12:\nvalidation_loss = 5.6015 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 13:\nvalidation_loss = 5.9957 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 14:\nvalidation_loss = 6.2467 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'Epoch 15:\nvalidation_loss = 6.3514 | CWA=0.969 SWA=0.979 HPA=0.974', '\\n', 'TEST  |\nCWA=0.987 SWA=0.991 HPA=0.989', '\\n', 'Saved logs ->', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/experiment_data.npy', '\\n', 'Execution time:\n4 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "", "", "", "", "", "The execution failed due to a `NameError` caused by the undefined variable\n`shape2idx`. The variable `shape2idx` was intended to map shape tokens to\nindices, but it was not initialized properly in the code. The issue arises\nbecause the line `shape2idx` appears without any assignment or initialization.\nTo fix this, initialize `shape2idx` as an empty dictionary (`shape2idx = {}`) at\nthe beginning of the script, similar to how `color2idx` is initialized.", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, "NameError", null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, {"args": ["name 'shape2idx' is not defined"], "name": "shape2idx"}, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 85, "<module>", "shape2idx"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "final training loss", "lower_is_better": true, "description": "The final loss value achieved during training.", "data": [{"dataset_name": "SPR", "final_value": 0.4482, "best_value": 0.4482}]}, {"metric_name": "final validation loss", "lower_is_better": true, "description": "The final loss value achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.3505, "best_value": 0.3505}]}, {"metric_name": "final validation accuracy", "lower_is_better": false, "description": "The final accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8833, "best_value": 0.8833}]}, {"metric_name": "final validation color weighted accuracy", "lower_is_better": false, "description": "The final color-weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8854, "best_value": 0.8854}]}, {"metric_name": "final validation shape weighted accuracy", "lower_is_better": false, "description": "The final shape-weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8727, "best_value": 0.8727}]}, {"metric_name": "final validation complexity adjusted accuracy", "lower_is_better": false, "description": "The final complexity-adjusted accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8792, "best_value": 0.8792}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.0463, "best_value": 0.0463}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.132, "best_value": 0.132}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.97, "best_value": 0.97}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy, weighted by color categories, during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.9717, "best_value": 0.9717}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy, weighted by shape categories, during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.9785, "best_value": 0.9785}]}, {"metric_name": "validation harmonic poly accuracy", "lower_is_better": false, "description": "Measures the harmonic mean accuracy for poly categories during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.9751, "best_value": 0.9751}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model during training", "data": [{"dataset_name": "SPR", "final_value": 0.3934, "best_value": 0.3934}]}, {"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy of the model during training", "data": [{"dataset_name": "SPR", "final_value": 0.84, "best_value": 0.84}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model during validation", "data": [{"dataset_name": "SPR", "final_value": 0.4218, "best_value": 0.4218}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model during validation", "data": [{"dataset_name": "SPR", "final_value": 0.8417, "best_value": 0.8417}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model during validation", "data": [{"dataset_name": "SPR", "final_value": 0.8336, "best_value": 0.8336}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model during validation", "data": [{"dataset_name": "SPR", "final_value": 0.8538, "best_value": 0.8538}]}, {"metric_name": "validation harmonic poly accuracy", "lower_is_better": false, "description": "The harmonic poly accuracy of the model during validation", "data": [{"dataset_name": "SPR", "final_value": 0.8436, "best_value": 0.8436}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.3939, "best_value": 0.3939}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.4826, "best_value": 0.4826}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.815, "best_value": 0.815}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Measures the CWA metric during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.8217, "best_value": 0.8217}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Measures the SWA metric during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.8337, "best_value": 0.8337}]}, {"metric_name": "validation HPA", "lower_is_better": false, "description": "Measures the HPA metric during validation. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.8276, "best_value": 0.8276}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR", "final_value": 0.8067, "best_value": 0.8067}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, which measures the error between predicted and actual values.", "data": [{"dataset_name": "SPR", "final_value": 0.476033, "best_value": 0.476033}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, which measures the error between predicted and actual values.", "data": [{"dataset_name": "SPR", "final_value": 0.474007, "best_value": 0.474007}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model during validation, measuring the proportion of correctly predicted instances.", "data": [{"dataset_name": "SPR", "final_value": 0.81, "best_value": 0.81}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "Class-weighted accuracy during validation, which accounts for imbalances between classes.", "data": [{"dataset_name": "SPR", "final_value": 0.812918, "best_value": 0.812918}]}, {"metric_name": "validation swa", "lower_is_better": false, "description": "Sample-weighted accuracy during validation, which weighs each sample differently.", "data": [{"dataset_name": "SPR", "final_value": 0.819383, "best_value": 0.819383}]}, {"metric_name": "validation hpa", "lower_is_better": false, "description": "Harmonic mean of precision and accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.816138, "best_value": 0.816138}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model fits the training data.", "data": [{"dataset_name": "SPR", "final_value": 0.0141, "best_value": 0.0141}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, indicating how well the model generalizes.", "data": [{"dataset_name": "SPR", "final_value": 0.0844, "best_value": 0.0844}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset, indicating how well the model performs on unseen data.", "data": [{"dataset_name": "SPR", "final_value": 0.9833, "best_value": 0.9833}]}, {"metric_name": "validation colour-weighted accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset, weighted by colour classes.", "data": [{"dataset_name": "SPR", "final_value": 0.987, "best_value": 0.987}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset, weighted by shape classes.", "data": [{"dataset_name": "SPR", "final_value": 0.9928, "best_value": 0.9928}]}, {"metric_name": "validation harmonic poly accuracy", "lower_is_better": false, "description": "The harmonic mean accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9899, "best_value": 0.9899}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9867, "best_value": 0.9867}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.0288, "best_value": 0.0288}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.0357, "best_value": 0.0357}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.993, "best_value": 0.993}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The CWA (Custom Weighted Accuracy) during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.997, "best_value": 0.997}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The SWA (Specific Weighted Accuracy) during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.998, "best_value": 0.998}]}, {"metric_name": "validation HPA", "lower_is_better": false, "description": "The HPA (High Precision Accuracy) during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.997, "best_value": 0.997}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy on the test dataset.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during the training process.", "data": [{"dataset_name": "SPR", "final_value": 0.0035, "best_value": 0.0035}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during the validation process.", "data": [{"dataset_name": "SPR", "final_value": 0.0225, "best_value": 0.0225}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correctly Weighted Accuracy (CWA) during validation.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Simple Weighted Accuracy (SWA) during validation.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation HPA", "lower_is_better": false, "description": "The Highest Precision Accuracy (HPA) during validation.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.97, "best_value": 0.97}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.0047, "best_value": 0.0047}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.5623, "best_value": 0.5623}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The validation Correctly Weighted Accuracy (CWA).", "data": [{"dataset_name": "SPR", "final_value": 0.9867, "best_value": 0.9867}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The validation Sample Weighted Accuracy (SWA).", "data": [{"dataset_name": "SPR", "final_value": 0.9902, "best_value": 0.9902}]}, {"metric_name": "validation HPA", "lower_is_better": false, "description": "The validation Harmonic Precision Accuracy (HPA).", "data": [{"dataset_name": "SPR", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy on the test dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error in predictions during training.", "data": [{"dataset_name": "SPR", "final_value": 0.0088, "best_value": 0.0088}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error in predictions on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.6571, "best_value": 0.6571}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation Class-Wise Accuracy.", "data": [{"dataset_name": "SPR", "final_value": 0.9736, "best_value": 0.9736}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation Sample-Wise Accuracy.", "data": [{"dataset_name": "SPR", "final_value": 0.9816, "best_value": 0.9816}]}, {"metric_name": "validation HPA", "lower_is_better": false, "description": "Validation Harmonic-Mean Accuracy.", "data": [{"dataset_name": "SPR", "final_value": 0.9775, "best_value": 0.9775}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.985, "best_value": 0.985}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during the training phase.", "data": [{"dataset_name": "SPR", "final_value": 0.0088, "best_value": 0.0088}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during the validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.6571, "best_value": 0.6571}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correctly Weighted Accuracy during the validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.9736, "best_value": 0.9736}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Sample Weighted Accuracy during the validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.9816, "best_value": 0.9816}]}, {"metric_name": "validation HPA", "lower_is_better": false, "description": "The Harmonic Precision Accuracy during the validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.9775, "best_value": 0.9775}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy on the test dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.985, "best_value": 0.985}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_accuracy_vs_caa.png", "../../logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_cwa_swa_curves.png"], ["../../logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_acc_hpa.png", "../../logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_cwa_swa.png", "../../logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_label_counts.png"], ["../../logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_cwa_swa_curves.png", "../../logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_hpa_curve.png"], ["../../logs/0-run/experiment_results/experiment_1a129b8b9a9146518336e904b756dab6_proc_1445297/SPR_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_1a129b8b9a9146518336e904b756dab6_proc_1445297/SPR_val_metrics.png"], ["../../logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_train_loss.png", "../../logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_val_loss.png", "../../logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_val_acc.png", "../../logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_cwa_swa.png"], ["../../logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_test_distribution.png"], ["../../logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_acc_hpa.png", "../../logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_cwa_swa.png", "../../logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_test_distribution.png"], [], ["../../logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_weighted_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_test_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_weighted_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_test_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_weighted_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_test_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_weighted_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_test_label_distribution.png"], ["../../logs/0-run/experiment_results/seed_aggregation_05aea87565ea4895aba97c6f3fbaf36e/SPR_agg_loss.png", "../../logs/0-run/experiment_results/seed_aggregation_05aea87565ea4895aba97c6f3fbaf36e/SPR_agg_val_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_05aea87565ea4895aba97c6f3fbaf36e/SPR_agg_label_distribution.png"]], "plot_paths": [["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_accuracy_vs_caa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_cwa_swa_curves.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_acc_hpa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_cwa_swa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_label_counts.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_cwa_swa_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_hpa_curve.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1a129b8b9a9146518336e904b756dab6_proc_1445297/SPR_train_val_loss.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1a129b8b9a9146518336e904b756dab6_proc_1445297/SPR_val_metrics.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_train_loss.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_val_loss.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_val_acc.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_cwa_swa.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_train_val_loss.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_validation_metrics.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_test_distribution.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_acc_hpa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_cwa_swa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_test_distribution.png"], [], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_weighted_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_test_label_distribution.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_weighted_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_test_label_distribution.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_weighted_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_test_label_distribution.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_weighted_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_test_label_distribution.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_05aea87565ea4895aba97c6f3fbaf36e/SPR_agg_loss.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_05aea87565ea4895aba97c6f3fbaf36e/SPR_agg_val_metrics.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_05aea87565ea4895aba97c6f3fbaf36e/SPR_agg_label_distribution.png"]], "plot_analyses": [[{"analysis": "The training loss decreases consistently for all learning rates, with the fastest decrease observed for lr=3e-03, followed by lr=1e-03 and lr=3e-04. This indicates that a higher learning rate accelerates convergence during training. However, in validation loss, the lr=3e-03 curve exhibits significant fluctuations, suggesting potential overfitting or instability. The lr=1e-03 curve stabilizes better, while lr=3e-04 shows a steady decrease but at a slower rate, indicating underfitting or slow convergence.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_loss_curves.png"}, {"analysis": "The accuracy and complexity-adjusted accuracy remain constant across epochs for all learning rates. This suggests that the model's performance is not improving with training, potentially due to the limitations in the model's ability to generalize or suboptimal hyperparameter settings. Additionally, the choice of metrics might not be sensitive enough to reflect small performance changes.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_accuracy_vs_caa.png"}, {"analysis": "Both color-weighted accuracy and shape-weighted accuracy remain constant across epochs for all learning rates. This indicates that the model is not learning to leverage the structural information represented by these metrics. The lack of variation could also suggest that the dataset might not be challenging enough, or the model's architecture and hyperparameters are not well-suited for these metrics.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_cwa_swa_curves.png"}], [{"analysis": "The training loss decreases steadily over epochs, indicating that the model is learning effectively during training. However, there are spikes in the training loss, particularly around epochs 10 and 12, which could suggest instability or overfitting in certain phases of training. The validation loss shows a similar trend with fluctuations, suggesting that the model's generalization to unseen data is inconsistent. This behavior might indicate the need for regularization techniques or adjustments in learning rates.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_loss_curves.png"}, {"analysis": "Accuracy metrics remain constant throughout the training process, which is unusual. This could indicate that the model has reached its performance ceiling early on or that there is an issue with the evaluation metric implementation. Further investigation is required to ensure the metrics are correctly computed and aligned with the task objectives.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_acc_hpa.png"}, {"analysis": "Both Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) remain constant across epochs, similar to the overall accuracy metrics. This suggests that the model's performance does not improve over time on these specific metrics. This could be due to a lack of sufficient learning signal from the loss function or a potential bottleneck in the model architecture.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_cwa_swa.png"}, {"analysis": "The ground truth versus predicted label counts show a strong bias towards class 0, with very few instances of class 1 being predicted. This indicates a significant class imbalance problem in the predictions, which might be reflective of the dataset itself or a model bias. Techniques like class re-weighting or data augmentation could help mitigate this issue.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_82d3e7d431844d6eac73150a8e78f2f5_proc_1445295/SPR_label_counts.png"}], [{"analysis": "The training and validation loss decrease steadily across epochs, indicating that the model is learning effectively. However, the validation loss plateaus after a few epochs, suggesting that the model may not be improving further on unseen data. This could indicate the need for additional regularization or hyperparameter tuning to enhance generalization.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_loss_curves.png"}, {"analysis": "The training accuracy improves rapidly and stabilizes, while the validation accuracy remains constant after an initial increase. This suggests that the model achieves high performance early on but does not improve further, likely due to a lack of additional learning capacity or overfitting.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_accuracy_curves.png"}, {"analysis": "Both Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) remain constant and do not improve over epochs. This indicates that the model's ability to capture the relational structure of the sequences is not being enhanced with training. Further investigation into the model architecture, edge definitions, or feature representations may be necessary to improve these metrics.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_cwa_swa_curves.png"}, {"analysis": "The Harmonic Poly Accuracy (HPA) remains constant throughout training, similar to the CWA and SWA metrics. This suggests that the model is not effectively leveraging the graph structure to improve its understanding of the rules governing the sequences. Revisiting the graph construction process or incorporating additional relational features may help address this issue.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_f3f42e01c87f4dde94003e059a5674c0_proc_1445296/SPR_hpa_curve.png"}], [{"analysis": "The training loss decreases consistently over the epochs, indicating that the model is learning from the training data. However, the validation loss does not follow a similar trend. Instead, it decreases initially but then fluctuates significantly after a few epochs, suggesting potential overfitting or instability in the model's generalization to unseen data. This behavior warrants further investigation into the model's architecture, regularization techniques, and hyperparameter tuning.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1a129b8b9a9146518336e904b756dab6_proc_1445297/SPR_train_val_loss.png"}, {"analysis": "The validation metrics (Accuracy, CWA, SWA, and HPA) remain constant across all epochs. While this stability might suggest that the model has reached a plateau in terms of performance, it could also indicate an issue with the evaluation process, such as a bug in the metric computation or insufficient sensitivity of the metrics to capture performance changes. Further analysis of the metric definitions and the evaluation pipeline is recommended to ensure accuracy.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1a129b8b9a9146518336e904b756dab6_proc_1445297/SPR_val_metrics.png"}], [{"analysis": "This plot shows the training loss for the SPR task across six epochs for three different learning rates (0.0003, 0.001, and 0.003). The model with a learning rate of 0.003 achieves the fastest reduction in loss, indicating that it converges more quickly during training. The learning rate of 0.0003, while slower to converge initially, continues to decrease steadily, suggesting better long-term optimization. The learning rate of 0.001 balances between convergence speed and stability, showing consistent loss reduction.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_train_loss.png"}, {"analysis": "This plot illustrates the validation loss for the SPR task across six epochs for the same three learning rates. The learning rate of 0.003 shows an increase in validation loss after the second epoch, suggesting overfitting. The learning rate of 0.0003 exhibits a steady decrease in validation loss, indicating consistent generalization. The learning rate of 0.001 initially decreases but starts to increase slightly, which may indicate mild overfitting or insufficient regularization.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_val_loss.png"}, {"analysis": "This plot represents the validation accuracy across six epochs for the three learning rates. Surprisingly, the accuracy remains constant at approximately 0.81 for all learning rates. This suggests that the model's performance on the validation set is not sensitive to the learning rate within the tested range, or it may indicate a bottleneck in the model architecture or data representation.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_val_acc.png"}, {"analysis": "These plots compare Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) for the SPR task across six epochs for three learning rates. Both metrics remain constant at approximately 0.82 for all learning rates and epochs. This indicates that the model's performance on these metrics is not improving during training, suggesting potential issues with the model's ability to learn the task-specific rules or limitations in the dataset representations.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_615b99e7813b49348aa494088a998dfb_proc_1445298/SPR_cwa_swa.png"}], [{"analysis": "The loss curves indicate a significant overfitting issue. While the training loss steadily decreases over epochs, the validation loss begins to increase after the second epoch, diverging sharply. This suggests that the model is fitting well to the training data but fails to generalize to unseen validation data. Regularization techniques such as dropout, weight decay, or early stopping should be considered to mitigate overfitting.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_train_val_loss.png"}, {"analysis": "The metrics plots show constant values of 1.0 for all validation metrics (accuracy, CWA, SWA, and HPA) across epochs. This is highly unusual and suggests a potential issue with the evaluation metric implementation or data preprocessing. It is critical to verify the calculation of validation metrics and ensure that the validation dataset is correctly labeled and representative of the task.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_validation_metrics.png"}, {"analysis": "The bar chart comparing ground truth and predictions for the test set shows a strong bias toward class 0, with almost no predictions for class 1. This indicates a class imbalance issue in the model's predictions, which could stem from imbalanced training data or inappropriate loss function weighting. Addressing class imbalance in the training data or using a weighted loss function might help in improving the model's performance on underrepresented classes.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c190e7c5a0b54bd1bf293080440c307f_proc_1445296/SPR_test_distribution.png"}], [{"analysis": "The training loss decreases rapidly over the first few epochs, indicating that the model is learning effectively. However, there is a spike in training loss at epoch 5, which suggests some instability in the learning process, possibly due to an inappropriate learning rate or overfitting to certain patterns in the data. The validation loss decreases steadily and converges, which is a positive sign of generalization.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_loss_curves.png"}, {"analysis": "The validation accuracy remains almost constant across all epochs, with a very high value close to 1. This suggests that the model is achieving excellent performance on the validation set, potentially indicating overfitting to the training data or a lack of challenging examples in the validation set.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_acc_hpa.png"}, {"analysis": "Both Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) exhibit consistently high values close to 1 throughout the epochs. This indicates that the model performs exceptionally well on both metrics, but it also raises questions about whether the benchmark dataset is sufficiently challenging or diverse.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_cwa_swa.png"}, {"analysis": "The predictions closely match the ground truth, as seen in the almost identical bar heights for the two categories. This suggests that the model is highly accurate on the test set. However, the lack of variability in the results might imply that the task is not sufficiently complex or that the model has overfitted to the training data.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_fad89eacecbb4279acaf37c3d356c01c_proc_1445297/SPR_test_distribution.png"}], [], [{"analysis": "The training loss decreases steadily and approaches near-zero values, indicating that the model is learning effectively on the training data. The validation loss also decreases overall but exhibits some fluctuations, suggesting that the model is generalizing reasonably well but might be slightly sensitive to the validation set or experiencing minor overfitting. The consistent downward trend for both losses is a positive indicator of the model's performance.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_loss_curves.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) metrics both show high scores, with values stabilizing close to 0.99 after some initial fluctuations. This indicates that the model is performing exceptionally well in capturing both color and shape-based rules in the validation set. The Harmonic Poly Accuracy (HPA) metric also stabilizes at a high value, further confirming the model's strong performance across different weighted accuracy metrics.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_weighted_accuracy_curves.png"}, {"analysis": "The class label distribution for both ground truth and predictions shows a strong alignment, with almost identical counts for each class. This suggests that the model has learned to predict the class distribution accurately, with minimal bias or imbalance in its predictions. The small discrepancy in the minor class indicates a slight room for improvement in handling less frequent labels.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_8c3bdfc4c3064befa8eae50a48f257bd_proc_1445298/SPR_test_label_distribution.png"}], [{"analysis": "The training loss plot shows a steady decrease over epochs, indicating that the model is learning effectively on the training data. However, the validation loss increases significantly after the initial epochs, suggesting overfitting. This implies that while the model is performing well on the training data, it fails to generalize to unseen data. Regularization techniques or early stopping could be considered to address this issue.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_loss_curves.png"}, {"analysis": "The CWA and SWA validation accuracy metrics initially start high but show a sharp drop around epoch 6, followed by recovery and stabilization. This behavior might indicate some instability in the training process or the presence of noise in the data. The final stabilized values are high, but the drop in the middle epochs warrants further investigation. The Harmonic Poly Accuracy (HPA) plot mirrors this behavior, emphasizing the need to explore possible causes like inappropriate learning rate or overfitting.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_weighted_accuracy_curves.png"}, {"analysis": "The bar plot comparing ground truth and predictions shows a close alignment for class '0', indicating that the model performs well for this class. However, the extremely small count for class '1' suggests a class imbalance issue. The model might not be learning enough about the minority class, leading to poor generalization for it. Addressing class imbalance through techniques like oversampling, undersampling, or class-weighted loss functions could improve performance.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/SPR_test_label_distribution.png"}], [{"analysis": "The training loss decreases steadily over epochs, indicating that the model is learning effectively from the training data. However, the validation loss increases significantly after an initial decrease, suggesting overfitting. This indicates that the model generalizes poorly to unseen data, and regularization techniques or early stopping might need to be employed.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_loss_curves.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) show high initial variability but stabilize around 96% and 98%, respectively, after epoch 8. This suggests that the model performs well on the validation set in terms of these metrics. However, the harmonic poly accuracy (HPA) plot reveals instability in early epochs but eventually stabilizes at a high value, indicating improved consistency in performance.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_weighted_accuracy_curves.png"}, {"analysis": "The test set label frequencies show that the model's predictions closely match the ground truth for the majority class (label 0). However, the minority class (label 1) is predicted very infrequently, indicating a class imbalance issue. The model may need techniques like resampling or adjusted loss functions to handle this imbalance effectively.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/SPR_test_label_distribution.png"}], [{"analysis": "The training loss on the left plot decreases steadily and approaches zero by the 10th epoch, indicating effective learning from the training data. However, the validation loss on the right plot increases consistently after an initial dip, suggesting overfitting. This implies that the model is memorizing the training data rather than generalizing to unseen data. Regularization techniques, such as dropout or weight decay, or reducing model complexity might help mitigate this issue.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_loss_curves.png"}, {"analysis": "The left plot shows that both Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) reach high values, stabilizing around 0.96 and 0.98, respectively. The right plot indicates similar trends in Harmonic Poly Accuracy (HPA), which also stabilizes at a high value. However, the fluctuations in the earlier epochs suggest instability, possibly due to inappropriate learning rates or insufficient training data. Despite these fluctuations, the final performance metrics are strong and suggest the model captures the underlying rules effectively.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_weighted_accuracy_curves.png"}, {"analysis": "The bar chart illustrates that the predicted class label frequencies closely match the ground truth frequencies for the majority class (label 0), but there is a significant disparity for the minority class (label 1). This indicates a class imbalance issue, where the model struggles to predict the minority class correctly. Techniques like oversampling the minority class, using class weights, or applying focal loss could improve the model's performance on the minority class.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/SPR_test_label_distribution.png"}], []], "vlm_feedback_summary": ["The plots highlight issues with model generalization and metric sensitivity.\nWhile the training loss decreases for higher learning rates, validation loss\ninstability and flat accuracy metrics suggest the need for further tuning of the\nmodel and hyperparameters. The metrics used may also require reconsideration to\nbetter reflect performance improvements.", "The plots reveal a mix of learning progress and potential issues. While the\nmodel shows a decrease in training loss, the instability and flat accuracy\nmetrics highlight areas for improvement. The imbalance in predicted labels is a\ncritical concern that needs to be addressed for better performance.", "The plots indicate that while the model is learning effectively in terms of loss\nreduction and initial accuracy improvements, the weighted accuracy metrics (CWA,\nSWA, and HPA) remain stagnant, suggesting limited success in capturing the\nrelational structure of the sequences. Further experimentation with the model\narchitecture and features is recommended.", "The training process shows learning progression in terms of training loss, but\nvalidation loss fluctuations suggest potential overfitting or instability.\nValidation metrics remain constant, raising concerns about the evaluation\nprocess or metric sensitivity.", "The plots indicate that while training loss decreases for all learning rates,\nvalidation loss and accuracy metrics do not improve significantly. This suggests\npotential overfitting for higher learning rates and limited model\ngeneralization. Despite distinct learning rate behaviors during training,\nvalidation accuracy, and task-specific metrics (CWA and SWA) remain constant,\npointing to possible bottlenecks in the model or data.", "The provided plots reveal significant issues in the experimental results,\nincluding overfitting, potential metric calculation errors, and class imbalance.\nThese problems need to be addressed to ensure reliable and meaningful evaluation\nof the proposed GNN model for the SPR task.", "The plots indicate strong model performance on all metrics, but there are\nconcerns about potential overfitting and the simplicity of the benchmark\ndataset. The results suggest that the model is capable of learning the task\neffectively, but further analysis is needed to ensure robustness and\ngeneralization.", "[]", "The provided plots indicate that the GNN-based model is performing well on the\nSPR task. Training and validation losses decrease consistently, with minor\nfluctuations in validation loss. Weighted accuracy metrics (CWA, SWA, HPA) are\nhigh and stable, demonstrating the model's effectiveness in capturing the\nrelational structure of the sequences. Class label predictions closely align\nwith ground truth distributions, showing balanced and accurate predictions.\nOverall, the results suggest the model surpasses SOTA benchmarks and validates\nthe hypothesis.", "The provided plots highlight significant issues with overfitting and potential\nclass imbalance. While the training loss decreases steadily, the increasing\nvalidation loss suggests poor generalization. Validation accuracy metrics show\ninstability, and the class imbalance in the dataset is evident from the label\nfrequency plot. Regularization, early stopping, and addressing class imbalance\nare recommended.", "The plots provide insights into the model's performance, highlighting\noverfitting during training, high validation accuracy with some initial\ninstability, and class imbalance in predictions.", "The provided plots reveal key insights into the experiment. The training loss\ndecreases effectively, but validation loss trends upward, indicating\noverfitting. Accuracy metrics (CWA, SWA, HPA) stabilize at high values but show\nearly instability. Class imbalance is evident, with poor prediction for the\nminority class. Improvements in regularization, learning stability, and\naddressing class imbalance are recommended.", "[]"], "exec_time": [3.3532469272613525, 6.409804105758667, 3.4573965072631836, 4.08199667930603, 4.7894606590271, 6.737875461578369, 4.0474934577941895, 2.7557361125946045, 5.4065117835998535, 5.804126739501953, 4.319311618804932, 4.5017969608306885, null], "exec_time_feedback": ["", "", "", "", "Implementation works but runs too quickly (0.11 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["\"\""], ["[]"], ["[]"], ["\"\""], ["\"\""], ["[]"], ["['SPR']"], [], ["['SPR']"], ["[]"], ["['SPR']"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data[\"learning_rate\"][\"SPR\"]\n    lrs = sorted(runs.keys(), key=lambda x: float(x.replace(\"e-\", \"e-0\")))\n    epochs = len(next(iter(runs.values()))[\"losses\"][\"train\"])\n\n    # Helper to pull metric list\n    def metric_list(run, split, field):\n        if field == \"losses\":\n            return run[\"losses\"][split]\n        else:  # metrics\n            return [m[field] for m in run[\"metrics\"][split]]\n\n    # ----------- Figure 1: Train/Val Loss -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            ax[0].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"train\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n            ax[1].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"val\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Loss Curves Across Learning Rates\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 2: Accuracy & CAA -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            acc = [m[\"acc\"] for m in run[\"metrics\"][\"val\"]]\n            caa = [m[\"caa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), acc, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), caa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Accuracy\")\n        ax[1].set_title(\"Right: Complexity Adjusted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Basic vs Complexity-Aware Accuracy\")\n        fname = os.path.join(working_dir, \"SPR_accuracy_vs_caa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy/CAA plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 3: Color vs Shape Weighted Acc -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            cwa = [m[\"cwa\"] for m in run[\"metrics\"][\"val\"]]\n            swa = [m[\"swa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), cwa, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), swa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Color-Weighted Accuracy\")\n        ax[1].set_title(\"Right: Shape-Weighted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Color vs Shape Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------- set up -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    for dset_name, logs in experiment_data.items():\n        # ---------- helpers ----------\n        tr_loss = logs[\"losses\"].get(\"train\", [])\n        val_loss = logs[\"losses\"].get(\"val\", [])\n        val_metrics = logs[\"metrics\"].get(\"val\", [])\n        epochs = list(range(1, len(val_loss) + 1))\n\n        def get_metric(field):\n            return [m.get(field) for m in val_metrics] if val_metrics else []\n\n        acc = get_metric(\"acc\")\n        hpa = get_metric(\"HPA\")\n        cwa = get_metric(\"CWA\")\n        swa = get_metric(\"SWA\")\n        preds = logs.get(\"predictions\", [])\n        gts = logs.get(\"ground_truth\", [])\n\n        # ---------- Fig 1: Loss curves ----------\n        try:\n            fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n            if tr_loss:\n                ax[0].plot(epochs, tr_loss, label=\"train\")\n            if val_loss:\n                ax[1].plot(epochs, val_loss, label=\"val\", color=\"tab:orange\")\n            ax[0].set_title(\"Left: Train Loss\")\n            ax[1].set_title(\"Right: Validation Loss\")\n            for a in ax:\n                a.set_xlabel(\"Epoch\")\n                a.set_ylabel(\"Loss\")\n                a.legend()\n            fig.suptitle(f\"{dset_name} Loss Curves\")\n            fname = os.path.join(working_dir, f\"{dset_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating loss plot for {dset_name}: {e}\")\n            plt.close()\n\n        # ---------- Fig 2: Acc & HPA ----------\n        try:\n            fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n            if acc:\n                ax[0].plot(epochs, acc, label=\"Accuracy\")\n            if hpa:\n                ax[1].plot(epochs, hpa, label=\"HPA\", color=\"tab:green\")\n            ax[0].set_title(\"Left: Accuracy\")\n            ax[1].set_title(\"Right: Harmonic Poly Accuracy\")\n            for a in ax:\n                a.set_xlabel(\"Epoch\")\n                a.set_ylabel(\"Score\")\n                a.legend()\n            fig.suptitle(f\"{dset_name} Accuracy Metrics\")\n            fname = os.path.join(working_dir, f\"{dset_name}_acc_hpa.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating acc/HPA plot for {dset_name}: {e}\")\n            plt.close()\n\n        # ---------- Fig 3: CWA & SWA ----------\n        try:\n            fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n            if cwa:\n                ax[0].plot(epochs, cwa, label=\"CWA\")\n            if swa:\n                ax[1].plot(epochs, swa, label=\"SWA\", color=\"tab:red\")\n            ax[0].set_title(\"Left: Color-Weighted Acc\")\n            ax[1].set_title(\"Right: Shape-Weighted Acc\")\n            for a in ax:\n                a.set_xlabel(\"Epoch\")\n                a.set_ylabel(\"Score\")\n                a.legend()\n            fig.suptitle(f\"{dset_name} Weighted Accuracies\")\n            fname = os.path.join(working_dir, f\"{dset_name}_cwa_swa.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating CWA/SWA plot for {dset_name}: {e}\")\n            plt.close()\n\n        # ---------- Fig 4: Prediction distribution ----------\n        try:\n            if preds and gts:\n                labels = sorted(set(gts + preds))\n                gt_counts = [gts.count(l) for l in labels]\n                pr_counts = [preds.count(l) for l in labels]\n\n                x = np.arange(len(labels))\n                width = 0.35\n                plt.figure(figsize=(6, 4), dpi=120)\n                plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n                plt.bar(x + width / 2, pr_counts, width, label=\"Predicted\")\n                plt.xticks(x, labels)\n                plt.title(f\"{dset_name}: GT vs Predicted Label Counts\")\n                plt.xlabel(\"Class\")\n                plt.ylabel(\"Count\")\n                plt.legend()\n                fname = os.path.join(working_dir, f\"{dset_name}_label_counts.png\")\n                plt.savefig(fname)\n                plt.close()\n                print(f\"Saved {fname}\")\n        except Exception as e:\n            print(f\"Error creating label count plot for {dset_name}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- set up ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    run = experiment_data[\"SPR\"]\n    epochs = run[\"epochs\"]\n    x = range(1, len(epochs) + 1)\n\n    # helpers\n    t_loss = run[\"losses\"][\"train\"]\n    v_loss = run[\"losses\"][\"val\"]\n    t_acc = [m[\"acc\"] for m in run[\"metrics\"][\"train\"]]\n    v_acc = [m[\"acc\"] for m in run[\"metrics\"][\"val\"]]\n    cwa = [m[\"cwa\"] for m in run[\"metrics\"][\"val\"]]\n    swa = [m[\"swa\"] for m in run[\"metrics\"][\"val\"]]\n    hpa = [m[\"hpa\"] for m in run[\"metrics\"][\"val\"]]\n\n    # -------- Figure 1: Loss curves --------\n    try:\n        plt.figure(figsize=(8, 4), dpi=120)\n        plt.plot(x, t_loss, label=\"Train\")\n        plt.plot(x, v_loss, label=\"Validation\")\n        plt.title(\"SPR \u2013 Left: Train Loss, Right: Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------- Figure 2: Accuracy curves --------\n    try:\n        plt.figure(figsize=(8, 4), dpi=120)\n        plt.plot(x, t_acc, label=\"Train\")\n        plt.plot(x, v_acc, label=\"Validation\")\n        plt.title(\"SPR \u2013 Left: Train Acc, Right: Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # -------- Figure 3: CWA & SWA --------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(x, cwa, label=\"CWA\", color=\"tab:blue\")\n        ax[1].plot(x, swa, label=\"SWA\", color=\"tab:orange\")\n        ax[0].set_title(\"Left: Color-Weighted Acc\")\n        ax[1].set_title(\"Right: Shape-Weighted Acc\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n\n    # -------- Figure 4: Harmonic Poly Accuracy --------\n    try:\n        plt.figure(figsize=(6, 4), dpi=120)\n        plt.plot(x, hpa, label=\"HPA\", color=\"tab:green\")\n        plt.title(\"SPR \u2013 Harmonic Poly Accuracy (HPA)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HPA Score\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_hpa_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating HPA plot: {e}\")\n        plt.close()\n\n    # -------- print summary metrics --------\n    best_idx = int(np.argmax(v_acc))\n    print(\n        f\"Best epoch @ {best_idx+1}: ValAcc={v_acc[best_idx]:.3f}, \"\n        f\"CWA={cwa[best_idx]:.3f}, SWA={swa[best_idx]:.3f}, HPA={hpa[best_idx]:.3f}\"\n    )\nelse:\n    print(\"No SPR data found.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------- prepare -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    run = experiment_data[\"SPR\"]  # single run\n    epochs = len(run[\"losses\"][\"train\"])\n\n    # helper to fetch metric list safely\n    def get_metric(lst, key):\n        return [m.get(key, np.nan) for m in lst] if lst else [np.nan] * epochs\n\n    # ============== Figure 1: Loss curves ==============\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(9, 4), dpi=120)\n        ax[0].plot(range(1, epochs + 1), run[\"losses\"][\"train\"], label=\"Train\")\n        ax[1].plot(\n            range(1, epochs + 1), run[\"losses\"][\"val\"], label=\"Val\", color=\"orange\"\n        )\n        ax[0].set_title(\"Train Loss\")\n        ax[1].set_title(\"Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2014 Left: Train Loss, Right: Validation Loss\")\n        fname = os.path.join(working_dir, \"SPR_train_val_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ============== Figure 2: Validation metrics ==============\n    try:\n        acc = get_metric(run[\"metrics\"][\"val\"], \"acc\")\n        cwa = get_metric(run[\"metrics\"][\"val\"], \"cwa\")\n        swa = get_metric(run[\"metrics\"][\"val\"], \"swa\")\n        hpa = get_metric(run[\"metrics\"][\"val\"], \"hpa\")\n\n        fig, ax = plt.subplots(2, 2, figsize=(10, 6), dpi=120)\n        titles = [\n            \"Accuracy\",\n            \"Color-Weighted Accuracy (CWA)\",\n            \"Shape-Weighted Accuracy (SWA)\",\n            \"Harmonic (HPA)\",\n        ]\n        data = [acc, cwa, swa, hpa]\n        for i, a in enumerate(ax.flatten()):\n            a.plot(range(1, epochs + 1), data[i])\n            a.set_title(titles[i])\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.set_ylim(0, 1)\n        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fig.suptitle(\"SPR Dataset \u2014 Validation Metrics per Epoch\")\n        fname = os.path.join(working_dir, \"SPR_val_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating metrics plot: {e}\")\n        plt.close()\nelse:\n    print(\"No SPR data found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None and \"SPR\" in exp:\n    lrs = [3e-4, 1e-3, 3e-3]  # learning rates used during training\n    epochs = 6  # epochs per LR\n    tr_loss = exp[\"SPR\"][\"losses\"][\"train\"]\n    val_loss = exp[\"SPR\"][\"losses\"][\"val\"]\n    val_metrics = exp[\"SPR\"][\"metrics\"][\"val\"]\n\n    # helper: slice the flat list into per-LR chunks\n    def chunk(lst):\n        return [lst[i * epochs : (i + 1) * epochs] for i in range(len(lrs))]\n\n    tr_chunks, val_chunks = chunk(tr_loss), chunk(val_loss)\n    acc_chunks = chunk([m[\"acc\"] for m in val_metrics])\n    cwa_chunks = chunk([m[\"cwa\"] for m in val_metrics])\n    swa_chunks = chunk([m[\"swa\"] for m in val_metrics])\n    hpa_chunks = chunk([m[\"hpa\"] for m in val_metrics])\n\n    # ---- Figure 1: Train Loss ----\n    try:\n        plt.figure(figsize=(5, 4), dpi=120)\n        for lr, tl in zip(lrs, tr_chunks):\n            plt.plot(range(1, epochs + 1), tl, label=f\"lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR Train Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_train_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating train-loss plot: {e}\")\n        plt.close()\n\n    # ---- Figure 2: Validation Loss ----\n    try:\n        plt.figure(figsize=(5, 4), dpi=120)\n        for lr, vl in zip(lrs, val_chunks):\n            plt.plot(range(1, epochs + 1), vl, label=f\"lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_val_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating val-loss plot: {e}\")\n        plt.close()\n\n    # ---- Figure 3: Validation Accuracy ----\n    try:\n        plt.figure(figsize=(5, 4), dpi=120)\n        for lr, ac in zip(lrs, acc_chunks):\n            plt.plot(range(1, epochs + 1), ac, label=f\"lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR Validation Accuracy\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_val_acc.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating val-acc plot: {e}\")\n        plt.close()\n\n    # ---- Figure 4: Color vs Shape Weighted Accuracy ----\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr, cw, sw in zip(lrs, cwa_chunks, swa_chunks):\n            ax[0].plot(range(1, epochs + 1), cw, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), sw, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Color-Weighted Acc\")\n        ax[1].set_title(\"Right: Shape-Weighted Acc\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Color vs Shape Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n\n    # ---- Print final HPA ----\n    print(\"\\nFinal Harmonic-Poly Accuracy per LR\")\n    for lr, hp in zip(lrs, [h[-1] for h in hpa_chunks]):\n        print(f\"  lr={lr:.0e}: HPA={hp:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    spr = experiment_data[\"SPR\"]\n    ep = spr.get(\"epochs\", list(range(1, len(spr[\"losses\"][\"train\"]) + 1)))\n\n    # ---- helper lambdas ----\n    def arr(path, default=[]):\n        cur = spr\n        for k in path.split(\"/\"):\n            cur = cur.get(k, {})\n        return cur if isinstance(cur, (list, np.ndarray)) else default\n\n    # --------------- Figure 1: Loss curves ---------------\n    try:\n        plt.figure(figsize=(6, 4), dpi=120)\n        plt.plot(ep, arr(\"losses/train\"), label=\"Train\")\n        plt.plot(ep, arr(\"losses/val\"), label=\"Validation\")\n        plt.title(\"SPR Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_train_val_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # --------------- Figure 2: Validation metrics ---------------\n    try:\n        metrics = spr[\"metrics\"][\"val\"]\n        keys = [\"acc\", \"CWA\", \"SWA\", \"HPA\"]\n        figs, ax = plt.subplots(2, 2, figsize=(8, 6), dpi=120)\n        ax = ax.flatten()\n        for i, k in enumerate(keys):\n            ax[i].plot(ep, [m[k] for m in metrics], label=k)\n            ax[i].set_title(k)\n            ax[i].set_xlabel(\"Epoch\")\n            ax[i].set_ylabel(\"Score\")\n            ax[i].set_ylim(0, 1)\n        plt.suptitle(\"SPR Validation Metrics Over Epochs\")\n        fname = os.path.join(working_dir, \"SPR_validation_metrics.png\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating metrics plot: {e}\")\n        plt.close()\n\n    # --------------- Figure 3: Test prediction distribution ---------------\n    try:\n        preds = np.array(spr.get(\"predictions\", []))\n        gts = np.array(spr.get(\"ground_truth\", []))\n        if preds.size and gts.size:\n            classes = sorted(set(gts) | set(preds))\n            width = 0.35\n            x = np.arange(len(classes))\n            plt.figure(figsize=(6, 4), dpi=120)\n            plt.bar(\n                x - width / 2,\n                [np.sum(gts == c) for c in classes],\n                width=width,\n                label=\"Ground Truth\",\n            )\n            plt.bar(\n                x + width / 2,\n                [np.sum(preds == c) for c in classes],\n                width=width,\n                label=\"Predictions\",\n            )\n            plt.xticks(x, classes)\n            plt.xlabel(\"Class\")\n            plt.ylabel(\"Count\")\n            plt.title(\"SPR Test Set: Ground Truth vs Predictions\")\n            plt.legend()\n            fname = os.path.join(working_dir, \"SPR_test_distribution.png\")\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating distribution plot: {e}\")\n        plt.close()\nelse:\n    print(\"No SPR data available to plot.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    spr = experiment_data[\"SPR\"]\n    tr_loss = spr[\"losses\"][\"train\"]\n    val_loss = spr[\"losses\"][\"val\"]\n    metrics_val = spr[\"metrics\"][\"val\"]\n    epochs = np.arange(1, len(tr_loss) + 1)\n\n    # helpers\n    def mlist(field):\n        return [m[field] for m in metrics_val]\n\n    # -------- Plot 1: Train vs Val loss -------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR \u2013 Train vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------- Plot 2: Accuracy vs HPA -------------\n    try:\n        plt.figure()\n        plt.plot(epochs, mlist(\"acc\"), label=\"Accuracy\")\n        plt.plot(epochs, mlist(\"HPA\"), label=\"Harmonic Poly Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR \u2013 Validation Accuracy vs HPA\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_acc_hpa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating acc/HPA plot: {e}\")\n        plt.close()\n\n    # -------- Plot 3: CWA vs SWA -------------\n    try:\n        plt.figure()\n        plt.plot(epochs, mlist(\"CWA\"), label=\"CWA\")\n        plt.plot(epochs, mlist(\"SWA\"), label=\"SWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR \u2013 Color vs Shape Weighted Acc\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_cwa_swa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n\n    # -------- Plot 4: Test label distribution -------------\n    try:\n        preds = np.array(spr[\"predictions\"])\n        gts = np.array(spr[\"ground_truth\"])\n        classes = sorted(set(gts.tolist() + preds.tolist()))\n        width = 0.35\n        plt.figure()\n        plt.bar(\n            np.arange(len(classes)) - width / 2,\n            [np.sum(gts == c) for c in classes],\n            width,\n            label=\"Ground Truth\",\n        )\n        plt.bar(\n            np.arange(len(classes)) + width / 2,\n            [np.sum(preds == c) for c in classes],\n            width,\n            label=\"Predictions\",\n        )\n        plt.xticks(classes)\n        plt.xlabel(\"Class label\")\n        plt.ylabel(\"Count\")\n        plt.title(\"SPR \u2013 Test Set: Ground Truth vs Predictions\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_test_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating distribution plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    run = experiment_data[\"SPR\"]\n    tr_loss = run[\"losses\"][\"train\"]\n    val_loss = run[\"losses\"][\"val\"]\n    epochs = run[\"epochs\"]\n    # unpack validation metrics\n    cwa = [m[\"CWA\"] for m in run[\"metrics\"][\"val\"]]\n    swa = [m[\"SWA\"] for m in run[\"metrics\"][\"val\"]]\n    hpa = [m[\"HPA\"] for m in run[\"metrics\"][\"val\"]]\n    # test label dists\n    preds = np.array(run[\"predictions\"])\n    gts = np.array(run[\"ground_truth\"])\n\n    # -------- Fig 1: loss curves ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, tr_loss, label=\"Train\")\n        ax[1].plot(epochs, val_loss, label=\"Validation\", color=\"orange\")\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Cross-Entropy Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Training vs Validation Loss\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # -------- Fig 2: weighted accuracies ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, cwa, label=\"CWA\")\n        ax[0].plot(epochs, swa, label=\"SWA\")\n        ax[1].plot(epochs, hpa, label=\"HPA\", color=\"green\")\n        ax[0].set_title(\"Left: CWA & SWA\")\n        ax[1].set_title(\"Right: Harmonic Poly Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Validation Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_weighted_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating weighted accuracy curves: {e}\")\n        plt.close()\n\n    # -------- Fig 3: test label distribution ----------\n    try:\n        fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=120)\n        labels = sorted(list(set(gts)))\n        gt_counts = [(gts == l).sum() for l in labels]\n        pred_counts = [(preds == l).sum() for l in labels]\n        width = 0.35\n        x = np.arange(len(labels))\n        ax.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        ax.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(labels)\n        ax.set_xlabel(\"Class Label\")\n        ax.set_ylabel(\"Count\")\n        ax.set_title(\n            \"Test Set Label Frequencies \u2013 SPR\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        ax.legend()\n        fname = os.path.join(working_dir, \"SPR_test_label_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    run = experiment_data[\"SPR\"]\n    tr_loss = run[\"losses\"][\"train\"]\n    val_loss = run[\"losses\"][\"val\"]\n    epochs = run[\"epochs\"]\n    # unpack validation metrics\n    cwa = [m[\"CWA\"] for m in run[\"metrics\"][\"val\"]]\n    swa = [m[\"SWA\"] for m in run[\"metrics\"][\"val\"]]\n    hpa = [m[\"HPA\"] for m in run[\"metrics\"][\"val\"]]\n    # test label dists\n    preds = np.array(run[\"predictions\"])\n    gts = np.array(run[\"ground_truth\"])\n\n    # -------- Fig 1: loss curves ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, tr_loss, label=\"Train\")\n        ax[1].plot(epochs, val_loss, label=\"Validation\", color=\"orange\")\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Cross-Entropy Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Training vs Validation Loss\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # -------- Fig 2: weighted accuracies ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, cwa, label=\"CWA\")\n        ax[0].plot(epochs, swa, label=\"SWA\")\n        ax[1].plot(epochs, hpa, label=\"HPA\", color=\"green\")\n        ax[0].set_title(\"Left: CWA & SWA\")\n        ax[1].set_title(\"Right: Harmonic Poly Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Validation Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_weighted_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating weighted accuracy curves: {e}\")\n        plt.close()\n\n    # -------- Fig 3: test label distribution ----------\n    try:\n        fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=120)\n        labels = sorted(list(set(gts)))\n        gt_counts = [(gts == l).sum() for l in labels]\n        pred_counts = [(preds == l).sum() for l in labels]\n        width = 0.35\n        x = np.arange(len(labels))\n        ax.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        ax.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(labels)\n        ax.set_xlabel(\"Class Label\")\n        ax.set_ylabel(\"Count\")\n        ax.set_title(\n            \"Test Set Label Frequencies \u2013 SPR\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        ax.legend()\n        fname = os.path.join(working_dir, \"SPR_test_label_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    run = experiment_data[\"SPR\"]\n    tr_loss = run[\"losses\"][\"train\"]\n    val_loss = run[\"losses\"][\"val\"]\n    epochs = run[\"epochs\"]\n    # unpack validation metrics\n    cwa = [m[\"CWA\"] for m in run[\"metrics\"][\"val\"]]\n    swa = [m[\"SWA\"] for m in run[\"metrics\"][\"val\"]]\n    hpa = [m[\"HPA\"] for m in run[\"metrics\"][\"val\"]]\n    # test label dists\n    preds = np.array(run[\"predictions\"])\n    gts = np.array(run[\"ground_truth\"])\n\n    # -------- Fig 1: loss curves ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, tr_loss, label=\"Train\")\n        ax[1].plot(epochs, val_loss, label=\"Validation\", color=\"orange\")\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Cross-Entropy Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Training vs Validation Loss\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # -------- Fig 2: weighted accuracies ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, cwa, label=\"CWA\")\n        ax[0].plot(epochs, swa, label=\"SWA\")\n        ax[1].plot(epochs, hpa, label=\"HPA\", color=\"green\")\n        ax[0].set_title(\"Left: CWA & SWA\")\n        ax[1].set_title(\"Right: Harmonic Poly Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Validation Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_weighted_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating weighted accuracy curves: {e}\")\n        plt.close()\n\n    # -------- Fig 3: test label distribution ----------\n    try:\n        fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=120)\n        labels = sorted(list(set(gts)))\n        gt_counts = [(gts == l).sum() for l in labels]\n        pred_counts = [(preds == l).sum() for l in labels]\n        width = 0.35\n        x = np.arange(len(labels))\n        ax.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        ax.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(labels)\n        ax.set_xlabel(\"Class Label\")\n        ax.set_ylabel(\"Count\")\n        ax.set_title(\n            \"Test Set Label Frequencies \u2013 SPR\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        ax.legend()\n        fname = os.path.join(working_dir, \"SPR_test_label_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------- load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR\" in experiment_data:\n    run = experiment_data[\"SPR\"]\n    tr_loss = run[\"losses\"][\"train\"]\n    val_loss = run[\"losses\"][\"val\"]\n    epochs = run[\"epochs\"]\n    # unpack validation metrics\n    cwa = [m[\"CWA\"] for m in run[\"metrics\"][\"val\"]]\n    swa = [m[\"SWA\"] for m in run[\"metrics\"][\"val\"]]\n    hpa = [m[\"HPA\"] for m in run[\"metrics\"][\"val\"]]\n    # test label dists\n    preds = np.array(run[\"predictions\"])\n    gts = np.array(run[\"ground_truth\"])\n\n    # -------- Fig 1: loss curves ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, tr_loss, label=\"Train\")\n        ax[1].plot(epochs, val_loss, label=\"Validation\", color=\"orange\")\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Cross-Entropy Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Training vs Validation Loss\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # -------- Fig 2: weighted accuracies ----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        ax[0].plot(epochs, cwa, label=\"CWA\")\n        ax[0].plot(epochs, swa, label=\"SWA\")\n        ax[1].plot(epochs, hpa, label=\"HPA\", color=\"green\")\n        ax[0].set_title(\"Left: CWA & SWA\")\n        ax[1].set_title(\"Right: Harmonic Poly Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Dataset \u2013 Validation Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_weighted_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating weighted accuracy curves: {e}\")\n        plt.close()\n\n    # -------- Fig 3: test label distribution ----------\n    try:\n        fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=120)\n        labels = sorted(list(set(gts)))\n        gt_counts = [(gts == l).sum() for l in labels]\n        pred_counts = [(preds == l).sum() for l in labels]\n        width = 0.35\n        x = np.arange(len(labels))\n        ax.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        ax.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(labels)\n        ax.set_xlabel(\"Class Label\")\n        ax.set_ylabel(\"Count\")\n        ax.set_title(\n            \"Test Set Label Frequencies \u2013 SPR\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        ax.legend()\n        fname = os.path.join(working_dir, \"SPR_test_label_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport math\n\n# ------------------------------------------------------------------\n# directory / data loading\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data_path_list = [\n    \"experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_3d72bce5bcc043758ff7ed4905aa4f0c_proc_1445295/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1dc2bcaf11bf440e86d9dd3e95028c99_proc_1445296/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a0d62a1c0b394cd59e9e875ad0c9dc74_proc_1445297/experiment_data.npy\",\n]\n\nall_runs_by_dataset = {}\n\n# ------------------ Load every file ------------------\nfor p in experiment_data_path_list:\n    try:\n        abs_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        edict = np.load(abs_path, allow_pickle=True).item()\n        for dset_name, run in edict.items():\n            all_runs_by_dataset.setdefault(dset_name, []).append(run)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\n\n# ------------------------------------------------------------------\n# Utility to make mean \u00b1 s.e.m.  Truncates to min length.\ndef stack_and_trim(list_of_lists):\n    if not list_of_lists:\n        return None\n    min_len = min(len(x) for x in list_of_lists)\n    arr = np.stack([np.asarray(x[:min_len]) for x in list_of_lists])\n    return arr\n\n\n# ------------------------------------------------------------------\n# Iterate over datasets and create aggregated plots\nfor dset_name, runs in all_runs_by_dataset.items():\n    # ----------------------- collect series -----------------------\n    train_losses = stack_and_trim([r[\"losses\"][\"train\"] for r in runs if \"losses\" in r])\n    val_losses = stack_and_trim([r[\"losses\"][\"val\"] for r in runs if \"losses\" in r])\n    epochs_vec = np.arange(train_losses.shape[1]) if train_losses is not None else None\n\n    cwa = stack_and_trim(\n        [[m[\"CWA\"] for m in r[\"metrics\"][\"val\"]] for r in runs if \"metrics\" in r]\n    )\n    swa = stack_and_trim(\n        [[m[\"SWA\"] for m in r[\"metrics\"][\"val\"]] for r in runs if \"metrics\" in r]\n    )\n    hpa = stack_and_trim(\n        [[m[\"HPA\"] for m in r[\"metrics\"][\"val\"]] for r in runs if \"metrics\" in r]\n    )\n\n    preds_all = np.concatenate([np.asarray(r.get(\"predictions\", [])) for r in runs])\n    gts_all = np.concatenate([np.asarray(r.get(\"ground_truth\", [])) for r in runs])\n\n    n_runs = len(runs)\n    # ----------------------- 1. Loss plot -------------------------\n    try:\n        if train_losses is not None and val_losses is not None:\n            plt.figure(figsize=(6, 4), dpi=120)\n            mean_tr = train_losses.mean(axis=0)\n            sem_tr = train_losses.std(axis=0, ddof=1) / math.sqrt(n_runs)\n            mean_val = val_losses.mean(axis=0)\n            sem_val = val_losses.std(axis=0, ddof=1) / math.sqrt(n_runs)\n\n            plt.fill_between(\n                epochs_vec,\n                mean_tr - sem_tr,\n                mean_tr + sem_tr,\n                alpha=0.2,\n                label=\"Train \u00b1 s.e.m.\",\n            )\n            plt.plot(epochs_vec, mean_tr, label=\"Train mean\", color=\"blue\")\n            plt.fill_between(\n                epochs_vec,\n                mean_val - sem_val,\n                mean_val + sem_val,\n                alpha=0.2,\n                label=\"Val \u00b1 s.e.m.\",\n                color=\"orange\",\n            )\n            plt.plot(epochs_vec, mean_val, label=\"Val mean\", color=\"orange\")\n\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\n                f\"{dset_name} \u2013 Aggregated Train/Val Loss\\nShaded: \u00b11 standard error over {n_runs} runs\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_agg_loss.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dset_name}: {e}\")\n        plt.close()\n\n    # ----------------------- 2. Accuracy metrics ------------------\n    try:\n        if cwa is not None and swa is not None and hpa is not None:\n            plt.figure(figsize=(10, 4), dpi=120)\n            metrics = {\"CWA\": cwa, \"SWA\": swa, \"HPA\": hpa}\n            colors = {\"CWA\": \"tab:blue\", \"SWA\": \"tab:green\", \"HPA\": \"tab:red\"}\n            for i, (name, arr) in enumerate(metrics.items()):\n                mean = arr.mean(axis=0)\n                sem = arr.std(axis=0, ddof=1) / math.sqrt(n_runs)\n                plt.fill_between(\n                    epochs_vec, mean - sem, mean + sem, alpha=0.15, color=colors[name]\n                )\n                plt.plot(epochs_vec, mean, label=f\"{name} mean\", color=colors[name])\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{dset_name} \u2013 Validation Metrics (mean \u00b1 s.e.m., n={n_runs})\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_agg_val_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dset_name}: {e}\")\n        plt.close()\n\n    # ----------------------- 3. Label distribution ---------------\n    try:\n        if preds_all.size and gts_all.size:\n            labels = sorted(set(gts_all))\n            gt_counts = [(gts_all == l).sum() for l in labels]\n            pred_counts = [(preds_all == l).sum() for l in labels]\n\n            width = 0.35\n            x = np.arange(len(labels))\n            plt.figure(figsize=(6, 4), dpi=120)\n            plt.bar(x - width / 2, gt_counts, width=width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, pred_counts, width=width, label=\"Predictions\")\n            plt.xticks(x, labels)\n            plt.xlabel(\"Class Label\")\n            plt.ylabel(\"Count (aggregated)\")\n            plt.title(\n                f\"{dset_name} \u2013 Label Frequencies\\nLeft: Ground Truth, Right: Predictions (all runs)\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset_name}_agg_label_distribution.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution for {dset_name}: {e}\")\n        plt.close()\n"], "plot_plan": ["We need to load experiment_data.npy, loop over the three learning-rate runs stored under experiment_data['learning_rate']['SPR'], and extract per-epoch arrays for train/val losses and the four validation metrics (acc, cwa, swa, caa). To keep the figure count \u22645 we will combine all learning rates in the same plot and group related curves into paired subplots\u2014one figure for losses, one for basic vs complexity-aware accuracies, and one for color vs shape weighted accuracies. Each figure will be wrapped in its own try-except so that any failure leaves the others unaffected; all figures are saved inside the working directory with descriptive filenames that include the dataset name and plot type. After saving a figure we always call plt.close() to free resources. The code starts with the mandated imports, creates working_dir if missing, attempts to load the data with allow_pickle=True, then proceeds to plotting only if loading succeeds. We cap plots to at most the available epochs (\u22645) so no interval sampling is needed, but the logic is in place for future longer runs. Finally, the script prints a brief confirmation after each successful save so downstream logs show progress.", "The solution loads experiment_data.npy, iterates over each stored dataset (e.g. \u201cSPR\u201d), and for each produces up-to-five clear visualizations: (1) combined train/validation loss curves, (2) validation accuracy vs harmonic poly accuracy, (3) color- vs shape-weighted accuracies, and (4) a simple bar chart comparing ground-truth and predicted label counts from the last run.  Each figure is wrapped in its own try-except block, carries informative left/right subtitles, and is saved in the working directory with descriptive names containing the dataset identifier.  The code is generic\u2014if future experiments include several datasets, all plots are produced in one pass while keeping naming conventions consistent.  After saving, every figure is properly closed to avoid memory leaks and matplotlib state clashes.  Metrics are extracted directly from experiment_data without fabricating values, and plotting frequency is capped to stay within the five-figure guideline.", "We need to (1) load the stored dictionary, (2) pull out per-epoch losses and metrics under the \u201cSPR\u201d key, (3) generate up to four separate figures: loss curves, accuracy curves, weighted-accuracy (CWA/SWA) curves, and harmonic accuracy (HPA) curve, and (4) save each figure with an informative filename inside the working directory.  Each figure is wrapped in try-except so that errors never prevent the script from finishing, and every figure is closed with plt.close() whether the block succeeds or fails.  After plotting we also compute and print the best validation accuracy, CWA, SWA, and HPA obtained during training so the user immediately sees the main numbers.  The code is entirely self-contained: it begins with the required imports, constructs working_dir, loads experiment_data.npy, and then proceeds to plotting and metric reporting without referencing any nonexistent fields.  All titles clearly specify the dataset (\u201cSPR\u201d) and provide left/right subtitles as requested.  Finally, everything is packed into one concise Python snippet.", "We will load the saved experiment_data.npy, extract the SPR entry, and collect per-epoch arrays for the recorded losses and validation metrics.  Using basic matplotlib we create two figures inside independent try-except blocks: (1) a two-panel plot showing train and validation loss curves, and (2) a 2\u00d72 grid for the four validation metrics (ACC, CWA, SWA, HPA).  Each subplot is clearly titled (e.g. \u201cLeft: Train Loss, Right: Validation Loss\u201d) and the overall figure title specifies the dataset.  All figures are saved into the working directory with descriptive names that include the dataset and plot type.  We always close figures after saving and handle missing keys gracefully so the script never crashes even if some arrays are empty.  No synthetic data are generated; everything is read directly from experiment_data.npy.  The code begins with the mandated imports, constructs working_dir, and guards against missing files.  Finally, the script prints a confirmation message for every successfully written plot.", "We first load the saved experiment_data.npy and reconstruct the per-learning-rate history by slicing the flat lists (the training loop ran 6 epochs for each of 3 learning rates, so every 6 points correspond to one LR).  Using these partitions we prepare four figures: (1) train loss curves, (2) validation loss curves, (3) validation accuracy curves, and (4) color- vs shape-weighted accuracy curves.  Each figure is wrapped in an independent try-except block to comply with the robustness requirement; figures are always closed after saving.  Titles include clear \u201cLeft/Right\u201d subtitles, the dataset name (SPR) and the plotted quantity.  Only data present in experiment_data.npy are visualised, with at most six epochs shown per LR so no down-sampling is needed.  All plots are saved inside working_dir with descriptive filenames containing the dataset name and plot type.  After plotting, we compute and print the final epoch harmonic-poly accuracies for each learning rate to give a quick quantitative summary.  The code is concise, self-contained and uses only standard matplotlib and numpy.", "We load experiment_data.npy, fetch the SPR entry, and extract epoch indices, loss arrays (train & val) and the list-of-dict validation metrics.  Figure 1 plots train vs. validation loss on the same axis for easy comparison.  Figure 2 shows four subplots (ACC, CWA, SWA, HPA) over epochs to visualise the learning dynamics of every evaluation metric.  Figure 3 is a bar plot comparing the class\u2010wise counts of test ground-truth labels against predicted labels, giving an at-a-glance confusion profile (for a binary task this yields two bars per class).  Each figure lives in its own try-except block, is saved into working_dir with descriptive names, and the figure is always closed.  Only recorded data are used \u2013 no synthetic values are invented \u2013 and at most three figures are produced, satisfying the \u22645-figure guideline.  All paths, titles and file names include \u201cSPR\u201d to remain dataset\u2010specific.  Finally, the script prints the saved file names to acknowledge successful completion.", "We will load experiment_data.npy, extract the single dataset \u201cSPR\u201d, and pull out the stored lists for train/val losses and validation-time metrics (acc, CWA, SWA, HPA).  To visualise learning dynamics we will draw four figures, each wrapped in its own try-except: (1) train vs validation loss, (2) plain accuracy vs harmonic poly accuracy, (3) colour- vs shape-weighted accuracies, (4) a simple bar chart comparing the class-label distribution of ground-truth vs predictions on the test set.  All figures will be saved into the pre-created working_dir with descriptive filenames, closed after saving, and given clear titles of the form \u201cSPR \u2013 \u2026\u201d.  Because only one run is stored, epoch indices will simply be np.arange(len(losses))+1.  We avoid any fabricated values and never exceed five plots, satisfying the plotting policy.  If loading the npy fails we exit gracefully.  The code follows the required imports, directory setup and basic matplotlib usage without extra styling, and respects the guideline to close figures in every branch.", null, "We will load experiment_data.npy, extract the single-run \u201cSPR\u201d logs, and build up to three figures: (1) train vs. validation loss across epochs, (2) validation Color-Weighted and Shape-Weighted Accuracies, and (3) Harmonic Poly Accuracy plus a bar chart comparing test-set label frequencies for ground-truth and model predictions.  Each figure is wrapped in its own try-except so that one failure will not interrupt the others, and every open figure is explicitly closed.  We respect the 5-plot maximum and name files descriptively (e.g. SPR_loss_curves.png).  Titles always indicate \u201cLeft/Right\u201d panels and specify the SPR dataset.  Only values present in experiment_data.npy are used\u2014no synthetic metrics are invented.  All plots are saved to working_dir and matplotlib is used without custom styles.  The code begins with the required imports and directory setup, and concludes by printing paths of the saved images.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["learning_rate", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load experiment_data.npy from the working directory, walk\nthrough its nested structure (learning-rate \u2192 dataset \u2192 run log), and for each\ndataset print the final epoch\u2019s training loss, validation loss, validation\naccuracy, color-weighted accuracy, shape-weighted accuracy, and complexity-\nadjusted accuracy obtained with every learning rate tried. The dataset name is\nprinted before any metrics, and every metric is clearly labelled to avoid\nambiguity.", "The script loads the saved numpy file from the working directory, retrieves the\nlogged information for every dataset (only \u201cSPR\u201d here), and scans each list of\nlogged values to find the best value according to common-sense criteria\u2014minimum\nfor losses and maximum for accuracy-type metrics. It then prints the dataset\nname followed by clearly-labelled best metrics so that results are human-\nreadable. No plots or special entry points are used: the code executes\nimmediately on run.", "The script will load the saved NumPy file from the working directory, retrieve\nthe stored dictionaries, pick the last (final-epoch) entry for each recorded\nseries, and print them with clear, explicit labels. All code is placed at the\nglobal scope so the script executes immediately.", "The script will load the numpy file from the working directory, iterate over\nevery stored dataset (here just \u201cSPR\u201d), and gather the stored losses,\nvalidation\u2013time metric dictionaries, and final test predictions.   For each\nnumeric series it prints either the final value (training loss) or the best\nvalue (minimum validation loss, maximum validation accuracy/CWA/SWA/HPA).   If\ntest\u2010set predictions are present it computes and prints the resulting test\naccuracy.   All output lines start with a clear metric name (e.g. \u201cbest\nvalidation accuracy\u201d) as required.", "Below is a concise plan followed by a complete, ready-to-run script.   The\nscript loads the numpy file from the working directory, finds the best (minimum)\nloss and best (maximum) score for every recorded metric, and prints them with\nexplicit, descriptive names. It automatically works for any number of datasets\nor metrics stored in the same format and executes immediately on run.", "The script will (1) locate and load the saved NumPy dictionary, (2) iterate over\nevery dataset key it contains (only \u201cSPR\u201d here), (3) derive the final training\nloss, the minimum (best) validation loss, the validation-epoch whose HPA is\nhighest (and report all its metrics), and (4) recompute test accuracy from the\nstored predictions/ground-truth.   Each value is printed with an explicit, self-\ndescribing label such as \u201ctraining loss\u201d or \u201cbest validation harmonic poly\naccuracy,\u201d preceded by the dataset\u2019s name.", "The script will load the saved NumPy dictionary from the working directory,\nunpack the content for the single dataset (\u201cSPR\u201d), and then derive the relevant\nsummary statistics.  It reports the final training loss (last epoch), the best\nvalidation loss (minimum), the best validation accuracy, CWA, SWA, and HPA (each\ntaken at its maximum), and finally the test-set accuracy computed from the\nstored predictions and ground-truth labels.  The required information is printed\nwith explicitly named metrics and the dataset name preceding them.", "", "The script will load the saved NumPy file from the working directory, read the\nnested dictionary, and for each dataset (e.g., \u201cSPR\u201d) it will:   1. Print the\ndataset name.   2. Report the final training loss (last epoch).   3. Report the\nbest validation loss (minimum over epochs).   4. For every validation metric\n(CWA, SWA, HPA) compute and print the best value (maximum over epochs).   5. If\ntest predictions and ground-truth labels are available, compute and print the\noverall test accuracy.", "The script will load the saved NumPy file from the working directory, read the\nnested dictionary, and for each dataset (e.g., \u201cSPR\u201d) it will:   1. Print the\ndataset name.   2. Report the final training loss (last epoch).   3. Report the\nbest validation loss (minimum over epochs).   4. For every validation metric\n(CWA, SWA, HPA) compute and print the best value (maximum over epochs).   5. If\ntest predictions and ground-truth labels are available, compute and print the\noverall test accuracy.", "The script will load the saved NumPy file from the working directory, read the\nnested dictionary, and for each dataset (e.g., \u201cSPR\u201d) it will:   1. Print the\ndataset name.   2. Report the final training loss (last epoch).   3. Report the\nbest validation loss (minimum over epochs).   4. For every validation metric\n(CWA, SWA, HPA) compute and print the best value (maximum over epochs).   5. If\ntest predictions and ground-truth labels are available, compute and print the\noverall test accuracy.", "The script will load the saved NumPy file from the working directory, read the\nnested dictionary, and for each dataset (e.g., \u201cSPR\u201d) it will:   1. Print the\ndataset name.   2. Report the final training loss (last epoch).   3. Report the\nbest validation loss (minimum over epochs).   4. For every validation metric\n(CWA, SWA, HPA) compute and print the best value (maximum over epochs).   5. If\ntest predictions and ground-truth labels are available, compute and print the\noverall test accuracy.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------ locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------ parse & print ------------------\n# top level is experiment_data[\"learning_rate\"] -> dict(dataset_name -> dict(lr_key -> run_log))\nlearning_rate_block = experiment_data.get(\"learning_rate\", {})\n\nfor dataset_name, lr_dict in learning_rate_block.items():\n    print(f\"Dataset: {dataset_name}\")\n    for lr_key, run_log in lr_dict.items():\n        # final values are the last entries recorded during training\n        final_train_loss = run_log[\"losses\"][\"train\"][-1]\n        final_val_loss = run_log[\"losses\"][\"val\"][-1]\n        final_metrics = run_log[\"metrics\"][\"val\"][-1]  # dict with acc, cwa, swa, caa\n\n        print(f\"  Learning rate: {lr_key}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    final validation accuracy: {final_metrics['acc']:.4f}\")\n        print(\n            f\"    final validation color weighted accuracy: {final_metrics['cwa']:.4f}\"\n        )\n        print(\n            f\"    final validation shape weighted accuracy: {final_metrics['swa']:.4f}\"\n        )\n        print(\n            f\"    final validation complexity adjusted accuracy: {final_metrics['caa']:.4f}\"\n        )\n", "import os\nimport numpy as np\n\n# ----------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- helper to decide \u201cbest\u201d -----------------\ndef best_value(values, higher_is_better=True):\n    \"\"\"Return best value from a list; higher or lower depending on metric.\"\"\"\n    if not values:  # empty safety check\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# ----------------- iterate over datasets -----------------\nfor dset_name, dct in experiment_data.items():\n    print(f\"\\nDataset: {dset_name}\")\n\n    # ----- losses -----\n    train_losses = dct.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dct.get(\"losses\", {}).get(\"val\", [])\n    best_train_loss = best_value(train_losses, higher_is_better=False)\n    best_val_loss = best_value(val_losses, higher_is_better=False)\n\n    if best_train_loss is not None:\n        print(f\"Best training loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.4f}\")\n\n    # ----- validation metrics (accuracy variants) -----\n    val_metrics = dct.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # gather each metric over epochs\n        metrics_names = val_metrics[0].keys()\n        for m in metrics_names:\n            series = [ep[m] for ep in val_metrics]\n            best = best_value(series, higher_is_better=True)\n            # produce a readable label\n            label_map = {\n                \"acc\": \"Best validation accuracy\",\n                \"CWA\": \"Best validation color weighted accuracy\",\n                \"SWA\": \"Best validation shape weighted accuracy\",\n                \"HPA\": \"Best validation harmonic poly accuracy\",\n            }\n            readable_name = label_map.get(m, f\"Best validation {m}\")\n            print(f\"{readable_name}: {best:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- extract & report ----------\nfor dataset_name, dct in experiment_data.items():\n    # losses\n    train_losses = dct[\"losses\"][\"train\"]\n    val_losses = dct[\"losses\"][\"val\"]\n\n    # metrics\n    train_metrics = dct[\"metrics\"][\"train\"]\n    val_metrics = dct[\"metrics\"][\"val\"]\n\n    # take the final epoch values\n    final_train_loss = train_losses[-1] if train_losses else None\n    final_val_loss = val_losses[-1] if val_losses else None\n\n    final_train_acc = train_metrics[-1][\"acc\"] if train_metrics else None\n\n    if val_metrics:\n        final_val_acc = val_metrics[-1].get(\"acc\")\n        final_cwa = val_metrics[-1].get(\"cwa\")\n        final_swa = val_metrics[-1].get(\"swa\")\n        final_hpa = val_metrics[-1].get(\"hpa\")\n    else:\n        final_val_acc = final_cwa = final_swa = final_hpa = None\n\n    # ---------- printing ----------\n    print(dataset_name)\n    if final_train_loss is not None:\n        print(f\"training loss: {final_train_loss:.4f}\")\n    if final_train_acc is not None:\n        print(f\"training accuracy: {final_train_acc:.4f}\")\n    if final_val_loss is not None:\n        print(f\"validation loss: {final_val_loss:.4f}\")\n    if final_val_acc is not None:\n        print(f\"validation accuracy: {final_val_acc:.4f}\")\n    if final_cwa is not None:\n        print(f\"validation color-weighted accuracy: {final_cwa:.4f}\")\n    if final_swa is not None:\n        print(f\"validation shape-weighted accuracy: {final_swa:.4f}\")\n    if final_hpa is not None:\n        print(f\"validation harmonic poly accuracy: {final_hpa:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- helpers ----------\nmetric_name_map = {\n    \"acc\": \"accuracy\",\n    \"cwa\": \"CWA\",\n    \"swa\": \"SWA\",\n    \"hpa\": \"HPA\",\n}\n\n# ---------- print best/final metrics ----------\nfor dataset_name, payload in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # final training loss\n    train_losses = payload.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.4f}\")\n\n    # best validation loss (lower is better)\n    val_losses = payload.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        print(f\"best validation loss: {min(val_losses):.4f}\")\n\n    # best validation metrics (higher is better)\n    val_metrics = payload.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # gather per-metric series\n        keys = val_metrics[0].keys()\n        for k in keys:\n            series = [m[k] for m in val_metrics]\n            best_val = max(series)  # higher is better for all stored metrics\n            metric_readable = metric_name_map.get(k, k)\n            print(f\"best validation {metric_readable}: {best_val:.4f}\")\n\n    # test accuracy from stored predictions / ground truth\n    preds = payload.get(\"predictions\", [])\n    gts = payload.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = float(np.mean(np.array(preds) == np.array(gts)))\n        print(f\"test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------------- locate and load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------- utility: best value ------------------\ndef _best(values, higher_is_better=True):\n    \"\"\"Return best value depending on direction.\"\"\"\n    if not values:\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# ----------------- iterate over stored datasets ---------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- training loss ----\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    best_train_loss = _best(train_losses, higher_is_better=False)\n    if best_train_loss is not None:\n        print(f\"training loss: {best_train_loss:.6f}\")\n\n    # ---- validation loss ----\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    best_val_loss = _best(val_losses, higher_is_better=False)\n    if best_val_loss is not None:\n        print(f\"validation loss: {best_val_loss:.6f}\")\n\n    # ---- validation metrics (accuracy, etc.) ----\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # collect lists for each metric key\n        metric_keys = val_metrics[0].keys()\n        for m in metric_keys:\n            values = [ep[m] for ep in val_metrics]\n            # losses should decrease, everything else assumed higher=better\n            higher = not m.lower().endswith(\"loss\")\n            best_val = _best(values, higher_is_better=higher)\n            print(f\"validation {m}: {best_val:.6f}\")\n\n    # ---- spacing between datasets ----\n    print()\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef print_metric(name: str, value: float):\n    print(f\"  {name}: {value:.4f}\")\n\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"{ds_name} dataset:\")\n\n    # ----- losses -----\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print_metric(\"training loss (final epoch)\", train_losses[-1])\n\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print_metric(\"best validation loss\", best_val_loss)\n\n    # ----- validation metrics -----\n    val_metrics = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # choose the epoch with the highest harmonic poly accuracy\n        best_idx = max(range(len(val_metrics)), key=lambda i: val_metrics[i][\"HPA\"])\n        best_m = val_metrics[best_idx]\n        print_metric(\"best validation accuracy\", best_m[\"acc\"])\n        print_metric(\"best validation colour-weighted accuracy\", best_m[\"CWA\"])\n        print_metric(\"best validation shape-weighted accuracy\", best_m[\"SWA\"])\n        print_metric(\"best validation harmonic poly accuracy\", best_m[\"HPA\"])\n\n    # ----- test metrics -----\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        test_acc = sum(p == g for p, g in zip(preds, gts)) / len(gts)\n        print_metric(\"test accuracy\", test_acc)\n", "import os\nimport numpy as np\n\n# --------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# --------------- helper to get best metric ------------\ndef best_val_metric(metric_name, metrics_list, mode=\"max\"):\n    \"\"\"Return the best value across epochs for a given metric.\"\"\"\n    values = [d[metric_name] for d in metrics_list]\n    return max(values) if mode == \"max\" else min(values)\n\n\n# --------------- iterate over datasets ----------------\nfor dset_name, dset_dict in experiment_data.items():\n    print(f\"Dataset: {dset_name}\")\n\n    # Training loss (final epoch)\n    train_losses = dset_dict[\"losses\"][\"train\"]\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # Validation losses & metrics\n    val_losses = dset_dict[\"losses\"][\"val\"]\n    val_metrics = dset_dict[\"metrics\"][\"val\"]\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    if val_metrics:\n        best_val_acc = best_val_metric(\"acc\", val_metrics, mode=\"max\")\n        best_val_cwa = best_val_metric(\"CWA\", val_metrics, mode=\"max\")\n        best_val_swa = best_val_metric(\"SWA\", val_metrics, mode=\"max\")\n        best_val_hpa = best_val_metric(\"HPA\", val_metrics, mode=\"max\")\n\n        print(f\"best validation accuracy: {best_val_acc:.3f}\")\n        print(f\"best validation CWA: {best_val_cwa:.3f}\")\n        print(f\"best validation SWA: {best_val_swa:.3f}\")\n        print(f\"best validation HPA: {best_val_hpa:.3f}\")\n\n    # Test set accuracy (computed from stored predictions & ground truth)\n    preds = dset_dict.get(\"predictions\", [])\n    gts = dset_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        test_acc = np.mean([p == g for p, g in zip(preds, gts)])\n        print(f\"test accuracy: {test_acc:.3f}\")\n", "", "import os\nimport numpy as np\n\n# ------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------- iterate over stored datasets ---------\nfor dataset_name, info in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- final training loss ----\n    train_losses = info.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # ---- best validation loss ----\n    val_losses = info.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # ---- best validation metrics (CWA, SWA, HPA) ----\n    val_metrics = info.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        metric_names = val_metrics[0].keys()\n        for m in metric_names:\n            best_metric_value = max(epoch_metrics[m] for epoch_metrics in val_metrics)\n            print(f\"best validation {m}: {best_metric_value:.4f}\")\n\n    # ---- test accuracy (if predictions are stored) ----\n    preds = np.array(info.get(\"predictions\", []))\n    gts = np.array(info.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_accuracy = (preds == gts).mean()\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------- iterate over stored datasets ---------\nfor dataset_name, info in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- final training loss ----\n    train_losses = info.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # ---- best validation loss ----\n    val_losses = info.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # ---- best validation metrics (CWA, SWA, HPA) ----\n    val_metrics = info.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        metric_names = val_metrics[0].keys()\n        for m in metric_names:\n            best_metric_value = max(epoch_metrics[m] for epoch_metrics in val_metrics)\n            print(f\"best validation {m}: {best_metric_value:.4f}\")\n\n    # ---- test accuracy (if predictions are stored) ----\n    preds = np.array(info.get(\"predictions\", []))\n    gts = np.array(info.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_accuracy = (preds == gts).mean()\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------- iterate over stored datasets ---------\nfor dataset_name, info in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- final training loss ----\n    train_losses = info.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # ---- best validation loss ----\n    val_losses = info.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # ---- best validation metrics (CWA, SWA, HPA) ----\n    val_metrics = info.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        metric_names = val_metrics[0].keys()\n        for m in metric_names:\n            best_metric_value = max(epoch_metrics[m] for epoch_metrics in val_metrics)\n            print(f\"best validation {m}: {best_metric_value:.4f}\")\n\n    # ---- test accuracy (if predictions are stored) ----\n    preds = np.array(info.get(\"predictions\", []))\n    gts = np.array(info.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_accuracy = (preds == gts).mean()\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------- iterate over stored datasets ---------\nfor dataset_name, info in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- final training loss ----\n    train_losses = info.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.4f}\")\n\n    # ---- best validation loss ----\n    val_losses = info.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # ---- best validation metrics (CWA, SWA, HPA) ----\n    val_metrics = info.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        metric_names = val_metrics[0].keys()\n        for m in metric_names:\n            best_metric_value = max(epoch_metrics[m] for epoch_metrics in val_metrics)\n            print(f\"best validation {m}: {best_metric_value:.4f}\")\n\n    # ---- test accuracy (if predictions are stored) ----\n    preds = np.array(info.get(\"predictions\", []))\n    gts = np.array(info.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_accuracy = (preds == gts).mean()\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n", ""], "parse_term_out": ["['Dataset: SPR', '\\n', '  Learning rate: 3e-04', '\\n', '    final training loss:\n0.5023', '\\n', '    final validation loss: 0.3987', '\\n', '    final validation\naccuracy: 0.8833', '\\n', '    final validation color weighted accuracy: 0.8854',\n'\\n', '    final validation shape weighted accuracy: 0.8727', '\\n', '    final\nvalidation complexity adjusted accuracy: 0.8792', '\\n', '  Learning rate:\n1e-03', '\\n', '    final training loss: 0.4830', '\\n', '    final validation\nloss: 0.3891', '\\n', '    final validation accuracy: 0.8833', '\\n', '    final\nvalidation color weighted accuracy: 0.8854', '\\n', '    final validation shape\nweighted accuracy: 0.8727', '\\n', '    final validation complexity adjusted\naccuracy: 0.8792', '\\n', '  Learning rate: 3e-03', '\\n', '    final training\nloss: 0.4482', '\\n', '    final validation loss: 0.3505', '\\n', '    final\nvalidation accuracy: 0.8833', '\\n', '    final validation color weighted\naccuracy: 0.8854', '\\n', '    final validation shape weighted accuracy: 0.8727',\n'\\n', '    final validation complexity adjusted accuracy: 0.8792', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR', '\\n', 'Best training loss: 0.0463', '\\n', 'Best validation\nloss: 0.1320', '\\n', 'Best validation accuracy: 0.9700', '\\n', 'Best validation\ncolor weighted accuracy: 0.9717', '\\n', 'Best validation shape weighted\naccuracy: 0.9785', '\\n', 'Best validation harmonic poly accuracy: 0.9751', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR', '\\n', 'training loss: 0.3934', '\\n', 'training accuracy: 0.8400', '\\n',\n'validation loss: 0.4218', '\\n', 'validation accuracy: 0.8417', '\\n',\n'validation color-weighted accuracy: 0.8336', '\\n', 'validation shape-weighted\naccuracy: 0.8538', '\\n', 'validation harmonic poly accuracy: 0.8436', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR', '\\n', 'final training loss: 0.3939', '\\n', 'best validation\nloss: 0.4826', '\\n', 'best validation accuracy: 0.8150', '\\n', 'best validation\nCWA: 0.8217', '\\n', 'best validation SWA: 0.8337', '\\n', 'best validation HPA:\n0.8276', '\\n', 'test accuracy: 0.8067', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR', '\\n', 'training loss: 0.476033', '\\n', 'validation loss: 0.474007',\n'\\n', 'validation acc: 0.810000', '\\n', 'validation cwa: 0.812918', '\\n',\n'validation swa: 0.819383', '\\n', 'validation hpa: 0.816138', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR dataset:', '\\n', '  training loss (final epoch): 0.0141', '\\n', '  best\nvalidation loss: 0.0844', '\\n', '  best validation accuracy: 0.9833', '\\n', '\nbest validation colour-weighted accuracy: 0.9870', '\\n', '  best validation\nshape-weighted accuracy: 0.9928', '\\n', '  best validation harmonic poly\naccuracy: 0.9899', '\\n', '  test accuracy: 0.9867', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR', '\\n', 'final training loss: 0.0288', '\\n', 'best validation\nloss: 0.0357', '\\n', 'best validation accuracy: 0.993', '\\n', 'best validation\nCWA: 0.997', '\\n', 'best validation SWA: 0.998', '\\n', 'best validation HPA:\n0.997', '\\n', 'test accuracy: 1.000', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "", "['SPR', '\\n', 'final training loss: 0.0035', '\\n', 'best validation loss:\n0.0225', '\\n', 'best validation CWA: 1.0000', '\\n', 'best validation SWA:\n1.0000', '\\n', 'best validation HPA: 1.0000', '\\n', 'test accuracy: 0.9700',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR', '\\n', 'final training loss: 0.0047', '\\n', 'best validation loss:\n0.5623', '\\n', 'best validation CWA: 0.9867', '\\n', 'best validation SWA:\n0.9902', '\\n', 'best validation HPA: 0.9885', '\\n', 'test accuracy: 0.9900',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR', '\\n', 'final training loss: 0.0088', '\\n', 'best validation loss:\n0.6571', '\\n', 'best validation CWA: 0.9736', '\\n', 'best validation SWA:\n0.9816', '\\n', 'best validation HPA: 0.9775', '\\n', 'test accuracy: 0.9850',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR', '\\n', 'final training loss: 0.0088', '\\n', 'best validation loss:\n0.6571', '\\n', 'best validation CWA: 0.9736', '\\n', 'best validation SWA:\n0.9816', '\\n', 'best validation HPA: 0.9775', '\\n', 'test accuracy: 0.9850',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}