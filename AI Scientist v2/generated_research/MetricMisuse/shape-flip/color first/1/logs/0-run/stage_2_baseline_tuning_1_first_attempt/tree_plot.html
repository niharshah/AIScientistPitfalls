<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 6], [0, 2], [0, 4], [0, 1], [0, 3], [0, 8], [2, 9], [2, 12], [2, 10], [2, 11], [3, 7], [3, 5]], "layout": [[0.5777777777777777, 0.0], [0.06666666666666667, 0.5], [0.2, 0.5], [0.6, 0.5], [0.7333333333333333, 0.5], [0.5333333333333333, 1.0], [0.8666666666666667, 0.5], [0.6666666666666666, 1.0], [1.0, 0.5], [0.0, 1.0], [0.13333333333333333, 1.0], [0.26666666666666666, 1.0], [0.4, 1.0]], "plan": ["We turn each symbolic sequence into a simple chain graph whose nodes are the\ntokens and whose edges connect consecutive tokens in both directions.  A\nvocabulary built from the training split maps every token string to a unique\ninteger, giving a learnable embedding as the initial node feature; the whole\ngraph is summarised with mean-pooling after two GraphSAGE layers and classified\nwith a linear head.  We train the model for a few epochs with cross-entropy,\nevaluating after every epoch on the development set while tracking validation\nloss, Color-Weighted Accuracy, Shape-Weighted Accuracy, and the requested\nComplexity-Adjusted Accuracy (complexity = unique colors + unique shapes).  All\ntensors, the model, and the optimizer are kept on GPU when available, and\nbatches are moved to the device inside the training loop.  Metrics, losses,\npredictions, and ground-truth labels are stored in a hierarchical\nexperiment_data dict and saved to disk as a NumPy file at the end of the run.\nIf the benchmark folder is missing we fall back to a tiny synthetic dataset so\nthe script is always runnable.  The code executes immediately when run, needs no\nentry point, and creates any plots or temporary files inside the ./working\ndirectory.", "Hyperparam tuning name: epochs. We untie the bottleneck by making \u201cepochs\u201d a\ntunable hyper-parameter.  After rebuilding the original pipeline, the script now\ntrains for up to 30 epochs (with a simple early\u2013stopping safeguard) and records\nfull loss/metric curves so you can compare longer training runs with the\noriginal 5-epoch baseline.  The results are stored in the required\nexperiment_data.npy file under the top-level key \u201cepochs\u201d, ready for plotting or\nfurther analysis.", "Hyperparam tuning name: learning_rate. We keep the original pipeline (data\npreparation, model, metrics, training loop) and wrap it inside a grid-search\nloop that re-initializes the model for each candidate learning rate (3e-4, 1e-3,\n3e-3). Validation scores, losses and predictions for every epoch of every run\nare stored under experiment_data['learning_rate']['SPR'][<lr>]. Everything is\nfinally saved to \u2018experiment_data.npy\u2019. The rest of the code remains unchanged\nso the script is fully self-contained and executable.", "Hyperparam tuning name: batch_size. The script below extends the baseline by\nlooping over different training batch sizes (16, 32, 64, 128).  For each size it\nrebuilds the model, trains for a few epochs, evaluates on the dev set, logs per-\nepoch losses and metrics, and finally stores everything in the required\nexperiment_data dictionary before saving it to experiment_data.npy.  This makes\nit easy to compare how batch size influences convergence and validation\nperformance.", "Hyperparam tuning name: weight_decay. Below is a compact extension of the\nbaseline that grid-searches Adam\u2019s weight_decay over the set {0, 1e-5, 1e-4,\n1e-3, 1e-2}.   For every value it trains for 5 epochs, logs per-epoch\nlosses/metrics, and stores everything in the required experiment_data structure\nbefore writing it to \u201cexperiment_data.npy\u201d.", "The crash was caused by passing the keyword argument num_classes to\nGNNClassifier while the constructor expects classes. The fix is to instantiate\nthe model with classes=num_classes (or, equivalently, rename the constructor\nargument). I also add the required Harmonic-Weighted Accuracy metric, make sure\ntensors/models are always moved to the chosen device, and store every metric and\nloss in experiment_data before saving it to ./working/experiment_data.npy.\nEverything now runs end-to-end and respects the execution-time, GPU-handling and\nlogging guidelines.", "Hyperparam tuning name: embed_dim. We loop over several candidate embed_dim\nvalues (16, 32, 64, 128), build a fresh GNNClassifier with the given\ndimensionality, train it for a small number of epochs, evaluate on the dev split\nand store all losses / metrics / predictions inside a hierarchical\nexperiment_data dictionary whose top-level key is the hyperparameter being tuned\n(\u201cembed_dim\u201d). Finally we save the whole structure to experiment_data.npy.", "The failure was caused by passing an unexpected keyword argument (`num_classes`)\nto `GNNClassifier`.  We simply rename the constructor parameter from `classes`\nto `num_classes` so it matches the call site.  While touching the code we also:\n(1) add a Harmonic-Weighted-Accuracy (HWA) metric, (2) respect all device-\nplacement rules, and (3) make sure results are logged and saved as required.\nNothing else about the architecture is changed, so this stays within the\n\u201cbaseline_tuning\u201d stage.", "Hyperparam tuning name: dropout_rate. The solution introduces a `dropout` hyper-\nparameter into the GNN by inserting `nn.Dropout` layers after every SAGEConv\noutput and before the final linear layer. We then sweep through keep-\nprobabilities (0.0, 0.1, 0.25, 0.4, 0.5), training a fresh model for five epochs\non each setting, and log per-epoch losses plus four validation metrics. All\nresults are stored in a nested NumPy-serialisable dictionary under the key\n`dropout_tuning`, and finally written to `experiment_data.npy`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, string, time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# --------------------------------------------------------------\n# working directory & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# --------------------------------------------------------------\n\n\n# --------------------- helper: metrics -------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\n# --------------------------------------------------------------\n\n\n# --------------------- data loading ---------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef generate_synthetic(num_train=200, num_dev=60, num_test=100):\n    shapes = list(string.ascii_uppercase[:6])  # 6 shapes\n    colors = list(string.ascii_lowercase[:6])  # 6 colors\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            tokens = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq = \" \".join(tokens)\n            # Simple rule: label is 1 if first and last shape same else 0\n            label = int(tokens[0][0] == tokens[-1][0])\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    train = _gen(num_train)\n    dev = _gen(num_dev)\n    test = _gen(num_test)\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(train),\n            \"dev\": Dataset.from_dict(dev),\n            \"test\": Dataset.from_dict(test),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Loaded dataset with {len(dataset['train'])} train samples, \"\n    f\"{len(dataset['dev'])} dev samples, num_classes={num_classes}\"\n)\n# --------------------------------------------------------------\n\n# --------------------- vocabulary build -----------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n# --------------------------------------------------------------\n\n\n# -------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    # edges: consecutive tokens, undirected\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq  # store original string for metric computation\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n# --------------------------------------------------------------\n\n\n# -------------------------- model -----------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())  # (N, embed_dim)\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)  # (batch, hidden)\n        return self.lin(x)\n\n\nmodel = GNNClassifier(vocab, num_classes=num_classes).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n# --------------------------------------------------------------\n\n# ---------------------- data loaders --------------------------\ntrain_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=64)\ntest_loader = DataLoader(test_graphs, batch_size=64)\n# --------------------------------------------------------------\n\n# ------------------ experiment tracking -----------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n# --------------------------------------------------------------\n\n# ---------------------- training loop -------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    train_loss = total_loss / len(train_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation\n    model.eval()\n    dev_loss, preds, gts, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            dev_loss += loss.item() * batch.num_graphs\n            pred = logits.argmax(dim=-1).cpu().tolist()\n            gt = batch.y.cpu().tolist()\n            seqs.extend(batch.seq_raw)\n            preds.extend(pred)\n            gts.extend(gt)\n    dev_loss /= len(dev_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(dev_loss)\n\n    acc = np.mean([p == t for p, t in zip(preds, gts)])\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    caa = complexity_adjusted_accuracy(seqs, gts, preds)\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n    )\n    experiment_data[\"SPR\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, validation_loss={dev_loss:.4f}, \"\n        f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n    )\n\n# save dev predictions\nexperiment_data[\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\nprint(\n    \"Training complete. Metrics saved to\",\n    os.path.join(working_dir, \"experiment_data.npy\"),\n)\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# --------------- experiment bookkeeping -----------------\nexperiment_data = {\n    \"epochs\": {  # hyper-parameter we tune\n        \"SPR\": {  # dataset name\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n        }\n    }\n}\n# --------------------------------------------------------\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------ metric helpers ----------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef _weighted_acc(weights, y_true, y_pred):\n    return sum(w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)) / max(\n        sum(weights), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    return _weighted_acc([count_color_variety(s) for s in seqs], y_true, y_pred)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    return _weighted_acc([count_shape_variety(s) for s in seqs], y_true, y_pred)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    return _weighted_acc(w, y_true, y_pred)\n\n\n# --------------------------------------------------------\n\n\n# ----------------- dataset loading ----------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef generate_synthetic(n_train=200, n_dev=60, n_test=100):\n    shapes, colors = list(string.ascii_uppercase[:6]), list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            toks = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seqs.append(\" \".join(toks))\n            labels.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        train=Dataset.from_dict(_gen(n_train)),\n        dev=Dataset.from_dict(_gen(n_dev)),\n        test=Dataset.from_dict(_gen(n_test)),\n    )\n\n\ndataset = try_load_benchmark() or generate_synthetic()\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Data: {len(dataset['train'])} train / {len(dataset['dev'])} dev samples, classes={num_classes}\"\n)\n\n# ------------------ vocabulary --------------------------\nvocab = {}\n\n\ndef add(tok):\n    vocab.setdefault(tok, len(vocab))\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for t in seq.split():\n        add(t)\nprint(\"Vocabulary size:\", len(vocab))\n\n\n# --------------- graph construction ---------------------\ndef seq_to_graph(seq, label):\n    node_ids = torch.tensor([vocab[t] for t in seq.split()], dtype=torch.long)\n    if len(node_ids) > 1:\n        src = list(range(len(node_ids) - 1))\n        dst = list(range(1, len(node_ids)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    d = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    d.seq_raw = seq\n    return d\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\ntrain_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=64)\ntest_loader = DataLoader(test_graphs, batch_size=64)\n\n\n# ------------------- model ------------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab_sz, embed_dim=32, hidden_dim=64, n_cls=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, n_cls)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\nmodel = GNNClassifier(len(vocab), n_cls=num_classes).to(device)\noptimizer, criterion = Adam(model.parameters(), lr=1e-3), nn.CrossEntropyLoss()\n\n# -------------- training with more epochs ---------------\nmax_epochs, patience = 30, 5\nbest_val_loss, epochs_no_improve = float(\"inf\"), 0\n\nfor epoch in range(1, max_epochs + 1):\n    # ---- train ----\n    model.train()\n    tot_loss = 0\n    correct = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch.num_graphs\n        correct += (out.argmax(1) == batch.y).sum().item()\n    train_loss = tot_loss / len(train_loader.dataset)\n    train_acc = correct / len(train_loader.dataset)\n    experiment_data[\"epochs\"][\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"epochs\"][\"SPR\"][\"metrics\"][\"train\"].append({\"acc\": train_acc})\n\n    # ---- validation ----\n    model.eval()\n    v_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = batch.to(device)\n            logits = model(batch)\n            v_loss += criterion(logits, batch.y).item() * batch.num_graphs\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(batch.y.cpu().tolist())\n            seqs.extend(batch.seq_raw)\n    v_loss /= len(dev_loader.dataset)\n    acc = np.mean([p == t for p, t in zip(preds, gts)])\n    cwa = color_weighted_accuracy(seqs, gts, preds)\n    swa = shape_weighted_accuracy(seqs, gts, preds)\n    caa = complexity_adjusted_accuracy(seqs, gts, preds)\n    experiment_data[\"epochs\"][\"SPR\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"epochs\"][\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n    )\n    experiment_data[\"epochs\"][\"SPR\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}/{max_epochs} \"\n        f\"train_loss={train_loss:.4f} val_loss={v_loss:.4f} \"\n        f\"val_acc={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CAA={caa:.3f}\"\n    )\n\n    # early stopping\n    if v_loss < best_val_loss - 1e-4:\n        best_val_loss, epochs_no_improve = v_loss, 0\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# save final dev predictions / ground truths\nexperiment_data[\"epochs\"][\"SPR\"][\"predictions\"] = preds\nexperiment_data[\"epochs\"][\"SPR\"][\"ground_truth\"] = gts\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All metrics saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, math, json, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------- helper: metrics ------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\n# --------------------- data loading ---------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef generate_synthetic(num_train=200, num_dev=60, num_test=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            tokens = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq = \" \".join(tokens)\n            label = int(tokens[0][0] == tokens[-1][0])\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(num_train)),\n            \"dev\": Dataset.from_dict(_gen(num_dev)),\n            \"test\": Dataset.from_dict(_gen(num_test)),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Loaded dataset with {len(dataset['train'])} train, {len(dataset['dev'])} dev samples, classes={num_classes}\"\n)\n\n# ------------------ vocabulary build --------------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n\n# -------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# -------------------------- model -----------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------ experiment tracking -----------------------\nexperiment_data = {\"learning_rate\": {\"SPR\": {}}}\n\n# ------------------ hyperparameter sweep ----------------------\nlearning_rates = [3e-4, 1e-3, 3e-3]\nepochs = 5\nbatch_size_train = 32\nbatch_size_eval = 64\n\nfor lr in learning_rates:\n    lr_key = f\"{lr:.0e}\" if lr < 1 else str(lr)\n    print(f\"\\n=== Training with learning rate {lr_key} ===\")\n    model = GNNClassifier(vocab, num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(train_graphs, batch_size=batch_size_train, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=batch_size_eval)\n\n    run_log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # ---- train\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n        run_log[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation\n        model.eval()\n        dev_loss, preds, gts, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                pred = logits.argmax(dim=-1).cpu().tolist()\n                gt = batch.y.cpu().tolist()\n                seqs.extend(batch.seq_raw)\n                preds.extend(pred)\n                gts.extend(gt)\n        dev_loss /= len(dev_loader.dataset)\n        run_log[\"losses\"][\"val\"].append(dev_loss)\n\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        run_log[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n        run_log[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={dev_loss:.4f}, \"\n            f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n        )\n\n    run_log[\"predictions\"] = preds\n    run_log[\"ground_truth\"] = gts\n    experiment_data[\"learning_rate\"][\"SPR\"][lr_key] = run_log\n\n# --------------- save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All runs finished. Data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, warnings\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\nwarnings.filterwarnings(\"ignore\")\n# ---------------- environment -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------- helpers ---------------------\ndef count_color_variety(sequence):\n    return len(set(t[1] for t in sequence.split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence):\n    return len(set(t[0] for t in sequence.split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [w_i if t == p else 0 for w_i, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [w_i if t == p else 0 for w_i, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    c = [w_i if t == p else 0 for w_i, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\n# ---------- data acquisition / generation -----\ndef try_load_benchmark():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    from datasets import load_dataset, DatasetDict\n\n    def _ld(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef generate_synthetic(nt=200, nd=60, nte=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _g(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            toks = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seqs.append(\" \".join(toks))\n            labels.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        train=Dataset.from_dict(_g(nt)),\n        dev=Dataset.from_dict(_g(nd)),\n        test=Dataset.from_dict(_g(nte)),\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Dataset sizes: train={len(dataset['train'])}, dev={len(dataset['dev'])}, classes={num_classes}\"\n)\n\n# -------------- build vocab -------------------\nvocab = {}\n\n\ndef add_token(t):\n    if t not in vocab:\n        vocab[t] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nprint(\"Vocab size:\", len(vocab))\n\n\n# -------------- graphs -------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    if len(toks) > 1:\n        src = list(range(len(toks) - 1))\n        dst = list(range(1, len(toks)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    d = Data(x=node_ids.unsqueeze(-1), edge_index=edge_index, y=torch.tensor([label]))\n    d.seq_raw = seq\n    return d\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# -------------- model --------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hid=64, classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.lin = nn.Linear(hid, classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# -------------- hyperparameter tuning ----------\nbatch_sizes = [16, 32, 64, 128]\nepochs = 5\nexperiment_data = {\"batch_size_tuning\": {}}\n\nfor bs in batch_sizes:\n    tag = f\"bs_{bs}\"\n    print(f\"\\n--- Training with batch_size={bs} ---\")\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    model = GNNClassifier(len(vocab), num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=bs, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=max(64, bs))\n    # training loop\n    for ep in range(1, epochs + 1):\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch.num_graphs\n        tr_loss = tot_loss / len(train_loader.dataset)\n        exp[\"losses\"][\"train\"].append(tr_loss)\n        # validation\n        model.eval()\n        dv_loss = 0.0\n        preds = []\n        gts = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dv_loss += loss.item() * batch.num_graphs\n                p = logits.argmax(-1).cpu().tolist()\n                g = batch.y.cpu().tolist()\n                preds.extend(p)\n                gts.extend(g)\n                seqs.extend(batch.seq_raw)\n        dv_loss /= len(dev_loader.dataset)\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        exp[\"losses\"][\"val\"].append(dv_loss)\n        exp[\"metrics\"][\"val\"].append({\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa})\n        exp[\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {ep}/{epochs} | train_loss={tr_loss:.4f} | val_loss={dv_loss:.4f} | acc={acc:.3f}\"\n        )\n    exp[\"predictions\"] = preds\n    exp[\"ground_truth\"] = gts\n    experiment_data[\"batch_size_tuning\"][tag] = exp\n\n# -------------- save results ------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nFinished. Saved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, warnings\n\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ------------------- reproducibility / device -----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------- helper metrics ---------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\n# ------------------- dataset utilities -----------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(train=_load(\"train\"), dev=_load(\"dev\"), test=_load(\"test\"))\n\n\ndef generate_synthetic(nt=200, nd=60, nte=100):\n    shapes, colors = string.ascii_uppercase[:6], string.ascii_lowercase[:6]\n\n    def _gen(n):\n        seq, lab = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            toks = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq.append(\" \".join(toks))\n            lab.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seq, \"label\": lab}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        train=Dataset.from_dict(_gen(nt)),\n        dev=Dataset.from_dict(_gen(nd)),\n        test=Dataset.from_dict(_gen(nte)),\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found \u2013 generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Train {len(dataset['train'])}, Dev {len(dataset['dev'])}, \"\n    f\"Test {len(dataset['test'])}, num_classes={num_classes}\"\n)\n\n# ------------------- vocabulary ------------------------------\nvocab = {}\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocabulary size:\", vocab_size)\n\n\n# ------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    if len(toks) > 1:\n        src = list(range(len(toks) - 1))\n        dst = list(range(1, len(toks)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    d = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    d.seq_raw = seq\n    return d\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\ntrain_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=64)\ntest_loader = DataLoader(test_graphs, batch_size=64)\n\n\n# ------------------- model -----------------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hidden=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden)\n        self.conv2 = SAGEConv(hidden, hidden)\n        self.lin = nn.Linear(hidden, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------- training utility ------------------------\ndef run_training(weight_decay, epochs=5):\n    model = GNNClassifier(len(vocab), num_classes=num_classes).to(device)\n    optim = Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n    cri = nn.CrossEntropyLoss()\n\n    record = {\n        \"weight_decay\": weight_decay,\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for ep in range(1, epochs + 1):\n        # --- train\n        model.train()\n        tloss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optim.zero_grad()\n            out = model(batch)\n            loss = cri(out, batch.y)\n            loss.backward()\n            optim.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(train_loader.dataset)\n        record[\"losses\"][\"train\"].append(tloss)\n\n        # --- validation\n        model.eval()\n        vloss = 0.0\n        preds = []\n        gts = []\n        seqs = []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = cri(logits, batch.y)\n                vloss += loss.item() * batch.num_graphs\n                preds.extend(logits.argmax(-1).cpu().tolist())\n                gts.extend(batch.y.cpu().tolist())\n                seqs.extend(batch.seq_raw)\n        vloss /= len(dev_loader.dataset)\n        record[\"losses\"][\"val\"].append(vloss)\n\n        acc = np.mean([p == t for p, t in zip(preds, gts)])\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        record[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n\n        print(\n            f\"wd={weight_decay:>5}: epoch {ep} | \"\n            f\"train_loss {tloss:.4f} | val_loss {vloss:.4f} | acc {acc:.3f}\"\n        )\n\n    record[\"predictions\"] = preds\n    record[\"ground_truth\"] = gts\n    return record\n\n\n# ------------------- hyperparameter sweep --------------------\nweight_decays = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\nexperiment_data = {\"weight_decay\": {\"SPR\": {}}}\n\nfor wd in weight_decays:\n    result = run_training(wd)\n    experiment_data[\"weight_decay\"][\"SPR\"][f\"wd_{wd}\"] = result\n\n# ------------------- save ------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All runs finished. Data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "import os, pathlib, random, string, time, numpy as np, torch, warnings\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\nfrom datasets import load_dataset, Dataset, DatasetDict\n\n# ------------------- mandatory boiler-plate -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# ------------------- metrics ---------------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if len(t) > 0))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [w_i if t == p else 0 for w_i, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [w_i if t == p else 0 for w_i, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / max(cwa + swa, 1e-9)\n\n\n# ------------------- data utilities --------------------------\ndef try_load_spr_bench():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _ld(fname):\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef generate_synthetic(nt=200, nd=60, nte=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _make(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            toks = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seqs.append(\" \".join(toks))\n            labels.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    return DatasetDict(\n        train=Dataset.from_dict(_make(nt)),\n        dev=Dataset.from_dict(_make(nd)),\n        test=Dataset.from_dict(_make(nte)),\n    )\n\n\ndataset = try_load_spr_bench()\nif dataset is None:\n    print(\"Benchmark not found; using synthetic dataset.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Dataset sizes: train={len(dataset['train'])}, dev={len(dataset['dev'])}, classes={num_classes}\"\n)\n\n# ------------------- vocabulary ------------------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nprint(\"Vocabulary size:\", len(vocab))\n\n\n# ------------------- graph conversion ------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    if len(toks) > 1:\n        src = list(range(len(toks) - 1))\n        dst = list(range(1, len(toks)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    g = Data(x=node_ids.unsqueeze(-1), edge_index=edge_index, y=torch.tensor([label]))\n    g.seq_raw = seq\n    return g\n\n\ndef build_graph_list(split):\n    return [\n        seq_to_graph(s, l)\n        for s, l in zip(dataset[split][\"sequence\"], dataset[split][\"label\"])\n    ]\n\n\ntrain_graphs = build_graph_list(\"train\")\ndev_graphs = build_graph_list(\"dev\")\n\n\n# ------------------- model -----------------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hid=64, classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.lin = nn.Linear(hid, classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------- experiment tracking ---------------------\nexperiment_data = {\n    \"batch_size_tuning\": {  # single dataset bucket\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ------------------- training / tuning -----------------------\nbatch_sizes = [16, 32, 64, 128]\nepochs = 5\n\nfor bs in batch_sizes:\n    print(f\"\\n===== Training with batch_size={bs} =====\")\n    model = GNNClassifier(len(vocab), classes=num_classes).to(device)  # bug fixed\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=bs, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=max(64, bs))\n    for ep in range(1, epochs + 1):\n        # ---- training ----\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch.num_graphs\n        tr_loss = tot_loss / len(train_loader.dataset)\n        experiment_data[\"batch_size_tuning\"][\"losses\"][\"train\"].append(tr_loss)\n\n        # ---- validation ----\n        model.eval()\n        val_loss, preds, gts, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                p = logits.argmax(-1).cpu().tolist()\n                g = batch.y.cpu().tolist()\n                preds.extend(p)\n                gts.extend(g)\n                seqs.extend(batch.seq_raw)\n        val_loss /= len(dev_loader.dataset)\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n\n        experiment_data[\"batch_size_tuning\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"batch_size_tuning\"][\"metrics\"][\"val\"].append(\n            {\"epoch\": ep, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"hwa\": hwa}\n        )\n        experiment_data[\"batch_size_tuning\"][\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {ep}/{epochs}: validation_loss = {val_loss:.4f} | acc={acc:.3f} | CWA={cwa:.3f} | SWA={swa:.3f} | HWA={hwa:.3f}\"\n        )\n\n    experiment_data[\"batch_size_tuning\"][\"predictions\"].append(preds)\n    experiment_data[\"batch_size_tuning\"][\"ground_truth\"].append(gts)\n\n# ------------------- save results ----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f\"\\nFinished. Results saved to {os.path.join(working_dir, 'experiment_data.npy')}\"\n)\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# -------------------------------------------------------------- #\n# experiment bookkeeping                                         #\n# -------------------------------------------------------------- #\nexperiment_data = {\n    \"embed_dim\": {  # hyperparameter being tuned\n        \"SPR\": []  # list holding results for every embed_dim tried\n    }\n}\n\n# -------------------------------------------------------------- #\n# device & working dir                                           #\n# -------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# --------------------- helper: metrics ------------------------- #\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# ----------------------- data utils --------------------------- #\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _l(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = lambda x: _l(f\"{x}.csv\")\n    return DatasetDict(train=d(\"train\"), dev=d(\"dev\"), test=d(\"test\"))\n\n\ndef generate_synthetic(n_tr=200, n_dev=60, n_test=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            toks = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seqs.append(\" \".join(toks))\n            labels.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        train=Dataset.from_dict(_gen(n_tr)),\n        dev=Dataset.from_dict(_gen(n_dev)),\n        test=Dataset.from_dict(_gen(n_test)),\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found, generating synthetic dataset.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Dataset sizes: train={len(dataset['train'])}, dev={len(dataset['dev'])}, test={len(dataset['test'])}\"\n)\n\n# -------------------- build vocabulary ------------------------ #\nvocab = {}\n\n\ndef add_token(t):\n    vocab.setdefault(t, len(vocab))\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\n# ------------------ sequence to graph ------------------------- #\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\ntrain_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=64)\n\n\n# -------------------------- model ----------------------------- #\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------- training routine ------------------------- #\ndef run_experiment(embed_dim, epochs=5):\n    model = GNNClassifier(\n        vocab_size, embed_dim, hidden_dim=64, num_classes=num_classes\n    ).to(device)\n    optim = Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    metrics_train, metrics_val = [], []\n    losses_train, losses_val = [], []\n    for ep in range(1, epochs + 1):\n        # train\n        model.train()\n        tot = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optim.zero_grad()\n            out = model(batch)\n            loss = crit(out, batch.y)\n            loss.backward()\n            optim.step()\n            tot += loss.item() * batch.num_graphs\n        tr_loss = tot / len(train_loader.dataset)\n        losses_train.append(tr_loss)\n        # val\n        model.eval()\n        tot = 0.0\n        preds, gts, seqs = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = crit(logits, batch.y)\n                tot += loss.item() * batch.num_graphs\n                preds.extend(logits.argmax(-1).cpu().tolist())\n                gts.extend(batch.y.cpu().tolist())\n                seqs.extend(batch.seq_raw)\n        val_loss = tot / len(dev_loader.dataset)\n        losses_val.append(val_loss)\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        metrics_val.append({\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa})\n        print(\n            f\"[embed_dim={embed_dim}] Epoch {ep}: train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, acc={acc:.3f}\"\n        )\n    return {\n        \"embed_dim\": embed_dim,\n        \"metrics\": {\"train\": metrics_train, \"val\": metrics_val},\n        \"losses\": {\"train\": losses_train, \"val\": losses_val},\n        \"predictions\": preds,\n        \"ground_truth\": gts,\n        \"timestamps\": time.time(),\n    }\n\n\n# ----------------- hyperparameter sweep ----------------------- #\nfor dim in [16, 32, 64, 128]:\n    result = run_experiment(dim, epochs=5)\n    experiment_data[\"embed_dim\"][\"SPR\"].append(result)\n\n# ----------------------- save results ------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All experiments complete. Data saved to\",\n    os.path.join(working_dir, \"experiment_data.npy\"),\n)\n", "# ----------------- environment & paths -----------------\nimport os, pathlib, random, string, time, numpy as np, torch, warnings\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader, Batch\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\nwarnings.filterwarnings(\"ignore\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- metric helpers ----------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 0.0 if (cwa + swa) == 0 else 2 * cwa * swa / (cwa + swa)\n\n\n# ----------------- data utils --------------------------\ndef try_load_benchmark():\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n    from datasets import load_dataset, DatasetDict\n\n    def _ld(fname):\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\ndef generate_synthetic(nt=200, nd=60, nte=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _make(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            ln = random.randint(4, 15)\n            toks = [random.choice(shapes) + random.choice(colors) for _ in range(ln)]\n            seqs.append(\" \".join(toks))\n            labels.append(int(toks[0][0] == toks[-1][0]))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        train=Dataset.from_dict(_make(nt)),\n        dev=Dataset.from_dict(_make(nd)),\n        test=Dataset.from_dict(_make(nte)),\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(f\"Dataset sizes: train={len(dataset['train'])}, dev={len(dataset['dev'])}\")\n\n# ----------------- vocabulary --------------------------\nvocab = {}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_tok(tok)\nprint(\"Vocab size:\", len(vocab))\n\n\n# ----------------- graph conversion --------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    if len(toks) > 1:\n        src = list(range(len(toks) - 1))\n        dst = list(range(1, len(toks)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    g = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n        seq_raw=seq,\n    )\n    return g\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# ----------------- model -------------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hid=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hid)\n        self.conv2 = SAGEConv(hid, hid)\n        self.lin = nn.Linear(hid, num_classes)\n\n    def forward(self, data: Batch):\n        x = self.embed(data.x.squeeze(-1))\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- experiment tracking -----------------\nexperiment_data = {\"batch_size_tuning\": {}}\n\n# ----------------- hyper-parameter search ---------------\nbatch_sizes = [16, 32, 64, 128]\nepochs = 5\n\nfor bs in batch_sizes:\n    tag = f\"bs_{bs}\"\n    print(f\"\\n=== Training with batch_size={bs} ===\")\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    model = GNNClassifier(len(vocab), num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=bs, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=max(bs, 64))\n\n    for epoch in range(1, epochs + 1):\n        # -------- training ----------\n        model.train()\n        running_loss, n_graphs = 0.0, 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            logits = model(batch)\n            loss = criterion(logits, batch.y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch.num_graphs\n            n_graphs += batch.num_graphs\n        train_loss = running_loss / n_graphs\n        exp[\"losses\"][\"train\"].append(train_loss)\n\n        # -------- validation --------\n        model.eval()\n        val_loss, preds, gts, seqs, n_val = 0.0, [], [], [], 0\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                n_val += batch.num_graphs\n                p = logits.argmax(-1).cpu().tolist()\n                g = batch.y.cpu().tolist()\n                s = batch.seq_raw  # list of raw sequences\n                preds.extend(p)\n                gts.extend(g)\n                seqs.extend(s)\n        val_loss /= n_val\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"val\"].append({\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n        exp[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}/{epochs} | train_loss={train_loss:.4f} | \"\n            f\"val_loss={val_loss:.4f} | acc={acc:.3f} | CWA={cwa:.3f} | \"\n            f\"SWA={swa:.3f} | HWA={hwa:.3f}\"\n        )\n\n    exp[\"predictions\"] = preds\n    exp[\"ground_truth\"] = gts\n    experiment_data[\"batch_size_tuning\"][tag] = exp\n\n# ----------------- save result -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Finished. Saved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, string, time, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ---------------- working dir & device ------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n# --------------------------------------------------------------\n\n\n# ---------------- helper: metrics -----------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\n# --------------------------------------------------------------\n\n\n# --------------- data loading / generation --------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef generate_synthetic(ntr=200, ndev=60, ntst=100):\n    shapes, colors = list(string.ascii_uppercase[:6]), list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            tokens = [\n                random.choice(shapes) + random.choice(colors)\n                for _ in range(random.randint(4, 15))\n            ]\n            seqs.append(\" \".join(tokens))\n            labels.append(int(tokens[0][0] == tokens[-1][0]))\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(ntr)),\n            \"dev\": Dataset.from_dict(_gen(ndev)),\n            \"test\": Dataset.from_dict(_gen(ntst)),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic.\")\n    dataset = generate_synthetic()\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Dataset: {len(dataset['train'])} train, {len(dataset['dev'])} dev, classes={num_classes}\"\n)\n# --------------------------------------------------------------\n\n# ---------------- vocabulary build ----------------------------\nvocab = {}\n\n\ndef add_token(t):\n    if t not in vocab:\n        vocab[t] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n# --------------------------------------------------------------\n\n\n# ---------------- graph construction --------------------------\ndef seq_to_graph(seq, label):\n    toks = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n    if len(toks) > 1:\n        src = list(range(len(toks) - 1))\n        dst = list(range(1, len(toks)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    d = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    d.seq_raw = seq\n    return d\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n# --------------------------------------------------------------\n\n\n# ----------------- model definition ---------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2, dropout=0.0):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.drop = nn.Dropout(dropout)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.drop(F.relu(self.conv1(x, data.edge_index)))\n        x = self.drop(F.relu(self.conv2(x, data.edge_index)))\n        x = global_mean_pool(x, data.batch)\n        x = self.drop(x)\n        return self.lin(x)\n\n\n# --------------------------------------------------------------\n\n# ------------------ experiment container ----------------------\nexperiment_data = {\"dropout_tuning\": {}}  # will fill with each rate\n# --------------------------------------------------------------\n\ndropout_rates = [0.0, 0.1, 0.25, 0.4, 0.5]\nepochs = 5\nfor dr in dropout_rates:\n    print(f\"\\n=== Training with dropout={dr} ===\")\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    model = GNNClassifier(vocab, num_classes=num_classes, dropout=dr).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=64)\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n        exp_rec[\"losses\"][\"train\"].append(train_loss)\n\n        model.eval()\n        dev_loss = 0.0\n        preds, gts, seqs = [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                preds.extend(logits.argmax(-1).cpu().tolist())\n                gts.extend(batch.y.cpu().tolist())\n                seqs.extend(batch.seq_raw)\n        dev_loss /= len(dev_loader.dataset)\n        exp_rec[\"losses\"][\"val\"].append(dev_loss)\n\n        acc = np.mean([p == t for p, t in zip(preds, gts)])\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        exp_rec[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n        exp_rec[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={dev_loss:.4f}, \"\n            f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n        )\n\n    exp_rec[\"predictions\"] = preds\n    exp_rec[\"ground_truth\"] = gts\n    experiment_data[\"dropout_tuning\"][f\"dropout_{dr}\"] = exp_rec\n\n# --------------- save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All experiments complete. Saved to\",\n    os.path.join(working_dir, \"experiment_data.npy\"),\n)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, math, json, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------- helper: metrics ------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\n# --------------------- data loading ---------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef generate_synthetic(num_train=200, num_dev=60, num_test=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            tokens = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq = \" \".join(tokens)\n            label = int(tokens[0][0] == tokens[-1][0])\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(num_train)),\n            \"dev\": Dataset.from_dict(_gen(num_dev)),\n            \"test\": Dataset.from_dict(_gen(num_test)),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Loaded dataset with {len(dataset['train'])} train, {len(dataset['dev'])} dev samples, classes={num_classes}\"\n)\n\n# ------------------ vocabulary build --------------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n\n# -------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# -------------------------- model -----------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------ experiment tracking -----------------------\nexperiment_data = {\"learning_rate\": {\"SPR\": {}}}\n\n# ------------------ hyperparameter sweep ----------------------\nlearning_rates = [3e-4, 1e-3, 3e-3]\nepochs = 5\nbatch_size_train = 32\nbatch_size_eval = 64\n\nfor lr in learning_rates:\n    lr_key = f\"{lr:.0e}\" if lr < 1 else str(lr)\n    print(f\"\\n=== Training with learning rate {lr_key} ===\")\n    model = GNNClassifier(vocab, num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(train_graphs, batch_size=batch_size_train, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=batch_size_eval)\n\n    run_log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # ---- train\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n        run_log[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation\n        model.eval()\n        dev_loss, preds, gts, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                pred = logits.argmax(dim=-1).cpu().tolist()\n                gt = batch.y.cpu().tolist()\n                seqs.extend(batch.seq_raw)\n                preds.extend(pred)\n                gts.extend(gt)\n        dev_loss /= len(dev_loader.dataset)\n        run_log[\"losses\"][\"val\"].append(dev_loss)\n\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        run_log[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n        run_log[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={dev_loss:.4f}, \"\n            f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n        )\n\n    run_log[\"predictions\"] = preds\n    run_log[\"ground_truth\"] = gts\n    experiment_data[\"learning_rate\"][\"SPR\"][lr_key] = run_log\n\n# --------------- save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All runs finished. Data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, math, json, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------- helper: metrics ------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\n# --------------------- data loading ---------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef generate_synthetic(num_train=200, num_dev=60, num_test=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            tokens = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq = \" \".join(tokens)\n            label = int(tokens[0][0] == tokens[-1][0])\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(num_train)),\n            \"dev\": Dataset.from_dict(_gen(num_dev)),\n            \"test\": Dataset.from_dict(_gen(num_test)),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Loaded dataset with {len(dataset['train'])} train, {len(dataset['dev'])} dev samples, classes={num_classes}\"\n)\n\n# ------------------ vocabulary build --------------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n\n# -------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# -------------------------- model -----------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------ experiment tracking -----------------------\nexperiment_data = {\"learning_rate\": {\"SPR\": {}}}\n\n# ------------------ hyperparameter sweep ----------------------\nlearning_rates = [3e-4, 1e-3, 3e-3]\nepochs = 5\nbatch_size_train = 32\nbatch_size_eval = 64\n\nfor lr in learning_rates:\n    lr_key = f\"{lr:.0e}\" if lr < 1 else str(lr)\n    print(f\"\\n=== Training with learning rate {lr_key} ===\")\n    model = GNNClassifier(vocab, num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(train_graphs, batch_size=batch_size_train, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=batch_size_eval)\n\n    run_log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # ---- train\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n        run_log[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation\n        model.eval()\n        dev_loss, preds, gts, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                pred = logits.argmax(dim=-1).cpu().tolist()\n                gt = batch.y.cpu().tolist()\n                seqs.extend(batch.seq_raw)\n                preds.extend(pred)\n                gts.extend(gt)\n        dev_loss /= len(dev_loader.dataset)\n        run_log[\"losses\"][\"val\"].append(dev_loss)\n\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        run_log[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n        run_log[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={dev_loss:.4f}, \"\n            f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n        )\n\n    run_log[\"predictions\"] = preds\n    run_log[\"ground_truth\"] = gts\n    experiment_data[\"learning_rate\"][\"SPR\"][lr_key] = run_log\n\n# --------------- save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All runs finished. Data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, string, time, math, json, warnings\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working directory & device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------- helper: metrics ------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef complexity_adjusted_accuracy(seqs, y_true, y_pred):\n    weights = [count_color_variety(s) + count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\n# --------------------- data loading ---------------------------\ndef try_load_benchmark():\n    from datasets import load_dataset, DatasetDict\n\n    root = pathlib.Path(\"./SPR_BENCH\")\n    if not root.exists():\n        return None\n\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef generate_synthetic(num_train=200, num_dev=60, num_test=100):\n    shapes = list(string.ascii_uppercase[:6])\n    colors = list(string.ascii_lowercase[:6])\n\n    def _gen(n):\n        seqs, labels = [], []\n        for _ in range(n):\n            length = random.randint(4, 15)\n            tokens = [\n                random.choice(shapes) + random.choice(colors) for _ in range(length)\n            ]\n            seq = \" \".join(tokens)\n            label = int(tokens[0][0] == tokens[-1][0])\n            seqs.append(seq)\n            labels.append(label)\n        return {\"sequence\": seqs, \"label\": labels}\n\n    from datasets import Dataset, DatasetDict\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_dict(_gen(num_train)),\n            \"dev\": Dataset.from_dict(_gen(num_dev)),\n            \"test\": Dataset.from_dict(_gen(num_test)),\n        }\n    )\n\n\ndataset = try_load_benchmark()\nif dataset is None:\n    print(\"Benchmark not found; generating synthetic data.\")\n    dataset = generate_synthetic()\n\nnum_classes = len(set(dataset[\"train\"][\"label\"]))\nprint(\n    f\"Loaded dataset with {len(dataset['train'])} train, {len(dataset['dev'])} dev samples, classes={num_classes}\"\n)\n\n# ------------------ vocabulary build --------------------------\nvocab = {}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in dataset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n\n# -------------------- graph construction ----------------------\ndef seq_to_graph(seq, label):\n    tokens = seq.split()\n    node_ids = torch.tensor([vocab[t] for t in tokens], dtype=torch.long)\n    if len(tokens) > 1:\n        src = list(range(len(tokens) - 1))\n        dst = list(range(1, len(tokens)))\n        edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=node_ids.unsqueeze(-1),\n        edge_index=edge_index,\n        y=torch.tensor([label], dtype=torch.long),\n    )\n    data.seq_raw = seq\n    return data\n\n\ntrain_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"train\"][\"sequence\"], dataset[\"train\"][\"label\"])\n]\ndev_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"dev\"][\"sequence\"], dataset[\"dev\"][\"label\"])\n]\ntest_graphs = [\n    seq_to_graph(s, l)\n    for s, l in zip(dataset[\"test\"][\"sequence\"], dataset[\"test\"][\"label\"])\n]\n\n\n# -------------------------- model -----------------------------\nclass GNNClassifier(nn.Module):\n    def __init__(self, vocab, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), embed_dim)\n        self.conv1 = SAGEConv(embed_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = F.relu(self.conv1(x, data.edge_index))\n        x = F.relu(self.conv2(x, data.edge_index))\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ------------------ experiment tracking -----------------------\nexperiment_data = {\"learning_rate\": {\"SPR\": {}}}\n\n# ------------------ hyperparameter sweep ----------------------\nlearning_rates = [3e-4, 1e-3, 3e-3]\nepochs = 5\nbatch_size_train = 32\nbatch_size_eval = 64\n\nfor lr in learning_rates:\n    lr_key = f\"{lr:.0e}\" if lr < 1 else str(lr)\n    print(f\"\\n=== Training with learning rate {lr_key} ===\")\n    model = GNNClassifier(vocab, num_classes=num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(train_graphs, batch_size=batch_size_train, shuffle=True)\n    dev_loader = DataLoader(dev_graphs, batch_size=batch_size_eval)\n\n    run_log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        # ---- train\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n        run_log[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation\n        model.eval()\n        dev_loss, preds, gts, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = criterion(logits, batch.y)\n                dev_loss += loss.item() * batch.num_graphs\n                pred = logits.argmax(dim=-1).cpu().tolist()\n                gt = batch.y.cpu().tolist()\n                seqs.extend(batch.seq_raw)\n                preds.extend(pred)\n                gts.extend(gt)\n        dev_loss /= len(dev_loader.dataset)\n        run_log[\"losses\"][\"val\"].append(dev_loss)\n\n        acc = float(np.mean([p == t for p, t in zip(preds, gts)]))\n        cwa = color_weighted_accuracy(seqs, gts, preds)\n        swa = shape_weighted_accuracy(seqs, gts, preds)\n        caa = complexity_adjusted_accuracy(seqs, gts, preds)\n        run_log[\"metrics\"][\"val\"].append(\n            {\"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"caa\": caa}\n        )\n        run_log[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={dev_loss:.4f}, \"\n            f\"acc={acc:.3f}, CWA={cwa:.3f}, SWA={swa:.3f}, CAA={caa:.3f}\"\n        )\n\n    run_log[\"predictions\"] = preds\n    run_log[\"ground_truth\"] = gts\n    experiment_data[\"learning_rate\"][\"SPR\"][lr_key] = run_log\n\n# --------------- save experiment data -------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"All runs finished. Data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Loaded dataset with 200 train samples, 60 dev samples, num_classes=2',\n'\\n', 'Vocabulary size: 36', '\\n', 'Epoch 1: train_loss=0.5999,\nvalidation_loss=0.5249, acc=0.800, CWA=0.791, SWA=0.805, CAA=0.798', '\\n',\n'Epoch 2: train_loss=0.5210, validation_loss=0.5157, acc=0.800, CWA=0.791,\nSWA=0.805, CAA=0.798', '\\n', 'Epoch 3: train_loss=0.5269,\nvalidation_loss=0.5219, acc=0.800, CWA=0.791, SWA=0.805, CAA=0.798', '\\n',\n'Epoch 4: train_loss=0.5172, validation_loss=0.5161, acc=0.800, CWA=0.791,\nSWA=0.805, CAA=0.798', '\\n', 'Epoch 5: train_loss=0.5068,\nvalidation_loss=0.5167, acc=0.800, CWA=0.791, SWA=0.805, CAA=0.798', '\\n',\n'Training complete. Metrics saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-4/working/experiment_data.npy', '\\n', 'Execution time: 2\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Data: 200 train / 60 dev samples,\nclasses=2', '\\n', 'Vocabulary size:', ' ', '36', '\\n', 'Epoch 01/30\ntrain_loss=0.5876 val_loss=0.5003 val_acc=0.817 CWA=0.831 SWA=0.823 CAA=0.827',\n'\\n', 'Epoch 02/30 train_loss=0.4899 val_loss=0.4897 val_acc=0.817 CWA=0.831\nSWA=0.823 CAA=0.827', '\\n', 'Epoch 03/30 train_loss=0.4770 val_loss=0.4992\nval_acc=0.817 CWA=0.831 SWA=0.823 CAA=0.827', '\\n', 'Epoch 04/30\ntrain_loss=0.4709 val_loss=0.5012 val_acc=0.817 CWA=0.831 SWA=0.823 CAA=0.827',\n'\\n', 'Epoch 05/30 train_loss=0.4583 val_loss=0.5005 val_acc=0.817 CWA=0.831\nSWA=0.823 CAA=0.827', '\\n', 'Epoch 06/30 train_loss=0.4510 val_loss=0.5028\nval_acc=0.817 CWA=0.831 SWA=0.823 CAA=0.827', '\\n', 'Epoch 07/30\ntrain_loss=0.4464 val_loss=0.5064 val_acc=0.817 CWA=0.831 SWA=0.823 CAA=0.827',\n'\\n', 'Early stopping triggered.', '\\n', 'All metrics saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Loaded dataset with 200 train, 60 dev samples, classes=2', '\\n',\n'Vocabulary size: 36', '\\n', '\\n=== Training with learning rate 3e-04 ===',\n'\\n', 'Epoch 1: train_loss=0.6352, val_loss=0.5775, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'Epoch 2: train_loss=0.5833, val_loss=0.5152,\nacc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 3: train_loss=0.5450,\nval_loss=0.4630, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 4:\ntrain_loss=0.5159, val_loss=0.4244, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879',\n'\\n', 'Epoch 5: train_loss=0.5023, val_loss=0.3987, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', '\\n=== Training with learning rate 1e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.6095, val_loss=0.4598, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'Epoch 2: train_loss=0.5035, val_loss=0.3763,\nacc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 3: train_loss=0.5014,\nval_loss=0.3691, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 4:\ntrain_loss=0.4946, val_loss=0.3781, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879',\n'\\n', 'Epoch 5: train_loss=0.4830, val_loss=0.3891, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', '\\n=== Training with learning rate 3e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.5605, val_loss=0.3556, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'Epoch 2: train_loss=0.5234, val_loss=0.3617,\nacc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 3: train_loss=0.4790,\nval_loss=0.4085, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879', '\\n', 'Epoch 4:\ntrain_loss=0.4715, val_loss=0.3680, acc=0.883, CWA=0.885, SWA=0.873, CAA=0.879',\n'\\n', 'Epoch 5: train_loss=0.4482, val_loss=0.3505, acc=0.883, CWA=0.885,\nSWA=0.873, CAA=0.879', '\\n', 'All runs finished. Data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Benchmark not found; generating synthetic\ndata.', '\\n', 'Dataset sizes: train=200, dev=60, classes=2', '\\n', 'Vocab\nsize:', ' ', '36', '\\n', '\\n--- Training with batch_size=16 ---', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 167, in\n<module>\\n    model = GNNClassifier(len(vocab),\nnum_classes=num_classes).to(device)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError:\nGNNClassifier.__init__() got an unexpected keyword argument \\'num_classes\\'\\n',\n'Execution time: 2 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Benchmark not found \u2013 generating synthetic\ndata.', '\\n', 'Train 200, Dev 60, Test 100, num_classes=2', '\\n', 'Vocabulary\nsize:', ' ', '36', '\\n', 'wd=  0.0: epoch 1 | train_loss 0.6783 | val_loss\n0.5289 | acc 0.833', '\\n', 'wd=  0.0: epoch 2 | train_loss 0.4837 | val_loss\n0.4539 | acc 0.833', '\\n', 'wd=  0.0: epoch 3 | train_loss 0.4433 | val_loss\n0.4634 | acc 0.833', '\\n', 'wd=  0.0: epoch 4 | train_loss 0.4407 | val_loss\n0.4723 | acc 0.833', '\\n', 'wd=  0.0: epoch 5 | train_loss 0.4295 | val_loss\n0.4645 | acc 0.833', '\\n', 'wd=1e-05: epoch 1 | train_loss 0.5549 | val_loss\n0.4706 | acc 0.833', '\\n', 'wd=1e-05: epoch 2 | train_loss 0.4503 | val_loss\n0.4597 | acc 0.833', '\\n', 'wd=1e-05: epoch 3 | train_loss 0.4537 | val_loss\n0.4767 | acc 0.833', '\\n', 'wd=1e-05: epoch 4 | train_loss 0.4416 | val_loss\n0.4566 | acc 0.833', '\\n', 'wd=1e-05: epoch 5 | train_loss 0.4194 | val_loss\n0.4545 | acc 0.833', '\\n', 'wd=0.0001: epoch 1 | train_loss 0.6273 | val_loss\n0.4937 | acc 0.833', '\\n', 'wd=0.0001: epoch 2 | train_loss 0.4536 | val_loss\n0.4445 | acc 0.833', '\\n', 'wd=0.0001: epoch 3 | train_loss 0.4534 | val_loss\n0.4621 | acc 0.833', '\\n', 'wd=0.0001: epoch 4 | train_loss 0.4402 | val_loss\n0.4471 | acc 0.833', '\\n', 'wd=0.0001: epoch 5 | train_loss 0.4310 | val_loss\n0.4474 | acc 0.833', '\\n', 'wd=0.001: epoch 1 | train_loss 0.5576 | val_loss\n0.4711 | acc 0.833', '\\n', 'wd=0.001: epoch 2 | train_loss 0.4454 | val_loss\n0.4650 | acc 0.833', '\\n', 'wd=0.001: epoch 3 | train_loss 0.4430 | val_loss\n0.4797 | acc 0.833', '\\n', 'wd=0.001: epoch 4 | train_loss 0.4377 | val_loss\n0.4672 | acc 0.833', '\\n', 'wd=0.001: epoch 5 | train_loss 0.4233 | val_loss\n0.4604 | acc 0.833', '\\n', 'wd= 0.01: epoch 1 | train_loss 0.5912 | val_loss\n0.4895 | acc 0.833', '\\n', 'wd= 0.01: epoch 2 | train_loss 0.4594 | val_loss\n0.4445 | acc 0.833', '\\n', 'wd= 0.01: epoch 3 | train_loss 0.4386 | val_loss\n0.4503 | acc 0.833', '\\n', 'wd= 0.01: epoch 4 | train_loss 0.4352 | val_loss\n0.4462 | acc 0.833', '\\n', 'wd= 0.01: epoch 5 | train_loss 0.4276 | val_loss\n0.4439 | acc 0.833', '\\n', 'All runs finished. Data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Benchmark not found; using synthetic dataset.',\n'\\n', 'Dataset sizes: train=200, dev=60, classes=2', '\\n', 'Vocabulary size:', '\n', '36', '\\n', '\\n===== Training with batch_size=16 =====', '\\n', 'Epoch 1/5:\nvalidation_loss = 0.5867 | acc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n',\n'Epoch 2/5: validation_loss = 0.6105 | acc=0.767 | CWA=0.786 | SWA=0.811 |\nHWA=0.798', '\\n', 'Epoch 3/5: validation_loss = 0.5916 | acc=0.767 | CWA=0.786 |\nSWA=0.811 | HWA=0.798', '\\n', 'Epoch 4/5: validation_loss = 0.6005 | acc=0.767 |\nCWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', 'Epoch 5/5: validation_loss = 0.5806 |\nacc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', '\\n===== Training with\nbatch_size=32 =====', '\\n', 'Epoch 1/5: validation_loss = 0.5554 | acc=0.767 |\nCWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', 'Epoch 2/5: validation_loss = 0.5782 |\nacc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', 'Epoch 3/5:\nvalidation_loss = 0.6261 | acc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n',\n'Epoch 4/5: validation_loss = 0.5836 | acc=0.767 | CWA=0.786 | SWA=0.811 |\nHWA=0.798', '\\n', 'Epoch 5/5: validation_loss = 0.5514 | acc=0.767 | CWA=0.786 |\nSWA=0.811 | HWA=0.798', '\\n', '\\n===== Training with batch_size=64 =====', '\\n',\n'Epoch 1/5: validation_loss = 0.5853 | acc=0.767 | CWA=0.786 | SWA=0.811 |\nHWA=0.798', '\\n', 'Epoch 2/5: validation_loss = 0.5470 | acc=0.767 | CWA=0.786 |\nSWA=0.811 | HWA=0.798', '\\n', 'Epoch 3/5: validation_loss = 0.5569 | acc=0.767 |\nCWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', 'Epoch 4/5: validation_loss = 0.5891 |\nacc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', 'Epoch 5/5:\nvalidation_loss = 0.5938 | acc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n',\n'\\n===== Training with batch_size=128 =====', '\\n', 'Epoch 1/5: validation_loss\n= 0.6181 | acc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', 'Epoch 2/5:\nvalidation_loss = 0.5802 | acc=0.767 | CWA=0.786 | SWA=0.811 | HWA=0.798', '\\n',\n'Epoch 3/5: validation_loss = 0.5542 | acc=0.767 | CWA=0.786 | SWA=0.811 |\nHWA=0.798', '\\n', 'Epoch 4/5: validation_loss = 0.5411 | acc=0.767 | CWA=0.786 |\nSWA=0.811 | HWA=0.798', '\\n', 'Epoch 5/5: validation_loss = 0.5415 | acc=0.767 |\nCWA=0.786 | SWA=0.811 | HWA=0.798', '\\n', '\\nFinished. Results saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Benchmark not found, generating synthetic\ndataset.', '\\n', 'Dataset sizes: train=200, dev=60, test=100', '\\n', 'Vocab\nsize:', ' ', '36', '\\n', '[embed_dim=16] Epoch 1: train_loss=0.5241,\nval_loss=0.5180, acc=0.783', '\\n', '[embed_dim=16] Epoch 2: train_loss=0.4805,\nval_loss=0.5353, acc=0.783', '\\n', '[embed_dim=16] Epoch 3: train_loss=0.4863,\nval_loss=0.5275, acc=0.783', '\\n', '[embed_dim=16] Epoch 4: train_loss=0.4761,\nval_loss=0.5172, acc=0.783', '\\n', '[embed_dim=16] Epoch 5: train_loss=0.4660,\nval_loss=0.5126, acc=0.783', '\\n', '[embed_dim=32] Epoch 1: train_loss=0.5695,\nval_loss=0.5239, acc=0.783', '\\n', '[embed_dim=32] Epoch 2: train_loss=0.4795,\nval_loss=0.5293, acc=0.783', '\\n', '[embed_dim=32] Epoch 3: train_loss=0.4744,\nval_loss=0.5397, acc=0.783', '\\n', '[embed_dim=32] Epoch 4: train_loss=0.4680,\nval_loss=0.5357, acc=0.783', '\\n', '[embed_dim=32] Epoch 5: train_loss=0.4599,\nval_loss=0.5329, acc=0.783', '\\n', '[embed_dim=64] Epoch 1: train_loss=0.6213,\nval_loss=0.5291, acc=0.783', '\\n', '[embed_dim=64] Epoch 2: train_loss=0.4839,\nval_loss=0.5289, acc=0.783', '\\n', '[embed_dim=64] Epoch 3: train_loss=0.4857,\nval_loss=0.5375, acc=0.783', '\\n', '[embed_dim=64] Epoch 4: train_loss=0.4702,\nval_loss=0.5157, acc=0.783', '\\n', '[embed_dim=64] Epoch 5: train_loss=0.4516,\nval_loss=0.5080, acc=0.783', '\\n', '[embed_dim=128] Epoch 1: train_loss=0.5225,\nval_loss=0.5163, acc=0.783', '\\n', '[embed_dim=128] Epoch 2: train_loss=0.4754,\nval_loss=0.5279, acc=0.783', '\\n', '[embed_dim=128] Epoch 3: train_loss=0.4517,\nval_loss=0.5080, acc=0.783', '\\n', '[embed_dim=128] Epoch 4: train_loss=0.4301,\nval_loss=0.5054, acc=0.783', '\\n', '[embed_dim=128] Epoch 5: train_loss=0.4137,\nval_loss=0.5126, acc=0.783', '\\n', 'All experiments complete. Data saved to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Dataset sizes: train=200, dev=60', '\\n', 'Vocab size:', ' ', '36', '\\n',\n'\\n=== Training with batch_size=16 ===', '\\n', 'Epoch 1/5 | train_loss=0.5491 |\nval_loss=0.6056 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch\n2/5 | train_loss=0.5122 | val_loss=0.6139 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', 'Epoch 3/5 | train_loss=0.4824 | val_loss=0.6006 | acc=0.750 |\nCWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch 4/5 | train_loss=0.4667 |\nval_loss=0.6239 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch\n5/5 | train_loss=0.4532 | val_loss=0.6407 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', '\\n=== Training with batch_size=32 ===', '\\n', 'Epoch 1/5 |\ntrain_loss=0.6210 | val_loss=0.5703 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', 'Epoch 2/5 | train_loss=0.5182 | val_loss=0.5769 | acc=0.750 |\nCWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch 3/5 | train_loss=0.5029 |\nval_loss=0.5822 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch\n4/5 | train_loss=0.4976 | val_loss=0.5804 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', 'Epoch 5/5 | train_loss=0.4920 | val_loss=0.5735 | acc=0.750 |\nCWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', '\\n=== Training with batch_size=64\n===', '\\n', 'Epoch 1/5 | train_loss=0.6343 | val_loss=0.5931 | acc=0.750 |\nCWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch 2/5 | train_loss=0.5432 |\nval_loss=0.5667 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch\n3/5 | train_loss=0.5081 | val_loss=0.5873 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', 'Epoch 4/5 | train_loss=0.5022 | val_loss=0.6179 | acc=0.750 |\nCWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch 5/5 | train_loss=0.5058 |\nval_loss=0.6161 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', '\\n===\nTraining with batch_size=128 ===', '\\n', 'Epoch 1/5 | train_loss=0.6538 |\nval_loss=0.6222 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch\n2/5 | train_loss=0.5984 | val_loss=0.5901 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', 'Epoch 3/5 | train_loss=0.5584 | val_loss=0.5714 | acc=0.750 |\nCWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch 4/5 | train_loss=0.5263 |\nval_loss=0.5657 | acc=0.750 | CWA=0.769 | SWA=0.765 | HWA=0.767', '\\n', 'Epoch\n5/5 | train_loss=0.5079 | val_loss=0.5722 | acc=0.750 | CWA=0.769 | SWA=0.765 |\nHWA=0.767', '\\n', 'Finished. Saved results to', ' ', '/home/zxl240011/AI-Scienti\nst-v2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 3\nseconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Benchmark not found; generating\nsynthetic.', '\\n', 'Dataset: 200 train, 60 dev, classes=2', '\\n', 'Vocab size:',\n' ', '36', '\\n', '\\n=== Training with dropout=0.0 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6178, val_loss=0.4957, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841',\n'\\n', 'Epoch 2: train_loss=0.5368, val_loss=0.4631, acc=0.833, CWA=0.846,\nSWA=0.836, CAA=0.841', '\\n', 'Epoch 3: train_loss=0.5165, val_loss=0.4672,\nacc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 4: train_loss=0.5088,\nval_loss=0.4746, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 5:\ntrain_loss=0.5027, val_loss=0.4741, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841',\n'\\n', '\\n=== Training with dropout=0.1 ===', '\\n', 'Epoch 1: train_loss=0.6296,\nval_loss=0.5109, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 2:\ntrain_loss=0.5452, val_loss=0.4691, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841',\n'\\n', 'Epoch 3: train_loss=0.5277, val_loss=0.4641, acc=0.833, CWA=0.846,\nSWA=0.836, CAA=0.841', '\\n', 'Epoch 4: train_loss=0.5200, val_loss=0.4653,\nacc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 5: train_loss=0.5169,\nval_loss=0.4653, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', '\\n===\nTraining with dropout=0.25 ===', '\\n', 'Epoch 1: train_loss=0.6207,\nval_loss=0.5114, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 2:\ntrain_loss=0.5374, val_loss=0.4590, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841',\n'\\n', 'Epoch 3: train_loss=0.5278, val_loss=0.4504, acc=0.833, CWA=0.846,\nSWA=0.836, CAA=0.841', '\\n', 'Epoch 4: train_loss=0.5373, val_loss=0.4566,\nacc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 5: train_loss=0.5209,\nval_loss=0.4667, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', '\\n===\nTraining with dropout=0.4 ===', '\\n', 'Epoch 1: train_loss=0.6341,\nval_loss=0.5212, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 2:\ntrain_loss=0.5549, val_loss=0.4624, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841',\n'\\n', 'Epoch 3: train_loss=0.5401, val_loss=0.4574, acc=0.833, CWA=0.846,\nSWA=0.836, CAA=0.841', '\\n', 'Epoch 4: train_loss=0.5242, val_loss=0.4572,\nacc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 5: train_loss=0.5309,\nval_loss=0.4617, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', '\\n===\nTraining with dropout=0.5 ===', '\\n', 'Epoch 1: train_loss=0.6189,\nval_loss=0.5027, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 2:\ntrain_loss=0.5475, val_loss=0.4647, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841',\n'\\n', 'Epoch 3: train_loss=0.5520, val_loss=0.4597, acc=0.833, CWA=0.846,\nSWA=0.836, CAA=0.841', '\\n', 'Epoch 4: train_loss=0.5474, val_loss=0.4616,\nacc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'Epoch 5: train_loss=0.5372,\nval_loss=0.4714, acc=0.833, CWA=0.846, SWA=0.836, CAA=0.841', '\\n', 'All\nexperiments complete. Saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time: 6\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Loaded dataset with 200 train, 60 dev samples, classes=2', '\\n',\n'Vocabulary size: 36', '\\n', '\\n=== Training with learning rate 3e-04 ===',\n'\\n', 'Epoch 1: train_loss=0.5430, val_loss=0.4539, acc=0.917, CWA=0.916,\nSWA=0.930, CAA=0.923', '\\n', 'Epoch 2: train_loss=0.4773, val_loss=0.3873,\nacc=0.917, CWA=0.916, SWA=0.930, CAA=0.923', '\\n', 'Epoch 3: train_loss=0.4398,\nval_loss=0.3498, acc=0.917, CWA=0.916, SWA=0.930, CAA=0.923', '\\n', 'Epoch 4:\ntrain_loss=0.4234, val_loss=0.3263, acc=0.917, CWA=0.916, SWA=0.930, CAA=0.923',\n'\\n', 'Epoch 5: train_loss=0.4172, val_loss=0.3119, acc=0.917, CWA=0.916,\nSWA=0.930, CAA=0.923', '\\n', '\\n=== Training with learning rate 1e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.6156, val_loss=0.4262, acc=0.917, CWA=0.916,\nSWA=0.930, CAA=0.923', '\\n', 'Epoch 2: train_loss=0.4572, val_loss=0.3058,\nacc=0.917, CWA=0.916, SWA=0.930, CAA=0.923', '\\n', 'Epoch 3: train_loss=0.4256,\nval_loss=0.2931, acc=0.917, CWA=0.916, SWA=0.930, CAA=0.923', '\\n', 'Epoch 4:\ntrain_loss=0.4147, val_loss=0.3017, acc=0.917, CWA=0.916, SWA=0.930, CAA=0.923',\n'\\n', 'Epoch 5: train_loss=0.4082, val_loss=0.3103, acc=0.917, CWA=0.916,\nSWA=0.930, CAA=0.923', '\\n', '\\n=== Training with learning rate 3e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.6171, val_loss=0.3194, acc=0.917, CWA=0.916,\nSWA=0.930, CAA=0.923', '\\n', 'Epoch 2: train_loss=0.4369, val_loss=0.3024,\nacc=0.917, CWA=0.916, SWA=0.930, CAA=0.923', '\\n', 'Epoch 3: train_loss=0.4153,\nval_loss=0.3136, acc=0.917, CWA=0.916, SWA=0.930, CAA=0.923', '\\n', 'Epoch 4:\ntrain_loss=0.3890, val_loss=0.3270, acc=0.917, CWA=0.916, SWA=0.930, CAA=0.923',\n'\\n', 'Epoch 5: train_loss=0.3736, val_loss=0.3173, acc=0.917, CWA=0.916,\nSWA=0.930, CAA=0.923', '\\n', 'All runs finished. Data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Loaded dataset with 200 train, 60 dev samples, classes=2', '\\n',\n'Vocabulary size: 36', '\\n', '\\n=== Training with learning rate 3e-04 ===',\n'\\n', 'Epoch 1: train_loss=0.6697, val_loss=0.5877, acc=0.950, CWA=0.950,\nSWA=0.947, CAA=0.948', '\\n', 'Epoch 2: train_loss=0.5721, val_loss=0.4853,\nacc=0.950, CWA=0.950, SWA=0.947, CAA=0.948', '\\n', 'Epoch 3: train_loss=0.4928,\nval_loss=0.4013, acc=0.950, CWA=0.950, SWA=0.947, CAA=0.948', '\\n', 'Epoch 4:\ntrain_loss=0.4378, val_loss=0.3338, acc=0.950, CWA=0.950, SWA=0.947, CAA=0.948',\n'\\n', 'Epoch 5: train_loss=0.3968, val_loss=0.2896, acc=0.950, CWA=0.950,\nSWA=0.947, CAA=0.948', '\\n', '\\n=== Training with learning rate 1e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.5590, val_loss=0.3064, acc=0.950, CWA=0.950,\nSWA=0.947, CAA=0.948', '\\n', 'Epoch 2: train_loss=0.3717, val_loss=0.2123,\nacc=0.950, CWA=0.950, SWA=0.947, CAA=0.948', '\\n', 'Epoch 3: train_loss=0.3800,\nval_loss=0.2069, acc=0.950, CWA=0.950, SWA=0.947, CAA=0.948', '\\n', 'Epoch 4:\ntrain_loss=0.3674, val_loss=0.2157, acc=0.950, CWA=0.950, SWA=0.947, CAA=0.948',\n'\\n', 'Epoch 5: train_loss=0.3485, val_loss=0.2264, acc=0.950, CWA=0.950,\nSWA=0.947, CAA=0.948', '\\n', '\\n=== Training with learning rate 3e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.5289, val_loss=0.1917, acc=0.950, CWA=0.950,\nSWA=0.947, CAA=0.948', '\\n', 'Epoch 2: train_loss=0.4089, val_loss=0.1964,\nacc=0.950, CWA=0.950, SWA=0.947, CAA=0.948', '\\n', 'Epoch 3: train_loss=0.3566,\nval_loss=0.2793, acc=0.950, CWA=0.950, SWA=0.947, CAA=0.948', '\\n', 'Epoch 4:\ntrain_loss=0.3545, val_loss=0.2530, acc=0.950, CWA=0.950, SWA=0.947, CAA=0.948',\n'\\n', 'Epoch 5: train_loss=0.3279, val_loss=0.2247, acc=0.950, CWA=0.950,\nSWA=0.947, CAA=0.948', '\\n', 'All runs finished. Data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Benchmark not found; generating synthetic data.',\n'\\n', 'Loaded dataset with 200 train, 60 dev samples, classes=2', '\\n',\n'Vocabulary size: 36', '\\n', '\\n=== Training with learning rate 3e-04 ===',\n'\\n', 'Epoch 1: train_loss=0.7124, val_loss=0.6453, acc=0.850, CWA=0.858,\nSWA=0.882, CAA=0.870', '\\n', 'Epoch 2: train_loss=0.6154, val_loss=0.5596,\nacc=0.867, CWA=0.875, SWA=0.899, CAA=0.887', '\\n', 'Epoch 3: train_loss=0.5484,\nval_loss=0.4942, acc=0.867, CWA=0.875, SWA=0.899, CAA=0.887', '\\n', 'Epoch 4:\ntrain_loss=0.4986, val_loss=0.4523, acc=0.867, CWA=0.875, SWA=0.899, CAA=0.887',\n'\\n', 'Epoch 5: train_loss=0.4684, val_loss=0.4254, acc=0.867, CWA=0.875,\nSWA=0.899, CAA=0.887', '\\n', '\\n=== Training with learning rate 1e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.6058, val_loss=0.4582, acc=0.867, CWA=0.875,\nSWA=0.899, CAA=0.887', '\\n', 'Epoch 2: train_loss=0.4664, val_loss=0.3968,\nacc=0.867, CWA=0.875, SWA=0.899, CAA=0.887', '\\n', 'Epoch 3: train_loss=0.4660,\nval_loss=0.4024, acc=0.867, CWA=0.875, SWA=0.899, CAA=0.887', '\\n', 'Epoch 4:\ntrain_loss=0.4598, val_loss=0.3992, acc=0.867, CWA=0.875, SWA=0.899, CAA=0.887',\n'\\n', 'Epoch 5: train_loss=0.4439, val_loss=0.3999, acc=0.867, CWA=0.875,\nSWA=0.899, CAA=0.887', '\\n', '\\n=== Training with learning rate 3e-03 ===',\n'\\n', 'Epoch 1: train_loss=0.5613, val_loss=0.3994, acc=0.867, CWA=0.875,\nSWA=0.899, CAA=0.887', '\\n', 'Epoch 2: train_loss=0.4522, val_loss=0.4222,\nacc=0.867, CWA=0.875, SWA=0.899, CAA=0.887', '\\n', 'Epoch 3: train_loss=0.4322,\nval_loss=0.4248, acc=0.867, CWA=0.875, SWA=0.899, CAA=0.887', '\\n', 'Epoch 4:\ntrain_loss=0.4127, val_loss=0.4257, acc=0.867, CWA=0.875, SWA=0.899, CAA=0.887',\n'\\n', 'Epoch 5: train_loss=0.4045, val_loss=0.4303, acc=0.867, CWA=0.875,\nSWA=0.899, CAA=0.887', '\\n', 'All runs finished. Data saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n30_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["", "The execution output shows a successful run of the training script with early\nstopping triggered after 7 epochs. The model achieved stable validation metrics\n(accuracy, CWA, SWA, CAA) across multiple epochs, indicating no major issues in\ntraining. All metrics were saved successfully, and the execution time was well\nwithin the allowed limit. No bugs were detected.", "", "The error occurred because the constructor of the GNNClassifier class does not\naccept a 'num_classes' argument, but it was provided during instantiation. To\nfix this, the GNNClassifier class constructor should be updated to include a\n'num_classes' parameter and properly assign it to the output layer dimension in\nthe model. Adjust the '__init__' method of the GNNClassifier class as follows:\n```python class GNNClassifier(nn.Module):     def __init__(self, vocab_size,\nembed_dim=32, hid=64, num_classes=2):         super().__init__()\nself.embed = nn.Embedding(vocab_size, embed_dim)         self.conv1 =\nSAGEConv(embed_dim, hid)         self.conv2 = SAGEConv(hid, hid)\nself.lin = nn.Linear(hid, num_classes)  # Use num_classes here ```", "The training script executed successfully without any bugs. The model trained\nwith different weight decay values, and metrics were logged appropriately. The\nexecution time was well within the limit, and the results were saved\nsuccessfully. However, the accuracy remains constant across all hyperparameter\nconfigurations, suggesting the need for further investigation into the model's\nlearning capacity or the synthetic dataset's complexity.", "The execution was successful, and the script ran without any bugs or errors. The\ntraining process for different batch sizes completed as expected, and results\nwere saved successfully. However, the model's performance metrics (accuracy,\nCWA, SWA, and HWA) remained constant across all epochs and batch sizes,\nsuggesting that the model may not be learning effectively. This could be due to\nthe small synthetic dataset size (200 training samples) or insufficient model\ncomplexity. Further investigation and tuning are recommended to improve\nperformance.", "", "The script executed successfully without any errors or bugs. The training\nprocess was conducted for different batch sizes (16, 32, 64, 128), and the\nmodel's performance metrics (accuracy, CWA, SWA, HWA) were logged at each epoch.\nHowever, the results indicate no change in performance across all\nconfigurations, with accuracy consistently at 0.750 and other metrics also\nremaining constant. This suggests that the model might have reached a\nperformance plateau or that the synthetic dataset is too simplistic to reveal\nmeaningful differences. Further investigation into the dataset complexity or\nmodel configuration might be needed to improve performance.", "", "The training script executed successfully without any errors or bugs. The model\nwas trained on synthetic data with three different learning rates (3e-4, 1e-3,\nand 3e-3), and the results were logged for each epoch. The model's performance\nmetrics, including accuracy, Color-Weighted Accuracy (CWA), Shape-Weighted\nAccuracy (SWA), and Complexity-Adjusted Accuracy (CAA), remained constant across\nall epochs and learning rates. While the results indicate no improvement in\nperformance, this is not a bug but rather a potential limitation of the current\nmodel or experiment setup. Further investigation or adjustments may be needed to\nimprove performance.", "", "", ""], "exc_type": [null, null, null, "TypeError", null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, {"args": ["GNNClassifier.__init__() got an unexpected keyword argument 'num_classes'"]}, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 167, "<module>", "model = GNNClassifier(len(vocab), num_classes=num_classes).to(device)"]], null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.5068, "best_value": 0.5068}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.5157, "best_value": 0.5157}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8, "best_value": 0.8}]}, {"metric_name": "validation cwa", "lower_is_better": false, "description": "The class-weighted accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.7909, "best_value": 0.7909}]}, {"metric_name": "validation swa", "lower_is_better": false, "description": "The sample-weighted accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8055, "best_value": 0.8055}]}, {"metric_name": "validation caa", "lower_is_better": false, "description": "The class-average accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.7983, "best_value": 0.7983}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.4464, "best_value": 0.4464}]}, {"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy during training.", "data": [{"dataset_name": "SPR", "final_value": 0.82, "best_value": 0.82}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.5064, "best_value": 0.4897}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8167, "best_value": 0.8167}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.831, "best_value": 0.831}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8227, "best_value": 0.8227}]}, {"metric_name": "validation complexity-adjusted accuracy", "lower_is_better": false, "description": "The complexity-adjusted accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8269, "best_value": 0.8269}]}]}, {"metric_names": [{"metric_name": "final training loss", "lower_is_better": true, "description": "The final loss value achieved during training.", "data": [{"dataset_name": "SPR", "final_value": 0.4482, "best_value": 0.4482}]}, {"metric_name": "final validation loss", "lower_is_better": true, "description": "The final loss value achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.3505, "best_value": 0.3505}]}, {"metric_name": "final validation accuracy", "lower_is_better": false, "description": "The final accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8833, "best_value": 0.8833}]}, {"metric_name": "final validation color weighted accuracy", "lower_is_better": false, "description": "The final color-weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8854, "best_value": 0.8854}]}, {"metric_name": "final validation shape weighted accuracy", "lower_is_better": false, "description": "The final shape-weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8727, "best_value": 0.8727}]}, {"metric_name": "final validation complexity adjusted accuracy", "lower_is_better": false, "description": "The final complexity-adjusted accuracy achieved during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.8792, "best_value": 0.8792}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training.", "data": [{"dataset_name": "SPR", "final_value": 0.4194, "best_value": 0.4194}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation set.", "data": [{"dataset_name": "SPR", "final_value": 0.4439, "best_value": 0.4439}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation set.", "data": [{"dataset_name": "SPR", "final_value": 0.8333, "best_value": 0.8333}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "Measures the color-weighted accuracy on the validation set.", "data": [{"dataset_name": "SPR", "final_value": 0.8201, "best_value": 0.8201}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Measures the shape-weighted accuracy on the validation set.", "data": [{"dataset_name": "SPR", "final_value": 0.8293, "best_value": 0.8293}]}, {"metric_name": "validation complexity-adjusted accuracy", "lower_is_better": false, "description": "Measures the complexity-adjusted accuracy on the validation set.", "data": [{"dataset_name": "SPR", "final_value": 0.8247, "best_value": 0.8247}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss value during training, lower values indicate better performance.", "data": [{"dataset_name": "training set", "final_value": 0.445357, "best_value": 0.445357}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation set, lower values indicate better performance.", "data": [{"dataset_name": "validation set", "final_value": 0.541456, "best_value": 0.541456}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "validation set", "final_value": 0.766667, "best_value": 0.766667}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "validation set", "final_value": 0.786232, "best_value": 0.786232}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "validation set", "final_value": 0.810526, "best_value": 0.810526}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "Harmonic-weighted accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "validation set", "final_value": 0.798194, "best_value": 0.798194}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss calculated on the training dataset at the end of training.", "data": [{"dataset_name": "SPR, embed_dim=16", "final_value": 0.466, "best_value": 0.466}, {"dataset_name": "SPR, embed_dim=32", "final_value": 0.4599, "best_value": 0.4599}, {"dataset_name": "SPR, embed_dim=64", "final_value": 0.4516, "best_value": 0.4516}, {"dataset_name": "SPR, embed_dim=128", "final_value": 0.4137, "best_value": 0.4137}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss calculated on the validation dataset at the end of training.", "data": [{"dataset_name": "SPR, embed_dim=16", "final_value": 0.5126, "best_value": 0.5126}, {"dataset_name": "SPR, embed_dim=32", "final_value": 0.5329, "best_value": 0.5329}, {"dataset_name": "SPR, embed_dim=64", "final_value": 0.508, "best_value": 0.508}, {"dataset_name": "SPR, embed_dim=128", "final_value": 0.5126, "best_value": 0.5126}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy calculated on the validation dataset at the end of training.", "data": [{"dataset_name": "SPR, embed_dim=16", "final_value": 0.7833, "best_value": 0.7833}, {"dataset_name": "SPR, embed_dim=32", "final_value": 0.7833, "best_value": 0.7833}, {"dataset_name": "SPR, embed_dim=64", "final_value": 0.7833, "best_value": 0.7833}, {"dataset_name": "SPR, embed_dim=128", "final_value": 0.7833, "best_value": 0.7833}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy calculated on the validation dataset at the end of training.", "data": [{"dataset_name": "SPR, embed_dim=16", "final_value": 0.7664, "best_value": 0.7664}, {"dataset_name": "SPR, embed_dim=32", "final_value": 0.7664, "best_value": 0.7664}, {"dataset_name": "SPR, embed_dim=64", "final_value": 0.7664, "best_value": 0.7664}, {"dataset_name": "SPR, embed_dim=128", "final_value": 0.7664, "best_value": 0.7664}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy calculated on the validation dataset at the end of training.", "data": [{"dataset_name": "SPR, embed_dim=16", "final_value": 0.7794, "best_value": 0.7794}, {"dataset_name": "SPR, embed_dim=32", "final_value": 0.7794, "best_value": 0.7794}, {"dataset_name": "SPR, embed_dim=64", "final_value": 0.7794, "best_value": 0.7794}, {"dataset_name": "SPR, embed_dim=128", "final_value": 0.7794, "best_value": 0.7794}]}, {"metric_name": "validation complexity adjusted accuracy", "lower_is_better": false, "description": "Complexity-adjusted accuracy calculated on the validation dataset at the end of training.", "data": [{"dataset_name": "SPR, embed_dim=16", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "SPR, embed_dim=32", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "SPR, embed_dim=64", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "SPR, embed_dim=128", "final_value": 0.7729, "best_value": 0.7729}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed on the training dataset.", "data": [{"dataset_name": "bs_16", "final_value": 0.4532, "best_value": 0.4532}, {"dataset_name": "bs_32", "final_value": 0.492, "best_value": 0.492}, {"dataset_name": "bs_64", "final_value": 0.5058, "best_value": 0.5058}, {"dataset_name": "bs_128", "final_value": 0.5079, "best_value": 0.5079}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed on the validation dataset.", "data": [{"dataset_name": "bs_16", "final_value": 0.6407, "best_value": 0.6407}, {"dataset_name": "bs_32", "final_value": 0.5735, "best_value": 0.5735}, {"dataset_name": "bs_64", "final_value": 0.6161, "best_value": 0.6161}, {"dataset_name": "bs_128", "final_value": 0.5722, "best_value": 0.5722}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy computed on the validation dataset.", "data": [{"dataset_name": "bs_16", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "bs_32", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "bs_64", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "bs_128", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "bs_16", "final_value": 0.7692, "best_value": 0.7692}, {"dataset_name": "bs_32", "final_value": 0.7692, "best_value": 0.7692}, {"dataset_name": "bs_64", "final_value": 0.7692, "best_value": 0.7692}, {"dataset_name": "bs_128", "final_value": 0.7692, "best_value": 0.7692}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "bs_16", "final_value": 0.7653, "best_value": 0.7653}, {"dataset_name": "bs_32", "final_value": 0.7653, "best_value": 0.7653}, {"dataset_name": "bs_64", "final_value": 0.7653, "best_value": 0.7653}, {"dataset_name": "bs_128", "final_value": 0.7653, "best_value": 0.7653}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "bs_16", "final_value": 0.7673, "best_value": 0.7673}, {"dataset_name": "bs_32", "final_value": 0.7673, "best_value": 0.7673}, {"dataset_name": "bs_64", "final_value": 0.7673, "best_value": 0.7673}, {"dataset_name": "bs_128", "final_value": 0.7673, "best_value": 0.7673}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training indicating the model's performance on the training data.", "data": [{"dataset_name": "dropout_0.0", "final_value": 0.5027, "best_value": 0.5027}, {"dataset_name": "dropout_0.1", "final_value": 0.5169, "best_value": 0.5169}, {"dataset_name": "dropout_0.25", "final_value": 0.5209, "best_value": 0.5209}, {"dataset_name": "dropout_0.4", "final_value": 0.5242, "best_value": 0.5242}, {"dataset_name": "dropout_0.5", "final_value": 0.5372, "best_value": 0.5372}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation indicating the model's performance on unseen validation data.", "data": [{"dataset_name": "dropout_0.0", "final_value": 0.4631, "best_value": 0.4631}, {"dataset_name": "dropout_0.1", "final_value": 0.4641, "best_value": 0.4641}, {"dataset_name": "dropout_0.25", "final_value": 0.4504, "best_value": 0.4504}, {"dataset_name": "dropout_0.4", "final_value": 0.4572, "best_value": 0.4572}, {"dataset_name": "dropout_0.5", "final_value": 0.4597, "best_value": 0.4597}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "dropout_0.0", "final_value": 0.8333, "best_value": 0.8333}, {"dataset_name": "dropout_0.1", "final_value": 0.8333, "best_value": 0.8333}, {"dataset_name": "dropout_0.25", "final_value": 0.8333, "best_value": 0.8333}, {"dataset_name": "dropout_0.4", "final_value": 0.8333, "best_value": 0.8333}, {"dataset_name": "dropout_0.5", "final_value": 0.8333, "best_value": 0.8333}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "dropout_0.0", "final_value": 0.8456, "best_value": 0.8456}, {"dataset_name": "dropout_0.1", "final_value": 0.8456, "best_value": 0.8456}, {"dataset_name": "dropout_0.25", "final_value": 0.8456, "best_value": 0.8456}, {"dataset_name": "dropout_0.4", "final_value": 0.8456, "best_value": 0.8456}, {"dataset_name": "dropout_0.5", "final_value": 0.8456, "best_value": 0.8456}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "dropout_0.0", "final_value": 0.8357, "best_value": 0.8357}, {"dataset_name": "dropout_0.1", "final_value": 0.8357, "best_value": 0.8357}, {"dataset_name": "dropout_0.25", "final_value": 0.8357, "best_value": 0.8357}, {"dataset_name": "dropout_0.4", "final_value": 0.8357, "best_value": 0.8357}, {"dataset_name": "dropout_0.5", "final_value": 0.8357, "best_value": 0.8357}]}, {"metric_name": "validation complexity-adjusted accuracy", "lower_is_better": false, "description": "The complexity-adjusted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "dropout_0.0", "final_value": 0.8407, "best_value": 0.8407}, {"dataset_name": "dropout_0.1", "final_value": 0.8407, "best_value": 0.8407}, {"dataset_name": "dropout_0.25", "final_value": 0.8407, "best_value": 0.8407}, {"dataset_name": "dropout_0.4", "final_value": 0.8407, "best_value": 0.8407}, {"dataset_name": "dropout_0.5", "final_value": 0.8407, "best_value": 0.8407}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.3736, "best_value": 0.3736}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.3103, "best_value": 0.3103}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9167, "best_value": 0.9167}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9158, "best_value": 0.9158}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9304, "best_value": 0.9304}]}, {"metric_name": "validation complexity adjusted accuracy", "lower_is_better": false, "description": "The complexity-adjusted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9231, "best_value": 0.9231}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.3279, "best_value": 0.3279}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.2247, "best_value": 0.2247}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.95, "best_value": 0.95}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9498, "best_value": 0.9498}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.947, "best_value": 0.947}]}, {"metric_name": "validation complexity adjusted accuracy", "lower_is_better": false, "description": "The complexity adjusted accuracy calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9485, "best_value": 0.9485}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final training loss value after training.", "data": [{"dataset_name": "SPR", "final_value": 0.4045, "best_value": 0.4045}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final validation loss value after training.", "data": [{"dataset_name": "SPR", "final_value": 0.3999, "best_value": 0.3999}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The final validation accuracy value after training.", "data": [{"dataset_name": "SPR", "final_value": 0.8667, "best_value": 0.8667}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The final validation color weighted accuracy value after training.", "data": [{"dataset_name": "SPR", "final_value": 0.875, "best_value": 0.875}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The final validation shape weighted accuracy value after training.", "data": [{"dataset_name": "SPR", "final_value": 0.899, "best_value": 0.899}]}, {"metric_name": "validation complexity adjusted accuracy", "lower_is_better": false, "description": "The final validation complexity adjusted accuracy value after training.", "data": [{"dataset_name": "SPR", "final_value": 0.887, "best_value": 0.887}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_weighted_accuracies.png", "../../logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_weighted_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_accuracy_vs_caa.png", "../../logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_cwa_swa_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_loss_curves_weight_decay.png", "../../logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_val_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_final_acc_vs_weight_decay.png"], ["../../logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_hwa_curves.png", "../../logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_best_hwa_by_bs.png"], ["../../logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_final_metrics.png"], ["../../logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_16.png", "../../logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_32.png", "../../logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_64.png", "../../logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_128.png", "../../logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_hwa_batchsize_comparison.png"], ["../../logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_loss_curves_all_dropout.png", "../../logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_val_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_final_val_accuracy_bar.png"], ["../../logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_accuracy_vs_caa.png", "../../logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_cwa_swa_curves.png"], ["../../logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_accuracy_vs_caa.png", "../../logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_cwa_swa_curves.png"], ["../../logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_accuracy_vs_caa.png", "../../logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_cwa_swa_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_3827268c9ed5459a924c6984b95d5037/SPR_aggregated_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_3827268c9ed5459a924c6984b95d5037/SPR_aggregated_accuracy_caa.png", "../../logs/0-run/experiment_results/seed_aggregation_3827268c9ed5459a924c6984b95d5037/SPR_aggregated_cwa_swa.png"]], "plot_paths": [["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_loss_curve.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_val_accuracy.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_weighted_accuracies.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_confusion_matrix.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_weighted_accuracy_curves.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_accuracy_vs_caa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_cwa_swa_curves.png"], [], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_loss_curves_weight_decay.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_val_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_final_acc_vs_weight_decay.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_hwa_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_best_hwa_by_bs.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_final_metrics.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_16.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_32.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_64.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_128.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_hwa_batchsize_comparison.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_loss_curves_all_dropout.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_val_accuracy_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_final_val_accuracy_bar.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_accuracy_vs_caa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_cwa_swa_curves.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_accuracy_vs_caa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_cwa_swa_curves.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_accuracy_vs_caa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_cwa_swa_curves.png"], ["experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_3827268c9ed5459a924c6984b95d5037/SPR_aggregated_loss_curves.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_3827268c9ed5459a924c6984b95d5037/SPR_aggregated_accuracy_caa.png", "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_3827268c9ed5459a924c6984b95d5037/SPR_aggregated_cwa_swa.png"]], "plot_analyses": [[{"analysis": "The training loss decreases consistently over the epochs, indicating that the model is learning from the training data. However, the validation loss remains relatively flat after an initial drop, suggesting that the model is not improving on unseen data beyond the first epoch. This could indicate overfitting or limited generalization capacity of the current model.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_loss_curve.png"}, {"analysis": "Validation accuracy remains constant at approximately 0.80 across all epochs. This plateau suggests that the model's capacity to generalize to the validation set has not improved with additional training, which could be due to model underfitting or limitations in the training process.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_val_accuracy.png"}, {"analysis": "The weighted accuracy metrics (CWA, SWA, and CAA) remain constant across all epochs. This indicates that the model's performance on the weighted metrics does not improve with training, further supporting the observation that the model's generalization ability is not increasing.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_weighted_accuracies.png"}, {"analysis": "The confusion matrix shows that the model predicts only one class (True 0) for all instances in the development set. This indicates a severe class imbalance issue or a model bias toward one class, resulting in poor performance on the other class (True 1).", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d24058baf6d74e128ac0add4385488fc_proc_1437074/SPR_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning from the training data. However, the validation loss decreases initially but starts to increase slightly after the third epoch, suggesting potential overfitting to the training data. This implies that the model's generalization to unseen data might be limited, and regularization techniques or early stopping might be required to address this issue.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_loss_curves.png"}, {"analysis": "The training accuracy remains constant at a high value, while the validation accuracy also stays constant but at a slightly lower value. This lack of improvement in validation accuracy despite the decrease in training loss could indicate that the model is not effectively learning features that generalize well to the validation set. This could be due to a mismatch between the training and validation data distributions or insufficient model capacity to capture the complexity of the task.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_accuracy_curves.png"}, {"analysis": "The weighted accuracy metrics (CWA, SWA, and CAA) remain constant throughout the epochs, indicating that the model's performance on these metrics does not improve with training. This stagnation suggests that the current hyperparameter settings or model configuration may not be optimal for improving these specific metrics. It may be necessary to explore different hyperparameter values or revisit the model's design to enhance performance on these weighted accuracy metrics.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_6ddbdf35c7d04407bf95963adfc05594_proc_1441296/SPR_weighted_accuracy_curves.png"}], [{"analysis": "The training loss decreases consistently for all learning rates, with the fastest decrease observed for lr=3e-03, followed by lr=1e-03 and lr=3e-04. This indicates that a higher learning rate accelerates convergence during training. However, in validation loss, the lr=3e-03 curve exhibits significant fluctuations, suggesting potential overfitting or instability. The lr=1e-03 curve stabilizes better, while lr=3e-04 shows a steady decrease but at a slower rate, indicating underfitting or slow convergence.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_loss_curves.png"}, {"analysis": "The accuracy and complexity-adjusted accuracy remain constant across epochs for all learning rates. This suggests that the model's performance is not improving with training, potentially due to the limitations in the model's ability to generalize or suboptimal hyperparameter settings. Additionally, the choice of metrics might not be sensitive enough to reflect small performance changes.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_accuracy_vs_caa.png"}, {"analysis": "Both color-weighted accuracy and shape-weighted accuracy remain constant across epochs for all learning rates. This indicates that the model is not learning to leverage the structural information represented by these metrics. The lack of variation could also suggest that the dataset might not be challenging enough, or the model's architecture and hyperparameters are not well-suited for these metrics.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_869bad450d6e4d8fb3f6149546f7258c_proc_1441297/SPR_cwa_swa_curves.png"}], [], [{"analysis": "The plot shows the training and validation loss trends for different learning rates over five epochs. The training loss (dashed lines) generally decreases steadily across all learning rates, indicating that the model is learning effectively during training. However, validation loss (solid lines) exhibits a more complex behavior. For some learning rates, such as 0.0001, the validation loss decreases initially and then stabilizes. For higher learning rates like 0.01, the validation loss remains relatively flat, suggesting potential underfitting or insufficient training time. The learning rate of 1e-05 seems to strike a balance between reducing training and validation loss, indicating it could be a promising choice for further tuning.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_loss_curves_weight_decay.png"}, {"analysis": "This plot illustrates the validation accuracy trends for different weight decay values over five epochs. The validation accuracy remains constant across all weight decay values, suggesting that weight decay does not significantly affect the model's performance in this setup. This could indicate that the regularization effect of weight decay is either unnecessary or not impactful for this specific task and model configuration.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_val_accuracy_curves.png"}, {"analysis": "No meaningful data or trends are present in this image. It appears to be blank or irrelevant to the analysis.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1f0abffd5614434fabd72cbb9ce5d4fc_proc_1441299/SPR_final_acc_vs_weight_decay.png"}], [{"analysis": "The first plot illustrates the training and validation cross-entropy losses across different batch sizes (16, 32, 64, 128) over five epochs. For smaller batch sizes (16, 32), the training loss decreases more consistently, while validation loss fluctuates significantly, indicating potential overfitting. Larger batch sizes (64, 128) show stable validation loss but slower training loss reduction, suggesting reduced overfitting but slower convergence. The choice of batch size seems to impact the loss dynamics differently for training and validation.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_loss_curves.png"}, {"analysis": "The second plot shows validation accuracy across epochs for different batch sizes. Interestingly, the validation accuracy remains constant at approximately 0.77 irrespective of the batch size or epoch. This indicates that the changes in batch size and epochs did not significantly impact the validation accuracy, suggesting the model might have reached a performance plateau or is insensitive to these hyperparameters.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_accuracy_curves.png"}, {"analysis": "The third plot presents the validation harmonic weighted accuracy (HWA) across epochs for different batch sizes. Similar to validation accuracy, HWA remains constant at around 0.80 across all epochs and batch sizes. This suggests that the harmonic weighted accuracy metric is also unaffected by the hyperparameter variations in this specific setup.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_hwa_curves.png"}, {"analysis": "The fourth plot compares the best harmonic weighted accuracy (HWA) achieved for each batch size. All batch sizes (16, 32, 64, 128) achieve nearly identical HWA values of approximately 0.80. This further reinforces the observation that the batch size does not significantly influence the model's performance in terms of HWA, indicating that other factors, such as the model architecture or dataset properties, might be the limiting factors.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_0464bd2a77234c55ae288f784d2e3e7b_proc_1441298/synthetic_best_hwa_by_bs.png"}], [{"analysis": "The training and validation loss curves indicate that for smaller embedding dimensions (e.g., 16 and 32), the model converges faster and achieves lower loss values compared to larger dimensions (e.g., 64 and 128). However, for larger dimensions, the validation loss does not show significant improvement over epochs, suggesting potential overfitting or insufficient regularization for larger models.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_loss_curves.png"}, {"analysis": "The validation accuracy remains constant across epochs for all embedding dimensions, which could indicate a bottleneck in the model's capacity to learn more complex patterns or a limitation in the dataset's variability. This flat trend suggests that changing the embedding dimension alone does not significantly influence the overall accuracy.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_accuracy_curves.png"}, {"analysis": "The final-epoch validation metrics show consistent performance across all embedding dimensions, with minimal variation in ACC, CWA, SWA, and CAA. This consistency implies that the model's performance is relatively insensitive to the choice of embedding dimension within the tested range. However, the scores being similar across metrics might indicate the model's inability to exploit the nuances in the SPR task effectively.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_06aacc1f20dc4b13a576d84011c75d64_proc_1441296/SPR_final_metrics.png"}], [{"analysis": "The plot shows the loss curves for batch size 16. The training loss consistently decreases, indicating that the model is learning from the data. However, the validation loss increases over time, which suggests overfitting. The model performs well on the training data but struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_16.png"}, {"analysis": "The plot shows the loss curves for batch size 32. The training loss decreases significantly, and the validation loss initially increases slightly but stabilizes. This suggests better generalization compared to the smaller batch size, though there is still a slight gap between training and validation loss.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_32.png"}, {"analysis": "The plot shows the loss curves for batch size 64. The training loss decreases rapidly, and the validation loss initially decreases but increases again after the third epoch. This indicates potential overfitting as the model performs well on the training data but less consistently on validation data.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_64.png"}, {"analysis": "The plot shows the loss curves for batch size 128. Both training and validation losses decrease steadily, with a smaller gap between the two compared to other batch sizes. This suggests that a larger batch size may help improve generalization and reduce overfitting.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_loss_bs_128.png"}, {"analysis": "The plot compares Harmonic Weighted Accuracy (HWA) across different batch sizes. The HWA remains constant across all epochs and batch sizes, indicating that the batch size does not significantly impact this metric. This may suggest that other factors, such as model architecture or learning rate, could have a more substantial effect on HWA.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_2de90dfaad614d2d829eeecc01f9741f_proc_1441297/synthetic_hwa_batchsize_comparison.png"}], [{"analysis": "The first plot shows the training and validation loss curves for different dropout rates. The training loss decreases steadily with epochs for all dropout rates, indicating that the model is learning effectively. However, the validation loss behaves differently depending on the dropout rate. Lower dropout rates (e.g., 0.0) result in a slight increase in validation loss after an initial decrease, suggesting potential overfitting. Higher dropout rates (e.g., 0.5) lead to more stable validation loss, but the overall loss remains higher. This indicates a trade-off between overfitting and underfitting based on the dropout rate.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_loss_curves_all_dropout.png"}, {"analysis": "The second plot illustrates the validation accuracy across epochs for different dropout rates. Interestingly, the validation accuracy remains constant at 0.83 for all dropout rates and across all epochs, suggesting that the dropout rate does not significantly impact validation accuracy in this experiment. This could indicate that the model's performance is robust to variations in dropout rate or that the metric used does not capture subtle differences.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_val_accuracy_curves.png"}, {"analysis": "The third plot summarizes the final validation accuracy at the last epoch for each dropout rate. All dropout rates achieve the same final validation accuracy of 0.83, reinforcing the observation that dropout rate does not significantly influence the model's validation performance in this setting. This suggests that other factors, such as model architecture or dataset characteristics, may play a more critical role in determining performance.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_64d09b17141e4192b7ed37352aae412f_proc_1441299/synthetic_final_val_accuracy_bar.png"}], [{"analysis": "The train loss decreases steadily for all learning rates, with the fastest convergence observed for the highest learning rate (lr=3e-03). However, the validation loss shows a different trend: while lr=3e-03 initially achieves the lowest validation loss, it increases slightly after epoch 2, suggesting possible overfitting. The learning rate lr=1e-03 maintains a balance between training and validation loss, with a steady decrease in both metrics. The lowest learning rate (lr=3e-04) shows the slowest convergence, but it maintains a consistent decrease in validation loss, indicating stable learning.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_loss_curves.png"}, {"analysis": "Both the basic accuracy and complexity-adjusted accuracy remain constant across all epochs and learning rates. This plateau suggests that the model's performance on these metrics is not sensitive to the chosen learning rates or the number of epochs within the current experimental setup. It might indicate a bottleneck in the model's ability to learn beyond a certain point, possibly due to limitations in the architecture or features.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_accuracy_vs_caa.png"}, {"analysis": "Color-weighted and shape-weighted accuracies also remain constant throughout the training process for all learning rates. This lack of variation implies that the model's ability to capture the underlying rules related to color and shape is not improving with training. This could point to a need for architectural changes or additional regularization techniques to better capture these relationships.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/SPR_cwa_swa_curves.png"}], [{"analysis": "The loss curves show a clear dependence on the learning rate. For the training loss (left plot), the learning rate of 3e-03 achieves the lowest loss by the end of training, indicating faster convergence. The learning rate of 1e-03 also performs well, while 3e-04 shows slower convergence, as evidenced by the higher loss values throughout training. For the validation loss (right plot), the learning rate of 3e-03 initially decreases but shows fluctuations, which could indicate overfitting or instability. The learning rate of 1e-03 appears more stable, with a steady decrease in validation loss, suggesting it might be a better choice for balancing convergence and generalization.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_loss_curves.png"}, {"analysis": "The accuracy plots indicate that the learning rate of 3e-03 achieves a constant accuracy of approximately 0.95 across epochs for both basic and complexity-aware accuracies. This suggests that the model reaches a performance plateau early in training and does not improve further. The other learning rates do not show any significant improvement in accuracy either, which might indicate a limitation in the current model architecture or insufficiently tuned hyperparameters.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_accuracy_vs_caa.png"}, {"analysis": "The color-weighted and shape-weighted accuracy plots display similar trends to the basic and complexity-aware accuracy plots, with the learning rate of 3e-03 maintaining a steady accuracy of around 0.95. This suggests that the model's ability to capture color and shape-related complexities in the data is not improving across epochs, possibly due to the same reasons as mentioned for the basic accuracy plots.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/SPR_cwa_swa_curves.png"}], [{"analysis": "The train and validation loss plots indicate that the learning rate has a significant impact on the model's convergence. The learning rate of 3e-03 achieves the lowest loss values for both training and validation, suggesting faster and more effective convergence compared to lower learning rates. However, the validation loss for 3e-03 stabilizes early, indicating potential overfitting or a plateau in learning.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_loss_curves.png"}, {"analysis": "The accuracy plots show that the highest learning rate (3e-03) achieves the best performance, with the model quickly reaching and maintaining a high accuracy score. The lower learning rates (3e-04 and 1e-03) lag behind, with slower improvements and lower final accuracies.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_accuracy_vs_caa.png"}, {"analysis": "The color-weighted and shape-weighted accuracy plots mirror the trend observed in the basic and complexity-adjusted accuracy plots. The learning rate of 3e-03 consistently achieves the highest scores, indicating that this learning rate allows the model to effectively capture the relationships in the data that contribute to these specialized metrics.", "plot_path": "experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/SPR_cwa_swa_curves.png"}], []], "vlm_feedback_summary": ["The plots collectively suggest that the model is not generalizing well to unseen\ndata and is biased toward predicting one class. There is no improvement in\nvalidation performance or weighted accuracy metrics, and the confusion matrix\nhighlights a significant issue with class imbalance or model bias. Further\ninvestigation and adjustments to the model architecture, training process, or\ndata preprocessing are necessary to address these limitations.", "The provided plots reveal potential overfitting in the training process, as\nevidenced by the divergence between training and validation loss. Furthermore,\nthe lack of improvement in validation accuracy and weighted accuracy metrics\nsuggests that the current model configuration or hyperparameters may not be\nwell-suited for the task. Regularization techniques, hyperparameter tuning, or\narchitectural adjustments should be considered to address these issues.", "The plots highlight issues with model generalization and metric sensitivity.\nWhile the training loss decreases for higher learning rates, validation loss\ninstability and flat accuracy metrics suggest the need for further tuning of the\nmodel and hyperparameters. The metrics used may also require reconsideration to\nbetter reflect performance improvements.", "[]", "The provided plots offer insights into the impact of hyperparameter tuning on\nthe model's performance. The loss trends suggest that a learning rate of 1e-05\nmight be optimal for balancing training and validation loss. However, weight\ndecay seems to have no significant impact on validation accuracy, indicating it\nmight not require further tuning at this stage. The third image does not contain\nany relevant information for analysis.", "The plots indicate that batch size variations have minimal impact on the\nperformance metrics (validation accuracy and harmonic weighted accuracy).\nTraining dynamics show some sensitivity to batch size, with smaller batch sizes\nleading to more consistent training loss reduction but more volatile validation\nloss. The model appears to have reached a performance plateau, suggesting that\nfurther improvements may require architectural changes or better feature\nengineering.", "The plots reveal that the embedding dimension does not significantly impact the\nmodel's performance, as indicated by consistent validation metrics and flat\naccuracy trends. The loss curves suggest potential overfitting for larger\ndimensions, while the lack of accuracy improvement highlights the need for\nfurther architectural or hyperparameter refinements.", "The analysis of the provided plots highlights the impact of batch size on\ntraining and validation loss, with larger batch sizes showing better\ngeneralization and reduced overfitting. However, the Harmonic Weighted Accuracy\n(HWA) remains unchanged across all batch sizes, suggesting that batch size does\nnot influence this metric. Further exploration of other hyperparameters or\narchitectural changes may be needed to improve HWA.", "The plots indicate that while dropout rate affects the loss curves, it does not\nsignificantly influence validation accuracy, which remains constant across all\ndropout rates. This suggests that the model's performance is robust to dropout\nvariations, and further tuning or architectural changes may be needed to achieve\nbetter results.", "The plots reveal that while the loss metrics show some variation across learning\nrates, the accuracy metrics remain constant, indicating potential limitations in\nthe model's ability to improve performance. Further investigation into\narchitectural improvements or feature engineering may be necessary to address\nthese challenges.", "The plots reveal that while the loss decreases with training, accuracy metrics\nremain constant at around 0.95 for all learning rates. This suggests that the\ncurrent model is not effectively learning to improve performance beyond a\ncertain level, potentially due to architectural limitations or insufficient\nhyperparameter tuning.", "The plots reveal that a learning rate of 3e-03 leads to the best overall\nperformance across all metrics, including train/validation loss, basic accuracy,\ncomplexity-adjusted accuracy, color-weighted accuracy, and shape-weighted\naccuracy. This suggests that this learning rate is optimal for the current model\nconfiguration.", "[]"], "exec_time": [2.885544538497925, 4.393215656280518, 3.3532469272613525, 2.486079692840576, 5.464195966720581, 5.001561403274536, 3.15946888923645, 3.4910333156585693, 6.192312479019165, 3.9658925533294678, 3.5516257286071777, 3.8570556640625, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["\"\""], ["[]"], ["\"\""], [], ["[\"SPR\"]"], ["[\"Synthetic Shapes-Colors\"]"], ["\"\""], ["['batch_size_128']"], ["[\"Synthetic Dataset\"]"], ["\"\""], ["\"\""], ["[\"SPR\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# required working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------ load experiment data ------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# quick guard\nif \"SPR\" not in experiment_data:\n    print(\"No SPR experiment data found, exiting.\")\n    exit()\n\nspr = experiment_data[\"SPR\"]\ntrain_losses = spr[\"losses\"][\"train\"]\nval_losses = spr[\"losses\"][\"val\"]\nval_metrics = spr[\"metrics\"][\"val\"]  # list of dicts\n\nepochs = list(range(1, len(train_losses) + 1))\n\n# gather metric curves\nacc_curve = [m[\"acc\"] for m in val_metrics]\ncwa_curve = [m[\"cwa\"] for m in val_metrics]\nswa_curve = [m[\"swa\"] for m in val_metrics]\ncaa_curve = [m[\"caa\"] for m in val_metrics]\n\npreds = np.array(spr.get(\"predictions\", []))\ngts = np.array(spr.get(\"ground_truth\", []))\n\n\n# ------------- helper to close safely -------------\ndef close_fig():\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\n\n\n# ------------- Plot 1: Loss curves -------------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_losses, label=\"Train Loss\")\n    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR Dataset \u2013 Training vs. Validation Loss\")\n    plt.legend()\n    fname = \"SPR_loss_curve.png\"\n    close_fig()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------- Plot 2: Validation Accuracy -------------\ntry:\n    plt.figure()\n    plt.plot(epochs, acc_curve, marker=\"o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR Dataset \u2013 Validation Accuracy per Epoch\")\n    fname = \"SPR_val_accuracy.png\"\n    close_fig()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# ------------- Plot 3: Weighted Accuracies -------------\ntry:\n    plt.figure()\n    plt.plot(epochs, cwa_curve, label=\"CWA\")\n    plt.plot(epochs, swa_curve, label=\"SWA\")\n    plt.plot(epochs, caa_curve, label=\"CAA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR Dataset \u2013 Weighted Accuracies per Epoch\")\n    plt.legend()\n    fname = \"SPR_weighted_accuracies.png\"\n    close_fig()\nexcept Exception as e:\n    print(f\"Error creating weighted accuracy curves: {e}\")\n    plt.close()\n\n# ------------- Plot 4: Confusion Matrix -------------\ntry:\n    if preds.size and gts.size:\n        cm = np.zeros((2, 2), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"red\")\n        plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n        plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n        plt.title(\"SPR Dataset \u2013 Confusion Matrix (Dev Set)\")\n        fname = \"SPR_confusion_matrix.png\"\n        close_fig()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------- Print final metrics -------------\nif preds.size and gts.size:\n    final_idx = -1\n    print(\"Final Validation Metrics:\")\n    print(f\"  Accuracy: {acc_curve[final_idx]:.3f}\")\n    print(f\"  Color-Weighted Accuracy: {cwa_curve[final_idx]:.3f}\")\n    print(f\"  Shape-Weighted Accuracy: {swa_curve[final_idx]:.3f}\")\n    print(f\"  Complexity-Adjusted Accuracy: {caa_curve[final_idx]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to optionally down-sample epochs to \u2264 100 points for clarity\ndef trim(xs, ys, max_pts=100):\n    if len(xs) <= max_pts:\n        return xs, ys\n    step = max(1, len(xs) // max_pts)\n    return xs[::step], ys[::step]\n\n\nfor ds_name, ds_data in experiment_data.get(\"epochs\", {}).items():\n    losses_tr = np.array(ds_data[\"losses\"][\"train\"])\n    losses_val = np.array(ds_data[\"losses\"][\"val\"])\n    acc_tr = np.array([m[\"acc\"] for m in ds_data[\"metrics\"][\"train\"]])\n    acc_val = np.array([m[\"acc\"] for m in ds_data[\"metrics\"][\"val\"]])\n    cwa = np.array([m[\"cwa\"] for m in ds_data[\"metrics\"][\"val\"]])\n    swa = np.array([m[\"swa\"] for m in ds_data[\"metrics\"][\"val\"]])\n    caa = np.array([m[\"caa\"] for m in ds_data[\"metrics\"][\"val\"]])\n    epochs = np.arange(1, len(losses_tr) + 1)\n\n    # 1) loss curves\n    try:\n        plt.figure()\n        xs, tr = trim(epochs, losses_tr)\n        _, va = trim(epochs, losses_val)\n        plt.plot(xs, tr, label=\"Train\")\n        plt.plot(xs, va, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Dataset Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) accuracy curves\n    try:\n        plt.figure()\n        xs, tr = trim(epochs, acc_tr)\n        _, va = trim(epochs, acc_val)\n        plt.plot(xs, tr, label=\"Train\")\n        plt.plot(xs, va, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{ds_name} Dataset Training vs Validation Accuracy\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) weighted accuracy variants\n    try:\n        plt.figure()\n        xs, c = trim(epochs, cwa)\n        _, s = trim(epochs, swa)\n        _, k = trim(epochs, caa)\n        plt.plot(xs, c, label=\"CWA\")\n        plt.plot(xs, s, label=\"SWA\")\n        plt.plot(xs, k, label=\"CAA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(f\"{ds_name} Dataset Validation Weighted Accuracies\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{ds_name}_weighted_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating weighted accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # print evaluation summary\n    best_val_acc = acc_val.max() if acc_val.size else float(\"nan\")\n    print(\n        f\"{ds_name}: best_val_acc={best_val_acc:.3f}, \"\n        f\"final_val_acc={acc_val[-1]:.3f}, final_train_acc={acc_tr[-1]:.3f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data[\"learning_rate\"][\"SPR\"]\n    lrs = sorted(runs.keys(), key=lambda x: float(x.replace(\"e-\", \"e-0\")))\n    epochs = len(next(iter(runs.values()))[\"losses\"][\"train\"])\n\n    # Helper to pull metric list\n    def metric_list(run, split, field):\n        if field == \"losses\":\n            return run[\"losses\"][split]\n        else:  # metrics\n            return [m[field] for m in run[\"metrics\"][split]]\n\n    # ----------- Figure 1: Train/Val Loss -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            ax[0].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"train\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n            ax[1].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"val\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Loss Curves Across Learning Rates\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 2: Accuracy & CAA -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            acc = [m[\"acc\"] for m in run[\"metrics\"][\"val\"]]\n            caa = [m[\"caa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), acc, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), caa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Accuracy\")\n        ax[1].set_title(\"Right: Complexity Adjusted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Basic vs Complexity-Aware Accuracy\")\n        fname = os.path.join(working_dir, \"SPR_accuracy_vs_caa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy/CAA plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 3: Color vs Shape Weighted Acc -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            cwa = [m[\"cwa\"] for m in run[\"metrics\"][\"val\"]]\n            swa = [m[\"swa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), cwa, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), swa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Color-Weighted Accuracy\")\n        ax[1].set_title(\"Right: Shape-Weighted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Color vs Shape Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"weight_decay\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = {}\n\n\n# helper to parse float value from key 'wd_1e-3'\ndef _wd_float(k):\n    try:\n        return float(k.split(\"_\", 1)[1])\n    except Exception:\n        return np.nan\n\n\nepochs = range(1, 1 + max(len(v[\"losses\"][\"train\"]) for v in spr_data.values()))\n\n# ---------- figure 1: loss curves ----------\ntry:\n    plt.figure(figsize=(7, 4))\n    for k, v in spr_data.items():\n        plt.plot(epochs, v[\"losses\"][\"train\"], \"--\", label=f\"{_wd_float(k):g} train\")\n        plt.plot(epochs, v[\"losses\"][\"val\"], \"-\", label=f\"{_wd_float(k):g} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR \u2013 Train vs Validation Loss\\nLeft: dashed=train, solid=val\")\n    plt.legend(fontsize=6)\n    fname = os.path.join(working_dir, \"SPR_loss_curves_weight_decay.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- figure 2: accuracy curves ----------\ntry:\n    plt.figure(figsize=(7, 4))\n    for k, v in spr_data.items():\n        acc = [m[\"acc\"] for m in v[\"metrics\"][\"val\"]]\n        plt.plot(epochs, acc, label=f\"{_wd_float(k):g}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"SPR \u2013 Validation Accuracy\\nOne line per weight decay\")\n    plt.legend(title=\"weight_decay\", fontsize=6)\n    fname = os.path.join(working_dir, \"SPR_val_accuracy_curves.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- figure 3: final acc vs weight decay ----------\ntry:\n    plt.figure(figsize=(5, 4))\n    x, y = [], []\n    for k, v in spr_data.items():\n        x.append(_wd_float(k))\n        y.append(v[\"metrics\"][\"val\"][-1][\"acc\"])\n    plt.scatter(x, y)\n    for xi, yi in zip(x, y):\n        plt.text(xi, yi, f\"{xi:g}\", fontsize=7, ha=\"right\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Weight Decay (log scale)\")\n    plt.ylabel(\"Final Val Accuracy\")\n    plt.title(\"SPR \u2013 Final Validation Accuracy vs Weight Decay\")\n    fname = os.path.join(working_dir, \"SPR_final_acc_vs_weight_decay.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating scatter plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nif spr_data:\n    print(\"\\nFinal Validation Accuracies:\")\n    for k, v in sorted(spr_data.items(), key=lambda x: _wd_float(x[0])):\n        print(f\"wd={_wd_float(k):g} -> acc={v['metrics']['val'][-1]['acc']:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    # ---------- unpack data ----------\n    losses_tr = experiment_data[\"batch_size_tuning\"][\"losses\"][\"train\"]\n    losses_val = experiment_data[\"batch_size_tuning\"][\"losses\"][\"val\"]\n    metrics_val = experiment_data[\"batch_size_tuning\"][\"metrics\"][\"val\"]\n\n    batch_sizes = [16, 32, 64, 128]  # known from training script\n    epochs = 5\n    n_bs = len(batch_sizes)\n\n    # reshape helpers\n    lt = np.array(losses_tr).reshape(n_bs, epochs)\n    lv = np.array(losses_val).reshape(n_bs, epochs)\n\n    # accuracy / hwa matrices\n    acc = np.array(\n        [\n            [m[\"acc\"] for m in metrics_val[i * epochs : (i + 1) * epochs]]\n            for i in range(n_bs)\n        ]\n    )\n    hwa = np.array(\n        [\n            [m[\"hwa\"] for m in metrics_val[i * epochs : (i + 1) * epochs]]\n            for i in range(n_bs)\n        ]\n    )\n\n    # ---------- plotting ----------\n    # 1. Loss curves\n    try:\n        plt.figure()\n        for i, bs in enumerate(batch_sizes):\n            plt.plot(range(1, epochs + 1), lt[i], \"--\", label=f\"train bs={bs}\")\n            plt.plot(range(1, epochs + 1), lv[i], \"-\", label=f\"val bs={bs}\")\n        plt.title(\"Training & Validation Losses\\nDataset: Synthetic Shapes-Colors\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"synthetic_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 2. Validation accuracy curves\n    try:\n        plt.figure()\n        for i, bs in enumerate(batch_sizes):\n            plt.plot(range(1, epochs + 1), acc[i], label=f\"bs={bs}\")\n        plt.title(\"Validation Accuracy vs. Epoch\\nDataset: Synthetic Shapes-Colors\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"synthetic_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # 3. Validation HWA curves\n    try:\n        plt.figure()\n        for i, bs in enumerate(batch_sizes):\n            plt.plot(range(1, epochs + 1), hwa[i], label=f\"bs={bs}\")\n        plt.title(\n            \"Validation Harmonic Weighted Accuracy vs. Epoch\\nDataset: Synthetic Shapes-Colors\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"synthetic_hwa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # 4. Best HWA per batch size bar chart\n    try:\n        best_hwa = hwa.max(axis=1)\n        plt.figure()\n        plt.bar([str(bs) for bs in batch_sizes], best_hwa)\n        plt.title(\n            \"Best Harmonic Weighted Accuracy by Batch Size\\nDataset: Synthetic Shapes-Colors\"\n        )\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"Best HWA\")\n        fname = os.path.join(working_dir, \"synthetic_best_hwa_by_bs.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating best HWA bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    results = experiment_data[\"embed_dim\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    results = []\n\n\n# helper: collect data if it exists\ndef collect(metric_key):\n    xs, ys = [], {}\n    for r in results:\n        dim = r[\"embed_dim\"]\n        ys[dim] = [m[metric_key] for m in r[\"metrics\"][\"val\"]]\n        xs = list(range(1, len(ys[dim]) + 1))\n    return xs, ys\n\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    for r in results:\n        dim = r[\"embed_dim\"]\n        plt.plot(r[\"losses\"][\"train\"], label=f\"train dim={dim}\", linestyle=\"--\")\n        plt.plot(r[\"losses\"][\"val\"], label=f\"val dim={dim}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n    plt.savefig(fname)\n    print(\"Saved\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: accuracy curves ----------\ntry:\n    xs, ys = collect(\"acc\")\n    plt.figure()\n    for dim, vals in ys.items():\n        plt.plot(xs, vals, label=f\"dim={dim}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"SPR: Validation Accuracy over Epochs\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_accuracy_curves.png\")\n    plt.savefig(fname)\n    print(\"Saved\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: final metrics bar chart ----------\ntry:\n    metrics = [\"acc\", \"cwa\", \"swa\", \"caa\"]\n    dims = [r[\"embed_dim\"] for r in results]\n    width = 0.18\n    x = np.arange(len(dims))\n    plt.figure()\n    for i, m in enumerate(metrics):\n        vals = [r[\"metrics\"][\"val\"][-1][m] for r in results]\n        plt.bar(x + i * width - width * 1.5, vals, width, label=m.upper())\n    plt.xticks(x, [str(d) for d in dims])\n    plt.xlabel(\"Embedding Dimension\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR: Final-Epoch Validation Metrics\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_final_metrics.png\")\n    plt.savefig(fname)\n    print(\"Saved\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final metrics plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nbst_data = experiment_data.get(\"batch_size_tuning\", {})\nif not bst_data:\n    print(\"No batch_size_tuning data found.\")\nelse:\n    # ------- individual loss curves ----------\n    for idx, (tag, exp) in enumerate(bst_data.items()):\n        try:\n            epochs = list(range(1, len(exp[\"losses\"][\"train\"]) + 1))\n            plt.figure()\n            plt.plot(epochs, exp[\"losses\"][\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, exp[\"losses\"][\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\n                f\"Synthetic Dataset \u2013 Loss Curves (batch_size={tag.split('_')[-1]})\"\n            )\n            plt.legend()\n            fname = f\"synthetic_loss_{tag}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {tag}: {e}\")\n            plt.close()\n\n    # ------- aggregated HWA comparison --------\n    try:\n        plt.figure()\n        for tag, exp in bst_data.items():\n            epochs = list(range(1, len(exp[\"metrics\"][\"val\"]) + 1))\n            hwa = [m[\"hwa\"] for m in exp[\"metrics\"][\"val\"]]\n            plt.plot(epochs, hwa, marker=\"o\", label=f\"bs={tag.split('_')[-1]}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"Synthetic Dataset \u2013 HWA Comparison Across Batch Sizes\")\n        plt.legend()\n        fname = \"synthetic_hwa_batchsize_comparison.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA comparison plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    drop_dict = experiment_data.get(\"dropout_tuning\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    drop_dict = {}\n\n# Figure 1: Loss curves (train & val) for all dropout rates\ntry:\n    plt.figure(figsize=(8, 5))\n    for dr_str, rec in drop_dict.items():\n        epochs = np.arange(1, len(rec[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, rec[\"losses\"][\"train\"], \"--\", label=f\"{dr_str} train\")\n        plt.plot(epochs, rec[\"losses\"][\"val\"], \"-\", label=f\"{dr_str} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Synthetic Dataset \u2014 Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.legend(fontsize=7, ncol=2)\n    fname = os.path.join(working_dir, \"synthetic_loss_curves_all_dropout.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Figure 2: Validation accuracy curves for all dropout rates\ntry:\n    plt.figure(figsize=(8, 5))\n    for dr_str, rec in drop_dict.items():\n        accs = [m[\"acc\"] for m in rec[\"metrics\"][\"val\"]]\n        epochs = np.arange(1, len(accs) + 1)\n        plt.plot(epochs, accs, marker=\"o\", label=dr_str)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.ylim(0, 1.05)\n    plt.title(\"Synthetic Dataset \u2014 Validation Accuracy vs. Epoch\")\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"synthetic_val_accuracy_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Figure 3: Final validation accuracy bar chart per dropout\ntry:\n    plt.figure(figsize=(6, 4))\n    drs, finals = [], []\n    for dr_str, rec in drop_dict.items():\n        drs.append(dr_str.replace(\"dropout_\", \"\"))\n        finals.append(rec[\"metrics\"][\"val\"][-1][\"acc\"])\n    plt.bar(drs, finals, color=\"skyblue\")\n    plt.xlabel(\"Dropout Rate\")\n    plt.ylabel(\"Final Validation Accuracy\")\n    for i, v in enumerate(finals):\n        plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\", fontsize=8)\n    plt.ylim(0, 1.05)\n    plt.title(\"Synthetic Dataset \u2014 Final Epoch Validation Accuracies\")\n    fname = os.path.join(working_dir, \"synthetic_final_val_accuracy_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data[\"learning_rate\"][\"SPR\"]\n    lrs = sorted(runs.keys(), key=lambda x: float(x.replace(\"e-\", \"e-0\")))\n    epochs = len(next(iter(runs.values()))[\"losses\"][\"train\"])\n\n    # Helper to pull metric list\n    def metric_list(run, split, field):\n        if field == \"losses\":\n            return run[\"losses\"][split]\n        else:  # metrics\n            return [m[field] for m in run[\"metrics\"][split]]\n\n    # ----------- Figure 1: Train/Val Loss -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            ax[0].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"train\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n            ax[1].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"val\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Loss Curves Across Learning Rates\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 2: Accuracy & CAA -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            acc = [m[\"acc\"] for m in run[\"metrics\"][\"val\"]]\n            caa = [m[\"caa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), acc, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), caa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Accuracy\")\n        ax[1].set_title(\"Right: Complexity Adjusted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Basic vs Complexity-Aware Accuracy\")\n        fname = os.path.join(working_dir, \"SPR_accuracy_vs_caa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy/CAA plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 3: Color vs Shape Weighted Acc -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            cwa = [m[\"cwa\"] for m in run[\"metrics\"][\"val\"]]\n            swa = [m[\"swa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), cwa, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), swa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Color-Weighted Accuracy\")\n        ax[1].set_title(\"Right: Shape-Weighted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Color vs Shape Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data[\"learning_rate\"][\"SPR\"]\n    lrs = sorted(runs.keys(), key=lambda x: float(x.replace(\"e-\", \"e-0\")))\n    epochs = len(next(iter(runs.values()))[\"losses\"][\"train\"])\n\n    # Helper to pull metric list\n    def metric_list(run, split, field):\n        if field == \"losses\":\n            return run[\"losses\"][split]\n        else:  # metrics\n            return [m[field] for m in run[\"metrics\"][split]]\n\n    # ----------- Figure 1: Train/Val Loss -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            ax[0].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"train\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n            ax[1].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"val\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Loss Curves Across Learning Rates\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 2: Accuracy & CAA -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            acc = [m[\"acc\"] for m in run[\"metrics\"][\"val\"]]\n            caa = [m[\"caa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), acc, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), caa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Accuracy\")\n        ax[1].set_title(\"Right: Complexity Adjusted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Basic vs Complexity-Aware Accuracy\")\n        fname = os.path.join(working_dir, \"SPR_accuracy_vs_caa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy/CAA plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 3: Color vs Shape Weighted Acc -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            cwa = [m[\"cwa\"] for m in run[\"metrics\"][\"val\"]]\n            swa = [m[\"swa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), cwa, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), swa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Color-Weighted Accuracy\")\n        ax[1].set_title(\"Right: Shape-Weighted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Color vs Shape Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs = experiment_data[\"learning_rate\"][\"SPR\"]\n    lrs = sorted(runs.keys(), key=lambda x: float(x.replace(\"e-\", \"e-0\")))\n    epochs = len(next(iter(runs.values()))[\"losses\"][\"train\"])\n\n    # Helper to pull metric list\n    def metric_list(run, split, field):\n        if field == \"losses\":\n            return run[\"losses\"][split]\n        else:  # metrics\n            return [m[field] for m in run[\"metrics\"][split]]\n\n    # ----------- Figure 1: Train/Val Loss -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            ax[0].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"train\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n            ax[1].plot(\n                range(1, epochs + 1),\n                metric_list(run, \"val\", \"losses\"),\n                label=f\"lr={lr}\",\n            )\n        ax[0].set_title(\"Left: Train Loss\")\n        ax[1].set_title(\"Right: Validation Loss\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Loss\")\n            a.legend()\n        fig.suptitle(\"SPR Loss Curves Across Learning Rates\")\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 2: Accuracy & CAA -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            acc = [m[\"acc\"] for m in run[\"metrics\"][\"val\"]]\n            caa = [m[\"caa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), acc, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), caa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Accuracy\")\n        ax[1].set_title(\"Right: Complexity Adjusted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Basic vs Complexity-Aware Accuracy\")\n        fname = os.path.join(working_dir, \"SPR_accuracy_vs_caa.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy/CAA plot: {e}\")\n        plt.close()\n\n    # ----------- Figure 3: Color vs Shape Weighted Acc -----------\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n        for lr in lrs:\n            run = runs[lr]\n            cwa = [m[\"cwa\"] for m in run[\"metrics\"][\"val\"]]\n            swa = [m[\"swa\"] for m in run[\"metrics\"][\"val\"]]\n            ax[0].plot(range(1, epochs + 1), cwa, label=f\"lr={lr}\")\n            ax[1].plot(range(1, epochs + 1), swa, label=f\"lr={lr}\")\n        ax[0].set_title(\"Left: Color-Weighted Accuracy\")\n        ax[1].set_title(\"Right: Shape-Weighted Accuracy\")\n        for a in ax:\n            a.set_xlabel(\"Epoch\")\n            a.set_ylabel(\"Score\")\n            a.legend()\n        fig.suptitle(\"SPR Color vs Shape Weighted Accuracies\")\n        fname = os.path.join(working_dir, \"SPR_cwa_swa_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating CWA/SWA plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# Basic setup\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load every experiment_data.npy that was provided\n# ------------------------------------------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a54bca8557344951b55e3158bd8ba95e_proc_1441297/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_aabb56a02b9d4130823593c412b4808c_proc_1441298/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-30_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_c9282642aa774286ba9a258a1ceda2a1_proc_1441296/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        exp = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p), allow_pickle=True\n        ).item()\n        all_experiment_data.append(exp)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif not all_experiment_data:\n    print(\"No experiment data could be loaded \u2013 aborting plot generation.\")\n    exit()\n\n# ------------------------------------------------------------------\n# Aggregate data across files for each learning-rate\n# ------------------------------------------------------------------\nagg = {}  # {lr : dict of lists of arrays}\nfor exp in all_experiment_data:\n    try:\n        runs = exp[\"learning_rate\"][\"SPR\"]\n    except KeyError:\n        print(\"Missing expected keys in experiment_data file \u2013 skipping.\")\n        continue\n    for lr, run in runs.items():\n        bucket = agg.setdefault(\n            lr,\n            {\n                \"train_loss\": [],\n                \"val_loss\": [],\n                \"val_acc\": [],\n                \"val_caa\": [],\n                \"val_cwa\": [],\n                \"val_swa\": [],\n            },\n        )\n        bucket[\"train_loss\"].append(np.asarray(run[\"losses\"][\"train\"], dtype=float))\n        bucket[\"val_loss\"].append(np.asarray(run[\"losses\"][\"val\"], dtype=float))\n        # Pull validation metrics\n        v_metrics = run[\"metrics\"][\"val\"]\n        bucket[\"val_acc\"].append(np.asarray([m[\"acc\"] for m in v_metrics], dtype=float))\n        bucket[\"val_caa\"].append(np.asarray([m[\"caa\"] for m in v_metrics], dtype=float))\n        bucket[\"val_cwa\"].append(np.asarray([m[\"cwa\"] for m in v_metrics], dtype=float))\n        bucket[\"val_swa\"].append(np.asarray([m[\"swa\"] for m in v_metrics], dtype=float))\n\n\n# Helper that stacks arrays trimming to the minimal length\ndef stack_and_get_mean_sem(list_of_arrays):\n    min_len = min(arr.shape[0] for arr in list_of_arrays)\n    trimmed = np.stack([arr[:min_len] for arr in list_of_arrays], axis=0)\n    mean = trimmed.mean(axis=0)\n    sem = trimmed.std(axis=0, ddof=1) / np.sqrt(trimmed.shape[0])\n    return mean, sem, min_len\n\n\n# Color cycle to remain consistent\ncolors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\n# ------------------------------------------------------------------\n# FIGURE 1  \u2013 Train & Val Loss with SEM\n# ------------------------------------------------------------------\ntry:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n    for idx, (lr, data) in enumerate(\n        sorted(agg.items(), key=lambda x: float(x[0].replace(\"e-\", \"e-0\")))\n    ):\n        m_train, sem_train, ep_train = stack_and_get_mean_sem(data[\"train_loss\"])\n        m_val, sem_val, ep_val = stack_and_get_mean_sem(data[\"val_loss\"])\n        epochs = np.arange(1, min(ep_train, ep_val) + 1)\n        c = colors[idx % len(colors)]\n\n        ax[0].plot(epochs, m_train, color=c, label=f\"lr={lr}\")\n        ax[0].fill_between(\n            epochs,\n            m_train - sem_train[: len(epochs)],\n            m_train + sem_train[: len(epochs)],\n            color=c,\n            alpha=0.3,\n        )\n        ax[1].plot(epochs, m_val, color=c, label=f\"lr={lr}\")\n        ax[1].fill_between(\n            epochs,\n            m_val - sem_val[: len(epochs)],\n            m_val + sem_val[: len(epochs)],\n            color=c,\n            alpha=0.3,\n        )\n\n    ax[0].set_title(\"Left: Train Loss (mean \u00b1 SEM)\")\n    ax[1].set_title(\"Right: Validation Loss (mean \u00b1 SEM)\")\n    for a in ax:\n        a.set_xlabel(\"Epoch\")\n        a.set_ylabel(\"Loss\")\n        a.legend()\n    fig.suptitle(\"SPR \u2013 Aggregated Loss Curves Across Learning Rates\")\n    fname = os.path.join(working_dir, \"SPR_aggregated_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# FIGURE 2  \u2013 Accuracy & CAA with SEM\n# ------------------------------------------------------------------\ntry:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n    for idx, (lr, data) in enumerate(\n        sorted(agg.items(), key=lambda x: float(x[0].replace(\"e-\", \"e-0\")))\n    ):\n        m_acc, sem_acc, ep_acc = stack_and_get_mean_sem(data[\"val_acc\"])\n        m_caa, sem_caa, ep_caa = stack_and_get_mean_sem(data[\"val_caa\"])\n        epochs = np.arange(1, min(ep_acc, ep_caa) + 1)\n        c = colors[idx % len(colors)]\n\n        ax[0].plot(epochs, m_acc, color=c, label=f\"lr={lr}\")\n        ax[0].fill_between(\n            epochs,\n            m_acc - sem_acc[: len(epochs)],\n            m_acc + sem_acc[: len(epochs)],\n            color=c,\n            alpha=0.3,\n        )\n        ax[1].plot(epochs, m_caa, color=c, label=f\"lr={lr}\")\n        ax[1].fill_between(\n            epochs,\n            m_caa - sem_caa[: len(epochs)],\n            m_caa + sem_caa[: len(epochs)],\n            color=c,\n            alpha=0.3,\n        )\n\n    ax[0].set_title(\"Left: Accuracy (mean \u00b1 SEM)\")\n    ax[1].set_title(\"Right: Complexity-Adjusted Accuracy (mean \u00b1 SEM)\")\n    for a in ax:\n        a.set_xlabel(\"Epoch\")\n        a.set_ylabel(\"Score\")\n        a.legend()\n    fig.suptitle(\"SPR \u2013 Aggregated Accuracy Metrics\")\n    fname = os.path.join(working_dir, \"SPR_aggregated_accuracy_caa.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated accuracy/CAA plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# FIGURE 3  \u2013 CWA & SWA with SEM\n# ------------------------------------------------------------------\ntry:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n    for idx, (lr, data) in enumerate(\n        sorted(agg.items(), key=lambda x: float(x[0].replace(\"e-\", \"e-0\")))\n    ):\n        m_cwa, sem_cwa, ep_cwa = stack_and_get_mean_sem(data[\"val_cwa\"])\n        m_swa, sem_swa, ep_swa = stack_and_get_mean_sem(data[\"val_swa\"])\n        epochs = np.arange(1, min(ep_cwa, ep_swa) + 1)\n        c = colors[idx % len(colors)]\n\n        ax[0].plot(epochs, m_cwa, color=c, label=f\"lr={lr}\")\n        ax[0].fill_between(\n            epochs,\n            m_cwa - sem_cwa[: len(epochs)],\n            m_cwa + sem_cwa[: len(epochs)],\n            color=c,\n            alpha=0.3,\n        )\n        ax[1].plot(epochs, m_swa, color=c, label=f\"lr={lr}\")\n        ax[1].fill_between(\n            epochs,\n            m_swa - sem_swa[: len(epochs)],\n            m_swa + sem_swa[: len(epochs)],\n            color=c,\n            alpha=0.3,\n        )\n\n    ax[0].set_title(\"Left: Color-Weighted Accuracy (mean \u00b1 SEM)\")\n    ax[1].set_title(\"Right: Shape-Weighted Accuracy (mean \u00b1 SEM)\")\n    for a in ax:\n        a.set_xlabel(\"Epoch\")\n        a.set_ylabel(\"Score\")\n        a.legend()\n    fig.suptitle(\"SPR \u2013 Aggregated CWA vs SWA\")\n    fname = os.path.join(working_dir, \"SPR_aggregated_cwa_swa.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated CWA/SWA plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We will load experiment_data.npy from the working directory, extract the stored train/validation losses, validation metrics per epoch, and the final prediction versus ground-truth arrays. Four plots will be produced at most: (1) train vs. validation loss curve, (2) validation accuracy curve, (3) combined weighted accuracies (CWA, SWA, CAA) curve, and (4) a 2\u00d72 confusion-matrix heatmap for the final predictions. Every plot is wrapped in its own try-except block, saved with descriptive file names containing the dataset (\u201cSPR\u201d) and plot type, titled appropriately, and closed after saving. The code also prints the final evaluation numbers (accuracy and weighted accuracies) derived from the stored data. No synthetic data are generated; only the contents of experiment_data.npy are used. All plots are saved to the pre-defined working_dir, and the script adheres strictly to basic matplotlib functionality. Finally, figures are generated only once per category, ensuring the five-figure maximum is respected.", "We first load experiment_data.npy, identify all dataset keys (here \u201cSPR\u201d) and extract per-epoch arrays for losses and metrics. For each dataset we make up to three simple line plots: (1) training vs validation loss, (2) training vs validation accuracy, and (3) validation weighted accuracies (CWA, SWA, CAA). Each figure is wrapped in a try-except block, titled clearly, saved to working_dir with descriptive names, and then closed. We limit figures by plotting every k-th epoch if total epochs > 50 so that at most 5 figures would ever be created. After plotting we also print the best validation accuracy and the final epoch\u2019s main metrics for quick inspection. Only data already present in experiment_data.npy are used\u2014no simulation or fabrication. The code stays concise, follows the requested imports order, and respects all plotting guidelines. Finally, all figures are saved and the script exits.", "We need to load experiment_data.npy, loop over the three learning-rate runs stored under experiment_data['learning_rate']['SPR'], and extract per-epoch arrays for train/val losses and the four validation metrics (acc, cwa, swa, caa). To keep the figure count \u22645 we will combine all learning rates in the same plot and group related curves into paired subplots\u2014one figure for losses, one for basic vs complexity-aware accuracies, and one for color vs shape weighted accuracies. Each figure will be wrapped in its own try-except so that any failure leaves the others unaffected; all figures are saved inside the working directory with descriptive filenames that include the dataset name and plot type. After saving a figure we always call plt.close() to free resources. The code starts with the mandated imports, creates working_dir if missing, attempts to load the data with allow_pickle=True, then proceeds to plotting only if loading succeeds. We cap plots to at most the available epochs (\u22645) so no interval sampling is needed, but the logic is in place for future longer runs. Finally, the script prints a brief confirmation after each successful save so downstream logs show progress.", null, "We will load the saved experiment_data.npy file, focus on the \u201cSPR\u201d dataset under the weight-decay sweep, and pull out per-epoch train/val losses and validation accuracy. Three figures are produced (all saved in working_dir): (1) combined train/val loss curves for every weight-decay value, (2) validation accuracy curves for every weight-decay value, and (3) a summary scatter of the final-epoch validation accuracy versus weight decay. Each figure is wrapped in its own try-except block to guarantee clean failure handling and figure closure. Plot titles clearly mention the dataset and contain a subtitle explaining the plotted content (e.g. \u201cTrain vs Validation Loss\u201d). Filenames also contain the dataset name plus a short descriptor. We plot all epochs because there are at most five per run, and we only have five distinct weight-decay settings, so the \u22645-figure rule is respected. After the plots are generated, the script prints a compact table of final validation accuracies to stdout as an extra sanity check. The code uses only basic matplotlib/NumPy, stays concise, and follows the required import/working_dir pattern.", "We will first load experiment_data.npy, then recover the list of batch-sizes and the fixed number of epochs (inferable because loss arrays have length = #batch_sizes \u00d7 epochs).  \nFor every batch-size we slice the loss lists into per-epoch chunks so we can plot paired training/validation loss curves.  \nFigure 1 will overlay these loss curves for all four batch sizes, using a legend that states \u201cbs=16\u201d, \u201cbs=32\u201d, etc.  \nNext we collect the validation metrics dicts, separate out plain accuracy and harmonic weighted accuracy, and build two numpy arrays shaped (batch_size, epochs).  \nFigure 2 will plot accuracy vs. epoch for all batch sizes; Figure 3 will do the same for HWA.  \nTo summarize overall performance we compute the best epoch HWA for each batch size and place those numbers in a bar chart (Figure 4).  \nAll figures are wrapped in individual try-except blocks, follow the naming/titling rules, and are saved under working_dir with clear filenames.  \nWe respect the \u201cmax 5 figures\u201d rule and close every figure.", "We load the saved experiment_data.npy, safely extract the list of results for the SPR dataset, and iterate through the four embed_dim configurations (16, 32, 64, 128). From each result we pull the recorded training/validation losses and the validation metrics (accuracy, color-weighted, shape-weighted, complexity-adjusted) per epoch. Three plots are created inside independent try-except blocks: (1) a combined line plot of training and validation loss versus epoch with a separate color per embed_dim; (2) a combined line plot of standard validation accuracy versus epoch for every embed_dim; (3) a grouped bar chart showing the four final-epoch validation metrics for each embed_dim. Each figure is titled, saved to working_dir with descriptive filenames, and closed to free memory. A maximum of three figures keeps us within the five-plot guideline. If data loading fails or a plotting step errors, the exception is caught, logged, and the figure is closed, ensuring the script runs gracefully in all circumstances. Finally, the script prints the paths of generated plots so the user can quickly locate them.", "We first load the saved numpy dictionary, extract the \u201cbatch_size_tuning\u201d section and iterate over every batch-size tag. For each tag we pull the per-epoch training and validation losses plus the validation HWA values. Four individual figures are created\u2014one per batch size\u2014showing training vs. validation loss over the five epochs; each figure has a clear title indicating the dataset (synthetic) and the batch size. A fifth figure overlays the harmonic weighted accuracy (HWA) curves for all batch sizes so we can quickly compare generalisation behaviour. Every figure is wrapped in its own try-except block, saved into working_dir with descriptive names, and immediately closed. All plotting obeys the \u22645-figure limit and uses only the data that exists in experiment_data.npy. No extra styling beyond basic matplotlib is added, and failures during plotting are caught and reported without interrupting subsequent plots.", "We first load the saved experiment_data.npy and extract the dropout_tuning dictionary. From each dropout entry we gather (a) the per-epoch training and validation losses, and (b) the per-epoch validation metrics (we focus on plain accuracy). We then create three figures: 1) a combined line plot showing training and validation loss curves for all dropout settings; 2) a combined line plot showing validation accuracy curves for all dropout settings; 3) a bar chart comparing the final-epoch validation accuracies across dropout rates. Each figure is wrapped in its own try-except block, saved to the working directory with descriptive filenames, titled clearly, and the figure is always closed afterward. We limit ourselves to these three figures so we stay under the five-figure cap while still conveying all key trends. Only data actually present in experiment_data.npy are plotted\u2014no synthetic values are fabricated. The code follows the specified import order and directory setup, and prints nothing except potential error messages raised inside the try-except blocks.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "epochs", "learning_rate", "batch_size", "weight_decay", null, "embed_dim", null, "dropout_rate", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script loads the saved experiment_data.npy file from the working sub-\ndirectory, extracts the stored losses and validation metrics for every dataset\nentry (here \u201cSPR\u201d), determines the best value for each quantity (minimum for\nlosses, maximum for accuracy-type metrics), and prints them in a clear, labeled\nformat. It also reports the epoch at which each best score occurred so you know\nwhen it was reached. All logic is implemented at the top level so the file runs\nimmediately without any special entry point.", "Below is a concise loader that reads the saved NumPy file, extracts the stored\ntraining/validation statistics, determines the final and best values, and prints\nthem with explicit metric names for every dataset contained in the file.", "The script will load experiment_data.npy from the working directory, walk\nthrough its nested structure (learning-rate \u2192 dataset \u2192 run log), and for each\ndataset print the final epoch\u2019s training loss, validation loss, validation\naccuracy, color-weighted accuracy, shape-weighted accuracy, and complexity-\nadjusted accuracy obtained with every learning rate tried. The dataset name is\nprinted before any metrics, and every metric is clearly labelled to avoid\nambiguity.", "", "The script will (1) locate and load the saved experiment data, (2) iterate\nthrough the nested dictionary to gather the metrics for each weight-decay run,\n(3) keep the best (lowest or highest, as appropriate) value encountered across\nall runs for every metric, and (4) print a concise summary per dataset with\nclear metric names. No plotting or special entry point is used, so the code\nexecutes immediately.", "The script will 1) load experiment_data.npy from the working directory, 2)\nnavigate the nested dictionary to fetch the loss lists and validation-metrics\nlist, 3) take the last element of each list as the \u201cfinal\u201d result, and 4) print\nthose values with explicit dataset and metric names.", "The script will load the saved numpy file from the working directory, traverse\nits nested structure (experiment_data \u279c embed_dim \u279c SPR \u279c list of experiment\nruns), and, for each run, extract the final (last\u2013epoch) values for training\nloss, validation loss, validation accuracy, color-weighted accuracy, shape-\nweighted accuracy, and complexity-adjusted accuracy. It then prints each dataset\nname (here \u201cSPR\u201d together with the embedding dimension tried) followed by\nclearly labeled metric/value pairs. No plots are generated, and everything runs\nat global scope.", "The script will load the saved NumPy dictionary, iterate over every batch-size\nexperiment (each acts as an individual \u201cdataset\u201d here), pick the final epoch\u2019s\nstatistics, and print them with clear, explicit names such as \u201cfinal training\nloss\u201d or \u201cfinal validation accuracy.\u201d No plotting or special entry point is\nused; everything runs at import time. This meets the formatting requirements by\nprinting the dataset name first, followed by each clearly-labeled metric value.", "The script will locate the saved experiment_data.npy inside the \u201cworking\u201d\ndirectory, load it, and convert it to a Python dict.   For each dropout\nexperiment (treated here as a separate \u201cdataset\u201d), it scans the recorded epoch-\nwise losses and metrics, picks the lowest loss values and the highest accuracy-\ntype values, and then prints them with explicit, self-descriptive labels.   All\nlogic is placed in top-level code so the file runs immediately when executed\u2014no\nspecial entry points or plotting are used.", "The script will load experiment_data.npy from the working directory, walk\nthrough its nested structure (learning-rate \u2192 dataset \u2192 run log), and for each\ndataset print the final epoch\u2019s training loss, validation loss, validation\naccuracy, color-weighted accuracy, shape-weighted accuracy, and complexity-\nadjusted accuracy obtained with every learning rate tried. The dataset name is\nprinted before any metrics, and every metric is clearly labelled to avoid\nambiguity.", "The script will load experiment_data.npy from the working directory, walk\nthrough its nested structure (learning-rate \u2192 dataset \u2192 run log), and for each\ndataset print the final epoch\u2019s training loss, validation loss, validation\naccuracy, color-weighted accuracy, shape-weighted accuracy, and complexity-\nadjusted accuracy obtained with every learning rate tried. The dataset name is\nprinted before any metrics, and every metric is clearly labelled to avoid\nambiguity.", "The script will load experiment_data.npy from the working directory, walk\nthrough its nested structure (learning-rate \u2192 dataset \u2192 run log), and for each\ndataset print the final epoch\u2019s training loss, validation loss, validation\naccuracy, color-weighted accuracy, shape-weighted accuracy, and complexity-\nadjusted accuracy obtained with every learning rate tried. The dataset name is\nprinted before any metrics, and every metric is clearly labelled to avoid\nambiguity.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# --------------------------------------------------------------\n# locate and load experiment data\n# --------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------------------\n# utility helpers\n# --------------------------------------------------------------\ndef _best(loss_or_metric_list, maximize=False):\n    \"\"\"\n    Return best value and its epoch index from a list.\n    For metrics set maximize=True, for losses keep default (minimize).\n    \"\"\"\n    if not loss_or_metric_list:\n        return None, None\n    vals = np.array(loss_or_metric_list, dtype=float)\n    idx = vals.argmax() if maximize else vals.argmin()\n    return float(vals[idx]), int(idx + 1)  # epochs are 1-based\n\n\n# --------------------------------------------------------------\n# iterate over stored datasets and print best scores\n# --------------------------------------------------------------\nfor dataset_name, data_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---------- losses ----------\n    train_losses = data_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data_dict.get(\"losses\", {}).get(\"val\", [])\n\n    best_train_loss, epoch_train = _best(train_losses, maximize=False)\n    best_val_loss, epoch_val = _best(val_losses, maximize=False)\n\n    if best_train_loss is not None:\n        print(f\"best train loss: {best_train_loss:.4f} (epoch {epoch_train})\")\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.4f} (epoch {epoch_val})\")\n\n    # ---------- validation metrics ----------\n    val_metrics = data_dict.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # collect lists for each metric name\n        metric_names = val_metrics[0].keys()\n        for m in metric_names:\n            series = [epoch_metrics[m] for epoch_metrics in val_metrics]\n            best_value, best_epoch = _best(series, maximize=True)  # higher is better\n            print(f\"best validation {m}: {best_value:.4f} (epoch {best_epoch})\")\n\n    # ---------- final epoch predictions vs ground truth ----------\n    preds = data_dict.get(\"predictions\", [])\n    gts = data_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        final_accuracy = np.mean([p == t for p, t in zip(preds, gts)])\n        print(\n            f\"final validation accuracy (from stored predictions): {final_accuracy:.4f}\"\n        )\n", "import os\nimport numpy as np\n\n# 0. Locate and load the npy file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nepochs_dict = experiment_data.get(\"epochs\", {})\n\n\ndef summarize_dataset(name, data_dict):\n    \"\"\"Print final and best metrics for one dataset.\"\"\"\n    train_losses = data_dict[\"losses\"][\"train\"]\n    val_losses = data_dict[\"losses\"][\"val\"]\n    train_metrics = data_dict[\"metrics\"][\"train\"]\n    val_metrics = data_dict[\"metrics\"][\"val\"]\n\n    # Final values (last epoch)\n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    final_train_acc = train_metrics[-1][\"acc\"]\n    final_val_acc = val_metrics[-1][\"acc\"]\n    final_val_cwa = val_metrics[-1][\"cwa\"]\n    final_val_swa = val_metrics[-1][\"swa\"]\n    final_val_caa = val_metrics[-1][\"caa\"]\n\n    # Best values across epochs\n    best_val_loss = min(val_losses)\n    best_val_acc = max(m[\"acc\"] for m in val_metrics)\n    best_val_cwa = max(m[\"cwa\"] for m in val_metrics)\n    best_val_swa = max(m[\"swa\"] for m in val_metrics)\n    best_val_caa = max(m[\"caa\"] for m in val_metrics)\n\n    # Printing section\n    print(f\"Dataset: {name}\")\n    print(f\"  training loss (final): {final_train_loss:.4f}\")\n    print(f\"  training accuracy (final): {final_train_acc:.4f}\")\n    print(f\"  validation loss (final): {final_val_loss:.4f}\")\n    print(f\"  validation loss (best): {best_val_loss:.4f}\")\n    print(f\"  validation accuracy (final): {final_val_acc:.4f}\")\n    print(f\"  validation accuracy (best): {best_val_acc:.4f}\")\n    print(f\"  validation color-weighted accuracy (final): {final_val_cwa:.4f}\")\n    print(f\"  validation color-weighted accuracy (best): {best_val_cwa:.4f}\")\n    print(f\"  validation shape-weighted accuracy (final): {final_val_swa:.4f}\")\n    print(f\"  validation shape-weighted accuracy (best): {best_val_swa:.4f}\")\n    print(f\"  validation complexity-adjusted accuracy (final): {final_val_caa:.4f}\")\n    print(f\"  validation complexity-adjusted accuracy (best): {best_val_caa:.4f}\")\n    print()  # blank line for readability\n\n\n# 2\u20135. Iterate through each dataset and display metrics\nfor dataset_name, dataset_data in epochs_dict.items():\n    summarize_dataset(dataset_name, dataset_data)\n", "import os\nimport numpy as np\n\n# ------------------ locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------ parse & print ------------------\n# top level is experiment_data[\"learning_rate\"] -> dict(dataset_name -> dict(lr_key -> run_log))\nlearning_rate_block = experiment_data.get(\"learning_rate\", {})\n\nfor dataset_name, lr_dict in learning_rate_block.items():\n    print(f\"Dataset: {dataset_name}\")\n    for lr_key, run_log in lr_dict.items():\n        # final values are the last entries recorded during training\n        final_train_loss = run_log[\"losses\"][\"train\"][-1]\n        final_val_loss = run_log[\"losses\"][\"val\"][-1]\n        final_metrics = run_log[\"metrics\"][\"val\"][-1]  # dict with acc, cwa, swa, caa\n\n        print(f\"  Learning rate: {lr_key}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    final validation accuracy: {final_metrics['acc']:.4f}\")\n        print(\n            f\"    final validation color weighted accuracy: {final_metrics['cwa']:.4f}\"\n        )\n        print(\n            f\"    final validation shape weighted accuracy: {final_metrics['swa']:.4f}\"\n        )\n        print(\n            f\"    final validation complexity adjusted accuracy: {final_metrics['caa']:.4f}\"\n        )\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the experiment data saved by the original script\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper to update best values (lower is better for loss, higher for others)\n# ------------------------------------------------------------------\ndef update_best(current_best, candidate, higher_is_better):\n    if current_best is None:\n        return candidate\n    if higher_is_better:\n        return candidate if candidate > current_best else current_best\n    else:\n        return candidate if candidate < current_best else current_best\n\n\n# ------------------------------------------------------------------\n# iterate over datasets inside the experiment_data structure\n# ------------------------------------------------------------------\nfor dataset_name, wd_runs in experiment_data[\"weight_decay\"].items():\n    best_final_train_loss = None\n    best_val_loss = None\n    best_acc = None\n    best_cwa = None\n    best_swa = None\n    best_caa = None\n\n    # go through every weight-decay run\n    for wd_key, record in wd_runs.items():\n        # ------------- losses -------------\n        final_train_loss = record[\"losses\"][\"train\"][-1]\n        min_val_loss = min(record[\"losses\"][\"val\"])\n\n        best_final_train_loss = update_best(\n            best_final_train_loss, final_train_loss, higher_is_better=False\n        )\n        best_val_loss = update_best(best_val_loss, min_val_loss, higher_is_better=False)\n\n        # ------------- validation metrics -------------\n        for m in record[\"metrics\"][\"val\"]:\n            best_acc = update_best(best_acc, m[\"acc\"], higher_is_better=True)\n            best_cwa = update_best(best_cwa, m[\"cwa\"], higher_is_better=True)\n            best_swa = update_best(best_swa, m[\"swa\"], higher_is_better=True)\n            best_caa = update_best(best_caa, m[\"caa\"], higher_is_better=True)\n\n    # ------------------------------------------------------------------\n    # printing section \u2013 follow naming rules strictly\n    # ------------------------------------------------------------------\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"best final training loss: {best_final_train_loss:.4f}\")\n    print(f\"best validation loss: {best_val_loss:.4f}\")\n    print(f\"best validation accuracy: {best_acc:.4f}\")\n    print(f\"best validation color-weighted accuracy: {best_cwa:.4f}\")\n    print(f\"best validation shape-weighted accuracy: {best_swa:.4f}\")\n    print(f\"best validation complexity-adjusted accuracy: {best_caa:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nbucket = experiment_data[\"batch_size_tuning\"]\n\n# ---------- training split ----------\ntrain_losses = bucket[\"losses\"][\"train\"]\nif train_losses:\n    print(\"training set\")\n    print(f\"training loss: {train_losses[-1]:.6f}\")\n\n# ---------- validation split ----------\nval_losses = bucket[\"losses\"][\"val\"]\nval_metrics = bucket[\"metrics\"][\"val\"]\n\nif val_losses and val_metrics:\n    last_loss = val_losses[-1]\n    last_metric = val_metrics[-1]  # dict with acc, cwa, swa, hwa\n\n    print(\"validation set\")\n    print(f\"validation loss: {last_loss:.6f}\")\n    print(f\"validation accuracy: {last_metric['acc']:.6f}\")\n    print(f\"validation color weighted accuracy: {last_metric['cwa']:.6f}\")\n    print(f\"validation shape weighted accuracy: {last_metric['swa']:.6f}\")\n    print(f\"validation harmonic weighted accuracy: {last_metric['hwa']:.6f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------\n# locate and load the experiment data\n# --------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# --------------------------------------------------------------\n# iterate over datasets and experiments, printing final metrics\n# --------------------------------------------------------------\nfor dataset_name, experiments in experiment_data[\"embed_dim\"].items():\n    for exp in experiments:\n        embed_dim = exp.get(\"embed_dim\", \"N/A\")\n        # Retrieve losses\n        train_losses = exp[\"losses\"][\"train\"]\n        val_losses = exp[\"losses\"][\"val\"]\n        final_train_loss = train_losses[-1] if train_losses else None\n        final_val_loss = val_losses[-1] if val_losses else None\n\n        # Retrieve validation metrics (last epoch)\n        val_metrics = exp[\"metrics\"][\"val\"]\n        if val_metrics:\n            last_metrics = val_metrics[-1]\n            final_val_acc = last_metrics.get(\"acc\")\n            final_val_cwa = last_metrics.get(\"cwa\")\n            final_val_swa = last_metrics.get(\"swa\")\n            final_val_caa = last_metrics.get(\"caa\")\n        else:\n            final_val_acc = final_val_cwa = final_val_swa = final_val_caa = None\n\n        # ---------------- print results ---------------- #\n        print(f\"Dataset: {dataset_name}, embed_dim={embed_dim}\")\n        if final_train_loss is not None:\n            print(f\"  final training loss: {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"  final validation loss: {final_val_loss:.4f}\")\n        if final_val_acc is not None:\n            print(f\"  final validation accuracy: {final_val_acc:.4f}\")\n        if final_val_cwa is not None:\n            print(f\"  final validation color weighted accuracy: {final_val_cwa:.4f}\")\n        if final_val_swa is not None:\n            print(f\"  final validation shape weighted accuracy: {final_val_swa:.4f}\")\n        if final_val_caa is not None:\n            print(\n                f\"  final validation complexity adjusted accuracy: {final_val_caa:.4f}\"\n            )\n", "import os\nimport numpy as np\n\n# ----------------- load data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------------- extract & print metrics -----------------\nfor dataset_name, exp in experiment_data.get(\"batch_size_tuning\", {}).items():\n    # The last epoch contains the final values\n    final_train_loss = exp[\"losses\"][\"train\"][-1]\n    final_validation_loss = exp[\"losses\"][\"val\"][-1]\n\n    final_metrics = exp[\"metrics\"][\"val\"][-1]  # dict with acc, cwa, swa, hwa\n    final_val_accuracy = final_metrics[\"acc\"]\n    final_val_cwa = final_metrics[\"cwa\"]\n    final_val_swa = final_metrics[\"swa\"]\n    final_val_hwa = final_metrics[\"hwa\"]\n\n    # Print results\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"final training loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_validation_loss:.4f}\")\n    print(f\"final validation accuracy: {final_val_accuracy:.4f}\")\n    print(f\"final validation color weighted accuracy: {final_val_cwa:.4f}\")\n    print(f\"final validation shape weighted accuracy: {final_val_swa:.4f}\")\n    print(f\"final validation harmonic weighted accuracy: {final_val_hwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at: {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper for printing best (or final) metric values\n# ------------------------------------------------------------------\ndef summarize_experiment(name: str, record: dict):\n    \"\"\"\n    Print best/final metrics for a single experiment run.\n    \"\"\"\n    train_losses = record[\"losses\"].get(\"train\", [])\n    val_losses = record[\"losses\"].get(\"val\", [])\n    val_metrics = record[\"metrics\"].get(\"val\", [])\n\n    # Determine best / final values\n    lowest_train_loss = min(train_losses) if train_losses else None\n    lowest_val_loss = min(val_losses) if val_losses else None\n\n    # For accuracy-type metrics pick the maximum value observed\n    best_vals = {}\n    if val_metrics:\n        keys = val_metrics[0].keys()\n        for k in keys:\n            best_vals[k] = max(m[k] for m in val_metrics)\n\n    # ------------------ OUTPUT ------------------\n    print(f\"\\nDataset: {name}\")\n    if lowest_train_loss is not None:\n        print(f\"lowest training loss: {lowest_train_loss:.4f}\")\n    if lowest_val_loss is not None:\n        print(f\"lowest validation loss: {lowest_val_loss:.4f}\")\n\n    for metric_name, val in best_vals.items():\n        pretty_name = {\n            \"acc\": \"best validation accuracy\",\n            \"cwa\": \"best validation color-weighted accuracy\",\n            \"swa\": \"best validation shape-weighted accuracy\",\n            \"caa\": \"best validation complexity-adjusted accuracy\",\n        }.get(metric_name, metric_name)\n        print(f\"{pretty_name}: {val:.4f}\")\n\n\n# ------------------------------------------------------------------\n# Iterate over all stored experiments (dropout settings)\n# ------------------------------------------------------------------\ndropout_dict = experiment_data.get(\"dropout_tuning\", {})\nfor exp_name, exp_record in dropout_dict.items():\n    summarize_experiment(exp_name, exp_record)\n", "import os\nimport numpy as np\n\n# ------------------ locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------ parse & print ------------------\n# top level is experiment_data[\"learning_rate\"] -> dict(dataset_name -> dict(lr_key -> run_log))\nlearning_rate_block = experiment_data.get(\"learning_rate\", {})\n\nfor dataset_name, lr_dict in learning_rate_block.items():\n    print(f\"Dataset: {dataset_name}\")\n    for lr_key, run_log in lr_dict.items():\n        # final values are the last entries recorded during training\n        final_train_loss = run_log[\"losses\"][\"train\"][-1]\n        final_val_loss = run_log[\"losses\"][\"val\"][-1]\n        final_metrics = run_log[\"metrics\"][\"val\"][-1]  # dict with acc, cwa, swa, caa\n\n        print(f\"  Learning rate: {lr_key}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    final validation accuracy: {final_metrics['acc']:.4f}\")\n        print(\n            f\"    final validation color weighted accuracy: {final_metrics['cwa']:.4f}\"\n        )\n        print(\n            f\"    final validation shape weighted accuracy: {final_metrics['swa']:.4f}\"\n        )\n        print(\n            f\"    final validation complexity adjusted accuracy: {final_metrics['caa']:.4f}\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------ locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------ parse & print ------------------\n# top level is experiment_data[\"learning_rate\"] -> dict(dataset_name -> dict(lr_key -> run_log))\nlearning_rate_block = experiment_data.get(\"learning_rate\", {})\n\nfor dataset_name, lr_dict in learning_rate_block.items():\n    print(f\"Dataset: {dataset_name}\")\n    for lr_key, run_log in lr_dict.items():\n        # final values are the last entries recorded during training\n        final_train_loss = run_log[\"losses\"][\"train\"][-1]\n        final_val_loss = run_log[\"losses\"][\"val\"][-1]\n        final_metrics = run_log[\"metrics\"][\"val\"][-1]  # dict with acc, cwa, swa, caa\n\n        print(f\"  Learning rate: {lr_key}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    final validation accuracy: {final_metrics['acc']:.4f}\")\n        print(\n            f\"    final validation color weighted accuracy: {final_metrics['cwa']:.4f}\"\n        )\n        print(\n            f\"    final validation shape weighted accuracy: {final_metrics['swa']:.4f}\"\n        )\n        print(\n            f\"    final validation complexity adjusted accuracy: {final_metrics['caa']:.4f}\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------ locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------ parse & print ------------------\n# top level is experiment_data[\"learning_rate\"] -> dict(dataset_name -> dict(lr_key -> run_log))\nlearning_rate_block = experiment_data.get(\"learning_rate\", {})\n\nfor dataset_name, lr_dict in learning_rate_block.items():\n    print(f\"Dataset: {dataset_name}\")\n    for lr_key, run_log in lr_dict.items():\n        # final values are the last entries recorded during training\n        final_train_loss = run_log[\"losses\"][\"train\"][-1]\n        final_val_loss = run_log[\"losses\"][\"val\"][-1]\n        final_metrics = run_log[\"metrics\"][\"val\"][-1]  # dict with acc, cwa, swa, caa\n\n        print(f\"  Learning rate: {lr_key}\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    final validation accuracy: {final_metrics['acc']:.4f}\")\n        print(\n            f\"    final validation color weighted accuracy: {final_metrics['cwa']:.4f}\"\n        )\n        print(\n            f\"    final validation shape weighted accuracy: {final_metrics['swa']:.4f}\"\n        )\n        print(\n            f\"    final validation complexity adjusted accuracy: {final_metrics['caa']:.4f}\"\n        )\n", ""], "parse_term_out": ["['\\nDataset: SPR', '\\n', 'best train loss: 0.5068 (epoch 5)', '\\n', 'best\nvalidation loss: 0.5157 (epoch 2)', '\\n', 'best validation acc: 0.8000 (epoch\n1)', '\\n', 'best validation cwa: 0.7909 (epoch 1)', '\\n', 'best validation swa:\n0.8055 (epoch 1)', '\\n', 'best validation caa: 0.7983 (epoch 1)', '\\n', 'final\nvalidation accuracy (from stored predictions): 0.8000', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR', '\\n', '  training loss (final): 0.4464', '\\n', '  training\naccuracy (final): 0.8200', '\\n', '  validation loss (final): 0.5064', '\\n', '\nvalidation loss (best): 0.4897', '\\n', '  validation accuracy (final): 0.8167',\n'\\n', '  validation accuracy (best): 0.8167', '\\n', '  validation color-weighted\naccuracy (final): 0.8310', '\\n', '  validation color-weighted accuracy (best):\n0.8310', '\\n', '  validation shape-weighted accuracy (final): 0.8227', '\\n', '\nvalidation shape-weighted accuracy (best): 0.8227', '\\n', '  validation\ncomplexity-adjusted accuracy (final): 0.8269', '\\n', '  validation complexity-\nadjusted accuracy (best): 0.8269', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR', '\\n', '  Learning rate: 3e-04', '\\n', '    final training loss:\n0.5023', '\\n', '    final validation loss: 0.3987', '\\n', '    final validation\naccuracy: 0.8833', '\\n', '    final validation color weighted accuracy: 0.8854',\n'\\n', '    final validation shape weighted accuracy: 0.8727', '\\n', '    final\nvalidation complexity adjusted accuracy: 0.8792', '\\n', '  Learning rate:\n1e-03', '\\n', '    final training loss: 0.4830', '\\n', '    final validation\nloss: 0.3891', '\\n', '    final validation accuracy: 0.8833', '\\n', '    final\nvalidation color weighted accuracy: 0.8854', '\\n', '    final validation shape\nweighted accuracy: 0.8727', '\\n', '    final validation complexity adjusted\naccuracy: 0.8792', '\\n', '  Learning rate: 3e-03', '\\n', '    final training\nloss: 0.4482', '\\n', '    final validation loss: 0.3505', '\\n', '    final\nvalidation accuracy: 0.8833', '\\n', '    final validation color weighted\naccuracy: 0.8854', '\\n', '    final validation shape weighted accuracy: 0.8727',\n'\\n', '    final validation complexity adjusted accuracy: 0.8792', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['Dataset: SPR', '\\n', 'best final training loss: 0.4194', '\\n', 'best\nvalidation loss: 0.4439', '\\n', 'best validation accuracy: 0.8333', '\\n', 'best\nvalidation color-weighted accuracy: 0.8201', '\\n', 'best validation shape-\nweighted accuracy: 0.8293', '\\n', 'best validation complexity-adjusted accuracy:\n0.8247', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['training set', '\\n', 'training loss: 0.445357', '\\n', 'validation set', '\\n',\n'validation loss: 0.541456', '\\n', 'validation accuracy: 0.766667', '\\n',\n'validation color weighted accuracy: 0.786232', '\\n', 'validation shape weighted\naccuracy: 0.810526', '\\n', 'validation harmonic weighted accuracy: 0.798194',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR, embed_dim=16', '\\n', '  final training loss: 0.4660', '\\n', '\nfinal validation loss: 0.5126', '\\n', '  final validation accuracy: 0.7833',\n'\\n', '  final validation color weighted accuracy: 0.7664', '\\n', '  final\nvalidation shape weighted accuracy: 0.7794', '\\n', '  final validation\ncomplexity adjusted accuracy: 0.7729', '\\n', 'Dataset: SPR, embed_dim=32', '\\n',\n'  final training loss: 0.4599', '\\n', '  final validation loss: 0.5329', '\\n',\n'  final validation accuracy: 0.7833', '\\n', '  final validation color weighted\naccuracy: 0.7664', '\\n', '  final validation shape weighted accuracy: 0.7794',\n'\\n', '  final validation complexity adjusted accuracy: 0.7729', '\\n', 'Dataset:\nSPR, embed_dim=64', '\\n', '  final training loss: 0.4516', '\\n', '  final\nvalidation loss: 0.5080', '\\n', '  final validation accuracy: 0.7833', '\\n', '\nfinal validation color weighted accuracy: 0.7664', '\\n', '  final validation\nshape weighted accuracy: 0.7794', '\\n', '  final validation complexity adjusted\naccuracy: 0.7729', '\\n', 'Dataset: SPR, embed_dim=128', '\\n', '  final training\nloss: 0.4137', '\\n', '  final validation loss: 0.5126', '\\n', '  final\nvalidation accuracy: 0.7833', '\\n', '  final validation color weighted accuracy:\n0.7664', '\\n', '  final validation shape weighted accuracy: 0.7794', '\\n', '\nfinal validation complexity adjusted accuracy: 0.7729', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: bs_16', '\\n', 'final training loss: 0.4532', '\\n', 'final\nvalidation loss: 0.6407', '\\n', 'final validation accuracy: 0.7500', '\\n',\n'final validation color weighted accuracy: 0.7692', '\\n', 'final validation\nshape weighted accuracy: 0.7653', '\\n', 'final validation harmonic weighted\naccuracy: 0.7673', '\\n', '\\nDataset: bs_32', '\\n', 'final training loss:\n0.4920', '\\n', 'final validation loss: 0.5735', '\\n', 'final validation\naccuracy: 0.7500', '\\n', 'final validation color weighted accuracy: 0.7692',\n'\\n', 'final validation shape weighted accuracy: 0.7653', '\\n', 'final\nvalidation harmonic weighted accuracy: 0.7673', '\\n', '\\nDataset: bs_64', '\\n',\n'final training loss: 0.5058', '\\n', 'final validation loss: 0.6161', '\\n',\n'final validation accuracy: 0.7500', '\\n', 'final validation color weighted\naccuracy: 0.7692', '\\n', 'final validation shape weighted accuracy: 0.7653',\n'\\n', 'final validation harmonic weighted accuracy: 0.7673', '\\n', '\\nDataset:\nbs_128', '\\n', 'final training loss: 0.5079', '\\n', 'final validation loss:\n0.5722', '\\n', 'final validation accuracy: 0.7500', '\\n', 'final validation\ncolor weighted accuracy: 0.7692', '\\n', 'final validation shape weighted\naccuracy: 0.7653', '\\n', 'final validation harmonic weighted accuracy: 0.7673',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: dropout_0.0', '\\n', 'lowest training loss: 0.5027', '\\n', 'lowest\nvalidation loss: 0.4631', '\\n', 'best validation accuracy: 0.8333', '\\n', 'best\nvalidation color-weighted accuracy: 0.8456', '\\n', 'best validation shape-\nweighted accuracy: 0.8357', '\\n', 'best validation complexity-adjusted accuracy:\n0.8407', '\\n', '\\nDataset: dropout_0.1', '\\n', 'lowest training loss: 0.5169',\n'\\n', 'lowest validation loss: 0.4641', '\\n', 'best validation accuracy:\n0.8333', '\\n', 'best validation color-weighted accuracy: 0.8456', '\\n', 'best\nvalidation shape-weighted accuracy: 0.8357', '\\n', 'best validation complexity-\nadjusted accuracy: 0.8407', '\\n', '\\nDataset: dropout_0.25', '\\n', 'lowest\ntraining loss: 0.5209', '\\n', 'lowest validation loss: 0.4504', '\\n', 'best\nvalidation accuracy: 0.8333', '\\n', 'best validation color-weighted accuracy:\n0.8456', '\\n', 'best validation shape-weighted accuracy: 0.8357', '\\n', 'best\nvalidation complexity-adjusted accuracy: 0.8407', '\\n', '\\nDataset:\ndropout_0.4', '\\n', 'lowest training loss: 0.5242', '\\n', 'lowest validation\nloss: 0.4572', '\\n', 'best validation accuracy: 0.8333', '\\n', 'best validation\ncolor-weighted accuracy: 0.8456', '\\n', 'best validation shape-weighted\naccuracy: 0.8357', '\\n', 'best validation complexity-adjusted accuracy: 0.8407',\n'\\n', '\\nDataset: dropout_0.5', '\\n', 'lowest training loss: 0.5372', '\\n',\n'lowest validation loss: 0.4597', '\\n', 'best validation accuracy: 0.8333',\n'\\n', 'best validation color-weighted accuracy: 0.8456', '\\n', 'best validation\nshape-weighted accuracy: 0.8357', '\\n', 'best validation complexity-adjusted\naccuracy: 0.8407', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Dataset: SPR', '\\n', '  Learning rate: 3e-04', '\\n', '    final training loss:\n0.4172', '\\n', '    final validation loss: 0.3119', '\\n', '    final validation\naccuracy: 0.9167', '\\n', '    final validation color weighted accuracy: 0.9158',\n'\\n', '    final validation shape weighted accuracy: 0.9304', '\\n', '    final\nvalidation complexity adjusted accuracy: 0.9231', '\\n', '  Learning rate:\n1e-03', '\\n', '    final training loss: 0.4082', '\\n', '    final validation\nloss: 0.3103', '\\n', '    final validation accuracy: 0.9167', '\\n', '    final\nvalidation color weighted accuracy: 0.9158', '\\n', '    final validation shape\nweighted accuracy: 0.9304', '\\n', '    final validation complexity adjusted\naccuracy: 0.9231', '\\n', '  Learning rate: 3e-03', '\\n', '    final training\nloss: 0.3736', '\\n', '    final validation loss: 0.3173', '\\n', '    final\nvalidation accuracy: 0.9167', '\\n', '    final validation color weighted\naccuracy: 0.9158', '\\n', '    final validation shape weighted accuracy: 0.9304',\n'\\n', '    final validation complexity adjusted accuracy: 0.9231', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR', '\\n', '  Learning rate: 3e-04', '\\n', '    final training loss:\n0.3968', '\\n', '    final validation loss: 0.2896', '\\n', '    final validation\naccuracy: 0.9500', '\\n', '    final validation color weighted accuracy: 0.9498',\n'\\n', '    final validation shape weighted accuracy: 0.9470', '\\n', '    final\nvalidation complexity adjusted accuracy: 0.9485', '\\n', '  Learning rate:\n1e-03', '\\n', '    final training loss: 0.3485', '\\n', '    final validation\nloss: 0.2264', '\\n', '    final validation accuracy: 0.9500', '\\n', '    final\nvalidation color weighted accuracy: 0.9498', '\\n', '    final validation shape\nweighted accuracy: 0.9470', '\\n', '    final validation complexity adjusted\naccuracy: 0.9485', '\\n', '  Learning rate: 3e-03', '\\n', '    final training\nloss: 0.3279', '\\n', '    final validation loss: 0.2247', '\\n', '    final\nvalidation accuracy: 0.9500', '\\n', '    final validation color weighted\naccuracy: 0.9498', '\\n', '    final validation shape weighted accuracy: 0.9470',\n'\\n', '    final validation complexity adjusted accuracy: 0.9485', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR', '\\n', '  Learning rate: 3e-04', '\\n', '    final training loss:\n0.4684', '\\n', '    final validation loss: 0.4254', '\\n', '    final validation\naccuracy: 0.8667', '\\n', '    final validation color weighted accuracy: 0.8750',\n'\\n', '    final validation shape weighted accuracy: 0.8990', '\\n', '    final\nvalidation complexity adjusted accuracy: 0.8870', '\\n', '  Learning rate:\n1e-03', '\\n', '    final training loss: 0.4439', '\\n', '    final validation\nloss: 0.3999', '\\n', '    final validation accuracy: 0.8667', '\\n', '    final\nvalidation color weighted accuracy: 0.8750', '\\n', '    final validation shape\nweighted accuracy: 0.8990', '\\n', '    final validation complexity adjusted\naccuracy: 0.8870', '\\n', '  Learning rate: 3e-03', '\\n', '    final training\nloss: 0.4045', '\\n', '    final validation loss: 0.4303', '\\n', '    final\nvalidation accuracy: 0.8667', '\\n', '    final validation color weighted\naccuracy: 0.8750', '\\n', '    final validation shape weighted accuracy: 0.8990',\n'\\n', '    final validation complexity adjusted accuracy: 0.8870', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
