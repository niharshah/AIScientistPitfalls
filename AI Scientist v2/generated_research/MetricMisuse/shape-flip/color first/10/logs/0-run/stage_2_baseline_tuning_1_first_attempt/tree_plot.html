<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 3], [0, 2], [0, 1], [0, 7], [0, 5], [0, 8], [0, 6], [0, 4], [1, 9], [1, 10], [1, 11], [1, 12]], "layout": [[0.5882352941176471, 0.0], [0.17647058823529413, 0.5], [0.29411764705882354, 0.5], [0.4117647058823529, 0.5], [0.5294117647058824, 0.5], [0.6470588235294118, 0.5], [0.7647058823529411, 0.5], [0.8823529411764706, 0.5], [1.0, 0.5], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["The baseline converts each SPR sequence into a simple graph where tokens are\nnodes and consecutive tokens form undirected edges.  A single embedding layer\nconverts token-IDs to dense vectors; two GraphSAGE layers with global mean\npooling yield a graph representation that feeds a softmax classifier.  We\nconstruct vocabularies for tokens and labels from the training split (adding an\n<UNK> token), transform every split into torch-geometric `Data` objects, and\ntrain for a handful of epochs while reporting loss plus Color-Weighted, Shape-\nWeighted and Harmonic-Weighted accuracies on the dev set.  All tensors and the\nmodel are moved to GPU when available as required, and metrics/losses are stored\nin `experiment_data`, saved at the end.  The code also falls back to a tiny\nsynthetic dataset if the real benchmark folder is absent so it can always run.\nThis provides a clean, end-to-end, minimally tuned GNN baseline that we will\nrefine in later stages.", "Hyperparam tuning name: EPOCHS. We will enlarge the search space of the single\nhyper-parameter \u201cnumber of training epochs\u201d.   The script below trains separate\nmodels for several candidate epoch counts (5 \u2192 50), records per-epoch training /\nvalidation curves, keeps the run that yields the best validation HWA score, and\nfinally reports the test performance of that best checkpoint.   All results are\nstored in an `experiment_data` dictionary under the key `\"EPOCHS\"` and saved to\n`experiment_data.npy`.", "Hyperparam tuning name: learning_rate. We keep the original data-loading, graph\nconstruction and evaluation logic, but wrap the training loop inside a function\nthat we call for a list of learning-rates {3e-4, 5e-4, 1e-3, 2e-3}.   For every\nlearning-rate we re-instantiate the model/optimizer, train for a few epochs,\nevaluate on the dev split, and store per-epoch losses and weighted accuracies\ninside an experiment_data dictionary under the key \"learning_rate\".   After the\nsweep we pick the best learning-rate according to dev HWA, run a final test\nevaluation with that model, and save everything to experiment_data.npy.", "Hyperparam tuning name: batch_size. We loop over three candidate mini-batch\nsizes (32, 64, 128).   For every value we rebuild the training DataLoader, re-\ninitialise a fresh model, train for five epochs, evaluate on the dev split after\neach epoch, then evaluate on the test split once training finishes.   All\nlosses, metrics, predictions and timestamps are stored in the hierarchical\nexperiment_data dictionary under the top-level key 'batch_size' \u2192 'SPR_BENCH' \u2192\n'<batch_size>'.   Finally the complete experiment_data structure is saved to\nworking/experiment_data.npy so it can be inspected or plotted later.", "Hyperparam tuning name: emb_dim. We loop over the candidate embedding dimensions\n(16, 32, 64, 128); for each value we train the same Graph-SAGE classifier for a\nfew epochs, evaluate on the dev split after every epoch, and finally evaluate on\nthe test split.   All losses, per-epoch validation metrics and final predictions\nare stored under experiment_data['emb_dim']['SPR_BENCH']['runs'] so the file has\nfull provenance across the sweep.   The script keeps the original data-set\nconstruction, loaders and evaluation utilities intact; only the model\ninstantiation is varied inside the sweep.", "Hyperparam tuning name: hidden_dim. We will grid-search the two SAGEConv hidden\ndimension sizes {32, 64, 128, 256}.   For every hidden_dim we (1) build a fresh\nmodel, (2) train it for a few epochs, (3) record train/validation losses and\nweighted accuracies, and (4) keep the best-performing configuration.   All\nresults for every hidden size are stored inside the required `experiment_data`\ndictionary and saved as `experiment_data.npy`.   After tuning we report test\nperformance of the best model.", "Hyperparam tuning name: weight_decay. We sweep five weight-decay values for the\nAdam optimizer, training a freshly\u2013initialized model for each coefficient and\nrecording per-epoch losses as well as development/test weighted-accuracy\nmetrics.   Results for every run are stored in the experiment_data dictionary\nunder keys \u201cweight_decay_{value} \u279c SPR_BENCH \u2026\u201d, then saved to\n\u2018experiment_data.npy\u2019.", "Hyperparam tuning name: num_gnn_layers. We load the SPR-BENCH dataset (or\ngenerate a tiny synthetic fallback), turn each sequence into a PyG graph once,\nand then loop over four depths for the GraphSAGE backbone (1, 2, 3, 4 layers).\nFor every depth we create a fresh model, train it for a few epochs, evaluate on\nthe dev and test splits, and keep per-epoch losses together with the\ncolour/shape/harmonic weighted accuracies.   All results are stored in the dict\nexperiment_data under the key 'num_gnn_layers'\u2192'SPR_BENCH'\u2192layer_depth so that\ndownstream analysis can compare the influence of varying message-passing depth.\nFinally the whole experiment_data structure (including predictions and ground-\ntruth labels) is saved into working/experiment_data.npy.", "Hyperparam tuning name: dropout_rate. We add a dropout\u2010aware GraphClassifier\nthat applies nn.Dropout after every GraphSAGE block and before the final linear\nlayer, parameterised by a probability p.   A small grid search (p \u2208 {0.0, 0.2,\n0.4, 0.6}) is run: for each rate we train the model for a few epochs, track\nlosses and harmonic weighted accuracy on train/dev splits, evaluate on the test\nsplit, and store all results in experiment_data under the top-level key\n\"dropout_rate\".   The script is fully self-contained, downloads/creates the\ndataset if missing, and finally serialises experiment_data to\nworking/experiment_data.npy.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, time, math, sys, itertools\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:  # self loop for single node graphs\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    # build vocab & label map from train split\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())  # [N] -> [N,emb]\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)  # [batch,hidden]\n        return self.lin(x)\n\n\n# ----------------- attempt to load real dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    # -------------- synthetic tiny fallback -----------------\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):  # generate n seq/label\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(tokens)\n            label = random.choice([\"yes\", \"no\"])\n            seqs.append(seq)\n            labs.append(label)\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\n# ----------------- build graph datasets -----------------\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\n\n# ----------------- instantiate model -----------------\nmodel = GraphClassifier(\n    vocab_size=len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\n\n# ----------------- training loop -----------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = cross_entropy(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    train_loss = total_loss / len(graph_sets[\"train\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- evaluation on dev ----\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(dev_loader):\n            seq_batch = spr_bench[\"dev\"][\"sequence\"][\n                batch_idx * 128 : batch_idx * 128 + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seq_batch)\n    val_loss /= len(graph_sets[\"dev\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    # translate back to label strings\n    inv_label = {v: k for k, v in label2idx.items()}\n    pred_lbls = [inv_label[p] for p in all_preds]\n    true_lbls = [inv_label[t] for t in all_labels]\n    cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    hwa = harmonic_weighted_accuracy(cwa, swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa}\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n        f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n    )\n\n# ----------------- final evaluation on test set -----------------\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\nmodel.eval()\ntest_preds, test_labels, test_seqs = [], [], []\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_loader):\n        seq_batch = spr_bench[\"test\"][\"sequence\"][\n            batch_idx * 128 : batch_idx * 128 + batch.num_graphs\n        ]\n        batch = batch.to(device)\n        logits = model(batch)\n        preds = logits.argmax(dim=-1).cpu().tolist()\n        labels = batch.y.cpu().tolist()\n        test_preds.extend(preds)\n        test_labels.extend(labels)\n        test_seqs.extend(seq_batch)\ninv_label = {v: k for k, v in label2idx.items()}\npred_lbls = [inv_label[p] for p in test_preds]\ntrue_lbls = [inv_label[t] for t in test_labels]\ncwa_test = color_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\nswa_test = shape_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\nhwa_test = harmonic_weighted_accuracy(cwa_test, swa_test)\nprint(f\"Test  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"cwa\": cwa_test,\n    \"swa\": swa_test,\n    \"hwa\": hwa_test,\n}\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = pred_lbls\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = true_lbls\n\n# ----------------- save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# ---------------------------------------------------------\n# Hyper-parameter tuning:  E P O C H S\n# ---------------------------------------------------------\nimport os, pathlib, random, time, math, sys, itertools, copy\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment bookkeeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"EPOCHS\": {\"SPR_BENCH\": {}}}  # hyper-parameter family we are tuning\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- dataset loading -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\ninv_label = {v: k for k, v in label2idx.items()}\n\n\n# ----------------- evaluation helper -----------------\ndef evaluate(model, loader, seqs_split):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for idx, batch in enumerate(loader):\n            seq_batch = seqs_split[\n                idx * loader.batch_size : idx * loader.batch_size + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seq_batch)\n    val_loss /= len(loader.dataset)\n    pred_lbls = [inv_label[p] for p in all_preds]\n    true_lbls = [inv_label[t] for t in all_labels]\n    cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    hwa = harmonic_weighted_accuracy(cwa, swa)\n    return val_loss, cwa, swa, hwa, pred_lbls, true_lbls\n\n\n# ----------------- hyper-parameter search -----------------\ncandidate_epochs = [5, 15, 30, 50]  # search grid\nbest_hwa = -1.0\nbest_state = None\nbest_epochs = None\n\nfor EPOCHS in candidate_epochs:\n    print(f\"\\n===== Training for {EPOCHS} epochs =====\")\n    random.seed(0)\n    np.random.seed(0)\n    torch.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    model = GraphClassifier(\n        len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    run_train_losses, run_val_losses, run_val_metrics = [], [], []\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n\n        val_loss, cwa, swa, hwa, _, _ = evaluate(\n            model, dev_loader, spr_bench[\"dev\"][\"sequence\"]\n        )\n\n        run_train_losses.append(train_loss)\n        run_val_losses.append(val_loss)\n        run_val_metrics.append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n\n        print(\n            f\"Epoch {epoch:02d}/{EPOCHS}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # save run data\n    experiment_data[\"EPOCHS\"][\"SPR_BENCH\"][f\"run_{EPOCHS}\"] = {\n        \"losses\": {\"train\": run_train_losses, \"val\": run_val_losses},\n        \"metrics\": {\"val\": run_val_metrics},\n        \"final_val_hwa\": run_val_metrics[-1][\"hwa\"],\n        \"final_val_loss\": run_val_losses[-1],\n    }\n\n    # check for best\n    if run_val_metrics[-1][\"hwa\"] > best_hwa:\n        best_hwa = run_val_metrics[-1][\"hwa\"]\n        best_state = copy.deepcopy(model.state_dict())\n        best_epochs = EPOCHS\n\n    del model\n    torch.cuda.empty_cache()\n\nprint(f\"\\nBest validation HWA={best_hwa:.3f} achieved with {best_epochs} epochs.\")\n\n# ----------------- final evaluation on test -----------------\nbest_model = GraphClassifier(\n    len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n).to(device)\nbest_model.load_state_dict(best_state)\ntest_loss, cwa_test, swa_test, hwa_test, test_preds_lbl, test_true_lbl = evaluate(\n    best_model, test_loader, spr_bench[\"test\"][\"sequence\"]\n)\nprint(f\"TEST  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\")\n\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"best_run\"] = best_epochs\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"metrics_test\"] = {\n    \"cwa\": cwa_test,\n    \"swa\": swa_test,\n    \"hwa\": hwa_test,\n}\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"predictions\"] = test_preds_lbl\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"ground_truth\"] = test_true_lbl\n\n# ----------------- save all experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, random, time, math, sys, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment dict -----------------\nexperiment_data = {\n    \"learning_rate\": {\n        \"SPR_BENCH\": {\n            \"lr_values\": [],\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n        }\n    }\n}\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    vocab = {tok for seq in seqs for tok in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        return [\n            sequence_to_graph(s, token2idx, label2idx[l])\n            for s, l in zip(spr[split][\"sequence\"], spr[split][\"label\"])\n        ]\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- get dataset (real or synthetic) -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found. Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            seq = \" \".join(\n                random.choice(shapes) + random.choice(colors) for _ in range(L)\n            )\n            seqs.append(seq)\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader_all = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader_all = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader_all = DataLoader(graph_sets[\"test\"], batch_size=128)\n\ninv_label = {v: k for k, v in label2idx.items()}\n\n\n# ----------------- training util -----------------\ndef run_training(lr, epochs=5):\n    model = GraphClassifier(len(token2idx), 32, 64, num_classes).to(device)\n    opt = Adam(model.parameters(), lr=lr)\n    train_losses, val_losses, val_metrics = [], [], []\n    for ep in range(1, epochs + 1):\n        model.train()\n        tloss = 0.0\n        for batch in train_loader_all:\n            batch = batch.to(device)\n            opt.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            opt.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(graph_sets[\"train\"])\n        train_losses.append(tloss)\n\n        # val\n        model.eval()\n        vloss = 0.0\n        preds = []\n        labels = []\n        seqs = []\n        with torch.no_grad():\n            for idx, batch in enumerate(dev_loader_all):\n                seq_batch = spr_bench[\"dev\"][\"sequence\"][\n                    idx * 128 : idx * 128 + batch.num_graphs\n                ]\n                batch = batch.to(device)\n                outs = model(batch)\n                loss = cross_entropy(outs, batch.y)\n                vloss += loss.item() * batch.num_graphs\n                preds += outs.argmax(-1).cpu().tolist()\n                labels += batch.y.cpu().tolist()\n                seqs += seq_batch\n        vloss /= len(graph_sets[\"dev\"])\n        val_losses.append(vloss)\n        pred_lbl = [inv_label[p] for p in preds]\n        true_lbl = [inv_label[t] for t in labels]\n        cwa = color_weighted_accuracy(seqs, true_lbl, pred_lbl)\n        swa = shape_weighted_accuracy(seqs, true_lbl, pred_lbl)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        val_metrics.append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n        print(\n            f\"[lr={lr}] Epoch {ep}: train_loss={tloss:.4f} val_loss={vloss:.4f} HWA={hwa:.3f}\"\n        )\n    return model, train_losses, val_losses, val_metrics\n\n\n# ----------------- learning-rate sweep -----------------\nlr_values = [3e-4, 5e-4, 1e-3, 2e-3]\nbest_hwa, best_lr, best_model = -1, None, None\n\nfor lr in lr_values:\n    model, tr_losses, va_losses, va_metrics = run_training(lr, epochs=5)\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"lr_values\"].append(lr)\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_losses)\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(va_losses)\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(va_metrics)\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    final_hwa = va_metrics[-1][\"hwa\"]\n    if final_hwa > best_hwa:\n        best_hwa, best_lr, best_model = final_hwa, lr, model\n\nprint(f\"Best lr according to dev HWA: {best_lr} (HWA={best_hwa:.3f})\")\n\n# ----------------- test evaluation with best model -----------------\nbest_model.eval()\npreds_test = []\nlabels_test = []\nseqs_test = []\nwith torch.no_grad():\n    for idx, batch in enumerate(test_loader_all):\n        seq_batch = spr_bench[\"test\"][\"sequence\"][\n            idx * 128 : idx * 128 + batch.num_graphs\n        ]\n        batch = batch.to(device)\n        outs = best_model(batch)\n        preds_test += outs.argmax(-1).cpu().tolist()\n        labels_test += batch.y.cpu().tolist()\n        seqs_test += seq_batch\npred_lbl = [inv_label[p] for p in preds_test]\ntrue_lbl = [inv_label[t] for t in labels_test]\ncwa_test = color_weighted_accuracy(seqs_test, true_lbl, pred_lbl)\nswa_test = shape_weighted_accuracy(seqs_test, true_lbl, pred_lbl)\nhwa_test = harmonic_weighted_accuracy(cwa_test, swa_test)\nprint(f\"Test  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\")\n\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"cwa\": cwa_test,\n    \"swa\": swa_test,\n    \"hwa\": hwa_test,\n}\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"metrics\"][\"train\"] = \"N/A\"\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"predictions\"] = pred_lbl\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"ground_truth\"] = true_lbl\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_lr\"] = best_lr\n\n# ----------------- save -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Data saved to {os.path.join(working_dir,\"experiment_data.npy\")}')\n", "import os, pathlib, random, time, math, sys, itertools\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment data container -----------------\nexperiment_data = {\n    \"batch_size\": {  # hyper-parameter tuning type\n        \"SPR_BENCH\": {}  # individual runs will be stored here keyed by batch size\n    }\n}\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:  # self loop for single node graphs\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())  # [N] -> [N,emb]\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)  # [batch,hidden]\n        return self.lin(x)\n\n\n# ----------------- attempt to load real dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\n# ----------------- build graph datasets -----------------\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\n# ----------------- hyper-parameter tuning loop -----------------\ncandidate_batch_sizes = [32, 64, 128]\nEPOCHS = 5\n\nfor bs in candidate_batch_sizes:\n    print(f\"\\n=== Training with batch size {bs} ===\")\n    run_key = str(bs)\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    # loaders\n    train_loader = DataLoader(graph_sets[\"train\"], batch_size=bs, shuffle=True)\n    dev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\n    test_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\n    # model & optimiser\n    model = GraphClassifier(\n        vocab_size=len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    # ------------- training epochs -------------\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(graph_sets[\"train\"])\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"losses\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"metrics\"][\"train\"].append(\n            {}\n        )  # placeholder to respect structure\n\n        # ---- evaluation on dev ----\n        model.eval()\n        all_preds, all_labels, all_seqs = [], [], []\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(dev_loader):\n                seq_batch = spr_bench[\"dev\"][\"sequence\"][\n                    batch_idx * 128 : batch_idx * 128 + batch.num_graphs\n                ]\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = cross_entropy(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                preds = logits.argmax(dim=-1).cpu().tolist()\n                labels = batch.y.cpu().tolist()\n                all_preds.extend(preds)\n                all_labels.extend(labels)\n                all_seqs.extend(seq_batch)\n        val_loss /= len(graph_sets[\"dev\"])\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        inv_label = {v: k for k, v in label2idx.items()}\n        pred_lbls = [inv_label[p] for p in all_preds]\n        true_lbls = [inv_label[t] for t in all_labels]\n        cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"metrics\"][\"val\"].append(\n            {\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa}\n        )\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"timestamps\"].append(\n            time.time()\n        )\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # ------------- final test evaluation -------------\n    model.eval()\n    test_preds, test_labels, test_seqs = [], [], []\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(test_loader):\n            seq_batch = spr_bench[\"test\"][\"sequence\"][\n                batch_idx * 128 : batch_idx * 128 + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            test_preds.extend(preds)\n            test_labels.extend(labels)\n            test_seqs.extend(seq_batch)\n    inv_label = {v: k for k, v in label2idx.items()}\n    pred_lbls = [inv_label[p] for p in test_preds]\n    true_lbls = [inv_label[t] for t in test_labels]\n    cwa_test = color_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    swa_test = shape_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    hwa_test = harmonic_weighted_accuracy(cwa_test, swa_test)\n    print(\n        f\"Test results (bs={bs})  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\"\n    )\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"metrics\"][\"test\"] = {\n        \"cwa\": cwa_test,\n        \"swa\": swa_test,\n        \"hwa\": hwa_test,\n    }\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"predictions\"] = pred_lbls\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][run_key][\"ground_truth\"] = true_lbls\n\n# ----------------- save experiment data -----------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# ----------------- imports -----------------\nimport os, pathlib, random, time, math, sys\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment data container -----------------\nexperiment_data = {\n    \"emb_dim\": {  # hyper-parameter tuning type\n        \"SPR_BENCH\": {  # dataset name\n            \"runs\": []  # each element will correspond to one emb_dim value\n        }\n    }\n}\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        return [\n            sequence_to_graph(seq, token2idx, label2idx[lab])\n            for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"])\n        ]\n\n    return {s: _convert(s) for s in [\"train\", \"dev\", \"test\"]}, token2idx, label2idx\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- load real or synthetic dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found. Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for i in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(300)),\n            \"dev\": Dataset.from_dict(synth(80)),\n            \"test\": Dataset.from_dict(synth(80)),\n        }\n    )\n\n# ----------------- build graph datasets -----------------\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\n# Pre-build loaders to reuse across runs\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\n# ----------------- hyper-parameter sweep -----------------\nEMB_DIMS = [16, 32, 64, 128]\nEPOCHS = 5\nhidden_dim = 64\n\nfor emb in EMB_DIMS:\n    print(f\"\\n==== Training with emb_dim={emb} ====\")\n    run_record = {\n        \"emb_dim\": emb,\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    model = GraphClassifier(\n        vocab_size=len(token2idx),\n        emb_dim=emb,\n        hidden_dim=hidden_dim,\n        num_classes=num_classes,\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    # ---- training epochs ----\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(graph_sets[\"train\"])\n        run_record[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- dev evaluation ----\n        model.eval()\n        val_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for b_idx, batch in enumerate(dev_loader):\n                seq_batch = spr_bench[\"dev\"][\"sequence\"][\n                    b_idx * 128 : b_idx * 128 + batch.num_graphs\n                ]\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = cross_entropy(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                preds = logits.argmax(1).cpu().tolist()\n                labels = batch.y.cpu().tolist()\n                all_preds.extend(preds)\n                all_labels.extend(labels)\n                all_seqs.extend(seq_batch)\n        val_loss /= len(graph_sets[\"dev\"])\n        run_record[\"losses\"][\"val\"].append(val_loss)\n\n        inv_label = {v: k for k, v in label2idx.items()}\n        pred_lbls = [inv_label[p] for p in all_preds]\n        true_lbls = [inv_label[t] for t in all_labels]\n        cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        run_record[\"metrics\"][\"val\"].append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n\n        print(\n            f\"Epoch {epoch} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | HWA={hwa:.3f}\"\n        )\n\n    # ---- final test evaluation ----\n    model.eval()\n    test_preds, test_labels, test_seqs = [], [], []\n    with torch.no_grad():\n        for b_idx, batch in enumerate(test_loader):\n            seq_batch = spr_bench[\"test\"][\"sequence\"][\n                b_idx * 128 : b_idx * 128 + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            preds = logits.argmax(1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            test_preds.extend(preds)\n            test_labels.extend(labels)\n            test_seqs.extend(seq_batch)\n    inv_label = {v: k for k, v in label2idx.items()}\n    pred_lbls = [inv_label[p] for p in test_preds]\n    true_lbls = [inv_label[t] for t in test_labels]\n    cwa_t = color_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    swa_t = shape_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    hwa_t = harmonic_weighted_accuracy(cwa_t, swa_t)\n    run_record[\"metrics\"][\"test\"] = {\"cwa\": cwa_t, \"swa\": swa_t, \"hwa\": hwa_t}\n    run_record[\"predictions\"] = pred_lbls\n    run_record[\"ground_truth\"] = true_lbls\n    print(\n        f\"Test results for emb_dim={emb}: CWA={cwa_t:.3f}  SWA={swa_t:.3f}  HWA={hwa_t:.3f}\"\n    )\n\n    # ---- store run ----\n    experiment_data[\"emb_dim\"][\"SPR_BENCH\"][\"runs\"].append(run_record)\n\n# ----------------- save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f'All experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}'\n)\n", "import os, pathlib, random, time, math, sys, itertools\nimport numpy as np, torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment store -----------------\nexperiment_data = {\n    \"hidden_dim_tuning\": {\"SPR_BENCH\": {}}  # will be filled with per-hidden_dim results\n}\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {tok for seq in sequences for tok in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        return [\n            sequence_to_graph(seq, token2idx, label2idx[lab])\n            for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"])\n        ]\n\n    return {s: _convert(s) for s in [\"train\", \"dev\", \"test\"]}, token2idx, label2idx\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- load dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found, generating synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(400)),\n            \"dev\": Dataset.from_dict(synth(100)),\n            \"test\": Dataset.from_dict(synth(100)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\ninv_label = {v: k for k, v in label2idx.items()}\nprint(f\"Vocab size {len(token2idx)}, #classes {num_classes}\")\n\n\n# ----------------- train/val helpers -----------------\ndef run_epoch(model, loader, opt=None):\n    training = opt is not None\n    total = 0.0\n    preds = []\n    labs = []\n    seqs = []\n    if training:\n        model.train()\n    else:\n        model.eval()\n    for batch_idx, batch in enumerate(loader):\n        if not training:\n            seq_batch = seq_provider(loader.dataset_split, batch_idx, batch.num_graphs)\n        batch = batch.to(device)\n        if training:\n            opt.zero_grad()\n        out = model(batch)\n        loss = cross_entropy(out, batch.y)\n        if training:\n            loss.backward()\n            opt.step()\n        total += loss.item() * batch.num_graphs\n        if not training:\n            preds.extend(out.argmax(-1).cpu().tolist())\n            labs.extend(batch.y.cpu().tolist())\n            seqs.extend(seq_batch)\n    avg_loss = total / len(loader.dataset)\n    return avg_loss, (preds, labs, seqs)\n\n\ndef seq_provider(split: str, batch_idx: int, num: int):\n    base = batch_idx * loader_bs[split]\n    return spr_bench[split][\"sequence\"][base : base + num]\n\n\n# ----------------- hyperparameter tuning -----------------\nEPOCHS = 5\nhidden_dims = [32, 64, 128, 256]\nloader_bs = {\"train\": 64, \"dev\": 128, \"test\": 128}\ntrain_loader = DataLoader(\n    graph_sets[\"train\"], batch_size=loader_bs[\"train\"], shuffle=True\n)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=loader_bs[\"dev\"])\nbest_hwa = -1\nbest_state = None\nbest_hidden = None\n\nfor hd in hidden_dims:\n    key = f\"hidden_{hd}\"\n    experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    model = GraphClassifier(\n        len(token2idx), emb_dim=32, hidden_dim=hd, num_classes=num_classes\n    ).to(device)\n    opt = Adam(model.parameters(), lr=1e-3)\n    print(f\"\\n--- Training with hidden_dim={hd} ---\")\n    for epoch in range(1, EPOCHS + 1):\n        tr_loss, _ = run_epoch(model, train_loader, opt)\n        val_loss, (preds, labs, seqs) = run_epoch(model, dev_loader, None)\n        cwa = color_weighted_accuracy(\n            seqs, [inv_label[l] for l in labs], [inv_label[p] for p in preds]\n        )\n        swa = shape_weighted_accuracy(\n            seqs, [inv_label[l] for l in labs], [inv_label[p] for p in preds]\n        )\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        d = experiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][key]\n        d[\"losses\"][\"train\"].append(tr_loss)\n        d[\"losses\"][\"val\"].append(val_loss)\n        d[\"metrics\"][\"val\"].append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n        d[\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {epoch}: tr_loss {tr_loss:.4f} val_loss {val_loss:.4f} HWA {hwa:.3f}\"\n        )\n    # keep best model wrt final dev hwa\n    if hwa > best_hwa:\n        best_hwa = hwa\n        best_state = model.state_dict()\n        best_hidden = hd\n\nprint(f\"\\nBest hidden_dim={best_hidden} with dev HWA={best_hwa:.3f}\")\n\n# ----------------- test with best model -----------------\nbest_model = GraphClassifier(len(token2idx), 32, best_hidden, num_classes).to(device)\nbest_model.load_state_dict(best_state)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=loader_bs[\"test\"])\nbest_model.eval()\ntest_preds, test_labels, test_seqs = [], [], []\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_loader):\n        seq_batch = seq_provider(\"test\", batch_idx, batch.num_graphs)\n        batch = batch.to(device)\n        logits = best_model(batch)\n        test_preds.extend(logits.argmax(-1).cpu().tolist())\n        test_labels.extend(batch.y.cpu().tolist())\n        test_seqs.extend(seq_batch)\ncwa_t = color_weighted_accuracy(\n    test_seqs, [inv_label[l] for l in test_labels], [inv_label[p] for p in test_preds]\n)\nswa_t = shape_weighted_accuracy(\n    test_seqs, [inv_label[l] for l in test_labels], [inv_label[p] for p in test_preds]\n)\nhwa_t = harmonic_weighted_accuracy(cwa_t, swa_t)\nprint(\n    f\"Test results (hidden_dim={best_hidden}): CWA {cwa_t:.3f} SWA {swa_t:.3f} HWA {hwa_t:.3f}\"\n)\n\nexperiment_data[\"hidden_dim_tuning\"][\"SPR_BENCH\"][\"best\"] = {\n    \"hidden_dim\": best_hidden,\n    \"metrics\": {\"test\": {\"cwa\": cwa_t, \"swa\": swa_t, \"hwa\": hwa_t}},\n}\n\n# ----------------- save experiment data -----------------\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"Experiment data saved to experiment_data.npy\")\n", "import os, pathlib, random, time, math, sys, itertools\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment log -----------------\nexperiment_data = {}\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- load or create dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for i in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seq = \" \".join(tokens)\n            labs.append(random.choice([\"yes\", \"no\"]))\n            seqs.append(seq)\n        from datasets import Dataset\n\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\n# ----------------- preprocessing -----------------\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128, shuffle=False)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128, shuffle=False)\n\n# ----------------- weight-decay sweep -----------------\nweight_decay_values = [0, 1e-4, 5e-4, 1e-3, 1e-2]\nEPOCHS = 5\n\nfor wd in weight_decay_values:\n    run_key = f\"weight_decay_{wd}\"\n    experiment_data[run_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n        }\n    }\n    # fresh model & optimiser\n    model = GraphClassifier(len(token2idx), 32, 64, num_classes).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    print(f\"\\n=== Training run: weight_decay={wd} ===\")\n    # ----------------- training loop -----------------\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch.num_graphs\n        train_loss = tot_loss / len(graph_sets[\"train\"])\n        experiment_data[run_key][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n        # ------ dev eval ------\n        model.eval()\n        val_loss = 0.0\n        all_preds, all_labels, all_seqs = [], [], []\n        with torch.no_grad():\n            for idx, batch in enumerate(dev_loader):\n                seq_batch = spr_bench[\"dev\"][\"sequence\"][\n                    idx * 128 : idx * 128 + batch.num_graphs\n                ]\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = cross_entropy(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                all_preds.extend(logits.argmax(dim=-1).cpu().tolist())\n                all_labels.extend(batch.y.cpu().tolist())\n                all_seqs.extend(seq_batch)\n        val_loss /= len(graph_sets[\"dev\"])\n        experiment_data[run_key][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n        inv_label = {v: k for k, v in label2idx.items()}\n        pred_lbls = [inv_label[p] for p in all_preds]\n        true_lbls = [inv_label[t] for t in all_labels]\n        cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        experiment_data[run_key][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n            {\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa}\n        )\n        experiment_data[run_key][\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # ----------------- test evaluation -----------------\n    model.eval()\n    test_preds, test_labels, test_seqs = [], [], []\n    with torch.no_grad():\n        for idx, batch in enumerate(test_loader):\n            seq_batch = spr_bench[\"test\"][\"sequence\"][\n                idx * 128 : idx * 128 + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            test_preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            test_labels.extend(batch.y.cpu().tolist())\n            test_seqs.extend(seq_batch)\n    inv_label = {v: k for k, v in label2idx.items()}\n    pred_lbls = [inv_label[p] for p in test_preds]\n    true_lbls = [inv_label[t] for t in test_labels]\n    cwa_t = color_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    swa_t = shape_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    hwa_t = harmonic_weighted_accuracy(cwa_t, swa_t)\n    print(f\"Test results (wd={wd}): CWA={cwa_t:.3f} SWA={swa_t:.3f} HWA={hwa_t:.3f}\")\n    experiment_data[run_key][\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n        \"cwa\": cwa_t,\n        \"swa\": swa_t,\n        \"hwa\": hwa_t,\n    }\n    experiment_data[run_key][\"SPR_BENCH\"][\"predictions\"] = pred_lbls\n    experiment_data[run_key][\"SPR_BENCH\"][\"ground_truth\"] = true_lbls\n\n# ----------------- save all results -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {os.path.join(working_dir,'experiment_data.npy')}\")\n", "import os, pathlib, random, time, math, sys, itertools\nimport numpy as np\nimport torch, warnings\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import Dataset, DatasetDict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment store -----------------\nexperiment_data = {\n    \"num_gnn_layers\": {\"SPR_BENCH\": {}}  # each layer setting will be stored here\n}\n\n# ----------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- metrics helpers -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences):\n    vocab = {tok for seq in sequences for tok in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq, token2idx, label_idx):\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        return [\n            sequence_to_graph(seq, token2idx, label2idx[lab])\n            for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"])\n        ]\n\n    return {s: _convert(s) for s in [\"train\", \"dev\", \"test\"]}, token2idx, label2idx\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes, num_layers=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        convs = []\n        if num_layers >= 1:\n            convs.append(SAGEConv(emb_dim, hidden_dim))\n            for _ in range(num_layers - 1):\n                convs.append(SAGEConv(hidden_dim, hidden_dim))\n        self.convs = nn.ModuleList(convs)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        for conv in self.convs:\n            x = conv(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- load dataset -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    from datasets import load_dataset\n\n    def _ld(f):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(DATA_PATH / f),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    spr_bench = DatasetDict(\n        {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    )\nelse:\n    print(\"Real dataset not found. Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for i in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(300)),\n            \"dev\": Dataset.from_dict(synth(80)),\n            \"test\": Dataset.from_dict(synth(80)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\n# ----------------- hyperparameter search -----------------\nLAYER_CHOICES = [1, 2, 3, 4]\nEPOCHS = 5\nfor n_layers in LAYER_CHOICES:\n    print(f\"\\n===== Training model with {n_layers} GraphSAGE layer(s) =====\")\n    model = GraphClassifier(\n        vocab_size=len(token2idx),\n        emb_dim=32,\n        hidden_dim=64,\n        num_classes=num_classes,\n        num_layers=n_layers,\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    # prepare storage dict\n    exp_rec = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        # ---- training ----\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(graph_sets[\"train\"])\n        exp_rec[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation ----\n        model.eval()\n        val_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for b_idx, batch in enumerate(dev_loader):\n                seqs = spr_bench[\"dev\"][\"sequence\"][\n                    b_idx * 128 : b_idx * 128 + batch.num_graphs\n                ]\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = cross_entropy(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                preds = logits.argmax(dim=-1).cpu().tolist()\n                labs = batch.y.cpu().tolist()\n                all_preds.extend(preds)\n                all_labels.extend(labs)\n                all_seqs.extend(seqs)\n        val_loss /= len(graph_sets[\"dev\"])\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n\n        inv_label = {v: k for k, v in label2idx.items()}\n        pred_lbls = [inv_label[p] for p in all_preds]\n        true_lbls = [inv_label[t] for t in all_labels]\n        cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        exp_rec[\"metrics\"][\"val\"].append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n        exp_rec[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # ---- final test ----\n    test_preds, test_labels, test_seqs = [], [], []\n    model.eval()\n    with torch.no_grad():\n        for b_idx, batch in enumerate(test_loader):\n            seqs = spr_bench[\"test\"][\"sequence\"][\n                b_idx * 128 : b_idx * 128 + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labs = batch.y.cpu().tolist()\n            test_preds.extend(preds)\n            test_labels.extend(labs)\n            test_seqs.extend(seqs)\n    inv_label = {v: k for k, v in label2idx.items()}\n    pred_lbls = [inv_label[p] for p in test_preds]\n    true_lbls = [inv_label[t] for t in test_labels]\n    cwa_t = color_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    swa_t = shape_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    hwa_t = harmonic_weighted_accuracy(cwa_t, swa_t)\n    exp_rec[\"metrics\"][\"test\"] = {\"cwa\": cwa_t, \"swa\": swa_t, \"hwa\": hwa_t}\n    exp_rec[\"predictions\"] = pred_lbls\n    exp_rec[\"ground_truth\"] = true_lbls\n    print(f\"Test CWA={cwa_t:.3f} SWA={swa_t:.3f} HWA={hwa_t:.3f}\")\n\n    # store results\n    experiment_data[\"num_gnn_layers\"][\"SPR_BENCH\"][n_layers] = exp_rec\n\n# ----------------- save -----------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Experiment data saved to {os.path.join(working_dir,'experiment_data.npy')}\")\n", "import os, pathlib, random, time, math, sys, itertools, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- working dir & experiment dict -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"dropout_rate\": {\"SPR_BENCH\": {}}}  # each p will be inserted here\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        return [\n            sequence_to_graph(seq, token2idx, label2idx[lab])\n            for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"])\n        ]\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes, dropout):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.drop(x)\n        x = self.conv2(x, data.edge_index).relu()\n        x = self.drop(x)\n        x = global_mean_pool(x, data.batch)\n        x = self.drop(x)\n        return self.lin(x)\n\n\n# ----------------- data loading -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes, colors = \"ABCD\", \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\ninv_label = {v: k for k, v in label2idx.items()}\n\n# ----------------- hyperparameter grid -----------------\ndropout_grid = [0.0, 0.2, 0.4, 0.6]\nEPOCHS = 5  # small for demo\n\nfor p in dropout_grid:\n    print(f\"\\n=== Training with dropout p={p} ===\")\n    # storage\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(p)] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    # loaders\n    train_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\n    dev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\n    test_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\n    # model / optim\n    model = GraphClassifier(len(token2idx), 32, 64, num_classes, dropout=p).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, EPOCHS + 1):\n        # ---- train ----\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(graph_sets[\"train\"])\n        experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(p)][\"losses\"][\"train\"].append(\n            train_loss\n        )\n\n        # ---- eval(dev) ----\n        model.eval()\n        val_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(dev_loader):\n                seq_batch = spr_bench[\"dev\"][\"sequence\"][\n                    batch_idx * 128 : batch_idx * 128 + batch.num_graphs\n                ]\n                batch = batch.to(device)\n                logits = model(batch)\n                loss = cross_entropy(logits, batch.y)\n                val_loss += loss.item() * batch.num_graphs\n                preds = logits.argmax(dim=-1).cpu().tolist()\n                labels = batch.y.cpu().tolist()\n                all_preds.extend(preds)\n                all_labels.extend(labels)\n                all_seqs.extend(seq_batch)\n        val_loss /= len(graph_sets[\"dev\"])\n        experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(p)][\"losses\"][\"val\"].append(\n            val_loss\n        )\n\n        pred_lbls = [inv_label[i] for i in all_preds]\n        true_lbls = [inv_label[i] for i in all_labels]\n        cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n        hwa = harmonic_weighted_accuracy(cwa, swa)\n        experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(p)][\"metrics\"][\"val\"].append(\n            {\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa}\n        )\n        experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(p)][\"timestamps\"].append(\n            time.time()\n        )\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.3f} val_loss={val_loss:.3f} HWA={hwa:.3f}\"\n        )\n\n    # ---- final test ----\n    model.eval()\n    test_preds, test_labels, test_seqs = [], [], []\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(test_loader):\n            seq_batch = spr_bench[\"test\"][\"sequence\"][\n                batch_idx * 128 : batch_idx * 128 + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            test_preds.extend(preds)\n            test_labels.extend(labels)\n            test_seqs.extend(seq_batch)\n    pred_lbls = [inv_label[i] for i in test_preds]\n    true_lbls = [inv_label[i] for i in test_labels]\n    cwa_test = color_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    swa_test = shape_weighted_accuracy(test_seqs, true_lbls, pred_lbls)\n    hwa_test = harmonic_weighted_accuracy(cwa_test, swa_test)\n    print(\n        f\"Test results (p={p}): CWA={cwa_test:.3f} SWA={swa_test:.3f} HWA={hwa_test:.3f}\"\n    )\n\n    # store\n    exp_slot = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(p)]\n    exp_slot[\"metrics\"][\"test\"] = {\"cwa\": cwa_test, \"swa\": swa_test, \"hwa\": hwa_test}\n    exp_slot[\"predictions\"] = pred_lbls\n    exp_slot[\"ground_truth\"] = true_lbls\n\n# ----------------- save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# ---------------------------------------------------------\n# Hyper-parameter tuning:  E P O C H S\n# ---------------------------------------------------------\nimport os, pathlib, random, time, math, sys, itertools, copy\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment bookkeeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"EPOCHS\": {\"SPR_BENCH\": {}}}  # hyper-parameter family we are tuning\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- dataset loading -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\ninv_label = {v: k for k, v in label2idx.items()}\n\n\n# ----------------- evaluation helper -----------------\ndef evaluate(model, loader, seqs_split):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for idx, batch in enumerate(loader):\n            seq_batch = seqs_split[\n                idx * loader.batch_size : idx * loader.batch_size + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seq_batch)\n    val_loss /= len(loader.dataset)\n    pred_lbls = [inv_label[p] for p in all_preds]\n    true_lbls = [inv_label[t] for t in all_labels]\n    cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    hwa = harmonic_weighted_accuracy(cwa, swa)\n    return val_loss, cwa, swa, hwa, pred_lbls, true_lbls\n\n\n# ----------------- hyper-parameter search -----------------\ncandidate_epochs = [5, 15, 30, 50]  # search grid\nbest_hwa = -1.0\nbest_state = None\nbest_epochs = None\n\nfor EPOCHS in candidate_epochs:\n    print(f\"\\n===== Training for {EPOCHS} epochs =====\")\n    random.seed(0)\n    np.random.seed(0)\n    torch.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    model = GraphClassifier(\n        len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    run_train_losses, run_val_losses, run_val_metrics = [], [], []\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n\n        val_loss, cwa, swa, hwa, _, _ = evaluate(\n            model, dev_loader, spr_bench[\"dev\"][\"sequence\"]\n        )\n\n        run_train_losses.append(train_loss)\n        run_val_losses.append(val_loss)\n        run_val_metrics.append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n\n        print(\n            f\"Epoch {epoch:02d}/{EPOCHS}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # save run data\n    experiment_data[\"EPOCHS\"][\"SPR_BENCH\"][f\"run_{EPOCHS}\"] = {\n        \"losses\": {\"train\": run_train_losses, \"val\": run_val_losses},\n        \"metrics\": {\"val\": run_val_metrics},\n        \"final_val_hwa\": run_val_metrics[-1][\"hwa\"],\n        \"final_val_loss\": run_val_losses[-1],\n    }\n\n    # check for best\n    if run_val_metrics[-1][\"hwa\"] > best_hwa:\n        best_hwa = run_val_metrics[-1][\"hwa\"]\n        best_state = copy.deepcopy(model.state_dict())\n        best_epochs = EPOCHS\n\n    del model\n    torch.cuda.empty_cache()\n\nprint(f\"\\nBest validation HWA={best_hwa:.3f} achieved with {best_epochs} epochs.\")\n\n# ----------------- final evaluation on test -----------------\nbest_model = GraphClassifier(\n    len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n).to(device)\nbest_model.load_state_dict(best_state)\ntest_loss, cwa_test, swa_test, hwa_test, test_preds_lbl, test_true_lbl = evaluate(\n    best_model, test_loader, spr_bench[\"test\"][\"sequence\"]\n)\nprint(f\"TEST  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\")\n\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"best_run\"] = best_epochs\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"metrics_test\"] = {\n    \"cwa\": cwa_test,\n    \"swa\": swa_test,\n    \"hwa\": hwa_test,\n}\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"predictions\"] = test_preds_lbl\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"ground_truth\"] = test_true_lbl\n\n# ----------------- save all experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# ---------------------------------------------------------\n# Hyper-parameter tuning:  E P O C H S\n# ---------------------------------------------------------\nimport os, pathlib, random, time, math, sys, itertools, copy\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment bookkeeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"EPOCHS\": {\"SPR_BENCH\": {}}}  # hyper-parameter family we are tuning\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- dataset loading -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\ninv_label = {v: k for k, v in label2idx.items()}\n\n\n# ----------------- evaluation helper -----------------\ndef evaluate(model, loader, seqs_split):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for idx, batch in enumerate(loader):\n            seq_batch = seqs_split[\n                idx * loader.batch_size : idx * loader.batch_size + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seq_batch)\n    val_loss /= len(loader.dataset)\n    pred_lbls = [inv_label[p] for p in all_preds]\n    true_lbls = [inv_label[t] for t in all_labels]\n    cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    hwa = harmonic_weighted_accuracy(cwa, swa)\n    return val_loss, cwa, swa, hwa, pred_lbls, true_lbls\n\n\n# ----------------- hyper-parameter search -----------------\ncandidate_epochs = [5, 15, 30, 50]  # search grid\nbest_hwa = -1.0\nbest_state = None\nbest_epochs = None\n\nfor EPOCHS in candidate_epochs:\n    print(f\"\\n===== Training for {EPOCHS} epochs =====\")\n    random.seed(0)\n    np.random.seed(0)\n    torch.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    model = GraphClassifier(\n        len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    run_train_losses, run_val_losses, run_val_metrics = [], [], []\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n\n        val_loss, cwa, swa, hwa, _, _ = evaluate(\n            model, dev_loader, spr_bench[\"dev\"][\"sequence\"]\n        )\n\n        run_train_losses.append(train_loss)\n        run_val_losses.append(val_loss)\n        run_val_metrics.append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n\n        print(\n            f\"Epoch {epoch:02d}/{EPOCHS}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # save run data\n    experiment_data[\"EPOCHS\"][\"SPR_BENCH\"][f\"run_{EPOCHS}\"] = {\n        \"losses\": {\"train\": run_train_losses, \"val\": run_val_losses},\n        \"metrics\": {\"val\": run_val_metrics},\n        \"final_val_hwa\": run_val_metrics[-1][\"hwa\"],\n        \"final_val_loss\": run_val_losses[-1],\n    }\n\n    # check for best\n    if run_val_metrics[-1][\"hwa\"] > best_hwa:\n        best_hwa = run_val_metrics[-1][\"hwa\"]\n        best_state = copy.deepcopy(model.state_dict())\n        best_epochs = EPOCHS\n\n    del model\n    torch.cuda.empty_cache()\n\nprint(f\"\\nBest validation HWA={best_hwa:.3f} achieved with {best_epochs} epochs.\")\n\n# ----------------- final evaluation on test -----------------\nbest_model = GraphClassifier(\n    len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n).to(device)\nbest_model.load_state_dict(best_state)\ntest_loss, cwa_test, swa_test, hwa_test, test_preds_lbl, test_true_lbl = evaluate(\n    best_model, test_loader, spr_bench[\"test\"][\"sequence\"]\n)\nprint(f\"TEST  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\")\n\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"best_run\"] = best_epochs\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"metrics_test\"] = {\n    \"cwa\": cwa_test,\n    \"swa\": swa_test,\n    \"hwa\": hwa_test,\n}\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"predictions\"] = test_preds_lbl\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"ground_truth\"] = test_true_lbl\n\n# ----------------- save all experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\n# ---------------------------------------------------------\n# Hyper-parameter tuning:  E P O C H S\n# ---------------------------------------------------------\nimport os, pathlib, random, time, math, sys, itertools, copy\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn.functional import cross_entropy\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv, global_mean_pool\n\n# ----------------- experiment bookkeeping -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"EPOCHS\": {\"SPR_BENCH\": {}}}  # hyper-parameter family we are tuning\n\n# ----------------- device -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper: load dataset -----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ----------------- metrics -----------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(correct) / (sum(w) or 1)\n\n\ndef harmonic_weighted_accuracy(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------- dataset processing -----------------\nUNK = \"<UNK>\"\n\n\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {token for seq in sequences for token in seq.split()}\n    token2idx = {tok: i + 1 for i, tok in enumerate(sorted(vocab))}\n    token2idx[UNK] = 0\n    return token2idx\n\n\ndef sequence_to_graph(seq: str, token2idx: Dict[str, int], label_idx: int) -> Data:\n    tokens = seq.strip().split()\n    n = len(tokens)\n    ids = [token2idx.get(t, token2idx[UNK]) for t in tokens]\n    if n == 1:\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    else:\n        src = torch.arange(0, n - 1, dtype=torch.long)\n        dst = torch.arange(1, n, dtype=torch.long)\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    x = torch.tensor(ids, dtype=torch.long)\n    y = torch.tensor([label_idx], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=y)\n\n\ndef prepare_graph_datasets(spr: DatasetDict):\n    token2idx = build_vocab(spr[\"train\"][\"sequence\"])\n    labels = sorted(set(spr[\"train\"][\"label\"]))\n    label2idx = {l: i for i, l in enumerate(labels)}\n\n    def _convert(split):\n        graphs = []\n        for seq, lab in zip(spr[split][\"sequence\"], spr[split][\"label\"]):\n            graphs.append(sequence_to_graph(seq, token2idx, label2idx[lab]))\n        return graphs\n\n    return (\n        {split: _convert(split) for split in [\"train\", \"dev\", \"test\"]},\n        token2idx,\n        label2idx,\n    )\n\n\n# ----------------- model -----------------\nclass GraphClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim)\n        self.conv1 = SAGEConv(emb_dim, hidden_dim)\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, data):\n        x = self.embed(data.x.squeeze())\n        x = self.conv1(x, data.edge_index).relu()\n        x = self.conv2(x, data.edge_index).relu()\n        x = global_mean_pool(x, data.batch)\n        return self.lin(x)\n\n\n# ----------------- dataset loading -----------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nprint(f\"Looking for data at: {DATA_PATH}\")\nif DATA_PATH.exists():\n    spr_bench = load_spr_bench(DATA_PATH)\nelse:\n    print(\"Real dataset not found.  Using synthetic placeholder.\")\n\n    def synth(n):\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        seqs, labs = [], []\n        for _ in range(n):\n            L = random.randint(4, 8)\n            tokens = [random.choice(shapes) + random.choice(colors) for _ in range(L)]\n            seqs.append(\" \".join(tokens))\n            labs.append(random.choice([\"yes\", \"no\"]))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labs}\n\n    from datasets import Dataset\n\n    spr_bench = DatasetDict(\n        {\n            \"train\": Dataset.from_dict(synth(200)),\n            \"dev\": Dataset.from_dict(synth(50)),\n            \"test\": Dataset.from_dict(synth(50)),\n        }\n    )\n\ngraph_sets, token2idx, label2idx = prepare_graph_datasets(spr_bench)\nnum_classes = len(label2idx)\nprint(f\"Vocab size: {len(token2idx)} | #classes: {num_classes}\")\n\ntrain_loader = DataLoader(graph_sets[\"train\"], batch_size=64, shuffle=True)\ndev_loader = DataLoader(graph_sets[\"dev\"], batch_size=128)\ntest_loader = DataLoader(graph_sets[\"test\"], batch_size=128)\n\ninv_label = {v: k for k, v in label2idx.items()}\n\n\n# ----------------- evaluation helper -----------------\ndef evaluate(model, loader, seqs_split):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for idx, batch in enumerate(loader):\n            seq_batch = seqs_split[\n                idx * loader.batch_size : idx * loader.batch_size + batch.num_graphs\n            ]\n            batch = batch.to(device)\n            logits = model(batch)\n            loss = cross_entropy(logits, batch.y)\n            val_loss += loss.item() * batch.num_graphs\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seq_batch)\n    val_loss /= len(loader.dataset)\n    pred_lbls = [inv_label[p] for p in all_preds]\n    true_lbls = [inv_label[t] for t in all_labels]\n    cwa = color_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    swa = shape_weighted_accuracy(all_seqs, true_lbls, pred_lbls)\n    hwa = harmonic_weighted_accuracy(cwa, swa)\n    return val_loss, cwa, swa, hwa, pred_lbls, true_lbls\n\n\n# ----------------- hyper-parameter search -----------------\ncandidate_epochs = [5, 15, 30, 50]  # search grid\nbest_hwa = -1.0\nbest_state = None\nbest_epochs = None\n\nfor EPOCHS in candidate_epochs:\n    print(f\"\\n===== Training for {EPOCHS} epochs =====\")\n    random.seed(0)\n    np.random.seed(0)\n    torch.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    model = GraphClassifier(\n        len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    run_train_losses, run_val_losses, run_val_metrics = [], [], []\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch)\n            loss = cross_entropy(out, batch.y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch.num_graphs\n        train_loss = total_loss / len(train_loader.dataset)\n\n        val_loss, cwa, swa, hwa, _, _ = evaluate(\n            model, dev_loader, spr_bench[\"dev\"][\"sequence\"]\n        )\n\n        run_train_losses.append(train_loss)\n        run_val_losses.append(val_loss)\n        run_val_metrics.append({\"cwa\": cwa, \"swa\": swa, \"hwa\": hwa})\n\n        print(\n            f\"Epoch {epoch:02d}/{EPOCHS}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"CWA={cwa:.3f} SWA={swa:.3f} HWA={hwa:.3f}\"\n        )\n\n    # save run data\n    experiment_data[\"EPOCHS\"][\"SPR_BENCH\"][f\"run_{EPOCHS}\"] = {\n        \"losses\": {\"train\": run_train_losses, \"val\": run_val_losses},\n        \"metrics\": {\"val\": run_val_metrics},\n        \"final_val_hwa\": run_val_metrics[-1][\"hwa\"],\n        \"final_val_loss\": run_val_losses[-1],\n    }\n\n    # check for best\n    if run_val_metrics[-1][\"hwa\"] > best_hwa:\n        best_hwa = run_val_metrics[-1][\"hwa\"]\n        best_state = copy.deepcopy(model.state_dict())\n        best_epochs = EPOCHS\n\n    del model\n    torch.cuda.empty_cache()\n\nprint(f\"\\nBest validation HWA={best_hwa:.3f} achieved with {best_epochs} epochs.\")\n\n# ----------------- final evaluation on test -----------------\nbest_model = GraphClassifier(\n    len(token2idx), emb_dim=32, hidden_dim=64, num_classes=num_classes\n).to(device)\nbest_model.load_state_dict(best_state)\ntest_loss, cwa_test, swa_test, hwa_test, test_preds_lbl, test_true_lbl = evaluate(\n    best_model, test_loader, spr_bench[\"test\"][\"sequence\"]\n)\nprint(f\"TEST  CWA={cwa_test:.3f}  SWA={swa_test:.3f}  HWA={hwa_test:.3f}\")\n\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"best_run\"] = best_epochs\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"metrics_test\"] = {\n    \"cwa\": cwa_test,\n    \"swa\": swa_test,\n    \"hwa\": hwa_test,\n}\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"predictions\"] = test_preds_lbl\nexperiment_data[\"EPOCHS\"][\"SPR_BENCH\"][\"ground_truth\"] = test_true_lbl\n\n# ----------------- save all experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '\\rGenerating train split: 20000 examples [00:00, 187370.35\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 185237.05\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 464753.12\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 692357.87\nexamples/s]', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', 'Epoch 1:\ntrain_loss=0.4856  val_loss=0.3570  CWA=0.854 SWA=0.853 HWA=0.853', '\\n', 'Epoch\n2: train_loss=0.3195  val_loss=0.2876  CWA=0.895 SWA=0.894 HWA=0.894', '\\n',\n'Epoch 3: train_loss=0.2754  val_loss=0.2609  CWA=0.914 SWA=0.913 HWA=0.914',\n'\\n', 'Epoch 4: train_loss=0.2555  val_loss=0.2486  CWA=0.920 SWA=0.919\nHWA=0.919', '\\n', 'Epoch 5: train_loss=0.2416  val_loss=0.2341  CWA=0.928\nSWA=0.927 HWA=0.928', '\\n', 'Test  CWA=0.672  SWA=0.630  HWA=0.651', '\\n',\n'Experiment data saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n30_23-24-25_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n2/working/experiment_data.npy', '\\n', 'Execution time: 13 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 627542.23\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 528369.66\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 661092.92\nexamples/s]', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n===== Training for\n5 epochs =====', '\\n', 'Epoch 01/5: train_loss=0.5012 val_loss=0.3863 CWA=0.831\nSWA=0.829 HWA=0.830', '\\n', 'Epoch 02/5: train_loss=0.3270 val_loss=0.3046\nCWA=0.890 SWA=0.891 HWA=0.890', '\\n', 'Epoch 03/5: train_loss=0.2823\nval_loss=0.2848 CWA=0.911 SWA=0.913 HWA=0.912', '\\n', 'Epoch 04/5:\ntrain_loss=0.2624 val_loss=0.2872 CWA=0.889 SWA=0.885 HWA=0.887', '\\n', 'Epoch\n05/5: train_loss=0.2475 val_loss=0.2406 CWA=0.927 SWA=0.925 HWA=0.926', '\\n',\n'\\n===== Training for 15 epochs =====', '\\n', 'Epoch 01/15: train_loss=0.5012\nval_loss=0.3863 CWA=0.831 SWA=0.829 HWA=0.830', '\\n', 'Epoch 02/15:\ntrain_loss=0.3269 val_loss=0.3044 CWA=0.890 SWA=0.891 HWA=0.890', '\\n', 'Epoch\n03/15: train_loss=0.2822 val_loss=0.2842 CWA=0.911 SWA=0.913 HWA=0.912', '\\n',\n'Epoch 04/15: train_loss=0.2625 val_loss=0.2872 CWA=0.889 SWA=0.885 HWA=0.887',\n'\\n', 'Epoch 05/15: train_loss=0.2476 val_loss=0.2410 CWA=0.927 SWA=0.925\nHWA=0.926', '\\n', 'Epoch 06/15: train_loss=0.2403 val_loss=0.2885 CWA=0.877\nSWA=0.871 HWA=0.874', '\\n', 'Epoch 07/15: train_loss=0.2283 val_loss=0.2248\nCWA=0.932 SWA=0.931 HWA=0.931', '\\n', 'Epoch 08/15: train_loss=0.2154\nval_loss=0.2302 CWA=0.933 SWA=0.933 HWA=0.933', '\\n', 'Epoch 09/15:\ntrain_loss=0.2098 val_loss=0.2126 CWA=0.942 SWA=0.942 HWA=0.942', '\\n', 'Epoch\n10/15: train_loss=0.1998 val_loss=0.2069 CWA=0.941 SWA=0.941 HWA=0.941', '\\n',\n'Epoch 11/15: train_loss=0.1954 val_loss=0.2030 CWA=0.943 SWA=0.943 HWA=0.943',\n'\\n', 'Epoch 12/15: train_loss=0.1914 val_loss=0.1972 CWA=0.940 SWA=0.938\nHWA=0.939', '\\n', 'Epoch 13/15: train_loss=0.1853 val_loss=0.2142 CWA=0.938\nSWA=0.938 HWA=0.938', '\\n', 'Epoch 14/15: train_loss=0.1808 val_loss=0.1950\nCWA=0.948 SWA=0.947 HWA=0.948', '\\n', 'Epoch 15/15: train_loss=0.1763\nval_loss=0.1880 CWA=0.950 SWA=0.949 HWA=0.950', '\\n', '\\n===== Training for 30\nepochs =====', '\\n', 'Epoch 01/30: train_loss=0.5012 val_loss=0.3863 CWA=0.831\nSWA=0.829 HWA=0.830', '\\n', 'Epoch 02/30: train_loss=0.3270 val_loss=0.3046\nCWA=0.890 SWA=0.891 HWA=0.891', '\\n', 'Epoch 03/30: train_loss=0.2822\nval_loss=0.2843 CWA=0.912 SWA=0.914 HWA=0.913', '\\n', 'Epoch 04/30:\ntrain_loss=0.2623 val_loss=0.2878 CWA=0.889 SWA=0.886 HWA=0.888', '\\n', 'Epoch\n05/30: train_loss=0.2474 val_loss=0.2412 CWA=0.927 SWA=0.925 HWA=0.926', '\\n',\n'Epoch 06/30: train_loss=0.2407 val_loss=0.2889 CWA=0.879 SWA=0.872 HWA=0.875',\n'\\n', 'Epoch 07/30: train_loss=0.2284 val_loss=0.2257 CWA=0.931 SWA=0.930\nHWA=0.931', '\\n', 'Epoch 08/30: train_loss=0.2155 val_loss=0.2292 CWA=0.934\nSWA=0.935 HWA=0.935', '\\n', 'Epoch 09/30: train_loss=0.2099 val_loss=0.2126\nCWA=0.941 SWA=0.941 HWA=0.941', '\\n', 'Epoch 10/30: train_loss=0.2001\nval_loss=0.2077 CWA=0.941 SWA=0.941 HWA=0.941', '\\n', 'Epoch 11/30:\ntrain_loss=0.1954 val_loss=0.2026 CWA=0.943 SWA=0.942 HWA=0.942', '\\n', 'Epoch\n12/30: train_loss=0.1913 val_loss=0.1966 CWA=0.942 SWA=0.940 HWA=0.941', '\\n',\n'Epoch 13/30: train_loss=0.1851 val_loss=0.2143 CWA=0.937 SWA=0.937 HWA=0.937',\n'\\n', 'Epoch 14/30: train_loss=0.1807 val_loss=0.1947 CWA=0.948 SWA=0.947\nHWA=0.948', '\\n', 'Epoch 15/30: train_loss=0.1759 val_loss=0.1891 CWA=0.948\nSWA=0.948 HWA=0.948', '\\n', 'Epoch 16/30: train_loss=0.1732 val_loss=0.1906\nCWA=0.950 SWA=0.949 HWA=0.950', '\\n', 'Epoch 17/30: train_loss=0.1687\nval_loss=0.1797 CWA=0.955 SWA=0.954 HWA=0.954', '\\n', 'Epoch 18/30:\ntrain_loss=0.1647 val_loss=0.1817 CWA=0.953 SWA=0.953 HWA=0.953', '\\n', 'Epoch\n19/30: train_loss=0.1602 val_loss=0.1795 CWA=0.952 SWA=0.951 HWA=0.952', '\\n',\n'Epoch 20/30: train_loss=0.1597 val_loss=0.1782 CWA=0.954 SWA=0.953 HWA=0.954',\n'\\n', 'Epoch 21/30: train_loss=0.1590 val_loss=0.1905 CWA=0.950 SWA=0.950\nHWA=0.950', '\\n', 'Epoch 22/30: train_loss=0.1566 val_loss=0.1748 CWA=0.955\nSWA=0.955 HWA=0.955', '\\n', 'Epoch 23/30: train_loss=0.1513 val_loss=0.1843\nCWA=0.953 SWA=0.953 HWA=0.953', '\\n', 'Epoch 24/30: train_loss=0.1486\nval_loss=0.1784 CWA=0.950 SWA=0.947 HWA=0.949', '\\n', 'Epoch 25/30:\ntrain_loss=0.1463 val_loss=0.1699 CWA=0.952 SWA=0.949 HWA=0.951', '\\n', 'Epoch\n26/30: train_loss=0.1443 val_loss=0.1821 CWA=0.948 SWA=0.944 HWA=0.946', '\\n',\n'Epoch 27/30: train_loss=0.1425 val_loss=0.1678 CWA=0.954 SWA=0.953 HWA=0.953',\n'\\n', 'Epoch 28/30: train_loss=0.1399 val_loss=0.1830 CWA=0.944 SWA=0.940\nHWA=0.942', '\\n', 'Epoch 29/30: train_loss=0.1388 val_loss=0.1780 CWA=0.956\nSWA=0.956 HWA=0.956', '\\n', 'Epoch 30/30: train_loss=0.1361 val_loss=0.1741\nCWA=0.956 SWA=0.956 HWA=0.956', '\\n', '\\n===== Training for 50 epochs =====',\n'\\n', 'Epoch 01/50: train_loss=0.5012 val_loss=0.3863 CWA=0.831 SWA=0.829\nHWA=0.830', '\\n', 'Epoch 02/50: train_loss=0.3269 val_loss=0.3045 CWA=0.890\nSWA=0.891 HWA=0.890', '\\n', 'Epoch 03/50: train_loss=0.2821 val_loss=0.2844\nCWA=0.912 SWA=0.914 HWA=0.913', '\\n', 'Epoch 04/50: train_loss=0.2623\nval_loss=0.2872 CWA=0.889 SWA=0.885 HWA=0.887', '\\n', 'Epoch 05/50:\ntrain_loss=0.2473 val_loss=0.2407 CWA=0.927 SWA=0.925 HWA=0.926', '\\n', 'Epoch\n06/50: train_loss=0.2404 val_loss=0.2877 CWA=0.878 SWA=0.872 HWA=0.875', '\\n',\n'Epoch 07/50: train_loss=0.2282 val_loss=0.2248 CWA=0.931 SWA=0.931 HWA=0.931',\n'\\n', 'Epoch 08/50: train_loss=0.2155 val_loss=0.2284 CWA=0.934 SWA=0.935\nHWA=0.935', '\\n', 'Epoch 09/50: train_loss=0.2096 val_loss=0.2118 CWA=0.942\nSWA=0.942 HWA=0.942', '\\n', 'Epoch 10/50: train_loss=0.1998 val_loss=0.2070\nCWA=0.942 SWA=0.942 HWA=0.942', '\\n', 'Epoch 11/50: train_loss=0.1955\nval_loss=0.2024 CWA=0.945 SWA=0.944 HWA=0.945', '\\n', 'Epoch 12/50:\ntrain_loss=0.1915 val_loss=0.1965 CWA=0.941 SWA=0.939 HWA=0.940', '\\n', 'Epoch\n13/50: train_loss=0.1853 val_loss=0.2154 CWA=0.938 SWA=0.938 HWA=0.938', '\\n',\n'Epoch 14/50: train_loss=0.1812 val_loss=0.1948 CWA=0.947 SWA=0.947 HWA=0.947',\n'\\n', 'Epoch 15/50: train_loss=0.1765 val_loss=0.1871 CWA=0.950 SWA=0.950\nHWA=0.950', '\\n', 'Epoch 16/50: train_loss=0.1739 val_loss=0.1919 CWA=0.950\nSWA=0.949 HWA=0.950', '\\n', 'Epoch 17/50: train_loss=0.1695 val_loss=0.1798\nCWA=0.954 SWA=0.954 HWA=0.954', '\\n', 'Epoch 18/50: train_loss=0.1659\nval_loss=0.1809 CWA=0.953 SWA=0.953 HWA=0.953', '\\n', 'Epoch 19/50:\ntrain_loss=0.1608 val_loss=0.1793 CWA=0.953 SWA=0.952 HWA=0.952', '\\n', 'Epoch\n20/50: train_loss=0.1605 val_loss=0.1791 CWA=0.955 SWA=0.955 HWA=0.955', '\\n',\n'Epoch 21/50: train_loss=0.1594 val_loss=0.1903 CWA=0.950 SWA=0.950 HWA=0.950',\n'\\n', 'Epoch 22/50: train_loss=0.1576 val_loss=0.1739 CWA=0.956 SWA=0.955\nHWA=0.955', '\\n', 'Epoch 23/50: train_loss=0.1520 val_loss=0.1825 CWA=0.953\nSWA=0.953 HWA=0.953', '\\n', 'Epoch 24/50: train_loss=0.1492 val_loss=0.1778\nCWA=0.953 SWA=0.950 HWA=0.952', '\\n', 'Epoch 25/50: train_loss=0.1471\nval_loss=0.1694 CWA=0.954 SWA=0.952 HWA=0.953', '\\n', 'Epoch 26/50:\ntrain_loss=0.1447 val_loss=0.1777 CWA=0.951 SWA=0.948 HWA=0.949', '\\n', 'Epoch\n27/50: train_loss=0.1431 val_loss=0.1717 CWA=0.955 SWA=0.954 HWA=0.954', '\\n',\n'Epoch 28/50: train_loss=0.1418 val_loss=0.1814 CWA=0.944 SWA=0.940 HWA=0.942',\n'\\n', 'Epoch 29/50: train_loss=0.1395 val_loss=0.1736 CWA=0.960 SWA=0.959\nHWA=0.959', '\\n', 'Epoch 30/50: train_loss=0.1366 val_loss=0.1749 CWA=0.955\nSWA=0.954 HWA=0.955', '\\n', 'Epoch 31/50: train_loss=0.1344 val_loss=0.1730\nCWA=0.958 SWA=0.957 HWA=0.957', '\\n', 'Epoch 32/50: train_loss=0.1341\nval_loss=0.1602 CWA=0.962 SWA=0.961 HWA=0.962', '\\n', 'Epoch 33/50:\ntrain_loss=0.1329 val_loss=0.1566 CWA=0.961 SWA=0.960 HWA=0.961', '\\n', 'Epoch\n34/50: train_loss=0.1307 val_loss=0.1582 CWA=0.960 SWA=0.958 HWA=0.959', '\\n',\n'Epoch 35/50: train_loss=0.1292 val_loss=0.1570 CWA=0.962 SWA=0.961 HWA=0.962',\n'\\n', 'Epoch 36/50: train_loss=0.1239 val_loss=0.1552 CWA=0.961 SWA=0.961\nHWA=0.961', '\\n', 'Epoch 37/50: train_loss=0.1285 val_loss=0.1650 CWA=0.961\nSWA=0.960 HWA=0.960', '\\n', 'Epoch 38/50: train_loss=0.1239 val_loss=0.2438\nCWA=0.899 SWA=0.892 HWA=0.896', '\\n', 'Epoch 39/50: train_loss=0.1241\nval_loss=0.1609 CWA=0.955 SWA=0.954 HWA=0.954', '\\n', 'Epoch 40/50:\ntrain_loss=0.1231 val_loss=0.1557 CWA=0.961 SWA=0.958 HWA=0.959', '\\n', 'Epoch\n41/50: train_loss=0.1222 val_loss=0.1694 CWA=0.949 SWA=0.946 HWA=0.947', '\\n',\n'Epoch 42/50: train_loss=0.1182 val_loss=0.1548 CWA=0.963 SWA=0.962 HWA=0.963',\n'\\n', 'Epoch 43/50: train_loss=0.1172 val_loss=0.1547 CWA=0.963 SWA=0.962\nHWA=0.962', '\\n', 'Epoch 44/50: train_loss=0.1164 val_loss=0.1534 CWA=0.962\nSWA=0.961 HWA=0.961', '\\n', 'Epoch 45/50: train_loss=0.1169 val_loss=0.1570\nCWA=0.957 SWA=0.956 HWA=0.957', '\\n', 'Epoch 46/50: train_loss=0.1129\nval_loss=0.1528 CWA=0.964 SWA=0.963 HWA=0.963', '\\n', 'Epoch 47/50:\ntrain_loss=0.1106 val_loss=0.1520 CWA=0.965 SWA=0.963 HWA=0.964', '\\n', 'Epoch\n48/50: train_loss=0.1135 val_loss=0.1626 CWA=0.951 SWA=0.951 HWA=0.951', '\\n',\n'Epoch 49/50: train_loss=0.1134 val_loss=0.1589 CWA=0.954 SWA=0.952 HWA=0.953',\n'\\n', 'Epoch 50/50: train_loss=0.1124 val_loss=0.1793 CWA=0.953 SWA=0.953\nHWA=0.953', '\\n', '\\nBest validation HWA=0.956 achieved with 30 epochs.', '\\n',\n'TEST  CWA=0.682  SWA=0.638  HWA=0.659', '\\n', 'Experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_23-24-\n25_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 8 minutes seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 484571.32\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 587997.53\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 328421.51\nexamples/s]', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '[lr=0.0003] Epoch 1:\ntrain_loss=0.5779 val_loss=0.5061 HWA=0.767', '\\n', '[lr=0.0003] Epoch 2:\ntrain_loss=0.4541 val_loss=0.4086 HWA=0.820', '\\n', '[lr=0.0003] Epoch 3:\ntrain_loss=0.3706 val_loss=0.3454 HWA=0.868', '\\n', '[lr=0.0003] Epoch 4:\ntrain_loss=0.3267 val_loss=0.3168 HWA=0.885', '\\n', '[lr=0.0003] Epoch 5:\ntrain_loss=0.3008 val_loss=0.3018 HWA=0.895', '\\n', '[lr=0.0005] Epoch 1:\ntrain_loss=0.5355 val_loss=0.4361 HWA=0.813', '\\n', '[lr=0.0005] Epoch 2:\ntrain_loss=0.3816 val_loss=0.3463 HWA=0.870', '\\n', '[lr=0.0005] Epoch 3:\ntrain_loss=0.3199 val_loss=0.3069 HWA=0.899', '\\n', '[lr=0.0005] Epoch 4:\ntrain_loss=0.2913 val_loss=0.2866 HWA=0.904', '\\n', '[lr=0.0005] Epoch 5:\ntrain_loss=0.2763 val_loss=0.2767 HWA=0.915', '\\n', '[lr=0.001] Epoch 1:\ntrain_loss=0.4759 val_loss=0.3644 HWA=0.845', '\\n', '[lr=0.001] Epoch 2:\ntrain_loss=0.3125 val_loss=0.2995 HWA=0.900', '\\n', '[lr=0.001] Epoch 3:\ntrain_loss=0.2755 val_loss=0.2638 HWA=0.917', '\\n', '[lr=0.001] Epoch 4:\ntrain_loss=0.2569 val_loss=0.2685 HWA=0.918', '\\n', '[lr=0.001] Epoch 5:\ntrain_loss=0.2421 val_loss=0.2483 HWA=0.914', '\\n', '[lr=0.002] Epoch 1:\ntrain_loss=0.4252 val_loss=0.3018 HWA=0.889', '\\n', '[lr=0.002] Epoch 2:\ntrain_loss=0.2843 val_loss=0.2540 HWA=0.913', '\\n', '[lr=0.002] Epoch 3:\ntrain_loss=0.2468 val_loss=0.3203 HWA=0.898', '\\n', '[lr=0.002] Epoch 4:\ntrain_loss=0.2275 val_loss=0.2185 HWA=0.932', '\\n', '[lr=0.002] Epoch 5:\ntrain_loss=0.2093 val_loss=0.2012 HWA=0.944', '\\n', 'Best lr according to dev\nHWA: 0.002 (HWA=0.944)', '\\n', 'Test  CWA=0.676  SWA=0.633  HWA=0.654', '\\n',\n'Data saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_23-24-\n25_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 39 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 532775.78\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 548877.72\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 601609.91\nexamples/s]', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n=== Training with\nbatch size 32 ===', '\\n', 'Epoch 1: train_loss=0.4359  val_loss=0.3215\nCWA=0.866 SWA=0.863 HWA=0.864', '\\n', 'Epoch 2: train_loss=0.2943\nval_loss=0.2668  CWA=0.912 SWA=0.912 HWA=0.912', '\\n', 'Epoch 3:\ntrain_loss=0.2588  val_loss=0.2469  CWA=0.919 SWA=0.918 HWA=0.918', '\\n', 'Epoch\n4: train_loss=0.2404  val_loss=0.2369  CWA=0.924 SWA=0.923 HWA=0.924', '\\n',\n'Epoch 5: train_loss=0.2242  val_loss=0.2153  CWA=0.940 SWA=0.940 HWA=0.940',\n'\\n', 'Test results (bs=32)  CWA=0.676  SWA=0.633  HWA=0.653', '\\n', '\\n===\nTraining with batch size 64 ===', '\\n', 'Epoch 1: train_loss=0.4724\nval_loss=0.3630  CWA=0.848 SWA=0.848 HWA=0.848', '\\n', 'Epoch 2:\ntrain_loss=0.3285  val_loss=0.3177  CWA=0.889 SWA=0.891 HWA=0.890', '\\n', 'Epoch\n3: train_loss=0.2939  val_loss=0.2976  CWA=0.901 SWA=0.903 HWA=0.902', '\\n',\n'Epoch 4: train_loss=0.2721  val_loss=0.2704  CWA=0.910 SWA=0.910 HWA=0.910',\n'\\n', 'Epoch 5: train_loss=0.2602  val_loss=0.2557  CWA=0.917 SWA=0.916\nHWA=0.916', '\\n', 'Test results (bs=64)  CWA=0.670  SWA=0.628  HWA=0.648', '\\n',\n'\\n=== Training with batch size 128 ===', '\\n', 'Epoch 1: train_loss=0.5301\nval_loss=0.4200  CWA=0.818 SWA=0.819 HWA=0.818', '\\n', 'Epoch 2:\ntrain_loss=0.3490  val_loss=0.3150  CWA=0.880 SWA=0.880 HWA=0.880', '\\n', 'Epoch\n3: train_loss=0.2959  val_loss=0.3092  CWA=0.897 SWA=0.900 HWA=0.899', '\\n',\n'Epoch 4: train_loss=0.2764  val_loss=0.2719  CWA=0.910 SWA=0.910 HWA=0.910',\n'\\n', 'Epoch 5: train_loss=0.2617  val_loss=0.2643  CWA=0.909 SWA=0.906\nHWA=0.908', '\\n', 'Test results (bs=128)  CWA=0.675  SWA=0.632  HWA=0.653',\n'\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n36 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 495666.37\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 506374.98\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 625810.03\nexamples/s]', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n==== Training with\nemb_dim=16 ====', '\\n', 'Epoch 1 | train_loss=0.5275 | val_loss=0.4438 |\nHWA=0.797', '\\n', 'Epoch 2 | train_loss=0.4083 | val_loss=0.3714 | HWA=0.844',\n'\\n', 'Epoch 3 | train_loss=0.3361 | val_loss=0.3154 | HWA=0.892', '\\n', 'Epoch\n4 | train_loss=0.3025 | val_loss=0.2937 | HWA=0.893', '\\n', 'Epoch 5 |\ntrain_loss=0.2795 | val_loss=0.2872 | HWA=0.888', '\\n', 'Test results for\nemb_dim=16: CWA=0.670  SWA=0.629  HWA=0.649', '\\n', '\\n==== Training with\nemb_dim=32 ====', '\\n', 'Epoch 1 | train_loss=0.4840 | val_loss=0.3678 |\nHWA=0.862', '\\n', 'Epoch 2 | train_loss=0.3217 | val_loss=0.3031 | HWA=0.895',\n'\\n', 'Epoch 3 | train_loss=0.2838 | val_loss=0.2748 | HWA=0.914', '\\n', 'Epoch\n4 | train_loss=0.2589 | val_loss=0.2868 | HWA=0.905', '\\n', 'Epoch 5 |\ntrain_loss=0.2435 | val_loss=0.2394 | HWA=0.926', '\\n', 'Test results for\nemb_dim=32: CWA=0.671  SWA=0.628  HWA=0.649', '\\n', '\\n==== Training with\nemb_dim=64 ====', '\\n', 'Epoch 1 | train_loss=0.4529 | val_loss=0.3210 |\nHWA=0.887', '\\n', 'Epoch 2 | train_loss=0.2989 | val_loss=0.2759 | HWA=0.913',\n'\\n', 'Epoch 3 | train_loss=0.2608 | val_loss=0.2536 | HWA=0.927', '\\n', 'Epoch\n4 | train_loss=0.2406 | val_loss=0.2304 | HWA=0.931', '\\n', 'Epoch 5 |\ntrain_loss=0.2276 | val_loss=0.2175 | HWA=0.937', '\\n', 'Test results for\nemb_dim=64: CWA=0.675  SWA=0.632  HWA=0.652', '\\n', '\\n==== Training with\nemb_dim=128 ====', '\\n', 'Epoch 1 | train_loss=0.4103 | val_loss=0.3001 |\nHWA=0.896', '\\n', 'Epoch 2 | train_loss=0.2745 | val_loss=0.2565 | HWA=0.923',\n'\\n', 'Epoch 3 | train_loss=0.2438 | val_loss=0.2527 | HWA=0.924', '\\n', 'Epoch\n4 | train_loss=0.2250 | val_loss=0.2313 | HWA=0.931', '\\n', 'Epoch 5 |\ntrain_loss=0.2091 | val_loss=0.2022 | HWA=0.942', '\\n', 'Test results for\nemb_dim=128: CWA=0.679  SWA=0.636  HWA=0.656', '\\n', 'All experiment data saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_23-24-\n25_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Vocab size 17, #classes 2', '\\n', '\\n---\nTraining with hidden_dim=32 ---', '\\n', 'Traceback (most recent call last):\\n\nFile \"runfile.py\", line 219, in <module>\\n    val_loss, (preds, labs, seqs) =\nrun_epoch(model, dev_loader, None)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 168, in\nrun_epoch\\n    seq_batch = seq_provider(loader.dataset_split, batch_idx,\nbatch.num_graphs)\\n\n^^^^^^^^^^^^^^^^^^^^\\nAttributeError: \\'DataLoader\\' object has no attribute\n\\'dataset_split\\'\\n', 'Execution time: 11 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n=== Training run: weight_decay=0 ===', '\\n',\n'Epoch 1: train_loss=0.4856 val_loss=0.3570 CWA=0.853 SWA=0.852 HWA=0.853',\n'\\n', 'Epoch 2: train_loss=0.3194 val_loss=0.2875 CWA=0.894 SWA=0.893\nHWA=0.894', '\\n', 'Epoch 3: train_loss=0.2751 val_loss=0.2606 CWA=0.913\nSWA=0.912 HWA=0.913', '\\n', 'Epoch 4: train_loss=0.2551 val_loss=0.2487\nCWA=0.920 SWA=0.918 HWA=0.919', '\\n', 'Epoch 5: train_loss=0.2412\nval_loss=0.2339 CWA=0.928 SWA=0.928 HWA=0.928', '\\n', 'Test results (wd=0):\nCWA=0.673 SWA=0.630 HWA=0.651', '\\n', '\\n=== Training run: weight_decay=0.0001\n===', '\\n', 'Epoch 1: train_loss=0.4729 val_loss=0.3638 CWA=0.847 SWA=0.846\nHWA=0.846', '\\n', 'Epoch 2: train_loss=0.3290 val_loss=0.3187 CWA=0.889\nSWA=0.890 HWA=0.890', '\\n', 'Epoch 3: train_loss=0.2947 val_loss=0.2995\nCWA=0.900 SWA=0.902 HWA=0.901', '\\n', 'Epoch 4: train_loss=0.2731\nval_loss=0.2707 CWA=0.911 SWA=0.910 HWA=0.910', '\\n', 'Epoch 5:\ntrain_loss=0.2615 val_loss=0.2567 CWA=0.917 SWA=0.915 HWA=0.916', '\\n', 'Test\nresults (wd=0.0001): CWA=0.671 SWA=0.629 HWA=0.649', '\\n', '\\n=== Training run:\nweight_decay=0.0005 ===', '\\n', 'Epoch 1: train_loss=0.4719 val_loss=0.3514\nCWA=0.868 SWA=0.871 HWA=0.870', '\\n', 'Epoch 2: train_loss=0.3111\nval_loss=0.2997 CWA=0.900 SWA=0.902 HWA=0.901', '\\n', 'Epoch 3:\ntrain_loss=0.2796 val_loss=0.2923 CWA=0.904 SWA=0.907 HWA=0.906', '\\n', 'Epoch\n4: train_loss=0.2604 val_loss=0.2540 CWA=0.915 SWA=0.913 HWA=0.914', '\\n',\n'Epoch 5: train_loss=0.2463 val_loss=0.2470 CWA=0.920 SWA=0.917 HWA=0.919',\n'\\n', 'Test results (wd=0.0005): CWA=0.676 SWA=0.634 HWA=0.654', '\\n', '\\n===\nTraining run: weight_decay=0.001 ===', '\\n', 'Epoch 1: train_loss=0.4743\nval_loss=0.3544 CWA=0.850 SWA=0.848 HWA=0.849', '\\n', 'Epoch 2:\ntrain_loss=0.3138 val_loss=0.2971 CWA=0.902 SWA=0.904 HWA=0.903', '\\n', 'Epoch\n3: train_loss=0.2807 val_loss=0.2688 CWA=0.910 SWA=0.909 HWA=0.909', '\\n',\n'Epoch 4: train_loss=0.2658 val_loss=0.2560 CWA=0.918 SWA=0.917 HWA=0.918',\n'\\n', 'Epoch 5: train_loss=0.2528 val_loss=0.2504 CWA=0.924 SWA=0.925\nHWA=0.925', '\\n', 'Test results (wd=0.001): CWA=0.666 SWA=0.624 HWA=0.644',\n'\\n', '\\n=== Training run: weight_decay=0.01 ===', '\\n', 'Epoch 1:\ntrain_loss=0.4997 val_loss=0.3935 CWA=0.842 SWA=0.845 HWA=0.843', '\\n', 'Epoch\n2: train_loss=0.3694 val_loss=0.3557 CWA=0.881 SWA=0.885 HWA=0.883', '\\n',\n'Epoch 3: train_loss=0.3461 val_loss=0.3625 CWA=0.878 SWA=0.883 HWA=0.881',\n'\\n', 'Epoch 4: train_loss=0.3416 val_loss=0.3437 CWA=0.889 SWA=0.892\nHWA=0.891', '\\n', 'Epoch 5: train_loss=0.3447 val_loss=0.3428 CWA=0.881\nSWA=0.882 HWA=0.882', '\\n', 'Test results (wd=0.01): CWA=0.659 SWA=0.621\nHWA=0.639', '\\n', 'Saved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n51 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n=====\nTraining model with 1 GraphSAGE layer(s) =====', '\\n', 'Epoch 1/5 |\ntrain_loss=0.5061 val_loss=0.4165 CWA=0.820 SWA=0.818 HWA=0.819', '\\n', 'Epoch\n2/5 | train_loss=0.3578 val_loss=0.3317 CWA=0.876 SWA=0.877 HWA=0.877', '\\n',\n'Epoch 3/5 | train_loss=0.3138 val_loss=0.3185 CWA=0.891 SWA=0.893 HWA=0.892',\n'\\n', 'Epoch 4/5 | train_loss=0.2961 val_loss=0.2945 CWA=0.897 SWA=0.897\nHWA=0.897', '\\n', 'Epoch 5/5 | train_loss=0.2854 val_loss=0.2886 CWA=0.900\nSWA=0.901 HWA=0.901', '\\n', 'Test CWA=0.659 SWA=0.619 HWA=0.638', '\\n', '\\n=====\nTraining model with 2 GraphSAGE layer(s) =====', '\\n', 'Epoch 1/5 |\ntrain_loss=0.4676 val_loss=0.3425 CWA=0.865 SWA=0.867 HWA=0.866', '\\n', 'Epoch\n2/5 | train_loss=0.3174 val_loss=0.3012 CWA=0.903 SWA=0.905 HWA=0.904', '\\n',\n'Epoch 3/5 | train_loss=0.2836 val_loss=0.2886 CWA=0.887 SWA=0.880 HWA=0.883',\n'\\n', 'Epoch 4/5 | train_loss=0.2627 val_loss=0.2596 CWA=0.911 SWA=0.909\nHWA=0.910', '\\n', 'Epoch 5/5 | train_loss=0.2482 val_loss=0.2472 CWA=0.916\nSWA=0.912 HWA=0.914', '\\n', 'Test CWA=0.677 SWA=0.634 HWA=0.655', '\\n', '\\n=====\nTraining model with 3 GraphSAGE layer(s) =====', '\\n', 'Epoch 1/5 |\ntrain_loss=0.4650 val_loss=0.3303 CWA=0.861 SWA=0.859 HWA=0.860', '\\n', 'Epoch\n2/5 | train_loss=0.2853 val_loss=0.2578 CWA=0.920 SWA=0.921 HWA=0.920', '\\n',\n'Epoch 3/5 | train_loss=0.2396 val_loss=0.2619 CWA=0.921 SWA=0.923 HWA=0.922',\n'\\n', 'Epoch 4/5 | train_loss=0.2129 val_loss=0.2036 CWA=0.944 SWA=0.943\nHWA=0.944', '\\n', 'Epoch 5/5 | train_loss=0.1909 val_loss=0.1900 CWA=0.947\nSWA=0.946 HWA=0.947', '\\n', 'Test CWA=0.682 SWA=0.638 HWA=0.659', '\\n', '\\n=====\nTraining model with 4 GraphSAGE layer(s) =====', '\\n', 'Epoch 1/5 |\ntrain_loss=0.4590 val_loss=0.2799 CWA=0.900 SWA=0.898 HWA=0.899', '\\n', 'Epoch\n2/5 | train_loss=0.2525 val_loss=0.2210 CWA=0.935 SWA=0.936 HWA=0.936', '\\n',\n'Epoch 3/5 | train_loss=0.1956 val_loss=0.2039 CWA=0.944 SWA=0.944 HWA=0.944',\n'\\n', 'Epoch 4/5 | train_loss=0.1700 val_loss=0.1638 CWA=0.955 SWA=0.954\nHWA=0.954', '\\n', 'Epoch 5/5 | train_loss=0.1508 val_loss=0.2220 CWA=0.938\nSWA=0.937 HWA=0.938', '\\n', 'Test CWA=0.674 SWA=0.631 HWA=0.652', '\\n',\n'Experiment data saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n30_23-24-25_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 44 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n===\nTraining with dropout p=0.0 ===', '\\n', 'Epoch 1: train_loss=0.486\nval_loss=0.357 HWA=0.853', '\\n', 'Epoch 2: train_loss=0.319 val_loss=0.288\nHWA=0.894', '\\n', 'Epoch 3: train_loss=0.275 val_loss=0.261 HWA=0.913', '\\n',\n'Epoch 4: train_loss=0.255 val_loss=0.249 HWA=0.919', '\\n', 'Epoch 5:\ntrain_loss=0.241 val_loss=0.234 HWA=0.928', '\\n', 'Test results (p=0.0):\nCWA=0.673 SWA=0.630 HWA=0.651', '\\n', '\\n=== Training with dropout p=0.2 ===',\n'\\n', 'Epoch 1: train_loss=0.522 val_loss=0.401 HWA=0.833', '\\n', 'Epoch 2:\ntrain_loss=0.406 val_loss=0.377 HWA=0.852', '\\n', 'Epoch 3: train_loss=0.358\nval_loss=0.304 HWA=0.896', '\\n', 'Epoch 4: train_loss=0.337 val_loss=0.292\nHWA=0.900', '\\n', 'Epoch 5: train_loss=0.325 val_loss=0.273 HWA=0.911', '\\n',\n'Test results (p=0.2): CWA=0.665 SWA=0.624 HWA=0.644', '\\n', '\\n=== Training\nwith dropout p=0.4 ===', '\\n', 'Epoch 1: train_loss=0.567 val_loss=0.468\nHWA=0.802', '\\n', 'Epoch 2: train_loss=0.467 val_loss=0.388 HWA=0.846', '\\n',\n'Epoch 3: train_loss=0.419 val_loss=0.353 HWA=0.869', '\\n', 'Epoch 4:\ntrain_loss=0.398 val_loss=0.332 HWA=0.887', '\\n', 'Epoch 5: train_loss=0.379\nval_loss=0.312 HWA=0.891', '\\n', 'Test results (p=0.4): CWA=0.663 SWA=0.624\nHWA=0.643', '\\n', '\\n=== Training with dropout p=0.6 ===', '\\n', 'Epoch 1:\ntrain_loss=0.622 val_loss=0.521 HWA=0.764', '\\n', 'Epoch 2: train_loss=0.540\nval_loss=0.474 HWA=0.801', '\\n', 'Epoch 3: train_loss=0.505 val_loss=0.432\nHWA=0.808', '\\n', 'Epoch 4: train_loss=0.484 val_loss=0.412 HWA=0.836', '\\n',\n'Epoch 5: train_loss=0.466 val_loss=0.397 HWA=0.834', '\\n', 'Test results\n(p=0.6): CWA=0.639 SWA=0.604 HWA=0.621', '\\n', 'Experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_23-24-\n25_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 2 minutes seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n=====\nTraining for 5 epochs =====', '\\n', 'Epoch 01/5: train_loss=0.5012\nval_loss=0.3863 CWA=0.831 SWA=0.829 HWA=0.830', '\\n', 'Epoch 02/5:\ntrain_loss=0.3270 val_loss=0.3046 CWA=0.890 SWA=0.890 HWA=0.890', '\\n', 'Epoch\n03/5: train_loss=0.2823 val_loss=0.2848 CWA=0.911 SWA=0.913 HWA=0.912', '\\n',\n'Epoch 04/5: train_loss=0.2624 val_loss=0.2868 CWA=0.889 SWA=0.886 HWA=0.888',\n'\\n', 'Epoch 05/5: train_loss=0.2475 val_loss=0.2406 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', '\\n===== Training for 15 epochs =====', '\\n', 'Epoch 01/15:\ntrain_loss=0.5012 val_loss=0.3863 CWA=0.831 SWA=0.829 HWA=0.830', '\\n', 'Epoch\n02/15: train_loss=0.3269 val_loss=0.3044 CWA=0.890 SWA=0.891 HWA=0.890', '\\n',\n'Epoch 03/15: train_loss=0.2821 val_loss=0.2846 CWA=0.912 SWA=0.914 HWA=0.913',\n'\\n', 'Epoch 04/15: train_loss=0.2623 val_loss=0.2875 CWA=0.889 SWA=0.885\nHWA=0.887', '\\n', 'Epoch 05/15: train_loss=0.2474 val_loss=0.2410 CWA=0.927\nSWA=0.925 HWA=0.926', '\\n', 'Epoch 06/15: train_loss=0.2404 val_loss=0.2897\nCWA=0.876 SWA=0.869 HWA=0.873', '\\n', 'Epoch 07/15: train_loss=0.2283\nval_loss=0.2254 CWA=0.931 SWA=0.930 HWA=0.930', '\\n', 'Epoch 08/15:\ntrain_loss=0.2155 val_loss=0.2295 CWA=0.934 SWA=0.933 HWA=0.933', '\\n', 'Epoch\n09/15: train_loss=0.2095 val_loss=0.2116 CWA=0.942 SWA=0.943 HWA=0.942', '\\n',\n'Epoch 10/15: train_loss=0.1996 val_loss=0.2066 CWA=0.942 SWA=0.942 HWA=0.942',\n'\\n', 'Epoch 11/15: train_loss=0.1954 val_loss=0.2027 CWA=0.944 SWA=0.943\nHWA=0.943', '\\n', 'Epoch 12/15: train_loss=0.1912 val_loss=0.1962 CWA=0.942\nSWA=0.940 HWA=0.941', '\\n', 'Epoch 13/15: train_loss=0.1851 val_loss=0.2148\nCWA=0.938 SWA=0.938 HWA=0.938', '\\n', 'Epoch 14/15: train_loss=0.1807\nval_loss=0.1945 CWA=0.947 SWA=0.947 HWA=0.947', '\\n', 'Epoch 15/15:\ntrain_loss=0.1760 val_loss=0.1880 CWA=0.950 SWA=0.950 HWA=0.950', '\\n', '\\n=====\nTraining for 30 epochs =====', '\\n', 'Epoch 01/30: train_loss=0.5013\nval_loss=0.3865 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/30:\ntrain_loss=0.3270 val_loss=0.3042 CWA=0.891 SWA=0.892 HWA=0.891', '\\n', 'Epoch\n03/30: train_loss=0.2821 val_loss=0.2840 CWA=0.912 SWA=0.914 HWA=0.913', '\\n',\n'Epoch 04/30: train_loss=0.2621 val_loss=0.2874 CWA=0.889 SWA=0.885 HWA=0.887',\n'\\n', 'Epoch 05/30: train_loss=0.2471 val_loss=0.2403 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', 'Epoch 06/30: train_loss=0.2399 val_loss=0.2867 CWA=0.879\nSWA=0.872 HWA=0.876', '\\n', 'Epoch 07/30: train_loss=0.2277 val_loss=0.2258\nCWA=0.932 SWA=0.931 HWA=0.931', '\\n', 'Epoch 08/30: train_loss=0.2153\nval_loss=0.2292 CWA=0.933 SWA=0.933 HWA=0.933', '\\n', 'Epoch 09/30:\ntrain_loss=0.2096 val_loss=0.2129 CWA=0.942 SWA=0.943 HWA=0.942', '\\n', 'Epoch\n10/30: train_loss=0.1994 val_loss=0.2074 CWA=0.941 SWA=0.941 HWA=0.941', '\\n',\n'Epoch 11/30: train_loss=0.1951 val_loss=0.2026 CWA=0.943 SWA=0.943 HWA=0.943',\n'\\n', 'Epoch 12/30: train_loss=0.1910 val_loss=0.1968 CWA=0.941 SWA=0.940\nHWA=0.940', '\\n', 'Epoch 13/30: train_loss=0.1853 val_loss=0.2170 CWA=0.938\nSWA=0.938 HWA=0.938', '\\n', 'Epoch 14/30: train_loss=0.1808 val_loss=0.1946\nCWA=0.948 SWA=0.948 HWA=0.948', '\\n', 'Epoch 15/30: train_loss=0.1762\nval_loss=0.1888 CWA=0.950 SWA=0.949 HWA=0.950', '\\n', 'Epoch 16/30:\ntrain_loss=0.1736 val_loss=0.1921 CWA=0.949 SWA=0.948 HWA=0.948', '\\n', 'Epoch\n17/30: train_loss=0.1697 val_loss=0.1804 CWA=0.955 SWA=0.955 HWA=0.955', '\\n',\n'Epoch 18/30: train_loss=0.1655 val_loss=0.1824 CWA=0.954 SWA=0.953 HWA=0.954',\n'\\n', 'Epoch 19/30: train_loss=0.1607 val_loss=0.1791 CWA=0.952 SWA=0.950\nHWA=0.951', '\\n', 'Epoch 20/30: train_loss=0.1608 val_loss=0.1782 CWA=0.956\nSWA=0.955 HWA=0.956', '\\n', 'Epoch 21/30: train_loss=0.1592 val_loss=0.1899\nCWA=0.950 SWA=0.950 HWA=0.950', '\\n', 'Epoch 22/30: train_loss=0.1578\nval_loss=0.1746 CWA=0.956 SWA=0.955 HWA=0.956', '\\n', 'Epoch 23/30:\ntrain_loss=0.1519 val_loss=0.1793 CWA=0.954 SWA=0.954 HWA=0.954', '\\n', 'Epoch\n24/30: train_loss=0.1493 val_loss=0.1789 CWA=0.950 SWA=0.946 HWA=0.948', '\\n',\n'Epoch 25/30: train_loss=0.1476 val_loss=0.1704 CWA=0.952 SWA=0.949 HWA=0.950',\n'\\n', 'Epoch 26/30: train_loss=0.1448 val_loss=0.1801 CWA=0.948 SWA=0.945\nHWA=0.946', '\\n', 'Epoch 27/30: train_loss=0.1435 val_loss=0.1681 CWA=0.954\nSWA=0.953 HWA=0.953', '\\n', 'Epoch 28/30: train_loss=0.1419 val_loss=0.1867\nCWA=0.942 SWA=0.937 HWA=0.940', '\\n', 'Epoch 29/30: train_loss=0.1397\nval_loss=0.1841 CWA=0.956 SWA=0.955 HWA=0.955', '\\n', 'Epoch 30/30:\ntrain_loss=0.1378 val_loss=0.1706 CWA=0.957 SWA=0.956 HWA=0.956', '\\n', '\\n=====\nTraining for 50 epochs =====', '\\n', 'Epoch 01/50: train_loss=0.5013\nval_loss=0.3861 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/50:\ntrain_loss=0.3272 val_loss=0.3043 CWA=0.889 SWA=0.890 HWA=0.890', '\\n', 'Epoch\n03/50: train_loss=0.2815 val_loss=0.2832 CWA=0.913 SWA=0.915 HWA=0.914', '\\n',\n'Epoch 04/50: train_loss=0.2611 val_loss=0.2883 CWA=0.888 SWA=0.884 HWA=0.886',\n'\\n', 'Epoch 05/50: train_loss=0.2467 val_loss=0.2405 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', 'Epoch 06/50: train_loss=0.2400 val_loss=0.2868 CWA=0.878\nSWA=0.871 HWA=0.874', '\\n', 'Epoch 07/50: train_loss=0.2281 val_loss=0.2259\nCWA=0.931 SWA=0.931 HWA=0.931', '\\n', 'Epoch 08/50: train_loss=0.2158\nval_loss=0.2305 CWA=0.933 SWA=0.933 HWA=0.933', '\\n', 'Epoch 09/50:\ntrain_loss=0.2106 val_loss=0.2135 CWA=0.942 SWA=0.942 HWA=0.942', '\\n', 'Epoch\n10/50: train_loss=0.2004 val_loss=0.2082 CWA=0.941 SWA=0.940 HWA=0.941', '\\n',\n'Epoch 11/50: train_loss=0.1961 val_loss=0.2036 CWA=0.944 SWA=0.943 HWA=0.944',\n'\\n', 'Epoch 12/50: train_loss=0.1920 val_loss=0.1974 CWA=0.940 SWA=0.939\nHWA=0.939', '\\n', 'Epoch 13/50: train_loss=0.1860 val_loss=0.2199 CWA=0.936\nSWA=0.936 HWA=0.936', '\\n', 'Epoch 14/50: train_loss=0.1824 val_loss=0.1966\nCWA=0.948 SWA=0.947 HWA=0.947', '\\n', 'Epoch 15/50: train_loss=0.1769\nval_loss=0.1895 CWA=0.948 SWA=0.949 HWA=0.948', '\\n', 'Epoch 16/50:\ntrain_loss=0.1741 val_loss=0.1919 CWA=0.949 SWA=0.948 HWA=0.948', '\\n', 'Epoch\n17/50: train_loss=0.1704 val_loss=0.1826 CWA=0.954 SWA=0.953 HWA=0.954', '\\n',\n'Epoch 18/50: train_loss=0.1664 val_loss=0.1844 CWA=0.952 SWA=0.952 HWA=0.952',\n'\\n', 'Epoch 19/50: train_loss=0.1614 val_loss=0.1816 CWA=0.953 SWA=0.952\nHWA=0.952', '\\n', 'Epoch 20/50: train_loss=0.1607 val_loss=0.1829 CWA=0.953\nSWA=0.953 HWA=0.953', '\\n', 'Epoch 21/50: train_loss=0.1595 val_loss=0.1974\nCWA=0.948 SWA=0.948 HWA=0.948', '\\n', 'Epoch 22/50: train_loss=0.1588\nval_loss=0.1759 CWA=0.955 SWA=0.953 HWA=0.954', '\\n', 'Epoch 23/50:\ntrain_loss=0.1520 val_loss=0.1818 CWA=0.953 SWA=0.953 HWA=0.953', '\\n', 'Epoch\n24/50: train_loss=0.1491 val_loss=0.1781 CWA=0.952 SWA=0.950 HWA=0.951', '\\n',\n'Epoch 25/50: train_loss=0.1472 val_loss=0.1722 CWA=0.953 SWA=0.951 HWA=0.952',\n'\\n', 'Epoch 26/50: train_loss=0.1450 val_loss=0.1829 CWA=0.946 SWA=0.942\nHWA=0.944', '\\n', 'Epoch 27/50: train_loss=0.1428 val_loss=0.1697 CWA=0.955\nSWA=0.954 HWA=0.954', '\\n', 'Epoch 28/50: train_loss=0.1416 val_loss=0.1878\nCWA=0.941 SWA=0.936 HWA=0.939', '\\n', 'Epoch 29/50: train_loss=0.1390\nval_loss=0.1801 CWA=0.958 SWA=0.957 HWA=0.958', '\\n', 'Epoch 30/50:\ntrain_loss=0.1377 val_loss=0.1780 CWA=0.955 SWA=0.954 HWA=0.954', '\\n', 'Epoch\n31/50: train_loss=0.1344 val_loss=0.1738 CWA=0.957 SWA=0.956 HWA=0.956', '\\n',\n'Epoch 32/50: train_loss=0.1345 val_loss=0.1614 CWA=0.959 SWA=0.958 HWA=0.958',\n'\\n', 'Epoch 33/50: train_loss=0.1328 val_loss=0.1623 CWA=0.959 SWA=0.957\nHWA=0.958', '\\n', 'Epoch 34/50: train_loss=0.1306 val_loss=0.1626 CWA=0.958\nSWA=0.956 HWA=0.957', '\\n', 'Epoch 35/50: train_loss=0.1296 val_loss=0.1584\nCWA=0.962 SWA=0.961 HWA=0.961', '\\n', 'Epoch 36/50: train_loss=0.1243\nval_loss=0.1627 CWA=0.960 SWA=0.960 HWA=0.960', '\\n', 'Epoch 37/50:\ntrain_loss=0.1279 val_loss=0.1687 CWA=0.960 SWA=0.959 HWA=0.959', '\\n', 'Epoch\n38/50: train_loss=0.1245 val_loss=0.2358 CWA=0.905 SWA=0.898 HWA=0.901', '\\n',\n'Epoch 39/50: train_loss=0.1235 val_loss=0.1607 CWA=0.958 SWA=0.956 HWA=0.957',\n'\\n', 'Epoch 40/50: train_loss=0.1228 val_loss=0.1603 CWA=0.958 SWA=0.955\nHWA=0.956', '\\n', 'Epoch 41/50: train_loss=0.1222 val_loss=0.1843 CWA=0.941\nSWA=0.936 HWA=0.939', '\\n', 'Epoch 42/50: train_loss=0.1193 val_loss=0.1570\nCWA=0.962 SWA=0.961 HWA=0.961', '\\n', 'Epoch 43/50: train_loss=0.1169\nval_loss=0.1588 CWA=0.959 SWA=0.957 HWA=0.958', '\\n', 'Epoch 44/50:\ntrain_loss=0.1169 val_loss=0.1576 CWA=0.962 SWA=0.960 HWA=0.961', '\\n', 'Epoch\n45/50: train_loss=0.1164 val_loss=0.1637 CWA=0.954 SWA=0.952 HWA=0.953', '\\n',\n'Epoch 46/50: train_loss=0.1123 val_loss=0.1575 CWA=0.962 SWA=0.961 HWA=0.961',\n'\\n', 'Epoch 47/50: train_loss=0.1104 val_loss=0.1561 CWA=0.963 SWA=0.962\nHWA=0.963', '\\n', 'Epoch 48/50: train_loss=0.1117 val_loss=0.1639 CWA=0.953\nSWA=0.953 HWA=0.953', '\\n', 'Epoch 49/50: train_loss=0.1126 val_loss=0.1678\nCWA=0.949 SWA=0.947 HWA=0.948', '\\n', 'Epoch 50/50: train_loss=0.1119\nval_loss=0.1723 CWA=0.957 SWA=0.956 HWA=0.957', '\\n', '\\nBest validation\nHWA=0.957 achieved with 50 epochs.', '\\n', 'TEST  CWA=0.684  SWA=0.640\nHWA=0.661', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 6\nminutes seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n=====\nTraining for 5 epochs =====', '\\n', 'Epoch 01/5: train_loss=0.5012\nval_loss=0.3859 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/5:\ntrain_loss=0.3267 val_loss=0.3040 CWA=0.890 SWA=0.890 HWA=0.890', '\\n', 'Epoch\n03/5: train_loss=0.2813 val_loss=0.2825 CWA=0.913 SWA=0.915 HWA=0.914', '\\n',\n'Epoch 04/5: train_loss=0.2609 val_loss=0.2887 CWA=0.886 SWA=0.883 HWA=0.885',\n'\\n', 'Epoch 05/5: train_loss=0.2462 val_loss=0.2399 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', '\\n===== Training for 15 epochs =====', '\\n', 'Epoch 01/15:\ntrain_loss=0.5012 val_loss=0.3863 CWA=0.831 SWA=0.829 HWA=0.830', '\\n', 'Epoch\n02/15: train_loss=0.3269 val_loss=0.3044 CWA=0.890 SWA=0.891 HWA=0.890', '\\n',\n'Epoch 03/15: train_loss=0.2821 val_loss=0.2846 CWA=0.912 SWA=0.914 HWA=0.913',\n'\\n', 'Epoch 04/15: train_loss=0.2623 val_loss=0.2874 CWA=0.889 SWA=0.886\nHWA=0.888', '\\n', 'Epoch 05/15: train_loss=0.2474 val_loss=0.2409 CWA=0.927\nSWA=0.925 HWA=0.926', '\\n', 'Epoch 06/15: train_loss=0.2405 val_loss=0.2893\nCWA=0.877 SWA=0.870 HWA=0.873', '\\n', 'Epoch 07/15: train_loss=0.2284\nval_loss=0.2258 CWA=0.931 SWA=0.931 HWA=0.931', '\\n', 'Epoch 08/15:\ntrain_loss=0.2156 val_loss=0.2283 CWA=0.934 SWA=0.934 HWA=0.934', '\\n', 'Epoch\n09/15: train_loss=0.2098 val_loss=0.2118 CWA=0.942 SWA=0.943 HWA=0.942', '\\n',\n'Epoch 10/15: train_loss=0.1998 val_loss=0.2076 CWA=0.942 SWA=0.942 HWA=0.942',\n'\\n', 'Epoch 11/15: train_loss=0.1955 val_loss=0.2026 CWA=0.944 SWA=0.943\nHWA=0.943', '\\n', 'Epoch 12/15: train_loss=0.1915 val_loss=0.1963 CWA=0.941\nSWA=0.940 HWA=0.941', '\\n', 'Epoch 13/15: train_loss=0.1854 val_loss=0.2162\nCWA=0.938 SWA=0.938 HWA=0.938', '\\n', 'Epoch 14/15: train_loss=0.1815\nval_loss=0.1951 CWA=0.947 SWA=0.947 HWA=0.947', '\\n', 'Epoch 15/15:\ntrain_loss=0.1765 val_loss=0.1877 CWA=0.950 SWA=0.950 HWA=0.950', '\\n', '\\n=====\nTraining for 30 epochs =====', '\\n', 'Epoch 01/30: train_loss=0.5013\nval_loss=0.3862 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/30:\ntrain_loss=0.3270 val_loss=0.3041 CWA=0.890 SWA=0.890 HWA=0.890', '\\n', 'Epoch\n03/30: train_loss=0.2815 val_loss=0.2840 CWA=0.912 SWA=0.914 HWA=0.913', '\\n',\n'Epoch 04/30: train_loss=0.2612 val_loss=0.2879 CWA=0.887 SWA=0.883 HWA=0.885',\n'\\n', 'Epoch 05/30: train_loss=0.2466 val_loss=0.2404 CWA=0.927 SWA=0.925\nHWA=0.926', '\\n', 'Epoch 06/30: train_loss=0.2399 val_loss=0.2858 CWA=0.877\nSWA=0.871 HWA=0.874', '\\n', 'Epoch 07/30: train_loss=0.2278 val_loss=0.2257\nCWA=0.931 SWA=0.930 HWA=0.930', '\\n', 'Epoch 08/30: train_loss=0.2155\nval_loss=0.2282 CWA=0.934 SWA=0.934 HWA=0.934', '\\n', 'Epoch 09/30:\ntrain_loss=0.2098 val_loss=0.2133 CWA=0.941 SWA=0.942 HWA=0.941', '\\n', 'Epoch\n10/30: train_loss=0.2001 val_loss=0.2079 CWA=0.942 SWA=0.942 HWA=0.942', '\\n',\n'Epoch 11/30: train_loss=0.1957 val_loss=0.2034 CWA=0.943 SWA=0.942 HWA=0.942',\n'\\n', 'Epoch 12/30: train_loss=0.1915 val_loss=0.1970 CWA=0.940 SWA=0.938\nHWA=0.939', '\\n', 'Epoch 13/30: train_loss=0.1854 val_loss=0.2196 CWA=0.937\nSWA=0.937 HWA=0.937', '\\n', 'Epoch 14/30: train_loss=0.1814 val_loss=0.1945\nCWA=0.947 SWA=0.947 HWA=0.947', '\\n', 'Epoch 15/30: train_loss=0.1763\nval_loss=0.1873 CWA=0.950 SWA=0.950 HWA=0.950', '\\n', 'Epoch 16/30:\ntrain_loss=0.1735 val_loss=0.1911 CWA=0.949 SWA=0.948 HWA=0.949', '\\n', 'Epoch\n17/30: train_loss=0.1699 val_loss=0.1812 CWA=0.955 SWA=0.955 HWA=0.955', '\\n',\n'Epoch 18/30: train_loss=0.1654 val_loss=0.1812 CWA=0.954 SWA=0.953 HWA=0.954',\n'\\n', 'Epoch 19/30: train_loss=0.1606 val_loss=0.1791 CWA=0.953 SWA=0.952\nHWA=0.952', '\\n', 'Epoch 20/30: train_loss=0.1606 val_loss=0.1777 CWA=0.956\nSWA=0.955 HWA=0.955', '\\n', 'Epoch 21/30: train_loss=0.1589 val_loss=0.1876\nCWA=0.951 SWA=0.951 HWA=0.951', '\\n', 'Epoch 22/30: train_loss=0.1576\nval_loss=0.1758 CWA=0.956 SWA=0.955 HWA=0.956', '\\n', 'Epoch 23/30:\ntrain_loss=0.1515 val_loss=0.1767 CWA=0.955 SWA=0.954 HWA=0.954', '\\n', 'Epoch\n24/30: train_loss=0.1488 val_loss=0.1755 CWA=0.953 SWA=0.951 HWA=0.952', '\\n',\n'Epoch 25/30: train_loss=0.1471 val_loss=0.1707 CWA=0.954 SWA=0.951 HWA=0.953',\n'\\n', 'Epoch 26/30: train_loss=0.1443 val_loss=0.1794 CWA=0.948 SWA=0.945\nHWA=0.947', '\\n', 'Epoch 27/30: train_loss=0.1425 val_loss=0.1702 CWA=0.954\nSWA=0.953 HWA=0.954', '\\n', 'Epoch 28/30: train_loss=0.1419 val_loss=0.1847\nCWA=0.943 SWA=0.938 HWA=0.940', '\\n', 'Epoch 29/30: train_loss=0.1385\nval_loss=0.1762 CWA=0.959 SWA=0.958 HWA=0.958', '\\n', 'Epoch 30/30:\ntrain_loss=0.1369 val_loss=0.1744 CWA=0.955 SWA=0.954 HWA=0.955', '\\n', '\\n=====\nTraining for 50 epochs =====', '\\n', 'Epoch 01/50: train_loss=0.5013\nval_loss=0.3861 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/50:\ntrain_loss=0.3271 val_loss=0.3042 CWA=0.890 SWA=0.891 HWA=0.891', '\\n', 'Epoch\n03/50: train_loss=0.2816 val_loss=0.2842 CWA=0.911 SWA=0.913 HWA=0.912', '\\n',\n'Epoch 04/50: train_loss=0.2614 val_loss=0.2890 CWA=0.887 SWA=0.884 HWA=0.885',\n'\\n', 'Epoch 05/50: train_loss=0.2467 val_loss=0.2408 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', 'Epoch 06/50: train_loss=0.2400 val_loss=0.2856 CWA=0.879\nSWA=0.873 HWA=0.876', '\\n', 'Epoch 07/50: train_loss=0.2282 val_loss=0.2267\nCWA=0.930 SWA=0.930 HWA=0.930', '\\n', 'Epoch 08/50: train_loss=0.2160\nval_loss=0.2291 CWA=0.934 SWA=0.934 HWA=0.934', '\\n', 'Epoch 09/50:\ntrain_loss=0.2105 val_loss=0.2145 CWA=0.942 SWA=0.942 HWA=0.942', '\\n', 'Epoch\n10/50: train_loss=0.2010 val_loss=0.2082 CWA=0.941 SWA=0.941 HWA=0.941', '\\n',\n'Epoch 11/50: train_loss=0.1965 val_loss=0.2042 CWA=0.943 SWA=0.942 HWA=0.943',\n'\\n', 'Epoch 12/50: train_loss=0.1923 val_loss=0.1974 CWA=0.941 SWA=0.940\nHWA=0.941', '\\n', 'Epoch 13/50: train_loss=0.1863 val_loss=0.2202 CWA=0.937\nSWA=0.937 HWA=0.937', '\\n', 'Epoch 14/50: train_loss=0.1820 val_loss=0.1953\nCWA=0.949 SWA=0.948 HWA=0.949', '\\n', 'Epoch 15/50: train_loss=0.1768\nval_loss=0.1886 CWA=0.949 SWA=0.949 HWA=0.949', '\\n', 'Epoch 16/50:\ntrain_loss=0.1738 val_loss=0.1914 CWA=0.950 SWA=0.949 HWA=0.949', '\\n', 'Epoch\n17/50: train_loss=0.1700 val_loss=0.1810 CWA=0.955 SWA=0.954 HWA=0.954', '\\n',\n'Epoch 18/50: train_loss=0.1655 val_loss=0.1824 CWA=0.954 SWA=0.954 HWA=0.954',\n'\\n', 'Epoch 19/50: train_loss=0.1611 val_loss=0.1800 CWA=0.953 SWA=0.952\nHWA=0.953', '\\n', 'Epoch 20/50: train_loss=0.1605 val_loss=0.1814 CWA=0.953\nSWA=0.953 HWA=0.953', '\\n', 'Epoch 21/50: train_loss=0.1598 val_loss=0.1948\nCWA=0.949 SWA=0.949 HWA=0.949', '\\n', 'Epoch 22/50: train_loss=0.1582\nval_loss=0.1752 CWA=0.955 SWA=0.954 HWA=0.955', '\\n', 'Epoch 23/50:\ntrain_loss=0.1519 val_loss=0.1798 CWA=0.954 SWA=0.954 HWA=0.954', '\\n', 'Epoch\n24/50: train_loss=0.1489 val_loss=0.1750 CWA=0.953 SWA=0.951 HWA=0.952', '\\n',\n'Epoch 25/50: train_loss=0.1475 val_loss=0.1691 CWA=0.954 SWA=0.952 HWA=0.953',\n'\\n', 'Epoch 26/50: train_loss=0.1454 val_loss=0.1798 CWA=0.947 SWA=0.943\nHWA=0.945', '\\n', 'Epoch 27/50: train_loss=0.1435 val_loss=0.1684 CWA=0.955\nSWA=0.954 HWA=0.954', '\\n', 'Epoch 28/50: train_loss=0.1420 val_loss=0.1862\nCWA=0.941 SWA=0.935 HWA=0.938', '\\n', 'Epoch 29/50: train_loss=0.1401\nval_loss=0.1790 CWA=0.958 SWA=0.957 HWA=0.958', '\\n', 'Epoch 30/50:\ntrain_loss=0.1379 val_loss=0.1735 CWA=0.956 SWA=0.955 HWA=0.956', '\\n', 'Epoch\n31/50: train_loss=0.1346 val_loss=0.1732 CWA=0.957 SWA=0.957 HWA=0.957', '\\n',\n'Epoch 32/50: train_loss=0.1355 val_loss=0.1628 CWA=0.960 SWA=0.959 HWA=0.959',\n'\\n', 'Epoch 33/50: train_loss=0.1347 val_loss=0.1609 CWA=0.959 SWA=0.958\nHWA=0.958', '\\n', 'Epoch 34/50: train_loss=0.1309 val_loss=0.1596 CWA=0.958\nSWA=0.956 HWA=0.957', '\\n', 'Epoch 35/50: train_loss=0.1306 val_loss=0.1561\nCWA=0.964 SWA=0.963 HWA=0.963', '\\n', 'Epoch 36/50: train_loss=0.1244\nval_loss=0.1614 CWA=0.960 SWA=0.960 HWA=0.960', '\\n', 'Epoch 37/50:\ntrain_loss=0.1287 val_loss=0.1657 CWA=0.961 SWA=0.960 HWA=0.961', '\\n', 'Epoch\n38/50: train_loss=0.1240 val_loss=0.2317 CWA=0.908 SWA=0.901 HWA=0.904', '\\n',\n'Epoch 39/50: train_loss=0.1235 val_loss=0.1592 CWA=0.958 SWA=0.956 HWA=0.957',\n'\\n', 'Epoch 40/50: train_loss=0.1234 val_loss=0.1555 CWA=0.961 SWA=0.959\nHWA=0.960', '\\n', 'Epoch 41/50: train_loss=0.1220 val_loss=0.1910 CWA=0.935\nSWA=0.930 HWA=0.933', '\\n', 'Epoch 42/50: train_loss=0.1193 val_loss=0.1554\nCWA=0.961 SWA=0.959 HWA=0.960', '\\n', 'Epoch 43/50: train_loss=0.1168\nval_loss=0.1560 CWA=0.960 SWA=0.958 HWA=0.959', '\\n', 'Epoch 44/50:\ntrain_loss=0.1160 val_loss=0.1557 CWA=0.962 SWA=0.961 HWA=0.961', '\\n', 'Epoch\n45/50: train_loss=0.1166 val_loss=0.1629 CWA=0.953 SWA=0.951 HWA=0.952', '\\n',\n'Epoch 46/50: train_loss=0.1124 val_loss=0.1585 CWA=0.959 SWA=0.958 HWA=0.959',\n'\\n', 'Epoch 47/50: train_loss=0.1105 val_loss=0.1533 CWA=0.963 SWA=0.962\nHWA=0.963', '\\n', 'Epoch 48/50: train_loss=0.1133 val_loss=0.1671 CWA=0.949\nSWA=0.949 HWA=0.949', '\\n', 'Epoch 49/50: train_loss=0.1131 val_loss=0.1793\nCWA=0.942 SWA=0.938 HWA=0.940', '\\n', 'Epoch 50/50: train_loss=0.1116\nval_loss=0.1713 CWA=0.957 SWA=0.956 HWA=0.956', '\\n', '\\nBest validation\nHWA=0.956 achieved with 50 epochs.', '\\n', 'TEST  CWA=0.685  SWA=0.640\nHWA=0.662', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time: 2\nminutes seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Looking for data at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size: 17 | #classes: 2', '\\n', '\\n=====\nTraining for 5 epochs =====', '\\n', 'Epoch 01/5: train_loss=0.5012\nval_loss=0.3863 CWA=0.831 SWA=0.829 HWA=0.830', '\\n', 'Epoch 02/5:\ntrain_loss=0.3270 val_loss=0.3046 CWA=0.890 SWA=0.891 HWA=0.891', '\\n', 'Epoch\n03/5: train_loss=0.2822 val_loss=0.2843 CWA=0.912 SWA=0.914 HWA=0.913', '\\n',\n'Epoch 04/5: train_loss=0.2623 val_loss=0.2877 CWA=0.889 SWA=0.885 HWA=0.887',\n'\\n', 'Epoch 05/5: train_loss=0.2475 val_loss=0.2412 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', '\\n===== Training for 15 epochs =====', '\\n', 'Epoch 01/15:\ntrain_loss=0.5012 val_loss=0.3863 CWA=0.831 SWA=0.829 HWA=0.830', '\\n', 'Epoch\n02/15: train_loss=0.3269 val_loss=0.3044 CWA=0.890 SWA=0.891 HWA=0.891', '\\n',\n'Epoch 03/15: train_loss=0.2822 val_loss=0.2847 CWA=0.912 SWA=0.914 HWA=0.913',\n'\\n', 'Epoch 04/15: train_loss=0.2623 val_loss=0.2874 CWA=0.889 SWA=0.886\nHWA=0.887', '\\n', 'Epoch 05/15: train_loss=0.2474 val_loss=0.2405 CWA=0.927\nSWA=0.926 HWA=0.927', '\\n', 'Epoch 06/15: train_loss=0.2407 val_loss=0.2888\nCWA=0.878 SWA=0.871 HWA=0.874', '\\n', 'Epoch 07/15: train_loss=0.2284\nval_loss=0.2256 CWA=0.930 SWA=0.930 HWA=0.930', '\\n', 'Epoch 08/15:\ntrain_loss=0.2159 val_loss=0.2284 CWA=0.935 SWA=0.935 HWA=0.935', '\\n', 'Epoch\n09/15: train_loss=0.2099 val_loss=0.2119 CWA=0.942 SWA=0.943 HWA=0.942', '\\n',\n'Epoch 10/15: train_loss=0.2001 val_loss=0.2071 CWA=0.942 SWA=0.942 HWA=0.942',\n'\\n', 'Epoch 11/15: train_loss=0.1958 val_loss=0.2027 CWA=0.944 SWA=0.943\nHWA=0.944', '\\n', 'Epoch 12/15: train_loss=0.1916 val_loss=0.1962 CWA=0.941\nSWA=0.939 HWA=0.940', '\\n', 'Epoch 13/15: train_loss=0.1855 val_loss=0.2151\nCWA=0.938 SWA=0.938 HWA=0.938', '\\n', 'Epoch 14/15: train_loss=0.1817\nval_loss=0.1949 CWA=0.947 SWA=0.947 HWA=0.947', '\\n', 'Epoch 15/15:\ntrain_loss=0.1764 val_loss=0.1888 CWA=0.950 SWA=0.949 HWA=0.949', '\\n', '\\n=====\nTraining for 30 epochs =====', '\\n', 'Epoch 01/30: train_loss=0.5013\nval_loss=0.3861 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/30:\ntrain_loss=0.3271 val_loss=0.3042 CWA=0.890 SWA=0.891 HWA=0.891', '\\n', 'Epoch\n03/30: train_loss=0.2816 val_loss=0.2841 CWA=0.911 SWA=0.913 HWA=0.912', '\\n',\n'Epoch 04/30: train_loss=0.2614 val_loss=0.2881 CWA=0.887 SWA=0.884 HWA=0.885',\n'\\n', 'Epoch 05/30: train_loss=0.2469 val_loss=0.2407 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', 'Epoch 06/30: train_loss=0.2401 val_loss=0.2859 CWA=0.879\nSWA=0.872 HWA=0.876', '\\n', 'Epoch 07/30: train_loss=0.2283 val_loss=0.2265\nCWA=0.931 SWA=0.930 HWA=0.931', '\\n', 'Epoch 08/30: train_loss=0.2160\nval_loss=0.2288 CWA=0.934 SWA=0.935 HWA=0.935', '\\n', 'Epoch 09/30:\ntrain_loss=0.2105 val_loss=0.2144 CWA=0.941 SWA=0.942 HWA=0.941', '\\n', 'Epoch\n10/30: train_loss=0.2009 val_loss=0.2087 CWA=0.940 SWA=0.940 HWA=0.940', '\\n',\n'Epoch 11/30: train_loss=0.1965 val_loss=0.2041 CWA=0.943 SWA=0.943 HWA=0.943',\n'\\n', 'Epoch 12/30: train_loss=0.1927 val_loss=0.1971 CWA=0.941 SWA=0.940\nHWA=0.940', '\\n', 'Epoch 13/30: train_loss=0.1864 val_loss=0.2203 CWA=0.936\nSWA=0.936 HWA=0.936', '\\n', 'Epoch 14/30: train_loss=0.1823 val_loss=0.1952\nCWA=0.948 SWA=0.947 HWA=0.948', '\\n', 'Epoch 15/30: train_loss=0.1770\nval_loss=0.1891 CWA=0.948 SWA=0.948 HWA=0.948', '\\n', 'Epoch 16/30:\ntrain_loss=0.1741 val_loss=0.1920 CWA=0.948 SWA=0.947 HWA=0.948', '\\n', 'Epoch\n17/30: train_loss=0.1704 val_loss=0.1827 CWA=0.954 SWA=0.953 HWA=0.953', '\\n',\n'Epoch 18/30: train_loss=0.1661 val_loss=0.1825 CWA=0.954 SWA=0.953 HWA=0.953',\n'\\n', 'Epoch 19/30: train_loss=0.1612 val_loss=0.1812 CWA=0.951 SWA=0.950\nHWA=0.951', '\\n', 'Epoch 20/30: train_loss=0.1607 val_loss=0.1792 CWA=0.956\nSWA=0.955 HWA=0.956', '\\n', 'Epoch 21/30: train_loss=0.1596 val_loss=0.1956\nCWA=0.949 SWA=0.948 HWA=0.948', '\\n', 'Epoch 22/30: train_loss=0.1580\nval_loss=0.1776 CWA=0.956 SWA=0.955 HWA=0.955', '\\n', 'Epoch 23/30:\ntrain_loss=0.1522 val_loss=0.1791 CWA=0.954 SWA=0.954 HWA=0.954', '\\n', 'Epoch\n24/30: train_loss=0.1494 val_loss=0.1765 CWA=0.953 SWA=0.950 HWA=0.952', '\\n',\n'Epoch 25/30: train_loss=0.1475 val_loss=0.1708 CWA=0.954 SWA=0.951 HWA=0.952',\n'\\n', 'Epoch 26/30: train_loss=0.1449 val_loss=0.1833 CWA=0.947 SWA=0.943\nHWA=0.945', '\\n', 'Epoch 27/30: train_loss=0.1437 val_loss=0.1703 CWA=0.955\nSWA=0.954 HWA=0.954', '\\n', 'Epoch 28/30: train_loss=0.1422 val_loss=0.1867\nCWA=0.941 SWA=0.936 HWA=0.938', '\\n', 'Epoch 29/30: train_loss=0.1399\nval_loss=0.1813 CWA=0.957 SWA=0.956 HWA=0.957', '\\n', 'Epoch 30/30:\ntrain_loss=0.1379 val_loss=0.1736 CWA=0.955 SWA=0.954 HWA=0.955', '\\n', '\\n=====\nTraining for 50 epochs =====', '\\n', 'Epoch 01/50: train_loss=0.5013\nval_loss=0.3865 CWA=0.830 SWA=0.828 HWA=0.829', '\\n', 'Epoch 02/50:\ntrain_loss=0.3270 val_loss=0.3042 CWA=0.891 SWA=0.892 HWA=0.891', '\\n', 'Epoch\n03/50: train_loss=0.2821 val_loss=0.2839 CWA=0.912 SWA=0.914 HWA=0.913', '\\n',\n'Epoch 04/50: train_loss=0.2621 val_loss=0.2873 CWA=0.889 SWA=0.886 HWA=0.887',\n'\\n', 'Epoch 05/50: train_loss=0.2470 val_loss=0.2403 CWA=0.926 SWA=0.924\nHWA=0.925', '\\n', 'Epoch 06/50: train_loss=0.2398 val_loss=0.2866 CWA=0.878\nSWA=0.872 HWA=0.875', '\\n', 'Epoch 07/50: train_loss=0.2279 val_loss=0.2256\nCWA=0.932 SWA=0.931 HWA=0.931', '\\n', 'Epoch 08/50: train_loss=0.2155\nval_loss=0.2288 CWA=0.932 SWA=0.932 HWA=0.932', '\\n', 'Epoch 09/50:\ntrain_loss=0.2097 val_loss=0.2128 CWA=0.943 SWA=0.943 HWA=0.943', '\\n', 'Epoch\n10/50: train_loss=0.1997 val_loss=0.2075 CWA=0.941 SWA=0.941 HWA=0.941', '\\n',\n'Epoch 11/50: train_loss=0.1954 val_loss=0.2030 CWA=0.943 SWA=0.943 HWA=0.943',\n'\\n', 'Epoch 12/50: train_loss=0.1911 val_loss=0.1962 CWA=0.941 SWA=0.940\nHWA=0.941', '\\n', 'Epoch 13/50: train_loss=0.1856 val_loss=0.2176 CWA=0.937\nSWA=0.937 HWA=0.937', '\\n', 'Epoch 14/50: train_loss=0.1810 val_loss=0.1945\nCWA=0.947 SWA=0.947 HWA=0.947', '\\n', 'Epoch 15/50: train_loss=0.1766\nval_loss=0.1878 CWA=0.950 SWA=0.950 HWA=0.950', '\\n', 'Epoch 16/50:\ntrain_loss=0.1734 val_loss=0.1921 CWA=0.949 SWA=0.948 HWA=0.948', '\\n', 'Epoch\n17/50: train_loss=0.1696 val_loss=0.1799 CWA=0.955 SWA=0.954 HWA=0.955', '\\n',\n'Epoch 18/50: train_loss=0.1654 val_loss=0.1815 CWA=0.954 SWA=0.953 HWA=0.953',\n'\\n', 'Epoch 19/50: train_loss=0.1607 val_loss=0.1779 CWA=0.953 SWA=0.952\nHWA=0.953', '\\n', 'Epoch 20/50: train_loss=0.1612 val_loss=0.1778 CWA=0.956\nSWA=0.956 HWA=0.956', '\\n', 'Epoch 21/50: train_loss=0.1597 val_loss=0.1890\nCWA=0.950 SWA=0.950 HWA=0.950', '\\n', 'Epoch 22/50: train_loss=0.1576\nval_loss=0.1756 CWA=0.956 SWA=0.955 HWA=0.955', '\\n', 'Epoch 23/50:\ntrain_loss=0.1519 val_loss=0.1783 CWA=0.954 SWA=0.954 HWA=0.954', '\\n', 'Epoch\n24/50: train_loss=0.1495 val_loss=0.1774 CWA=0.952 SWA=0.949 HWA=0.950', '\\n',\n'Epoch 25/50: train_loss=0.1475 val_loss=0.1691 CWA=0.956 SWA=0.954 HWA=0.955',\n'\\n', 'Epoch 26/50: train_loss=0.1450 val_loss=0.1809 CWA=0.947 SWA=0.944\nHWA=0.946', '\\n', 'Epoch 27/50: train_loss=0.1434 val_loss=0.1669 CWA=0.955\nSWA=0.954 HWA=0.955', '\\n', 'Epoch 28/50: train_loss=0.1422 val_loss=0.1848\nCWA=0.943 SWA=0.938 HWA=0.940', '\\n', 'Epoch 29/50: train_loss=0.1388\nval_loss=0.1795 CWA=0.957 SWA=0.956 HWA=0.956', '\\n', 'Epoch 30/50:\ntrain_loss=0.1375 val_loss=0.1708 CWA=0.957 SWA=0.957 HWA=0.957', '\\n', 'Epoch\n31/50: train_loss=0.1341 val_loss=0.1713 CWA=0.956 SWA=0.955 HWA=0.956', '\\n',\n'Epoch 32/50: train_loss=0.1347 val_loss=0.1617 CWA=0.961 SWA=0.960 HWA=0.960',\n'\\n', 'Epoch 33/50: train_loss=0.1326 val_loss=0.1597 CWA=0.960 SWA=0.959\nHWA=0.959', '\\n', 'Epoch 34/50: train_loss=0.1303 val_loss=0.1610 CWA=0.959\nSWA=0.956 HWA=0.958', '\\n', 'Epoch 35/50: train_loss=0.1298 val_loss=0.1582\nCWA=0.962 SWA=0.961 HWA=0.961', '\\n', 'Epoch 36/50: train_loss=0.1247\nval_loss=0.1592 CWA=0.960 SWA=0.960 HWA=0.960', '\\n', 'Epoch 37/50:\ntrain_loss=0.1283 val_loss=0.1646 CWA=0.962 SWA=0.961 HWA=0.961', '\\n', 'Epoch\n38/50: train_loss=0.1236 val_loss=0.2422 CWA=0.901 SWA=0.895 HWA=0.898', '\\n',\n'Epoch 39/50: train_loss=0.1222 val_loss=0.1594 CWA=0.956 SWA=0.954 HWA=0.955',\n'\\n', 'Epoch 40/50: train_loss=0.1225 val_loss=0.1563 CWA=0.960 SWA=0.958\nHWA=0.959', '\\n', 'Epoch 41/50: train_loss=0.1216 val_loss=0.1723 CWA=0.950\nSWA=0.946 HWA=0.948', '\\n', 'Epoch 42/50: train_loss=0.1177 val_loss=0.1552\nCWA=0.962 SWA=0.961 HWA=0.962', '\\n', 'Epoch 43/50: train_loss=0.1160\nval_loss=0.1576 CWA=0.960 SWA=0.958 HWA=0.959', '\\n', 'Epoch 44/50:\ntrain_loss=0.1159 val_loss=0.1569 CWA=0.960 SWA=0.959 HWA=0.959', '\\n', 'Epoch\n45/50: train_loss=0.1159 val_loss=0.1622 CWA=0.954 SWA=0.952 HWA=0.953', '\\n',\n'Epoch 46/50: train_loss=0.1121 val_loss=0.1566 CWA=0.959 SWA=0.958 HWA=0.959',\n'\\n', 'Epoch 47/50: train_loss=0.1096 val_loss=0.1531 CWA=0.964 SWA=0.963\nHWA=0.964', '\\n', 'Epoch 48/50: train_loss=0.1113 val_loss=0.1658 CWA=0.951\nSWA=0.952 HWA=0.951', '\\n', 'Epoch 49/50: train_loss=0.1116 val_loss=0.1704\nCWA=0.945 SWA=0.942 HWA=0.944', '\\n', 'Epoch 50/50: train_loss=0.1108\nval_loss=0.1678 CWA=0.960 SWA=0.959 HWA=0.959', '\\n', '\\nBest validation\nHWA=0.959 achieved with 50 epochs.', '\\n', 'TEST  CWA=0.683  SWA=0.639\nHWA=0.660', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time: 2\nminutes seconds (time limit is 30 minutes).']", ""], "analysis": ["", "The training script executed successfully without any bugs. The model was\ntrained for various epochs (5, 15, 30, and 50), and the best validation\nperformance (HWA=0.956) was achieved with 30 epochs. However, the test\nperformance was lower (HWA=0.659), which indicates potential overfitting or a\nmismatch between training and test data distributions. Experiment data was saved\nsuccessfully, and the execution completed within the time limit.", "", "", "", "The execution failed due to an AttributeError in the `run_epoch` function.\nSpecifically, the 'DataLoader' object does not have an attribute\n`dataset_split`. This issue arises because the `seq_provider` function expects a\n`dataset_split` attribute from the loader, but such an attribute is not defined\nor provided in the DataLoader object. To fix this, modify the `seq_provider`\nfunction or its call to correctly access the dataset split information. For\nexample, you can directly pass the split name as an argument to the\n`seq_provider` function instead of relying on an undefined attribute.", "", "", "", "The training script executed successfully without any errors or bugs. The model\nwas trained with different epoch values (5, 15, 30, and 50), and the best\nvalidation Harmonic Weighted Accuracy (HWA) of 0.957 was achieved with 50\nepochs. However, the test set performance showed a significant drop, with a CWA\nof 0.684, SWA of 0.640, and HWA of 0.661. This indicates potential overfitting\nto the validation set. Further experiments could focus on regularization\ntechniques or adjusting the learning rate to improve generalization.", "", "", ""], "exc_type": [null, null, null, null, null, "AttributeError", null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, {"args": ["'DataLoader' object has no attribute 'dataset_split'"], "name": "dataset_split", "obj": "<torch_geometric.loader.dataloader.DataLoader object at 0x7ff02ec91850>"}, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 219, "<module>", "val_loss, (preds, labs, seqs) = run_epoch(model, dev_loader, None)"], ["runfile.py", 168, "run_epoch", "seq_batch = seq_provider(loader.dataset_split, batch_idx, batch.num_graphs)"]], null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2416, "best_value": 0.2416}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2341, "best_value": 0.2341}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy of the model on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9281, "best_value": 0.9281}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy of the model on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9273, "best_value": 0.9273}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic weighted accuracy of the model on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9277, "best_value": 0.9277}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The color weighted accuracy of the model on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6724, "best_value": 0.6724}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The shape weighted accuracy of the model on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6301, "best_value": 0.6301}]}, {"metric_name": "test harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic weighted accuracy of the model on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6506, "best_value": 0.6506}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1361, "best_value": 0.1361}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1741, "best_value": 0.1741}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation Correct Weighted Accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.956, "best_value": 0.956}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation Simple Weighted Accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.956, "best_value": 0.956}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "Validation Harmonic Weighted Accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.956, "best_value": 0.956}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "Test Correct Weighted Accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.682, "best_value": 0.682}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "Test Simple Weighted Accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.638, "best_value": 0.638}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "Test Harmonic Weighted Accuracy. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.659, "best_value": 0.659}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2093, "best_value": 0.2093}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2012, "best_value": 0.2012}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The accuracy for color-weighted validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9442, "best_value": 0.9442}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy for shape-weighted validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9442, "best_value": 0.9442}]}, {"metric_name": "validation harmonic-weighted accuracy", "lower_is_better": false, "description": "The accuracy for harmonic-weighted validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9442, "best_value": 0.9442}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The accuracy for color-weighted test.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6763, "best_value": 0.6763}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy for shape-weighted test.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6329, "best_value": 0.6329}]}, {"metric_name": "test harmonic-weighted accuracy", "lower_is_better": false, "description": "The accuracy for harmonic-weighted test.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6539, "best_value": 0.6539}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating model performance on the training dataset.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.2242, "best_value": 0.2242}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.2602, "best_value": 0.2602}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.2617, "best_value": 0.2617}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating model performance on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.2153, "best_value": 0.2153}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.2557, "best_value": 0.2557}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.2643, "best_value": 0.2643}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correctly Weighted Accuracy (CWA) during validation.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.9403, "best_value": 0.9403}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.9173, "best_value": 0.9173}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.9102, "best_value": 0.9102}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Smooth Weighted Accuracy (SWA) during validation.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.94, "best_value": 0.94}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.9156, "best_value": 0.9156}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.9101, "best_value": 0.9101}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "The Harmonic Weighted Accuracy (HWA) during validation.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.9402, "best_value": 0.9402}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.9165, "best_value": 0.9165}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.9102, "best_value": 0.9102}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "The Correctly Weighted Accuracy (CWA) during testing.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.6757, "best_value": 0.6757}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.6701, "best_value": 0.6701}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.6746, "best_value": 0.6746}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "The Smooth Weighted Accuracy (SWA) during testing.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.6326, "best_value": 0.6326}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.628, "best_value": 0.628}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.6324, "best_value": 0.6324}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "The Harmonic Weighted Accuracy (HWA) during testing.", "data": [{"dataset_name": "SPR_BENCH (batch size 32)", "final_value": 0.6534, "best_value": 0.6534}, {"dataset_name": "SPR_BENCH (batch size 64)", "final_value": 0.6484, "best_value": 0.6484}, {"dataset_name": "SPR_BENCH (batch size 128)", "final_value": 0.6528, "best_value": 0.6528}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.279486, "best_value": 0.279486}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.243485, "best_value": 0.243485}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.227589, "best_value": 0.227589}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.209115, "best_value": 0.209115}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.28719, "best_value": 0.28719}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.239435, "best_value": 0.239435}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.217457, "best_value": 0.217457}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.20224, "best_value": 0.20224}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy of color predictions during validation.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.890123, "best_value": 0.890123}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.926789, "best_value": 0.926789}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.937771, "best_value": 0.937771}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.942041, "best_value": 0.942041}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy of shape predictions during validation.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.886234, "best_value": 0.886234}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.926171, "best_value": 0.926171}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.936868, "best_value": 0.936868}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.941925, "best_value": 0.941925}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic mean of weighted accuracies during validation.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.888174, "best_value": 0.888174}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.92648, "best_value": 0.92648}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.937319, "best_value": 0.937319}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.941983, "best_value": 0.941983}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy of color predictions during testing.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.670191, "best_value": 0.670191}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.670677, "best_value": 0.670677}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.674566, "best_value": 0.674566}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.678698, "best_value": 0.678698}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy of shape predictions during testing.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.629329, "best_value": 0.629329}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.628373, "best_value": 0.628373}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.631792, "best_value": 0.631792}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.635676, "best_value": 0.635676}]}, {"metric_name": "test harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic mean of weighted accuracies during testing.", "data": [{"dataset_name": "SPR_BENCH | embedding dimension: 16", "final_value": 0.649118, "best_value": 0.649118}, {"dataset_name": "SPR_BENCH | embedding dimension: 32", "final_value": 0.648836, "best_value": 0.648836}, {"dataset_name": "SPR_BENCH | embedding dimension: 64", "final_value": 0.652479, "best_value": 0.652479}, {"dataset_name": "SPR_BENCH | embedding dimension: 128", "final_value": 0.656483, "best_value": 0.656483}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.2412, "best_value": 0.2412}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.2615, "best_value": 0.2615}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.2463, "best_value": 0.2463}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.2528, "best_value": 0.2528}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.3447, "best_value": 0.3447}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.2339, "best_value": 0.2339}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.2567, "best_value": 0.2567}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.247, "best_value": 0.247}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.2504, "best_value": 0.2504}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.3428, "best_value": 0.3428}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correct Weighted Average (CWA) during validation", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.9282, "best_value": 0.9282}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.917, "best_value": 0.917}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.9202, "best_value": 0.9202}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.9244, "best_value": 0.9244}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.8812, "best_value": 0.8812}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Smoothed Weighted Average (SWA) during validation", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.9276, "best_value": 0.9276}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.9154, "best_value": 0.9154}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.9174, "best_value": 0.9174}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.9252, "best_value": 0.9252}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.8819, "best_value": 0.8819}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "The Harmonic Weighted Average (HWA) during validation", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.9279, "best_value": 0.9279}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.9162, "best_value": 0.9162}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.9188, "best_value": 0.9188}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.9248, "best_value": 0.9248}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.8815, "best_value": 0.8815}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "The Correct Weighted Average (CWA) on the test dataset", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.6726, "best_value": 0.6726}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.6707, "best_value": 0.6707}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.6755, "best_value": 0.6755}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.6658, "best_value": 0.6658}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.6586, "best_value": 0.6586}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "The Smoothed Weighted Average (SWA) on the test dataset", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.6302, "best_value": 0.6302}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.6289, "best_value": 0.6289}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.6337, "best_value": 0.6337}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.6243, "best_value": 0.6243}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.6205, "best_value": 0.6205}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "The Harmonic Weighted Average (HWA) on the test dataset", "data": [{"dataset_name": "SPR_BENCH (weight_decay = 0)", "final_value": 0.6507, "best_value": 0.6507}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0001)", "final_value": 0.6491, "best_value": 0.6491}, {"dataset_name": "SPR_BENCH (weight_decay = 0.0005)", "final_value": 0.6539, "best_value": 0.6539}, {"dataset_name": "SPR_BENCH (weight_decay = 0.001)", "final_value": 0.6443, "best_value": 0.6443}, {"dataset_name": "SPR_BENCH (weight_decay = 0.01)", "final_value": 0.639, "best_value": 0.639}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, where lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1508, "best_value": 0.1508}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, where lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.19, "best_value": 0.19}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The accuracy of color classification during validation, weighted by class.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9473, "best_value": 0.9473}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The accuracy of shape classification during validation, weighted by class.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.946, "best_value": 0.946}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic mean of color and shape weighted accuracies during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9466, "best_value": 0.9466}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The accuracy of color classification during testing, weighted by class.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6815, "best_value": 0.6815}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The accuracy of shape classification during testing, weighted by class.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6377, "best_value": 0.6377}]}, {"metric_name": "test harmonic weighted accuracy", "lower_is_better": false, "description": "The harmonic mean of color and shape weighted accuracies during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6589, "best_value": 0.6589}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final training loss for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.2412, "best_value": 0.2412}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.3252, "best_value": 0.3252}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.4661, "best_value": 0.4661}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final validation loss for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.2339, "best_value": 0.2339}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.2732, "best_value": 0.2732}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.3124, "best_value": 0.3124}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.3967, "best_value": 0.3967}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The final validation color weighted accuracy for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.9282, "best_value": 0.9282}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.911, "best_value": 0.911}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.8907, "best_value": 0.8907}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.8332, "best_value": 0.8332}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The final validation shape weighted accuracy for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.9276, "best_value": 0.9276}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.9115, "best_value": 0.9115}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.8905, "best_value": 0.8905}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.8353, "best_value": 0.8353}]}, {"metric_name": "validation harmonic weighted accuracy", "lower_is_better": false, "description": "The final validation harmonic weighted accuracy for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.9279, "best_value": 0.9279}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.9112, "best_value": 0.9112}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.8906, "best_value": 0.8906}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.8343, "best_value": 0.8343}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The test color weighted accuracy for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.6726, "best_value": 0.6726}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.6648, "best_value": 0.6648}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.6629, "best_value": 0.6629}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.639, "best_value": 0.639}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The test shape weighted accuracy for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.6302, "best_value": 0.6302}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.6243, "best_value": 0.6243}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.6244, "best_value": 0.6244}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.6041, "best_value": 0.6041}]}, {"metric_name": "test harmonic weighted accuracy", "lower_is_better": false, "description": "The test harmonic weighted accuracy for the model.", "data": [{"dataset_name": "SPR_BENCH (dropout rate = 0.0)", "final_value": 0.6507, "best_value": 0.6507}, {"dataset_name": "SPR_BENCH (dropout rate = 0.2)", "final_value": 0.6439, "best_value": 0.6439}, {"dataset_name": "SPR_BENCH (dropout rate = 0.4)", "final_value": 0.643, "best_value": 0.643}, {"dataset_name": "SPR_BENCH (dropout rate = 0.6)", "final_value": 0.6211, "best_value": 0.6211}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training, lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1119, "best_value": 0.1119}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1723, "best_value": 0.1723}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correct Weighted Accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.957, "best_value": 0.957}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Smoothed Weighted Accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.956, "best_value": 0.956}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "The Harmonic Weighted Accuracy on the validation set, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.957, "best_value": 0.957}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "The Correct Weighted Accuracy on the test set, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.684, "best_value": 0.684}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "The Smoothed Weighted Accuracy on the test set, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.64, "best_value": 0.64}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "The Harmonic Weighted Accuracy on the test set, higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.661, "best_value": 0.661}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1116, "best_value": 0.1116}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1713, "best_value": 0.1713}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "CWA metric during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.957, "best_value": 0.957}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "SWA metric during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.956, "best_value": 0.956}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "HWA metric during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.956, "best_value": 0.956}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "CWA metric during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.685, "best_value": 0.685}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "SWA metric during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.64, "best_value": 0.64}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "HWA metric during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.662, "best_value": 0.662}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Measures the loss during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1108, "best_value": 0.1108}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1678, "best_value": 0.1678}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation metric for CWA.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.96, "best_value": 0.96}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation metric for SWA.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.959, "best_value": 0.959}]}, {"metric_name": "validation HWA", "lower_is_better": false, "description": "Validation metric for HWA.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.959, "best_value": 0.959}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "Test metric for CWA.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.683, "best_value": 0.683}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "Test metric for SWA.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.639, "best_value": 0.639}]}, {"metric_name": "test HWA", "lower_is_better": false, "description": "Test metric for HWA.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.66, "best_value": 0.66}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_metric_curves.png", "../../logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_5_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_15_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_30_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_50_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_5_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_15_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_30_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_50_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_final_hwa_bar.png", "../../logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_confusion_matrix_best.png"], ["../../logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_hwa_curves.png", "../../logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_final_hwa_bar.png"], ["../../logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_val_hwa.png", "../../logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_test_hwa_bars.png"], ["../../logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_embSweep_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_embSweep_HWA_curves.png", "../../logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_test_HWA_bar.png"], [], ["../../logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_val_HWA_curves.png", "../../logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_test_HWA_bars.png"], ["../../logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_val_HWA.png", "../../logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_test_HWA_by_layers.png"], ["../../logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_val_HWA_curves.png", "../../logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_test_HWA_bar.png"], ["../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_5_loss_curves.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_15_loss_curves.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_30_loss_curves.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_50_loss_curves.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_5_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_15_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_30_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_50_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_final_hwa_bar.png", "../../logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_confusion_matrix_best.png"], ["../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_5_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_15_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_30_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_50_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_5_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_15_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_30_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_50_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_final_hwa_bar.png", "../../logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_confusion_matrix_best.png"], ["../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_5_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_15_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_30_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_50_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_5_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_15_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_30_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_50_hwa_curve.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_final_hwa_bar.png", "../../logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_confusion_matrix_best.png"], ["../../logs/0-run/experiment_results/seed_aggregation_d32578ed13ea45499fc5799acdecc7a5/spr_bench_aggregated_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_d32578ed13ea45499fc5799acdecc7a5/spr_bench_aggregated_val_hwa.png", "../../logs/0-run/experiment_results/seed_aggregation_d32578ed13ea45499fc5799acdecc7a5/spr_bench_final_val_hwa_bar.png"]], "plot_paths": [["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_metric_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_5_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_15_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_30_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_50_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_5_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_15_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_30_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_50_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_final_hwa_bar.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_confusion_matrix_best.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_hwa_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_final_hwa_bar.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_val_hwa.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_test_hwa_bars.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_embSweep_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_embSweep_HWA_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_test_HWA_bar.png"], [], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_val_HWA_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_test_HWA_bars.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_val_HWA.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_test_HWA_by_layers.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_val_HWA_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_test_HWA_bar.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_5_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_15_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_30_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_50_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_5_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_15_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_30_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_50_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_final_hwa_bar.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_confusion_matrix_best.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_5_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_15_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_30_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_50_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_5_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_15_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_30_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_50_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_final_hwa_bar.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_confusion_matrix_best.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_5_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_15_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_30_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_50_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_5_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_15_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_30_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_50_hwa_curve.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_final_hwa_bar.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_confusion_matrix_best.png"], ["experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_d32578ed13ea45499fc5799acdecc7a5/spr_bench_aggregated_loss_curves.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_d32578ed13ea45499fc5799acdecc7a5/spr_bench_aggregated_val_hwa.png", "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_d32578ed13ea45499fc5799acdecc7a5/spr_bench_final_val_hwa_bar.png"]], "plot_analyses": [[{"analysis": "This plot shows the loss curves for both the training and validation datasets over 5 epochs. The training loss decreases steadily, indicating that the model is learning effectively during training. The validation loss also decreases, which suggests that the model is generalizing well to unseen data without overfitting. The convergence of the training and validation loss curves towards the end indicates a stable training process.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the validation weighted accuracies (Color-Weighted Accuracy, Shape-Weighted Accuracy, and another metric HWA) over 5 epochs. The accuracies for all metrics improve consistently with the number of epochs, reaching approximately 93% by the 5th epoch. The close alignment of the three metrics suggests that the model performs uniformly across different evaluation criteria, demonstrating robustness.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_metric_curves.png"}, {"analysis": "This confusion matrix illustrates the model's performance on the test dataset. The true positives (3212) and true negatives (3497) are relatively high, indicating good predictive accuracy. However, there are noticeable false positives (2155) and false negatives (1136), which suggest areas for improvement in distinguishing between the two classes. The imbalance in errors between the two classes could be further analyzed to understand potential biases in the model.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b57fa289731a4fed93750e2c6c5c0314_proc_1541633/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the loss curves for training and validation over 5 epochs. Both training and validation losses decrease steadily, indicating that the model is learning effectively without overfitting within this range. The convergence of the two curves suggests balanced performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_5_loss_curves.png"}, {"analysis": "This plot presents the loss curves over 15 epochs. Training and validation losses decrease consistently, with slight fluctuations in the validation loss. This indicates that the model is learning effectively, but the fluctuations suggest minor instability in generalization, which may require further hyperparameter tuning.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_15_loss_curves.png"}, {"analysis": "The loss curves over 30 epochs show a consistent downward trend for both training and validation losses. The fluctuations in validation loss are reduced compared to the previous plot, indicating improved stability. The model appears to generalize well over this extended training period.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_30_loss_curves.png"}, {"analysis": "The loss curves over 50 epochs show sustained improvement in training loss and a gradual decrease in validation loss. However, a spike in validation loss towards the end suggests potential overfitting or instability, which may require early stopping or regularization techniques.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_50_loss_curves.png"}, {"analysis": "The plot shows the Harmonic Weighted Accuracy (HWA) over 5 epochs. HWA improves steadily, with a slight dip at epoch 4 before reaching its peak at epoch 5. This indicates consistent improvement in model performance with minor fluctuations.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_5_hwa_curve.png"}, {"analysis": "The HWA plot over 15 epochs shows a steady increase in accuracy, with occasional fluctuations. The model achieves a higher accuracy compared to the shorter training duration, indicating improved performance with extended training.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_15_hwa_curve.png"}, {"analysis": "The HWA plot over 30 epochs shows a consistent upward trend, with the accuracy stabilizing at a high level after around 15 epochs. This indicates that the model benefits from longer training, achieving strong generalization.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_30_hwa_curve.png"}, {"analysis": "The HWA plot over 50 epochs shows a high and stable accuracy after 15 epochs, with occasional dips. This suggests that the model maintains strong performance over extended training but may experience minor instabilities. Early stopping could be considered to avoid unnecessary computations.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_run_50_hwa_curve.png"}, {"analysis": "This bar chart compares the final validation HWA across different epoch settings. The accuracies are consistently high, with longer training durations slightly outperforming shorter ones. This highlights the benefit of extended training while maintaining high performance across all settings.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_final_hwa_bar.png"}, {"analysis": "The confusion matrix indicates the model's performance on the test set. The true positive and true negative counts suggest reasonably good classification performance, but the high false positive (2133) and false negative (1055) counts highlight potential areas for improvement in precision and recall.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c4ea7b14aa49f080d6935866182eaa_proc_1544297/spr_bench_confusion_matrix_best.png"}], [{"analysis": "The cross-entropy loss curves indicate that all learning rates lead to a decrease in training and validation loss over the epochs, suggesting that the model is learning effectively. Among the learning rates, 0.002 achieves the lowest validation loss by the end of training, indicating better generalization. However, the validation loss for learning rate 0.002 initially increases after the second epoch before decreasing again, which may suggest some instability in training with this learning rate.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_loss_curves.png"}, {"analysis": "The Harmonic Weighted Accuracy (HWA) over epochs shows that higher learning rates (0.001 and 0.002) achieve higher accuracy earlier in training compared to lower learning rates. Learning rate 0.002 achieves the highest HWA, consistently outperforming other learning rates throughout the epochs. This suggests that higher learning rates are more effective for this task, although careful monitoring is needed to avoid potential instability.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_hwa_curves.png"}, {"analysis": "The bar chart comparing final development HWA across learning rates reveals that the highest learning rate (0.002) achieves the best performance, followed closely by 0.001 and 0.0005. Learning rate 0.0003 lags behind, indicating that it may be too small to allow the model to converge effectively within the given epochs. This confirms the observations from the loss and HWA curves that higher learning rates are more suitable for this task.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00d00b477a7f470da265bfb642329be2_proc_1544298/SPR_BENCH_final_hwa_bar.png"}], [{"analysis": "This plot illustrates the training and validation loss trends for different batch sizes (32, 64, 128). Smaller batch sizes (e.g., 32) show faster convergence during training and lower final validation loss compared to larger batch sizes. Larger batch sizes (e.g., 128) converge more slowly and have higher validation loss, indicating possible underfitting. This suggests that smaller batch sizes might be more effective for this task.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the harmonic weighted accuracy (HWA) on the validation set across epochs for different batch sizes. Smaller batch sizes (e.g., 32) achieve higher HWA, while larger batch sizes (e.g., 128) lag behind, particularly in early epochs. By the final epoch, the larger batch sizes catch up somewhat, but the smaller batch size still outperforms overall. This reinforces the earlier observation that smaller batch sizes are better suited for this task.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_val_hwa.png"}, {"analysis": "This bar chart compares the test harmonic weighted accuracy (HWA) across different batch sizes. The results show minimal differences between batch sizes, with all achieving similar performance. This indicates that while batch size impacts training and validation dynamics, the final test performance is relatively stable across batch sizes.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_050d99b81d304503af658e5941d5cb3b_proc_1544299/SPR_BENCH_test_hwa_bars.png"}], [{"analysis": "This plot illustrates the training and validation loss curves for different embedding dimensions (16, 32, 64, 128). As the embedding size increases, both training and validation losses decrease more rapidly, indicating better learning capacity with larger embeddings. However, the gap between training and validation loss narrows for larger embeddings, suggesting reduced overfitting. Embedding size 128 achieves the lowest loss values, but further experiments are needed to confirm if this trend holds for more epochs.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_embSweep_loss_curves.png"}, {"analysis": "This plot shows the Harmonic Weighted Accuracy (HWA) on the validation set across epochs for various embedding dimensions. Larger embeddings consistently achieve higher HWA, with embedding size 128 performing the best. The accuracy curves for all embeddings plateau after 3-4 epochs, indicating convergence. This implies that larger embeddings better capture the structural and relational information in the data, leading to improved performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_embSweep_HWA_curves.png"}, {"analysis": "This bar chart compares the final test HWA for different embedding sizes. The results suggest that larger embeddings (64 and 128) slightly outperform smaller ones (16 and 32). However, the improvement margin is narrow, indicating diminishing returns with increasing embedding size. This might suggest that embedding size beyond 64 does not significantly enhance performance and could be a trade-off point between computational cost and accuracy.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_1bda7ce1d75a4eb1818945688d4a9054_proc_1544300/SPR_BENCH_test_HWA_bar.png"}], [], [{"analysis": "The first set of plots shows the training and validation loss curves for different weight decay values over five epochs. Lower weight decay values (0, 0.0001, 0.0005) lead to faster convergence of both training and validation loss compared to higher weight decay values (0.001, 0.01). However, the validation loss for weight decay 0.01 stagnates and remains higher, indicating over-regularization. The weight decay values 0, 0.0001, and 0.0005 show similar performance, with weight decay 0.0005 slightly outperforming the others in terms of validation loss.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot illustrates the HWA (Hypothetical Weighted Accuracy) on the validation set over epochs for different weight decay values. Weight decay 0.0005 achieves the highest HWA, showing consistent improvement across epochs and surpassing other configurations. Weight decay 0.01 performs the worst, with lower HWA and inconsistent progression, likely due to over-regularization. The results suggest that moderate weight decay values (0.0005) balance regularization and model performance effectively.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_val_HWA_curves.png"}, {"analysis": "The third plot displays the test HWA for different weight decay values. The performance across all weight decay values appears nearly identical, with no significant variation. This suggests that while weight decay impacts validation performance, its effect on the final test performance is minimal. This could indicate that the model has reached a performance plateau on the test set, or the test set may not be sensitive enough to reflect minor hyperparameter differences.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_983c661969a440b1ba04e987f3e9d37b_proc_1544299/SPR_BENCH_test_HWA_bars.png"}], [{"analysis": "The plot shows the cross-entropy loss for both training and validation sets across different learning rates (L1, L2, L3, L4) over five epochs. For all learning rates, the training loss steadily decreases, indicating the model is learning effectively. Validation loss also decreases for most learning rates, but the rate of decrease varies. L3 and L4 exhibit the most stable and consistent improvement in validation loss, suggesting these learning rates may be optimal for this task. L1 and L2 show higher validation losses, which might indicate underfitting or suboptimal hyperparameter settings.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the Harmonic Weighted Accuracy (HWA) on the validation set for different learning rates (L1, L2, L3, L4) over five epochs. HWA consistently improves across epochs for all learning rates, with L4 achieving the highest accuracy, followed by L3. This suggests that higher learning rates (L3 and L4) are more effective in capturing the relationships in the data and improving performance. The plateau observed for L3 and the slight decline for L4 in later epochs might indicate the need for regularization or early stopping to prevent overfitting.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_val_HWA.png"}, {"analysis": "This bar chart compares the Test HWA across different numbers of GraphSAGE layers. The results indicate that the number of layers does not significantly impact the performance, as the HWA remains relatively constant across all configurations. This suggests that the model's performance is robust to changes in the depth of the GraphSAGE architecture, or that other factors (e.g., learning rate, batch size) might play a more critical role in determining performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_9a9c85ac8fb645e3bfeca7bde4c2425a_proc_1544300/SPR_BENCH_test_HWA_by_layers.png"}], [{"analysis": "The training and validation loss curves indicate that as the dropout rate increases, the model's training loss consistently decreases while the validation loss stabilizes at higher values. This suggests that higher dropout rates may lead to underfitting, as the model struggles to generalize well on the validation set. However, for lower dropout rates, the gap between training and validation loss is smaller, indicating better generalization.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation harmonic weighted accuracy (HWA) improves across all dropout rates as the number of epochs increases. However, lower dropout rates (e.g., p=0.0 and p=0.2) consistently achieve higher HWA compared to higher dropout rates (e.g., p=0.4 and p=0.6). This suggests that lower dropout rates allow the model to better capture the underlying patterns in the data, leading to superior performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_val_HWA_curves.png"}, {"analysis": "The final test HWA per dropout rate shows a slight decline in performance as the dropout rate increases. The results indicate that dropout rates of 0.0 and 0.2 yield the highest HWA, while dropout rates of 0.4 and 0.6 lead to marginally lower performance. This reinforces the observation that lower dropout rates are more effective for this task, likely due to the model's ability to retain more information during training.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_7518589539bc45d08d8961c3b27115d1_proc_1544297/SPR_BENCH_test_HWA_bar.png"}], [{"analysis": "The loss curves for this run show a steady decrease in both training and validation loss across five epochs. The validation loss remains slightly lower than the training loss, indicating the model is not overfitting and is generalizing well to unseen data. However, the limited number of epochs might prevent the model from reaching its optimal performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_5_loss_curves.png"}, {"analysis": "This run includes 15 epochs, and the loss curves demonstrate consistent improvement with a gradual decline in both training and validation loss. The presence of minor fluctuations in the validation loss indicates some variability, but the general trend is downward, suggesting the model is learning effectively. The gap between training and validation loss is minimal, further supporting good generalization.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_15_loss_curves.png"}, {"analysis": "With 30 epochs, the loss curves show a smooth and continuous decline in both training and validation loss. The validation loss stabilizes after around 20 epochs, which could indicate convergence. The close alignment of training and validation loss suggests the model is robust and well-tuned for the task.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_30_loss_curves.png"}, {"analysis": "The loss curves for 50 epochs show a steady decline in training loss and a stabilization of validation loss after approximately 30 epochs. There are minor fluctuations in the validation loss, which could be attributed to noise or slight overfitting. The extended training does not significantly improve the validation loss after convergence, suggesting diminishing returns beyond 30 epochs.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_50_loss_curves.png"}, {"analysis": "The harmonic weighted accuracy (HWA) for this run improves consistently over five epochs, reaching a high value of 0.92. This indicates that the model quickly learns and achieves strong performance within a limited number of epochs.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_5_hwa_curve.png"}, {"analysis": "The HWA for this run demonstrates rapid improvement during the initial epochs, followed by stabilization around epoch 10. The final HWA is approximately 0.94, showing that the model benefits from extended training up to 15 epochs.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_15_hwa_curve.png"}, {"analysis": "The HWA for this run reaches its peak performance around 30 epochs, stabilizing at approximately 0.96. This suggests that 30 epochs are sufficient for the model to achieve optimal performance on the validation set.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_30_hwa_curve.png"}, {"analysis": "The HWA for this run stabilizes at approximately 0.96 after 30 epochs, with minor fluctuations observed in later epochs. This indicates that while the model maintains strong performance, additional training does not lead to significant improvements.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_run_50_hwa_curve.png"}, {"analysis": "The bar chart comparing final validation HWA across different epoch settings shows consistent improvement as the number of epochs increases, with the highest performance achieved for 30 and 50 epochs. However, the difference between 30 and 50 epochs is negligible, suggesting that 30 epochs might be the most efficient choice.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_final_hwa_bar.png"}, {"analysis": "The confusion matrix indicates that the model achieves a good balance between true positives and true negatives, but there are notable false positives and false negatives. Specifically, the model struggles slightly more with predicting class 0, as evidenced by the higher false positive rate for class 1. This could be addressed by refining the loss function or class weighting.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/spr_bench_confusion_matrix_best.png"}], [{"analysis": "The training and validation loss curves indicate consistent improvement over epochs. Both losses decrease steadily, with validation loss slightly diverging around epoch 4, suggesting potential overfitting or noise in the validation set. However, the final convergence of both losses shows a well-trained model.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_5_loss_curves.png"}, {"analysis": "The loss curves demonstrate a steady decline in both training and validation losses over 15 epochs. There are minor fluctuations in the validation loss, which might indicate variability in the validation set. The overall trend suggests effective training with minimal overfitting.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_15_loss_curves.png"}, {"analysis": "The training and validation losses continue to decrease consistently over 30 epochs. The convergence of the validation loss with the training loss indicates that the model generalizes well to unseen data. The stability of the loss curves suggests a well-optimized learning process.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_30_loss_curves.png"}, {"analysis": "The loss curves over 50 epochs show a smooth and consistent decrease in both training and validation losses. The validation loss remains closely aligned with the training loss, indicating excellent generalization. The extended training period does not appear to result in overfitting.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_50_loss_curves.png"}, {"analysis": "The Harmonic Weighted Accuracy (HWA) improves steadily over 5 epochs, with minor fluctuations around epoch 4. The final HWA indicates significant improvement, reflecting the model's ability to generalize well.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_5_hwa_curve.png"}, {"analysis": "The HWA improves consistently over 15 epochs, with minor oscillations early in training. The stability after epoch 8 suggests that the model achieves robust performance with sufficient training.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_15_hwa_curve.png"}, {"analysis": "The HWA shows a strong upward trend over 30 epochs, stabilizing after epoch 10. This indicates that the model reaches peak performance and maintains it, demonstrating its capability to handle the task effectively.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_30_hwa_curve.png"}, {"analysis": "The HWA over 50 epochs shows consistent improvement, with minor dips due to potential noise or variability in the validation set. The final accuracy demonstrates that extended training further enhances the model's performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_run_50_hwa_curve.png"}, {"analysis": "The bar chart shows that all epoch settings achieve high final validation HWA values, with longer training durations (30 and 50 epochs) yielding slightly better results. This suggests that the model benefits from extended training, though the improvement is marginal.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_final_hwa_bar.png"}, {"analysis": "The confusion matrix indicates that the model achieves a good balance between true positives and true negatives. However, there is room for improvement in reducing false positives and false negatives, especially in the 0-class predictions.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/spr_bench_confusion_matrix_best.png"}], [{"analysis": "The loss curves show a steady decrease in both training and validation loss over the epochs, indicating that the model is learning effectively. However, for the 5-epoch run, the validation loss plateaus slightly around epoch 3 before decreasing again, which may suggest that more epochs are needed for stabilization. The training and validation losses are close, which is a positive sign of minimal overfitting.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_5_loss_curves.png"}, {"analysis": "The loss curves for the 15-epoch run show consistent decreases in both training and validation loss. The validation loss stabilizes after around 10 epochs, suggesting that the model has converged. The absence of significant divergence between training and validation losses indicates that the model generalizes well to unseen data.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_15_loss_curves.png"}, {"analysis": "The 30-epoch run shows further reduction in both training and validation losses, with the validation loss stabilizing after around 20 epochs. The stability in the validation loss over the later epochs suggests that the model has reached a point of convergence, and additional epochs may not yield significant improvements.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_30_loss_curves.png"}, {"analysis": "The 50-epoch run demonstrates minimal further improvement in validation loss compared to the 30-epoch run, with occasional spikes in validation loss likely due to fluctuations in the optimization process. The training loss continues to decrease, but the marginal gains in validation loss suggest diminishing returns with additional epochs.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_50_loss_curves.png"}, {"analysis": "The Harmonic Weighted Accuracy (HWA) for the 5-epoch run shows an upward trend, with a slight dip at epoch 4 before reaching the highest value at epoch 5. This indicates that the model improves over epochs and achieves reasonable performance within a limited training time.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_5_hwa_curve.png"}, {"analysis": "The HWA for the 15-epoch run shows steady improvement, with minor fluctuations around epochs 4-7 before stabilizing at a high value. This suggests that the model benefits from additional epochs, achieving better performance as training progresses.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_15_hwa_curve.png"}, {"analysis": "The HWA for the 30-epoch run shows a consistent upward trend with minor fluctuations, stabilizing at a high value after around 20 epochs. This indicates that the model achieves strong performance and benefits from extended training.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_30_hwa_curve.png"}, {"analysis": "The HWA for the 50-epoch run shows stabilization at a high value after around 20 epochs, similar to the 30-epoch run. Occasional dips suggest minor instability, but overall performance remains strong, indicating that additional training beyond 30 epochs yields diminishing returns.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_run_50_hwa_curve.png"}, {"analysis": "The final validation HWA comparison shows that models trained for 15, 30, and 50 epochs achieve nearly identical final performance, while the 5-epoch model is slightly lower. This suggests that training for more than 15 epochs does not significantly improve performance, and 15 epochs may be an optimal trade-off between training time and performance.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_final_hwa_bar.png"}, {"analysis": "The confusion matrix indicates that the model performs reasonably well on the test set, with a higher number of true positives and true negatives compared to false positives and false negatives. However, there is a noticeable number of misclassifications (2113 false positives and 1064 false negatives), suggesting areas for potential improvement in model precision and recall.", "plot_path": "experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/spr_bench_confusion_matrix_best.png"}], []], "vlm_feedback_summary": ["The plots demonstrate a successful training process with decreasing loss and\nimproving weighted accuracies. The confusion matrix highlights good performance\nbut also reveals areas for improvement in reducing classification errors.", "The provided plots show that the model generally improves with longer training\ndurations, achieving high accuracy and stable loss trends. However, minor\nfluctuations in validation performance and a spike in loss at 50 epochs suggest\nareas for refinement, such as early stopping or regularization. The confusion\nmatrix indicates overall good classification performance but highlights\nopportunities to improve precision and recall.", "The analysis of the plots reveals that higher learning rates (0.001 and 0.002)\nconsistently lead to better performance in terms of both loss reduction and\nHarmonic Weighted Accuracy (HWA). Learning rate 0.002 achieves the best results\nbut may require careful monitoring for stability. Lower learning rates, such as\n0.0003, show slower convergence and inferior performance. Overall, the results\nsuggest that tuning the learning rate is critical, with higher values being more\neffective for this task.", "The provided plots indicate that smaller batch sizes (e.g., 32) lead to better\ntraining and validation performance, with faster convergence and lower\nvalidation loss. However, the test performance remains consistent across batch\nsizes, suggesting that the choice of batch size primarily affects the training\nprocess rather than the final generalization capability.", "The analysis highlights that larger embedding dimensions improve convergence\nrates and performance on both validation and test metrics (HWA). However, the\nimprovement margin diminishes with embedding sizes beyond 64, suggesting a\ntrade-off point for computational efficiency.", "[]", "The results indicate that moderate weight decay values (e.g., 0.0005) achieve\nthe best balance between regularization and performance. Validation loss and\nvalidation HWA metrics consistently favor this configuration, while test HWA\nshows little sensitivity to weight decay changes, suggesting a performance\nplateau on the test set.", "The plots provide valuable insights into the model's performance under different\nlearning rates and configurations. Higher learning rates (L3 and L4) demonstrate\nbetter performance in terms of validation loss and HWA, while the number of\nGraphSAGE layers does not significantly affect the HWA on the test set.\nRegularization or early stopping might be necessary to address the slight\noverfitting observed for higher learning rates in later epochs.", "The results highlight the importance of tuning the dropout rate for optimal\nperformance. Lower dropout rates (0.0 and 0.2) consistently outperform higher\ndropout rates in terms of both validation HWA and final test HWA. The training\nand validation loss curves further suggest that higher dropout rates may lead to\nunderfitting, while lower dropout rates strike a better balance between training\nand validation performance. These findings indicate that careful hyperparameter\ntuning is critical for maximizing the model's performance on the SPR task.", "The plots provide valuable insights into the training dynamics and model\nperformance. The loss curves demonstrate effective learning and good\ngeneralization, with diminishing returns beyond 30 epochs. The harmonic weighted\naccuracy plots confirm that the model achieves optimal performance around 30\nepochs. The confusion matrix highlights areas for potential improvement in\nclass-specific predictions, particularly for class 0.", "The plots demonstrate consistent improvement in training and validation losses,\nas well as harmonic weighted accuracy, across various epoch settings. The model\nshows strong generalization capabilities and benefits from extended training, as\nevidenced by the final metrics and confusion matrix analysis.", "The plots indicate that the model learns effectively and achieves strong\nperformance on the SPR_BENCH dataset. Validation loss and HWA metrics stabilize\nafter around 15-30 epochs, suggesting that extended training beyond this range\nyields diminishing returns. The confusion matrix highlights reasonable\nclassification performance with room for improvement in reducing\nmisclassifications.", "[]"], "exec_time": [13.291048526763916, 510.0788035392761, 39.67810249328613, 36.10714054107666, 75.78744339942932, 11.29474401473999, 51.59666585922241, 44.0899498462677, 122.70870447158813, 406.9342451095581, 139.94654369354248, 149.25833249092102, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['experiment_data']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor dname, ddata in experiment_data.items():\n    # ----- gather losses -----\n    train_losses = np.asarray(ddata[\"losses\"].get(\"train\", []), dtype=float)\n    val_losses = np.asarray(ddata[\"losses\"].get(\"val\", []), dtype=float)\n    epochs = np.arange(1, len(train_losses) + 1)\n\n    # ----- gather validation metrics -----\n    val_metrics = ddata[\"metrics\"].get(\"val\", [])\n    cwa = [m[\"cwa\"] for m in val_metrics] if val_metrics else []\n    swa = [m[\"swa\"] for m in val_metrics] if val_metrics else []\n    hwa = [m[\"hwa\"] for m in val_metrics] if val_metrics else []\n\n    # ----- build test confusion matrix -----\n    preds = ddata.get(\"predictions\", [])\n    gts = ddata.get(\"ground_truth\", [])\n    labels = sorted(set(gts + preds))\n    lab2idx = {l: i for i, l in enumerate(labels)}\n    conf = np.zeros((len(labels), len(labels)), dtype=int)\n    for gt, pr in zip(gts, preds):\n        conf[lab2idx[gt], lab2idx[pr]] += 1\n\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_losses, label=\"Train\")\n        plt.plot(epochs, val_losses, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname} Loss Curves\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # 2) Metric curves\n    try:\n        plt.figure()\n        if cwa:\n            plt.plot(epochs, cwa, label=\"CWA\")\n        if swa:\n            plt.plot(epochs, swa, label=\"SWA\")\n        if hwa:\n            plt.plot(epochs, hwa, label=\"HWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{dname} Validation Weighted Accuracies\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_metric_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dname}: {e}\")\n        plt.close()\n\n    # 3) Confusion matrix\n    try:\n        if conf.size:\n            plt.figure()\n            im = plt.imshow(conf, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xticks(range(len(labels)), labels, rotation=45)\n            plt.yticks(range(len(labels)), labels)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{dname} Confusion Matrix (Test)\")\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(j, i, str(conf[i, j]), ha=\"center\", va=\"center\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # ----- print test metrics -----\n    test_metrics = ddata[\"metrics\"].get(\"test\", {})\n    if test_metrics:\n        print(\n            f\"{dname} Test Metrics: \"\n            + \", \".join(f\"{k.upper()}={v:.3f}\" for k, v in test_metrics.items())\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs_dict = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\n    run_keys = sorted(\n        [k for k in runs_dict if k.startswith(\"run_\")],\n        key=lambda s: int(s.split(\"_\")[-1]),\n    )\n\n    # --------------------------------------------------------------\n    # 1. training/validation loss curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            losses = runs_dict[rk][\"losses\"]\n            epochs = np.arange(1, len(losses[\"train\"]) + 1)\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{rk}: Loss Curves (SPR_BENCH)\")\n            plt.legend()\n            fname = f\"spr_bench_{rk}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 2. validation HWA curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            hwa = [m[\"hwa\"] for m in runs_dict[rk][\"metrics\"][\"val\"]]\n            epochs = np.arange(1, len(hwa) + 1)\n            plt.figure()\n            plt.plot(epochs, hwa, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Harmonic Weighted Acc\")\n            plt.title(f\"{rk}: Validation HWA (SPR_BENCH)\")\n            fname = f\"spr_bench_{rk}_hwa_curve.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating HWA plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 3. final HWA per run (bar chart)\n    # --------------------------------------------------------------\n    try:\n        final_hwa = [runs_dict[rk][\"final_val_hwa\"] for rk in run_keys]\n        plt.figure()\n        plt.bar(run_keys, final_hwa)\n        plt.ylabel(\"Final Val HWA\")\n        plt.title(\"SPR_BENCH: Final Validation HWA by Epoch Setting\")\n        plt.xticks(rotation=45)\n        fname = \"spr_bench_final_hwa_bar.png\"\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final HWA bar chart: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 4. confusion matrix on test set for best run\n    # --------------------------------------------------------------\n    try:\n        best_r = runs_dict[\"best_run\"] if \"best_run\" in runs_dict else None\n        if best_r is not None:\n            preds = runs_dict[\"predictions\"]\n            golds = runs_dict[\"ground_truth\"]\n            labels = sorted(list(set(golds) | set(preds)))\n            lbl2idx = {l: i for i, l in enumerate(labels)}\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for g, p in zip(golds, preds):\n                cm[lbl2idx[g], lbl2idx[p]] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xticks(range(len(labels)), labels, rotation=45)\n            plt.yticks(range(len(labels)), labels)\n            plt.title(\n                \"SPR_BENCH Test Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\"\n            )\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            fname = \"spr_bench_confusion_matrix_best.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\"No best_run info for confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # print stored test metrics\n    # --------------------------------------------------------------\n    try:\n        test_metrics = runs_dict[\"metrics_test\"]\n        print(\n            f\"Stored TEST metrics -> CWA: {test_metrics['cwa']:.3f}, \"\n            f\"SWA: {test_metrics['swa']:.3f}, \"\n            f\"HWA: {test_metrics['hwa']:.3f}\"\n        )\n    except Exception as e:\n        print(f\"Error printing test metrics: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"learning_rate\", {}).get(\"SPR_BENCH\", {})\nlrs = data.get(\"lr_values\", [])\ntrain_losses = data.get(\"losses\", {}).get(\"train\", [])\nval_losses = data.get(\"losses\", {}).get(\"val\", [])\nval_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n\n# helper: extract HWA curves\nhwa_curves = []\nfor mlist in val_metrics:\n    hwa_curves.append([ep_dict[\"hwa\"] for ep_dict in mlist])\n\n# ---------- figure 1: loss curves ----------\ntry:\n    plt.figure(figsize=(6, 4))\n    for lr, tr, va in zip(lrs, train_losses, val_losses):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"lr={lr} train\", linewidth=1.5)\n        plt.plot(epochs, va, linestyle=\"--\", label=f\"lr={lr} val\", linewidth=1.5)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nSolid: Train, Dashed: Validation\")\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ---------- figure 2: HWA curves ----------\ntry:\n    plt.figure(figsize=(6, 4))\n    for lr, hwa in zip(lrs, hwa_curves):\n        epochs = range(1, len(hwa) + 1)\n        plt.plot(epochs, hwa, label=f\"lr={lr}\", linewidth=1.5)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Harmonic Weighted Acc.\")\n    plt.title(\"SPR_BENCH HWA over Epochs\\nLines: Validation HWA per LR\")\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_hwa_curves.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HWA plot: {e}\")\n    plt.close()\n\n# ---------- figure 3: final HWA bar chart ----------\ntry:\n    final_hwa = [curve[-1] if curve else 0 for curve in hwa_curves]\n    plt.figure(figsize=(6, 4))\n    plt.bar([str(lr) for lr in lrs], final_hwa, color=\"skyblue\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Final Dev HWA\")\n    plt.title(\"SPR_BENCH Final Dev HWA vs Learning Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_hwa_bar.png\")\n    plt.savefig(fname, dpi=150)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final HWA bar plot: {e}\")\n    plt.close()\n\n# ---------- print numerical summary ----------\nfor lr, hwa in zip(lrs, final_hwa):\n    print(f\"LR {lr:>6}: Final Dev HWA = {hwa:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    runs = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\n    bs_keys = sorted(runs.keys(), key=lambda x: int(x))  # ['32','64','128']\n    # --------- PLOT 1: train/val loss ----------\n    try:\n        plt.figure()\n        for bs in bs_keys:\n            tr = runs[bs][\"losses\"][\"train\"]\n            va = runs[bs][\"losses\"][\"val\"]\n            epochs = list(range(1, len(tr) + 1))\n            plt.plot(epochs, tr, label=f\"train bs={bs}\")\n            plt.plot(epochs, va, \"--\", label=f\"val bs={bs}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # --------- PLOT 2: validation HWA ----------\n    try:\n        plt.figure()\n        for bs in bs_keys:\n            hwa = [m[\"hwa\"] for m in runs[bs][\"metrics\"][\"val\"]]\n            epochs = list(range(1, len(hwa) + 1))\n            plt.plot(epochs, hwa, label=f\"bs={bs}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation HWA\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_hwa.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA plot: {e}\")\n        plt.close()\n\n    # --------- PLOT 3: test HWA bar chart ----------\n    try:\n        plt.figure()\n        hwa_vals = [runs[bs][\"metrics\"][\"test\"][\"hwa\"] for bs in bs_keys]\n        plt.bar(bs_keys, hwa_vals)\n        plt.xlabel(\"Batch size\")\n        plt.ylabel(\"Test HWA\")\n        plt.title(\"SPR_BENCH: Test HWA vs Batch Size\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_hwa_bars.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test HWA plot: {e}\")\n        plt.close()\n\n    # --------- print final test metrics ----------\n    print(\"Final test metrics per batch size\")\n    print(\"bs\\tCWA\\tSWA\\tHWA\")\n    for bs in bs_keys:\n        m = runs[bs][\"metrics\"][\"test\"]\n        print(f\"{bs}\\t{m['cwa']:.3f}\\t{m['swa']:.3f}\\t{m['hwa']:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = exp[\"emb_dim\"][\"SPR_BENCH\"][\"runs\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = []\n\nif runs:  # proceed only if we actually have data\n    # collect data\n    emb_dims = [r[\"emb_dim\"] for r in runs]\n    train_loss_curves = [r[\"losses\"][\"train\"] for r in runs]\n    val_loss_curves = [r[\"losses\"][\"val\"] for r in runs]\n    val_hwa_curves = [[m[\"hwa\"] for m in r[\"metrics\"][\"val\"]] for r in runs]\n    test_hwa = [r[\"metrics\"][\"test\"][\"hwa\"] for r in runs]\n\n    # 1) combined loss curves\n    try:\n        plt.figure()\n        for emb, tr, vl in zip(emb_dims, train_loss_curves, val_loss_curves):\n            ep = range(1, len(tr) + 1)\n            plt.plot(ep, tr, label=f\"train (emb={emb})\", linestyle=\"-\")\n            plt.plot(ep, vl, label=f\"val (emb={emb})\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_embSweep_loss_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve figure: {e}\")\n        plt.close()\n\n    # 2) combined HWA curves\n    try:\n        plt.figure()\n        for emb, hwa in zip(emb_dims, val_hwa_curves):\n            ep = range(1, len(hwa) + 1)\n            plt.plot(ep, hwa, label=f\"emb={emb}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation HWA Across Epochs\")\n        plt.legend()\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_embSweep_HWA_curves.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA curve figure: {e}\")\n        plt.close()\n\n    # 3) bar chart of final test HWA\n    try:\n        plt.figure()\n        plt.bar([str(e) for e in emb_dims], test_hwa, color=\"skyblue\")\n        plt.xlabel(\"Embedding Dimension\")\n        plt.ylabel(\"Test HWA\")\n        plt.title(\n            \"SPR_BENCH: Final Test HWA by Embedding Size\\n(Left: smaller emb, Right: larger emb)\"\n        )\n        plt.tight_layout()\n        save_path = os.path.join(working_dir, \"SPR_BENCH_test_HWA_bar.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test HWA bar figure: {e}\")\n        plt.close()\n\n    # ---------- print summary ----------\n    best_idx = int(np.argmax(test_hwa))\n    print(\"Test HWA per embedding:\", dict(zip(emb_dims, test_hwa)))\n    print(f\"Best embedding dim: {emb_dims[best_idx]} with HWA={test_hwa[best_idx]:.3f}\")\nelse:\n    print(\"No runs were found inside experiment_data.npy.\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper: collect runs\nruns = sorted(\n    [k for k in experiment_data.keys() if k.startswith(\"weight_decay_\")],\n    key=lambda s: float(s.split(\"_\")[-1]),\n)\n\n# Figure 1: loss curves\ntry:\n    plt.figure(figsize=(10, 4))\n    for rk in runs:\n        tr = experiment_data[rk][\"SPR_BENCH\"][\"losses\"][\"train\"]\n        vl = experiment_data[rk][\"SPR_BENCH\"][\"losses\"][\"val\"]\n        epochs = np.arange(1, len(tr) + 1)\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, tr, label=rk)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Train Loss\")\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, vl, label=rk)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Val Loss\")\n    plt.subplot(1, 2, 1)\n    plt.legend(fontsize=7)\n    plt.suptitle(\"SPR_BENCH Loss Curves (Left: Train, Right: Val)\")\n    save_path = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(save_path)\n    print(f\"Saved {save_path}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve figure: {e}\")\n    plt.close()\n\n# Figure 2: validation HWA curves\ntry:\n    plt.figure()\n    for rk in runs:\n        hwa = [m[\"hwa\"] for m in experiment_data[rk][\"SPR_BENCH\"][\"metrics\"][\"val\"]]\n        epochs = np.arange(1, len(hwa) + 1)\n        plt.plot(epochs, hwa, marker=\"o\", label=rk)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HWA\")\n    plt.title(\"SPR_BENCH Validation HWA vs Epoch\")\n    plt.legend(fontsize=7)\n    save_path = os.path.join(working_dir, \"SPR_BENCH_val_HWA_curves.png\")\n    plt.savefig(save_path)\n    print(f\"Saved {save_path}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HWA curve figure: {e}\")\n    plt.close()\n\n# Figure 3: test HWA bar chart\ntry:\n    plt.figure()\n    wd_vals = [float(r.split(\"_\")[-1]) for r in runs]\n    hwa_test = [experiment_data[r][\"SPR_BENCH\"][\"metrics\"][\"test\"][\"hwa\"] for r in runs]\n    plt.bar(range(len(wd_vals)), hwa_test, tick_label=wd_vals)\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test HWA\")\n    plt.title(\"SPR_BENCH Test HWA for different Weight Decay\")\n    save_path = os.path.join(working_dir, \"SPR_BENCH_test_HWA_bars.png\")\n    plt.savefig(save_path)\n    print(f\"Saved {save_path}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test HWA bar figure: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- setup ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"num_gnn_layers\", {}).get(\"SPR_BENCH\", {})\nif not spr_data:\n    print(\"No SPR_BENCH data found. Exiting.\")\n    quit()\n\nlayers = sorted(spr_data.keys())\nepochs = list(range(1, len(next(iter(spr_data.values()))[\"losses\"][\"train\"]) + 1))\n\n# ---------------- plot 1: loss curves ----------------\ntry:\n    plt.figure()\n    for l in layers:\n        plt.plot(epochs, spr_data[l][\"losses\"][\"train\"], label=f\"Train L{l}\")\n        plt.plot(\n            epochs, spr_data[l][\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val L{l}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training and Validation Loss vs Epochs\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------------- plot 2: validation HWA curves ----------------\ntry:\n    plt.figure()\n    for l in layers:\n        hwa = [m[\"hwa\"] for m in spr_data[l][\"metrics\"][\"val\"]]\n        plt.plot(epochs, hwa, label=f\"L{l}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Harmonic Weighted Accuracy\")\n    plt.title(\"SPR_BENCH: Validation HWA vs Epochs\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_HWA.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HWA plot: {e}\")\n    plt.close()\n\n# ---------------- plot 3: final test HWA ----------------\ntry:\n    plt.figure()\n    hwa_test = [spr_data[l][\"metrics\"][\"test\"][\"hwa\"] for l in layers]\n    plt.bar([str(l) for l in layers], hwa_test)\n    plt.xlabel(\"# GraphSAGE Layers\")\n    plt.ylabel(\"Test HWA\")\n    plt.title(\"SPR_BENCH: Test HWA by # Layers\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_HWA_by_layers.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test HWA bar plot: {e}\")\n    plt.close()\n\n# ---------------- print final metrics ----------------\nfor l in layers:\n    tm = spr_data[l][\"metrics\"][\"test\"]\n    print(\n        f\"Layers {l} - Test CWA: {tm['cwa']:.3f} | SWA: {tm['swa']:.3f} | HWA: {tm['hwa']:.3f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    ds_key = \"SPR_BENCH\"\n    runs = experiment_data[\"dropout_rate\"][ds_key]\n    dropout_vals = sorted(runs.keys(), key=float)  # e.g. ['0.0','0.2',...]\n    epochs = range(1, len(next(iter(runs.values()))[\"losses\"][\"train\"]) + 1)\n\n    # -------------- figure 1: loss curves -----------------\n    try:\n        plt.figure(figsize=(7, 4))\n        for p in dropout_vals:\n            train_loss = runs[p][\"losses\"][\"train\"]\n            val_loss = runs[p][\"losses\"][\"val\"]\n            plt.plot(\n                epochs, train_loss, linestyle=\"--\", marker=\"o\", label=f\"train p={p}\"\n            )\n            plt.plot(epochs, val_loss, linestyle=\"-\", marker=\"s\", label=f\"val p={p}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH Training & Validation Loss vs Epoch\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss figure: {e}\")\n        plt.close()\n\n    # -------------- figure 2: validation HWA -----------------\n    try:\n        plt.figure(figsize=(7, 4))\n        for p in dropout_vals:\n            hwa = [m[\"hwa\"] for m in runs[p][\"metrics\"][\"val\"]]\n            plt.plot(epochs, hwa, marker=\"^\", label=f\"p={p}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HWA\")\n        plt.title(\"SPR_BENCH Validation Harmonic Weighted Accuracy vs Epoch\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_HWA_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HWA figure: {e}\")\n        plt.close()\n\n    # -------------- figure 3: final test HWA bar chart -----------------\n    try:\n        test_hwa = [runs[p][\"metrics\"][\"test\"][\"hwa\"] for p in dropout_vals]\n        plt.figure(figsize=(6, 4))\n        plt.bar(dropout_vals, test_hwa, color=\"skyblue\")\n        plt.xlabel(\"Dropout rate\")\n        plt.ylabel(\"Test HWA\")\n        plt.title(\"SPR_BENCH Final Test HWA per Dropout Rate\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_HWA_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar chart: {e}\")\n        plt.close()\n\n    # -------------- print evaluation summary -----------------\n    print(\"\\n=== Final Test HWA Scores ===\")\n    for p, h in zip(dropout_vals, test_hwa):\n        print(f\"dropout {p}: HWA={h:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs_dict = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\n    run_keys = sorted(\n        [k for k in runs_dict if k.startswith(\"run_\")],\n        key=lambda s: int(s.split(\"_\")[-1]),\n    )\n\n    # --------------------------------------------------------------\n    # 1. training/validation loss curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            losses = runs_dict[rk][\"losses\"]\n            epochs = np.arange(1, len(losses[\"train\"]) + 1)\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{rk}: Loss Curves (SPR_BENCH)\")\n            plt.legend()\n            fname = f\"spr_bench_{rk}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 2. validation HWA curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            hwa = [m[\"hwa\"] for m in runs_dict[rk][\"metrics\"][\"val\"]]\n            epochs = np.arange(1, len(hwa) + 1)\n            plt.figure()\n            plt.plot(epochs, hwa, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Harmonic Weighted Acc\")\n            plt.title(f\"{rk}: Validation HWA (SPR_BENCH)\")\n            fname = f\"spr_bench_{rk}_hwa_curve.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating HWA plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 3. final HWA per run (bar chart)\n    # --------------------------------------------------------------\n    try:\n        final_hwa = [runs_dict[rk][\"final_val_hwa\"] for rk in run_keys]\n        plt.figure()\n        plt.bar(run_keys, final_hwa)\n        plt.ylabel(\"Final Val HWA\")\n        plt.title(\"SPR_BENCH: Final Validation HWA by Epoch Setting\")\n        plt.xticks(rotation=45)\n        fname = \"spr_bench_final_hwa_bar.png\"\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final HWA bar chart: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 4. confusion matrix on test set for best run\n    # --------------------------------------------------------------\n    try:\n        best_r = runs_dict[\"best_run\"] if \"best_run\" in runs_dict else None\n        if best_r is not None:\n            preds = runs_dict[\"predictions\"]\n            golds = runs_dict[\"ground_truth\"]\n            labels = sorted(list(set(golds) | set(preds)))\n            lbl2idx = {l: i for i, l in enumerate(labels)}\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for g, p in zip(golds, preds):\n                cm[lbl2idx[g], lbl2idx[p]] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xticks(range(len(labels)), labels, rotation=45)\n            plt.yticks(range(len(labels)), labels)\n            plt.title(\n                \"SPR_BENCH Test Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\"\n            )\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            fname = \"spr_bench_confusion_matrix_best.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\"No best_run info for confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # print stored test metrics\n    # --------------------------------------------------------------\n    try:\n        test_metrics = runs_dict[\"metrics_test\"]\n        print(\n            f\"Stored TEST metrics -> CWA: {test_metrics['cwa']:.3f}, \"\n            f\"SWA: {test_metrics['swa']:.3f}, \"\n            f\"HWA: {test_metrics['hwa']:.3f}\"\n        )\n    except Exception as e:\n        print(f\"Error printing test metrics: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs_dict = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\n    run_keys = sorted(\n        [k for k in runs_dict if k.startswith(\"run_\")],\n        key=lambda s: int(s.split(\"_\")[-1]),\n    )\n\n    # --------------------------------------------------------------\n    # 1. training/validation loss curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            losses = runs_dict[rk][\"losses\"]\n            epochs = np.arange(1, len(losses[\"train\"]) + 1)\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{rk}: Loss Curves (SPR_BENCH)\")\n            plt.legend()\n            fname = f\"spr_bench_{rk}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 2. validation HWA curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            hwa = [m[\"hwa\"] for m in runs_dict[rk][\"metrics\"][\"val\"]]\n            epochs = np.arange(1, len(hwa) + 1)\n            plt.figure()\n            plt.plot(epochs, hwa, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Harmonic Weighted Acc\")\n            plt.title(f\"{rk}: Validation HWA (SPR_BENCH)\")\n            fname = f\"spr_bench_{rk}_hwa_curve.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating HWA plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 3. final HWA per run (bar chart)\n    # --------------------------------------------------------------\n    try:\n        final_hwa = [runs_dict[rk][\"final_val_hwa\"] for rk in run_keys]\n        plt.figure()\n        plt.bar(run_keys, final_hwa)\n        plt.ylabel(\"Final Val HWA\")\n        plt.title(\"SPR_BENCH: Final Validation HWA by Epoch Setting\")\n        plt.xticks(rotation=45)\n        fname = \"spr_bench_final_hwa_bar.png\"\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final HWA bar chart: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 4. confusion matrix on test set for best run\n    # --------------------------------------------------------------\n    try:\n        best_r = runs_dict[\"best_run\"] if \"best_run\" in runs_dict else None\n        if best_r is not None:\n            preds = runs_dict[\"predictions\"]\n            golds = runs_dict[\"ground_truth\"]\n            labels = sorted(list(set(golds) | set(preds)))\n            lbl2idx = {l: i for i, l in enumerate(labels)}\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for g, p in zip(golds, preds):\n                cm[lbl2idx[g], lbl2idx[p]] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xticks(range(len(labels)), labels, rotation=45)\n            plt.yticks(range(len(labels)), labels)\n            plt.title(\n                \"SPR_BENCH Test Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\"\n            )\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            fname = \"spr_bench_confusion_matrix_best.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\"No best_run info for confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # print stored test metrics\n    # --------------------------------------------------------------\n    try:\n        test_metrics = runs_dict[\"metrics_test\"]\n        print(\n            f\"Stored TEST metrics -> CWA: {test_metrics['cwa']:.3f}, \"\n            f\"SWA: {test_metrics['swa']:.3f}, \"\n            f\"HWA: {test_metrics['hwa']:.3f}\"\n        )\n    except Exception as e:\n        print(f\"Error printing test metrics: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    runs_dict = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\n    run_keys = sorted(\n        [k for k in runs_dict if k.startswith(\"run_\")],\n        key=lambda s: int(s.split(\"_\")[-1]),\n    )\n\n    # --------------------------------------------------------------\n    # 1. training/validation loss curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            losses = runs_dict[rk][\"losses\"]\n            epochs = np.arange(1, len(losses[\"train\"]) + 1)\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{rk}: Loss Curves (SPR_BENCH)\")\n            plt.legend()\n            fname = f\"spr_bench_{rk}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 2. validation HWA curves\n    # --------------------------------------------------------------\n    for rk in run_keys:\n        try:\n            hwa = [m[\"hwa\"] for m in runs_dict[rk][\"metrics\"][\"val\"]]\n            epochs = np.arange(1, len(hwa) + 1)\n            plt.figure()\n            plt.plot(epochs, hwa, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Harmonic Weighted Acc\")\n            plt.title(f\"{rk}: Validation HWA (SPR_BENCH)\")\n            fname = f\"spr_bench_{rk}_hwa_curve.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating HWA plot for {rk}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    # 3. final HWA per run (bar chart)\n    # --------------------------------------------------------------\n    try:\n        final_hwa = [runs_dict[rk][\"final_val_hwa\"] for rk in run_keys]\n        plt.figure()\n        plt.bar(run_keys, final_hwa)\n        plt.ylabel(\"Final Val HWA\")\n        plt.title(\"SPR_BENCH: Final Validation HWA by Epoch Setting\")\n        plt.xticks(rotation=45)\n        fname = \"spr_bench_final_hwa_bar.png\"\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final HWA bar chart: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 4. confusion matrix on test set for best run\n    # --------------------------------------------------------------\n    try:\n        best_r = runs_dict[\"best_run\"] if \"best_run\" in runs_dict else None\n        if best_r is not None:\n            preds = runs_dict[\"predictions\"]\n            golds = runs_dict[\"ground_truth\"]\n            labels = sorted(list(set(golds) | set(preds)))\n            lbl2idx = {l: i for i, l in enumerate(labels)}\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for g, p in zip(golds, preds):\n                cm[lbl2idx[g], lbl2idx[p]] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xticks(range(len(labels)), labels, rotation=45)\n            plt.yticks(range(len(labels)), labels)\n            plt.title(\n                \"SPR_BENCH Test Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\"\n            )\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            fname = \"spr_bench_confusion_matrix_best.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        else:\n            print(\"No best_run info for confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # print stored test metrics\n    # --------------------------------------------------------------\n    try:\n        test_metrics = runs_dict[\"metrics_test\"]\n        print(\n            f\"Stored TEST metrics -> CWA: {test_metrics['cwa']:.3f}, \"\n            f\"SWA: {test_metrics['swa']:.3f}, \"\n            f\"HWA: {test_metrics['hwa']:.3f}\"\n        )\n    except Exception as e:\n        print(f\"Error printing test metrics: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------\n# Set up working directory\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# Collect all experiment_data dictionaries\n# ------------------------------------------------------------\ntry:\n    # Paths provided in the \"Experiment Data Path\" section\n    experiment_data_path_list = [\n        \"experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_af25fa889c504cd7895ce35ef07dc8cc_proc_1544298/experiment_data.npy\",\n        \"experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b84393766c71426599835acf6e80ada8_proc_1544300/experiment_data.npy\",\n        \"experiments/2025-08-30_23-24-25_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_00c6e23a0e4348779bdfe1cc20c26c85_proc_1544299/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        if os.path.isfile(full_path):\n            all_experiment_data.append(np.load(full_path, allow_pickle=True).item())\n        else:\n            print(f\"File not found: {full_path}\")\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n\n# ------------------------------------------------------------\n# Helper for mean and standard error\n# ------------------------------------------------------------\ndef mean_sem(arrays):\n    \"\"\"Stack 1-D arrays to compute mean and standard error along axis 0.\"\"\"\n    stack = np.stack(arrays, axis=0)\n    mean = stack.mean(axis=0)\n    sem = (\n        stack.std(axis=0, ddof=1) / np.sqrt(stack.shape[0])\n        if stack.shape[0] > 1\n        else np.zeros_like(mean)\n    )\n    return mean, sem\n\n\n# ------------------------------------------------------------\n# Aggregate and plot for every dataset present\n# ------------------------------------------------------------\ndatasets = set()\nfor ed in all_experiment_data:\n    datasets.update(ed.get(\"EPOCHS\", {}).keys())\n\nfor ds in sorted(datasets):\n    # Collect per-run time-series across all files\n    train_losses, val_losses, val_hwas = [], [], []\n    run_final_hwa = {}  # run_key -> list of values across files\n    test_hwas = []  # aggregated test HWA over files\n\n    for ed in all_experiment_data:\n        runs_dict = ed.get(\"EPOCHS\", {}).get(ds, {})\n        # Store overall test metric if present (once per file)\n        if \"metrics_test\" in runs_dict and \"hwa\" in runs_dict[\"metrics_test\"]:\n            test_hwas.append(runs_dict[\"metrics_test\"][\"hwa\"])\n        # Iterate runs\n        for rk, rv in runs_dict.items():\n            if not rk.startswith(\"run_\"):\n                continue\n            # losses\n            if \"losses\" in rv:\n                tl = np.asarray(rv[\"losses\"][\"train\"])\n                vl = np.asarray(rv[\"losses\"][\"val\"])\n                train_losses.append(tl)\n                val_losses.append(vl)\n            # val metrics\n            if \"metrics\" in rv and \"val\" in rv[\"metrics\"]:\n                hwa_series = [m[\"hwa\"] for m in rv[\"metrics\"][\"val\"]]\n                val_hwas.append(np.asarray(hwa_series))\n            # final HWA\n            if \"final_val_hwa\" in rv:\n                run_final_hwa.setdefault(rk, []).append(rv[\"final_val_hwa\"])\n\n    # --------------------------------------------------------\n    # Align lengths (truncate to minimum) for loss & HWA curves\n    # --------------------------------------------------------\n    if train_losses:\n        min_len_loss = min(map(len, train_losses + val_losses))\n        train_losses = [tl[:min_len_loss] for tl in train_losses]\n        val_losses = [vl[:min_len_loss] for vl in val_losses]\n\n    if val_hwas:\n        min_len_hwa = min(map(len, val_hwas))\n        val_hwas = [vh[:min_len_hwa] for vh in val_hwas]\n\n    # --------------------------------------------------------\n    # 1. Aggregated loss curves\n    # --------------------------------------------------------\n    try:\n        if train_losses:\n            epochs = np.arange(1, min_len_loss + 1)\n            m_tr, se_tr = mean_sem(train_losses)\n            m_val, se_val = mean_sem(val_losses)\n\n            plt.figure()\n            plt.plot(epochs, m_tr, label=\"Train Loss (mean)\")\n            plt.fill_between(\n                epochs, m_tr - se_tr, m_tr + se_tr, alpha=0.3, label=\"Train SEM\"\n            )\n            plt.plot(epochs, m_val, label=\"Val Loss (mean)\")\n            plt.fill_between(\n                epochs, m_val - se_val, m_val + se_val, alpha=0.3, label=\"Val SEM\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\n                f\"{ds}: Aggregated Loss Curves\\nMean \u00b1 SEM over {len(train_losses)} runs\"\n            )\n            plt.legend()\n            fname = f\"{ds.lower()}_aggregated_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {ds}: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 2. Aggregated validation HWA curves\n    # --------------------------------------------------------\n    try:\n        if val_hwas:\n            epochs = np.arange(1, min_len_hwa + 1)\n            m_hwa, se_hwa = mean_sem(val_hwas)\n            plt.figure()\n            plt.plot(epochs, m_hwa, marker=\"o\", label=\"Val HWA (mean)\")\n            plt.fill_between(\n                epochs, m_hwa - se_hwa, m_hwa + se_hwa, alpha=0.3, label=\"SEM\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Harmonic Weighted Acc\")\n            plt.title(\n                f\"{ds}: Aggregated Validation HWA\\nMean \u00b1 SEM over {len(val_hwas)} runs\"\n            )\n            plt.legend()\n            fname = f\"{ds.lower()}_aggregated_val_hwa.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated HWA plot for {ds}: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 3. Final validation HWA per run (bar with error bars)\n    # --------------------------------------------------------\n    try:\n        if run_final_hwa:\n            run_names = sorted(\n                run_final_hwa.keys(), key=lambda s: int(s.split(\"_\")[-1])\n            )\n            means = [np.mean(run_final_hwa[r]) for r in run_names]\n            sems = [\n                (\n                    np.std(run_final_hwa[r], ddof=1) / np.sqrt(len(run_final_hwa[r]))\n                    if len(run_final_hwa[r]) > 1\n                    else 0.0\n                )\n                for r in run_names\n            ]\n\n            plt.figure()\n            plt.bar(run_names, means, yerr=sems, capsize=5)\n            plt.ylabel(\"Final Val HWA\")\n            plt.title(\n                f\"{ds}: Final Validation HWA by Run\\nMean \u00b1 SEM over {len(all_experiment_data)} files\"\n            )\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            fname = f\"{ds.lower()}_final_val_hwa_bar.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating final HWA bar chart for {ds}: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 4. Print aggregated test metrics\n    # --------------------------------------------------------\n    try:\n        if test_hwas:\n            mean_test_hwa = np.mean(test_hwas)\n            sem_test_hwa = (\n                (np.std(test_hwas, ddof=1) / np.sqrt(len(test_hwas)))\n                if len(test_hwas) > 1\n                else 0.0\n            )\n            print(\n                f\"{ds} TEST HWA -> mean: {mean_test_hwa:.3f}, SEM: {sem_test_hwa:.3f}  (n={len(test_hwas)})\"\n            )\n    except Exception as e:\n        print(f\"Error printing test metrics for {ds}: {e}\")\n"], "plot_plan": ["We will load the saved experiment_data.npy, loop over each dataset (here only SPR_BENCH), and extract per-epoch training/validation losses, validation weighted accuracies (CWA, SWA, HWA), and final test predictions/ground-truth.  \nThree visualizations will be produced inside independent try-except blocks:  \n1) line plot of train vs. validation loss;  \n2) line plot of CWA, SWA and HWA over epochs;  \n3) confusion-matrix heat-map on the test split.  \nEach figure is titled, saved to working_dir with descriptive filenames, and closed afterwards.  \nIf plotting fails, the exception is reported and the figure is still closed.  \nFinally we print the stored test metrics so users can quickly read the numerical results from the console.", "We will load the saved experiment_data.npy, pull the dictionary under EPOCHS \u2192 SPR_BENCH and iterate over every recorded run. For every run we extract the epoch-level arrays of training loss, validation loss and validation HWA, then build three summary visualisations: (1) a joint plot of training and validation loss versus epoch, (2) a plot of validation HWA versus epoch, and (3) a bar chart of the final HWA achieved by each candidate epoch setting. Finally, for the best run (recorded in best_run) we compute a simple 2\u00d72 confusion matrix on the test split using the stored predictions and ground-truth labels and visualise it with imshow. Each figure is created inside an independent try-except so that a failure in one does not block the rest, all figures are titled clearly, saved into working_dir with descriptive names and then closed. We cap the number of similar plots at four (one per run), well within the five-figure limit, and print the test-set metrics already present in the data for reference. Everything relies solely on the content of experiment_data.npy; no synthetic numbers are invented. The script is completely self-contained and uses only matplotlib and numpy.", "We will load experiment_data.npy from the working directory, extract the learning-rate sweep results for the SPR_BENCH dataset, and then create three standard plots: (1) per-epoch training/validation loss curves for every tested learning rate, (2) per-epoch harmonic-weighted accuracy (HWA) curves for every learning rate, and (3) a bar chart of the final development-set HWA versus learning rate.  Each plot is wrapped in its own try-except block, saved with descriptive names that include the dataset and plot type, titled clearly, given a concise subtitle explaining line conventions, and closed whether or not an error occurs.  We plot only data contained in experiment_data.npy and generate at most three figures (well under the five-figure limit).  After plotting we also print the final dev-set HWA for each learning rate so the user can see numerical values alongside the visualizations.  All files are saved to the prescribed working_dir.", "We will load the saved experiment_data.npy, iterate through the three batch-size runs, and extract (1) epoch-wise training and validation losses, (2) epoch-wise validation harmonic weighted accuracy (HWA), and (3) final test HWA/CWA/SWA.  Three separate try/except blocks create: a combined line plot of training vs. validation loss, a line plot of validation HWA, and a bar chart of final test HWA across batch sizes.  Each figure is titled, saved in working_dir with descriptive filenames, and closed afterwards.  The code prints a concise table of final test metrics so they can be inspected without opening the plots.  No synthetic values are created\u2014everything comes directly from experiment_data.npy.  At most three figures are produced, satisfying the five-figure limit, and every figure follows the requested matplotlib conventions.  All plotting calls are wrapped in try/except to ensure graceful failure and figure closure.  The script begins with the required imports and working_dir definition.", "We will load the saved experiment_data.npy, extract every run recorded for the SPR_BENCH sweep, and create three concise visualizations:  \n1) a single figure with training and validation loss curves for every tested embedding size,  \n2) a single figure with validation HWA (harmonic-weighted accuracy) curves for every embedding size, and  \n3) a bar chart summarising the final test HWA obtained by each embedding size.  \nEach figure is wrapped in its own try-except block, saved to the working directory with a descriptive name, given a clear title/sub-title, and the figure is always closed.  \nWe plot at most three figures, thereby staying well under the five-figure limit even as the number of hyper-parameter settings grows.  \nAfter plotting we print the test HWA values and highlight which embedding size performed best.  \nOnly data already stored inside experiment_data.npy are accessed; nothing is synthesized.  \nAll requirements regarding imports, directory handling, figure closing, and code conciseness are respected.", null, "We will load experiment_data.npy, iterate over the five weight\u2013decay runs, and extract (a) per-epoch train/val losses, (b) per-epoch validation HWA, and (c) final test HWA.  \nFigure 1 overlays losses across runs using two subplots (Left: train, Right: val) so users can see convergence behaviour for SPR_BENCH.  \nFigure 2 overlays validation HWA vs. epoch for all runs to visualise quality trends.  \nFigure 3 is a bar chart of the final test HWA for each weight-decay value, giving an immediate hyper-parameter comparison.  \nAll plots are wrapped in separate try-except blocks, saved in working_dir with descriptive filenames, and figures are always closed.  \nWe restrict ourselves to these three plots (\u22645) and never fabricate data\u2014everything is read directly from experiment_data.  \nClear titles include the dataset name and subtitles explain subplots (e.g. \u201cLeft: Train Loss, Right: Val Loss\u201d).  \nThe code uses only numpy and matplotlib, starts with the mandated imports, and prints a short confirmation after saving every figure.", "We first load experiment_data.npy, then pull out the per-layer record stored under experiment_data['num_gnn_layers']['SPR_BENCH'].  For every GraphSAGE layer setting we read the epoch-wise train/val losses and the validation harmonic weighted accuracy (HWA), together with the single final test-set metrics.  Using basic matplotlib, we create three separate figures wrapped in try/except blocks: (1) combined train/val loss curves across epochs with one solid/dashed pair per layer, (2) validation HWA curves across epochs for each layer, and (3) a bar chart of the final test-set HWA versus number of layers.  Each plot is given a clear title and is saved to working_dir with descriptive filenames.  We always call plt.close() whether the plot succeeds or fails.  At the end we print the final test CWA/SWA/HWA for each layer so the user can see numerical results in the console.  The code respects the limit of at most five figures, never fabricates data, and assumes experiment_data.npy already exists in working_dir.  Everything is executed in a concise, self-contained script that begins with the required imports and directory creation.", "We will load experiment_data.npy, pull out the records for the SPR_BENCH dataset, and iterate over the four dropout settings. For each epoch\u2010level array (train/val loss and validation HWA) we will collate across dropouts so we can place all curves in a single figure, staying under the 5-plot limit. A second figure will show validation HWA vs. epoch, again overlaid for all dropouts. A third bar-chart figure will summarise the final test HWA for every dropout value. Each figure is wrapped in its own try-except block; figures are titled, saved in working_dir with descriptive names, and always closed. After plotting we also print a concise table of the test HWA scores to stdout so users can quickly spot the best setting without opening the plots.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "EPOCHS", "learning_rate", "batch_size", "emb_dim", "hidden_dim", "weight_decay", "num_gnn_layers", "dropout_rate", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script below loads the stored numpy file from the \u201cworking\u201d directory,\niterates over each dataset contained in it, and prints: (1) the final training\nloss, (2) the best (minimum) validation loss, (3) the validation CWA/SWA/HWA\ntaken from the epoch that achieved the highest HWA, and (4) the final test\nCWA/SWA/HWA. All metric names are printed explicitly, satisfying the format\nrules and executing immediately on run.", "The script will load the saved numpy file from the working directory, retrieve\nthe best-run identifier, gather the final train/validation losses and the\ncorresponding validation and test metrics, then print them with explicit labels\nfor each dataset split (train, validation, test). It executes immediately\nwithout any special entry point.", "The script will read the saved NumPy dictionary, iterate over every dataset\nstored under the learning-rate sweep, identify the run that used the recorded\n\u201cbest learning rate,\u201d and then print the final values associated with that run.\nFor each dataset it will report (1) the best learning rate, (2) the final\ntraining loss, (3) the final validation loss, (4) the final validation\nCWA/SWA/HWA, and (5) the test-set CWA/SWA/HWA. All values are labelled\nexplicitly so no ambiguous terms like \u201ctrain,\u201d \u201cval,\u201d or \u201ctest\u201d appear without\ncontext.", "The script will load the numpy file from the required working directory, iterate\nthrough every batch-size run saved for the SPR_BENCH dataset, and for each run\nidentify the best (i.e., minimum) training and validation loss as well as the\nmaximum validation CWA, SWA, and HWA. It will also read the final test\nCWA/SWA/HWA that were stored after training. All results are printed with\nexplicit, descriptive labels so that readers immediately know which figure they\nare seeing.", "The script will locate the working directory, load experiment_data.npy and\niterate through each saved run (one per embedding dimension).   For every run it\nprints the dataset name (\u201cSPR_BENCH\u201d) once, followed by clearly-labelled\nmetrics: the final training loss, the final validation loss and validation\nCWA/SWA/HWA, and the test CWA/SWA/HWA.   Metric labels are explicit (\u201ctraining\nloss\u201d, \u201cvalidation harmonic weighted accuracy\u201d, etc.) and no figures are\nplotted.   All logic is placed at the global level so the code executes\nimmediately when the file is run.", "", "We will load the saved numpy file from the working directory, iterate over every\nweight-decay run, and for each run extract:   \u2022 the final training loss,   \u2022 the\nbest (lowest) validation loss together with the corresponding best validation\nCWA/SWA/HWA,   \u2022 the test CWA/SWA/HWA (only one entry is stored).   For each run\nwe print the dataset name first (SPR_BENCH with weight-decay value) followed by\nclearly labelled metric names and their values.", "The script will locate the saved NumPy file in the \u201cworking\u201d directory, load the\nnested experiment dictionary, and iterate over every hyper-parameter setting\nstored for the SPR_BENCH dataset.   For each setting it extracts the final\nepoch\u2019s losses (training and validation) and the final colour-, shape- and\nharmonic-weighted accuracies for both validation and test splits.   Across all\nhyper-parameter settings it keeps the \u201cbest\u201d value for every metric (lowest\nloss, highest accuracy).   Finally, it prints the dataset name followed by each\nmetric\u2019s best value, naming every metric explicitly (e.g. \u201cbest validation\nharmonic weighted accuracy\u201d).", "The script first locates the saved NumPy file in the working directory, loads it\ninto a Python dict, and then walks through its hierarchical structure (hyper-\nparameter \u2192 dataset \u2192 run).   For every dropout setting of the SPR_BENCH dataset\nit prints the final training loss, the final validation loss, the final\nvalidation CWA/SWA/HWA, and the test CWA/SWA/HWA.   Each value is preceded by a\nclear, descriptive label so that the output is self-explanatory and unambiguous.", "The script will load the saved numpy file from the working directory, retrieve\nthe best-run identifier, gather the final train/validation losses and the\ncorresponding validation and test metrics, then print them with explicit labels\nfor each dataset split (train, validation, test). It executes immediately\nwithout any special entry point.", "The script will load the saved numpy file from the working directory, retrieve\nthe best-run identifier, gather the final train/validation losses and the\ncorresponding validation and test metrics, then print them with explicit labels\nfor each dataset split (train, validation, test). It executes immediately\nwithout any special entry point.", "The script will load the saved numpy file from the working directory, retrieve\nthe best-run identifier, gather the final train/validation losses and the\ncorresponding validation and test metrics, then print them with explicit labels\nfor each dataset split (train, validation, test). It executes immediately\nwithout any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ----------------- locate & load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ----------------- helper to format floats -----------------\ndef fmt(x):\n    return f\"{x:.4f}\"\n\n\n# ----------------- iterate over datasets -----------------\nfor ds_name, ds_content in experiment_data.items():\n    print(ds_name)  # dataset header\n\n    # ---------- training loss ----------\n    train_losses = ds_content.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(f\"final training loss: {fmt(train_losses[-1])}\")\n\n    # ---------- validation loss ----------\n    val_losses = ds_content.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {fmt(best_val_loss)}\")\n\n    # ---------- validation metrics ----------\n    val_metrics_list = ds_content.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics_list:\n        best_val_epoch = max(val_metrics_list, key=lambda m: m.get(\"hwa\", -1))\n        print(f\"best validation color weighted accuracy:  {fmt(best_val_epoch['cwa'])}\")\n        print(\n            f\"best validation shape weighted accuracy:   {fmt(best_val_epoch['swa'])}\"\n        )\n        print(\n            f\"best validation harmonic weighted accuracy: {fmt(best_val_epoch['hwa'])}\"\n        )\n\n    # ---------- test metrics ----------\n    test_metrics = ds_content.get(\"metrics\", {}).get(\"test\", {})\n    if test_metrics:\n        print(f\"test color weighted accuracy:  {fmt(test_metrics['cwa'])}\")\n        print(f\"test shape weighted accuracy:   {fmt(test_metrics['swa'])}\")\n        print(f\"test harmonic weighted accuracy: {fmt(test_metrics['hwa'])}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# Locate and load the saved experiment data\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Cannot find experiment data at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# --------------------------------------------------\n# Extract information for the SPR_BENCH experiment\n# --------------------------------------------------\nspr_data = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\nbest_run = spr_data[\"best_run\"]  # e.g. 15\nbest_key = f\"run_{best_run}\"\nrun_data = spr_data[best_key]\n\n# Final / best values\nfinal_train_loss = run_data[\"losses\"][\"train\"][-1]\nfinal_val_loss = run_data[\"losses\"][\"val\"][-1]\nval_metrics = run_data[\"metrics\"][\"val\"][-1]  # dict with cwa, swa, hwa\ntest_metrics = spr_data[\"metrics_test\"]  # dict with cwa, swa, hwa\n\n# --------------------------------------------------\n# Pretty-print results\n# --------------------------------------------------\nprint(\"SPR_BENCH - TRAIN\")\nprint(f\"train loss: {final_train_loss:.4f}\")\n\nprint(\"\\nSPR_BENCH - VALIDATION\")\nprint(f\"validation loss: {final_val_loss:.4f}\")\nprint(f\"validation CWA:  {val_metrics['cwa']:.3f}\")\nprint(f\"validation SWA:  {val_metrics['swa']:.3f}\")\nprint(f\"validation HWA:  {val_metrics['hwa']:.3f}\")\n\nprint(\"\\nSPR_BENCH - TEST\")\nprint(f\"test CWA:  {test_metrics['cwa']:.3f}\")\nprint(f\"test SWA:  {test_metrics['swa']:.3f}\")\nprint(f\"test HWA:  {test_metrics['hwa']:.3f}\")\n", "import os\nimport numpy as np\n\n# ----------------- load the stored results -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ----------------- pretty printing helper -----------------\ndef print_dataset_metrics(ds_name: str, ds_dict: dict):\n    lr_list = ds_dict[\"lr_values\"]\n    best_lr = ds_dict.get(\"best_lr\")\n    train_losses_all = ds_dict[\"losses\"][\"train\"]  # list over LR -> list over epochs\n    val_losses_all = ds_dict[\"losses\"][\"val\"]\n    val_metrics_all = ds_dict[\"metrics\"][\"val\"]  # list over LR -> list over epochs\n    test_metrics = ds_dict[\"metrics\"][\"test\"]  # dict with cwa/swa/hwa\n\n    # locate index of the best learning rate\n    best_idx = lr_list.index(best_lr)\n\n    final_train_loss = train_losses_all[best_idx][-1]\n    final_val_loss = val_losses_all[best_idx][-1]\n\n    final_val_metrics = val_metrics_all[best_idx][-1]  # dict with cwa/swa/hwa\n\n    # ----------- print -----------\n    print(f\"Dataset: {ds_name}\")\n    print(f\"Best learning rate: {best_lr}\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(f\"Final validation color-weighted accuracy: {final_val_metrics['cwa']:.4f}\")\n    print(f\"Final validation shape-weighted accuracy: {final_val_metrics['swa']:.4f}\")\n    print(\n        f\"Final validation harmonic-weighted accuracy: {final_val_metrics['hwa']:.4f}\"\n    )\n    print(f\"Test color-weighted accuracy: {test_metrics['cwa']:.4f}\")\n    print(f\"Test shape-weighted accuracy: {test_metrics['swa']:.4f}\")\n    print(f\"Test harmonic-weighted accuracy: {test_metrics['hwa']:.4f}\")\n    print()  # blank line for readability\n\n\n# ----------------- iterate over datasets -----------------\nfor ds_name, ds_dict in experiment_data[\"learning_rate\"].items():\n    print_dataset_metrics(ds_name, ds_dict)\n", "import os\nimport numpy as np\n\n# ------------------ locate and load data ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not locate experiment data at: {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------ helper for pretty printing ------------------\ndef print_metric(name: str, value):\n    if value is None:\n        print(f\"    {name}: N/A\")\n    else:\n        print(f\"    {name}: {value:.4f}\")\n\n\n# ------------------ parse & report ------------------\nfor tuning_param, datasets in experiment_data.items():  # e.g., \"batch_size\"\n    for dataset_name, runs in datasets.items():  # e.g., \"SPR_BENCH\"\n        for run_key, run_info in runs.items():  # e.g., \"32\", \"64\", \"128\"\n            # ----- pull losses -----\n            train_losses = run_info[\"losses\"].get(\"train\", [])\n            val_losses = run_info[\"losses\"].get(\"val\", [])\n\n            best_train_loss = min(train_losses) if train_losses else None\n            best_val_loss = min(val_losses) if val_losses else None\n\n            # ----- pull validation metrics (per-epoch list) -----\n            val_metrics = run_info[\"metrics\"].get(\"val\", [])\n            if val_metrics:\n                best_val_cwa = max(m[\"cwa\"] for m in val_metrics)\n                best_val_swa = max(m[\"swa\"] for m in val_metrics)\n                best_val_hwa = max(m[\"hwa\"] for m in val_metrics)\n            else:\n                best_val_cwa = best_val_swa = best_val_hwa = None\n\n            # ----- final test metrics -----\n            test_metrics = run_info[\"metrics\"].get(\"test\", {})\n            test_cwa = test_metrics.get(\"cwa\")\n            test_swa = test_metrics.get(\"swa\")\n            test_hwa = test_metrics.get(\"hwa\")\n\n            # ------------- output -------------\n            print(f\"Dataset: {dataset_name} (batch size {run_key})\")\n            print_metric(\"best training loss\", best_train_loss)\n            print_metric(\"best validation loss\", best_val_loss)\n            print_metric(\"best validation CWA\", best_val_cwa)\n            print_metric(\"best validation SWA\", best_val_swa)\n            print_metric(\"best validation HWA\", best_val_hwa)\n            print_metric(\"test CWA\", test_cwa)\n            print_metric(\"test SWA\", test_swa)\n            print_metric(\"test HWA\", test_hwa)\n            print(\"\")  # blank line between runs\n", "import os\nimport numpy as np\n\n# ----------------- locate and load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------------- iterate over stored runs -----------------\nhp_group = \"emb_dim\"  # only hyper-parameter sweep we performed\ndataset_name = \"SPR_BENCH\"  # dataset used in the sweep\n\nruns = experiment_data[hp_group][dataset_name][\"runs\"]\n\nfor run in runs:\n    emb_dim = run[\"emb_dim\"]\n    print(f\"Dataset: {dataset_name} | embedding dimension: {emb_dim}\")\n\n    # ---- training metrics ----\n    final_train_loss = run[\"losses\"][\"train\"][-1]\n    print(f\"  training loss: {final_train_loss:.6f}\")\n\n    # ---- validation metrics ----\n    final_val_loss = run[\"losses\"][\"val\"][-1]\n    final_val_metrics = run[\"metrics\"][\"val\"][-1]  # dictionary with cwa, swa, hwa\n    print(f\"  validation loss: {final_val_loss:.6f}\")\n    print(f\"  validation color weighted accuracy: {final_val_metrics['cwa']:.6f}\")\n    print(f\"  validation shape weighted accuracy: {final_val_metrics['swa']:.6f}\")\n    print(f\"  validation harmonic weighted accuracy: {final_val_metrics['hwa']:.6f}\")\n\n    # ---- test metrics ----\n    test_metrics = run[\"metrics\"][\"test\"]  # dictionary with cwa, swa, hwa\n    print(f\"  test color weighted accuracy: {test_metrics['cwa']:.6f}\")\n    print(f\"  test shape weighted accuracy: {test_metrics['swa']:.6f}\")\n    print(f\"  test harmonic weighted accuracy: {test_metrics['hwa']:.6f}\\n\")\n", "", "import os\nimport numpy as np\n\n# -------------------- locate and load data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to print metrics -----------------\ndef print_run_metrics(run_key: str, run_dict: dict):\n    ds_name = \"SPR_BENCH\"\n    ds_dict = run_dict[ds_name]\n\n    # losses\n    train_losses = ds_dict[\"losses\"][\"train\"]  # list over epochs\n    val_losses = ds_dict[\"losses\"][\"val\"]  # list over epochs\n\n    # final training loss\n    final_train_loss = train_losses[-1] if train_losses else float(\"nan\")\n\n    # best validation (lowest loss) index and metrics\n    if val_losses:\n        best_val_idx = int(np.argmin(val_losses))\n        best_val_loss = val_losses[best_val_idx]\n        best_val_metrics = ds_dict[\"metrics\"][\"val\"][best_val_idx]\n    else:  # should not occur, but for safety\n        best_val_idx = None\n        best_val_loss = float(\"nan\")\n        best_val_metrics = {\n            \"cwa\": float(\"nan\"),\n            \"swa\": float(\"nan\"),\n            \"hwa\": float(\"nan\"),\n        }\n\n    # test metrics (single dict)\n    test_metrics = ds_dict[\"metrics\"][\"test\"]\n\n    # -------------------- printing --------------------\n    # dataset name first\n    print(f\"{ds_name} (weight_decay = {run_key.split('_')[-1]})\")\n    print(f\"  final training loss      : {final_train_loss:.4f}\")\n    print(f\"  best validation loss     : {best_val_loss:.4f}\")\n    print(f\"  best validation CWA      : {best_val_metrics['cwa']:.4f}\")\n    print(f\"  best validation SWA      : {best_val_metrics['swa']:.4f}\")\n    print(f\"  best validation HWA      : {best_val_metrics['hwa']:.4f}\")\n    print(f\"  test CWA                 : {test_metrics['cwa']:.4f}\")\n    print(f\"  test SWA                 : {test_metrics['swa']:.4f}\")\n    print(f\"  test HWA                 : {test_metrics['hwa']:.4f}\")\n    print()  # blank line for readability\n\n\n# -------------------- iterate and display --------------------\nfor run_key, run_content in experiment_data.items():\n    print_run_metrics(run_key, run_content)\n", "import os\nimport numpy as np\n\n# ------------ locate & load -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------ constants -----------------\nDATASET_NAME = \"SPR_BENCH\"\nrecords = experiment_data[\"num_gnn_layers\"][DATASET_NAME]\n\n# initialise best trackers (loss \u2192 lower is better, accuracy \u2192 higher is better)\nbest_vals = {\n    \"training loss\": float(\"inf\"),\n    \"validation loss\": float(\"inf\"),\n    \"validation color weighted accuracy\": -1.0,\n    \"validation shape weighted accuracy\": -1.0,\n    \"validation harmonic weighted accuracy\": -1.0,\n    \"test color weighted accuracy\": -1.0,\n    \"test shape weighted accuracy\": -1.0,\n    \"test harmonic weighted accuracy\": -1.0,\n}\n\n# ------------ scan all hyper-parameter settings -------------\nfor n_layers, rec in records.items():\n    # final epoch indices\n    final_train_loss = rec[\"losses\"][\"train\"][-1]\n    final_val_loss = rec[\"losses\"][\"val\"][-1]\n\n    final_val_metrics = rec[\"metrics\"][\"val\"][-1]\n    final_test_metrics = rec[\"metrics\"][\"test\"]\n\n    # compare & keep best\n    best_vals[\"training loss\"] = min(best_vals[\"training loss\"], final_train_loss)\n    best_vals[\"validation loss\"] = min(best_vals[\"validation loss\"], final_val_loss)\n\n    for metric_name, key in [\n        (\"validation color weighted accuracy\", \"cwa\"),\n        (\"validation shape weighted accuracy\", \"swa\"),\n        (\"validation harmonic weighted accuracy\", \"hwa\"),\n    ]:\n        best_vals[metric_name] = max(best_vals[metric_name], final_val_metrics[key])\n\n    for metric_name, key in [\n        (\"test color weighted accuracy\", \"cwa\"),\n        (\"test shape weighted accuracy\", \"swa\"),\n        (\"test harmonic weighted accuracy\", \"hwa\"),\n    ]:\n        best_vals[metric_name] = max(best_vals[metric_name], final_test_metrics[key])\n\n# ------------ print results -------------\nprint(DATASET_NAME)\nfor metric, value in best_vals.items():\n    print(f\"best {metric}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(data_file):\n    raise FileNotFoundError(\n        f\"Could not find '{data_file}'. Make sure the experiment has been run.\"\n    )\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ---------- iterate and print metrics ----------\nfor hp_name, datasets in experiment_data.items():  # e.g., 'dropout_rate'\n    for dataset_name, hp_runs in datasets.items():  # e.g., 'SPR_BENCH'\n        for hp_value, results in hp_runs.items():  # e.g., '0.2', '0.4', ...\n            print(f\"Dataset: {dataset_name} (dropout rate = {hp_value})\")\n\n            # Final training loss\n            train_losses = results.get(\"losses\", {}).get(\"train\", [])\n            if train_losses:\n                print(f\"final training loss: {train_losses[-1]:.4f}\")\n\n            # Final validation loss\n            val_losses = results.get(\"losses\", {}).get(\"val\", [])\n            if val_losses:\n                print(f\"final validation loss: {val_losses[-1]:.4f}\")\n\n            # Final validation metrics\n            val_metrics_list = results.get(\"metrics\", {}).get(\"val\", [])\n            if val_metrics_list:\n                final_val_metrics = val_metrics_list[-1]\n                print(\n                    f\"final validation color weighted accuracy: {final_val_metrics['cwa']:.4f}\"\n                )\n                print(\n                    f\"final validation shape weighted accuracy: {final_val_metrics['swa']:.4f}\"\n                )\n                print(\n                    f\"final validation harmonic weighted accuracy: {final_val_metrics['hwa']:.4f}\"\n                )\n\n            # Test metrics\n            test_metrics = results.get(\"metrics\", {}).get(\"test\", {})\n            if test_metrics:\n                print(f\"test color weighted accuracy: {test_metrics['cwa']:.4f}\")\n                print(f\"test shape weighted accuracy: {test_metrics['swa']:.4f}\")\n                print(f\"test harmonic weighted accuracy: {test_metrics['hwa']:.4f}\")\n\n            print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# Locate and load the saved experiment data\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Cannot find experiment data at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# --------------------------------------------------\n# Extract information for the SPR_BENCH experiment\n# --------------------------------------------------\nspr_data = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\nbest_run = spr_data[\"best_run\"]  # e.g. 15\nbest_key = f\"run_{best_run}\"\nrun_data = spr_data[best_key]\n\n# Final / best values\nfinal_train_loss = run_data[\"losses\"][\"train\"][-1]\nfinal_val_loss = run_data[\"losses\"][\"val\"][-1]\nval_metrics = run_data[\"metrics\"][\"val\"][-1]  # dict with cwa, swa, hwa\ntest_metrics = spr_data[\"metrics_test\"]  # dict with cwa, swa, hwa\n\n# --------------------------------------------------\n# Pretty-print results\n# --------------------------------------------------\nprint(\"SPR_BENCH - TRAIN\")\nprint(f\"train loss: {final_train_loss:.4f}\")\n\nprint(\"\\nSPR_BENCH - VALIDATION\")\nprint(f\"validation loss: {final_val_loss:.4f}\")\nprint(f\"validation CWA:  {val_metrics['cwa']:.3f}\")\nprint(f\"validation SWA:  {val_metrics['swa']:.3f}\")\nprint(f\"validation HWA:  {val_metrics['hwa']:.3f}\")\n\nprint(\"\\nSPR_BENCH - TEST\")\nprint(f\"test CWA:  {test_metrics['cwa']:.3f}\")\nprint(f\"test SWA:  {test_metrics['swa']:.3f}\")\nprint(f\"test HWA:  {test_metrics['hwa']:.3f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# Locate and load the saved experiment data\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Cannot find experiment data at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# --------------------------------------------------\n# Extract information for the SPR_BENCH experiment\n# --------------------------------------------------\nspr_data = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\nbest_run = spr_data[\"best_run\"]  # e.g. 15\nbest_key = f\"run_{best_run}\"\nrun_data = spr_data[best_key]\n\n# Final / best values\nfinal_train_loss = run_data[\"losses\"][\"train\"][-1]\nfinal_val_loss = run_data[\"losses\"][\"val\"][-1]\nval_metrics = run_data[\"metrics\"][\"val\"][-1]  # dict with cwa, swa, hwa\ntest_metrics = spr_data[\"metrics_test\"]  # dict with cwa, swa, hwa\n\n# --------------------------------------------------\n# Pretty-print results\n# --------------------------------------------------\nprint(\"SPR_BENCH - TRAIN\")\nprint(f\"train loss: {final_train_loss:.4f}\")\n\nprint(\"\\nSPR_BENCH - VALIDATION\")\nprint(f\"validation loss: {final_val_loss:.4f}\")\nprint(f\"validation CWA:  {val_metrics['cwa']:.3f}\")\nprint(f\"validation SWA:  {val_metrics['swa']:.3f}\")\nprint(f\"validation HWA:  {val_metrics['hwa']:.3f}\")\n\nprint(\"\\nSPR_BENCH - TEST\")\nprint(f\"test CWA:  {test_metrics['cwa']:.3f}\")\nprint(f\"test SWA:  {test_metrics['swa']:.3f}\")\nprint(f\"test HWA:  {test_metrics['hwa']:.3f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------\n# Locate and load the saved experiment data\n# --------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Cannot find experiment data at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# --------------------------------------------------\n# Extract information for the SPR_BENCH experiment\n# --------------------------------------------------\nspr_data = experiment_data[\"EPOCHS\"][\"SPR_BENCH\"]\nbest_run = spr_data[\"best_run\"]  # e.g. 15\nbest_key = f\"run_{best_run}\"\nrun_data = spr_data[best_key]\n\n# Final / best values\nfinal_train_loss = run_data[\"losses\"][\"train\"][-1]\nfinal_val_loss = run_data[\"losses\"][\"val\"][-1]\nval_metrics = run_data[\"metrics\"][\"val\"][-1]  # dict with cwa, swa, hwa\ntest_metrics = spr_data[\"metrics_test\"]  # dict with cwa, swa, hwa\n\n# --------------------------------------------------\n# Pretty-print results\n# --------------------------------------------------\nprint(\"SPR_BENCH - TRAIN\")\nprint(f\"train loss: {final_train_loss:.4f}\")\n\nprint(\"\\nSPR_BENCH - VALIDATION\")\nprint(f\"validation loss: {final_val_loss:.4f}\")\nprint(f\"validation CWA:  {val_metrics['cwa']:.3f}\")\nprint(f\"validation SWA:  {val_metrics['swa']:.3f}\")\nprint(f\"validation HWA:  {val_metrics['hwa']:.3f}\")\n\nprint(\"\\nSPR_BENCH - TEST\")\nprint(f\"test CWA:  {test_metrics['cwa']:.3f}\")\nprint(f\"test SWA:  {test_metrics['swa']:.3f}\")\nprint(f\"test HWA:  {test_metrics['hwa']:.3f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'final training loss: 0.2416', '\\n', 'best validation loss:\n0.2341', '\\n', 'best validation color weighted accuracy:  0.9281', '\\n', 'best\nvalidation shape weighted accuracy:   0.9273', '\\n', 'best validation harmonic\nweighted accuracy: 0.9277', '\\n', 'test color weighted accuracy:  0.6724', '\\n',\n'test shape weighted accuracy:   0.6301', '\\n', 'test harmonic weighted\naccuracy: 0.6506', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH - TRAIN', '\\n', 'train loss: 0.1361', '\\n', '\\nSPR_BENCH -\nVALIDATION', '\\n', 'validation loss: 0.1741', '\\n', 'validation CWA:  0.956',\n'\\n', 'validation SWA:  0.956', '\\n', 'validation HWA:  0.956', '\\n',\n'\\nSPR_BENCH - TEST', '\\n', 'test CWA:  0.682', '\\n', 'test SWA:  0.638', '\\n',\n'test HWA:  0.659', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best learning rate: 0.002', '\\n', 'Final training\nloss: 0.2093', '\\n', 'Final validation loss: 0.2012', '\\n', 'Final validation\ncolor-weighted accuracy: 0.9442', '\\n', 'Final validation shape-weighted\naccuracy: 0.9442', '\\n', 'Final validation harmonic-weighted accuracy: 0.9442',\n'\\n', 'Test color-weighted accuracy: 0.6763', '\\n', 'Test shape-weighted\naccuracy: 0.6329', '\\n', 'Test harmonic-weighted accuracy: 0.6539', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH (batch size 32)', '\\n', '    best training loss: 0.2242',\n'\\n', '    best validation loss: 0.2153', '\\n', '    best validation CWA:\n0.9403', '\\n', '    best validation SWA: 0.9400', '\\n', '    best validation\nHWA: 0.9402', '\\n', '    test CWA: 0.6757', '\\n', '    test SWA: 0.6326', '\\n',\n'    test HWA: 0.6534', '\\n', '', '\\n', 'Dataset: SPR_BENCH (batch size 64)',\n'\\n', '    best training loss: 0.2602', '\\n', '    best validation loss:\n0.2557', '\\n', '    best validation CWA: 0.9173', '\\n', '    best validation\nSWA: 0.9156', '\\n', '    best validation HWA: 0.9165', '\\n', '    test CWA:\n0.6701', '\\n', '    test SWA: 0.6280', '\\n', '    test HWA: 0.6484', '\\n', '',\n'\\n', 'Dataset: SPR_BENCH (batch size 128)', '\\n', '    best training loss:\n0.2617', '\\n', '    best validation loss: 0.2643', '\\n', '    best validation\nCWA: 0.9102', '\\n', '    best validation SWA: 0.9101', '\\n', '    best\nvalidation HWA: 0.9102', '\\n', '    test CWA: 0.6746', '\\n', '    test SWA:\n0.6324', '\\n', '    test HWA: 0.6528', '\\n', '', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH | embedding dimension: 16', '\\n', '  training loss:\n0.279486', '\\n', '  validation loss: 0.287190', '\\n', '  validation color\nweighted accuracy: 0.890123', '\\n', '  validation shape weighted accuracy:\n0.886234', '\\n', '  validation harmonic weighted accuracy: 0.888174', '\\n', '\ntest color weighted accuracy: 0.670191', '\\n', '  test shape weighted accuracy:\n0.629329', '\\n', '  test harmonic weighted accuracy: 0.649118\\n', '\\n',\n'Dataset: SPR_BENCH | embedding dimension: 32', '\\n', '  training loss:\n0.243485', '\\n', '  validation loss: 0.239435', '\\n', '  validation color\nweighted accuracy: 0.926789', '\\n', '  validation shape weighted accuracy:\n0.926171', '\\n', '  validation harmonic weighted accuracy: 0.926480', '\\n', '\ntest color weighted accuracy: 0.670677', '\\n', '  test shape weighted accuracy:\n0.628373', '\\n', '  test harmonic weighted accuracy: 0.648836\\n', '\\n',\n'Dataset: SPR_BENCH | embedding dimension: 64', '\\n', '  training loss:\n0.227589', '\\n', '  validation loss: 0.217457', '\\n', '  validation color\nweighted accuracy: 0.937771', '\\n', '  validation shape weighted accuracy:\n0.936868', '\\n', '  validation harmonic weighted accuracy: 0.937319', '\\n', '\ntest color weighted accuracy: 0.674566', '\\n', '  test shape weighted accuracy:\n0.631792', '\\n', '  test harmonic weighted accuracy: 0.652479\\n', '\\n',\n'Dataset: SPR_BENCH | embedding dimension: 128', '\\n', '  training loss:\n0.209115', '\\n', '  validation loss: 0.202240', '\\n', '  validation color\nweighted accuracy: 0.942041', '\\n', '  validation shape weighted accuracy:\n0.941925', '\\n', '  validation harmonic weighted accuracy: 0.941983', '\\n', '\ntest color weighted accuracy: 0.678698', '\\n', '  test shape weighted accuracy:\n0.635676', '\\n', '  test harmonic weighted accuracy: 0.656483\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH (weight_decay = 0)', '\\n', '  final training loss      : 0.2412',\n'\\n', '  best validation loss     : 0.2339', '\\n', '  best validation CWA      :\n0.9282', '\\n', '  best validation SWA      : 0.9276', '\\n', '  best validation\nHWA      : 0.9279', '\\n', '  test CWA                 : 0.6726', '\\n', '  test\nSWA                 : 0.6302', '\\n', '  test HWA                 : 0.6507',\n'\\n', '\\n', 'SPR_BENCH (weight_decay = 0.0001)', '\\n', '  final training loss\n: 0.2615', '\\n', '  best validation loss     : 0.2567', '\\n', '  best validation\nCWA      : 0.9170', '\\n', '  best validation SWA      : 0.9154', '\\n', '  best\nvalidation HWA      : 0.9162', '\\n', '  test CWA                 : 0.6707',\n'\\n', '  test SWA                 : 0.6289', '\\n', '  test HWA                 :\n0.6491', '\\n', '\\n', 'SPR_BENCH (weight_decay = 0.0005)', '\\n', '  final\ntraining loss      : 0.2463', '\\n', '  best validation loss     : 0.2470', '\\n',\n'  best validation CWA      : 0.9202', '\\n', '  best validation SWA      :\n0.9174', '\\n', '  best validation HWA      : 0.9188', '\\n', '  test CWA\n: 0.6755', '\\n', '  test SWA                 : 0.6337', '\\n', '  test HWA\n: 0.6539', '\\n', '\\n', 'SPR_BENCH (weight_decay = 0.001)', '\\n', '  final\ntraining loss      : 0.2528', '\\n', '  best validation loss     : 0.2504', '\\n',\n'  best validation CWA      : 0.9244', '\\n', '  best validation SWA      :\n0.9252', '\\n', '  best validation HWA      : 0.9248', '\\n', '  test CWA\n: 0.6658', '\\n', '  test SWA                 : 0.6243', '\\n', '  test HWA\n: 0.6443', '\\n', '\\n', 'SPR_BENCH (weight_decay = 0.01)', '\\n', '  final\ntraining loss      : 0.3447', '\\n', '  best validation loss     : 0.3428', '\\n',\n'  best validation CWA      : 0.8812', '\\n', '  best validation SWA      :\n0.8819', '\\n', '  best validation HWA      : 0.8815', '\\n', '  test CWA\n: 0.6586', '\\n', '  test SWA                 : 0.6205', '\\n', '  test HWA\n: 0.6390', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'best training loss: 0.1508', '\\n', 'best validation loss:\n0.1900', '\\n', 'best validation color weighted accuracy: 0.9473', '\\n', 'best\nvalidation shape weighted accuracy: 0.9460', '\\n', 'best validation harmonic\nweighted accuracy: 0.9466', '\\n', 'best test color weighted accuracy: 0.6815',\n'\\n', 'best test shape weighted accuracy: 0.6377', '\\n', 'best test harmonic\nweighted accuracy: 0.6589', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH (dropout rate = 0.0)', '\\n', 'final training loss: 0.2412',\n'\\n', 'final validation loss: 0.2339', '\\n', 'final validation color weighted\naccuracy: 0.9282', '\\n', 'final validation shape weighted accuracy: 0.9276',\n'\\n', 'final validation harmonic weighted accuracy: 0.9279', '\\n', 'test color\nweighted accuracy: 0.6726', '\\n', 'test shape weighted accuracy: 0.6302', '\\n',\n'test harmonic weighted accuracy: 0.6507', '\\n', '\\n', 'Dataset: SPR_BENCH\n(dropout rate = 0.2)', '\\n', 'final training loss: 0.3252', '\\n', 'final\nvalidation loss: 0.2732', '\\n', 'final validation color weighted accuracy:\n0.9110', '\\n', 'final validation shape weighted accuracy: 0.9115', '\\n', 'final\nvalidation harmonic weighted accuracy: 0.9112', '\\n', 'test color weighted\naccuracy: 0.6648', '\\n', 'test shape weighted accuracy: 0.6243', '\\n', 'test\nharmonic weighted accuracy: 0.6439', '\\n', '\\n', 'Dataset: SPR_BENCH (dropout\nrate = 0.4)', '\\n', 'final training loss: 0.3789', '\\n', 'final validation loss:\n0.3124', '\\n', 'final validation color weighted accuracy: 0.8907', '\\n', 'final\nvalidation shape weighted accuracy: 0.8905', '\\n', 'final validation harmonic\nweighted accuracy: 0.8906', '\\n', 'test color weighted accuracy: 0.6629', '\\n',\n'test shape weighted accuracy: 0.6244', '\\n', 'test harmonic weighted accuracy:\n0.6430', '\\n', '\\n', 'Dataset: SPR_BENCH (dropout rate = 0.6)', '\\n', 'final\ntraining loss: 0.4661', '\\n', 'final validation loss: 0.3967', '\\n', 'final\nvalidation color weighted accuracy: 0.8332', '\\n', 'final validation shape\nweighted accuracy: 0.8353', '\\n', 'final validation harmonic weighted accuracy:\n0.8343', '\\n', 'test color weighted accuracy: 0.6390', '\\n', 'test shape\nweighted accuracy: 0.6041', '\\n', 'test harmonic weighted accuracy: 0.6211',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH - TRAIN', '\\n', 'train loss: 0.1119', '\\n', '\\nSPR_BENCH -\nVALIDATION', '\\n', 'validation loss: 0.1723', '\\n', 'validation CWA:  0.957',\n'\\n', 'validation SWA:  0.956', '\\n', 'validation HWA:  0.957', '\\n',\n'\\nSPR_BENCH - TEST', '\\n', 'test CWA:  0.684', '\\n', 'test SWA:  0.640', '\\n',\n'test HWA:  0.661', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH - TRAIN', '\\n', 'train loss: 0.1116', '\\n', '\\nSPR_BENCH -\nVALIDATION', '\\n', 'validation loss: 0.1713', '\\n', 'validation CWA:  0.957',\n'\\n', 'validation SWA:  0.956', '\\n', 'validation HWA:  0.956', '\\n',\n'\\nSPR_BENCH - TEST', '\\n', 'test CWA:  0.685', '\\n', 'test SWA:  0.640', '\\n',\n'test HWA:  0.662', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH - TRAIN', '\\n', 'train loss: 0.1108', '\\n', '\\nSPR_BENCH -\nVALIDATION', '\\n', 'validation loss: 0.1678', '\\n', 'validation CWA:  0.960',\n'\\n', 'validation SWA:  0.959', '\\n', 'validation HWA:  0.959', '\\n',\n'\\nSPR_BENCH - TEST', '\\n', 'test CWA:  0.683', '\\n', 'test SWA:  0.639', '\\n',\n'test HWA:  0.660', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
