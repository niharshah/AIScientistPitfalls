
% The paper 'Semi-Supervised Classification with Graph Convolutional Networks' by Kipf and Welling introduces Graph Convolutional Networks (GCNs) and demonstrates their effectiveness for semi-supervised learning on graph-structured data. This foundational work is critical to support the use of GNNs in SPR by establishing their capabilities in capturing relational and structural dependencies. It should be cited in the related work section to summarize the theoretical basis of GNNs and highlight their relevance to the proposed method.
@article{kipf2016semisupervisedcw,
 author = {Thomas Kipf and M. Welling},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Semi-Supervised Classification with Graph Convolutional Networks},
 volume = {abs/1609.02907},
 year = {2016}
}

% The paper 'Graph Attention Networks' by Velickovic et al. introduces Graph Attention Networks (GATs), which leverage masked self-attentional layers to address limitations of prior graph convolution methods. This foundational work is directly relevant to the proposed use of multi-head attention mechanisms in the research plan and should be cited in the related work and methodology sections to support the architectural choice and highlight the advancements in GNNs.
@article{velickovic2017graphan,
 author = {Petar Velickovic and Guillem Cucurull and Arantxa Casanova and Adriana Romero and P. Lio’ and Yoshua Bengio},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Graph Attention Networks},
 volume = {abs/1710.10903},
 year = {2017}
}

% The paper 'Attention is All You Need' by Vaswani et al. introduces the Transformer architecture, which is a foundational sequence model based on self-attention mechanisms. This work is critical for summarizing existing literature on sequence-based models and provides context for comparing the performance of GNNs to Transformers in the SPR task. It should be cited in the related work section to emphasize the dominance of sequence models and highlight the research gap addressed by this study.
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}

% The paper 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus' introduces a synthetic corpus named FLD for logical reasoning tasks. It provides insights into designing high-quality reasoning datasets, which is directly relevant to the SPR_BENCH benchmark used in this study. This paper should be cited in the related work section to highlight the importance of synthetic datasets for reasoning tasks and to draw comparisons between FLD and SPR_BENCH.
@article{morishita2024enhancingrc,
 author = {Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},
 booktitle = {Neural Information Processing Systems},
 journal = {ArXiv},
 title = {Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus},
 volume = {abs/2411.12498},
 year = {2024}
}

% The paper 'Graph Neural Networks for Routing Optimization: Challenges and Opportunities' provides a comprehensive review of the challenges and limitations of applying GNNs in real-world tasks, such as scalability, deployment, explainability, and security. These insights are directly relevant to the 'Risk Factors and Limitations' section of the paper, especially for discussing potential issues with computational complexity and generalization in the SPR task.
@article{jiang2024graphnn,
 author = {Weiwei Jiang and Haoyu Han and Yang Zhang and Ji’an Wang and Miao He and Weixi Gu and Jianbin Mu and Xirong Cheng},
 booktitle = {Sustainability},
 journal = {Sustainability},
 title = {Graph Neural Networks for Routing Optimization: Challenges and Opportunities},
 year = {2024}
}

% Paper 0 introduces a Depth-Wise Graph Neural Network (DepWiGNN) tailored for multi-hop spatial reasoning tasks, addressing challenges like over-smoothing in GNNs. It is relevant to the SPR task's requirement for capturing long dependencies and should be cited in the related work section to highlight advancements in using GNNs for reasoning tasks. Paper 2 discusses neural-symbolic reasoning on graphs, emphasizing relational inference and link prediction, which supports the argument for using GNNs to model symbolic data in SPR. This citation is important for the methodology and related work sections. Paper 3, focusing on symbolic graph reasoning with convolutions, is relevant for discussing the structural and relational modeling capabilities of GNNs in SPR and should be cited in the related work section.
@article{li2023depwignnad,
 author = {Shuaiyi Li and Yang Deng and Wai Lam},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 journal = {ArXiv},
 title = {DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text},
 volume = {abs/2310.12557},
 year = {2023}
}

@article{lemos2020neuralsymbolicrr,
 author = {Henrique Lemos and Pedro H. C. Avelar and Marcelo O. R. Prates and L. Lamb and A. Garcez},
 booktitle = {International Conference on Artificial Neural Networks},
 journal = {ArXiv},
 title = {Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases},
 volume = {abs/2005.02525},
 year = {2020}
}

@article{liang2018symbolicgr,
 author = {Xiaodan Liang and Zhiting Hu and Hao Zhang and Liang Lin and E. Xing},
 booktitle = {Neural Information Processing Systems},
 pages = {1858-1868},
 title = {Symbolic Graph Reasoning Meets Convolutions},
 year = {2018}
}

% The paper 'Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data' explores the use of graph-based synthetic reasoning data to improve reasoning performance in LLMs. This work is relevant to the SPR_BENCH benchmark and its evaluation metrics, providing support for using synthetic datasets to enhance logical reasoning capabilities. It should be cited in the related work section to highlight the relevance and effectiveness of synthetic datasets in reasoning tasks and to draw parallels with the SPR_BENCH benchmark.
@article{zhou2024enhancinglr,
 author = {Jiaming Zhou and Abbas Ghaddar and Ge Zhang and Liheng Ma and Yaochen Hu and Soumyasundar Pal and Mark Coates and Bin Wang and Yingxue Zhang and Jianye Hao},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data},
 volume = {abs/2409.12437},
 year = {2024}
}
