<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 10], [0, 9], [0, 11], [0, 8]], "layout": [[0.17647058823529413, 0.0], [0.29411764705882354, 0.0], [0.4117647058823529, 0.0], [0.5294117647058824, 0.0], [0.6470588235294118, 0.0], [0.7647058823529411, 0.0], [0.8823529411764706, 0.0], [1.0, 0.0], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["Hyperparam tuning name: num_epochs. We reload the dataset once, then loop over a\nlist of candidate maximum-epoch budgets (10-50).   For every budget we (1)\ncreate a fresh GCN model, (2) train up to that many epochs while using early-\nstopping on dev-set BWA with a patience of five, (3) keep the best model\nweights, (4) evaluate on the test set, and (5) store all curves, predictions,\netc. in the required experiment_data structure.   At the end we save\nexperiment_data.npy and write one BWA curve png per run.", "Hyperparam tuning name: learning_rate. We keep the original data-loading, graph\nconversion and model definitions unchanged, but wrap training/evaluation into a\nreusable function that accepts a learning-rate argument.   A logarithmic sweep\nover {3e-4, 5e-4, 1e-3, 3e-3} is executed; for each rate we train the GCN for an\nequal number of epochs, store losses/BWA curves in\n`experiment_data['learning_rate']['SPR_BENCH'][str(lr)]`, and track which rate\nyields the highest dev-set BWA after the final epoch.   The model obtained with\nthis best learning rate is then evaluated on the test set, predictions are\nrecorded, and everything is saved to `working/experiment_data.npy` together with\na plot of the BWA curves for the best learning rate.", "Hyperparam tuning name: batch_size. We loop over candidate batch sizes (32 / 64\n/ 128).   For every batch size we (re-)instantiate data-loaders, model,\noptimizer and run the same 5-epoch training routine, collecting train/dev losses\nand BWA.   After each run we evaluate on the test split and store all results in\na nested experiment_data dictionary under\nexperiment_data['batch_size']['SPR_BENCH'][str(bs)].   Finally we save\nexperiment_data.npy and plot the BWA curves of all batch-size variants in a\nsingle figure.", "Hyperparam tuning name: weight_decay. Below is a compact re-write of the\nbaseline that sweeps Adam\u2019s weight_decay over {0, 1e-5, 1e-4, 1e-3}.   For every\nsetting the model is re-initialised, trained for a few epochs, evaluated, and\nall results are stored in the required experiment_data dict and saved to\nworking/experiment_data.npy.", "Hyperparam tuning name: embed_dim. Below is the refactored script.   It loops\nover a list of candidate embedding dimensions, trains a fresh GCN for each, logs\nepoch-wise losses & metrics, picks the dimensionality with the best dev BWA, and\nfinally evaluates that best model on the test set.   All results are stored in\nthe required experiment_data structure and saved as experiment_data.npy.", "Hyperparam tuning name: dropout_rate. We will train identical GCN models while\nsweeping dropout rates {0 (no dropout), 0.1, 0.3, 0.5}.   For every rate we re-\ninitialise the network, train for a small fixed number of epochs, evaluate after\neach epoch on the dev set, and finally test on the held-out set.   All losses,\nweighted-accuracies and predictions are stored in a nested experiment_data\ndictionary under the key \"dropout_rate\" \u2192 \"SPR_BENCH\" \u2192 str(rate).   The whole\nscript is self-contained and can be executed directly.", "Hyperparam tuning name: num_gcn_layers. The solution loops over different\nnumbers of GCN layers (1 \u2013 4), trains a fresh model for each setting, records\nper-epoch losses / accuracies, keeps the best dev BWA, and finally evaluates the\nbest checkpoint on the test set. All results are stored in the required\n`experiment_data.npy`.", "Hyperparam tuning name: gcn_hidden_dim. A grid-search over the GCN hidden\ndimension is added.   For every value in a predefined list (32, 64, 128, 256) a\nfresh model is trained for a few epochs, its dev-set BWA is recorded and the\nbest configuration is kept for the final test evaluation.   All per-run metrics,\nlosses and predictions are stored in the mandatory experiment_data dict and\nsaved to experiment_data.npy, while a curve for the best hidden size is also\nplotted.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, copy, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Helper to locate the dataset -----------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Place the folder appropriately or set SPR_DATA_PATH.\"\n    )\n\n\n# ---------------------------------------------------------------------\n# ---------- Provided helpers -----------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        dset[sp] = _load(f\"{sp}.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\n# ---------------------------------------------------------------------\n# ---------- Load and preprocess dataset ------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\n\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_indices = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_indices, dtype=torch.long).unsqueeze(-1)\n\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# ---------- Model Definition -----------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\n# ---------------------------------------------------------------------\n# ---------- Evaluation helper ----------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            total_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    return total_loss / len(loader.dataset), bwa, cwa, swa, all_preds, all_labels\n\n\n# ---------------------------------------------------------------------\n# ---------- Hyperparameter tuning over num_epochs --------------------\ncandidate_max_epochs = [10, 20, 30, 40, 50]\npatience = 5\n\nexperiment_data = {\"num_epochs\": {}}\n\nfor max_ep in candidate_max_epochs:\n    run_key = f\"max_{max_ep}\"\n    experiment_data[\"num_epochs\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_bwa, best_state, wait = -1.0, None, 0\n\n    for epoch in range(1, max_ep + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(model, dev_loader)\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader)\n\n        # log\n        exd = experiment_data[\"num_epochs\"][run_key]\n        exd[\"losses\"][\"train\"].append(train_loss)\n        exd[\"losses\"][\"val\"].append(val_loss)\n        exd[\"metrics\"][\"train\"].append(train_bwa)\n        exd[\"metrics\"][\"val\"].append(val_bwa)\n        exd[\"timestamps\"].append(time.time())\n\n        print(\n            f\"[{run_key}] Epoch {epoch:3d}/{max_ep}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"BWA={val_bwa:.4f} (CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n        # early stopping\n        if val_bwa > best_bwa:\n            best_bwa, best_state, wait = val_bwa, copy.deepcopy(model.state_dict()), 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(f\"[{run_key}] Early stopping triggered at epoch {epoch}.\")\n                break\n\n    # restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # final test evaluation\n    test_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n        model, test_loader\n    )\n    exd[\"predictions\"] = test_preds\n    exd[\"ground_truth\"] = test_labels\n    exd[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"BWA\": test_bwa,\n        \"CWA\": test_cwa,\n        \"SWA\": test_swa,\n    }\n    print(\n        f\"[{run_key}] Test -> loss: {test_loss:.4f}  \"\n        f\"BWA: {test_bwa:.4f} (CWA={test_cwa:.4f} SWA={test_swa:.4f})\"\n    )\n\n    # -----------------------------------------------------------------\n    # plotting for this run\n    epochs_arr = np.arange(1, len(exd[\"metrics\"][\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs_arr, exd[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(epochs_arr, exd[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(f\"BWA curve ({run_key})\")\n    plt.legend()\n    plt.tight_layout()\n    plot_path = os.path.join(working_dir, f\"bwa_curve_{run_key}.png\")\n    plt.savefig(plot_path)\n    plt.close()\n    print(f\"[{run_key}] Curve saved to {plot_path}\")\n\n    # free gpu memory between runs\n    del model\n    torch.cuda.empty_cache()\n\n# ---------------------------------------------------------------------\n# ---------- Save experiment data -------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All experiment data saved to working/experiment_data.npy\")\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir ------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# device ---------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# dataset locating helpers --------------------------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Place it in cwd/.. or set SPR_DATA_PATH.\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# ---------------------------------------------------------------------\n# metric helpers -------------------------------------------------------\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1)\n\n\n# ---------------------------------------------------------------------\n# load dataset ---------------------------------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n# ---------------------------------------------------------------------\n# vocabularies ---------------------------------------------------------\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\n# ---------------------------------------------------------------------\n# graph construction ---------------------------------------------------\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_idx = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_idx, dtype=torch.long).unsqueeze(-1)\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    d = Data(x=x, edge_index=edge_index, y=y)\n    d.seq = seq\n    return d\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# model ----------------------------------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.drop = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.drop(x)\n        return self.lin(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# ---------------------------------------------------------------------\n# experiment data dict -------------------------------------------------\nexperiment_data = {\"learning_rate\": {\"SPR_BENCH\": {}}}\n\n\n# ---------------------------------------------------------------------\n# evaluation -----------------------------------------------------------\ndef evaluate(model: nn.Module, loader):\n    model.eval()\n    preds, labels, seqs = [], [], []\n    loss_sum = 0.0\n    with torch.no_grad():\n        for bt in loader:\n            bt = bt.to(device)\n            out = model(bt.x, bt.edge_index, bt.batch)\n            loss = criterion(out, bt.y.view(-1))\n            loss_sum += loss.item() * bt.num_graphs\n            preds.extend(out.argmax(-1).cpu().tolist())\n            labels.extend(bt.y.view(-1).cpu().tolist())\n            seqs.extend(bt.seq)\n    cwa = color_weighted_accuracy(seqs, labels, preds)\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    return loss_sum / len(loader.dataset), (cwa + swa) / 2.0, cwa, swa, preds, labels\n\n\n# ---------------------------------------------------------------------\n# training loop per LR -------------------------------------------------\ndef train_with_lr(lr: float, num_epochs: int = 5):\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=lr)\n    lr_key = f\"{lr:.0e}\" if lr < 1 else str(lr)\n    exp_entry = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        ep_loss = 0.0\n        for bt in train_loader:\n            bt = bt.to(device)\n            optim.zero_grad()\n            out = model(bt.x, bt.edge_index, bt.batch)\n            loss = criterion(out, bt.y.view(-1))\n            loss.backward()\n            optim.step()\n            ep_loss += loss.item() * bt.num_graphs\n        train_loss = ep_loss / len(train_loader.dataset)\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(model, dev_loader)\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader)\n\n        exp_entry[\"losses\"][\"train\"].append(train_loss)\n        exp_entry[\"losses\"][\"val\"].append(val_loss)\n        exp_entry[\"metrics\"][\"train\"].append(train_bwa)\n        exp_entry[\"metrics\"][\"val\"].append(val_bwa)\n        exp_entry[\"timestamps\"].append(time.time())\n\n        print(\n            f\"[lr={lr}] Epoch {ep}/{num_epochs} \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"BWA={val_bwa:.4f} (CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][lr_key] = exp_entry\n    return model, exp_entry[\"metrics\"][\"val\"][-1]  # final val BWA\n\n\n# ---------------------------------------------------------------------\n# hyperparameter sweep -------------------------------------------------\nlearning_rates = [3e-4, 5e-4, 1e-3, 3e-3]\nbest_lr, best_val_bwa, best_model = None, -1.0, None\n\nfor lr in learning_rates:\n    model, final_val_bwa = train_with_lr(lr, num_epochs=5)\n    if final_val_bwa > best_val_bwa:\n        best_val_bwa = final_val_bwa\n        best_lr = lr\n        best_model = model  # keep reference to best model\nprint(f\"\\nBest learning rate based on dev BWA: {best_lr} (BWA={best_val_bwa:.4f})\")\n\n# ---------------------------------------------------------------------\n# test evaluation with best model -------------------------------------\ntest_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n    best_model, test_loader\n)\nprint(\n    f\"Test results with best lr={best_lr}: \"\n    f\"loss={test_loss:.4f} BWA={test_bwa:.4f} \"\n    f\"CWA={test_cwa:.4f} SWA={test_swa:.4f}\"\n)\nbest_key = f\"{best_lr:.0e}\" if best_lr < 1 else str(best_lr)\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][best_key][\"predictions\"] = test_preds\nexperiment_data[\"learning_rate\"][\"SPR_BENCH\"][best_key][\"ground_truth\"] = test_labels\n\n# ---------------------------------------------------------------------\n# save experiment data -------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------------------------------------------------------------------\n# plot BWA curve for best lr ------------------------------------------\nepochs = np.arange(\n    1,\n    len(experiment_data[\"learning_rate\"][\"SPR_BENCH\"][best_key][\"metrics\"][\"val\"]) + 1,\n)\nplt.figure()\nplt.plot(\n    epochs,\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][best_key][\"metrics\"][\"train\"],\n    label=\"Train BWA\",\n)\nplt.plot(\n    epochs,\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][best_key][\"metrics\"][\"val\"],\n    label=\"Dev BWA\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BWA\")\nplt.title(f\"BWA over epochs (best lr={best_lr})\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"bwa_curve_spr.png\"))\nprint(f\"Curve saved to {os.path.join(working_dir, 'bwa_curve_spr.png')}\")\n", "import os, pathlib, time, gc, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------------#\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------#\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\n# ------------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) else 1)\n\n\n# ------------------------------------------------------------------#\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n# vocab\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\n# graphs\ndef seq_to_data(example):\n    tokens = extract_tokens(example[\"sequence\"])\n    x = torch.tensor([token2idx.get(t, 0) for t in tokens], dtype=torch.long).unsqueeze(\n        -1\n    )\n    if len(tokens) > 1:\n        src = torch.arange(len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    d = Data(x=x, edge_index=edge_index, y=y)\n    d.seq = example[\"sequence\"]\n    return d\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\n\n# ------------------------------------------------------------------#\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.drop = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.drop(x)\n        return self.lin(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, preds, labels, seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for b in loader:\n            b = b.to(device)\n            out = model(b.x, b.edge_index, b.batch)\n            loss = criterion(out, b.y.view(-1))\n            total_loss += loss.item() * b.num_graphs\n            preds.extend(out.argmax(-1).cpu().tolist())\n            labels.extend(b.y.view(-1).cpu().tolist())\n            seqs.extend(b.seq)\n    cwa = color_weighted_accuracy(seqs, labels, preds)\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    bwa = (cwa + swa) / 2\n    return total_loss / len(loader.dataset), bwa, cwa, swa, preds, labels\n\n\n# ------------------------------------------------------------------#\nexperiment_data = {\"batch_size\": {\"SPR_BENCH\": {}}}\n\nbatch_sizes = [32, 64, 128]\nnum_epochs = 5\ncolors = {32: \"r\", 64: \"g\", 128: \"b\"}\nplt.figure()\n\nfor bs in batch_sizes:\n    print(f\"\\n===== Training with batch_size={bs} =====\")\n    tr_loader = DataLoader(train_graphs, batch_size=bs, shuffle=True)\n    dv_loader = DataLoader(dev_graphs, batch_size=bs, shuffle=False)\n    te_loader = DataLoader(test_graphs, batch_size=bs, shuffle=False)\n\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    run_data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        ep_loss = 0.0\n        for batch in tr_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            ep_loss += loss.item() * batch.num_graphs\n        train_loss = ep_loss / len(tr_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(model, dv_loader)\n        _, train_bwa, _, _, _, _ = evaluate(model, tr_loader)\n\n        run_data[\"losses\"][\"train\"].append(train_loss)\n        run_data[\"losses\"][\"val\"].append(val_loss)\n        run_data[\"metrics\"][\"train\"].append(train_bwa)\n        run_data[\"metrics\"][\"val\"].append(val_bwa)\n        run_data[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}/{num_epochs} bs={bs} \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"BWA={val_bwa:.4f} (CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n    # test evaluation\n    test_loss, test_bwa, test_cwa, test_swa, preds, labels = evaluate(model, te_loader)\n    run_data[\"predictions\"] = preds\n    run_data[\"ground_truth\"] = labels\n    run_data[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"bwa\": test_bwa,\n        \"cwa\": test_cwa,\n        \"swa\": test_swa,\n    }\n    print(\n        f\"Test bs={bs} -> loss:{test_loss:.4f} BWA:{test_bwa:.4f} \"\n        f\"CWA:{test_cwa:.4f} SWA:{test_swa:.4f}\"\n    )\n\n    # store run_data\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][str(bs)] = run_data\n\n    epochs = np.arange(1, num_epochs + 1)\n    plt.plot(epochs, run_data[\"metrics\"][\"val\"], label=f\"bs={bs}\", color=colors[bs])\n\n    # cleanup\n    del model, optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Dev BWA\")\nplt.title(\"Batch size tuning on SPR_BENCH\")\nplt.legend()\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"bwa_batchsize_curve.png\")\nplt.savefig(plot_path)\nprint(f\"Plot saved to {plot_path}\")\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"experiment_data.npy saved.\")\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom datasets import DatasetDict, load_dataset\nfrom typing import List\n\n# ---------------------------------------------------------------------\n# working dir / device ------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------------------------------------------------------------\n# find SPR-BENCH -------------------------------------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    cands = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in cands:\n        if p and (p / \"train.csv\").exists():\n            print(\"Found SPR_BENCH at:\", p.resolve())\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset folder not found.\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(split):\n        return load_dataset(\n            \"csv\", data_files=str(root / split), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\n# ---------------------------------------------------------------------\n# metric helpers ------------------------------------------------------\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y, yhat):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if yi == pi else 0 for wi, yi, pi in zip(w, y, yhat)) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y, yhat):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if yi == pi else 0 for wi, yi, pi in zip(w, y, yhat)) / (sum(w) or 1)\n\n\n# ---------------------------------------------------------------------\n# data & vocab --------------------------------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\ntoken2idx, label2idx = {}, {}\nfor ex in spr[\"train\"]:\n    for tok in extract_tokens(ex[\"sequence\"]):\n        if tok not in token2idx:\n            token2idx[tok] = len(token2idx) + 1\n    if ex[\"label\"] not in label2idx:\n        label2idx[ex[\"label\"]] = len(label2idx)\nidx2label = {i: l for l, i in label2idx.items()}\n\n\ndef seq_to_data(example):\n    tokens = extract_tokens(example[\"sequence\"])\n    x = torch.tensor([token2idx.get(t, 0) for t in tokens], dtype=torch.long).unsqueeze(\n        -1\n    )\n    if len(tokens) > 1:\n        src = torch.arange(len(tokens) - 1)\n        dst = src + 1\n        edge = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])])\n    else:\n        edge = torch.zeros((2, 0), dtype=torch.long)\n    data = Data(\n        x=x,\n        edge_index=edge,\n        y=torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long),\n    )\n    data.seq = example[\"sequence\"]\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size)\n\n\n# ---------------------------------------------------------------------\n# model ---------------------------------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab, emb, ncls):\n        super().__init__()\n        self.embed = nn.Embedding(vocab + 1, emb, padding_idx=0)\n        self.g1, self.g2 = GCNConv(emb, 64), GCNConv(64, 64)\n        self.lin = nn.Linear(64, ncls)\n        self.do = nn.Dropout(0.2)\n\n    def forward(self, x, edge, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.g1(x, edge).relu()\n        x = self.g2(x, edge).relu()\n        x = global_mean_pool(x, batch)\n        return self.lin(self.do(x))\n\n\n# ---------------------------------------------------------------------\n# evaluation ----------------------------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    tot_loss, preds, labels, seqs = 0, [], [], []\n    with torch.no_grad():\n        for b in loader:\n            b = b.to(device)\n            out = model(b.x, b.edge_index, b.batch)\n            loss = criterion(out, b.y.view(-1))\n            tot_loss += loss.item() * b.num_graphs\n            pr = out.argmax(-1).cpu().tolist()\n            lb = b.y.view(-1).cpu().tolist()\n            preds += pr\n            labels += lb\n            seqs += b.seq\n    cwa = color_weighted_accuracy(seqs, labels, preds)\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    return tot_loss / len(loader.dataset), (cwa + swa) / 2, cwa, swa, preds, labels\n\n\n# ---------------------------------------------------------------------\n# hyper-parameter sweep ----------------------------------------------\nweight_decays = [0.0, 1e-5, 1e-4, 1e-3]\nnum_epochs = 5\nexperiment_data = {\"weight_decay\": {}}\n\nfor wd in weight_decays:\n    tag = f\"wd_{wd}\"\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n        \"weight_decay\": wd,\n    }\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        tloss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optim.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optim.step()\n            tloss += loss.item() * batch.num_graphs\n        tloss /= len(train_loader.dataset)\n        vloss, vbwa, vcwa, vswa, *_ = evaluate(model, dev_loader)\n        _, tbwa, _, _, *_ = evaluate(model, train_loader)\n        exp[\"losses\"][\"train\"].append(tloss)\n        exp[\"losses\"][\"val\"].append(vloss)\n        exp[\"metrics\"][\"train\"].append(tbwa)\n        exp[\"metrics\"][\"val\"].append(vbwa)\n        exp[\"timestamps\"].append(time.time())\n        print(\n            f\"[{tag}] Ep{ep}: train_loss {tloss:.4f} val_loss {vloss:.4f} \"\n            f\"BWA {vbwa:.4f} (CWA {vcwa:.4f}, SWA {vswa:.4f})\"\n        )\n\n    # final test run\n    tl, bwa, cwa, swa, pr, gt = evaluate(model, test_loader)\n    print(f\"[{tag}] Test -> loss {tl:.4f}  BWA {bwa:.4f}\")\n    exp[\"predictions\"] = pr\n    exp[\"ground_truth\"] = gt\n    experiment_data[\"weight_decay\"][tag] = exp\n\n    # optional plot per run\n    e = np.arange(1, num_epochs + 1)\n    plt.figure()\n    plt.plot(e, exp[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(e, exp[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.title(f\"BWA vs epochs ({tag})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.legend()\n    pth = os.path.join(working_dir, f\"bwa_curve_{tag}.png\")\n    plt.tight_layout()\n    plt.savefig(pth)\n    plt.close()\n    print(\"Curve saved to\", pth)\n\n# ---------------------------------------------------------------------\n# persist -------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# mandatory device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Helper to locate the dataset -----------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Place it in the current / parent dir \"\n        \"or set SPR_DATA_PATH env variable.\"\n    )\n\n\n# ---------------------------------------------------------------------\n# ---------- Provided helpers (unchanged aside from path logic) -------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\n# ---------------------------------------------------------------------\n# ---------- Load dataset ---------------------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------------------------------------------------------------\n# ---------- Build vocabularies ---------------------------------------\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\n\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\n# ---------------------------------------------------------------------\n# ---------- Graph construction ---------------------------------------\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_indices = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_indices, dtype=torch.long).unsqueeze(-1)\n\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq  # keep original for metrics\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader_full = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# ---------- Model -----------------------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\n# ---------------------------------------------------------------------\n# ---------- Evaluation function --------------------------------------\ndef evaluate(model, loader, criterion):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            total_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    avg_loss = total_loss / len(loader.dataset)\n    return avg_loss, bwa, cwa, swa, all_preds, all_labels\n\n\n# ---------------------------------------------------------------------\n# ---------- Hyperparameter tuning over embed_dim ---------------------\nembed_dims = [16, 32, 64, 128]\nnum_epochs = 5  # keep short for quick sweep\n\nexperiment_data = {\"embed_dim\": {\"SPR_BENCH\": {}}}\n\nbest_dev_bwa, best_dim, best_state = -1, None, None\n\nfor dim in embed_dims:\n    print(f\"\\n=== Training with embed_dim={dim} ===\")\n    # prepare loaders (reshuffle each run for fairness)\n    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n    model = SPR_GCN(len(token2idx), dim, len(label2idx)).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # logs for this dim\n    dim_logs = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(\n            model, dev_loader, criterion\n        )\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader, criterion)\n\n        dim_logs[\"losses\"][\"train\"].append(train_loss)\n        dim_logs[\"losses\"][\"val\"].append(val_loss)\n        dim_logs[\"metrics\"][\"train\"].append(train_bwa)\n        dim_logs[\"metrics\"][\"val\"].append(val_bwa)\n        dim_logs[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n            f\"val_loss={val_loss:.4f}  BWA={val_bwa:.4f} \"\n            f\"(CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n    # dev selection\n    final_dev_bwa = dim_logs[\"metrics\"][\"val\"][-1]\n    if final_dev_bwa > best_dev_bwa:\n        best_dev_bwa = final_dev_bwa\n        best_dim = dim\n        best_state = model.state_dict()\n\n    # store logs\n    experiment_data[\"embed_dim\"][\"SPR_BENCH\"][f\"dim_{dim}\"] = dim_logs\n    torch.cuda.empty_cache()\n\nprint(f\"\\nBest embed_dim according to dev BWA: {best_dim} ({best_dev_bwa:.4f})\")\n\n# ---------------------------------------------------------------------\n# ---------- Evaluate best model on test set --------------------------\nbest_model = SPR_GCN(len(token2idx), best_dim, len(label2idx)).to(device)\nbest_model.load_state_dict(best_state)\ncriterion = nn.CrossEntropyLoss()\n\ntest_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n    best_model, test_loader, criterion\n)\nprint(\n    f\"Final Test -> loss: {test_loss:.4f}  BWA: {test_bwa:.4f}  \"\n    f\"CWA: {test_cwa:.4f}  SWA: {test_swa:.4f}\"\n)\n\n# store predictions for best model\nexperiment_data[\"embed_dim\"][\"SPR_BENCH\"][f\"dim_{best_dim}\"][\"predictions\"] = test_preds\nexperiment_data[\"embed_dim\"][\"SPR_BENCH\"][f\"dim_{best_dim}\"][\n    \"ground_truth\"\n] = test_labels\nexperiment_data[\"embed_dim\"][\"SPR_BENCH\"][f\"dim_{best_dim}\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"BWA\": test_bwa,\n    \"CWA\": test_cwa,\n    \"SWA\": test_swa,\n}\n\n# ---------------------------------------------------------------------\n# ---------- Plot BWA curves for each embed_dim -----------------------\nplt.figure()\nfor dim in embed_dims:\n    bwa_vals = experiment_data[\"embed_dim\"][\"SPR_BENCH\"][f\"dim_{dim}\"][\"metrics\"][\"val\"]\n    plt.plot(range(1, len(bwa_vals) + 1), bwa_vals, label=f\"dim={dim}\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Dev BWA\")\nplt.title(\"Dev BWA vs Epoch for different embed_dim\")\nplt.legend()\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"bwa_curve_embed_dim.png\")\nplt.savefig(plot_path)\nprint(f\"Curve saved to {plot_path}\")\n\n# ---------------------------------------------------------------------\n# ---------- Persist experiment data ----------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# mandatory device handling\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Helper to locate the dataset -----------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Provide it or set SPR_DATA_PATH.\"\n    )\n\n\n# ---------------------------------------------------------------------\n# ---------- Provided helpers (unchanged aside from path logic) -------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({k: _load(f\"{k}.csv\") for k in [\"train\", \"dev\", \"test\"]})\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\n# ---------------------------------------------------------------------\n# ---------- Load dataset and build vocabs ----------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\n\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\n# ---------------------------------------------------------------------\n# ---------- Graph construction ---------------------------------------\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_indices = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_indices, dtype=torch.long).unsqueeze(-1)\n\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# ---------- Model -----------------------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(\n        self, vocab_size: int, embed_dim: int, num_classes: int, dropout_p: float\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\n# ---------------------------------------------------------------------\n# ---------- Evaluation function --------------------------------------\ndef evaluate(model, loader, criterion):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            total_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(batch.seq)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    avg_loss = total_loss / len(loader.dataset)\n    return avg_loss, bwa, cwa, swa, all_preds, all_labels\n\n\n# ---------------------------------------------------------------------\n# ---------- Hyperparameter sweep -------------------------------------\ndropout_rates = [0.0, 0.1, 0.3, 0.5]\nnum_epochs = 8\n\nexperiment_data = {\"dropout_rate\": {}}\n\nfor dr in dropout_rates:\n    print(f\"\\n=== Training with dropout_rate={dr} ===\")\n    model = SPR_GCN(len(token2idx), 32, len(label2idx), dropout_p=dr).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    exp_dict = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(\n            model, dev_loader, criterion\n        )\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader, criterion)\n\n        exp_dict[\"losses\"][\"train\"].append(train_loss)\n        exp_dict[\"losses\"][\"val\"].append(val_loss)\n        exp_dict[\"metrics\"][\"train\"].append(train_bwa)\n        exp_dict[\"metrics\"][\"val\"].append(val_bwa)\n        exp_dict[\"timestamps\"].append(time.time())\n\n        print(\n            f\"  Epoch {epoch:02d}: train_loss={train_loss:.4f}  \"\n            f\"val_loss={val_loss:.4f}  BWA={val_bwa:.4f} \"\n            f\"(CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n    # Final test evaluation\n    test_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n        model, test_loader, criterion\n    )\n    print(\n        f\"  >> Test -> loss: {test_loss:.4f}  BWA: {test_bwa:.4f}  \"\n        f\"CWA: {test_cwa:.4f}  SWA: {test_swa:.4f}\"\n    )\n\n    exp_dict[\"predictions\"] = test_preds\n    exp_dict[\"ground_truth\"] = test_labels\n    exp_dict[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"BWA\": test_bwa,\n        \"CWA\": test_cwa,\n        \"SWA\": test_swa,\n    }\n\n    # Store under main dictionary\n    if \"SPR_BENCH\" not in experiment_data[\"dropout_rate\"]:\n        experiment_data[\"dropout_rate\"][\"SPR_BENCH\"] = {}\n    experiment_data[\"dropout_rate\"][\"SPR_BENCH\"][str(dr)] = exp_dict\n\n    # Plot training curves for this dropout\n    epochs = np.arange(1, num_epochs + 1)\n    plt.figure()\n    plt.plot(epochs, exp_dict[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(epochs, exp_dict[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(f\"Dropout {dr} - BWA over epochs\")\n    plt.legend()\n    plt.tight_layout()\n    plot_path = os.path.join(working_dir, f\"bwa_curve_dropout_{dr}.png\")\n    plt.savefig(plot_path)\n    plt.close()\n    print(f\"  Curve saved to {plot_path}\")\n\n# ---------------------------------------------------------------------\n# ---------- Save all experiment data ---------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f\"\\nAll experiment data saved to {os.path.join(working_dir, 'experiment_data.npy')}\"\n)\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- working dir & device ---------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------- locate + load SPR-BENCH ------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    cands = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in cands:\n        if p and (p / \"train.csv\").exists():\n            print(\"Found SPR_BENCH at\", p.resolve())\n            return p.resolve()\n    raise FileNotFoundError(\"Place SPR_BENCH or set SPR_DATA_PATH\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\n# -------------------- metrics helpers --------------------------------\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wt for wt, yt, yp in zip(w, y_true, y_pred) if yt == yp) / (sum(w) or 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wt for wt, yt, yp in zip(w, y_true, y_pred) if yt == yp) / (sum(w) or 1)\n\n\n# -------------------- vocab build ------------------------------------\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\n# -------------------- graph construction -----------------------------\ndef seq_to_data(example):\n    tokens = extract_tokens(example[\"sequence\"])\n    x = torch.tensor([token2idx.get(t, 0) for t in tokens], dtype=torch.long).unsqueeze(\n        -1\n    )\n    if len(tokens) > 1:\n        src = torch.arange(len(tokens) - 1)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])])\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2idx[example[\"label\"]]])\n    d = Data(x=x, edge_index=edge_index, y=y)\n    d.seq = example[\"sequence\"]\n    return d\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size)\n\n\n# -------------------- GCN model factory ------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab, embed_dim, num_classes, num_layers=2, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab + 1, embed_dim, padding_idx=0)\n        self.convs = nn.ModuleList()\n        self.convs.append(GCNConv(embed_dim, hidden))\n        for _ in range(num_layers - 1):\n            self.convs.append(GCNConv(hidden, hidden))\n        self.lin = nn.Linear(hidden, num_classes)\n        self.drop = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        for conv in self.convs:\n            x = conv(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.drop(x)\n        return self.lin(x)\n\n\n# -------------------- evaluation util --------------------------------\ndef evaluate(model, loader, criterion):\n    model.eval()\n    tot_loss = 0\n    preds = []\n    labels = []\n    seqs = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            tot_loss += loss.item() * batch.num_graphs\n            preds.extend(out.argmax(-1).cpu().tolist())\n            labels.extend(batch.y.view(-1).cpu().tolist())\n            seqs.extend(batch.seq)\n    cwa = color_weighted_accuracy(seqs, labels, preds)\n    swa = shape_weighted_accuracy(seqs, labels, preds)\n    bwa = (cwa + swa) / 2\n    return tot_loss / len(loader.dataset), bwa, cwa, swa, preds, labels\n\n\n# -------------------- hyper-parameter tuning loop --------------------\nlayers_opts = [1, 2, 3, 4]\nnum_epochs = 5\nexperiment_data = {\"num_gcn_layers\": {\"SPR_BENCH\": {}}}\n\nfor num_layers in layers_opts:\n    key = f\"layers_{num_layers}\"\n    print(f\"\\n=== Training model with {num_layers} GCN layers ===\")\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    model = SPR_GCN(len(token2idx), 32, len(label2idx), num_layers=num_layers).to(\n        device\n    )\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    best_val_bwa = 0.0\n    best_state = None\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        epoch_loss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(\n            model, dev_loader, criterion\n        )\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader, criterion)\n\n        exp[\"losses\"][\"train\"].append(train_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"train\"].append(train_bwa)\n        exp[\"metrics\"][\"val\"].append(val_bwa)\n        exp[\"timestamps\"].append(time.time())\n\n        print(\n            f\"Epoch {epoch} | TrainLoss {train_loss:.4f}  ValLoss {val_loss:.4f} \"\n            f\"BWA {val_bwa:.4f} (C {val_cwa:.4f}, S {val_swa:.4f})\"\n        )\n\n        if val_bwa > best_val_bwa:\n            best_val_bwa = val_bwa\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n\n    # reload best model and evaluate on test\n    if best_state:\n        model.load_state_dict(best_state)\n    test_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n        model, test_loader, criterion\n    )\n    print(f\"Best Dev BWA {best_val_bwa:.4f} | Test BWA {test_bwa:.4f}\")\n    exp[\"predictions\"] = test_preds\n    exp[\"ground_truth\"] = test_labels\n    exp[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"BWA\": test_bwa,\n        \"CWA\": test_cwa,\n        \"SWA\": test_swa,\n    }\n    experiment_data[\"num_gcn_layers\"][\"SPR_BENCH\"][key] = exp\n\n    torch.cuda.empty_cache()\n\n# -------------------- save experiment data ---------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# -------------------- plotting ---------------------------------------\nplt.figure()\nfor key in experiment_data[\"num_gcn_layers\"][\"SPR_BENCH\"]:\n    val = experiment_data[\"num_gcn_layers\"][\"SPR_BENCH\"][key][\"metrics\"][\"val\"]\n    plt.plot(range(1, len(val) + 1), val, label=key)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Dev BWA\")\nplt.title(\"GCN depth tuning\")\nplt.legend()\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"bwa_curve_spr.png\")\nplt.savefig(plot_path)\nprint(\"Curve saved to\", plot_path)\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# locate SPR_BENCH\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in [\"train\", \"dev\", \"test\"]})\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)) / (\n        sum(w) or 1\n    )\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)) / (\n        sum(w) or 1\n    )\n\n\n# ------------------------------------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\n# ------------------------------------------------------------\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\n# ------------------------------------------------------------\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_idx = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_idx, dtype=torch.long).unsqueeze(-1)\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])])\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size)\n\n\n# ------------------------------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(\n        self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, hidden_dim)\n        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n        self.lin = nn.Linear(hidden_dim, num_classes)\n        self.drop = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.drop(x)\n        return self.lin(x)\n\n\n# ------------------------------------------------------------\ndef evaluate(model, loader, criterion):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    loss_sum = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss_sum += loss.item() * batch.num_graphs\n            preds = out.argmax(-1).cpu().tolist()\n            labs = batch.y.view(-1).cpu().tolist()\n            all_preds += preds\n            all_labels += labs\n            all_seqs += batch.seq\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    return loss_sum / len(loader.dataset), bwa, cwa, swa, all_preds, all_labels\n\n\n# ------------------------------------------------------------\nhidden_dim_grid = [32, 64, 128, 256]\nnum_epochs = 5\n\nexperiment_data = {\n    \"gcn_hidden_dim\": {\n        \"SPR_BENCH\": {\n            \"hidden_dims\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"timestamps\": [],\n        }\n    }\n}\n\nbest_bwa, best_dim, best_model_state = -1, None, None\nhistory_best_curve = {\"train\": [], \"val\": []}\n\nfor hd in hidden_dim_grid:\n    print(f\"\\n=== Training with hidden_dim={hd} ===\")\n    model = SPR_GCN(len(token2idx), 32, hd, len(label2idx)).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    train_curve, val_curve = [], []\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n        val_loss, val_bwa, val_cwa, val_swa, *_ = evaluate(model, dev_loader, criterion)\n        train_bwa, *_ = evaluate(model, train_loader, criterion)[1:]\n\n        train_curve.append(train_bwa)\n        val_curve.append(val_bwa)\n\n        print(\n            f\"Epoch {epoch}/{num_epochs} - train_loss {train_loss:.4f} \"\n            f\"val_loss {val_loss:.4f}  val_BWA {val_bwa:.4f}\"\n        )\n\n    # record\n    exp = experiment_data[\"gcn_hidden_dim\"][\"SPR_BENCH\"]\n    exp[\"hidden_dims\"].append(hd)\n    exp[\"metrics\"][\"train\"].append(train_curve[-1])\n    exp[\"metrics\"][\"val\"].append(val_curve[-1])\n    exp[\"losses\"][\"train\"].append(train_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"timestamps\"].append(time.time())\n\n    # keep best\n    if val_curve[-1] > best_bwa:\n        best_bwa, best_dim = val_curve[-1], hd\n        best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        history_best_curve[\"train\"] = train_curve\n        history_best_curve[\"val\"] = val_curve\n\n# ------------------------------------------------------------\nprint(f\"\\nBest hidden_dim = {best_dim} with dev BWA = {best_bwa:.4f}\")\nbest_model = SPR_GCN(len(token2idx), 32, best_dim, len(label2idx)).to(device)\nbest_model.load_state_dict(best_model_state)\n\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n    best_model, test_loader, criterion\n)\nprint(\n    f\"TEST -> loss {test_loss:.4f}  BWA {test_bwa:.4f}  CWA {test_cwa:.4f}  SWA {test_swa:.4f}\"\n)\n\n# store predictions / ground truth for best\nexp = experiment_data[\"gcn_hidden_dim\"][\"SPR_BENCH\"]\nexp[\"predictions\"] = test_preds\nexp[\"ground_truth\"] = test_labels\n\n# ------------------------------------------------------------\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ------------------------------------------------------------\n# Plot best curve\nepochs = np.arange(1, num_epochs + 1)\nplt.figure()\nplt.plot(epochs, history_best_curve[\"train\"], label=\"Train BWA\")\nplt.plot(epochs, history_best_curve[\"val\"], label=\"Dev BWA\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BWA\")\nplt.title(f\"BWA curve (hidden_dim={best_dim})\")\nplt.legend()\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"bwa_curve_best_hidden_dim.png\")\nplt.savefig(plot_path)\nprint(f\"Curve saved to {plot_path}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, copy, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Helper to locate the dataset -----------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Place the folder appropriately or set SPR_DATA_PATH.\"\n    )\n\n\n# ---------------------------------------------------------------------\n# ---------- Provided helpers -----------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        dset[sp] = _load(f\"{sp}.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\n# ---------------------------------------------------------------------\n# ---------- Load and preprocess dataset ------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\n\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_indices = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_indices, dtype=torch.long).unsqueeze(-1)\n\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# ---------- Model Definition -----------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\n# ---------------------------------------------------------------------\n# ---------- Evaluation helper ----------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            total_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    return total_loss / len(loader.dataset), bwa, cwa, swa, all_preds, all_labels\n\n\n# ---------------------------------------------------------------------\n# ---------- Hyperparameter tuning over num_epochs --------------------\ncandidate_max_epochs = [10, 20, 30, 40, 50]\npatience = 5\n\nexperiment_data = {\"num_epochs\": {}}\n\nfor max_ep in candidate_max_epochs:\n    run_key = f\"max_{max_ep}\"\n    experiment_data[\"num_epochs\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_bwa, best_state, wait = -1.0, None, 0\n\n    for epoch in range(1, max_ep + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(model, dev_loader)\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader)\n\n        # log\n        exd = experiment_data[\"num_epochs\"][run_key]\n        exd[\"losses\"][\"train\"].append(train_loss)\n        exd[\"losses\"][\"val\"].append(val_loss)\n        exd[\"metrics\"][\"train\"].append(train_bwa)\n        exd[\"metrics\"][\"val\"].append(val_bwa)\n        exd[\"timestamps\"].append(time.time())\n\n        print(\n            f\"[{run_key}] Epoch {epoch:3d}/{max_ep}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"BWA={val_bwa:.4f} (CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n        # early stopping\n        if val_bwa > best_bwa:\n            best_bwa, best_state, wait = val_bwa, copy.deepcopy(model.state_dict()), 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(f\"[{run_key}] Early stopping triggered at epoch {epoch}.\")\n                break\n\n    # restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # final test evaluation\n    test_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n        model, test_loader\n    )\n    exd[\"predictions\"] = test_preds\n    exd[\"ground_truth\"] = test_labels\n    exd[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"BWA\": test_bwa,\n        \"CWA\": test_cwa,\n        \"SWA\": test_swa,\n    }\n    print(\n        f\"[{run_key}] Test -> loss: {test_loss:.4f}  \"\n        f\"BWA: {test_bwa:.4f} (CWA={test_cwa:.4f} SWA={test_swa:.4f})\"\n    )\n\n    # -----------------------------------------------------------------\n    # plotting for this run\n    epochs_arr = np.arange(1, len(exd[\"metrics\"][\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs_arr, exd[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(epochs_arr, exd[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(f\"BWA curve ({run_key})\")\n    plt.legend()\n    plt.tight_layout()\n    plot_path = os.path.join(working_dir, f\"bwa_curve_{run_key}.png\")\n    plt.savefig(plot_path)\n    plt.close()\n    print(f\"[{run_key}] Curve saved to {plot_path}\")\n\n    # free gpu memory between runs\n    del model\n    torch.cuda.empty_cache()\n\n# ---------------------------------------------------------------------\n# ---------- Save experiment data -------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All experiment data saved to working/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, copy, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Helper to locate the dataset -----------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Place the folder appropriately or set SPR_DATA_PATH.\"\n    )\n\n\n# ---------------------------------------------------------------------\n# ---------- Provided helpers -----------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        dset[sp] = _load(f\"{sp}.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\n# ---------------------------------------------------------------------\n# ---------- Load and preprocess dataset ------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\n\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_indices = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_indices, dtype=torch.long).unsqueeze(-1)\n\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# ---------- Model Definition -----------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\n# ---------------------------------------------------------------------\n# ---------- Evaluation helper ----------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            total_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    return total_loss / len(loader.dataset), bwa, cwa, swa, all_preds, all_labels\n\n\n# ---------------------------------------------------------------------\n# ---------- Hyperparameter tuning over num_epochs --------------------\ncandidate_max_epochs = [10, 20, 30, 40, 50]\npatience = 5\n\nexperiment_data = {\"num_epochs\": {}}\n\nfor max_ep in candidate_max_epochs:\n    run_key = f\"max_{max_ep}\"\n    experiment_data[\"num_epochs\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_bwa, best_state, wait = -1.0, None, 0\n\n    for epoch in range(1, max_ep + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(model, dev_loader)\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader)\n\n        # log\n        exd = experiment_data[\"num_epochs\"][run_key]\n        exd[\"losses\"][\"train\"].append(train_loss)\n        exd[\"losses\"][\"val\"].append(val_loss)\n        exd[\"metrics\"][\"train\"].append(train_bwa)\n        exd[\"metrics\"][\"val\"].append(val_bwa)\n        exd[\"timestamps\"].append(time.time())\n\n        print(\n            f\"[{run_key}] Epoch {epoch:3d}/{max_ep}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"BWA={val_bwa:.4f} (CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n        # early stopping\n        if val_bwa > best_bwa:\n            best_bwa, best_state, wait = val_bwa, copy.deepcopy(model.state_dict()), 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(f\"[{run_key}] Early stopping triggered at epoch {epoch}.\")\n                break\n\n    # restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # final test evaluation\n    test_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n        model, test_loader\n    )\n    exd[\"predictions\"] = test_preds\n    exd[\"ground_truth\"] = test_labels\n    exd[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"BWA\": test_bwa,\n        \"CWA\": test_cwa,\n        \"SWA\": test_swa,\n    }\n    print(\n        f\"[{run_key}] Test -> loss: {test_loss:.4f}  \"\n        f\"BWA: {test_bwa:.4f} (CWA={test_cwa:.4f} SWA={test_swa:.4f})\"\n    )\n\n    # -----------------------------------------------------------------\n    # plotting for this run\n    epochs_arr = np.arange(1, len(exd[\"metrics\"][\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs_arr, exd[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(epochs_arr, exd[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(f\"BWA curve ({run_key})\")\n    plt.legend()\n    plt.tight_layout()\n    plot_path = os.path.join(working_dir, f\"bwa_curve_{run_key}.png\")\n    plt.savefig(plot_path)\n    plt.close()\n    print(f\"[{run_key}] Curve saved to {plot_path}\")\n\n    # free gpu memory between runs\n    del model\n    torch.cuda.empty_cache()\n\n# ---------------------------------------------------------------------\n# ---------- Save experiment data -------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All experiment data saved to working/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, copy, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom typing import List\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Helper to locate the dataset -----------------------------\ndef locate_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n        pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"\")),\n    ]\n    for p in candidates:\n        if p and (p / \"train.csv\").exists() and (p / \"dev.csv\").exists():\n            print(f\"Found SPR_BENCH at: {p.resolve()}\")\n            return p.resolve()\n    raise FileNotFoundError(\n        \"SPR_BENCH dataset not found. Place the folder appropriately or set SPR_DATA_PATH.\"\n    )\n\n\n# ---------------------------------------------------------------------\n# ---------- Provided helpers -----------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        dset[sp] = _load(f\"{sp}.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wt if yt == yp else 0 for wt, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / (sum(w) if sum(w) > 0 else 1)\n\n\n# ---------------------------------------------------------------------\n# ---------- Load and preprocess dataset ------------------------------\nDATA_PATH = locate_spr_bench()\nspr = load_spr_bench(DATA_PATH)\n\n\ndef extract_tokens(seq: str) -> List[str]:\n    return seq.strip().split()\n\n\ntoken_set, label_set = set(), set()\nfor ex in spr[\"train\"]:\n    token_set.update(extract_tokens(ex[\"sequence\"]))\n    label_set.add(ex[\"label\"])\n\ntoken2idx = {tok: i + 1 for i, tok in enumerate(sorted(token_set))}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(label_set))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\n\ndef seq_to_data(example):\n    seq = example[\"sequence\"]\n    tokens = extract_tokens(seq)\n    node_indices = [token2idx.get(tok, 0) for tok in tokens]\n    x = torch.tensor(node_indices, dtype=torch.long).unsqueeze(-1)\n\n    if len(tokens) > 1:\n        src = torch.arange(0, len(tokens) - 1, dtype=torch.long)\n        dst = src + 1\n        edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n    else:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n\n    y = torch.tensor([label2idx[example[\"label\"]]], dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, y=y)\n    data.seq = seq\n    return data\n\n\ntrain_graphs = [seq_to_data(ex) for ex in spr[\"train\"]]\ndev_graphs = [seq_to_data(ex) for ex in spr[\"dev\"]]\ntest_graphs = [seq_to_data(ex) for ex in spr[\"test\"]]\n\nbatch_size = 64\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_graphs, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n\n\n# ---------------------------------------------------------------------\n# ---------- Model Definition -----------------------------------------\nclass SPR_GCN(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n        self.gcn1 = GCNConv(embed_dim, 64)\n        self.gcn2 = GCNConv(64, 64)\n        self.lin = nn.Linear(64, num_classes)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, edge_index, batch):\n        x = self.embed(x.squeeze(-1))\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = global_mean_pool(x, batch)\n        x = self.dropout(x)\n        return self.lin(x)\n\n\n# ---------------------------------------------------------------------\n# ---------- Evaluation helper ----------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, all_preds, all_labels, all_seqs = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            total_loss += loss.item() * batch.num_graphs\n            preds = out.argmax(dim=-1).cpu().tolist()\n            labels = batch.y.view(-1).cpu().tolist()\n            seqs = batch.seq\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    bwa = (cwa + swa) / 2.0\n    return total_loss / len(loader.dataset), bwa, cwa, swa, all_preds, all_labels\n\n\n# ---------------------------------------------------------------------\n# ---------- Hyperparameter tuning over num_epochs --------------------\ncandidate_max_epochs = [10, 20, 30, 40, 50]\npatience = 5\n\nexperiment_data = {\"num_epochs\": {}}\n\nfor max_ep in candidate_max_epochs:\n    run_key = f\"max_{max_ep}\"\n    experiment_data[\"num_epochs\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n\n    model = SPR_GCN(len(token2idx), 32, len(label2idx)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_bwa, best_state, wait = -1.0, None, 0\n\n    for epoch in range(1, max_ep + 1):\n        model.train()\n        epoch_loss = 0.0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            out = model(batch.x, batch.edge_index, batch.batch)\n            loss = criterion(out, batch.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * batch.num_graphs\n        train_loss = epoch_loss / len(train_loader.dataset)\n\n        val_loss, val_bwa, val_cwa, val_swa, _, _ = evaluate(model, dev_loader)\n        _, train_bwa, _, _, _, _ = evaluate(model, train_loader)\n\n        # log\n        exd = experiment_data[\"num_epochs\"][run_key]\n        exd[\"losses\"][\"train\"].append(train_loss)\n        exd[\"losses\"][\"val\"].append(val_loss)\n        exd[\"metrics\"][\"train\"].append(train_bwa)\n        exd[\"metrics\"][\"val\"].append(val_bwa)\n        exd[\"timestamps\"].append(time.time())\n\n        print(\n            f\"[{run_key}] Epoch {epoch:3d}/{max_ep}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"BWA={val_bwa:.4f} (CWA={val_cwa:.4f}, SWA={val_swa:.4f})\"\n        )\n\n        # early stopping\n        if val_bwa > best_bwa:\n            best_bwa, best_state, wait = val_bwa, copy.deepcopy(model.state_dict()), 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(f\"[{run_key}] Early stopping triggered at epoch {epoch}.\")\n                break\n\n    # restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # final test evaluation\n    test_loss, test_bwa, test_cwa, test_swa, test_preds, test_labels = evaluate(\n        model, test_loader\n    )\n    exd[\"predictions\"] = test_preds\n    exd[\"ground_truth\"] = test_labels\n    exd[\"test_metrics\"] = {\n        \"loss\": test_loss,\n        \"BWA\": test_bwa,\n        \"CWA\": test_cwa,\n        \"SWA\": test_swa,\n    }\n    print(\n        f\"[{run_key}] Test -> loss: {test_loss:.4f}  \"\n        f\"BWA: {test_bwa:.4f} (CWA={test_cwa:.4f} SWA={test_swa:.4f})\"\n    )\n\n    # -----------------------------------------------------------------\n    # plotting for this run\n    epochs_arr = np.arange(1, len(exd[\"metrics\"][\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs_arr, exd[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(epochs_arr, exd[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(f\"BWA curve ({run_key})\")\n    plt.legend()\n    plt.tight_layout()\n    plot_path = os.path.join(working_dir, f\"bwa_curve_{run_key}.png\")\n    plt.savefig(plot_path)\n    plt.close()\n    print(f\"[{run_key}] Curve saved to {plot_path}\")\n\n    # free gpu memory between runs\n    del model\n    torch.cuda.empty_cache()\n\n# ---------------------------------------------------------------------\n# ---------- Save experiment data -------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All experiment data saved to working/experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 488977.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 620018.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 717073.10\nexamples/s]', '\\n', '[max_10] Epoch   1/10: train_loss=0.5347 val_loss=0.4714\nBWA=0.7964 (CWA=0.7939, SWA=0.7989)', '\\n', '[max_10] Epoch   2/10:\ntrain_loss=0.4395 val_loss=0.4109 BWA=0.8236 (CWA=0.8239, SWA=0.8232)', '\\n',\n'[max_10] Epoch   3/10: train_loss=0.3938 val_loss=0.3752 BWA=0.8489\n(CWA=0.8491, SWA=0.8487)', '\\n', '[max_10] Epoch   4/10: train_loss=0.3671\nval_loss=0.3571 BWA=0.8598 (CWA=0.8602, SWA=0.8593)', '\\n', '[max_10] Epoch\n5/10: train_loss=0.3509 val_loss=0.3532 BWA=0.8540 (CWA=0.8552, SWA=0.8527)',\n'\\n', '[max_10] Epoch   6/10: train_loss=0.3351 val_loss=0.3237 BWA=0.8830\n(CWA=0.8832, SWA=0.8829)', '\\n', '[max_10] Epoch   7/10: train_loss=0.3185\nval_loss=0.3104 BWA=0.8943 (CWA=0.8939, SWA=0.8947)', '\\n', '[max_10] Epoch\n8/10: train_loss=0.3111 val_loss=0.3045 BWA=0.8937 (CWA=0.8946, SWA=0.8927)',\n'\\n', '[max_10] Epoch   9/10: train_loss=0.2984 val_loss=0.3011 BWA=0.9048\n(CWA=0.9042, SWA=0.9054)', '\\n', '[max_10] Epoch  10/10: train_loss=0.2928\nval_loss=0.2822 BWA=0.9068 (CWA=0.9066, SWA=0.9070)', '\\n', '[max_10] Test ->\nloss: 0.9122  BWA: 0.6403 (CWA=0.6607 SWA=0.6200)', '\\n', '[max_10] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n10/working/bwa_curve_max_10.png', '\\n', '[max_20] Epoch   1/20:\ntrain_loss=0.5193 val_loss=0.4430 BWA=0.8107 (CWA=0.8095, SWA=0.8119)', '\\n',\n'[max_20] Epoch   2/20: train_loss=0.4184 val_loss=0.4046 BWA=0.8358\n(CWA=0.8348, SWA=0.8368)', '\\n', '[max_20] Epoch   3/20: train_loss=0.3899\nval_loss=0.4017 BWA=0.8216 (CWA=0.8215, SWA=0.8217)', '\\n', '[max_20] Epoch\n4/20: train_loss=0.3724 val_loss=0.3605 BWA=0.8643 (CWA=0.8641, SWA=0.8646)',\n'\\n', '[max_20] Epoch   5/20: train_loss=0.3528 val_loss=0.3383 BWA=0.8739\n(CWA=0.8733, SWA=0.8744)', '\\n', '[max_20] Epoch   6/20: train_loss=0.3325\nval_loss=0.3163 BWA=0.8909 (CWA=0.8906, SWA=0.8912)', '\\n', '[max_20] Epoch\n7/20: train_loss=0.3155 val_loss=0.3102 BWA=0.8923 (CWA=0.8923, SWA=0.8924)',\n'\\n', '[max_20] Epoch   8/20: train_loss=0.3047 val_loss=0.2866 BWA=0.9034\n(CWA=0.9039, SWA=0.9030)', '\\n', '[max_20] Epoch   9/20: train_loss=0.2915\nval_loss=0.2819 BWA=0.9059 (CWA=0.9065, SWA=0.9052)', '\\n', '[max_20] Epoch\n10/20: train_loss=0.2832 val_loss=0.3080 BWA=0.8992 (CWA=0.8985, SWA=0.9000)',\n'\\n', '[max_20] Epoch  11/20: train_loss=0.2805 val_loss=0.2612 BWA=0.9193\n(CWA=0.9196, SWA=0.9190)', '\\n', '[max_20] Epoch  12/20: train_loss=0.2719\nval_loss=0.2598 BWA=0.9190 (CWA=0.9189, SWA=0.9192)', '\\n', '[max_20] Epoch\n13/20: train_loss=0.2672 val_loss=0.2551 BWA=0.9168 (CWA=0.9173, SWA=0.9163)',\n'\\n', '[max_20] Epoch  14/20: train_loss=0.2607 val_loss=0.2517 BWA=0.9219\n(CWA=0.9216, SWA=0.9222)', '\\n', '[max_20] Epoch  15/20: train_loss=0.2535\nval_loss=0.2486 BWA=0.9235 (CWA=0.9240, SWA=0.9230)', '\\n', '[max_20] Epoch\n16/20: train_loss=0.2524 val_loss=0.2408 BWA=0.9207 (CWA=0.9214, SWA=0.9201)',\n'\\n', '[max_20] Epoch  17/20: train_loss=0.2507 val_loss=0.2409 BWA=0.9220\n(CWA=0.9226, SWA=0.9214)', '\\n', '[max_20] Epoch  18/20: train_loss=0.2435\nval_loss=0.2369 BWA=0.9241 (CWA=0.9237, SWA=0.9244)', '\\n', '[max_20] Epoch\n19/20: train_loss=0.2397 val_loss=0.2258 BWA=0.9337 (CWA=0.9338, SWA=0.9336)',\n'\\n', '[max_20] Epoch  20/20: train_loss=0.2371 val_loss=0.2231 BWA=0.9322\n(CWA=0.9323, SWA=0.9321)', '\\n', '[max_20] Test -> loss: 1.1232  BWA: 0.6552\n(CWA=0.6766 SWA=0.6339)', '\\n', '[max_20] Curve saved to /home/zxl240011/AI-Scie\nntist-v2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-10/working/bwa_curve_max_20.png', '\\n', '[max_30] Epoch\n1/30: train_loss=0.5252 val_loss=0.4527 BWA=0.7935 (CWA=0.7930, SWA=0.7940)',\n'\\n', '[max_30] Epoch   2/30: train_loss=0.4280 val_loss=0.3980 BWA=0.8346\n(CWA=0.8341, SWA=0.8350)', '\\n', '[max_30] Epoch   3/30: train_loss=0.3916\nval_loss=0.3758 BWA=0.8551 (CWA=0.8552, SWA=0.8550)', '\\n', '[max_30] Epoch\n4/30: train_loss=0.3693 val_loss=0.3626 BWA=0.8574 (CWA=0.8578, SWA=0.8571)',\n'\\n', '[max_30] Epoch   5/30: train_loss=0.3503 val_loss=0.3379 BWA=0.8765\n(CWA=0.8762, SWA=0.8769)', '\\n', '[max_30] Epoch   6/30: train_loss=0.3334\nval_loss=0.3227 BWA=0.8823 (CWA=0.8822, SWA=0.8823)', '\\n', '[max_30] Epoch\n7/30: train_loss=0.3189 val_loss=0.3192 BWA=0.8857 (CWA=0.8857, SWA=0.8856)',\n'\\n', '[max_30] Epoch   8/30: train_loss=0.3068 val_loss=0.3082 BWA=0.8943\n(CWA=0.8941, SWA=0.8944)', '\\n', '[max_30] Epoch   9/30: train_loss=0.2943\nval_loss=0.2900 BWA=0.9069 (CWA=0.9062, SWA=0.9076)', '\\n', '[max_30] Epoch\n10/30: train_loss=0.2859 val_loss=0.2751 BWA=0.9091 (CWA=0.9089, SWA=0.9094)',\n'\\n', '[max_30] Epoch  11/30: train_loss=0.2817 val_loss=0.2677 BWA=0.9145\n(CWA=0.9143, SWA=0.9147)', '\\n', '[max_30] Epoch  12/30: train_loss=0.2690\nval_loss=0.2610 BWA=0.9107 (CWA=0.9104, SWA=0.9109)', '\\n', '[max_30] Epoch\n13/30: train_loss=0.2650 val_loss=0.2578 BWA=0.9098 (CWA=0.9099, SWA=0.9097)',\n'\\n', '[max_30] Epoch  14/30: train_loss=0.2590 val_loss=0.2523 BWA=0.9203\n(CWA=0.9204, SWA=0.9203)', '\\n', '[max_30] Epoch  15/30: train_loss=0.2536\nval_loss=0.2452 BWA=0.9240 (CWA=0.9239, SWA=0.9241)', '\\n', '[max_30] Epoch\n16/30: train_loss=0.2484 val_loss=0.2394 BWA=0.9254 (CWA=0.9254, SWA=0.9254)',\n'\\n', '[max_30] Epoch  17/30: train_loss=0.2491 val_loss=0.2385 BWA=0.9240\n(CWA=0.9240, SWA=0.9240)', '\\n', '[max_30] Epoch  18/30: train_loss=0.2415\nval_loss=0.2315 BWA=0.9262 (CWA=0.9265, SWA=0.9259)', '\\n', '[max_30] Epoch\n19/30: train_loss=0.2396 val_loss=0.2423 BWA=0.9206 (CWA=0.9212, SWA=0.9199)',\n'\\n', '[max_30] Epoch  20/30: train_loss=0.2397 val_loss=0.2367 BWA=0.9171\n(CWA=0.9176, SWA=0.9166)', '\\n', '[max_30] Epoch  21/30: train_loss=0.2320\nval_loss=0.2260 BWA=0.9295 (CWA=0.9299, SWA=0.9291)', '\\n', '[max_30] Epoch\n22/30: train_loss=0.2299 val_loss=0.2227 BWA=0.9354 (CWA=0.9356, SWA=0.9352)',\n'\\n', '[max_30] Epoch  23/30: train_loss=0.2303 val_loss=0.2191 BWA=0.9307\n(CWA=0.9310, SWA=0.9305)', '\\n', '[max_30] Epoch  24/30: train_loss=0.2241\nval_loss=0.2154 BWA=0.9326 (CWA=0.9328, SWA=0.9323)', '\\n', '[max_30] Epoch\n25/30: train_loss=0.2225 val_loss=0.2107 BWA=0.9338 (CWA=0.9343, SWA=0.9333)',\n'\\n', '[max_30] Epoch  26/30: train_loss=0.2187 val_loss=0.2118 BWA=0.9384\n(CWA=0.9389, SWA=0.9379)', '\\n', '[max_30] Epoch  27/30: train_loss=0.2191\nval_loss=0.2104 BWA=0.9342 (CWA=0.9345, SWA=0.9340)', '\\n', '[max_30] Epoch\n28/30: train_loss=0.2154 val_loss=0.2113 BWA=0.9363 (CWA=0.9364, SWA=0.9361)',\n'\\n', '[max_30] Epoch  29/30: train_loss=0.2137 val_loss=0.2147 BWA=0.9298\n(CWA=0.9299, SWA=0.9296)', '\\n', '[max_30] Epoch  30/30: train_loss=0.2128\nval_loss=0.2109 BWA=0.9395 (CWA=0.9394, SWA=0.9397)', '\\n', '[max_30] Test ->\nloss: 1.2309  BWA: 0.6525 (CWA=0.6740 SWA=0.6311)', '\\n', '[max_30] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n10/working/bwa_curve_max_30.png', '\\n', '[max_40] Epoch   1/40:\ntrain_loss=0.5325 val_loss=0.4461 BWA=0.8048 (CWA=0.8040, SWA=0.8055)', '\\n',\n'[max_40] Epoch   2/40: train_loss=0.4217 val_loss=0.3998 BWA=0.8346\n(CWA=0.8348, SWA=0.8343)', '\\n', '[max_40] Epoch   3/40: train_loss=0.3897\nval_loss=0.3791 BWA=0.8461 (CWA=0.8462, SWA=0.8459)', '\\n', '[max_40] Epoch\n4/40: train_loss=0.3727 val_loss=0.3608 BWA=0.8659 (CWA=0.8651, SWA=0.8668)',\n'\\n', '[max_40] Epoch   5/40: train_loss=0.3548 val_loss=0.3453 BWA=0.8695\n(CWA=0.8691, SWA=0.8698)', '\\n', '[max_40] Epoch   6/40: train_loss=0.3387\nval_loss=0.3358 BWA=0.8709 (CWA=0.8706, SWA=0.8711)', '\\n', '[max_40] Epoch\n7/40: train_loss=0.3267 val_loss=0.3147 BWA=0.8941 (CWA=0.8934, SWA=0.8949)',\n'\\n', '[max_40] Epoch   8/40: train_loss=0.3114 val_loss=0.3011 BWA=0.8966\n(CWA=0.8961, SWA=0.8970)', '\\n', '[max_40] Epoch   9/40: train_loss=0.3026\nval_loss=0.2901 BWA=0.9044 (CWA=0.9039, SWA=0.9049)', '\\n', '[max_40] Epoch\n10/40: train_loss=0.2922 val_loss=0.2835 BWA=0.8954 (CWA=0.8963, SWA=0.8946)',\n'\\n', '[max_40] Epoch  11/40: train_loss=0.2825 val_loss=0.2754 BWA=0.9110\n(CWA=0.9108, SWA=0.9112)', '\\n', '[max_40] Epoch  12/40: train_loss=0.2755\nval_loss=0.2625 BWA=0.9147 (CWA=0.9149, SWA=0.9146)', '\\n', '[max_40] Epoch\n13/40: train_loss=0.2667 val_loss=0.2722 BWA=0.9131 (CWA=0.9128, SWA=0.9134)',\n'\\n', '[max_40] Epoch  14/40: train_loss=0.2635 val_loss=0.2501 BWA=0.9231\n(CWA=0.9233, SWA=0.9229)', '\\n', '[max_40] Epoch  15/40: train_loss=0.2570\nval_loss=0.2463 BWA=0.9240 (CWA=0.9236, SWA=0.9244)', '\\n', '[max_40] Epoch\n16/40: train_loss=0.2524 val_loss=0.2366 BWA=0.9262 (CWA=0.9264, SWA=0.9261)',\n'\\n', '[max_40] Epoch  17/40: train_loss=0.2477 val_loss=0.2359 BWA=0.9268\n(CWA=0.9270, SWA=0.9265)', '\\n', '[max_40] Epoch  18/40: train_loss=0.2451\nval_loss=0.2364 BWA=0.9251 (CWA=0.9260, SWA=0.9242)', '\\n', '[max_40] Epoch\n19/40: train_loss=0.2388 val_loss=0.2266 BWA=0.9297 (CWA=0.9300, SWA=0.9294)',\n'\\n', '[max_40] Epoch  20/40: train_loss=0.2390 val_loss=0.2263 BWA=0.9340\n(CWA=0.9344, SWA=0.9336)', '\\n', '[max_40] Epoch  21/40: train_loss=0.2376\nval_loss=0.2256 BWA=0.9301 (CWA=0.9305, SWA=0.9298)', '\\n', '[max_40] Epoch\n22/40: train_loss=0.2323 val_loss=0.2225 BWA=0.9328 (CWA=0.9326, SWA=0.9330)',\n'\\n', '[max_40] Epoch  23/40: train_loss=0.2290 val_loss=0.2186 BWA=0.9308\n(CWA=0.9305, SWA=0.9311)', '\\n', '[max_40] Epoch  24/40: train_loss=0.2270\nval_loss=0.2190 BWA=0.9346 (CWA=0.9345, SWA=0.9346)', '\\n', '[max_40] Epoch\n25/40: train_loss=0.2277 val_loss=0.2138 BWA=0.9393 (CWA=0.9392, SWA=0.9393)',\n'\\n', '[max_40] Epoch  26/40: train_loss=0.2273 val_loss=0.2113 BWA=0.9401\n(CWA=0.9398, SWA=0.9404)', '\\n', '[max_40] Epoch  27/40: train_loss=0.2222\nval_loss=0.2164 BWA=0.9385 (CWA=0.9380, SWA=0.9391)', '\\n', '[max_40] Epoch\n28/40: train_loss=0.2231 val_loss=0.2094 BWA=0.9423 (CWA=0.9420, SWA=0.9426)',\n'\\n', '[max_40] Epoch  29/40: train_loss=0.2205 val_loss=0.2096 BWA=0.9411\n(CWA=0.9416, SWA=0.9407)', '\\n', '[max_40] Epoch  30/40: train_loss=0.2155\nval_loss=0.2071 BWA=0.9420 (CWA=0.9422, SWA=0.9418)', '\\n', '[max_40] Epoch\n31/40: train_loss=0.2167 val_loss=0.2077 BWA=0.9409 (CWA=0.9407, SWA=0.9412)',\n'\\n', '[max_40] Epoch  32/40: train_loss=0.2195 val_loss=0.2079 BWA=0.9420\n(CWA=0.9419, SWA=0.9420)', '\\n', '[max_40] Epoch  33/40: train_loss=0.2111\nval_loss=0.2083 BWA=0.9418 (CWA=0.9409, SWA=0.9426)', '\\n', '[max_40] Early\nstopping triggered at epoch 33.', '\\n', '[max_40] Test -> loss: 1.2184  BWA:\n0.6532 (CWA=0.6746 SWA=0.6318)', '\\n', '[max_40] Curve saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n10/working/bwa_curve_max_40.png', '\\n', '[max_50] Epoch   1/50:\ntrain_loss=0.5281 val_loss=0.4545 BWA=0.8029 (CWA=0.8006, SWA=0.8053)', '\\n',\n'[max_50] Epoch   2/50: train_loss=0.4310 val_loss=0.4029 BWA=0.8314\n(CWA=0.8315, SWA=0.8314)', '\\n', '[max_50] Epoch   3/50: train_loss=0.3972\nval_loss=0.3809 BWA=0.8437 (CWA=0.8439, SWA=0.8435)', '\\n', '[max_50] Epoch\n4/50: train_loss=0.3759 val_loss=0.3688 BWA=0.8569 (CWA=0.8568, SWA=0.8571)',\n'\\n', '[max_50] Epoch   5/50: train_loss=0.3580 val_loss=0.3518 BWA=0.8679\n(CWA=0.8672, SWA=0.8685)', '\\n', '[max_50] Epoch   6/50: train_loss=0.3415\nval_loss=0.3274 BWA=0.8828 (CWA=0.8830, SWA=0.8826)', '\\n', '[max_50] Epoch\n7/50: train_loss=0.3295 val_loss=0.3146 BWA=0.8916 (CWA=0.8918, SWA=0.8914)',\n'\\n', '[max_50] Epoch   8/50: train_loss=0.3165 val_loss=0.3003 BWA=0.8975\n(CWA=0.8975, SWA=0.8976)', '\\n', '[max_50] Epoch   9/50: train_loss=0.3007\nval_loss=0.2962 BWA=0.8997 (CWA=0.9010, SWA=0.8984)', '\\n', '[max_50] Epoch\n10/50: train_loss=0.2887 val_loss=0.2772 BWA=0.9132 (CWA=0.9128, SWA=0.9136)',\n'\\n', '[max_50] Epoch  11/50: train_loss=0.2818 val_loss=0.2684 BWA=0.9178\n(CWA=0.9178, SWA=0.9179)', '\\n', '[max_50] Epoch  12/50: train_loss=0.2741\nval_loss=0.2628 BWA=0.9147 (CWA=0.9152, SWA=0.9141)', '\\n', '[max_50] Epoch\n13/50: train_loss=0.2701 val_loss=0.2558 BWA=0.9181 (CWA=0.9186, SWA=0.9177)',\n'\\n', '[max_50] Epoch  14/50: train_loss=0.2634 val_loss=0.2660 BWA=0.9045\n(CWA=0.9054, SWA=0.9035)', '\\n', '[max_50] Epoch  15/50: train_loss=0.2530\nval_loss=0.2443 BWA=0.9210 (CWA=0.9207, SWA=0.9212)', '\\n', '[max_50] Epoch\n16/50: train_loss=0.2501 val_loss=0.2378 BWA=0.9295 (CWA=0.9300, SWA=0.9291)',\n'\\n', '[max_50] Epoch  17/50: train_loss=0.2453 val_loss=0.2352 BWA=0.9264\n(CWA=0.9265, SWA=0.9262)', '\\n', '[max_50] Epoch  18/50: train_loss=0.2441\nval_loss=0.2312 BWA=0.9259 (CWA=0.9264, SWA=0.9254)', '\\n', '[max_50] Epoch\n19/50: train_loss=0.2369 val_loss=0.2267 BWA=0.9336 (CWA=0.9336, SWA=0.9336)',\n'\\n', '[max_50] Epoch  20/50: train_loss=0.2361 val_loss=0.2258 BWA=0.9394\n(CWA=0.9392, SWA=0.9397)', '\\n', '[max_50] Epoch  21/50: train_loss=0.2331\nval_loss=0.2223 BWA=0.9363 (CWA=0.9368, SWA=0.9358)', '\\n', '[max_50] Epoch\n22/50: train_loss=0.2323 val_loss=0.2187 BWA=0.9355 (CWA=0.9364, SWA=0.9347)',\n'\\n', '[max_50] Epoch  23/50: train_loss=0.2294 val_loss=0.2236 BWA=0.9364\n(CWA=0.9361, SWA=0.9368)', '\\n', '[max_50] Epoch  24/50: train_loss=0.2248\nval_loss=0.2158 BWA=0.9392 (CWA=0.9393, SWA=0.9391)', '\\n', '[max_50] Epoch\n25/50: train_loss=0.2232 val_loss=0.2258 BWA=0.9275 (CWA=0.9287, SWA=0.9263)',\n'\\n', '[max_50] Early stopping triggered at epoch 25.', '\\n', '[max_50] Test ->\nloss: 1.1268  BWA: 0.6534 (CWA=0.6751 SWA=0.6318)', '\\n', '[max_50] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n10/working/bwa_curve_max_50.png', '\\n', 'All experiment data saved to\nworking/experiment_data.npy', '\\n', 'Execution time: 16 minutes seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 545026.25\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 488539.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 694375.21\nexamples/s]', '\\n', '[lr=0.0003] Epoch 1/5 train_loss=0.5956 val_loss=0.5317\nBWA=0.7307 (CWA=0.7299, SWA=0.7316)', '\\n', '[lr=0.0003] Epoch 2/5\ntrain_loss=0.5168 val_loss=0.5000 BWA=0.7750 (CWA=0.7740, SWA=0.7761)', '\\n',\n'[lr=0.0003] Epoch 3/5 train_loss=0.4843 val_loss=0.4714 BWA=0.7963 (CWA=0.7942,\nSWA=0.7985)', '\\n', '[lr=0.0003] Epoch 4/5 train_loss=0.4568 val_loss=0.4439\nBWA=0.8108 (CWA=0.8103, SWA=0.8114)', '\\n', '[lr=0.0003] Epoch 5/5\ntrain_loss=0.4343 val_loss=0.4246 BWA=0.8205 (CWA=0.8201, SWA=0.8209)', '\\n',\n'[lr=0.0005] Epoch 1/5 train_loss=0.5693 val_loss=0.5058 BWA=0.7727 (CWA=0.7717,\nSWA=0.7737)', '\\n', '[lr=0.0005] Epoch 2/5 train_loss=0.4893 val_loss=0.4662\nBWA=0.7842 (CWA=0.7834, SWA=0.7851)', '\\n', '[lr=0.0005] Epoch 3/5\ntrain_loss=0.4516 val_loss=0.4331 BWA=0.8076 (CWA=0.8072, SWA=0.8081)', '\\n',\n'[lr=0.0005] Epoch 4/5 train_loss=0.4219 val_loss=0.4061 BWA=0.8266 (CWA=0.8270,\nSWA=0.8262)', '\\n', '[lr=0.0005] Epoch 5/5 train_loss=0.4025 val_loss=0.3920\nBWA=0.8347 (CWA=0.8351, SWA=0.8343)', '\\n', '[lr=0.001] Epoch 1/5\ntrain_loss=0.5234 val_loss=0.4580 BWA=0.7932 (CWA=0.7932, SWA=0.7933)', '\\n',\n'[lr=0.001] Epoch 2/5 train_loss=0.4307 val_loss=0.4037 BWA=0.8304 (CWA=0.8298,\nSWA=0.8309)', '\\n', '[lr=0.001] Epoch 3/5 train_loss=0.3929 val_loss=0.3778\nBWA=0.8472 (CWA=0.8471, SWA=0.8473)', '\\n', '[lr=0.001] Epoch 4/5\ntrain_loss=0.3725 val_loss=0.3590 BWA=0.8657 (CWA=0.8656, SWA=0.8658)', '\\n',\n'[lr=0.001] Epoch 5/5 train_loss=0.3574 val_loss=0.3486 BWA=0.8760 (CWA=0.8761,\nSWA=0.8760)', '\\n', '[lr=0.003] Epoch 1/5 train_loss=0.4679 val_loss=0.3946\nBWA=0.8329 (CWA=0.8334, SWA=0.8323)', '\\n', '[lr=0.003] Epoch 2/5\ntrain_loss=0.3702 val_loss=0.3423 BWA=0.8726 (CWA=0.8725, SWA=0.8727)', '\\n',\n'[lr=0.003] Epoch 3/5 train_loss=0.3330 val_loss=0.3057 BWA=0.9001 (CWA=0.8992,\nSWA=0.9011)', '\\n', '[lr=0.003] Epoch 4/5 train_loss=0.3033 val_loss=0.2795\nBWA=0.9130 (CWA=0.9126, SWA=0.9134)', '\\n', '[lr=0.003] Epoch 5/5\ntrain_loss=0.2803 val_loss=0.2859 BWA=0.8931 (CWA=0.8938, SWA=0.8925)', '\\n',\n'\\nBest learning rate based on dev BWA: 0.003 (BWA=0.8931)', '\\n', 'Test results\nwith best lr=0.003: loss=1.0228 BWA=0.6437 CWA=0.6636 SWA=0.6238', '\\n', 'Curve\nsaved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n11/working/bwa_curve_spr.png', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 565708.47\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 724029.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 817539.37\nexamples/s]', '\\n', '\\n===== Training with batch_size=32 =====', '\\n', 'Epoch\n1/5 bs=32 train_loss=0.5100 val_loss=0.4432 BWA=0.8222 (CWA=0.8201,\nSWA=0.8242)', '\\n', 'Epoch 2/5 bs=32 train_loss=0.4075 val_loss=0.3834\nBWA=0.8376 (CWA=0.8387, SWA=0.8364)', '\\n', 'Epoch 3/5 bs=32 train_loss=0.3694\nval_loss=0.3574 BWA=0.8692 (CWA=0.8684, SWA=0.8701)', '\\n', 'Epoch 4/5 bs=32\ntrain_loss=0.3443 val_loss=0.3351 BWA=0.8767 (CWA=0.8772, SWA=0.8761)', '\\n',\n'Epoch 5/5 bs=32 train_loss=0.3252 val_loss=0.3145 BWA=0.8891 (CWA=0.8898,\nSWA=0.8885)', '\\n', 'Test bs=32 -> loss:0.8762 BWA:0.6422 CWA:0.6618\nSWA:0.6227', '\\n', '\\n===== Training with batch_size=64 =====', '\\n', 'Epoch 1/5\nbs=64 train_loss=0.5379 val_loss=0.4691 BWA=0.7816 (CWA=0.7814, SWA=0.7818)',\n'\\n', 'Epoch 2/5 bs=64 train_loss=0.4418 val_loss=0.4194 BWA=0.8119 (CWA=0.8128,\nSWA=0.8111)', '\\n', 'Epoch 3/5 bs=64 train_loss=0.4047 val_loss=0.3964\nBWA=0.8347 (CWA=0.8348, SWA=0.8346)', '\\n', 'Epoch 4/5 bs=64 train_loss=0.3840\nval_loss=0.3693 BWA=0.8586 (CWA=0.8587, SWA=0.8585)', '\\n', 'Epoch 5/5 bs=64\ntrain_loss=0.3645 val_loss=0.3548 BWA=0.8639 (CWA=0.8644, SWA=0.8634)', '\\n',\n'Test bs=64 -> loss:0.8094 BWA:0.6344 CWA:0.6531 SWA:0.6157', '\\n', '\\n=====\nTraining with batch_size=128 =====', '\\n', 'Epoch 1/5 bs=128 train_loss=0.5681\nval_loss=0.4826 BWA=0.7849 (CWA=0.7838, SWA=0.7860)', '\\n', 'Epoch 2/5 bs=128\ntrain_loss=0.4524 val_loss=0.4228 BWA=0.8138 (CWA=0.8138, SWA=0.8138)', '\\n',\n'Epoch 3/5 bs=128 train_loss=0.4152 val_loss=0.4041 BWA=0.8384 (CWA=0.8372,\nSWA=0.8396)', '\\n', 'Epoch 4/5 bs=128 train_loss=0.3925 val_loss=0.3874\nBWA=0.8395 (CWA=0.8397, SWA=0.8392)', '\\n', 'Epoch 5/5 bs=128 train_loss=0.3782\nval_loss=0.3677 BWA=0.8595 (CWA=0.8593, SWA=0.8596)', '\\n', 'Test bs=128 ->\nloss:0.8026 BWA:0.6310 CWA:0.6490 SWA:0.6129', '\\n', 'Plot saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n12/working/bwa_batchsize_curve.png', '\\n', 'experiment_data.npy saved.', '\\n',\n'Execution time: 59 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Found SPR_BENCH at:', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples\n[00:00, 554835.14 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 5000 examples [00:00,\n611165.12 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 722122.48\nexamples/s]', '\\n', '[wd_0.0] Ep1: train_loss 0.5377 val_loss 0.4619 BWA 0.8013\n(CWA 0.7993, SWA 0.8033)', '\\n', '[wd_0.0] Ep2: train_loss 0.4299 val_loss\n0.4075 BWA 0.8283 (CWA 0.8288, SWA 0.8278)', '\\n', '[wd_0.0] Ep3: train_loss\n0.3933 val_loss 0.3758 BWA 0.8511 (CWA 0.8509, SWA 0.8512)', '\\n', '[wd_0.0]\nEp4: train_loss 0.3702 val_loss 0.3588 BWA 0.8621 (CWA 0.8626, SWA 0.8616)',\n'\\n', '[wd_0.0] Ep5: train_loss 0.3526 val_loss 0.3559 BWA 0.8537 (CWA 0.8551,\nSWA 0.8523)', '\\n', '[wd_0.0] Test -> loss 0.8358  BWA 0.6385', '\\n', 'Curve\nsaved to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n13/working/bwa_curve_wd_0.0.png', '\\n', '[wd_1e-05] Ep1: train_loss 0.5438\nval_loss 0.4933 BWA 0.7652 (CWA 0.7646, SWA 0.7657)', '\\n', '[wd_1e-05] Ep2:\ntrain_loss 0.4632 val_loss 0.4385 BWA 0.8026 (CWA 0.8028, SWA 0.8023)', '\\n',\n'[wd_1e-05] Ep3: train_loss 0.4140 val_loss 0.3978 BWA 0.8384 (CWA 0.8376, SWA\n0.8393)', '\\n', '[wd_1e-05] Ep4: train_loss 0.3882 val_loss 0.3717 BWA 0.8557\n(CWA 0.8553, SWA 0.8560)', '\\n', '[wd_1e-05] Ep5: train_loss 0.3695 val_loss\n0.3587 BWA 0.8575 (CWA 0.8575, SWA 0.8575)', '\\n', '[wd_1e-05] Test -> loss\n0.7938  BWA 0.6341', '\\n', 'Curve saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/bwa_curve_wd_1e-05.png', '\\n', '[wd_0.0001]\nEp1: train_loss 0.5357 val_loss 0.4559 BWA 0.7973 (CWA 0.7967, SWA 0.7978)',\n'\\n', '[wd_0.0001] Ep2: train_loss 0.4269 val_loss 0.4048 BWA 0.8203 (CWA\n0.8211, SWA 0.8194)', '\\n', '[wd_0.0001] Ep3: train_loss 0.3907 val_loss 0.3726\nBWA 0.8528 (CWA 0.8521, SWA 0.8535)', '\\n', '[wd_0.0001] Ep4: train_loss 0.3690\nval_loss 0.3542 BWA 0.8614 (CWA 0.8616, SWA 0.8612)', '\\n', '[wd_0.0001] Ep5:\ntrain_loss 0.3504 val_loss 0.3351 BWA 0.8783 (CWA 0.8779, SWA 0.8787)', '\\n',\n'[wd_0.0001] Test -> loss 0.8278  BWA 0.6357', '\\n', 'Curve saved to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n13/working/bwa_curve_wd_0.0001.png', '\\n', '[wd_0.001] Ep1: train_loss 0.5274\nval_loss 0.4654 BWA 0.8004 (CWA 0.7989, SWA 0.8019)', '\\n', '[wd_0.001] Ep2:\ntrain_loss 0.4368 val_loss 0.4129 BWA 0.8252 (CWA 0.8252, SWA 0.8252)', '\\n',\n'[wd_0.001] Ep3: train_loss 0.4029 val_loss 0.3932 BWA 0.8353 (CWA 0.8358, SWA\n0.8348)', '\\n', '[wd_0.001] Ep4: train_loss 0.3845 val_loss 0.3809 BWA 0.8567\n(CWA 0.8558, SWA 0.8576)', '\\n', '[wd_0.001] Ep5: train_loss 0.3725 val_loss\n0.3741 BWA 0.8426 (CWA 0.8447, SWA 0.8405)', '\\n', '[wd_0.001] Test -> loss\n0.7454  BWA 0.6401', '\\n', 'Curve saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/bwa_curve_wd_0.001.png', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 2 minutes seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n=== Training with embed_dim=16 ===', '\\n',\n'Epoch 1: train_loss=0.5623  val_loss=0.4953  BWA=0.7708 (CWA=0.7691,\nSWA=0.7724)', '\\n', 'Epoch 2: train_loss=0.4696  val_loss=0.4488  BWA=0.8068\n(CWA=0.8054, SWA=0.8081)', '\\n', 'Epoch 3: train_loss=0.4334  val_loss=0.4184\nBWA=0.8185 (CWA=0.8179, SWA=0.8191)', '\\n', 'Epoch 4: train_loss=0.4101\nval_loss=0.3974  BWA=0.8342 (CWA=0.8340, SWA=0.8345)', '\\n', 'Epoch 5:\ntrain_loss=0.3944  val_loss=0.3831  BWA=0.8496 (CWA=0.8489, SWA=0.8503)', '\\n',\n'\\n=== Training with embed_dim=32 ===', '\\n', 'Epoch 1: train_loss=0.5264\nval_loss=0.4662  BWA=0.7978 (CWA=0.7956, SWA=0.8000)', '\\n', 'Epoch 2:\ntrain_loss=0.4378  val_loss=0.4116  BWA=0.8280 (CWA=0.8269, SWA=0.8291)', '\\n',\n'Epoch 3: train_loss=0.4008  val_loss=0.3804  BWA=0.8508 (CWA=0.8507,\nSWA=0.8509)', '\\n', 'Epoch 4: train_loss=0.3731  val_loss=0.3603  BWA=0.8634\n(CWA=0.8639, SWA=0.8628)', '\\n', 'Epoch 5: train_loss=0.3535  val_loss=0.3424\nBWA=0.8760 (CWA=0.8762, SWA=0.8759)', '\\n', '\\n=== Training with embed_dim=64\n===', '\\n', 'Epoch 1: train_loss=0.5040  val_loss=0.4310  BWA=0.8253\n(CWA=0.8236, SWA=0.8271)', '\\n', 'Epoch 2: train_loss=0.4072  val_loss=0.3797\nBWA=0.8466 (CWA=0.8460, SWA=0.8473)', '\\n', 'Epoch 3: train_loss=0.3706\nval_loss=0.3572  BWA=0.8593 (CWA=0.8591, SWA=0.8596)', '\\n', 'Epoch 4:\ntrain_loss=0.3466  val_loss=0.3312  BWA=0.8786 (CWA=0.8785, SWA=0.8787)', '\\n',\n'Epoch 5: train_loss=0.3240  val_loss=0.3042  BWA=0.8969 (CWA=0.8968,\nSWA=0.8970)', '\\n', '\\n=== Training with embed_dim=128 ===', '\\n', 'Epoch 1:\ntrain_loss=0.4875  val_loss=0.4052  BWA=0.8310 (CWA=0.8303, SWA=0.8318)', '\\n',\n'Epoch 2: train_loss=0.3947  val_loss=0.3737  BWA=0.8550 (CWA=0.8551,\nSWA=0.8549)', '\\n', 'Epoch 3: train_loss=0.3621  val_loss=0.3366  BWA=0.8782\n(CWA=0.8787, SWA=0.8777)', '\\n', 'Epoch 4: train_loss=0.3336  val_loss=0.3126\nBWA=0.8922 (CWA=0.8914, SWA=0.8930)', '\\n', 'Epoch 5: train_loss=0.3062\nval_loss=0.2917  BWA=0.9002 (CWA=0.8999, SWA=0.9005)', '\\n', '\\nBest embed_dim\naccording to dev BWA: 128 (0.9002)', '\\n', 'Final Test -> loss: 0.8991  BWA:\n0.6406  CWA: 0.6610  SWA: 0.6203', '\\n', 'Curve saved to /home/zxl240011/AI-Scie\nntist-v2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-12/working/bwa_curve_embed_dim.png', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n=== Training with dropout_rate=0.0 ===', '\\n',\n'  Epoch 01: train_loss=0.5266  val_loss=0.4661  BWA=0.8011 (CWA=0.7986,\nSWA=0.8035)', '\\n', '  Epoch 02: train_loss=0.4234  val_loss=0.4033  BWA=0.8294\n(CWA=0.8298, SWA=0.8290)', '\\n', '  Epoch 03: train_loss=0.3768  val_loss=0.3677\nBWA=0.8559 (CWA=0.8556, SWA=0.8561)', '\\n', '  Epoch 04: train_loss=0.3473\nval_loss=0.3424  BWA=0.8655 (CWA=0.8657, SWA=0.8654)', '\\n', '  Epoch 05:\ntrain_loss=0.3243  val_loss=0.3231  BWA=0.8752 (CWA=0.8756, SWA=0.8748)', '\\n',\n'  Epoch 06: train_loss=0.3059  val_loss=0.3117  BWA=0.8884 (CWA=0.8874,\nSWA=0.8894)', '\\n', '  Epoch 07: train_loss=0.2917  val_loss=0.2927  BWA=0.9009\n(CWA=0.9000, SWA=0.9018)', '\\n', '  Epoch 08: train_loss=0.2790  val_loss=0.2743\nBWA=0.9112 (CWA=0.9105, SWA=0.9119)', '\\n', '  >> Test -> loss: 0.9829  BWA:\n0.6377  CWA: 0.6580  SWA: 0.6174', '\\n', '  Curve saved to /home/zxl240011/AI-Sc\nientist-v2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/bwa_curve_dropout_0.0.png', '\\n', '\\n===\nTraining with dropout_rate=0.1 ===', '\\n', '  Epoch 01: train_loss=0.5200\nval_loss=0.4407  BWA=0.8081 (CWA=0.8071, SWA=0.8092)', '\\n', '  Epoch 02:\ntrain_loss=0.4164  val_loss=0.3931  BWA=0.8317 (CWA=0.8325, SWA=0.8309)', '\\n',\n'  Epoch 03: train_loss=0.3822  val_loss=0.3691  BWA=0.8569 (CWA=0.8568,\nSWA=0.8571)', '\\n', '  Epoch 04: train_loss=0.3579  val_loss=0.3488  BWA=0.8689\n(CWA=0.8688, SWA=0.8690)', '\\n', '  Epoch 05: train_loss=0.3377  val_loss=0.3457\nBWA=0.8590 (CWA=0.8581, SWA=0.8599)', '\\n', '  Epoch 06: train_loss=0.3226\nval_loss=0.3109  BWA=0.8920 (CWA=0.8928, SWA=0.8913)', '\\n', '  Epoch 07:\ntrain_loss=0.3057  val_loss=0.2971  BWA=0.8938 (CWA=0.8940, SWA=0.8936)', '\\n',\n'  Epoch 08: train_loss=0.2938  val_loss=0.2797  BWA=0.9062 (CWA=0.9068,\nSWA=0.9057)', '\\n', '  >> Test -> loss: 0.9396  BWA: 0.6444  CWA: 0.6647  SWA:\n0.6241', '\\n', '  Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/bwa_curve_dropout_0.1.png', '\\n', '\\n===\nTraining with dropout_rate=0.3 ===', '\\n', '  Epoch 01: train_loss=0.5234\nval_loss=0.4452  BWA=0.8007 (CWA=0.8007, SWA=0.8007)', '\\n', '  Epoch 02:\ntrain_loss=0.4314  val_loss=0.4004  BWA=0.8419 (CWA=0.8412, SWA=0.8425)', '\\n',\n'  Epoch 03: train_loss=0.3961  val_loss=0.3776  BWA=0.8532 (CWA=0.8532,\nSWA=0.8533)', '\\n', '  Epoch 04: train_loss=0.3792  val_loss=0.3683  BWA=0.8591\n(CWA=0.8588, SWA=0.8593)', '\\n', '  Epoch 05: train_loss=0.3613  val_loss=0.3482\nBWA=0.8740 (CWA=0.8735, SWA=0.8745)', '\\n', '  Epoch 06: train_loss=0.3465\nval_loss=0.3328  BWA=0.8841 (CWA=0.8844, SWA=0.8839)', '\\n', '  Epoch 07:\ntrain_loss=0.3333  val_loss=0.3214  BWA=0.8912 (CWA=0.8916, SWA=0.8908)', '\\n',\n'  Epoch 08: train_loss=0.3217  val_loss=0.3096  BWA=0.8902 (CWA=0.8910,\nSWA=0.8893)', '\\n', '  >> Test -> loss: 0.8486  BWA: 0.6431  CWA: 0.6632  SWA:\n0.6230', '\\n', '  Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/bwa_curve_dropout_0.3.png', '\\n', '\\n===\nTraining with dropout_rate=0.5 ===', '\\n', '  Epoch 01: train_loss=0.5422\nval_loss=0.4643  BWA=0.7935 (CWA=0.7920, SWA=0.7951)', '\\n', '  Epoch 02:\ntrain_loss=0.4450  val_loss=0.4061  BWA=0.8347 (CWA=0.8342, SWA=0.8353)', '\\n',\n'  Epoch 03: train_loss=0.4060  val_loss=0.3804  BWA=0.8495 (CWA=0.8494,\nSWA=0.8497)', '\\n', '  Epoch 04: train_loss=0.3840  val_loss=0.3699  BWA=0.8563\n(CWA=0.8557, SWA=0.8569)', '\\n', '  Epoch 05: train_loss=0.3664  val_loss=0.3452\nBWA=0.8709 (CWA=0.8713, SWA=0.8706)', '\\n', '  Epoch 06: train_loss=0.3487\nval_loss=0.3391  BWA=0.8805 (CWA=0.8801, SWA=0.8809)', '\\n', '  Epoch 07:\ntrain_loss=0.3382  val_loss=0.3221  BWA=0.8891 (CWA=0.8891, SWA=0.8891)', '\\n',\n'  Epoch 08: train_loss=0.3278  val_loss=0.3102  BWA=0.8950 (CWA=0.8946,\nSWA=0.8954)', '\\n', '  >> Test -> loss: 0.8489  BWA: 0.6414  CWA: 0.6612  SWA:\n0.6216', '\\n', '  Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/bwa_curve_dropout_0.5.png', '\\n', '\\nAll\nexperiment data saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n30_17-49-45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 43, in <module>\\n    DATA_PATH = locate_spr_bench()\\n\n^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 26, in locate_spr_bench\\n    raise\nFileNotFoundError(\"Place SPR_BENCH or set SPR_DATA_PATH\")\\nFileNotFoundError:\nPlace SPR_BENCH or set SPR_DATA_PATH\\n', 'Execution time: a second seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\n=== Training with hidden_dim=32 ===', '\\n',\n'Epoch 1/5 - train_loss 0.5613 val_loss 0.4987  val_BWA 0.7673', '\\n', 'Epoch\n2/5 - train_loss 0.4721 val_loss 0.4323  val_BWA 0.8160', '\\n', 'Epoch 3/5 -\ntrain_loss 0.4176 val_loss 0.3963  val_BWA 0.8413', '\\n', 'Epoch 4/5 -\ntrain_loss 0.3926 val_loss 0.3761  val_BWA 0.8556', '\\n', 'Epoch 5/5 -\ntrain_loss 0.3784 val_loss 0.3671  val_BWA 0.8611', '\\n', '\\n=== Training with\nhidden_dim=64 ===', '\\n', 'Epoch 1/5 - train_loss 0.5317 val_loss 0.4617\nval_BWA 0.7827', '\\n', 'Epoch 2/5 - train_loss 0.4293 val_loss 0.4081  val_BWA\n0.8193', '\\n', 'Epoch 3/5 - train_loss 0.3873 val_loss 0.3653  val_BWA 0.8553',\n'\\n', 'Epoch 4/5 - train_loss 0.3641 val_loss 0.3470  val_BWA 0.8708', '\\n',\n'Epoch 5/5 - train_loss 0.3435 val_loss 0.3346  val_BWA 0.8775', '\\n', '\\n===\nTraining with hidden_dim=128 ===', '\\n', 'Epoch 1/5 - train_loss 0.4850 val_loss\n0.4039  val_BWA 0.8331', '\\n', 'Epoch 2/5 - train_loss 0.3858 val_loss 0.3699\nval_BWA 0.8464', '\\n', 'Epoch 3/5 - train_loss 0.3529 val_loss 0.3369  val_BWA\n0.8740', '\\n', 'Epoch 4/5 - train_loss 0.3294 val_loss 0.3253  val_BWA 0.8791',\n'\\n', 'Epoch 5/5 - train_loss 0.3111 val_loss 0.3067  val_BWA 0.8886', '\\n',\n'\\n=== Training with hidden_dim=256 ===', '\\n', 'Epoch 1/5 - train_loss 0.4740\nval_loss 0.3950  val_BWA 0.8404', '\\n', 'Epoch 2/5 - train_loss 0.3842 val_loss\n0.3644  val_BWA 0.8471', '\\n', 'Epoch 3/5 - train_loss 0.3442 val_loss 0.3349\nval_BWA 0.8832', '\\n', 'Epoch 4/5 - train_loss 0.3105 val_loss 0.3018  val_BWA\n0.8936', '\\n', 'Epoch 5/5 - train_loss 0.2879 val_loss 0.2937  val_BWA 0.8828',\n'\\n', '\\nBest hidden_dim = 128 with dev BWA = 0.8886', '\\n', 'TEST -> loss\n0.8337  BWA 0.6454  CWA 0.6653  SWA 0.6254', '\\n', 'Curve saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n10/working/bwa_curve_best_hidden_dim.png', '\\n', 'Execution time: 3 minutes\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '[max_10] Epoch   1/10: train_loss=0.5425\nval_loss=0.4687 BWA=0.7920 (CWA=0.7907, SWA=0.7932)', '\\n', '[max_10] Epoch\n2/10: train_loss=0.4370 val_loss=0.4044 BWA=0.8225 (CWA=0.8226, SWA=0.8223)',\n'\\n', '[max_10] Epoch   3/10: train_loss=0.3980 val_loss=0.3832 BWA=0.8502\n(CWA=0.8497, SWA=0.8508)', '\\n', '[max_10] Epoch   4/10: train_loss=0.3749\nval_loss=0.3634 BWA=0.8565 (CWA=0.8563, SWA=0.8567)', '\\n', '[max_10] Epoch\n5/10: train_loss=0.3596 val_loss=0.3515 BWA=0.8732 (CWA=0.8719, SWA=0.8744)',\n'\\n', '[max_10] Epoch   6/10: train_loss=0.3433 val_loss=0.3372 BWA=0.8682\n(CWA=0.8685, SWA=0.8679)', '\\n', '[max_10] Epoch   7/10: train_loss=0.3328\nval_loss=0.3181 BWA=0.8823 (CWA=0.8824, SWA=0.8822)', '\\n', '[max_10] Epoch\n8/10: train_loss=0.3193 val_loss=0.3125 BWA=0.8938 (CWA=0.8933, SWA=0.8944)',\n'\\n', '[max_10] Epoch   9/10: train_loss=0.3077 val_loss=0.3028 BWA=0.8942\n(CWA=0.8939, SWA=0.8945)', '\\n', '[max_10] Epoch  10/10: train_loss=0.2992\nval_loss=0.2865 BWA=0.9043 (CWA=0.9036, SWA=0.9051)', '\\n', '[max_10] Test ->\nloss: 0.9288  BWA: 0.6434 (CWA=0.6640 SWA=0.6229)', '\\n', '[max_10] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n13/working/bwa_curve_max_10.png', '\\n', '[max_20] Epoch   1/20:\ntrain_loss=0.5311 val_loss=0.4688 BWA=0.7823 (CWA=0.7824, SWA=0.7822)', '\\n',\n'[max_20] Epoch   2/20: train_loss=0.4336 val_loss=0.4147 BWA=0.8323\n(CWA=0.8315, SWA=0.8330)', '\\n', '[max_20] Epoch   3/20: train_loss=0.3966\nval_loss=0.3841 BWA=0.8499 (CWA=0.8497, SWA=0.8502)', '\\n', '[max_20] Epoch\n4/20: train_loss=0.3739 val_loss=0.3626 BWA=0.8601 (CWA=0.8599, SWA=0.8604)',\n'\\n', '[max_20] Epoch   5/20: train_loss=0.3563 val_loss=0.3459 BWA=0.8728\n(CWA=0.8730, SWA=0.8727)', '\\n', '[max_20] Epoch   6/20: train_loss=0.3405\nval_loss=0.3399 BWA=0.8718 (CWA=0.8719, SWA=0.8718)', '\\n', '[max_20] Epoch\n7/20: train_loss=0.3244 val_loss=0.3206 BWA=0.8904 (CWA=0.8898, SWA=0.8911)',\n'\\n', '[max_20] Epoch   8/20: train_loss=0.3118 val_loss=0.3042 BWA=0.9058\n(CWA=0.9053, SWA=0.9063)', '\\n', '[max_20] Epoch   9/20: train_loss=0.2991\nval_loss=0.2912 BWA=0.9028 (CWA=0.9030, SWA=0.9026)', '\\n', '[max_20] Epoch\n10/20: train_loss=0.2858 val_loss=0.2785 BWA=0.9092 (CWA=0.9093, SWA=0.9091)',\n'\\n', '[max_20] Epoch  11/20: train_loss=0.2789 val_loss=0.2737 BWA=0.9066\n(CWA=0.9065, SWA=0.9066)', '\\n', '[max_20] Epoch  12/20: train_loss=0.2727\nval_loss=0.2636 BWA=0.9202 (CWA=0.9198, SWA=0.9206)', '\\n', '[max_20] Epoch\n13/20: train_loss=0.2648 val_loss=0.2559 BWA=0.9209 (CWA=0.9206, SWA=0.9212)',\n'\\n', '[max_20] Epoch  14/20: train_loss=0.2588 val_loss=0.2491 BWA=0.9219\n(CWA=0.9218, SWA=0.9221)', '\\n', '[max_20] Epoch  15/20: train_loss=0.2564\nval_loss=0.2515 BWA=0.9177 (CWA=0.9167, SWA=0.9187)', '\\n', '[max_20] Epoch\n16/20: train_loss=0.2524 val_loss=0.2503 BWA=0.9223 (CWA=0.9217, SWA=0.9229)',\n'\\n', '[max_20] Epoch  17/20: train_loss=0.2477 val_loss=0.2441 BWA=0.9247\n(CWA=0.9245, SWA=0.9250)', '\\n', '[max_20] Epoch  18/20: train_loss=0.2433\nval_loss=0.2427 BWA=0.9192 (CWA=0.9199, SWA=0.9186)', '\\n', '[max_20] Epoch\n19/20: train_loss=0.2419 val_loss=0.2335 BWA=0.9318 (CWA=0.9313, SWA=0.9323)',\n'\\n', '[max_20] Epoch  20/20: train_loss=0.2384 val_loss=0.2259 BWA=0.9312\n(CWA=0.9314, SWA=0.9309)', '\\n', '[max_20] Test -> loss: 1.1425  BWA: 0.6500\n(CWA=0.6710 SWA=0.6290)', '\\n', '[max_20] Curve saved to /home/zxl240011/AI-Scie\nntist-v2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/bwa_curve_max_20.png', '\\n', '[max_30] Epoch\n1/30: train_loss=0.5336 val_loss=0.4645 BWA=0.7984 (CWA=0.7960, SWA=0.8007)',\n'\\n', '[max_30] Epoch   2/30: train_loss=0.4335 val_loss=0.4037 BWA=0.8316\n(CWA=0.8312, SWA=0.8321)', '\\n', '[max_30] Epoch   3/30: train_loss=0.3940\nval_loss=0.3791 BWA=0.8448 (CWA=0.8449, SWA=0.8447)', '\\n', '[max_30] Epoch\n4/30: train_loss=0.3722 val_loss=0.3629 BWA=0.8539 (CWA=0.8539, SWA=0.8539)',\n'\\n', '[max_30] Epoch   5/30: train_loss=0.3541 val_loss=0.3424 BWA=0.8707\n(CWA=0.8711, SWA=0.8703)', '\\n', '[max_30] Epoch   6/30: train_loss=0.3390\nval_loss=0.3404 BWA=0.8836 (CWA=0.8833, SWA=0.8839)', '\\n', '[max_30] Epoch\n7/30: train_loss=0.3253 val_loss=0.3143 BWA=0.8909 (CWA=0.8916, SWA=0.8902)',\n'\\n', '[max_30] Epoch   8/30: train_loss=0.3117 val_loss=0.3063 BWA=0.8943\n(CWA=0.8954, SWA=0.8932)', '\\n', '[max_30] Epoch   9/30: train_loss=0.2999\nval_loss=0.2923 BWA=0.9017 (CWA=0.9020, SWA=0.9015)', '\\n', '[max_30] Epoch\n10/30: train_loss=0.2911 val_loss=0.2815 BWA=0.9023 (CWA=0.9028, SWA=0.9018)',\n'\\n', '[max_30] Epoch  11/30: train_loss=0.2796 val_loss=0.2753 BWA=0.9098\n(CWA=0.9103, SWA=0.9093)', '\\n', '[max_30] Epoch  12/30: train_loss=0.2719\nval_loss=0.2671 BWA=0.9137 (CWA=0.9140, SWA=0.9133)', '\\n', '[max_30] Epoch\n13/30: train_loss=0.2665 val_loss=0.2621 BWA=0.9101 (CWA=0.9098, SWA=0.9104)',\n'\\n', '[max_30] Epoch  14/30: train_loss=0.2614 val_loss=0.2504 BWA=0.9271\n(CWA=0.9272, SWA=0.9270)', '\\n', '[max_30] Epoch  15/30: train_loss=0.2525\nval_loss=0.2561 BWA=0.9169 (CWA=0.9167, SWA=0.9170)', '\\n', '[max_30] Epoch\n16/30: train_loss=0.2536 val_loss=0.2414 BWA=0.9242 (CWA=0.9245, SWA=0.9240)',\n'\\n', '[max_30] Epoch  17/30: train_loss=0.2458 val_loss=0.2400 BWA=0.9314\n(CWA=0.9319, SWA=0.9309)', '\\n', '[max_30] Epoch  18/30: train_loss=0.2406\nval_loss=0.2374 BWA=0.9330 (CWA=0.9331, SWA=0.9330)', '\\n', '[max_30] Epoch\n19/30: train_loss=0.2414 val_loss=0.2325 BWA=0.9304 (CWA=0.9304, SWA=0.9303)',\n'\\n', '[max_30] Epoch  20/30: train_loss=0.2378 val_loss=0.2355 BWA=0.9276\n(CWA=0.9279, SWA=0.9272)', '\\n', '[max_30] Epoch  21/30: train_loss=0.2361\nval_loss=0.2277 BWA=0.9312 (CWA=0.9317, SWA=0.9307)', '\\n', '[max_30] Epoch\n22/30: train_loss=0.2298 val_loss=0.2358 BWA=0.9270 (CWA=0.9270, SWA=0.9269)',\n'\\n', '[max_30] Epoch  23/30: train_loss=0.2329 val_loss=0.2234 BWA=0.9318\n(CWA=0.9324, SWA=0.9312)', '\\n', '[max_30] Early stopping triggered at epoch\n23.', '\\n', '[max_30] Test -> loss: 1.0788  BWA: 0.6505 (CWA=0.6715\nSWA=0.6294)', '\\n', '[max_30] Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/bwa_curve_max_30.png', '\\n', '[max_40] Epoch\n1/40: train_loss=0.5380 val_loss=0.4733 BWA=0.7843 (CWA=0.7832, SWA=0.7855)',\n'\\n', '[max_40] Epoch   2/40: train_loss=0.4477 val_loss=0.4162 BWA=0.8159\n(CWA=0.8162, SWA=0.8155)', '\\n', '[max_40] Epoch   3/40: train_loss=0.4061\nval_loss=0.3858 BWA=0.8448 (CWA=0.8447, SWA=0.8448)', '\\n', '[max_40] Epoch\n4/40: train_loss=0.3824 val_loss=0.3671 BWA=0.8528 (CWA=0.8531, SWA=0.8525)',\n'\\n', '[max_40] Epoch   5/40: train_loss=0.3626 val_loss=0.3520 BWA=0.8634\n(CWA=0.8638, SWA=0.8629)', '\\n', '[max_40] Epoch   6/40: train_loss=0.3448\nval_loss=0.3336 BWA=0.8816 (CWA=0.8817, SWA=0.8815)', '\\n', '[max_40] Epoch\n7/40: train_loss=0.3303 val_loss=0.3265 BWA=0.8726 (CWA=0.8738, SWA=0.8714)',\n'\\n', '[max_40] Epoch   8/40: train_loss=0.3157 val_loss=0.3055 BWA=0.8924\n(CWA=0.8930, SWA=0.8919)', '\\n', '[max_40] Epoch   9/40: train_loss=0.3029\nval_loss=0.2989 BWA=0.8922 (CWA=0.8936, SWA=0.8909)', '\\n', '[max_40] Epoch\n10/40: train_loss=0.2941 val_loss=0.2802 BWA=0.9099 (CWA=0.9105, SWA=0.9093)',\n'\\n', '[max_40] Epoch  11/40: train_loss=0.2855 val_loss=0.2772 BWA=0.9099\n(CWA=0.9098, SWA=0.9101)', '\\n', '[max_40] Epoch  12/40: train_loss=0.2796\nval_loss=0.2661 BWA=0.9146 (CWA=0.9148, SWA=0.9144)', '\\n', '[max_40] Epoch\n13/40: train_loss=0.2702 val_loss=0.2639 BWA=0.9151 (CWA=0.9151, SWA=0.9151)',\n'\\n', '[max_40] Epoch  14/40: train_loss=0.2643 val_loss=0.2580 BWA=0.9141\n(CWA=0.9145, SWA=0.9137)', '\\n', '[max_40] Epoch  15/40: train_loss=0.2584\nval_loss=0.2473 BWA=0.9242 (CWA=0.9243, SWA=0.9242)', '\\n', '[max_40] Epoch\n16/40: train_loss=0.2534 val_loss=0.2432 BWA=0.9223 (CWA=0.9231, SWA=0.9215)',\n'\\n', '[max_40] Epoch  17/40: train_loss=0.2491 val_loss=0.2325 BWA=0.9286\n(CWA=0.9291, SWA=0.9281)', '\\n', '[max_40] Epoch  18/40: train_loss=0.2451\nval_loss=0.2306 BWA=0.9327 (CWA=0.9330, SWA=0.9325)', '\\n', '[max_40] Epoch\n19/40: train_loss=0.2384 val_loss=0.2312 BWA=0.9241 (CWA=0.9253, SWA=0.9230)',\n'\\n', '[max_40] Epoch  20/40: train_loss=0.2416 val_loss=0.2273 BWA=0.9289\n(CWA=0.9292, SWA=0.9286)', '\\n', '[max_40] Epoch  21/40: train_loss=0.2347\nval_loss=0.2266 BWA=0.9304 (CWA=0.9308, SWA=0.9301)', '\\n', '[max_40] Epoch\n22/40: train_loss=0.2308 val_loss=0.2218 BWA=0.9286 (CWA=0.9294, SWA=0.9279)',\n'\\n', '[max_40] Epoch  23/40: train_loss=0.2303 val_loss=0.2327 BWA=0.9300\n(CWA=0.9295, SWA=0.9305)', '\\n', '[max_40] Early stopping triggered at epoch\n23.', '\\n', '[max_40] Test -> loss: 1.0480  BWA: 0.6508 (CWA=0.6720\nSWA=0.6297)', '\\n', '[max_40] Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/bwa_curve_max_40.png', '\\n', '[max_50] Epoch\n1/50: train_loss=0.5291 val_loss=0.4510 BWA=0.8040 (CWA=0.8018, SWA=0.8062)',\n'\\n', '[max_50] Epoch   2/50: train_loss=0.4209 val_loss=0.3955 BWA=0.8449\n(CWA=0.8437, SWA=0.8461)', '\\n', '[max_50] Epoch   3/50: train_loss=0.3862\nval_loss=0.3745 BWA=0.8495 (CWA=0.8501, SWA=0.8489)', '\\n', '[max_50] Epoch\n4/50: train_loss=0.3651 val_loss=0.3621 BWA=0.8482 (CWA=0.8489, SWA=0.8475)',\n'\\n', '[max_50] Epoch   5/50: train_loss=0.3482 val_loss=0.3393 BWA=0.8744\n(CWA=0.8742, SWA=0.8745)', '\\n', '[max_50] Epoch   6/50: train_loss=0.3331\nval_loss=0.3276 BWA=0.8772 (CWA=0.8775, SWA=0.8770)', '\\n', '[max_50] Epoch\n7/50: train_loss=0.3169 val_loss=0.3110 BWA=0.8873 (CWA=0.8877, SWA=0.8869)',\n'\\n', '[max_50] Epoch   8/50: train_loss=0.3079 val_loss=0.2988 BWA=0.8978\n(CWA=0.8985, SWA=0.8970)', '\\n', '[max_50] Epoch   9/50: train_loss=0.2954\nval_loss=0.2865 BWA=0.9006 (CWA=0.9010, SWA=0.9002)', '\\n', '[max_50] Epoch\n10/50: train_loss=0.2852 val_loss=0.2753 BWA=0.9123 (CWA=0.9126, SWA=0.9121)',\n'\\n', '[max_50] Epoch  11/50: train_loss=0.2751 val_loss=0.2746 BWA=0.9084\n(CWA=0.9087, SWA=0.9080)', '\\n', '[max_50] Epoch  12/50: train_loss=0.2677\nval_loss=0.2613 BWA=0.9131 (CWA=0.9142, SWA=0.9120)', '\\n', '[max_50] Epoch\n13/50: train_loss=0.2593 val_loss=0.2586 BWA=0.9209 (CWA=0.9211, SWA=0.9206)',\n'\\n', '[max_50] Epoch  14/50: train_loss=0.2562 val_loss=0.2495 BWA=0.9236\n(CWA=0.9238, SWA=0.9234)', '\\n', '[max_50] Epoch  15/50: train_loss=0.2525\nval_loss=0.2435 BWA=0.9255 (CWA=0.9260, SWA=0.9251)', '\\n', '[max_50] Epoch\n16/50: train_loss=0.2473 val_loss=0.2360 BWA=0.9280 (CWA=0.9278, SWA=0.9282)',\n'\\n', '[max_50] Epoch  17/50: train_loss=0.2423 val_loss=0.2376 BWA=0.9293\n(CWA=0.9294, SWA=0.9292)', '\\n', '[max_50] Epoch  18/50: train_loss=0.2390\nval_loss=0.2307 BWA=0.9313 (CWA=0.9312, SWA=0.9313)', '\\n', '[max_50] Epoch\n19/50: train_loss=0.2346 val_loss=0.2293 BWA=0.9301 (CWA=0.9303, SWA=0.9300)',\n'\\n', '[max_50] Epoch  20/50: train_loss=0.2314 val_loss=0.2222 BWA=0.9358\n(CWA=0.9357, SWA=0.9358)', '\\n', '[max_50] Epoch  21/50: train_loss=0.2307\nval_loss=0.2223 BWA=0.9344 (CWA=0.9346, SWA=0.9342)', '\\n', '[max_50] Epoch\n22/50: train_loss=0.2261 val_loss=0.2218 BWA=0.9349 (CWA=0.9350, SWA=0.9348)',\n'\\n', '[max_50] Epoch  23/50: train_loss=0.2246 val_loss=0.2175 BWA=0.9388\n(CWA=0.9397, SWA=0.9379)', '\\n', '[max_50] Epoch  24/50: train_loss=0.2214\nval_loss=0.2184 BWA=0.9332 (CWA=0.9334, SWA=0.9330)', '\\n', '[max_50] Epoch\n25/50: train_loss=0.2207 val_loss=0.2162 BWA=0.9368 (CWA=0.9375, SWA=0.9361)',\n'\\n', '[max_50] Epoch  26/50: train_loss=0.2175 val_loss=0.2086 BWA=0.9403\n(CWA=0.9405, SWA=0.9402)', '\\n', '[max_50] Epoch  27/50: train_loss=0.2151\nval_loss=0.2098 BWA=0.9369 (CWA=0.9373, SWA=0.9365)', '\\n', '[max_50] Epoch\n28/50: train_loss=0.2126 val_loss=0.2040 BWA=0.9411 (CWA=0.9409, SWA=0.9413)',\n'\\n', '[max_50] Epoch  29/50: train_loss=0.2123 val_loss=0.2057 BWA=0.9377\n(CWA=0.9376, SWA=0.9379)', '\\n', '[max_50] Epoch  30/50: train_loss=0.2077\nval_loss=0.2053 BWA=0.9439 (CWA=0.9439, SWA=0.9440)', '\\n', '[max_50] Epoch\n31/50: train_loss=0.2086 val_loss=0.1994 BWA=0.9466 (CWA=0.9469, SWA=0.9463)',\n'\\n', '[max_50] Epoch  32/50: train_loss=0.2044 val_loss=0.2022 BWA=0.9424\n(CWA=0.9428, SWA=0.9421)', '\\n', '[max_50] Epoch  33/50: train_loss=0.2031\nval_loss=0.2025 BWA=0.9433 (CWA=0.9432, SWA=0.9434)', '\\n', '[max_50] Epoch\n34/50: train_loss=0.2042 val_loss=0.2027 BWA=0.9405 (CWA=0.9403, SWA=0.9407)',\n'\\n', '[max_50] Epoch  35/50: train_loss=0.2012 val_loss=0.1976 BWA=0.9457\n(CWA=0.9463, SWA=0.9452)', '\\n', '[max_50] Epoch  36/50: train_loss=0.2008\nval_loss=0.1937 BWA=0.9479 (CWA=0.9481, SWA=0.9477)', '\\n', '[max_50] Epoch\n37/50: train_loss=0.2014 val_loss=0.2034 BWA=0.9445 (CWA=0.9447, SWA=0.9443)',\n'\\n', '[max_50] Epoch  38/50: train_loss=0.1946 val_loss=0.1938 BWA=0.9454\n(CWA=0.9463, SWA=0.9445)', '\\n', '[max_50] Epoch  39/50: train_loss=0.1953\nval_loss=0.1910 BWA=0.9504 (CWA=0.9509, SWA=0.9499)', '\\n', '[max_50] Epoch\n40/50: train_loss=0.1962 val_loss=0.1915 BWA=0.9454 (CWA=0.9459, SWA=0.9448)',\n'\\n', '[max_50] Epoch  41/50: train_loss=0.1990 val_loss=0.1887 BWA=0.9489\n(CWA=0.9496, SWA=0.9483)', '\\n', '[max_50] Epoch  42/50: train_loss=0.1942\nval_loss=0.1969 BWA=0.9473 (CWA=0.9482, SWA=0.9464)', '\\n', '[max_50] Epoch\n43/50: train_loss=0.1919 val_loss=0.1919 BWA=0.9494 (CWA=0.9492, SWA=0.9496)',\n'\\n', '[max_50] Epoch  44/50: train_loss=0.1958 val_loss=0.1913 BWA=0.9476\n(CWA=0.9482, SWA=0.9470)', '\\n', '[max_50] Early stopping triggered at epoch\n44.', '\\n', '[max_50] Test -> loss: 1.4175  BWA: 0.6595 (CWA=0.6814\nSWA=0.6375)', '\\n', '[max_50] Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-13/working/bwa_curve_max_50.png', '\\n', 'All experiment\ndata saved to working/experiment_data.npy', '\\n', 'Execution time: 15 minutes\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '[max_10] Epoch   1/10: train_loss=0.5226\nval_loss=0.4498 BWA=0.8031 (CWA=0.8018, SWA=0.8044)', '\\n', '[max_10] Epoch\n2/10: train_loss=0.4275 val_loss=0.4032 BWA=0.8224 (CWA=0.8219, SWA=0.8229)',\n'\\n', '[max_10] Epoch   3/10: train_loss=0.3933 val_loss=0.3778 BWA=0.8554\n(CWA=0.8546, SWA=0.8562)', '\\n', '[max_10] Epoch   4/10: train_loss=0.3708\nval_loss=0.3614 BWA=0.8721 (CWA=0.8713, SWA=0.8729)', '\\n', '[max_10] Epoch\n5/10: train_loss=0.3540 val_loss=0.3526 BWA=0.8785 (CWA=0.8776, SWA=0.8794)',\n'\\n', '[max_10] Epoch   6/10: train_loss=0.3399 val_loss=0.3320 BWA=0.8879\n(CWA=0.8877, SWA=0.8880)', '\\n', '[max_10] Epoch   7/10: train_loss=0.3268\nval_loss=0.3258 BWA=0.8969 (CWA=0.8960, SWA=0.8977)', '\\n', '[max_10] Epoch\n8/10: train_loss=0.3154 val_loss=0.3067 BWA=0.9002 (CWA=0.9001, SWA=0.9002)',\n'\\n', '[max_10] Epoch   9/10: train_loss=0.3050 val_loss=0.2992 BWA=0.8965\n(CWA=0.8962, SWA=0.8969)', '\\n', '[max_10] Epoch  10/10: train_loss=0.2932\nval_loss=0.2856 BWA=0.9127 (CWA=0.9123, SWA=0.9131)', '\\n', '[max_10] Test ->\nloss: 0.9275  BWA: 0.6403 (CWA=0.6605 SWA=0.6201)', '\\n', '[max_10] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n11/working/bwa_curve_max_10.png', '\\n', '[max_20] Epoch   1/20:\ntrain_loss=0.5277 val_loss=0.4560 BWA=0.8032 (CWA=0.8018, SWA=0.8047)', '\\n',\n'[max_20] Epoch   2/20: train_loss=0.4254 val_loss=0.3983 BWA=0.8317\n(CWA=0.8325, SWA=0.8308)', '\\n', '[max_20] Epoch   3/20: train_loss=0.3843\nval_loss=0.3616 BWA=0.8656 (CWA=0.8654, SWA=0.8658)', '\\n', '[max_20] Epoch\n4/20: train_loss=0.3614 val_loss=0.3454 BWA=0.8729 (CWA=0.8729, SWA=0.8729)',\n'\\n', '[max_20] Epoch   5/20: train_loss=0.3427 val_loss=0.3337 BWA=0.8799\n(CWA=0.8799, SWA=0.8799)', '\\n', '[max_20] Epoch   6/20: train_loss=0.3287\nval_loss=0.3337 BWA=0.8828 (CWA=0.8821, SWA=0.8834)', '\\n', '[max_20] Epoch\n7/20: train_loss=0.3153 val_loss=0.3129 BWA=0.8893 (CWA=0.8892, SWA=0.8894)',\n'\\n', '[max_20] Epoch   8/20: train_loss=0.3039 val_loss=0.2978 BWA=0.9017\n(CWA=0.9013, SWA=0.9020)', '\\n', '[max_20] Epoch   9/20: train_loss=0.2950\nval_loss=0.2877 BWA=0.9075 (CWA=0.9076, SWA=0.9073)', '\\n', '[max_20] Epoch\n10/20: train_loss=0.2893 val_loss=0.2851 BWA=0.9092 (CWA=0.9091, SWA=0.9093)',\n'\\n', '[max_20] Epoch  11/20: train_loss=0.2786 val_loss=0.2731 BWA=0.9145\n(CWA=0.9151, SWA=0.9140)', '\\n', '[max_20] Epoch  12/20: train_loss=0.2733\nval_loss=0.2687 BWA=0.9162 (CWA=0.9165, SWA=0.9158)', '\\n', '[max_20] Epoch\n13/20: train_loss=0.2657 val_loss=0.2703 BWA=0.9187 (CWA=0.9182, SWA=0.9191)',\n'\\n', '[max_20] Epoch  14/20: train_loss=0.2620 val_loss=0.2671 BWA=0.9166\n(CWA=0.9168, SWA=0.9164)', '\\n', '[max_20] Epoch  15/20: train_loss=0.2577\nval_loss=0.2540 BWA=0.9216 (CWA=0.9218, SWA=0.9215)', '\\n', '[max_20] Epoch\n16/20: train_loss=0.2542 val_loss=0.2502 BWA=0.9266 (CWA=0.9265, SWA=0.9266)',\n'\\n', '[max_20] Epoch  17/20: train_loss=0.2513 val_loss=0.2510 BWA=0.9236\n(CWA=0.9229, SWA=0.9242)', '\\n', '[max_20] Epoch  18/20: train_loss=0.2468\nval_loss=0.2427 BWA=0.9290 (CWA=0.9292, SWA=0.9287)', '\\n', '[max_20] Epoch\n19/20: train_loss=0.2445 val_loss=0.2398 BWA=0.9291 (CWA=0.9294, SWA=0.9288)',\n'\\n', '[max_20] Epoch  20/20: train_loss=0.2418 val_loss=0.2407 BWA=0.9291\n(CWA=0.9295, SWA=0.9287)', '\\n', '[max_20] Test -> loss: 1.1019  BWA: 0.6510\n(CWA=0.6722 SWA=0.6298)', '\\n', '[max_20] Curve saved to /home/zxl240011/AI-Scie\nntist-v2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/bwa_curve_max_20.png', '\\n', '[max_30] Epoch\n1/30: train_loss=0.5275 val_loss=0.4531 BWA=0.7925 (CWA=0.7922, SWA=0.7928)',\n'\\n', '[max_30] Epoch   2/30: train_loss=0.4288 val_loss=0.3983 BWA=0.8335\n(CWA=0.8335, SWA=0.8336)', '\\n', '[max_30] Epoch   3/30: train_loss=0.3929\nval_loss=0.3767 BWA=0.8444 (CWA=0.8443, SWA=0.8444)', '\\n', '[max_30] Epoch\n4/30: train_loss=0.3726 val_loss=0.3608 BWA=0.8581 (CWA=0.8582, SWA=0.8581)',\n'\\n', '[max_30] Epoch   5/30: train_loss=0.3560 val_loss=0.3454 BWA=0.8761\n(CWA=0.8754, SWA=0.8768)', '\\n', '[max_30] Epoch   6/30: train_loss=0.3416\nval_loss=0.3355 BWA=0.8785 (CWA=0.8775, SWA=0.8794)', '\\n', '[max_30] Epoch\n7/30: train_loss=0.3274 val_loss=0.3248 BWA=0.8849 (CWA=0.8840, SWA=0.8859)',\n'\\n', '[max_30] Epoch   8/30: train_loss=0.3204 val_loss=0.3128 BWA=0.8938\n(CWA=0.8935, SWA=0.8940)', '\\n', '[max_30] Epoch   9/30: train_loss=0.3062\nval_loss=0.3032 BWA=0.8951 (CWA=0.8945, SWA=0.8957)', '\\n', '[max_30] Epoch\n10/30: train_loss=0.2956 val_loss=0.2896 BWA=0.9034 (CWA=0.9029, SWA=0.9038)',\n'\\n', '[max_30] Epoch  11/30: train_loss=0.2917 val_loss=0.2819 BWA=0.9141\n(CWA=0.9135, SWA=0.9147)', '\\n', '[max_30] Epoch  12/30: train_loss=0.2852\nval_loss=0.2752 BWA=0.9132 (CWA=0.9133, SWA=0.9131)', '\\n', '[max_30] Epoch\n13/30: train_loss=0.2782 val_loss=0.2677 BWA=0.9171 (CWA=0.9168, SWA=0.9173)',\n'\\n', '[max_30] Epoch  14/30: train_loss=0.2728 val_loss=0.2611 BWA=0.9235\n(CWA=0.9234, SWA=0.9235)', '\\n', '[max_30] Epoch  15/30: train_loss=0.2654\nval_loss=0.2620 BWA=0.9203 (CWA=0.9203, SWA=0.9202)', '\\n', '[max_30] Epoch\n16/30: train_loss=0.2607 val_loss=0.2535 BWA=0.9215 (CWA=0.9218, SWA=0.9211)',\n'\\n', '[max_30] Epoch  17/30: train_loss=0.2574 val_loss=0.2511 BWA=0.9214\n(CWA=0.9214, SWA=0.9213)', '\\n', '[max_30] Epoch  18/30: train_loss=0.2529\nval_loss=0.2457 BWA=0.9286 (CWA=0.9281, SWA=0.9291)', '\\n', '[max_30] Epoch\n19/30: train_loss=0.2501 val_loss=0.2404 BWA=0.9274 (CWA=0.9274, SWA=0.9275)',\n'\\n', '[max_30] Epoch  20/30: train_loss=0.2461 val_loss=0.2369 BWA=0.9316\n(CWA=0.9315, SWA=0.9318)', '\\n', '[max_30] Epoch  21/30: train_loss=0.2402\nval_loss=0.2361 BWA=0.9297 (CWA=0.9298, SWA=0.9297)', '\\n', '[max_30] Epoch\n22/30: train_loss=0.2398 val_loss=0.2570 BWA=0.9107 (CWA=0.9123, SWA=0.9091)',\n'\\n', '[max_30] Epoch  23/30: train_loss=0.2354 val_loss=0.2290 BWA=0.9333\n(CWA=0.9331, SWA=0.9336)', '\\n', '[max_30] Epoch  24/30: train_loss=0.2343\nval_loss=0.2351 BWA=0.9278 (CWA=0.9277, SWA=0.9279)', '\\n', '[max_30] Epoch\n25/30: train_loss=0.2290 val_loss=0.2355 BWA=0.9349 (CWA=0.9340, SWA=0.9358)',\n'\\n', '[max_30] Epoch  26/30: train_loss=0.2286 val_loss=0.2336 BWA=0.9284\n(CWA=0.9290, SWA=0.9279)', '\\n', '[max_30] Epoch  27/30: train_loss=0.2241\nval_loss=0.2187 BWA=0.9362 (CWA=0.9362, SWA=0.9363)', '\\n', '[max_30] Epoch\n28/30: train_loss=0.2243 val_loss=0.2191 BWA=0.9347 (CWA=0.9347, SWA=0.9347)',\n'\\n', '[max_30] Epoch  29/30: train_loss=0.2231 val_loss=0.2142 BWA=0.9333\n(CWA=0.9332, SWA=0.9334)', '\\n', '[max_30] Epoch  30/30: train_loss=0.2187\nval_loss=0.2245 BWA=0.9355 (CWA=0.9352, SWA=0.9359)', '\\n', '[max_30] Test ->\nloss: 1.1984  BWA: 0.6511 (CWA=0.6723 SWA=0.6299)', '\\n', '[max_30] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n11/working/bwa_curve_max_30.png', '\\n', '[max_40] Epoch   1/40:\ntrain_loss=0.5203 val_loss=0.4405 BWA=0.8011 (CWA=0.8006, SWA=0.8017)', '\\n',\n'[max_40] Epoch   2/40: train_loss=0.4223 val_loss=0.3968 BWA=0.8360\n(CWA=0.8361, SWA=0.8360)', '\\n', '[max_40] Epoch   3/40: train_loss=0.3945\nval_loss=0.3812 BWA=0.8457 (CWA=0.8456, SWA=0.8457)', '\\n', '[max_40] Epoch\n4/40: train_loss=0.3788 val_loss=0.3731 BWA=0.8561 (CWA=0.8553, SWA=0.8568)',\n'\\n', '[max_40] Epoch   5/40: train_loss=0.3627 val_loss=0.3584 BWA=0.8546\n(CWA=0.8546, SWA=0.8545)', '\\n', '[max_40] Epoch   6/40: train_loss=0.3469\nval_loss=0.3414 BWA=0.8785 (CWA=0.8784, SWA=0.8786)', '\\n', '[max_40] Epoch\n7/40: train_loss=0.3369 val_loss=0.3285 BWA=0.8839 (CWA=0.8837, SWA=0.8841)',\n'\\n', '[max_40] Epoch   8/40: train_loss=0.3228 val_loss=0.3168 BWA=0.8902\n(CWA=0.8902, SWA=0.8901)', '\\n', '[max_40] Epoch   9/40: train_loss=0.3082\nval_loss=0.3073 BWA=0.8907 (CWA=0.8913, SWA=0.8902)', '\\n', '[max_40] Epoch\n10/40: train_loss=0.2988 val_loss=0.2962 BWA=0.8986 (CWA=0.8988, SWA=0.8983)',\n'\\n', '[max_40] Epoch  11/40: train_loss=0.2902 val_loss=0.2864 BWA=0.9039\n(CWA=0.9041, SWA=0.9038)', '\\n', '[max_40] Epoch  12/40: train_loss=0.2800\nval_loss=0.2758 BWA=0.9109 (CWA=0.9113, SWA=0.9104)', '\\n', '[max_40] Epoch\n13/40: train_loss=0.2749 val_loss=0.2712 BWA=0.9092 (CWA=0.9109, SWA=0.9076)',\n'\\n', '[max_40] Epoch  14/40: train_loss=0.2673 val_loss=0.2601 BWA=0.9143\n(CWA=0.9143, SWA=0.9143)', '\\n', '[max_40] Epoch  15/40: train_loss=0.2614\nval_loss=0.2679 BWA=0.9048 (CWA=0.9057, SWA=0.9038)', '\\n', '[max_40] Epoch\n16/40: train_loss=0.2553 val_loss=0.2458 BWA=0.9251 (CWA=0.9254, SWA=0.9248)',\n'\\n', '[max_40] Epoch  17/40: train_loss=0.2488 val_loss=0.2462 BWA=0.9258\n(CWA=0.9256, SWA=0.9259)', '\\n', '[max_40] Epoch  18/40: train_loss=0.2474\nval_loss=0.2466 BWA=0.9241 (CWA=0.9237, SWA=0.9244)', '\\n', '[max_40] Epoch\n19/40: train_loss=0.2434 val_loss=0.2312 BWA=0.9261 (CWA=0.9263, SWA=0.9258)',\n'\\n', '[max_40] Epoch  20/40: train_loss=0.2401 val_loss=0.2337 BWA=0.9263\n(CWA=0.9264, SWA=0.9262)', '\\n', '[max_40] Epoch  21/40: train_loss=0.2354\nval_loss=0.2350 BWA=0.9308 (CWA=0.9306, SWA=0.9310)', '\\n', '[max_40] Epoch\n22/40: train_loss=0.2305 val_loss=0.2208 BWA=0.9370 (CWA=0.9374, SWA=0.9365)',\n'\\n', '[max_40] Epoch  23/40: train_loss=0.2297 val_loss=0.2222 BWA=0.9324\n(CWA=0.9326, SWA=0.9322)', '\\n', '[max_40] Epoch  24/40: train_loss=0.2263\nval_loss=0.2227 BWA=0.9343 (CWA=0.9344, SWA=0.9342)', '\\n', '[max_40] Epoch\n25/40: train_loss=0.2231 val_loss=0.2205 BWA=0.9326 (CWA=0.9334, SWA=0.9318)',\n'\\n', '[max_40] Epoch  26/40: train_loss=0.2202 val_loss=0.2278 BWA=0.9251\n(CWA=0.9265, SWA=0.9236)', '\\n', '[max_40] Epoch  27/40: train_loss=0.2201\nval_loss=0.2128 BWA=0.9366 (CWA=0.9375, SWA=0.9358)', '\\n', '[max_40] Early\nstopping triggered at epoch 27.', '\\n', '[max_40] Test -> loss: 1.1653  BWA:\n0.6525 (CWA=0.6738 SWA=0.6312)', '\\n', '[max_40] Curve saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n11/working/bwa_curve_max_40.png', '\\n', '[max_50] Epoch   1/50:\ntrain_loss=0.5278 val_loss=0.4579 BWA=0.8014 (CWA=0.8001, SWA=0.8028)', '\\n',\n'[max_50] Epoch   2/50: train_loss=0.4323 val_loss=0.4033 BWA=0.8316\n(CWA=0.8319, SWA=0.8314)', '\\n', '[max_50] Epoch   3/50: train_loss=0.3989\nval_loss=0.3861 BWA=0.8452 (CWA=0.8447, SWA=0.8457)', '\\n', '[max_50] Epoch\n4/50: train_loss=0.3824 val_loss=0.3701 BWA=0.8517 (CWA=0.8514, SWA=0.8521)',\n'\\n', '[max_50] Epoch   5/50: train_loss=0.3639 val_loss=0.3561 BWA=0.8639\n(CWA=0.8634, SWA=0.8643)', '\\n', '[max_50] Epoch   6/50: train_loss=0.3477\nval_loss=0.3370 BWA=0.8757 (CWA=0.8752, SWA=0.8762)', '\\n', '[max_50] Epoch\n7/50: train_loss=0.3325 val_loss=0.3255 BWA=0.8825 (CWA=0.8826, SWA=0.8823)',\n'\\n', '[max_50] Epoch   8/50: train_loss=0.3226 val_loss=0.3183 BWA=0.8871\n(CWA=0.8874, SWA=0.8868)', '\\n', '[max_50] Epoch   9/50: train_loss=0.3074\nval_loss=0.2982 BWA=0.8964 (CWA=0.8966, SWA=0.8963)', '\\n', '[max_50] Epoch\n10/50: train_loss=0.2986 val_loss=0.2928 BWA=0.8996 (CWA=0.8999, SWA=0.8994)',\n'\\n', '[max_50] Epoch  11/50: train_loss=0.2885 val_loss=0.2824 BWA=0.9047\n(CWA=0.9048, SWA=0.9047)', '\\n', '[max_50] Epoch  12/50: train_loss=0.2795\nval_loss=0.2738 BWA=0.9111 (CWA=0.9113, SWA=0.9109)', '\\n', '[max_50] Epoch\n13/50: train_loss=0.2709 val_loss=0.2722 BWA=0.9092 (CWA=0.9107, SWA=0.9077)',\n'\\n', '[max_50] Epoch  14/50: train_loss=0.2677 val_loss=0.2677 BWA=0.9188\n(CWA=0.9187, SWA=0.9189)', '\\n', '[max_50] Epoch  15/50: train_loss=0.2602\nval_loss=0.2624 BWA=0.9129 (CWA=0.9130, SWA=0.9128)', '\\n', '[max_50] Epoch\n16/50: train_loss=0.2516 val_loss=0.2448 BWA=0.9245 (CWA=0.9246, SWA=0.9244)',\n'\\n', '[max_50] Epoch  17/50: train_loss=0.2478 val_loss=0.2460 BWA=0.9201\n(CWA=0.9203, SWA=0.9199)', '\\n', '[max_50] Epoch  18/50: train_loss=0.2451\nval_loss=0.2383 BWA=0.9209 (CWA=0.9212, SWA=0.9206)', '\\n', '[max_50] Epoch\n19/50: train_loss=0.2433 val_loss=0.2372 BWA=0.9249 (CWA=0.9253, SWA=0.9244)',\n'\\n', '[max_50] Epoch  20/50: train_loss=0.2404 val_loss=0.2363 BWA=0.9315\n(CWA=0.9311, SWA=0.9319)', '\\n', '[max_50] Epoch  21/50: train_loss=0.2380\nval_loss=0.2266 BWA=0.9311 (CWA=0.9312, SWA=0.9311)', '\\n', '[max_50] Epoch\n22/50: train_loss=0.2340 val_loss=0.2293 BWA=0.9281 (CWA=0.9285, SWA=0.9277)',\n'\\n', '[max_50] Epoch  23/50: train_loss=0.2317 val_loss=0.2258 BWA=0.9331\n(CWA=0.9329, SWA=0.9334)', '\\n', '[max_50] Epoch  24/50: train_loss=0.2326\nval_loss=0.2191 BWA=0.9370 (CWA=0.9368, SWA=0.9372)', '\\n', '[max_50] Epoch\n25/50: train_loss=0.2245 val_loss=0.2218 BWA=0.9325 (CWA=0.9322, SWA=0.9329)',\n'\\n', '[max_50] Epoch  26/50: train_loss=0.2235 val_loss=0.2140 BWA=0.9386\n(CWA=0.9384, SWA=0.9388)', '\\n', '[max_50] Epoch  27/50: train_loss=0.2214\nval_loss=0.2190 BWA=0.9363 (CWA=0.9356, SWA=0.9370)', '\\n', '[max_50] Epoch\n28/50: train_loss=0.2229 val_loss=0.2196 BWA=0.9382 (CWA=0.9378, SWA=0.9386)',\n'\\n', '[max_50] Epoch  29/50: train_loss=0.2190 val_loss=0.2124 BWA=0.9379\n(CWA=0.9377, SWA=0.9380)', '\\n', '[max_50] Epoch  30/50: train_loss=0.2178\nval_loss=0.2075 BWA=0.9414 (CWA=0.9411, SWA=0.9416)', '\\n', '[max_50] Epoch\n31/50: train_loss=0.2159 val_loss=0.2089 BWA=0.9367 (CWA=0.9368, SWA=0.9365)',\n'\\n', '[max_50] Epoch  32/50: train_loss=0.2122 val_loss=0.2345 BWA=0.9289\n(CWA=0.9286, SWA=0.9293)', '\\n', '[max_50] Epoch  33/50: train_loss=0.2124\nval_loss=0.2087 BWA=0.9377 (CWA=0.9379, SWA=0.9376)', '\\n', '[max_50] Epoch\n34/50: train_loss=0.2119 val_loss=0.2088 BWA=0.9400 (CWA=0.9400, SWA=0.9399)',\n'\\n', '[max_50] Epoch  35/50: train_loss=0.2084 val_loss=0.2044 BWA=0.9365\n(CWA=0.9364, SWA=0.9365)', '\\n', '[max_50] Early stopping triggered at epoch\n35.', '\\n', '[max_50] Test -> loss: 1.2060  BWA: 0.6556 (CWA=0.6771\nSWA=0.6341)', '\\n', '[max_50] Curve saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-11/working/bwa_curve_max_50.png', '\\n', 'All experiment\ndata saved to working/experiment_data.npy', '\\n', 'Execution time: 5 minutes\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '[max_10] Epoch   1/10: train_loss=0.5341\nval_loss=0.4606 BWA=0.7994 (CWA=0.7984, SWA=0.8004)', '\\n', '[max_10] Epoch\n2/10: train_loss=0.4354 val_loss=0.4059 BWA=0.8323 (CWA=0.8325, SWA=0.8321)',\n'\\n', '[max_10] Epoch   3/10: train_loss=0.3963 val_loss=0.3784 BWA=0.8452\n(CWA=0.8458, SWA=0.8447)', '\\n', '[max_10] Epoch   4/10: train_loss=0.3704\nval_loss=0.3518 BWA=0.8659 (CWA=0.8658, SWA=0.8660)', '\\n', '[max_10] Epoch\n5/10: train_loss=0.3497 val_loss=0.3500 BWA=0.8524 (CWA=0.8535, SWA=0.8512)',\n'\\n', '[max_10] Epoch   6/10: train_loss=0.3302 val_loss=0.3177 BWA=0.8807\n(CWA=0.8808, SWA=0.8806)', '\\n', '[max_10] Epoch   7/10: train_loss=0.3134\nval_loss=0.3026 BWA=0.8948 (CWA=0.8952, SWA=0.8944)', '\\n', '[max_10] Epoch\n8/10: train_loss=0.3000 val_loss=0.2876 BWA=0.9040 (CWA=0.9045, SWA=0.9034)',\n'\\n', '[max_10] Epoch   9/10: train_loss=0.2917 val_loss=0.2763 BWA=0.9083\n(CWA=0.9086, SWA=0.9080)', '\\n', '[max_10] Epoch  10/10: train_loss=0.2823\nval_loss=0.2648 BWA=0.9142 (CWA=0.9143, SWA=0.9140)', '\\n', '[max_10] Test ->\nloss: 0.9065  BWA: 0.6440 (CWA=0.6648 SWA=0.6232)', '\\n', '[max_10] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n12/working/bwa_curve_max_10.png', '\\n', '[max_20] Epoch   1/20:\ntrain_loss=0.5288 val_loss=0.4518 BWA=0.7959 (CWA=0.7951, SWA=0.7968)', '\\n',\n'[max_20] Epoch   2/20: train_loss=0.4178 val_loss=0.3901 BWA=0.8367\n(CWA=0.8370, SWA=0.8365)', '\\n', '[max_20] Epoch   3/20: train_loss=0.3804\nval_loss=0.3641 BWA=0.8630 (CWA=0.8630, SWA=0.8630)', '\\n', '[max_20] Epoch\n4/20: train_loss=0.3578 val_loss=0.3453 BWA=0.8655 (CWA=0.8659, SWA=0.8652)',\n'\\n', '[max_20] Epoch   5/20: train_loss=0.3395 val_loss=0.3394 BWA=0.8793\n(CWA=0.8795, SWA=0.8791)', '\\n', '[max_20] Epoch   6/20: train_loss=0.3240\nval_loss=0.3210 BWA=0.8904 (CWA=0.8906, SWA=0.8902)', '\\n', '[max_20] Epoch\n7/20: train_loss=0.3105 val_loss=0.3035 BWA=0.8981 (CWA=0.8982, SWA=0.8980)',\n'\\n', '[max_20] Epoch   8/20: train_loss=0.3015 val_loss=0.2933 BWA=0.9095\n(CWA=0.9097, SWA=0.9093)', '\\n', '[max_20] Epoch   9/20: train_loss=0.2925\nval_loss=0.2830 BWA=0.9090 (CWA=0.9095, SWA=0.9084)', '\\n', '[max_20] Epoch\n10/20: train_loss=0.2808 val_loss=0.2727 BWA=0.9117 (CWA=0.9124, SWA=0.9111)',\n'\\n', '[max_20] Epoch  11/20: train_loss=0.2786 val_loss=0.2700 BWA=0.9135\n(CWA=0.9131, SWA=0.9139)', '\\n', '[max_20] Epoch  12/20: train_loss=0.2726\nval_loss=0.2664 BWA=0.9198 (CWA=0.9196, SWA=0.9200)', '\\n', '[max_20] Epoch\n13/20: train_loss=0.2630 val_loss=0.2552 BWA=0.9169 (CWA=0.9169, SWA=0.9169)',\n'\\n', '[max_20] Epoch  14/20: train_loss=0.2610 val_loss=0.2538 BWA=0.9216\n(CWA=0.9216, SWA=0.9217)', '\\n', '[max_20] Epoch  15/20: train_loss=0.2555\nval_loss=0.2448 BWA=0.9286 (CWA=0.9283, SWA=0.9288)', '\\n', '[max_20] Epoch\n16/20: train_loss=0.2490 val_loss=0.2423 BWA=0.9284 (CWA=0.9287, SWA=0.9280)',\n'\\n', '[max_20] Epoch  17/20: train_loss=0.2457 val_loss=0.2376 BWA=0.9286\n(CWA=0.9287, SWA=0.9285)', '\\n', '[max_20] Epoch  18/20: train_loss=0.2411\nval_loss=0.2451 BWA=0.9290 (CWA=0.9285, SWA=0.9296)', '\\n', '[max_20] Epoch\n19/20: train_loss=0.2395 val_loss=0.2363 BWA=0.9287 (CWA=0.9290, SWA=0.9284)',\n'\\n', '[max_20] Epoch  20/20: train_loss=0.2353 val_loss=0.2286 BWA=0.9345\n(CWA=0.9345, SWA=0.9345)', '\\n', '[max_20] Test -> loss: 1.1884  BWA: 0.6520\n(CWA=0.6737 SWA=0.6304)', '\\n', '[max_20] Curve saved to /home/zxl240011/AI-Scie\nntist-v2/experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/0-\nrun/process_ForkProcess-12/working/bwa_curve_max_20.png', '\\n', '[max_30] Epoch\n1/30: train_loss=0.5309 val_loss=0.4589 BWA=0.7875 (CWA=0.7868, SWA=0.7882)',\n'\\n', '[max_30] Epoch   2/30: train_loss=0.4404 val_loss=0.4120 BWA=0.8280\n(CWA=0.8274, SWA=0.8286)', '\\n', '[max_30] Epoch   3/30: train_loss=0.4059\nval_loss=0.3919 BWA=0.8330 (CWA=0.8334, SWA=0.8326)', '\\n', '[max_30] Epoch\n4/30: train_loss=0.3823 val_loss=0.3719 BWA=0.8609 (CWA=0.8599, SWA=0.8619)',\n'\\n', '[max_30] Epoch   5/30: train_loss=0.3632 val_loss=0.3507 BWA=0.8712\n(CWA=0.8708, SWA=0.8716)', '\\n', '[max_30] Epoch   6/30: train_loss=0.3448\nval_loss=0.3398 BWA=0.8689 (CWA=0.8698, SWA=0.8680)', '\\n', '[max_30] Epoch\n7/30: train_loss=0.3316 val_loss=0.3245 BWA=0.8817 (CWA=0.8821, SWA=0.8814)',\n'\\n', '[max_30] Epoch   8/30: train_loss=0.3191 val_loss=0.3207 BWA=0.8710\n(CWA=0.8710, SWA=0.8709)', '\\n', '[max_30] Epoch   9/30: train_loss=0.3063\nval_loss=0.2966 BWA=0.8940 (CWA=0.8945, SWA=0.8934)', '\\n', '[max_30] Epoch\n10/30: train_loss=0.2934 val_loss=0.2891 BWA=0.9019 (CWA=0.9026, SWA=0.9013)',\n'\\n', '[max_30] Epoch  11/30: train_loss=0.2857 val_loss=0.2775 BWA=0.9105\n(CWA=0.9109, SWA=0.9101)', '\\n', '[max_30] Epoch  12/30: train_loss=0.2791\nval_loss=0.2688 BWA=0.9113 (CWA=0.9125, SWA=0.9101)', '\\n', '[max_30] Epoch\n13/30: train_loss=0.2672 val_loss=0.2616 BWA=0.9205 (CWA=0.9207, SWA=0.9203)',\n'\\n', '[max_30] Epoch  14/30: train_loss=0.2627 val_loss=0.2538 BWA=0.9228\n(CWA=0.9231, SWA=0.9226)', '\\n', '[max_30] Epoch  15/30: train_loss=0.2581\nval_loss=0.2505 BWA=0.9227 (CWA=0.9237, SWA=0.9218)', '\\n', '[max_30] Epoch\n16/30: train_loss=0.2531 val_loss=0.2438 BWA=0.9260 (CWA=0.9270, SWA=0.9251)',\n'\\n', '[max_30] Epoch  17/30: train_loss=0.2472 val_loss=0.2393 BWA=0.9256\n(CWA=0.9259, SWA=0.9252)', '\\n', '[max_30] Epoch  18/30: train_loss=0.2451\nval_loss=0.2379 BWA=0.9245 (CWA=0.9247, SWA=0.9244)', '\\n', '[max_30] Epoch\n19/30: train_loss=0.2443 val_loss=0.2339 BWA=0.9278 (CWA=0.9279, SWA=0.9276)',\n'\\n', '[max_30] Epoch  20/30: train_loss=0.2387 val_loss=0.2299 BWA=0.9295\n(CWA=0.9303, SWA=0.9287)', '\\n', '[max_30] Epoch  21/30: train_loss=0.2367\nval_loss=0.2269 BWA=0.9332 (CWA=0.9336, SWA=0.9327)', '\\n', '[max_30] Epoch\n22/30: train_loss=0.2316 val_loss=0.2355 BWA=0.9306 (CWA=0.9305, SWA=0.9307)',\n'\\n', '[max_30] Epoch  23/30: train_loss=0.2300 val_loss=0.2243 BWA=0.9348\n(CWA=0.9350, SWA=0.9345)', '\\n', '[max_30] Epoch  24/30: train_loss=0.2252\nval_loss=0.2277 BWA=0.9352 (CWA=0.9354, SWA=0.9349)', '\\n', '[max_30] Epoch\n25/30: train_loss=0.2217 val_loss=0.2173 BWA=0.9357 (CWA=0.9362, SWA=0.9353)',\n'\\n', '[max_30] Epoch  26/30: train_loss=0.2251 val_loss=0.2114 BWA=0.9360\n(CWA=0.9367, SWA=0.9354)', '\\n', '[max_30] Epoch  27/30: train_loss=0.2213\nval_loss=0.2154 BWA=0.9370 (CWA=0.9371, SWA=0.9369)', '\\n', '[max_30] Epoch\n28/30: train_loss=0.2197 val_loss=0.2077 BWA=0.9395 (CWA=0.9398, SWA=0.9391)',\n'\\n', '[max_30] Epoch  29/30: train_loss=0.2178 val_loss=0.2034 BWA=0.9387\n(CWA=0.9395, SWA=0.9379)', '\\n', '[max_30] Epoch  30/30: train_loss=0.2153\nval_loss=0.2096 BWA=0.9424 (CWA=0.9426, SWA=0.9422)', '\\n', '[max_30] Test ->\nloss: 1.2242  BWA: 0.6570 (CWA=0.6784 SWA=0.6355)', '\\n', '[max_30] Curve saved\nto /home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n12/working/bwa_curve_max_30.png', '\\n', '[max_40] Epoch   1/40:\ntrain_loss=0.5268 val_loss=0.4587 BWA=0.7939 (CWA=0.7924, SWA=0.7954)', '\\n',\n'[max_40] Epoch   2/40: train_loss=0.4381 val_loss=0.4108 BWA=0.8315\n(CWA=0.8302, SWA=0.8327)', '\\n', '[max_40] Epoch   3/40: train_loss=0.3999\nval_loss=0.3879 BWA=0.8442 (CWA=0.8444, SWA=0.8439)', '\\n', '[max_40] Epoch\n4/40: train_loss=0.3735 val_loss=0.3668 BWA=0.8527 (CWA=0.8532, SWA=0.8523)',\n'\\n', '[max_40] Epoch   5/40: train_loss=0.3520 val_loss=0.3415 BWA=0.8765\n(CWA=0.8761, SWA=0.8769)', '\\n', '[max_40] Epoch   6/40: train_loss=0.3320\nval_loss=0.3243 BWA=0.8819 (CWA=0.8821, SWA=0.8816)', '\\n', '[max_40] Epoch\n7/40: train_loss=0.3210 val_loss=0.3092 BWA=0.8898 (CWA=0.8902, SWA=0.8895)',\n'\\n', '[max_40] Epoch   8/40: train_loss=0.3083 val_loss=0.2988 BWA=0.9022\n(CWA=0.9021, SWA=0.9022)', '\\n', '[max_40] Epoch   9/40: train_loss=0.2988\nval_loss=0.2904 BWA=0.9018 (CWA=0.9021, SWA=0.9015)', '\\n', '[max_40] Epoch\n10/40: train_loss=0.2899 val_loss=0.2854 BWA=0.9035 (CWA=0.9044, SWA=0.9025)',\n'\\n', '[max_40] Epoch  11/40: train_loss=0.2858 val_loss=0.2763 BWA=0.9134\n(CWA=0.9131, SWA=0.9136)', '\\n', '[max_40] Epoch  12/40: train_loss=0.2756\nval_loss=0.2812 BWA=0.9071 (CWA=0.9081, SWA=0.9062)', '\\n', '[max_40] Epoch\n13/40: train_loss=0.2709 val_loss=0.2689 BWA=0.9142 (CWA=0.9137, SWA=0.9147)',\n'\\n', '[max_40] Epoch  14/40: train_loss=0.2649 val_loss=0.2599 BWA=0.9184\n(CWA=0.9186, SWA=0.9181)', '\\n', '[max_40] Epoch  15/40: train_loss=0.2631\nval_loss=0.2554 BWA=0.9208 (CWA=0.9211, SWA=0.9205)', '\\n', '[max_40] Epoch\n16/40: train_loss=0.2577 val_loss=0.2505 BWA=0.9237 (CWA=0.9233, SWA=0.9242)',\n'\\n', '[max_40] Epoch  17/40: train_loss=0.2567 val_loss=0.2473 BWA=0.9265\n(CWA=0.9263, SWA=0.9268)', '\\n', '[max_40] Epoch  18/40: train_loss=0.2499\nval_loss=0.2396 BWA=0.9270 (CWA=0.9271, SWA=0.9269)', '\\n', '[max_40] Epoch\n19/40: train_loss=0.2464 val_loss=0.2360 BWA=0.9288 (CWA=0.9288, SWA=0.9287)',\n'\\n', '[max_40] Epoch  20/40: train_loss=0.2451 val_loss=0.2349 BWA=0.9289\n(CWA=0.9287, SWA=0.9291)', '\\n', '[max_40] Epoch  21/40: train_loss=0.2420\nval_loss=0.2393 BWA=0.9178 (CWA=0.9186, SWA=0.9170)', '\\n', '[max_40] Epoch\n22/40: train_loss=0.2358 val_loss=0.2309 BWA=0.9280 (CWA=0.9281, SWA=0.9280)',\n'\\n', '[max_40] Epoch  23/40: train_loss=0.2311 val_loss=0.2287 BWA=0.9307\n(CWA=0.9305, SWA=0.9309)', '\\n', '[max_40] Epoch  24/40: train_loss=0.2306\nval_loss=0.2236 BWA=0.9313 (CWA=0.9320, SWA=0.9306)', '\\n', '[max_40] Epoch\n25/40: train_loss=0.2272 val_loss=0.2304 BWA=0.9244 (CWA=0.9253, SWA=0.9234)',\n'\\n', '[max_40] Epoch  26/40: train_loss=0.2255 val_loss=0.2228 BWA=0.9325\n(CWA=0.9324, SWA=0.9326)', '\\n', '[max_40] Epoch  27/40: train_loss=0.2240\nval_loss=0.2173 BWA=0.9357 (CWA=0.9359, SWA=0.9355)', '\\n', '[max_40] Epoch\n28/40: train_loss=0.2219 val_loss=0.2196 BWA=0.9359 (CWA=0.9364, SWA=0.9355)',\n'\\n', '[max_40] Epoch  29/40: train_loss=0.2218 val_loss=0.2118 BWA=0.9338\n(CWA=0.9340, SWA=0.9335)', '\\n', '[max_40] Epoch  30/40: train_loss=0.2175\nval_loss=0.2095 BWA=0.9390 (CWA=0.9393, SWA=0.9386)', '\\n', '[max_40] Epoch\n31/40: train_loss=0.2186 val_loss=0.2099 BWA=0.9371 (CWA=0.9372, SWA=0.9370)',\n'\\n', '[max_40] Epoch  32/40: train_loss=0.2165 val_loss=0.2075 BWA=0.9407\n(CWA=0.9405, SWA=0.9410)', '\\n', '[max_40] Epoch  33/40: train_loss=0.2144\nval_loss=0.2052 BWA=0.9420 (CWA=0.9420, SWA=0.9419)', '\\n', '[max_40] Epoch\n34/40: train_loss=0.2139 val_loss=0.2224 BWA=0.9343 (CWA=0.9339, SWA=0.9347)',\n'\\n', '[max_40] Epoch  35/40: train_loss=0.2129 val_loss=0.2058 BWA=0.9420\n(CWA=0.9423, SWA=0.9416)', '\\n', '[max_40] Epoch  36/40: train_loss=0.2063\nval_loss=0.1997 BWA=0.9439 (CWA=0.9439, SWA=0.9439)', '\\n', '[max_40] Epoch\n37/40: train_loss=0.2080 val_loss=0.2034 BWA=0.9424 (CWA=0.9422, SWA=0.9425)',\n'\\n', '[max_40] Epoch  38/40: train_loss=0.2048 val_loss=0.1982 BWA=0.9423\n(CWA=0.9425, SWA=0.9422)', '\\n', '[max_40] Epoch  39/40: train_loss=0.2026\nval_loss=0.1989 BWA=0.9439 (CWA=0.9440, SWA=0.9438)', '\\n', '[max_40] Epoch\n40/40: train_loss=0.2049 val_loss=0.2001 BWA=0.9413 (CWA=0.9414, SWA=0.9412)',\n'\\n', '[max_40] Test -> loss: 1.2933  BWA: 0.6544 (CWA=0.6764 SWA=0.6323)',\n'\\n', '[max_40] Curve saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-\n08-30_17-49-45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n12/working/bwa_curve_max_40.png', '\\n', '[max_50] Epoch   1/50:\ntrain_loss=0.5255 val_loss=0.4480 BWA=0.8068 (CWA=0.8050, SWA=0.8087)', '\\n',\n'[max_50] Epoch   2/50: train_loss=0.4229 val_loss=0.3946 BWA=0.8390\n(CWA=0.8388, SWA=0.8393)', '\\n', '[max_50] Epoch   3/50: train_loss=0.3898\nval_loss=0.3744 BWA=0.8526 (CWA=0.8522, SWA=0.8529)', '\\n', '[max_50] Epoch\n4/50: train_loss=0.3667 val_loss=0.3525 BWA=0.8669 (CWA=0.8672, SWA=0.8666)',\n'\\n', '[max_50] Epoch   5/50: train_loss=0.3501 val_loss=0.3391 BWA=0.8678\n(CWA=0.8680, SWA=0.8676)', '\\n', '[max_50] Epoch   6/50: train_loss=0.3325\nval_loss=0.3234 BWA=0.8855 (CWA=0.8854, SWA=0.8857)', '\\n', '[max_50] Epoch\n7/50: train_loss=0.3240 val_loss=0.3143 BWA=0.8809 (CWA=0.8809, SWA=0.8808)',\n'\\n', '[max_50] Epoch   8/50: train_loss=0.3131 val_loss=0.2991 BWA=0.8943\n(CWA=0.8946, SWA=0.8940)', '\\n', '[max_50] Epoch   9/50: train_loss=0.2993\nval_loss=0.2922 BWA=0.9032 (CWA=0.9029, SWA=0.9036)', '\\n', '[max_50] Epoch\n10/50: train_loss=0.2908 val_loss=0.2760 BWA=0.9061 (CWA=0.9065, SWA=0.9056)',\n'\\n', '[max_50] Epoch  11/50: train_loss=0.2827 val_loss=0.2695 BWA=0.9145\n(CWA=0.9148, SWA=0.9141)', '\\n', '[max_50] Epoch  12/50: train_loss=0.2753\nval_loss=0.2616 BWA=0.9111 (CWA=0.9119, SWA=0.9102)', '\\n', '[max_50] Epoch\n13/50: train_loss=0.2671 val_loss=0.2578 BWA=0.9138 (CWA=0.9142, SWA=0.9134)',\n'\\n', '[max_50] Epoch  14/50: train_loss=0.2637 val_loss=0.2542 BWA=0.9131\n(CWA=0.9137, SWA=0.9124)', '\\n', '[max_50] Epoch  15/50: train_loss=0.2560\nval_loss=0.2464 BWA=0.9236 (CWA=0.9235, SWA=0.9237)', '\\n', '[max_50] Epoch\n16/50: train_loss=0.2507 val_loss=0.2389 BWA=0.9249 (CWA=0.9256, SWA=0.9242)',\n'\\n', '[max_50] Epoch  17/50: train_loss=0.2469 val_loss=0.2411 BWA=0.9197\n(CWA=0.9202, SWA=0.9192)', '\\n', '[max_50] Epoch  18/50: train_loss=0.2420\nval_loss=0.2367 BWA=0.9270 (CWA=0.9275, SWA=0.9265)', '\\n', '[max_50] Epoch\n19/50: train_loss=0.2402 val_loss=0.2405 BWA=0.9271 (CWA=0.9261, SWA=0.9281)',\n'\\n', '[max_50] Epoch  20/50: train_loss=0.2347 val_loss=0.2259 BWA=0.9289\n(CWA=0.9293, SWA=0.9286)', '\\n', '[max_50] Epoch  21/50: train_loss=0.2327\nval_loss=0.2394 BWA=0.9224 (CWA=0.9219, SWA=0.9228)', '\\n', '[max_50] Epoch\n22/50: train_loss=0.2347 val_loss=0.2242 BWA=0.9316 (CWA=0.9315, SWA=0.9318)',\n'\\n', '[max_50] Epoch  23/50: train_loss=0.2294 val_loss=0.2244 BWA=0.9297\n(CWA=0.9300, SWA=0.9294)', '\\n', '[max_50] Epoch  24/50: train_loss=0.2233\nval_loss=0.2188 BWA=0.9348 (CWA=0.9352, SWA=0.9344)', '\\n', '[max_50] Epoch\n25/50: train_loss=0.2260 val_loss=0.2163 BWA=0.9382 (CWA=0.9383, SWA=0.9381)',\n'\\n', '[max_50] Epoch  26/50: train_loss=0.2214 val_loss=0.2186 BWA=0.9322\n(CWA=0.9323, SWA=0.9321)', '\\n', '[max_50] Epoch  27/50: train_loss=0.2222\nval_loss=0.2180 BWA=0.9353 (CWA=0.9353, SWA=0.9353)', '\\n', '[max_50] Epoch\n28/50: train_loss=0.2212 val_loss=0.2152 BWA=0.9351 (CWA=0.9356, SWA=0.9346)',\n'\\n', '[max_50] Epoch  29/50: train_loss=0.2155 val_loss=0.2084 BWA=0.9389\n(CWA=0.9394, SWA=0.9384)', '\\n', '[max_50] Epoch  30/50: train_loss=0.2166\nval_loss=0.2173 BWA=0.9386 (CWA=0.9384, SWA=0.9387)', '\\n', '[max_50] Epoch\n31/50: train_loss=0.2119 val_loss=0.2104 BWA=0.9372 (CWA=0.9374, SWA=0.9369)',\n'\\n', '[max_50] Epoch  32/50: train_loss=0.2125 val_loss=0.2049 BWA=0.9438\n(CWA=0.9439, SWA=0.9436)', '\\n', '[max_50] Epoch  33/50: train_loss=0.2113\nval_loss=0.2089 BWA=0.9364 (CWA=0.9371, SWA=0.9356)', '\\n', '[max_50] Epoch\n34/50: train_loss=0.2101 val_loss=0.2041 BWA=0.9448 (CWA=0.9454, SWA=0.9443)',\n'\\n', '[max_50] Epoch  35/50: train_loss=0.2078 val_loss=0.2086 BWA=0.9330\n(CWA=0.9334, SWA=0.9327)', '\\n', '[max_50] Epoch  36/50: train_loss=0.2064\nval_loss=0.2079 BWA=0.9391 (CWA=0.9394, SWA=0.9388)', '\\n', '[max_50] Epoch\n37/50: train_loss=0.2115 val_loss=0.2068 BWA=0.9386 (CWA=0.9392, SWA=0.9380)',\n'\\n', '[max_50] Epoch  38/50: train_loss=0.2076 val_loss=0.2034 BWA=0.9447\n(CWA=0.9447, SWA=0.9447)', '\\n', '[max_50] Epoch  39/50: train_loss=0.2052\nval_loss=0.2047 BWA=0.9419 (CWA=0.9417, SWA=0.9420)', '\\n', '[max_50] Early\nstopping triggered at epoch 39.', '\\n', '[max_50] Test -> loss: 1.2331  BWA:\n0.6603 (CWA=0.6821 SWA=0.6385)', '\\n', '[max_50] Curve saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-30_17-49-\n45_gnn_for_spr_attempt_0/0-run/process_ForkProcess-\n12/working/bwa_curve_max_50.png', '\\n', 'All experiment data saved to\nworking/experiment_data.npy', '\\n', 'Execution time: 5 minutes seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["The training script executed successfully without any errors. The hyperparameter\ntuning process was carried out for different epoch values (10, 20, 30, 40, 50),\nand the results were logged for each configuration. The early stopping mechanism\nworked as intended, and the best models were saved for evaluation. The metrics,\nincluding Balanced Weighted Accuracy (BWA), Color-Weighted Accuracy (CWA), and\nShape-Weighted Accuracy (SWA), were calculated and logged for train, validation,\nand test sets. Experiment data was saved, and visual plots of BWA curves for\neach run were generated and stored. Overall, the script functioned as expected\nand met its objectives.", "", "", "", "", "", "The execution failed due to a FileNotFoundError. The script is unable to locate\nthe SPR_BENCH dataset directory. This error occurs because the locate_spr_bench\nfunction is unable to find the dataset in the specified paths or through the\nSPR_DATA_PATH environment variable.   Proposed Fix: 1. Ensure that the SPR_BENCH\ndirectory exists and contains the required CSV files (train.csv, dev.csv,\ntest.csv). 2. Place the SPR_BENCH directory in one of the expected paths\n(./SPR_BENCH or ../SPR_BENCH). 3. Alternatively, set the SPR_DATA_PATH\nenvironment variable to the absolute path of the SPR_BENCH directory. 4. Add\nproper error handling or logging to guide users on how to set the correct path\nif the dataset is not found.", "", "The execution successfully completed the hyperparameter tuning experiments\nacross various epoch settings (10, 20, 30, 40, and 50 epochs). The training,\nvalidation, and test results were computed and logged appropriately. Early\nstopping was triggered as expected for some configurations, and the results were\nsaved, including BWA curves and experiment data. However, while the training and\nvalidation BWA improved with increased epochs, the test BWA remained\nsignificantly lower, indicating potential overfitting or a need for additional\nregularization. This is not a bug but a challenge in model generalization that\ncan be addressed in future iterations.", "", "", ""], "exc_type": [null, null, null, null, null, null, "FileNotFoundError", null, null, null, null, null], "exc_info": [null, null, null, null, null, null, {"args": ["Place SPR_BENCH or set SPR_DATA_PATH"]}, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 43, "<module>", "DATA_PATH = locate_spr_bench()"], ["runfile.py", 26, "locate_spr_bench", "raise FileNotFoundError(\"Place SPR_BENCH or set SPR_DATA_PATH\")"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "balanced weighted accuracy", "lower_is_better": false, "description": "Balanced weighted accuracy measures the mean accuracy per class, weighted by class importance. Higher values are better.", "data": [{"dataset_name": "Training", "final_value": 0.9456, "best_value": 0.9478}, {"dataset_name": "Validation", "final_value": 0.9394, "best_value": 0.9423}, {"dataset_name": "Test", "final_value": 0.6534, "best_value": 0.6552}]}, {"metric_name": "lowest loss", "lower_is_better": true, "description": "The lowest loss achieved during training or evaluation. Lower values indicate better model performance.", "data": [{"dataset_name": "Training", "final_value": 0.2232, "best_value": 0.2111}, {"dataset_name": "Validation", "final_value": 0.2158, "best_value": 0.2071}, {"dataset_name": "Test", "final_value": 1.1268, "best_value": 0.9122}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by the color attribute. Higher values are better.", "data": [{"dataset_name": "Test", "final_value": 0.6751, "best_value": 0.6766}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by the shape attribute. Higher values are better.", "data": [{"dataset_name": "Test", "final_value": 0.6318, "best_value": 0.6339}]}]}, {"metric_names": [{"metric_name": "balanced weighted accuracy", "lower_is_better": false, "description": "The accuracy metric weighted for class imbalance.", "data": [{"dataset_name": "training", "final_value": 0.8996, "best_value": 0.8996}, {"dataset_name": "validation", "final_value": 0.8931, "best_value": 0.8931}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The loss value indicating the error rate during training or validation.", "data": [{"dataset_name": "training", "final_value": 0.2803, "best_value": 0.2803}, {"dataset_name": "validation", "final_value": 0.2859, "best_value": 0.2859}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "The accuracy metric for the test dataset.", "data": [{"dataset_name": "test", "final_value": 0.6626, "best_value": 0.6626}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3782, "best_value": 0.3252}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3677, "best_value": 0.3145}]}, {"metric_name": "training BWA", "lower_is_better": false, "description": "The BWA value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.856, "best_value": 0.888}]}, {"metric_name": "validation BWA", "lower_is_better": false, "description": "The BWA value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8595, "best_value": 0.8891}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8026, "best_value": 0.8026}]}, {"metric_name": "test BWA", "lower_is_better": false, "description": "The BWA value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.631, "best_value": 0.6422}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "The CWA value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.649, "best_value": 0.6618}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "The SWA value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6129, "best_value": 0.6227}]}]}, {"metric_names": [{"metric_name": "BWA", "lower_is_better": false, "description": "Balanced Weighted Accuracy, a measure of classification performance.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.8645, "best_value": 0.8645}, {"dataset_name": "wd_1e-05", "final_value": 0.8559, "best_value": 0.8575}, {"dataset_name": "wd_0.0001", "final_value": 0.8796, "best_value": 0.8796}, {"dataset_name": "wd_0.001", "final_value": 0.8566, "best_value": 0.8567}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss function value, a measure of model error.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.3526, "best_value": 0.3526}, {"dataset_name": "wd_1e-05", "final_value": 0.3695, "best_value": 0.3587}, {"dataset_name": "wd_0.0001", "final_value": 0.3504, "best_value": 0.3351}, {"dataset_name": "wd_0.001", "final_value": 0.3725, "best_value": 0.3725}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Test accuracy, a measure of the model's performance on unseen data.", "data": [{"dataset_name": "wd_0.0", "final_value": 0.6563, "best_value": 0.6563}, {"dataset_name": "wd_1e-05", "final_value": 0.6511, "best_value": 0.6511}, {"dataset_name": "wd_0.0001", "final_value": 0.6531, "best_value": 0.6531}, {"dataset_name": "wd_0.001", "final_value": 0.6575, "best_value": 0.6575}]}]}, {"metric_names": [{"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or difference between the predicted and actual values.", "data": [{"dataset_name": "training", "final_value": 0.3062, "best_value": 0.3062}, {"dataset_name": "validation", "final_value": 0.2917, "best_value": 0.2917}, {"dataset_name": "test", "final_value": 0.8991, "best_value": 0.8991}]}, {"metric_name": "balanced weighted accuracy", "lower_is_better": false, "description": "Measures the balanced accuracy of the model, weighted by class importance.", "data": [{"dataset_name": "training", "final_value": 0.8989, "best_value": 0.8989}, {"dataset_name": "validation", "final_value": 0.9002, "best_value": 0.9002}, {"dataset_name": "test", "final_value": 0.6406, "best_value": 0.6406}]}, {"metric_name": "color weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model in predicting color-related outputs, weighted by class importance.", "data": [{"dataset_name": "test", "final_value": 0.661, "best_value": 0.661}]}, {"metric_name": "shape weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model in predicting shape-related outputs, weighted by class importance.", "data": [{"dataset_name": "test", "final_value": 0.6203, "best_value": 0.6203}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3278, "best_value": 0.279}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3102, "best_value": 0.2743}]}, {"metric_name": "training BWA", "lower_is_better": false, "description": "The BWA value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8979, "best_value": 0.9127}]}, {"metric_name": "validation BWA", "lower_is_better": false, "description": "The BWA value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.895, "best_value": 0.9112}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8489, "best_value": 0.8486}]}, {"metric_name": "test BWA", "lower_is_better": false, "description": "The BWA value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6414, "best_value": 0.6444}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "The CWA value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6612, "best_value": 0.6647}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "The SWA value during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6216, "best_value": 0.6241}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "balanced weighted accuracy", "lower_is_better": false, "description": "The balanced weighted accuracy achieved during the training and validation phases.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8886, "best_value": 0.8886}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The loss value achieved during the training and validation phases.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3067, "best_value": 0.3067}]}]}, {"metric_names": [{"metric_name": "training best balanced weighted accuracy", "lower_is_better": false, "description": "The best balanced weighted accuracy achieved during training.", "data": [{"dataset_name": "max_10", "final_value": 0.9048, "best_value": 0.9048}, {"dataset_name": "max_20", "final_value": 0.9387, "best_value": 0.9387}, {"dataset_name": "max_30", "final_value": 0.9394, "best_value": 0.9394}, {"dataset_name": "max_40", "final_value": 0.9354, "best_value": 0.9354}, {"dataset_name": "max_50", "final_value": 0.9561, "best_value": 0.9561}]}, {"metric_name": "training lowest loss", "lower_is_better": true, "description": "The lowest loss achieved during training.", "data": [{"dataset_name": "max_10", "final_value": 0.2992, "best_value": 0.2992}, {"dataset_name": "max_20", "final_value": 0.2384, "best_value": 0.2384}, {"dataset_name": "max_30", "final_value": 0.2298, "best_value": 0.2298}, {"dataset_name": "max_40", "final_value": 0.2303, "best_value": 0.2303}, {"dataset_name": "max_50", "final_value": 0.1919, "best_value": 0.1919}]}, {"metric_name": "validation best balanced weighted accuracy", "lower_is_better": false, "description": "The best balanced weighted accuracy achieved during validation.", "data": [{"dataset_name": "max_10", "final_value": 0.9043, "best_value": 0.9043}, {"dataset_name": "max_20", "final_value": 0.9318, "best_value": 0.9318}, {"dataset_name": "max_30", "final_value": 0.933, "best_value": 0.933}, {"dataset_name": "max_40", "final_value": 0.9327, "best_value": 0.9327}, {"dataset_name": "max_50", "final_value": 0.9504, "best_value": 0.9504}]}, {"metric_name": "validation lowest loss", "lower_is_better": true, "description": "The lowest loss achieved during validation.", "data": [{"dataset_name": "max_10", "final_value": 0.2865, "best_value": 0.2865}, {"dataset_name": "max_20", "final_value": 0.2259, "best_value": 0.2259}, {"dataset_name": "max_30", "final_value": 0.2234, "best_value": 0.2234}, {"dataset_name": "max_40", "final_value": 0.2218, "best_value": 0.2218}, {"dataset_name": "max_50", "final_value": 0.1887, "best_value": 0.1887}]}, {"metric_name": "test balanced weighted accuracy", "lower_is_better": false, "description": "The balanced weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.6434, "best_value": 0.6434}, {"dataset_name": "max_20", "final_value": 0.65, "best_value": 0.65}, {"dataset_name": "max_30", "final_value": 0.6505, "best_value": 0.6505}, {"dataset_name": "max_40", "final_value": 0.6508, "best_value": 0.6508}, {"dataset_name": "max_50", "final_value": 0.6595, "best_value": 0.6595}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.664, "best_value": 0.664}, {"dataset_name": "max_20", "final_value": 0.671, "best_value": 0.671}, {"dataset_name": "max_30", "final_value": 0.6715, "best_value": 0.6715}, {"dataset_name": "max_40", "final_value": 0.672, "best_value": 0.672}, {"dataset_name": "max_50", "final_value": 0.6814, "best_value": 0.6814}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.6229, "best_value": 0.6229}, {"dataset_name": "max_20", "final_value": 0.629, "best_value": 0.629}, {"dataset_name": "max_30", "final_value": 0.6294, "best_value": 0.6294}, {"dataset_name": "max_40", "final_value": 0.6297, "best_value": 0.6297}, {"dataset_name": "max_50", "final_value": 0.6375, "best_value": 0.6375}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.9288, "best_value": 0.9288}, {"dataset_name": "max_20", "final_value": 1.1425, "best_value": 1.1425}, {"dataset_name": "max_30", "final_value": 1.0788, "best_value": 1.0788}, {"dataset_name": "max_40", "final_value": 1.048, "best_value": 1.048}, {"dataset_name": "max_50", "final_value": 1.4175, "best_value": 1.4175}]}]}, {"metric_names": [{"metric_name": "training balanced weighted accuracy", "lower_is_better": false, "description": "The best balanced weighted accuracy achieved during training.", "data": [{"dataset_name": "max_10", "final_value": 0.9132, "best_value": 0.9132}, {"dataset_name": "max_20", "final_value": 0.9367, "best_value": 0.9367}, {"dataset_name": "max_30", "final_value": 0.9428, "best_value": 0.9428}, {"dataset_name": "max_40", "final_value": 0.9449, "best_value": 0.9449}, {"dataset_name": "max_50", "final_value": 0.949, "best_value": 0.949}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The lowest loss achieved during training.", "data": [{"dataset_name": "max_10", "final_value": 0.2932, "best_value": 0.2932}, {"dataset_name": "max_20", "final_value": 0.2418, "best_value": 0.2418}, {"dataset_name": "max_30", "final_value": 0.2187, "best_value": 0.2187}, {"dataset_name": "max_40", "final_value": 0.2201, "best_value": 0.2201}, {"dataset_name": "max_50", "final_value": 0.2084, "best_value": 0.2084}]}, {"metric_name": "validation balanced weighted accuracy", "lower_is_better": false, "description": "The best balanced weighted accuracy achieved during validation.", "data": [{"dataset_name": "max_10", "final_value": 0.9127, "best_value": 0.9127}, {"dataset_name": "max_20", "final_value": 0.9291, "best_value": 0.9291}, {"dataset_name": "max_30", "final_value": 0.9362, "best_value": 0.9362}, {"dataset_name": "max_40", "final_value": 0.937, "best_value": 0.937}, {"dataset_name": "max_50", "final_value": 0.9414, "best_value": 0.9414}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The lowest loss achieved during validation.", "data": [{"dataset_name": "max_10", "final_value": 0.2856, "best_value": 0.2856}, {"dataset_name": "max_20", "final_value": 0.2398, "best_value": 0.2398}, {"dataset_name": "max_30", "final_value": 0.2142, "best_value": 0.2142}, {"dataset_name": "max_40", "final_value": 0.2128, "best_value": 0.2128}, {"dataset_name": "max_50", "final_value": 0.2044, "best_value": 0.2044}]}, {"metric_name": "test balanced weighted accuracy", "lower_is_better": false, "description": "The balanced weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.6403, "best_value": 0.6403}, {"dataset_name": "max_20", "final_value": 0.651, "best_value": 0.651}, {"dataset_name": "max_30", "final_value": 0.6511, "best_value": 0.6511}, {"dataset_name": "max_40", "final_value": 0.6525, "best_value": 0.6525}, {"dataset_name": "max_50", "final_value": 0.6556, "best_value": 0.6556}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.6605, "best_value": 0.6605}, {"dataset_name": "max_20", "final_value": 0.6722, "best_value": 0.6722}, {"dataset_name": "max_30", "final_value": 0.6723, "best_value": 0.6723}, {"dataset_name": "max_40", "final_value": 0.6738, "best_value": 0.6738}, {"dataset_name": "max_50", "final_value": 0.6771, "best_value": 0.6771}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.6201, "best_value": 0.6201}, {"dataset_name": "max_20", "final_value": 0.6298, "best_value": 0.6298}, {"dataset_name": "max_30", "final_value": 0.6299, "best_value": 0.6299}, {"dataset_name": "max_40", "final_value": 0.6312, "best_value": 0.6312}, {"dataset_name": "max_50", "final_value": 0.6341, "best_value": 0.6341}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss achieved on the test dataset.", "data": [{"dataset_name": "max_10", "final_value": 0.9275, "best_value": 0.9275}, {"dataset_name": "max_20", "final_value": 1.1019, "best_value": 1.1019}, {"dataset_name": "max_30", "final_value": 1.1984, "best_value": 1.1984}, {"dataset_name": "max_40", "final_value": 1.1653, "best_value": 1.1653}, {"dataset_name": "max_50", "final_value": 1.206, "best_value": 1.206}]}]}, {"metric_names": [{"metric_name": "balanced weighted accuracy", "lower_is_better": false, "description": "A metric that measures the overall accuracy of predictions, weighted to account for imbalanced datasets.", "data": [{"dataset_name": "Training dataset", "final_value": 0.9514, "best_value": 0.9514}, {"dataset_name": "Validation dataset", "final_value": 0.9448, "best_value": 0.9448}, {"dataset_name": "Test dataset", "final_value": 0.6603, "best_value": 0.6603}]}, {"metric_name": "lowest loss", "lower_is_better": true, "description": "A metric that measures the smallest loss achieved during training, which indicates better model performance.", "data": [{"dataset_name": "Training dataset", "final_value": 0.2052, "best_value": 0.2026}, {"dataset_name": "Validation dataset", "final_value": 0.2034, "best_value": 0.1982}, {"dataset_name": "Test dataset", "final_value": 1.2331, "best_value": 0.9065}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "A metric that measures the accuracy of color predictions, weighted for importance.", "data": [{"dataset_name": "Test dataset", "final_value": 0.6821, "best_value": 0.6821}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "A metric that measures the accuracy of shape predictions, weighted for importance.", "data": [{"dataset_name": "Test dataset", "final_value": 0.6385, "best_value": 0.6385}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_test_bwa_comparison.png", "../../logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_confusion_matrix_max_20.png"], ["../../logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/bwa_curve_spr.png", "../../logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_bwa_curve_lr_3e-03.png", "../../logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_loss_curve_lr_3e-03.png", "../../logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_val_bwa_by_lr.png", "../../logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_confusion_matrix_lr_3e-03.png"], ["../../logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/bwa_batchsize_curve.png", "../../logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_train_val_BWA.png", "../../logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.0.png", "../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_1e-05.png", "../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.0001.png", "../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.001.png", "../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_bwa_curves.png", "../../logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_final_bwa_bar.png"], ["../../logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/bwa_curve_embed_dim.png", "../../logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_bwa_curves.png", "../../logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_embed_dim_comparison.png"], ["../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.1.png", "../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.3.png", "../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.5.png", "../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_dev_BWA_all_dropouts.png", "../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_BWA_curves_best_dropout_0.0.png", "../../logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_test_BWA_vs_dropout.png"], [], ["../../logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/bwa_curve_best_hidden_dim.png", "../../logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_BWA_vs_hidden_dim.png", "../../logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_Loss_vs_hidden_dim.png", "../../logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_Confusion_Matrix.png"], ["../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_test_bwa_comparison.png", "../../logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_confusion_matrix_max_50.png"], ["../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_test_bwa_comparison.png", "../../logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_confusion_matrix_max_50.png"], ["../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_10.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_20.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_30.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_40.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_50.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_test_bwa_comparison.png", "../../logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_confusion_matrix_max_50.png"], ["../../logs/0-run/experiment_results/seed_aggregation_42818b1f38854f4a99f6d05f37aefa6b/spr_bench_bwa_mean_sem_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_42818b1f38854f4a99f6d05f37aefa6b/spr_bench_test_bwa_runs_and_mean.png"]], "plot_paths": [["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_test_bwa_comparison.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_confusion_matrix_max_20.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/bwa_curve_spr.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_bwa_curve_lr_3e-03.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_loss_curve_lr_3e-03.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_val_bwa_by_lr.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_confusion_matrix_lr_3e-03.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/bwa_batchsize_curve.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_train_val_BWA.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_train_val_loss.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.0.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_1e-05.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.0001.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.001.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_loss_curves.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_bwa_curves.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_final_bwa_bar.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/bwa_curve_embed_dim.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_loss_curves.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_bwa_curves.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_embed_dim_comparison.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.0.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.1.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.3.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.5.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_dev_BWA_all_dropouts.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_BWA_curves_best_dropout_0.0.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_test_BWA_vs_dropout.png"], [], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/bwa_curve_best_hidden_dim.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_BWA_vs_hidden_dim.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_Loss_vs_hidden_dim.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_Confusion_Matrix.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_test_bwa_comparison.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_confusion_matrix_max_50.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_test_bwa_comparison.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_confusion_matrix_max_50.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_10.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_20.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_30.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_40.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_50.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_test_bwa_comparison.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_confusion_matrix_max_50.png"], ["experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_42818b1f38854f4a99f6d05f37aefa6b/spr_bench_bwa_mean_sem_curve.png", "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_42818b1f38854f4a99f6d05f37aefa6b/spr_bench_test_bwa_runs_and_mean.png"]], "plot_analyses": [[{"analysis": "The plot indicates consistent improvement in both train and dev Balanced Weighted Accuracy (BWA) over epochs. The curves for train and dev BWA are closely aligned, suggesting minimal overfitting. The performance appears to plateau after epoch 8, indicating that further training beyond this point may yield diminishing returns.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_10.png"}, {"analysis": "The plot demonstrates steady improvement in both train and dev BWA as the number of epochs increases. The alignment of the two curves shows that the model generalizes well to the dev set. However, a slight divergence is observed towards the later epochs, which may indicate the early onset of overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_20.png"}, {"analysis": "The train and dev BWA curves continue to show improvement over 30 epochs. The alignment between train and dev BWA remains strong, with minimal divergence. However, the rate of improvement slows significantly after epoch 20, suggesting that additional training may not lead to substantial gains in performance.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_30.png"}, {"analysis": "The plot shows that train and dev BWA improve steadily up to around epoch 30, after which the performance appears to plateau. The alignment between the train and dev curves remains strong, indicating good generalization. There is no significant overfitting observed, even at higher epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_40.png"}, {"analysis": "The plot shows steady improvement in train and dev BWA up to around epoch 20. Beyond this point, the curves exhibit minor fluctuations, and the dev BWA starts to diverge slightly from the train BWA, indicating potential overfitting. The performance appears to plateau after epoch 25.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/bwa_curve_max_50.png"}, {"analysis": "This plot shows consistent improvement in both train and validation BWA over the first 10 epochs. The close alignment of the curves suggests good generalization, with minimal signs of overfitting. The performance appears to plateau after epoch 8, indicating that additional training may not yield significant improvements.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_10.png"}, {"analysis": "The plot demonstrates steady improvement in train and validation BWA over 20 epochs. The curves are closely aligned, suggesting strong generalization. However, a slight divergence is observed towards the later epochs, which may indicate the early onset of overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_20.png"}, {"analysis": "The train and validation BWA curves show consistent improvement over 30 epochs. The alignment between the two curves remains strong, with minimal divergence. However, the rate of improvement slows significantly after epoch 20, indicating that further training may have diminishing returns.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_30.png"}, {"analysis": "The plot shows steady improvement in train and validation BWA up to around epoch 30, after which the performance appears to plateau. The alignment between the train and validation curves remains strong, indicating good generalization. There is no significant overfitting observed, even at higher epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_40.png"}, {"analysis": "The plot shows steady improvement in train and validation BWA up to around epoch 20. Beyond this point, the curves exhibit minor fluctuations, and the validation BWA starts to diverge slightly from the train BWA, indicating potential overfitting. The performance appears to plateau after epoch 25.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b396019dcf9b4785902e93538f226955_proc_1445250/spr_bench_bwa_curve_max_50.png"}], [{"analysis": "The BWA (Balanced Weighted Accuracy) increases steadily for both training and development sets until epoch 4, after which it decreases slightly. This suggests that the model is learning effectively up to a point, but may begin to overfit after epoch 4. The close alignment of training and development BWA indicates that the model generalizes well to unseen data during these epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/bwa_curve_spr.png"}, {"analysis": "This plot reiterates the trends observed earlier, showing a steady increase in BWA for both training and development sets up to epoch 4, followed by a slight decline. The learning rate of 3e-03 appears to be effective, but further tuning may be required to avoid the slight overfitting observed after epoch 4.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_bwa_curve_lr_3e-03.png"}, {"analysis": "The cross-entropy loss decreases consistently for both training and development sets, indicating effective learning. However, the slight increase in development loss after epoch 4 suggests overfitting. Regularization techniques like dropout or weight decay could be beneficial to mitigate this.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_loss_curve_lr_3e-03.png"}, {"analysis": "The bar chart shows that a learning rate of 3e-03 achieves the highest final development BWA, outperforming other tested learning rates. This suggests that 3e-03 is a suitable choice for the learning rate, but further experimentation around this value could yield even better results.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_val_bwa_by_lr.png"}, {"analysis": "The confusion matrix indicates that the model performs well overall, with a high number of correct predictions in both classes. However, there is room for improvement in reducing the number of false positives and false negatives. This could be addressed by fine-tuning the decision threshold or incorporating class-specific weighting in the loss function.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_d26c0bc04dea4252b133033f817b39f4_proc_1445251/SPR_BENCH_confusion_matrix_lr_3e-03.png"}], [{"analysis": "The plot shows the impact of batch size on the development Balanced Weighted Accuracy (BWA) over epochs. Smaller batch sizes (bs=32) consistently yield higher BWA values compared to larger batch sizes (bs=64 and bs=128). This suggests that smaller batch sizes may allow the model to generalize better, potentially due to the increased stochasticity in gradient updates. The trend also indicates that increasing the number of epochs improves BWA for all batch sizes, but the rate of improvement is higher for smaller batch sizes.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/bwa_batchsize_curve.png"}, {"analysis": "This plot compares training and validation BWA for different batch sizes. For all batch sizes, the training BWA is slightly higher than the validation BWA, indicating some level of overfitting, especially for bs=32 where the gap is most pronounced. Smaller batch sizes show faster improvements in both training and validation BWA during the initial epochs, but the gap between training and validation metrics suggests that further regularization might be needed to address overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_train_val_BWA.png"}, {"analysis": "This plot illustrates the cross-entropy loss for training and validation sets across different batch sizes. Smaller batch sizes (bs=32) achieve lower training and validation losses, indicating better convergence. However, the gap between training and validation loss is larger for smaller batch sizes, which could be a sign of overfitting. Larger batch sizes (bs=128) show a smaller gap, suggesting better generalization but at the cost of higher loss values.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_train_val_loss.png"}, {"analysis": "This bar chart compares the test metrics (BWA, CWA, SWA) across different batch sizes. While all batch sizes perform similarly on the test set, smaller batch sizes (bs=32) slightly outperform others on BWA. CWA and SWA are relatively consistent across batch sizes, indicating that batch size has a more pronounced effect on BWA compared to the other metrics. This suggests that while smaller batch sizes may lead to better BWA, the impact on CWA and SWA is less significant.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec5c3a6ad791440f9351a2f9adb671e4_proc_1445252/SPR_BENCH_test_metrics.png"}], [{"analysis": "This plot shows the Balanced Weighted Accuracy (BWA) for both training and development sets across epochs with no weight decay (wd_0.0). The BWA increases steadily for both sets, peaking at epoch 4, but then slightly drops at epoch 5. This indicates potential overfitting, as the development set accuracy drops while the training set accuracy remains high at the end.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.0.png"}, {"analysis": "This plot represents BWA across epochs with a very small weight decay (wd_1e-05). The training and development BWA curves are closely aligned and show consistent improvement across epochs, plateauing at epoch 4 and maintaining stability. The absence of a drop in development BWA suggests reduced overfitting compared to wd_0.0.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_1e-05.png"}, {"analysis": "This plot illustrates BWA for training and development sets across epochs with a higher weight decay (wd_0.0001). Both curves show steady improvement and are closely aligned, with no significant drop at the end. The higher BWA achieved at epoch 5 indicates that this weight decay value helps improve generalization.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.0001.png"}, {"analysis": "This plot shows the BWA with a weight decay of wd_0.001. While the curves show steady improvement across epochs, there is a noticeable drop in development BWA after epoch 4. This suggests that the weight decay value may be too high, leading to underfitting and reduced performance on the development set.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/bwa_curve_wd_0.001.png"}, {"analysis": "This plot compares training and validation loss across epochs for different weight decay values. Higher weight decay values (e.g., wd_0.001) show slower convergence and higher final losses, while lower weight decay values (e.g., wd_1e-05) achieve lower losses. The wd_0.0001 configuration achieves a good balance, with both train and validation losses converging to low values, indicating effective regularization.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_loss_curves.png"}, {"analysis": "This plot compares BWA for training and validation sets across epochs for different weight decay values. Lower weight decay values (e.g., wd_1e-05 and wd_0.0001) yield higher BWA for both sets, indicating better performance. Higher weight decay values (e.g., wd_0.001) show reduced BWA, suggesting underfitting. The wd_0.0001 configuration achieves the best trade-off between train and validation performance.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_bwa_curves.png"}, {"analysis": "This bar plot summarizes the final development BWA for different weight decay values. The wd_0.0001 and wd_1e-05 configurations achieve the highest BWA, while wd_0.001 performs the worst, likely due to underfitting. This reinforces the finding that moderate weight decay values are optimal for this task.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_ec46137663d747afb0606716c563cd2a_proc_1445253/spr_bench_final_bwa_bar.png"}], [{"analysis": "This plot demonstrates that increasing the embedding dimension leads to a consistent improvement in the Balanced Weighted Accuracy (BWA) on the development set. The trend is evident across all epochs, with the highest embedding dimension (128) achieving the best performance. This suggests that larger embedding dimensions are more effective in capturing the underlying structure of the data for this task.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/bwa_curve_embed_dim.png"}, {"analysis": "The training and validation losses both decrease steadily over the epochs, with no signs of overfitting as the validation loss closely follows the training loss. This indicates that the model is learning effectively and that the chosen hyperparameters (e.g., learning rate and regularization) are appropriate for the task.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_loss_curves.png"}, {"analysis": "The training and validation BWA increase in tandem as the epochs progress, with almost identical performance by the end of training. This further supports the observation that the model generalizes well to the validation set and is not overfitting. The consistent improvement in BWA suggests that the model benefits from additional training epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_bwa_curves.png"}, {"analysis": "This bar chart highlights the final development BWA for each embedding dimension. The performance improves noticeably as the embedding dimension increases from 16 to 64, with a marginal gain observed between 64 and 128. This suggests diminishing returns for BWA improvement as the embedding dimension increases beyond 64, and further increases in embedding dimension may not be cost-effective.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_37211848f26d4d8bad94fd96221779e8_proc_1445252/spr_bench_embed_dim_comparison.png"}], [{"analysis": "This plot shows a consistent improvement in Balanced Weighted Accuracy (BWA) for both training and development sets as the number of epochs increases. The trend suggests that the model is learning effectively without overfitting, as the train and dev BWA values are closely aligned throughout the epochs. The absence of dropout appears to allow the model to fully utilize the data, leading to high BWA values.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.0.png"}, {"analysis": "Introducing a dropout rate of 0.1 slightly alters the learning dynamics. While both train and dev BWA show improvement over epochs, the dev BWA surpasses the train BWA in certain epochs, indicating potential regularization benefits. However, the overall trend remains consistent, showing effective learning.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.1.png"}, {"analysis": "With a dropout rate of 0.3, the train and dev BWA values remain closely aligned, although the rate of improvement appears slightly slower compared to lower dropout rates. The dev BWA marginally exceeds the train BWA in later epochs, suggesting improved generalization but potentially slightly hindered learning capacity.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.3.png"}, {"analysis": "At a dropout rate of 0.5, the train and dev BWA values are still closely aligned, but the improvement rate seems to plateau earlier compared to lower dropout rates. This may indicate that the higher dropout rate is limiting the model's capacity to learn effectively, though it still generalizes well.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/bwa_curve_dropout_0.5.png"}, {"analysis": "This comparative plot highlights how different dropout rates affect the dev BWA over epochs. Lower dropout rates (0.0 and 0.1) result in faster and higher BWA improvements, while higher dropout rates (0.3 and 0.5) lead to more stable but slower progress. The trend suggests that lower dropout rates might be more beneficial for this task.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_dev_BWA_all_dropouts.png"}, {"analysis": "This plot confirms that a dropout rate of 0.0 yields the best alignment and improvement in train and dev BWA over epochs. The close alignment indicates effective learning without overfitting, and the consistent improvement suggests that the model is performing optimally under these conditions.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_BWA_curves_best_dropout_0.0.png"}, {"analysis": "This bar plot shows that the test BWA remains almost constant across different dropout rates, indicating that dropout has minimal impact on the final test performance. This suggests that the model's performance is robust to changes in dropout for the test set.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_b798a20e617c4f7791cc5bf594d68b9e_proc_1445251/SPR_BENCH_test_BWA_vs_dropout.png"}], [], [{"analysis": "This plot shows the Balanced Weighted Accuracy (BWA) for training and development (dev) datasets over epochs with a hidden dimension of 128. Both curves show consistent improvement with increasing epochs, indicating effective learning. The training BWA slightly outperforms the dev BWA, which is expected, but the close alignment suggests minimal overfitting. The steep improvement between epochs 2 and 4 highlights a critical learning phase. Beyond epoch 4, the gains taper off, suggesting diminishing returns after this point.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/bwa_curve_best_hidden_dim.png"}, {"analysis": "This plot examines the impact of varying the hidden dimension of the Graph Convolutional Network (GCN) on the BWA for training and validation datasets. The BWA improves significantly with increasing hidden dimensions, peaking at 128. Beyond this point, the performance declines, especially for the validation set, indicating potential overfitting when the model becomes too complex. The alignment between training and validation BWA at lower dimensions suggests better generalization in simpler models.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_BWA_vs_hidden_dim.png"}, {"analysis": "This plot illustrates the relationship between the GCN hidden dimension and the cross-entropy loss for training and validation datasets. Both losses decrease as the hidden dimension increases, reflecting improved model capacity and learning. However, the gap between training and validation losses widens slightly at higher dimensions, indicating a risk of overfitting. The trend suggests that while larger dimensions improve performance initially, they might lead to diminishing returns and increased overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_Loss_vs_hidden_dim.png"}, {"analysis": "This confusion matrix visualizes the performance of the best model on the test dataset. The majority of predictions fall along the diagonal, indicating high accuracy. However, there is some degree of misclassification, particularly in one quadrant, which may suggest a bias or difficulty in distinguishing between specific classes. Further analysis of the misclassified samples could provide insights into model limitations.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_a60d2eb22eb94b04bf7d812a9d7d692a_proc_1445250/SPR_BENCH_Confusion_Matrix.png"}], [{"analysis": "The plot shows the Balanced Weighted Accuracy (BWA) for the training and development sets over 10 epochs. Both curves exhibit steady improvements with increasing epochs, converging towards 0.9. The close alignment between the two curves suggests minimal overfitting and good generalization performance for this epoch range.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_10.png"}, {"analysis": "This plot extends the training and development BWA evaluation to 20 epochs. The BWA continues to improve, reaching around 0.94 for training and slightly lower for development. The development curve starts to plateau around epoch 15, with minor deviations from the training curve indicating a slight overfitting tendency.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_20.png"}, {"analysis": "With 30 epochs, the BWA for training reaches around 0.94, while development BWA slightly lags behind. The gap between the two curves becomes more noticeable after epoch 20, indicating the onset of overfitting. The development curve shows minor fluctuations, suggesting that the model's performance on unseen data is less stable.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_30.png"}, {"analysis": "At 40 epochs, the training BWA maintains around 0.94, and development BWA also stabilizes but remains slightly lower. The gap between the two curves persists, with some fluctuations in the development curve. This indicates that further training may not significantly improve generalization and could exacerbate overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_40.png"}, {"analysis": "The training BWA continues to improve slightly with more epochs, reaching around 0.96 at 50 epochs. However, the development BWA plateaus earlier and shows more pronounced fluctuations, highlighting diminishing returns from additional training and increased overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/bwa_curve_max_50.png"}, {"analysis": "This plot evaluates the effect of different maximum epoch settings on the test BWA. All configurations achieve a similar test BWA of approximately 0.60, indicating limited sensitivity to the number of epochs for the test performance. This suggests that factors other than epoch count may be more critical for improving test BWA, such as regularization or hyperparameter tuning.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/spr_bench_bwa_curve_max_10.png"}], [{"analysis": "The plot shows the Balanced Weighted Accuracy (BWA) for training and development datasets over a maximum of 10 epochs. Both curves increase steadily, indicating that the model is learning effectively without overfitting. The training and development BWA values closely track each other, suggesting good generalization in this range of epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_10.png"}, {"analysis": "This plot extends the observation to 20 epochs. The training and development BWA continue to improve and remain closely aligned. There is no significant divergence between the two curves, which suggests that the model is not overfitting even with a higher number of epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_20.png"}, {"analysis": "The plot for 30 epochs shows a slight increase in variability in the training BWA curve while the development curve remains relatively stable. This could indicate the beginning of overfitting, as the model starts to perform slightly better on the training data than on the development data.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_30.png"}, {"analysis": "In this plot for 40 epochs, the training BWA shows more pronounced fluctuations, while the development BWA remains stable but slightly lower. This suggests that overfitting is becoming more apparent as the model continues to train for more epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_40.png"}, {"analysis": "The plot for 50 epochs reveals further divergence between the training and development BWA curves. The training BWA fluctuates more significantly, while the development BWA stabilizes at a slightly lower value. This indicates that overfitting is now evident, and further training does not improve generalization.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/bwa_curve_max_50.png"}, {"analysis": "This plot shows the training and validation BWA curves for a maximum of 10 epochs. Both curves increase steadily and remain closely aligned, indicating effective learning and good generalization within this range of epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_10.png"}, {"analysis": "The plot for a maximum of 20 epochs shows that both training and validation BWA curves continue to improve and remain closely aligned, confirming that the model is still generalizing well without signs of overfitting.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_20.png"}, {"analysis": "For a maximum of 30 epochs, the training BWA curve shows slightly more variability compared to the validation curve, which remains stable. This suggests that the model is starting to overfit to the training data as the number of epochs increases.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_30.png"}, {"analysis": "The plot for 40 epochs shows that the training BWA curve fluctuates more significantly, while the validation BWA curve stabilizes at a slightly lower value. This indicates that overfitting is becoming more pronounced as the model trains for more epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_40.png"}, {"analysis": "This bar chart summarizes the test BWA for different maximum epoch settings. The test BWA remains consistent across different epoch settings, suggesting that the model's generalization to unseen data is not significantly impacted by the number of training epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/spr_bench_bwa_curve_max_50.png"}], [{"analysis": "This plot shows the Balanced Weighted Accuracy (BWA) for both the training and development datasets over 10 epochs. The BWA for both datasets improves consistently with each epoch, with a slight convergence observed by the 10th epoch. The training and development curves are closely aligned, indicating minimal overfitting at this stage. The steady improvement suggests that the model is learning effectively.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_10.png"}, {"analysis": "The plot extends the analysis to 20 epochs, showing continued improvement in BWA for both training and development datasets. The alignment of the curves remains close, which is a positive sign that overfitting is still under control. The BWA appears to plateau slightly after 15 epochs, suggesting diminishing returns with additional epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_20.png"}, {"analysis": "This plot examines the performance over 30 epochs. Both training and development BWA continue to show improvement, though the rate of increase slows down significantly after 20 epochs. The alignment between the two curves persists, which is a good indicator of generalization. However, the plateauing trend becomes more pronounced, raising questions about the utility of further increasing the number of epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_30.png"}, {"analysis": "With 40 epochs, the BWA for both training and development datasets shows minor fluctuations but remains largely stable, with the training BWA slightly higher than the development BWA. These fluctuations may indicate the onset of overfitting, though the overall performance remains strong. The plateau observed earlier persists, reinforcing the diminishing returns of additional training epochs.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_40.png"}, {"analysis": "This plot is similar to the previous one but extends the analysis to 50 epochs. The fluctuations in BWA become more noticeable, particularly for the development dataset, which suggests overfitting may be becoming more pronounced. The training BWA continues to increase slightly, but the development BWA does not show significant improvement, confirming the plateauing trend.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/bwa_curve_max_50.png"}, {"analysis": "This plot mirrors the first plot but focuses on validation BWA instead of development BWA. The trends are similar, with consistent improvement over 10 epochs and close alignment between training and validation curves. This supports the earlier conclusion that the model is learning effectively without significant overfitting at this stage.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_10.png"}, {"analysis": "This plot extends the validation BWA analysis to 20 epochs. The trends are consistent with earlier observations: steady improvement, close alignment between training and validation curves, and a slight plateau after 15 epochs. The results continue to indicate good generalization and learning efficiency.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_20.png"}, {"analysis": "The plot for 30 epochs shows a continuation of earlier trends, with both training and validation BWA improving steadily but at a slower rate. The curves remain closely aligned, suggesting minimal overfitting. However, the plateauing trend is evident, indicating the model's capacity might be nearing its limit for this dataset.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_30.png"}, {"analysis": "This plot for 40 epochs shows minor fluctuations in both training and validation BWA, with the validation curve showing slight instability compared to earlier stages. This could indicate the beginning stages of overfitting. The plateauing trend persists, and the added epochs provide limited performance gains.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_40.png"}, {"analysis": "This bar plot summarizes the test BWA for different maximum epoch settings. The test BWA remains consistent across all settings, suggesting that increasing the number of epochs beyond 10 does not significantly impact test performance. This reinforces the earlier observation that the model's capacity is limited, and additional training epochs yield diminishing returns.", "plot_path": "experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/spr_bench_bwa_curve_max_50.png"}], []], "vlm_feedback_summary": ["The plots demonstrate consistent improvement in Balanced Weighted Accuracy (BWA)\nacross training and validation sets, with minimal overfitting observed. The\nalignment between training and validation curves indicates strong\ngeneralization, although slight divergence is observed at higher epochs in some\ncases. Performance tends to plateau after a certain number of epochs, suggesting\nthat additional training may not yield significant gains.", "The plots demonstrate effective learning and generalization up to epoch 4 with a\nlearning rate of 3e-03. However, there are signs of overfitting after epoch 4,\nas evidenced by the slight decline in BWA and the increase in development loss.\nRegularization techniques and further hyperparameter tuning around the learning\nrate could improve performance. The confusion matrix highlights areas for\nimprovement in reducing misclassifications.", "The experimental results highlight the trade-offs between batch size and\nperformance metrics. Smaller batch sizes provide better BWA but may lead to\noverfitting, as indicated by larger gaps between training and validation\nmetrics. Larger batch sizes generalize better but with slightly lower\nperformance. Regularization techniques could help mitigate overfitting for\nsmaller batch sizes.", "The analysis highlights the impact of weight decay on Balanced Weighted Accuracy\n(BWA) and loss. Moderate weight decay values (e.g., wd_0.0001) achieve the best\ntrade-off between training and validation performance, reducing overfitting\nwhile maintaining high accuracy. Excessively high weight decay (wd_0.001) leads\nto underfitting, while no weight decay (wd_0.0) results in slight overfitting.\nThe results suggest that wd_0.0001 is the optimal configuration for this task.", "The plots show clear evidence of improvement in model performance with\nincreasing embedding dimensions and training epochs. The model demonstrates good\ngeneralization capabilities, as indicated by the close alignment of training and\nvalidation metrics. The results suggest that embedding dimensions up to 64\nprovide significant gains, with diminishing returns beyond that point.", "The plots provide valuable insights into the effect of dropout rates on the\nmodel's BWA performance over epochs. Lower dropout rates (0.0 and 0.1) lead to\nfaster and higher improvements in BWA, while higher dropout rates (0.3 and 0.5)\nresult in slower but more stable progress. The test BWA remains largely\nunaffected by dropout rates, indicating robustness in test performance. The\nresults suggest that minimizing dropout may be optimal for this task.", "[]", "The plots provide a comprehensive view of the model's performance and its\ndependency on hyperparameters such as hidden dimension. The BWA curves show\neffective learning with minimal overfitting, while the hidden dimension analysis\nhighlights an optimal point for performance. The loss curves suggest diminishing\nreturns with excessive model complexity, and the confusion matrix confirms high\naccuracy with room for improvement in specific areas.", "The plots demonstrate the progression of Balanced Weighted Accuracy (BWA) over\ndifferent epoch ranges for training, development, and test sets. The training\nand development BWA curves show steady improvements initially but plateau and\ndiverge with more epochs, indicating overfitting. The test BWA remains\nconsistent across different epoch settings, suggesting that further optimization\nshould focus on regularization or other hyperparameters rather than merely\nincreasing the number of epochs.", "The plots indicate that the model learns effectively up to around 20 epochs,\nbeyond which overfitting starts to become apparent. The test BWA remains\nconsistent across different maximum epoch settings, suggesting that the model's\ngeneralization to unseen data is stable regardless of the number of training\nepochs. To further improve performance, focus on regularization techniques and\navoid excessive training epochs.", "The plots indicate that the model achieves steady performance improvements in\nBalanced Weighted Accuracy (BWA) during training and validation up to 20 epochs,\nafter which the gains plateau. Overfitting becomes more noticeable as the number\nof epochs increases beyond 30, with fluctuations in development and validation\nBWA. The test BWA remains consistent across different epoch settings, suggesting\nlimited benefits from extended training. Overall, the model demonstrates\neffective learning and generalization up to 20 epochs, but further optimization\nshould focus on reducing overfitting and exploring alternative regularization\ntechniques.", "[]"], "exec_time": [972.2795684337616, 67.08424830436707, 59.01266694068909, 166.42269372940063, 176.19898176193237, 91.3762276172638, 1.8990635871887207, 205.0090570449829, 900.734815120697, 309.7924544811249, 357.23871326446533, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['plot 1'", "'plot 2'", "'plot 3'", "'plot 4'", "'plot 5'", "'plot 6'", "'plot 7'", "'plot 8'", "'plot 9'", "'plot 10']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["['wd_0.0001'", "'wd_1e-05']"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], [], ["[\"SPR_BENCH\"]"], ["['experiment_data']"], ["[\"training\"", "\"development\"", "\"validation\"]"], ["['training'", "'development'", "'validation'", "'test']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data.get(\"num_epochs\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------------------------------------------------------------------\n# 1) Per\u2013run train/val BWA curves  (<=5 similar figures)\nfor i, (run_key, run_dict) in enumerate(runs.items()):\n    if i >= 5:  # obey \u201cat most 5 similar figures\u201d\n        break\n    try:\n        epochs = np.arange(1, len(run_dict[\"metrics\"][\"train\"]) + 1)\n        train_bwa = run_dict[\"metrics\"][\"train\"]\n        val_bwa = run_dict[\"metrics\"][\"val\"]\n\n        plt.figure()\n        plt.plot(epochs, train_bwa, label=\"Train BWA\")\n        plt.plot(epochs, val_bwa, label=\"Validation BWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BWA\")\n        plt.title(f\"SPR-BENCH BWA Learning Curve \u2013 {run_key}\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"spr_bench_bwa_curve_{run_key}.png\"\n        path = os.path.join(working_dir, fname)\n        plt.savefig(path)\n        plt.close()\n        print(f\"Saved {path}\")\n    except Exception as e:\n        print(f\"Error creating BWA curve for {run_key}: {e}\")\n        plt.close()\n\n# ---------------------------------------------------------------------\n# 2) Bar chart comparing test BWA across runs\ntry:\n    run_names = []\n    test_bwa_values = []\n    for rk, rd in runs.items():\n        if \"test_metrics\" in rd and \"BWA\" in rd[\"test_metrics\"]:\n            run_names.append(rk)\n            test_bwa_values.append(rd[\"test_metrics\"][\"BWA\"])\n\n    plt.figure()\n    x_pos = np.arange(len(run_names))\n    plt.bar(x_pos, test_bwa_values, color=\"skyblue\")\n    plt.xticks(x_pos, run_names, rotation=45, ha=\"right\")\n    plt.ylabel(\"Test BWA\")\n    plt.title(\"SPR-BENCH: Test BWA for Different max_epoch Settings\")\n    plt.tight_layout()\n    fname = \"spr_bench_test_bwa_comparison.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating test BWA comparison bar chart: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 3) Confusion matrix of best run (highest test BWA)\ntry:\n    # locate best run\n    best_run = max(\n        runs.items(),\n        key=lambda item: item[1].get(\"test_metrics\", {}).get(\"BWA\", -np.inf),\n    )[0]\n    preds = np.array(runs[best_run][\"predictions\"])\n    gts = np.array(runs[best_run][\"ground_truth\"])\n    num_classes = int(max(preds.max(), gts.max()) + 1)\n    conf_mat = np.zeros((num_classes, num_classes), dtype=int)\n    for gt, pr in zip(gts, preds):\n        conf_mat[gt, pr] += 1\n\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(conf_mat, cmap=\"Blues\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR-BENCH Confusion Matrix \u2013 Best Run: {best_run}\")\n    for (i, j), v in np.ndenumerate(conf_mat):\n        plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n    plt.tight_layout()\n    fname = f\"spr_bench_confusion_matrix_{best_run}.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------------------------------------------------------------------\n# identify best LR on dev BWA -----------------------------------------\nbest_lr, best_bwa = None, -1.0\ndev_bwa_per_lr = {}\nfor lr_key, rec in (\n    experiment_data.get(\"learning_rate\", {}).get(\"SPR_BENCH\", {}).items()\n):\n    val_series = rec[\"metrics\"][\"val\"]\n    if val_series:\n        final_bwa = val_series[-1]\n        dev_bwa_per_lr[lr_key] = final_bwa\n        if final_bwa > best_bwa:\n            best_bwa, best_lr = final_bwa, lr_key\n\nprint(f\"Best LR on dev: {best_lr}  |  Best dev BWA: {best_bwa:.4f}\")\n\n# Short-circuit if nothing to plot\nif not best_lr:\n    exit(0)\n\nbest_rec = experiment_data[\"learning_rate\"][\"SPR_BENCH\"][best_lr]\nepochs = np.arange(1, len(best_rec[\"metrics\"][\"train\"]) + 1)\n\n# ---------------------------------------------------------------------\n# 1) BWA curve ---------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, best_rec[\"metrics\"][\"train\"], label=\"Train BWA\")\n    plt.plot(epochs, best_rec[\"metrics\"][\"val\"], label=\"Dev BWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(f\"SPR_BENCH \u2013 BWA over Epochs (LR={best_lr})\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_bwa_curve_lr_{best_lr}.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating BWA curve: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 2) Loss curve --------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, best_rec[\"losses\"][\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, best_rec[\"losses\"][\"val\"], label=\"Dev Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"SPR_BENCH \u2013 Loss over Epochs (LR={best_lr})\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_loss_curve_lr_{best_lr}.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss curve: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 3) Dev BWA by LR -----------------------------------------------------\ntry:\n    plt.figure()\n    lrs, bwas = zip(*sorted(dev_bwa_per_lr.items(), key=lambda x: x[0]))\n    plt.bar(range(len(lrs)), bwas, tick_label=lrs)\n    plt.ylabel(\"Final Dev BWA\")\n    plt.xlabel(\"Learning Rate\")\n    plt.title(\"SPR_BENCH \u2013 Final Dev BWA by Learning Rate\")\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_bwa_by_lr.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating LR comparison bar plot: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 4) Confusion matrix on test set -------------------------------------\ntry:\n    preds = np.array(best_rec.get(\"predictions\", []))\n    gts = np.array(best_rec.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        num_classes = max(preds.max(), gts.max()) + 1\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"SPR_BENCH \u2013 Confusion Matrix (LR={best_lr})\")\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(\n                    j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n                )\n        plt.tight_layout()\n        fname = os.path.join(\n            working_dir, f\"SPR_BENCH_confusion_matrix_lr_{best_lr}.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# print final metrics --------------------------------------------------\ncwa = best_rec.get(\"metrics\", {}).get(\"val\", [-1])[-1]  # placeholder if missing\ntest_cwa = best_rec.get(\"metrics\", {}).get(\"val\", [-1])[-1]  # reuse placeholder\nprint(f\"Best LR={best_lr} | Dev BWA={best_bwa:.4f}\")\nif preds.size:\n    # compute test BWA components\n    def weighted_acc(weights):\n        return (weights * (preds == gts)).sum() / max(weights.sum(), 1)\n\n    # assume weights unavailable here, so just overall accuracy\n    test_bwa = (preds == gts).mean()\n    print(f\"Test accuracy (unweighted BWA proxy): {test_bwa:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------- #\n# load data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\n\n# helper to fetch nested dict safely\ndef get_batch_runs():\n    try:\n        return experiment_data[\"batch_size\"][\"SPR_BENCH\"]\n    except KeyError:\n        print(\"Unexpected experiment_data structure.\")\n        return {}\n\n\nruns = get_batch_runs()\nif not runs:\n    raise SystemExit\n\nbatch_sizes = sorted(int(k) for k in runs.keys())\ncolors = {bs: c for bs, c in zip(batch_sizes, [\"r\", \"g\", \"b\", \"c\", \"m\"])}\n\n# ---------------------------------------------------------------------- #\n# 1) Train/Val BWA curves\ntry:\n    plt.figure()\n    for bs in batch_sizes:\n        data = runs[str(bs)]\n        epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            data[\"metrics\"][\"train\"],\n            \"--\",\n            color=colors[bs],\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, data[\"metrics\"][\"val\"], \"-\", color=colors[bs], label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.title(\"SPR_BENCH: Train vs. Validation BWA\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_train_val_BWA.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating BWA plot: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------- #\n# 2) Train/Val Loss curves\ntry:\n    plt.figure()\n    for bs in batch_sizes:\n        data = runs[str(bs)]\n        epochs = np.arange(1, len(data[\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs,\n            data[\"losses\"][\"train\"],\n            \"--\",\n            color=colors[bs],\n            label=f\"train bs={bs}\",\n        )\n        plt.plot(\n            epochs, data[\"losses\"][\"val\"], \"-\", color=colors[bs], label=f\"val bs={bs}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs. Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_train_val_loss.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------- #\n# 3) Test metrics bar chart\ntry:\n    metrics = [\"bwa\", \"cwa\", \"swa\"]\n    x = np.arange(len(batch_sizes))\n    width = 0.25\n    plt.figure()\n    for i, m in enumerate(metrics):\n        vals = [runs[str(bs)][\"test_metrics\"][m] for bs in batch_sizes]\n        plt.bar(x + i * width, vals, width, label=m.upper())\n    plt.xticks(x + width, [str(bs) for bs in batch_sizes])\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Test Metrics by Batch Size\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metrics plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"weight_decay\", {})\ntags = list(runs.keys())\nnum_epochs = max(len(runs[t][\"losses\"][\"train\"]) for t in tags) if tags else 0\ne = np.arange(1, num_epochs + 1)\n\n# ------------------------------------------------------------------\n# 1) Combined loss curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for t in tags:\n        plt.plot(e, runs[t][\"losses\"][\"train\"], label=f\"{t} Train\")\n        plt.plot(e, runs[t][\"losses\"][\"val\"], linestyle=\"--\", label=f\"{t} Val\")\n    plt.title(\n        \"SPR_BENCH Training vs Validation Loss\\n(Left: Train, Right: Val curves across weight_decay)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fpath = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fpath)\n    plt.close()\n    print(\"Saved\", fpath)\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Combined BWA curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for t in tags:\n        plt.plot(e, runs[t][\"metrics\"][\"train\"], label=f\"{t} Train\")\n        plt.plot(e, runs[t][\"metrics\"][\"val\"], linestyle=\"--\", label=f\"{t} Val\")\n    plt.title(\"SPR_BENCH Balanced Weighted Accuracy\\n(Left: Train, Right: Val)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"BWA\")\n    plt.legend(fontsize=6)\n    plt.tight_layout()\n    fpath = os.path.join(working_dir, \"spr_bench_bwa_curves.png\")\n    plt.savefig(fpath)\n    plt.close()\n    print(\"Saved\", fpath)\nexcept Exception as e:\n    print(f\"Error creating BWA curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Final test BWA bar chart\ntry:\n    final_bwa = [\n        runs[t][\"metrics\"][\"val\"][-1] if runs[t][\"metrics\"][\"val\"] else 0 for t in tags\n    ]\n    plt.figure(figsize=(5, 3))\n    plt.bar(tags, final_bwa, color=\"skyblue\")\n    plt.title(\"SPR_BENCH Final Dev BWA by Weight Decay\")\n    plt.xlabel(\"Weight Decay Tag\")\n    plt.ylabel(\"Dev BWA (epoch last)\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    fpath = os.path.join(working_dir, \"spr_bench_final_bwa_bar.png\")\n    plt.savefig(fpath)\n    plt.close()\n    print(\"Saved\", fpath)\nexcept Exception as e:\n    print(f\"Error creating final BWA bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    section = experiment_data.get(\"embed_dim\", {}).get(\"SPR_BENCH\", {})\n    if not section:\n        print(\"No SPR_BENCH data found.\")\n    else:\n        # -------- pick best embed_dim by last dev BWA ---------\n        best_dim, best_bwa = None, -1\n        for k, v in section.items():\n            bwa_curve = v[\"metrics\"][\"val\"]\n            if bwa_curve and bwa_curve[-1] > best_bwa:\n                best_bwa, best_dim = bwa_curve[-1], k\n        # ----------------- Plot 1: loss curves ----------------\n        try:\n            logs = section[best_dim]\n            epochs = range(1, len(logs[\"losses\"][\"train\"]) + 1)\n            plt.figure()\n            plt.plot(epochs, logs[\"losses\"][\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, logs[\"losses\"][\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\n                f'SPR_BENCH (embed_dim={best_dim.split(\"_\")[-1]})\\n'\n                \"Training vs Validation Loss\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n            plt.tight_layout()\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Plot saved: {fname}\")\n        except Exception as e:\n            print(f\"Error creating loss curve plot: {e}\")\n            plt.close()\n        # --------------- Plot 2: BWA curves -------------------\n        try:\n            plt.figure()\n            plt.plot(epochs, logs[\"metrics\"][\"train\"], label=\"Train BWA\")\n            plt.plot(epochs, logs[\"metrics\"][\"val\"], label=\"Val BWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Balanced Weighted Accuracy\")\n            plt.title(\n                f'SPR_BENCH (embed_dim={best_dim.split(\"_\")[-1]})\\n'\n                \"Training vs Validation BWA\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, \"spr_bench_bwa_curves.png\")\n            plt.tight_layout()\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Plot saved: {fname}\")\n        except Exception as e:\n            print(f\"Error creating BWA curve plot: {e}\")\n            plt.close()\n        # ------- Plot 3: final dev BWA for each dim -----------\n        try:\n            dims, final_bwas = [], []\n            for k, v in section.items():\n                dims.append(int(k.split(\"_\")[-1]))\n                final_bwas.append(v[\"metrics\"][\"val\"][-1] if v[\"metrics\"][\"val\"] else 0)\n            order = np.argsort(dims)\n            dims = np.array(dims)[order]\n            final_bwas = np.array(final_bwas)[order]\n            plt.figure()\n            plt.bar(range(len(dims)), final_bwas, tick_label=dims)\n            plt.xlabel(\"embed_dim\")\n            plt.ylabel(\"Final Dev BWA\")\n            plt.title(\"SPR_BENCH\\nFinal Dev BWA for each embed_dim\")\n            fname = os.path.join(working_dir, \"spr_bench_embed_dim_comparison.png\")\n            plt.tight_layout()\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Plot saved: {fname}\")\n        except Exception as e:\n            print(f\"Error creating embed_dim comparison plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- Load experiment data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Abort early if dataset missing\nif (\n    not experiment_data\n    or \"dropout_rate\" not in experiment_data\n    or \"SPR_BENCH\" not in experiment_data[\"dropout_rate\"]\n):\n    print(\"No SPR_BENCH data found in experiment_data.npy\")\nelse:\n    spr_dict = experiment_data[\"dropout_rate\"][\"SPR_BENCH\"]\n    dropouts = sorted(float(k) for k in spr_dict.keys())\n    epochs = np.arange(1, len(next(iter(spr_dict.values()))[\"metrics\"][\"val\"]) + 1)\n\n    # Collect curves & test metrics\n    val_curves, train_curves, test_bwa = {}, {}, {}\n    for dr in dropouts:\n        d = spr_dict[str(dr)]\n        train_curves[dr] = d[\"metrics\"][\"train\"]\n        val_curves[dr] = d[\"metrics\"][\"val\"]\n        test_bwa[dr] = d[\"test_metrics\"][\"BWA\"]\n\n    # -------- Plot 1: Dev BWA across dropout rates --------\n    try:\n        plt.figure()\n        for dr in dropouts:\n            plt.plot(epochs, val_curves[dr], label=f\"dropout={dr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BWA\")\n        plt.title(\"SPR_BENCH: Dev BWA vs Epochs for different dropouts\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_dev_BWA_all_dropouts.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot1: {e}\")\n        plt.close()\n\n    # Identify best dropout (highest final dev BWA)\n    best_dr = max(dropouts, key=lambda d: val_curves[d][-1])\n\n    # -------- Plot 2: Train vs Dev BWA for best dropout --------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_curves[best_dr], label=\"Train BWA\")\n        plt.plot(epochs, val_curves[best_dr], label=\"Dev BWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BWA\")\n        plt.title(f\"SPR_BENCH: Train vs Dev BWA (best dropout={best_dr})\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(\n            working_dir, f\"SPR_BENCH_BWA_curves_best_dropout_{best_dr}.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot2: {e}\")\n        plt.close()\n\n    # -------- Plot 3: Test BWA bar chart --------\n    try:\n        plt.figure()\n        plt.bar([str(d) for d in dropouts], [test_bwa[d] for d in dropouts])\n        plt.xlabel(\"Dropout Rate\")\n        plt.ylabel(\"Test BWA\")\n        plt.title(\"SPR_BENCH: Test BWA vs Dropout Rate\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_BWA_vs_dropout.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot3: {e}\")\n        plt.close()\n\n    # -------- Console summary --------\n    print(f\"Best dropout (by final Dev BWA): {best_dr}\")\n    print(f\"  Final Dev BWA:  {val_curves[best_dr][-1]:.4f}\")\n    print(f\"  Test BWA:       {test_bwa[best_dr]:.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    exp = experiment_data[\"gcn_hidden_dim\"][\"SPR_BENCH\"]\n    hidden_dims = np.array(exp[\"hidden_dims\"])\n    train_bwa = np.array(exp[\"metrics\"][\"train\"])\n    val_bwa = np.array(exp[\"metrics\"][\"val\"])\n    train_loss = np.array(exp[\"losses\"][\"train\"])\n    val_loss = np.array(exp[\"losses\"][\"val\"])\n    preds = np.array(exp[\"predictions\"])\n    gts = np.array(exp[\"ground_truth\"])\n\n    # ------------------------------------------------------------------\n    # Plot 1: BWA vs hidden dim\n    try:\n        plt.figure()\n        plt.plot(hidden_dims, train_bwa, marker=\"o\", label=\"Train BWA\")\n        plt.plot(hidden_dims, val_bwa, marker=\"s\", label=\"Validation BWA\")\n        plt.xlabel(\"GCN Hidden Dimension\")\n        plt.ylabel(\"BWA\")\n        plt.title(\"SPR_BENCH: Train/Val BWA vs GCN Hidden Dim\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_BWA_vs_hidden_dim.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating BWA plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------\n    # Plot 2: Loss vs hidden dim\n    try:\n        plt.figure()\n        plt.plot(hidden_dims, train_loss, marker=\"o\", label=\"Train Loss\")\n        plt.plot(hidden_dims, val_loss, marker=\"s\", label=\"Validation Loss\")\n        plt.xlabel(\"GCN Hidden Dimension\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Train/Val Loss vs GCN Hidden Dim\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_Loss_vs_hidden_dim.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------\n    # Plot 3: Confusion matrix on test set for best model\n    try:\n        num_labels = int(max(preds.max(), gts.max())) + 1\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        # normalize rows\n        cm_norm = cm / (cm.sum(axis=1, keepdims=True) + 1e-9)\n\n        plt.figure(figsize=(5, 4))\n        im = plt.imshow(cm_norm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\"SPR_BENCH Test Confusion Matrix (Best Model)\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_Confusion_Matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Confusion Matrix plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------\n    # Print overall test accuracy\n    test_acc = (preds == gts).mean() if len(preds) else 0.0\n    print(f\"Overall test accuracy (best model): {test_acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data.get(\"num_epochs\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------------------------------------------------------------------\n# 1) Per\u2013run train/val BWA curves  (<=5 similar figures)\nfor i, (run_key, run_dict) in enumerate(runs.items()):\n    if i >= 5:  # obey \u201cat most 5 similar figures\u201d\n        break\n    try:\n        epochs = np.arange(1, len(run_dict[\"metrics\"][\"train\"]) + 1)\n        train_bwa = run_dict[\"metrics\"][\"train\"]\n        val_bwa = run_dict[\"metrics\"][\"val\"]\n\n        plt.figure()\n        plt.plot(epochs, train_bwa, label=\"Train BWA\")\n        plt.plot(epochs, val_bwa, label=\"Validation BWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BWA\")\n        plt.title(f\"SPR-BENCH BWA Learning Curve \u2013 {run_key}\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"spr_bench_bwa_curve_{run_key}.png\"\n        path = os.path.join(working_dir, fname)\n        plt.savefig(path)\n        plt.close()\n        print(f\"Saved {path}\")\n    except Exception as e:\n        print(f\"Error creating BWA curve for {run_key}: {e}\")\n        plt.close()\n\n# ---------------------------------------------------------------------\n# 2) Bar chart comparing test BWA across runs\ntry:\n    run_names = []\n    test_bwa_values = []\n    for rk, rd in runs.items():\n        if \"test_metrics\" in rd and \"BWA\" in rd[\"test_metrics\"]:\n            run_names.append(rk)\n            test_bwa_values.append(rd[\"test_metrics\"][\"BWA\"])\n\n    plt.figure()\n    x_pos = np.arange(len(run_names))\n    plt.bar(x_pos, test_bwa_values, color=\"skyblue\")\n    plt.xticks(x_pos, run_names, rotation=45, ha=\"right\")\n    plt.ylabel(\"Test BWA\")\n    plt.title(\"SPR-BENCH: Test BWA for Different max_epoch Settings\")\n    plt.tight_layout()\n    fname = \"spr_bench_test_bwa_comparison.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating test BWA comparison bar chart: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 3) Confusion matrix of best run (highest test BWA)\ntry:\n    # locate best run\n    best_run = max(\n        runs.items(),\n        key=lambda item: item[1].get(\"test_metrics\", {}).get(\"BWA\", -np.inf),\n    )[0]\n    preds = np.array(runs[best_run][\"predictions\"])\n    gts = np.array(runs[best_run][\"ground_truth\"])\n    num_classes = int(max(preds.max(), gts.max()) + 1)\n    conf_mat = np.zeros((num_classes, num_classes), dtype=int)\n    for gt, pr in zip(gts, preds):\n        conf_mat[gt, pr] += 1\n\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(conf_mat, cmap=\"Blues\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR-BENCH Confusion Matrix \u2013 Best Run: {best_run}\")\n    for (i, j), v in np.ndenumerate(conf_mat):\n        plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n    plt.tight_layout()\n    fname = f\"spr_bench_confusion_matrix_{best_run}.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data.get(\"num_epochs\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------------------------------------------------------------------\n# 1) Per\u2013run train/val BWA curves  (<=5 similar figures)\nfor i, (run_key, run_dict) in enumerate(runs.items()):\n    if i >= 5:  # obey \u201cat most 5 similar figures\u201d\n        break\n    try:\n        epochs = np.arange(1, len(run_dict[\"metrics\"][\"train\"]) + 1)\n        train_bwa = run_dict[\"metrics\"][\"train\"]\n        val_bwa = run_dict[\"metrics\"][\"val\"]\n\n        plt.figure()\n        plt.plot(epochs, train_bwa, label=\"Train BWA\")\n        plt.plot(epochs, val_bwa, label=\"Validation BWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BWA\")\n        plt.title(f\"SPR-BENCH BWA Learning Curve \u2013 {run_key}\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"spr_bench_bwa_curve_{run_key}.png\"\n        path = os.path.join(working_dir, fname)\n        plt.savefig(path)\n        plt.close()\n        print(f\"Saved {path}\")\n    except Exception as e:\n        print(f\"Error creating BWA curve for {run_key}: {e}\")\n        plt.close()\n\n# ---------------------------------------------------------------------\n# 2) Bar chart comparing test BWA across runs\ntry:\n    run_names = []\n    test_bwa_values = []\n    for rk, rd in runs.items():\n        if \"test_metrics\" in rd and \"BWA\" in rd[\"test_metrics\"]:\n            run_names.append(rk)\n            test_bwa_values.append(rd[\"test_metrics\"][\"BWA\"])\n\n    plt.figure()\n    x_pos = np.arange(len(run_names))\n    plt.bar(x_pos, test_bwa_values, color=\"skyblue\")\n    plt.xticks(x_pos, run_names, rotation=45, ha=\"right\")\n    plt.ylabel(\"Test BWA\")\n    plt.title(\"SPR-BENCH: Test BWA for Different max_epoch Settings\")\n    plt.tight_layout()\n    fname = \"spr_bench_test_bwa_comparison.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating test BWA comparison bar chart: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 3) Confusion matrix of best run (highest test BWA)\ntry:\n    # locate best run\n    best_run = max(\n        runs.items(),\n        key=lambda item: item[1].get(\"test_metrics\", {}).get(\"BWA\", -np.inf),\n    )[0]\n    preds = np.array(runs[best_run][\"predictions\"])\n    gts = np.array(runs[best_run][\"ground_truth\"])\n    num_classes = int(max(preds.max(), gts.max()) + 1)\n    conf_mat = np.zeros((num_classes, num_classes), dtype=int)\n    for gt, pr in zip(gts, preds):\n        conf_mat[gt, pr] += 1\n\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(conf_mat, cmap=\"Blues\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR-BENCH Confusion Matrix \u2013 Best Run: {best_run}\")\n    for (i, j), v in np.ndenumerate(conf_mat):\n        plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n    plt.tight_layout()\n    fname = f\"spr_bench_confusion_matrix_{best_run}.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data.get(\"num_epochs\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------------------------------------------------------------------\n# 1) Per\u2013run train/val BWA curves  (<=5 similar figures)\nfor i, (run_key, run_dict) in enumerate(runs.items()):\n    if i >= 5:  # obey \u201cat most 5 similar figures\u201d\n        break\n    try:\n        epochs = np.arange(1, len(run_dict[\"metrics\"][\"train\"]) + 1)\n        train_bwa = run_dict[\"metrics\"][\"train\"]\n        val_bwa = run_dict[\"metrics\"][\"val\"]\n\n        plt.figure()\n        plt.plot(epochs, train_bwa, label=\"Train BWA\")\n        plt.plot(epochs, val_bwa, label=\"Validation BWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"BWA\")\n        plt.title(f\"SPR-BENCH BWA Learning Curve \u2013 {run_key}\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"spr_bench_bwa_curve_{run_key}.png\"\n        path = os.path.join(working_dir, fname)\n        plt.savefig(path)\n        plt.close()\n        print(f\"Saved {path}\")\n    except Exception as e:\n        print(f\"Error creating BWA curve for {run_key}: {e}\")\n        plt.close()\n\n# ---------------------------------------------------------------------\n# 2) Bar chart comparing test BWA across runs\ntry:\n    run_names = []\n    test_bwa_values = []\n    for rk, rd in runs.items():\n        if \"test_metrics\" in rd and \"BWA\" in rd[\"test_metrics\"]:\n            run_names.append(rk)\n            test_bwa_values.append(rd[\"test_metrics\"][\"BWA\"])\n\n    plt.figure()\n    x_pos = np.arange(len(run_names))\n    plt.bar(x_pos, test_bwa_values, color=\"skyblue\")\n    plt.xticks(x_pos, run_names, rotation=45, ha=\"right\")\n    plt.ylabel(\"Test BWA\")\n    plt.title(\"SPR-BENCH: Test BWA for Different max_epoch Settings\")\n    plt.tight_layout()\n    fname = \"spr_bench_test_bwa_comparison.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating test BWA comparison bar chart: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# 3) Confusion matrix of best run (highest test BWA)\ntry:\n    # locate best run\n    best_run = max(\n        runs.items(),\n        key=lambda item: item[1].get(\"test_metrics\", {}).get(\"BWA\", -np.inf),\n    )[0]\n    preds = np.array(runs[best_run][\"predictions\"])\n    gts = np.array(runs[best_run][\"ground_truth\"])\n    num_classes = int(max(preds.max(), gts.max()) + 1)\n    conf_mat = np.zeros((num_classes, num_classes), dtype=int)\n    for gt, pr in zip(gts, preds):\n        conf_mat[gt, pr] += 1\n\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(conf_mat, cmap=\"Blues\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR-BENCH Confusion Matrix \u2013 Best Run: {best_run}\")\n    for (i, j), v in np.ndenumerate(conf_mat):\n        plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n    plt.tight_layout()\n    fname = f\"spr_bench_confusion_matrix_{best_run}.png\"\n    path = os.path.join(working_dir, fname)\n    plt.savefig(path)\n    plt.close()\n    print(f\"Saved {path}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# Load all experiment_data dictionaries that actually exist -------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_321ca94d4f414dc380718c6085100112_proc_1445253/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_21405c4999084797a998eac2c2647f5a_proc_1445251/experiment_data.npy\",\n    \"experiments/2025-08-30_17-49-45_gnn_for_spr_attempt_0/logs/0-run/experiment_results/experiment_999c6ddfd2c74187bb7d585669a2248a_proc_1445252/experiment_data.npy\",\n]\nall_experiment_data = []\n\n# try to load from the provided absolute paths first\nfor experiment_data_path in experiment_data_path_list:\n    try:\n        root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n        full_path = os.path.join(root, experiment_data_path)\n        if os.path.isfile(full_path):\n            ed = np.load(full_path, allow_pickle=True).item()\n            all_experiment_data.append(ed)\n            print(f\"Loaded {full_path}\")\n        else:\n            print(f\"File not found: {full_path}\")\n    except Exception as e:\n        print(f\"Error loading {experiment_data_path}: {e}\")\n\n# fallback: local copy dropped into working_dir\nfallback_path = os.path.join(working_dir, \"experiment_data.npy\")\nif os.path.isfile(fallback_path):\n    try:\n        ed = np.load(fallback_path, allow_pickle=True).item()\n        all_experiment_data.append(ed)\n        print(f\"Loaded fallback {fallback_path}\")\n    except Exception as e:\n        print(f\"Error loading fallback experiment_data: {e}\")\n\nif len(all_experiment_data) == 0:\n    print(\"No experiment data found, terminating plotting script.\")\nelse:\n    # -----------------------------------------------------------------\n    # Gather per-epoch train/val curves across ALL runs ---------------\n    all_train_curves, all_val_curves = [], []\n    min_common_len = None\n    for ed in all_experiment_data:\n        runs = ed.get(\"num_epochs\", {})\n        for run_key, run_dict in runs.items():\n            train = np.asarray(run_dict[\"metrics\"].get(\"train\", []), dtype=float)\n            val = np.asarray(run_dict[\"metrics\"].get(\"val\", []), dtype=float)\n            # Only keep runs that have both curves of the same length\n            if train.size == 0 or val.size == 0 or train.size != val.size:\n                continue\n            all_train_curves.append(train)\n            all_val_curves.append(val)\n            min_common_len = (\n                train.size\n                if min_common_len is None\n                else min(min_common_len, train.size)\n            )\n\n    # Trim all curves to the shortest common length so epochs align\n    if min_common_len is not None and len(all_train_curves) > 0:\n        all_train_curves = np.stack(\n            [c[:min_common_len] for c in all_train_curves], axis=0\n        )\n        all_val_curves = np.stack([c[:min_common_len] for c in all_val_curves], axis=0)\n        epochs = np.arange(1, min_common_len + 1)\n\n        train_mean = all_train_curves.mean(axis=0)\n        val_mean = all_val_curves.mean(axis=0)\n        train_sem = all_train_curves.std(axis=0, ddof=1) / np.sqrt(\n            all_train_curves.shape[0]\n        )\n        val_sem = all_val_curves.std(axis=0, ddof=1) / np.sqrt(all_val_curves.shape[0])\n    else:\n        train_mean = val_mean = train_sem = val_sem = epochs = None\n\n    # -----------------------------------------------------------------\n    # 1) Aggregated learning curve with SEM bands ---------------------\n    try:\n        if epochs is not None:\n            plt.figure()\n            plt.plot(epochs, train_mean, label=\"Train BWA \u2013 mean\", color=\"tab:blue\")\n            plt.fill_between(\n                epochs,\n                train_mean - train_sem,\n                train_mean + train_sem,\n                color=\"tab:blue\",\n                alpha=0.2,\n                label=\"Train SEM\",\n            )\n            plt.plot(epochs, val_mean, label=\"Val BWA \u2013 mean\", color=\"tab:orange\")\n            plt.fill_between(\n                epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                color=\"tab:orange\",\n                alpha=0.2,\n                label=\"Val SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"BWA\")\n            plt.title(\n                \"SPR-BENCH Mean \u00b1 SEM Train/Val BWA Learning Curve\\n(Aggregated over all runs)\"\n            )\n            plt.legend()\n            plt.tight_layout()\n            fname = \"spr_bench_bwa_mean_sem_curve.png\"\n            path = os.path.join(working_dir, fname)\n            plt.savefig(path)\n            plt.close()\n            print(f\"Saved {path}\")\n        else:\n            print(\"Skipping aggregated learning curve \u2013 insufficient aligned data.\")\n    except Exception as e:\n        print(f\"Error creating aggregated learning-curve plot: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------------------\n    # 2) Bar plot of test-set BWA for each run with overall mean+SEM ---\n    try:\n        run_names, test_bwa_values = [], []\n        for ed in all_experiment_data:\n            runs = ed.get(\"num_epochs\", {})\n            for rk, rd in runs.items():\n                if \"test_metrics\" in rd and \"BWA\" in rd[\"test_metrics\"]:\n                    run_names.append(rk)\n                    test_bwa_values.append(rd[\"test_metrics\"][\"BWA\"])\n\n        if len(test_bwa_values) > 0:\n            test_bwa_values = np.asarray(test_bwa_values, dtype=float)\n            overall_mean = test_bwa_values.mean()\n            overall_sem = test_bwa_values.std(ddof=1) / np.sqrt(test_bwa_values.size)\n\n            x_pos = np.arange(len(run_names))\n            plt.figure(figsize=(max(6, len(run_names) * 0.8), 4))\n            plt.bar(x_pos, test_bwa_values, color=\"skyblue\", label=\"Individual runs\")\n            plt.errorbar(\n                len(run_names) + 0.5,\n                overall_mean,\n                yerr=overall_sem,\n                fmt=\"o\",\n                color=\"red\",\n                label=\"Mean \u00b1 SEM\",\n            )\n            plt.axhline(overall_mean, color=\"red\", linestyle=\"--\", alpha=0.6)\n            plt.xticks(\n                list(x_pos) + [len(run_names) + 0.5],\n                run_names + [\"Mean\"],\n                rotation=45,\n                ha=\"right\",\n            )\n            plt.ylabel(\"Test BWA\")\n            plt.title(\"SPR-BENCH Test BWA Across Runs (Mean \u00b1 SEM)\")\n            plt.legend()\n            plt.tight_layout()\n            fname = \"spr_bench_test_bwa_runs_and_mean.png\"\n            path = os.path.join(working_dir, fname)\n            plt.savefig(path)\n            plt.close()\n            print(f\"Saved {path}\")\n        else:\n            print(\"Skipping test BWA bar chart \u2013 no test metrics available.\")\n    except Exception as e:\n        print(f\"Error creating test-BWA aggregated bar chart: {e}\")\n        plt.close()\n"], "plot_plan": ["We will first load the saved experiment_data.npy, detect every run stored under experiment_data['num_epochs'], and iterate through them. For each run we will pull the logged train/validation BWA sequences and plot them in a line chart so that learning dynamics are visible (one plot per run \u2013 there are at most five). After looping through runs we will aggregate the test\u2010set BWA scores and create a bar chart that compares generalisation quality across all hyper-parameter settings. To give a deeper look into a single model we will find the run with the highest test BWA, build a confusion-matrix style heat-map (counts only, no percentages) from its ground-truth and prediction arrays, and plot it. Each of the three plot types (curves, bar chart, confusion matrix) is wrapped in its own try-except so that failure in one plot does not affect the others; every figure is titled clearly and is saved to working_dir with descriptive file names that include the dataset and plot type. All figures are closed explicitly to avoid memory leakage. The code uses only numpy and matplotlib, assumes no missing keys, and never fabricates data. Finally, the script prints the paths of all successfully written images.", "We load the saved experiment_data.npy, detect the learning rate with the highest final dev BWA, and then generate four plots: (1) BWA train/validation curves for that best LR, (2) loss train/validation curves, (3) a bar chart comparing final dev BWA across all tried LRs, and (4) a simple confusion-matrix heat-map on the test set for the best LR. Each figure is produced in an isolated try-except block, saved into working_dir with descriptive names, and the corresponding figure is closed afterward. Finally, we print the best LR together with its test CWA, SWA and overall BWA to confirm the evaluation metrics.", "We will load the saved experiment_data.npy, inspect the nested dict for the \u201cbatch_size / SPR_BENCH / {bs}\u201d keys, and gather per-epoch losses and BWA metrics as well as the final test metrics for every batch size. Three plots are generated: (1) train vs. validation BWA across epochs for each batch size, (2) train vs. validation loss across epochs, and (3) a bar chart of test BWA, CWA, and SWA for every batch size. Each plot is wrapped in its own try-except block, saved with descriptive names that include the dataset and plot type, and figures are always closed. Only data that exist in experiment_data.npy are used; if the file is missing or malformed we simply print an error and exit. We plot at most the 5 epochs stored (so no need for extra subsampling). All images are written to working_dir, created if necessary.", "We will first load the persisted experiment_data.npy and detect all weight-decay sub-experiments.  \nFor each run we retrieve lists of epoch-wise losses and Balanced Weighted Accuracy (BWA) for both train and validation splits.  \nBecause we must keep the number of plots \u2264 5, we combine related curves: one figure with all loss curves (train/val) across the four weight-decay settings, one with all BWA curves, and one bar chart summarising final test BWA.  \nEvery plot is wrapped in a try-except block, saved inside working_dir with descriptive file names that include \u201cSPR_BENCH\u201d and the plot type, and the figure is always closed.  \nTitles clearly describe what is shown, and subtitles indicate split or hyper-parameter.  \nOnly data already present inside experiment_data.npy is accessed; if a required key is missing the except block simply logs the error and proceeds.  \nFinally, the script prints a short confirmation so the user can inspect the generated PNG files.", "We will first load the saved experiment_data.npy, locate the SPR_BENCH subtree, and identify the hyper-parameter setting (embed_dim) that achieved the highest dev BWA.  For this best setting we will extract the epoch-wise training/validation loss curves as well as the BWA curves and create two separate plots, each enclosed in its own try-except block.  Next we will build a third plot that compares the final dev BWA obtained for each embed_dim as a bar chart, allowing easy hyper-parameter comparison.  All plots follow the naming convention \u201cspr_bench_[content]_plot.png\u201d, are saved inside the mandatory working_dir, and are closed immediately after saving.  Every figure contains clear titles and axis labels; multi-panel subtitles are included where relevant.  We make sure not to fabricate data\u2014everything is read from experiment_data.  We also cap plotting to these three figures (\u22645) to obey the guideline.  Finally, the script prints the location of each saved figure so the user can verify output.  The code is concise, uses only basic matplotlib, and respects all given plotting guidelines.", "We will first load the saved experiment_data.npy dictionary and collect the list of available dropout settings for the \u201cSPR_BENCH\u201d dataset. For each dropout we extract the per-epoch train/val BWA curves and the final test BWA. These tensors are stacked so we can: (1) draw a single multi-line plot that compares dev-set BWA across epochs for all dropout rates, (2) identify the dropout giving the highest final dev BWA and plot its detailed train vs. dev curves, and (3) summarise test BWA versus dropout as a bar chart. Each plot is wrapped in its own try-except block, saved in working_dir with descriptive names, titled clearly, and the figure is closed afterwards. We keep the total number of figures \u2264 5 and avoid touching any non-existent fields. Finally, the script prints a small textual summary (best dropout and its dev/test BWA) so users can see the evaluation metric in the notebook/stdout.", null, "We will first load experiment_data.npy, extract the SPR_BENCH results for the GCN\u2010hidden\u2010dim sweep, and collect hidden_dims, final BWA metrics (train/val), final losses (train/val), and the stored test predictions/ground truth of the best model. Three plots are generated, each in its own try\u2013except block: (1) a line plot of Train/Validation BWA versus hidden dimension, (2) a line plot of Train/Validation loss versus hidden dimension, and (3) a confusion-matrix heat-map for the best model on the test set. All figures are titled clearly, saved to working_dir with descriptive names, and then closed. We also compute and print the overall test accuracy of the best model to give a quick numerical metric. If any plot fails, the error is reported but other plots still proceed. The script adheres strictly to using only data found in experiment_data.npy and follows the required plotting guidelines.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["num_epochs", "learning_rate", "batch_size", "weight_decay", "embed_dim", "dropout_rate", "num_gcn_layers", "gcn_hidden_dim", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script loads the saved NumPy dictionary from the working directory, iterates\nover every hyper-parameter run (e.g. \u201cmax_10\u201d), and for each run extracts the\nstored lists of losses and BWA (balanced weighted accuracy).   For the training\nand validation splits it reports the best (i.e. highest) BWA and the lowest loss\nobserved during training.   For the test split it simply reports the final loss\ntogether with the final BWA, CWA and SWA that were stored after training\nfinished.   Every output section starts with the dataset name (\u201cTraining\ndataset:\u201d, \u201cValidation dataset:\u201d, \u201cTest dataset:\u201d) followed by clearly labelled\nmetric lines.", "Below is a compact script that immediately loads the saved NumPy dictionary,\nidentifies the best learning-rate run for every dataset, and prints the final\nvalues of the most relevant metrics with explicit names. It follows the\nrequested directory conventions and contains no `if __name__ == \"__main__\":`\nguard.", "The code simply loads the saved NumPy dictionary, iterates through each dataset\n(here the only key is \u201cSPR_BENCH\u201d) and every batch-size configuration that was\nrun.   For every run it reports: the final training loss, the best validation\nloss, the final training balanced-weighted accuracy (BWA), the best validation\nBWA, and the four test-set metrics that were stored.   All metric names are\nprinted explicitly, no plots are generated, and the script executes immediately\non import.", "Below is a small utility that loads the saved \u201cexperiment_data.npy\u201d, loops over\nevery weight-decay experiment, and prints clear, self-describing metrics (best\ntrain BWA, best validation BWA, final train loss, final validation loss, and\nfinal test accuracy). It executes immediately and needs no special entry point.", "The script loads the saved experiment_data.npy file from the \u201cworking\u201d\ndirectory, inspects the nested dictionaries to find all runs for each dataset,\nand automatically selects the run whose final-epoch validation balanced-weighted\naccuracy (BWA) is highest. For that best run it extracts the last recorded\ntraining loss, validation loss, training BWA, validation BWA, and (if present)\nthe stored test-set metrics. Those values are printed with explicit metric names\nso the output is self-explanatory. Nothing is plotted and all executable\nstatements live at the top level so the file runs immediately when executed.", "The script will locate and load the file working/experiment_data.npy, convert it\nback to a Python dictionary, and then iterate over every dataset stored inside.\nFor each dataset (here that is \u201cSPR_BENCH\u201d) and for every hyper-parameter\nsetting (each dropout value), it will pull the last element of the training /\nvalidation loss and BWA arrays and the stored test metrics.   It prints the\ndataset name first, then prints each metric with an explicit, descriptive label\nsuch as \u201cfinal training loss,\u201d \u201cfinal validation BWA,\u201d \u201ctest CWA,\u201d etc.\nNothing is plotted and the code runs immediately on execution.", "", "The script loads the stored numpy dictionary, digs into its nested structure,\nidentifies the model configuration that achieved the highest validation\nbalanced-weighted accuracy (BWA), and then reports the corresponding final\nmetrics. It prints the dataset name first, followed by clearly labeled\nmetrics\u2014training BWA, validation BWA, training loss, and validation loss\u2014tied to\nthat best-performing configuration. No plots are generated and the code executes\nimmediately at global scope.", "The script loads the saved NumPy dictionary from the working directory, iterates\nover every hyper-parameter run (e.g. \u201cmax_10\u201d), and for each run extracts the\nstored lists of losses and BWA (balanced weighted accuracy).   For the training\nand validation splits it reports the best (i.e. highest) BWA and the lowest loss\nobserved during training.   For the test split it simply reports the final loss\ntogether with the final BWA, CWA and SWA that were stored after training\nfinished.   Every output section starts with the dataset name (\u201cTraining\ndataset:\u201d, \u201cValidation dataset:\u201d, \u201cTest dataset:\u201d) followed by clearly labelled\nmetric lines.", "The script loads the saved NumPy dictionary from the working directory, iterates\nover every hyper-parameter run (e.g. \u201cmax_10\u201d), and for each run extracts the\nstored lists of losses and BWA (balanced weighted accuracy).   For the training\nand validation splits it reports the best (i.e. highest) BWA and the lowest loss\nobserved during training.   For the test split it simply reports the final loss\ntogether with the final BWA, CWA and SWA that were stored after training\nfinished.   Every output section starts with the dataset name (\u201cTraining\ndataset:\u201d, \u201cValidation dataset:\u201d, \u201cTest dataset:\u201d) followed by clearly labelled\nmetric lines.", "The script loads the saved NumPy dictionary from the working directory, iterates\nover every hyper-parameter run (e.g. \u201cmax_10\u201d), and for each run extracts the\nstored lists of losses and BWA (balanced weighted accuracy).   For the training\nand validation splits it reports the best (i.e. highest) BWA and the lowest loss\nobserved during training.   For the test split it simply reports the final loss\ntogether with the final BWA, CWA and SWA that were stored after training\nfinished.   Every output section starts with the dataset name (\u201cTraining\ndataset:\u201d, \u201cValidation dataset:\u201d, \u201cTest dataset:\u201d) followed by clearly labelled\nmetric lines.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate experiment_data.npy in the working directory and load it\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# 1. Iterate over each hyper-parameter run and print aggregated metrics\nfor run_key, run_dict in experiment_data.get(\"num_epochs\", {}).items():\n    print(f\"\\n===== Results for run: {run_key} =====\")\n\n    # Retrieve lists\n    train_losses = run_dict[\"losses\"][\"train\"]\n    val_losses = run_dict[\"losses\"][\"val\"]\n    train_bwas = run_dict[\"metrics\"][\"train\"]\n    val_bwas = run_dict[\"metrics\"][\"val\"]\n\n    # Compute best / final values\n    best_train_bwa = max(train_bwas) if train_bwas else None\n    lowest_train_loss = min(train_losses) if train_losses else None\n\n    best_val_bwa = max(val_bwas) if val_bwas else None\n    lowest_val_loss = min(val_losses) if val_losses else None\n\n    test_metrics = run_dict.get(\"test_metrics\", {})\n    test_loss = test_metrics.get(\"loss\")\n    test_bwa = test_metrics.get(\"BWA\")\n    test_cwa = test_metrics.get(\"CWA\")\n    test_swa = test_metrics.get(\"SWA\")\n\n    # -----------------------------------------------------------------\n    # 2. Print metrics with explicit labels\n    print(\"Training dataset:\")\n    if best_train_bwa is not None:\n        print(f\"  training best balanced weighted accuracy: {best_train_bwa:.4f}\")\n    if lowest_train_loss is not None:\n        print(f\"  training lowest loss: {lowest_train_loss:.4f}\")\n\n    print(\"Validation dataset:\")\n    if best_val_bwa is not None:\n        print(f\"  validation best balanced weighted accuracy: {best_val_bwa:.4f}\")\n    if lowest_val_loss is not None:\n        print(f\"  validation lowest loss: {lowest_val_loss:.4f}\")\n\n    print(\"Test dataset:\")\n    if test_bwa is not None:\n        print(f\"  test balanced weighted accuracy: {test_bwa:.4f}\")\n    if test_cwa is not None:\n        print(f\"  test color-weighted accuracy: {test_cwa:.4f}\")\n    if test_swa is not None:\n        print(f\"  test shape-weighted accuracy: {test_swa:.4f}\")\n    if test_loss is not None:\n        print(f\"  test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate experiment file ----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# helper to compute plain accuracy on stored test predictions ----------\ndef simple_accuracy(preds, labels):\n    if not preds or not labels:\n        return float(\"nan\")\n    return sum(int(p == y) for p, y in zip(preds, labels)) / len(preds)\n\n\n# ---------------------------------------------------------------------\n# iterate over datasets ------------------------------------------------\nfor hp_group_name, datasets in experiment_data.items():  # e.g. 'learning_rate'\n    for dataset_name, lr_runs in datasets.items():  # e.g. 'SPR_BENCH'\n        # -----------------------------------------------------------------\n        # find the run (learning-rate) with the best final validation BWA --\n        best_lr_key, best_entry, best_val_bwa = None, None, -float(\"inf\")\n        for lr_key, entry in lr_runs.items():\n            final_val_bwa = (\n                entry[\"metrics\"][\"val\"][-1]\n                if entry[\"metrics\"][\"val\"]\n                else -float(\"inf\")\n            )\n            if final_val_bwa > best_val_bwa:\n                best_lr_key, best_entry, best_val_bwa = lr_key, entry, final_val_bwa\n\n        # -----------------------------------------------------------------\n        # print results ----------------------------------------------------\n        print(dataset_name)  # dataset header\n\n        # learning-rate used\n        print(f\"best learning rate: {best_lr_key}\")\n\n        # training / validation metrics\n        print(\n            f\"training balanced weighted accuracy: {best_entry['metrics']['train'][-1]:.4f}\"\n        )\n        print(\n            f\"validation balanced weighted accuracy: {best_entry['metrics']['val'][-1]:.4f}\"\n        )\n        print(f\"training loss: {best_entry['losses']['train'][-1]:.4f}\")\n        print(f\"validation loss: {best_entry['losses']['val'][-1]:.4f}\")\n\n        # optional test accuracy (plain) if predictions are stored\n        preds = best_entry.get(\"predictions\", [])\n        labels = best_entry.get(\"ground_truth\", [])\n        if preds and labels:\n            print(f\"test accuracy: {simple_accuracy(preds, labels):.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load the nested dictionary produced during training\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Traverse and print metrics\nbatch_size_section = experiment_data.get(\"batch_size\", {})\n\nfor dataset_name, runs in batch_size_section.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for bs_str, run_data in runs.items():\n        print(f\"  Configuration \u2013 batch size {bs_str}\")\n\n        # Series collected during training\n        train_losses = run_data[\"losses\"][\"train\"]\n        val_losses = run_data[\"losses\"][\"val\"]\n        train_bwa = run_data[\"metrics\"][\"train\"]\n        val_bwa = run_data[\"metrics\"][\"val\"]\n\n        # Final / best scores\n        final_train_loss = train_losses[-1]\n        best_val_loss = min(val_losses)\n        final_train_bwa = train_bwa[-1]\n        best_val_bwa = max(val_bwa)\n\n        # Test-set metrics\n        test_metrics = run_data[\"test_metrics\"]\n\n        # Print results with explicit metric names\n        print(f\"    training loss: {final_train_loss:.4f}\")\n        print(f\"    validation loss (best): {best_val_loss:.4f}\")\n        print(f\"    training BWA: {final_train_bwa:.4f}\")\n        print(f\"    validation BWA (best): {best_val_bwa:.4f}\")\n        print(f\"    test loss: {test_metrics['loss']:.4f}\")\n        print(f\"    test BWA: {test_metrics['bwa']:.4f}\")\n        print(f\"    test CWA: {test_metrics['cwa']:.4f}\")\n        print(f\"    test SWA: {test_metrics['swa']:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate experiment data ----------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# helper to compute simple accuracy for the held-out test split -------\ndef simple_accuracy(preds, labels):\n    correct = sum(int(p == y) for p, y in zip(preds, labels))\n    return correct / (len(labels) or 1)\n\n\n# ---------------------------------------------------------------------\n# iterate over every experimental run ---------------------------------\nfor tag, exp in experiment_data.get(\"weight_decay\", {}).items():\n    # Best (max) BWA on training and validation\n    best_train_bwa = max(exp[\"metrics\"][\"train\"]) if exp[\"metrics\"][\"train\"] else None\n    best_val_bwa = max(exp[\"metrics\"][\"val\"]) if exp[\"metrics\"][\"val\"] else None\n\n    # Final losses\n    final_train_loss = exp[\"losses\"][\"train\"][-1] if exp[\"losses\"][\"train\"] else None\n    final_val_loss = exp[\"losses\"][\"val\"][-1] if exp[\"losses\"][\"val\"] else None\n\n    # Test accuracy from stored predictions / ground truth\n    test_acc = simple_accuracy(exp.get(\"predictions\", []), exp.get(\"ground_truth\", []))\n\n    # -----------------------------------------------------------------\n    # printing ---------------------------------------------------------\n    print(f\"\\nDataset: {tag}\")  # dataset name\n    print(f\"best train BWA: {best_train_bwa:.4f}\")  # metric name + value\n    print(f\"best validation BWA: {best_val_bwa:.4f}\")\n    print(f\"final train loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_val_loss:.4f}\")\n    print(f\"test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate experiment file and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to pretty-print a value or fall back to \"N/A\"\ndef fmt(x, precision=4):\n    return f\"{x:.{precision}f}\" if isinstance(x, (float, int)) else \"N/A\"\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and report metrics\nfor dataset_name, runs in experiment_data.get(\"embed_dim\", {}).items():\n    # -----------------------------------------------------------------\n    # Identify the run (embed_dim) with the best final validation BWA\n    best_key, best_val_bwa = None, -float(\"inf\")\n    for run_key, run_dict in runs.items():\n        val_bwa_series = run_dict[\"metrics\"][\"val\"]\n        if val_bwa_series:  # make sure list is not empty\n            last_val_bwa = val_bwa_series[-1]\n            if last_val_bwa > best_val_bwa:\n                best_val_bwa = last_val_bwa\n                best_key = run_key\n\n    if best_key is None:\n        continue  # nothing to report for this dataset\n\n    best_run = runs[best_key]\n    embed_dim_value = best_key.split(\"_\")[-1]  # e.g., \"dim_64\" -> \"64\"\n\n    # -----------------------------------------------------------------\n    # Extract final-epoch training / validation metrics\n    final_train_loss = best_run[\"losses\"][\"train\"][-1]\n    final_val_loss = best_run[\"losses\"][\"val\"][-1]\n    final_train_bwa = best_run[\"metrics\"][\"train\"][-1]\n    final_val_bwa = best_run[\"metrics\"][\"val\"][-1]\n\n    # Test metrics are stored only for the best model (should be this run)\n    test_metrics = best_run.get(\"test_metrics\", {})\n    test_loss = test_metrics.get(\"loss\")\n    test_bwa = test_metrics.get(\"BWA\")\n    test_cwa = test_metrics.get(\"CWA\")\n    test_swa = test_metrics.get(\"SWA\")\n\n    # -----------------------------------------------------------------\n    # Print results\n    print(dataset_name)  # dataset header\n    print(f\"best embedding dimension: {embed_dim_value}\")\n    print(f\"training loss: {fmt(final_train_loss)}\")\n    print(f\"validation loss: {fmt(final_val_loss)}\")\n    print(f\"training balanced weighted accuracy: {fmt(final_train_bwa)}\")\n    print(f\"validation balanced weighted accuracy: {fmt(final_val_bwa)}\")\n    print(f\"test loss: {fmt(test_loss)}\")\n    print(f\"test balanced weighted accuracy: {fmt(test_bwa)}\")\n    print(f\"test color weighted accuracy: {fmt(test_cwa)}\")\n    print(f\"test shape weighted accuracy: {fmt(test_swa)}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not find {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# Traverse the stored results and print the requested metrics\nfor sweep_name, datasets in experiment_data.items():  # e.g. 'dropout_rate'\n    for dataset_name, runs in datasets.items():  # e.g. 'SPR_BENCH'\n        print(dataset_name)  # dataset header\n        for run_key, run_dict in runs.items():  # e.g. dropout value '0.1'\n            # Extract final epoch values\n            final_train_loss = run_dict[\"losses\"][\"train\"][-1]\n            final_val_loss = run_dict[\"losses\"][\"val\"][-1]\n            final_train_bwa = run_dict[\"metrics\"][\"train\"][-1]\n            final_val_bwa = run_dict[\"metrics\"][\"val\"][-1]\n\n            # Test metrics\n            test_metrics = run_dict[\"test_metrics\"]\n            test_loss = test_metrics[\"loss\"]\n            test_bwa = test_metrics[\"BWA\"]\n            test_cwa = test_metrics[\"CWA\"]\n            test_swa = test_metrics[\"SWA\"]\n\n            # -----------------------------------------------------------------\n            # Printing with clear, descriptive labels\n            print(f\"  {sweep_name} value: {run_key}\")\n            print(f\"    final training loss: {final_train_loss:.4f}\")\n            print(f\"    final validation loss: {final_val_loss:.4f}\")\n            print(f\"    final training BWA: {final_train_bwa:.4f}\")\n            print(f\"    final validation BWA: {final_val_bwa:.4f}\")\n            print(f\"    test loss: {test_loss:.4f}\")\n            print(f\"    test BWA: {test_bwa:.4f}\")\n            print(f\"    test CWA: {test_cwa:.4f}\")\n            print(f\"    test SWA: {test_swa:.4f}\")\n            print()  # blank line for readability\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# Locate the saved numpy results file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load the dictionary that was saved with np.save(..., allow_pickle=True)\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------\n# Loop through experiments and datasets, then print best metrics\nfor exp_name, datasets in experiment_data.items():  # exp_name == \"gcn_hidden_dim\"\n    for dataset_name, info in datasets.items():  # dataset_name == \"SPR_BENCH\"\n        # Fetch per-hidden-dim lists\n        hidden_dims = info[\"hidden_dims\"]\n        train_bwa_list = info[\"metrics\"][\"train\"]\n        val_bwa_list = info[\"metrics\"][\"val\"]\n        train_loss_list = info[\"losses\"][\"train\"]\n        val_loss_list = info[\"losses\"][\"val\"]\n\n        if not hidden_dims:  # Safety check\n            continue\n\n        # Select the configuration with the highest validation BWA\n        best_idx = int(np.argmax(val_bwa_list))\n        best_hidden_dim = hidden_dims[best_idx]\n\n        # Corresponding final metrics\n        best_train_bwa = train_bwa_list[best_idx]\n        best_val_bwa = val_bwa_list[best_idx]\n        best_train_loss = train_loss_list[best_idx]\n        best_val_loss = val_loss_list[best_idx]\n\n        # ----------------------------------------------------\n        # Print results\n        print(f\"Dataset: {dataset_name}\")\n        print(f\"best hidden dimension: {best_hidden_dim}\")\n        print(f\"training balanced weighted accuracy: {best_train_bwa:.4f}\")\n        print(f\"validation balanced weighted accuracy: {best_val_bwa:.4f}\")\n        print(f\"training loss: {best_train_loss:.4f}\")\n        print(f\"validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate experiment_data.npy in the working directory and load it\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# 1. Iterate over each hyper-parameter run and print aggregated metrics\nfor run_key, run_dict in experiment_data.get(\"num_epochs\", {}).items():\n    print(f\"\\n===== Results for run: {run_key} =====\")\n\n    # Retrieve lists\n    train_losses = run_dict[\"losses\"][\"train\"]\n    val_losses = run_dict[\"losses\"][\"val\"]\n    train_bwas = run_dict[\"metrics\"][\"train\"]\n    val_bwas = run_dict[\"metrics\"][\"val\"]\n\n    # Compute best / final values\n    best_train_bwa = max(train_bwas) if train_bwas else None\n    lowest_train_loss = min(train_losses) if train_losses else None\n\n    best_val_bwa = max(val_bwas) if val_bwas else None\n    lowest_val_loss = min(val_losses) if val_losses else None\n\n    test_metrics = run_dict.get(\"test_metrics\", {})\n    test_loss = test_metrics.get(\"loss\")\n    test_bwa = test_metrics.get(\"BWA\")\n    test_cwa = test_metrics.get(\"CWA\")\n    test_swa = test_metrics.get(\"SWA\")\n\n    # -----------------------------------------------------------------\n    # 2. Print metrics with explicit labels\n    print(\"Training dataset:\")\n    if best_train_bwa is not None:\n        print(f\"  training best balanced weighted accuracy: {best_train_bwa:.4f}\")\n    if lowest_train_loss is not None:\n        print(f\"  training lowest loss: {lowest_train_loss:.4f}\")\n\n    print(\"Validation dataset:\")\n    if best_val_bwa is not None:\n        print(f\"  validation best balanced weighted accuracy: {best_val_bwa:.4f}\")\n    if lowest_val_loss is not None:\n        print(f\"  validation lowest loss: {lowest_val_loss:.4f}\")\n\n    print(\"Test dataset:\")\n    if test_bwa is not None:\n        print(f\"  test balanced weighted accuracy: {test_bwa:.4f}\")\n    if test_cwa is not None:\n        print(f\"  test color-weighted accuracy: {test_cwa:.4f}\")\n    if test_swa is not None:\n        print(f\"  test shape-weighted accuracy: {test_swa:.4f}\")\n    if test_loss is not None:\n        print(f\"  test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate experiment_data.npy in the working directory and load it\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# 1. Iterate over each hyper-parameter run and print aggregated metrics\nfor run_key, run_dict in experiment_data.get(\"num_epochs\", {}).items():\n    print(f\"\\n===== Results for run: {run_key} =====\")\n\n    # Retrieve lists\n    train_losses = run_dict[\"losses\"][\"train\"]\n    val_losses = run_dict[\"losses\"][\"val\"]\n    train_bwas = run_dict[\"metrics\"][\"train\"]\n    val_bwas = run_dict[\"metrics\"][\"val\"]\n\n    # Compute best / final values\n    best_train_bwa = max(train_bwas) if train_bwas else None\n    lowest_train_loss = min(train_losses) if train_losses else None\n\n    best_val_bwa = max(val_bwas) if val_bwas else None\n    lowest_val_loss = min(val_losses) if val_losses else None\n\n    test_metrics = run_dict.get(\"test_metrics\", {})\n    test_loss = test_metrics.get(\"loss\")\n    test_bwa = test_metrics.get(\"BWA\")\n    test_cwa = test_metrics.get(\"CWA\")\n    test_swa = test_metrics.get(\"SWA\")\n\n    # -----------------------------------------------------------------\n    # 2. Print metrics with explicit labels\n    print(\"Training dataset:\")\n    if best_train_bwa is not None:\n        print(f\"  training best balanced weighted accuracy: {best_train_bwa:.4f}\")\n    if lowest_train_loss is not None:\n        print(f\"  training lowest loss: {lowest_train_loss:.4f}\")\n\n    print(\"Validation dataset:\")\n    if best_val_bwa is not None:\n        print(f\"  validation best balanced weighted accuracy: {best_val_bwa:.4f}\")\n    if lowest_val_loss is not None:\n        print(f\"  validation lowest loss: {lowest_val_loss:.4f}\")\n\n    print(\"Test dataset:\")\n    if test_bwa is not None:\n        print(f\"  test balanced weighted accuracy: {test_bwa:.4f}\")\n    if test_cwa is not None:\n        print(f\"  test color-weighted accuracy: {test_cwa:.4f}\")\n    if test_swa is not None:\n        print(f\"  test shape-weighted accuracy: {test_swa:.4f}\")\n    if test_loss is not None:\n        print(f\"  test loss: {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate experiment_data.npy in the working directory and load it\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# 1. Iterate over each hyper-parameter run and print aggregated metrics\nfor run_key, run_dict in experiment_data.get(\"num_epochs\", {}).items():\n    print(f\"\\n===== Results for run: {run_key} =====\")\n\n    # Retrieve lists\n    train_losses = run_dict[\"losses\"][\"train\"]\n    val_losses = run_dict[\"losses\"][\"val\"]\n    train_bwas = run_dict[\"metrics\"][\"train\"]\n    val_bwas = run_dict[\"metrics\"][\"val\"]\n\n    # Compute best / final values\n    best_train_bwa = max(train_bwas) if train_bwas else None\n    lowest_train_loss = min(train_losses) if train_losses else None\n\n    best_val_bwa = max(val_bwas) if val_bwas else None\n    lowest_val_loss = min(val_losses) if val_losses else None\n\n    test_metrics = run_dict.get(\"test_metrics\", {})\n    test_loss = test_metrics.get(\"loss\")\n    test_bwa = test_metrics.get(\"BWA\")\n    test_cwa = test_metrics.get(\"CWA\")\n    test_swa = test_metrics.get(\"SWA\")\n\n    # -----------------------------------------------------------------\n    # 2. Print metrics with explicit labels\n    print(\"Training dataset:\")\n    if best_train_bwa is not None:\n        print(f\"  training best balanced weighted accuracy: {best_train_bwa:.4f}\")\n    if lowest_train_loss is not None:\n        print(f\"  training lowest loss: {lowest_train_loss:.4f}\")\n\n    print(\"Validation dataset:\")\n    if best_val_bwa is not None:\n        print(f\"  validation best balanced weighted accuracy: {best_val_bwa:.4f}\")\n    if lowest_val_loss is not None:\n        print(f\"  validation lowest loss: {lowest_val_loss:.4f}\")\n\n    print(\"Test dataset:\")\n    if test_bwa is not None:\n        print(f\"  test balanced weighted accuracy: {test_bwa:.4f}\")\n    if test_cwa is not None:\n        print(f\"  test color-weighted accuracy: {test_cwa:.4f}\")\n    if test_swa is not None:\n        print(f\"  test shape-weighted accuracy: {test_swa:.4f}\")\n    if test_loss is not None:\n        print(f\"  test loss: {test_loss:.4f}\")\n", ""], "parse_term_out": ["['\\n===== Results for run: max_10 =====', '\\n', 'Training dataset:', '\\n', '\ntraining best balanced weighted accuracy: 0.9125', '\\n', '  training lowest\nloss: 0.2928', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9068', '\\n', '  validation lowest loss: 0.2822', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6403', '\\n', '\ntest color-weighted accuracy: 0.6607', '\\n', '  test shape-weighted accuracy:\n0.6200', '\\n', '  test loss: 0.9122', '\\n', '\\n===== Results for run: max_20\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9395', '\\n', '  training lowest loss: 0.2371', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9337', '\\n', '\nvalidation lowest loss: 0.2231', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6552', '\\n', '  test color-weighted accuracy: 0.6766',\n'\\n', '  test shape-weighted accuracy: 0.6339', '\\n', '  test loss: 1.1232',\n'\\n', '\\n===== Results for run: max_30 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9453', '\\n', '  training lowest\nloss: 0.2128', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9395', '\\n', '  validation lowest loss: 0.2104', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6525', '\\n', '\ntest color-weighted accuracy: 0.6740', '\\n', '  test shape-weighted accuracy:\n0.6311', '\\n', '  test loss: 1.2309', '\\n', '\\n===== Results for run: max_40\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9478', '\\n', '  training lowest loss: 0.2111', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9423', '\\n', '\nvalidation lowest loss: 0.2071', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6532', '\\n', '  test color-weighted accuracy: 0.6746',\n'\\n', '  test shape-weighted accuracy: 0.6318', '\\n', '  test loss: 1.2184',\n'\\n', '\\n===== Results for run: max_50 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9456', '\\n', '  training lowest\nloss: 0.2232', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9394', '\\n', '  validation lowest loss: 0.2158', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6534', '\\n', '\ntest color-weighted accuracy: 0.6751', '\\n', '  test shape-weighted accuracy:\n0.6318', '\\n', '  test loss: 1.1268', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best learning rate: 3e-03', '\\n', 'training balanced\nweighted accuracy: 0.8996', '\\n', 'validation balanced weighted accuracy:\n0.8931', '\\n', 'training loss: 0.2803', '\\n', 'validation loss: 0.2859', '\\n',\n'test accuracy: 0.6626', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration \u2013 batch size 32', '\\n', '\ntraining loss: 0.3252', '\\n', '    validation loss (best): 0.3145', '\\n', '\ntraining BWA: 0.8880', '\\n', '    validation BWA (best): 0.8891', '\\n', '\ntest loss: 0.8762', '\\n', '    test BWA: 0.6422', '\\n', '    test CWA: 0.6618',\n'\\n', '    test SWA: 0.6227', '\\n', '  Configuration \u2013 batch size 64', '\\n', '\ntraining loss: 0.3645', '\\n', '    validation loss (best): 0.3548', '\\n', '\ntraining BWA: 0.8629', '\\n', '    validation BWA (best): 0.8639', '\\n', '\ntest loss: 0.8094', '\\n', '    test BWA: 0.6344', '\\n', '    test CWA: 0.6531',\n'\\n', '    test SWA: 0.6157', '\\n', '  Configuration \u2013 batch size 128', '\\n', '\ntraining loss: 0.3782', '\\n', '    validation loss (best): 0.3677', '\\n', '\ntraining BWA: 0.8560', '\\n', '    validation BWA (best): 0.8595', '\\n', '\ntest loss: 0.8026', '\\n', '    test BWA: 0.6310', '\\n', '    test CWA: 0.6490',\n'\\n', '    test SWA: 0.6129', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: wd_0.0', '\\n', 'best train BWA: 0.8645', '\\n', 'best validation\nBWA: 0.8621', '\\n', 'final train loss: 0.3526', '\\n', 'final validation loss:\n0.3559', '\\n', 'test accuracy: 0.6563', '\\n', '\\nDataset: wd_1e-05', '\\n', 'best\ntrain BWA: 0.8559', '\\n', 'best validation BWA: 0.8575', '\\n', 'final train\nloss: 0.3695', '\\n', 'final validation loss: 0.3587', '\\n', 'test accuracy:\n0.6511', '\\n', '\\nDataset: wd_0.0001', '\\n', 'best train BWA: 0.8796', '\\n',\n'best validation BWA: 0.8783', '\\n', 'final train loss: 0.3504', '\\n', 'final\nvalidation loss: 0.3351', '\\n', 'test accuracy: 0.6531', '\\n', '\\nDataset:\nwd_0.001', '\\n', 'best train BWA: 0.8566', '\\n', 'best validation BWA: 0.8567',\n'\\n', 'final train loss: 0.3725', '\\n', 'final validation loss: 0.3741', '\\n',\n'test accuracy: 0.6575', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['SPR_BENCH', '\\n', 'best embedding dimension: 128', '\\n', 'training loss:\n0.3062', '\\n', 'validation loss: 0.2917', '\\n', 'training balanced weighted\naccuracy: 0.8989', '\\n', 'validation balanced weighted accuracy: 0.9002', '\\n',\n'test loss: 0.8991', '\\n', 'test balanced weighted accuracy: 0.6406', '\\n',\n'test color weighted accuracy: 0.6610', '\\n', 'test shape weighted accuracy:\n0.6203', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  dropout_rate value: 0.0', '\\n', '    final training loss:\n0.2790', '\\n', '    final validation loss: 0.2743', '\\n', '    final training\nBWA: 0.9127', '\\n', '    final validation BWA: 0.9112', '\\n', '    test loss:\n0.9829', '\\n', '    test BWA: 0.6377', '\\n', '    test CWA: 0.6580', '\\n', '\ntest SWA: 0.6174', '\\n', '\\n', '  dropout_rate value: 0.1', '\\n', '    final\ntraining loss: 0.2938', '\\n', '    final validation loss: 0.2797', '\\n', '\nfinal training BWA: 0.9079', '\\n', '    final validation BWA: 0.9062', '\\n', '\ntest loss: 0.9396', '\\n', '    test BWA: 0.6444', '\\n', '    test CWA: 0.6647',\n'\\n', '    test SWA: 0.6241', '\\n', '\\n', '  dropout_rate value: 0.3', '\\n', '\nfinal training loss: 0.3217', '\\n', '    final validation loss: 0.3096', '\\n', '\nfinal training BWA: 0.8884', '\\n', '    final validation BWA: 0.8902', '\\n', '\ntest loss: 0.8486', '\\n', '    test BWA: 0.6431', '\\n', '    test CWA: 0.6632',\n'\\n', '    test SWA: 0.6230', '\\n', '\\n', '  dropout_rate value: 0.5', '\\n', '\nfinal training loss: 0.3278', '\\n', '    final validation loss: 0.3102', '\\n', '\nfinal training BWA: 0.8979', '\\n', '    final validation BWA: 0.8950', '\\n', '\ntest loss: 0.8489', '\\n', '    test BWA: 0.6414', '\\n', '    test CWA: 0.6612',\n'\\n', '    test SWA: 0.6216', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "", "['Dataset: SPR_BENCH', '\\n', 'best hidden dimension: 128', '\\n', 'training\nbalanced weighted accuracy: 0.8948', '\\n', 'validation balanced weighted\naccuracy: 0.8886', '\\n', 'training loss: 0.3111', '\\n', 'validation loss:\n0.3067', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n===== Results for run: max_10 =====', '\\n', 'Training dataset:', '\\n', '\ntraining best balanced weighted accuracy: 0.9048', '\\n', '  training lowest\nloss: 0.2992', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9043', '\\n', '  validation lowest loss: 0.2865', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6434', '\\n', '\ntest color-weighted accuracy: 0.6640', '\\n', '  test shape-weighted accuracy:\n0.6229', '\\n', '  test loss: 0.9288', '\\n', '\\n===== Results for run: max_20\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9387', '\\n', '  training lowest loss: 0.2384', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9318', '\\n', '\nvalidation lowest loss: 0.2259', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6500', '\\n', '  test color-weighted accuracy: 0.6710',\n'\\n', '  test shape-weighted accuracy: 0.6290', '\\n', '  test loss: 1.1425',\n'\\n', '\\n===== Results for run: max_30 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9394', '\\n', '  training lowest\nloss: 0.2298', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9330', '\\n', '  validation lowest loss: 0.2234', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6505', '\\n', '\ntest color-weighted accuracy: 0.6715', '\\n', '  test shape-weighted accuracy:\n0.6294', '\\n', '  test loss: 1.0788', '\\n', '\\n===== Results for run: max_40\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9354', '\\n', '  training lowest loss: 0.2303', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9327', '\\n', '\nvalidation lowest loss: 0.2218', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6508', '\\n', '  test color-weighted accuracy: 0.6720',\n'\\n', '  test shape-weighted accuracy: 0.6297', '\\n', '  test loss: 1.0480',\n'\\n', '\\n===== Results for run: max_50 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9561', '\\n', '  training lowest\nloss: 0.1919', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9504', '\\n', '  validation lowest loss: 0.1887', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6595', '\\n', '\ntest color-weighted accuracy: 0.6814', '\\n', '  test shape-weighted accuracy:\n0.6375', '\\n', '  test loss: 1.4175', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\n===== Results for run: max_10 =====', '\\n', 'Training dataset:', '\\n', '\ntraining best balanced weighted accuracy: 0.9132', '\\n', '  training lowest\nloss: 0.2932', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9127', '\\n', '  validation lowest loss: 0.2856', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6403', '\\n', '\ntest color-weighted accuracy: 0.6605', '\\n', '  test shape-weighted accuracy:\n0.6201', '\\n', '  test loss: 0.9275', '\\n', '\\n===== Results for run: max_20\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9367', '\\n', '  training lowest loss: 0.2418', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9291', '\\n', '\nvalidation lowest loss: 0.2398', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6510', '\\n', '  test color-weighted accuracy: 0.6722',\n'\\n', '  test shape-weighted accuracy: 0.6298', '\\n', '  test loss: 1.1019',\n'\\n', '\\n===== Results for run: max_30 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9428', '\\n', '  training lowest\nloss: 0.2187', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9362', '\\n', '  validation lowest loss: 0.2142', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6511', '\\n', '\ntest color-weighted accuracy: 0.6723', '\\n', '  test shape-weighted accuracy:\n0.6299', '\\n', '  test loss: 1.1984', '\\n', '\\n===== Results for run: max_40\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9449', '\\n', '  training lowest loss: 0.2201', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9370', '\\n', '\nvalidation lowest loss: 0.2128', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6525', '\\n', '  test color-weighted accuracy: 0.6738',\n'\\n', '  test shape-weighted accuracy: 0.6312', '\\n', '  test loss: 1.1653',\n'\\n', '\\n===== Results for run: max_50 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9490', '\\n', '  training lowest\nloss: 0.2084', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9414', '\\n', '  validation lowest loss: 0.2044', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6556', '\\n', '\ntest color-weighted accuracy: 0.6771', '\\n', '  test shape-weighted accuracy:\n0.6341', '\\n', '  test loss: 1.2060', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\n===== Results for run: max_10 =====', '\\n', 'Training dataset:', '\\n', '\ntraining best balanced weighted accuracy: 0.9172', '\\n', '  training lowest\nloss: 0.2823', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9142', '\\n', '  validation lowest loss: 0.2648', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6440', '\\n', '\ntest color-weighted accuracy: 0.6648', '\\n', '  test shape-weighted accuracy:\n0.6232', '\\n', '  test loss: 0.9065', '\\n', '\\n===== Results for run: max_20\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9400', '\\n', '  training lowest loss: 0.2353', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9345', '\\n', '\nvalidation lowest loss: 0.2286', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6520', '\\n', '  test color-weighted accuracy: 0.6737',\n'\\n', '  test shape-weighted accuracy: 0.6304', '\\n', '  test loss: 1.1884',\n'\\n', '\\n===== Results for run: max_30 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9470', '\\n', '  training lowest\nloss: 0.2153', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9424', '\\n', '  validation lowest loss: 0.2034', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6570', '\\n', '\ntest color-weighted accuracy: 0.6784', '\\n', '  test shape-weighted accuracy:\n0.6355', '\\n', '  test loss: 1.2242', '\\n', '\\n===== Results for run: max_40\n=====', '\\n', 'Training dataset:', '\\n', '  training best balanced weighted\naccuracy: 0.9510', '\\n', '  training lowest loss: 0.2026', '\\n', 'Validation\ndataset:', '\\n', '  validation best balanced weighted accuracy: 0.9439', '\\n', '\nvalidation lowest loss: 0.1982', '\\n', 'Test dataset:', '\\n', '  test balanced\nweighted accuracy: 0.6544', '\\n', '  test color-weighted accuracy: 0.6764',\n'\\n', '  test shape-weighted accuracy: 0.6323', '\\n', '  test loss: 1.2933',\n'\\n', '\\n===== Results for run: max_50 =====', '\\n', 'Training dataset:', '\\n',\n'  training best balanced weighted accuracy: 0.9514', '\\n', '  training lowest\nloss: 0.2052', '\\n', 'Validation dataset:', '\\n', '  validation best balanced\nweighted accuracy: 0.9448', '\\n', '  validation lowest loss: 0.2034', '\\n',\n'Test dataset:', '\\n', '  test balanced weighted accuracy: 0.6603', '\\n', '\ntest color-weighted accuracy: 0.6821', '\\n', '  test shape-weighted accuracy:\n0.6385', '\\n', '  test loss: 1.2331', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
