<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 2], [0, 4], [0, 1], [0, 3], [0, 5], [0, 6], [0, 8], [0, 7]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0], [1.0, 1.0]], "plan": ["We improve rule induction by moving from order-agnostic histograms to an order-\naware sequence encoder.\u2028First, every glyph is mapped to a latent cluster (k=16)\nobtained with K-means on simple char-level vectors; this keeps the \u201csymbolic\nglyph clustering\u201d philosophy while remaining cheap. \u2028Each SPR sequence is then\nconverted to a list of cluster IDs and fed (after padding) into a small\nbidirectional GRU with learnable cluster embeddings; the final hidden state is\npooled and passed to a classifier. \u2028This architecture can capture ordering and\nrepetition patterns that histograms miss, yet is light enough to train in\nminutes. \u2028We implement early stopping on the dev Harmonic CSA, and at every\nepoch compute Color-, Shape-, and the required Sequence-Novelty-Weighted\nAccuracy (novelty is measured w.r.t. glyph\u2013cluster pairs unseen in training).\n\u2028All metrics, losses, predictions and ground-truth are stored in\nexperiment_data.npy for later analysis. \u2028The script follows the GPU/CPU and\ndata-saving rules and prints the final dev and test scores, aiming to push\nbeyond the current 70 / 65 SOTA. \u2028Everything runs end-to-end without manual\nentry points.", "Ablation name: No-Glyph-Clustering. The solution keeps the original metric\npipeline (which still needs the 16-way K-Means for SNWA) but bypasses the\ncluster abstraction when feeding the GRU.   We \u2776 compute the usual glyph-to-\ncluster map for metrics only, \u2777 build a full vocabulary over all unique glyph\nstrings, \u2778 set PAD_IDX = vocab_size, \u2779 convert sequences to raw-token indices,\nand \u277a train an otherwise identical bidirectional-GRU classifier.   All plottable\nartefacts are stored in an experiment_data dictionary and saved to\nexperiment_data.npy under the ablation key \u201cno_glyph_clustering\u201d.", "Ablation name: No-Bidirectional-GRU. The following script is identical to the\nbaseline except that the GRU is now single-direction (forward-only).  All hyper-\nparameters, the training loop, evaluation code, and the data-saving convention\nremain unchanged, so any change in HCSA or SNWA can be attributed solely to the\nremoval of the backward GRU.  Results are stored under the ablation key \"No-\nBidirectional-GRU\" in the same experiment_data.npy file.", "Ablation name: Mean-Pooling-Encoder. The GRU encoder is replaced by a\npermutation-invariant mean-pooling encoder: each glyph is embedded, the (non-\npadding) embeddings are averaged, and a linear layer maps the pooled vector to\nclass logits. This ablation keeps every other part of the pipeline (data\nloading, clustering, metrics, training loop, early stopping) unchanged so we can\ndirectly measure the importance of sequential structure. Results, losses, and\npredictions are stored under the \u201cMeanPoolEncoder/SPR_BENCH\u201d key in\nexperiment_data.npy for later plotting.", "Ablation name: Frozen-Cluster-Embeddings. Here is the ablation that freezes the\ncluster\u2013embedding layer so it remains random throughout training; everything\nelse (data-handling, GRU, objective, early stopping, logging) is unchanged.", "Ablation name: Random-Cluster-Assignments. Our solution keeps the entire\nbaseline training / evaluation pipeline unchanged, but it swaps the data-driven\nK-Means step with a reproducible, fixed random mapping that uniformly\ndistributes glyphs over the same 16 clusters. All bookkeeping, early-stopping,\nmetric tracking and file-saving follow the format required for ablation studies.", "Ablation name: Shuffled-Sequence-Order. We keep all components identical to the\nbaseline except for the data pipeline: before every forward pass, each sequence\nis randomly permuted so that positional information is erased. This is done with\na new dataset class that shuffles the glyph order inside __getitem__, meaning\nthe model always receives a bag-of-glyph representation. Training, validation\nand test loaders all use this shuffling dataset, allowing us to quantify\nperformance loss caused by destroying sequence order. All metrics, early-\nstopping logic and experiment logging remain unchanged and are stored under the\nablation key \u201cShuffled-Sequence-Order\u201d.", "Ablation name: No-Early-Stopping. The solution simply removes the early-stopping\nlogic so the GRU classifier always trains for the full max_epochs (25).   We\nkeep all other components unchanged (data loading, model, metrics, etc.) so any\nperformance difference directly reflects the impact of early stopping.\nTraining/validation losses and metrics are recorded every epoch in the required\nexperiment_data structure under the ablation tag \"no_early_stopping\", and final\npredictions/ground-truth for dev/test are saved.   All plottable data are stored\nin working/experiment_data.npy for later analysis.", "Ablation name: No-Sequence-Packing. Below is the ablation implementation outline\nfollowed by the full executable code.  Outline: 1. Keep all preprocessing /\ndata-loading logic unchanged. 2. Replace the original GRUClassifier with\nGRUClassifierNoPack that feeds the padded sequence directly to nn.GRU (no\npack_padded_sequence), still extracting the bidirectional last hidden state. 3.\nMaintain the same training / validation loop and early-stopping logic, but log\nresults under the ablation key 'no_sequence_packing'. 4. Save everything into\nexperiment_data.npy using the required dictionary structure."], "code": ["import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict, Tuple\n\n# ---------------------------------------------------- #\n# 0. house-keeping                                      #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ---------------------------------------------------- #\n# 1. locate SPR_BENCH                                   #\n# ---------------------------------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({split: _load(split) for split in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------------------------------- #\n# 2. helper metrics                                     #\n# ---------------------------------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------------------------------- #\n# 3. glyph clustering                                   #\n# ---------------------------------------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    if len(g) >= 2:\n        return [ord(g[0]) - 65, ord(g[1]) - 48]\n    return [ord(g[0]) - 65, 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ---------------------------------------------------- #\n# 4. Sequence\u2192tensor dataset                            #\n# ---------------------------------------------------- #\nPAD_IDX = k_clusters  # padding index after clusters 0..k-1\n\n\ndef seq_to_clusters(seq: str) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq.strip().split()]\n\n\ndef build_known_pairs(seqs) -> set:\n    s = set()\n    for seq in seqs:\n        for tok in seq.strip().split():\n            s.add((tok, glyph_to_cluster.get(tok, 0)))\n    return s\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total = 0\n    novel = 0\n    for tok in seq.strip().split():\n        pair = (tok, glyph_to_cluster.get(tok, 0))\n        total += 1\n        if pair not in train_known_pairs:\n            novel += 1\n    novelty_ratio = novel / total if total else 0.0\n    return 1.0 + novelty_ratio\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, sequences: List[str], labels: List[int]):\n        self.seqs = [seq_to_clusters(s) for s in sequences]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    return {\"seq\": seq_tensor, \"len\": lengths, \"label\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------- #\n# 5. model                                             #\n# ---------------------------------------------------- #\nclass GRUClassifier(nn.Module):\n    def __init__(\n        self,\n        n_clusters: int,\n        pad_idx: int,\n        emb_dim: int = 64,\n        hid: int = 128,\n        num_classes: int = 10,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)\n        return self.out(h)\n\n\nmodel = GRUClassifier(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------- #\n# 6. evaluation helper                                 #\n# ---------------------------------------------------- #\ndef evaluate(model, loader, seq_raw) -> Dict[str, float]:\n    model.eval()\n    totals, preds, gts = 0.0, [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        idx_start = 0\n        for batch in loader:\n            bseq = batch[\"seq\"].to(device)\n            blen = batch[\"len\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(bseq, blen)\n            loss = criterion(logits, lbl)\n            total_loss += loss.item() * lbl.size(0)\n            pred = logits.argmax(1)\n            preds.extend(pred.cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n            idx_start += lbl.size(0)\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------------------------------- #\n# 7. training loop with early stopping                 #\n# ---------------------------------------------------- #\nmax_epochs = 25\npatience = 5\nbest_hcs = -1.0\nsince_best = 0\nbest_state = None\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": {\"dev\": [], \"test\": []},\n        \"ground_truth\": {\"dev\": [], \"test\": []},\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    # --- train ---\n    model.train()\n    total_loss = 0.0\n    n_seen = 0\n    for batch in train_loader:\n        bseq = batch[\"seq\"].to(device)\n        blen = batch[\"len\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(bseq, blen)\n        loss = criterion(logits, lbl)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * lbl.size(0)\n        n_seen += lbl.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append((epoch, train_loss))\n    # --- validate ---\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append((epoch, val_stats[\"loss\"]))\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f'Epoch {epoch}: validation_loss = {val_stats[\"loss\"]:.4f} '\n        f'HCSA={val_stats[\"HCSA\"]:.3f} SNWA={val_stats[\"SNWA\"]:.3f}'\n    )\n    # early stopping\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs = val_stats[\"HCSA\"]\n        best_state = copy.deepcopy(model.state_dict())\n        since_best = 0\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\n# restore best\nif best_state:\n    model.load_state_dict(best_state)\n\n# ---------------------------------------------------- #\n# 8. final evaluation                                  #\n# ---------------------------------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_final[\"preds\"]\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_final[\"gts\"]\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_final[\"preds\"]\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_final[\"gts\"]\n\nprint(\n    f'Final Dev - CWA:{dev_final[\"CWA\"]:.3f} SWA:{dev_final[\"SWA\"]:.3f} '\n    f'HCSA:{dev_final[\"HCSA\"]:.3f} SNWA:{dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'Final Test - CWA:{test_final[\"CWA\"]:.3f} SWA:{test_final[\"SWA\"]:.3f} '\n    f'HCSA:{test_final[\"HCSA\"]:.3f} SNWA:{test_final[\"SNWA\"]:.3f}'\n)\n\n# ---------------------------------------------------- #\n# 9. save experiment data                              #\n# ---------------------------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict, Tuple\n\n# ---------------------------------------------------- #\n# 0. house-keeping                                      #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ---------------------------------------------------- #\n# 1. locate SPR_BENCH                                   #\n# ---------------------------------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{split}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(s) for s in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------------------------------- #\n# 2. helper metrics (unchanged)                         #\n# ---------------------------------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------------------------------- #\n# 3. glyph clustering (only for metrics)                #\n# ---------------------------------------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    if len(g) >= 2:\n        return [ord(g[0]) - 65, ord(g[1]) - 48]\n    return [ord(g[0]) - 65, 0]\n\n\nall_glyphs_metric = set(\n    tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split()\n)\nvecs = np.array([glyph_vector(g) for g in all_glyphs_metric])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs_metric, kmeans.fit_predict(vecs))}\nprint(\n    f\"Clustered {len(all_glyphs_metric)} glyphs into {k_clusters} clusters for metrics.\"\n)\n\n\ndef build_known_pairs(seqs) -> set:\n    s = set()\n    for seq in seqs:\n        for tok in seq.strip().split():\n            s.add((tok, glyph_to_cluster.get(tok, 0)))\n    return s\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total = 0\n    novel = 0\n    for tok in seq.strip().split():\n        pair = (tok, glyph_to_cluster.get(tok, 0))\n        total += 1\n        if pair not in train_known_pairs:\n            novel += 1\n    novelty_ratio = novel / total if total else 0.0\n    return 1.0 + novelty_ratio\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\n# ---------------------------------------------------- #\n# 4. vocabulary without clustering                      #\n# ---------------------------------------------------- #\nall_glyphs_vocab = sorted(\n    {\n        tok\n        for split in [\"train\", \"dev\", \"test\"]\n        for seq in spr[split][\"sequence\"]\n        for tok in seq.strip().split()\n    }\n)\nglyph_to_id = {g: i for i, g in enumerate(all_glyphs_vocab)}\nvocab_size = len(glyph_to_id)\nPAD_IDX = vocab_size  # padding index at the end\nprint(f\"Vocabulary size (no clustering) = {vocab_size}\")\n\n\ndef seq_to_ids(seq: str) -> List[int]:\n    return [glyph_to_id[tok] for tok in seq.strip().split()]\n\n\n# ---------------------------------------------------- #\n# 5. Dataset & DataLoader                               #\n# ---------------------------------------------------- #\nclass SPRSeqDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.seqs = [seq_to_ids(s) for s in sequences]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lengths, dtype=torch.long),\n        \"label\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------- #\n# 6. Model                                              #\n# ---------------------------------------------------- #\nclass GRUClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        pad_idx: int,\n        emb_dim: int = 64,\n        hid: int = 128,\n        num_classes: int = 10,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)\n        return self.out(h)\n\n\nmodel = GRUClassifier(vocab_size, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------- #\n# 7. Evaluation helper                                  #\n# ---------------------------------------------------- #\ndef evaluate(model, loader, seq_raw) -> Dict[str, float]:\n    model.eval()\n    preds, gts = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            bseq = batch[\"seq\"].to(device)\n            blen = batch[\"len\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(bseq, blen)\n            loss = criterion(logits, lbl)\n            total_loss += loss.item() * lbl.size(0)\n            pred = logits.argmax(1)\n            preds.extend(pred.cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------------------------------- #\n# 8. Training loop                                      #\n# ---------------------------------------------------- #\nmax_epochs, patience = 25, 5\nbest_hcs, since_best, best_state = -1.0, 0, None\n\nexperiment_data = {\n    \"no_glyph_clustering\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    model.train()\n    total_loss = 0.0\n    n_seen = 0\n    for batch in train_loader:\n        bseq = batch[\"seq\"].to(device)\n        blen = batch[\"len\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(bseq, blen)\n        loss = criterion(logits, lbl)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * lbl.size(0)\n        n_seen += lbl.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"no_glyph_clustering\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"no_glyph_clustering\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"no_glyph_clustering\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f'Epoch {epoch}: val_loss={val_stats[\"loss\"]:.4f} HCSA={val_stats[\"HCSA\"]:.3f} SNWA={val_stats[\"SNWA\"]:.3f}'\n    )\n\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs = val_stats[\"HCSA\"]\n        best_state = copy.deepcopy(model.state_dict())\n        since_best = 0\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state is not None:\n    model.load_state_dict(best_state)\n\n# ---------------------------------------------------- #\n# 9. Final evaluation                                   #\n# ---------------------------------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\nexp = experiment_data[\"no_glyph_clustering\"][\"SPR_BENCH\"]\nexp[\"predictions\"][\"dev\"], exp[\"ground_truth\"][\"dev\"] = (\n    dev_final[\"preds\"],\n    dev_final[\"gts\"],\n)\nexp[\"predictions\"][\"test\"], exp[\"ground_truth\"][\"test\"] = (\n    test_final[\"preds\"],\n    test_final[\"gts\"],\n)\n\nprint(\n    f'Final Dev  - CWA:{dev_final[\"CWA\"]:.3f} SWA:{dev_final[\"SWA\"]:.3f} HCSA:{dev_final[\"HCSA\"]:.3f} SNWA:{dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'Final Test - CWA:{test_final[\"CWA\"]:.3f} SWA:{test_final[\"SWA\"]:.3f} HCSA:{test_final[\"HCSA\"]:.3f} SNWA:{test_final[\"SNWA\"]:.3f}'\n)\n\n# ---------------------------------------------------- #\n# 10. save                                              #\n# ---------------------------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "# ---------------------------------------------------- #\n# 0. house-keeping                                      #\n# ---------------------------------------------------- #\nimport os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ---------------------------------------------------- #\n# 1. locate SPR_BENCH                                   #\n# ---------------------------------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({split: _load(split) for split in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------------------------------- #\n# 2. helper metrics                                     #\n# ---------------------------------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------------------------------- #\n# 3. glyph clustering                                   #\n# ---------------------------------------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    if len(g) >= 2:\n        return [ord(g[0]) - 65, ord(g[1]) - 48]\n    return [ord(g[0]) - 65, 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ---------------------------------------------------- #\n# 4. Sequence\u2192tensor dataset                            #\n# ---------------------------------------------------- #\nPAD_IDX = k_clusters  # padding index after clusters 0..k-1\n\n\ndef seq_to_clusters(seq: str) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq.strip().split()]\n\n\ndef build_known_pairs(seqs) -> set:\n    s = set()\n    for seq in seqs:\n        for tok in seq.strip().split():\n            s.add((tok, glyph_to_cluster.get(tok, 0)))\n    return s\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total = novel = 0\n    for tok in seq.strip().split():\n        total += 1\n        if (tok, glyph_to_cluster.get(tok, 0)) not in train_known_pairs:\n            novel += 1\n    return 1.0 + (novel / total if total else 0.0)\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.seqs = [seq_to_clusters(s) for s in sequences]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lengths, dtype=torch.long),\n        \"label\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------- #\n# 5. model (No-Bidirectional-GRU)                       #\n# ---------------------------------------------------- #\nclass GRUClassifier(nn.Module):\n    def __init__(self, n_clusters, pad_idx, emb_dim=64, hid=128, num_classes=10):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=False)\n        self.out = nn.Linear(hid, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)  # h: (1, batch, hid)\n        return self.out(h[-1])\n\n\nmodel = GRUClassifier(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------- #\n# 6. evaluation helper                                 #\n# ---------------------------------------------------- #\ndef evaluate(model, loader, seq_raw) -> Dict[str, float]:\n    model.eval()\n    preds, gts = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            logits = model(batch[\"seq\"].to(device), batch[\"len\"].to(device))\n            lbl = batch[\"label\"].to(device)\n            total_loss += criterion(logits, lbl).item() * lbl.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------------------------------- #\n# 7. training loop with early stopping                 #\n# ---------------------------------------------------- #\nexperiment_data = {\n    \"No-Bidirectional-GRU\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\nmax_epochs, patience = 25, 5\nbest_hcs, since_best, best_state = -1.0, 0, None\n\nfor epoch in range(1, max_epochs + 1):\n    # --- train ---\n    model.train()\n    total_loss, n_seen = 0.0, 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"seq\"].to(device), batch[\"len\"].to(device))\n        loss = criterion(logits, batch[\"label\"].to(device))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"label\"].size(0)\n        n_seen += batch[\"label\"].size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"No-Bidirectional-GRU\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    # --- validate ---\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"No-Bidirectional-GRU\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"No-Bidirectional-GRU\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f'Epoch {epoch}: val_loss={val_stats[\"loss\"]:.4f} HCSA={val_stats[\"HCSA\"]:.3f} SNWA={val_stats[\"SNWA\"]:.3f}'\n    )\n\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs, best_state, since_best = (\n            val_stats[\"HCSA\"],\n            copy.deepcopy(model.state_dict()),\n            0,\n        )\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state:\n    model.load_state_dict(best_state)\n\n# ---------------------------------------------------- #\n# 8. final evaluation                                  #\n# ---------------------------------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\ned = experiment_data[\"No-Bidirectional-GRU\"][\"SPR_BENCH\"]\ned[\"predictions\"][\"dev\"], ed[\"ground_truth\"][\"dev\"] = (\n    dev_final[\"preds\"],\n    dev_final[\"gts\"],\n)\ned[\"predictions\"][\"test\"], ed[\"ground_truth\"][\"test\"] = (\n    test_final[\"preds\"],\n    test_final[\"gts\"],\n)\n\nprint(\n    f'Final Dev  - CWA:{dev_final[\"CWA\"]:.3f} SWA:{dev_final[\"SWA\"]:.3f} HCSA:{dev_final[\"HCSA\"]:.3f} SNWA:{dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'Final Test - CWA:{test_final[\"CWA\"]:.3f} SWA:{test_final[\"SWA\"]:.3f} HCSA:{test_final[\"HCSA\"]:.3f} SNWA:{test_final[\"SNWA\"]:.3f}'\n)\n\n# ---------------------------------------------------- #\n# 9. save experiment data                              #\n# ---------------------------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\n# ----------------------- 0. house-keeping ----------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ----------------------- 1. locate SPR_BENCH -------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands += [\n        pathlib.Path.home() / p for p in [\"SPR_BENCH\", \"AI-Scientist-v2/SPR_BENCH\"]\n    ]\n    cands += [pathlib.Path(\"/workspace/SPR_BENCH\"), pathlib.Path(\"/data/SPR_BENCH\")]\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(\"Found SPR_BENCH at\", p)\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(s) for s in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ----------------------- 2. helper metrics ---------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------------- 3. glyph clustering -------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    return [ord(g[0]) - 65, ord(g[1]) - 48 if len(g) >= 2 else 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ----------------------- 4. dataset ----------------------------- #\nPAD_IDX = k_clusters\n\n\ndef seq_to_clusters(seq: str) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq.split()]\n\n\ntrain_known_pairs = set(\n    (tok, glyph_to_cluster.get(tok, 0))\n    for seq in spr[\"train\"][\"sequence\"]\n    for tok in seq.split()\n)\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total = novel = 0\n    for tok in seq.split():\n        pair = (tok, glyph_to_cluster.get(tok, 0))\n        total += 1\n        if pair not in train_known_pairs:\n            novel += 1\n    return 1.0 + novel / total if total else 1.0\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, seqs, List_labels):\n        self.seqs = [seq_to_clusters(s) for s in seqs]\n        self.labels = torch.tensor(List_labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lens = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lens)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lens, dtype=torch.long),\n        \"label\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ----------------------- 5. MeanPooling model ------------------- #\nclass MeanPoolClassifier(nn.Module):\n    def __init__(\n        self, n_clusters: int, pad_idx: int, emb_dim: int = 64, num_classes: int = 10\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.out = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x, lens):\n        emb = self.emb(x)  # B,L,E\n        mask = (x != PAD_IDX).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(dim=1)  # B,E\n        lengths = lens.clamp(min=1).unsqueeze(-1).type_as(summed)\n        mean = summed / lengths  # B,E\n        return self.out(mean)\n\n\nmodel = MeanPoolClassifier(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ----------------------- 6. evaluation -------------------------- #\ndef evaluate(model, loader, seq_raw):\n    model.eval()\n    preds = []\n    gts = []\n    total_loss = 0.0\n    with torch.no_grad():\n        for i, batch in enumerate(loader):\n            x = batch[\"seq\"].to(device)\n            lens = batch[\"len\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = model(x, lens)\n            loss = criterion(logits, y)\n            total_loss += loss.item() * y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(y.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ----------------------- 7. training loop ----------------------- #\nmax_epochs = 25\npatience = 5\nbest_hcs = -1.0\nsince_best = 0\nbest_state = None\nexperiment_data = {\n    \"MeanPoolEncoder\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    model.train()\n    total_loss = 0.0\n    n_seen = 0\n    for batch in train_loader:\n        x = batch[\"seq\"].to(device)\n        lens = batch[\"len\"].to(device)\n        y = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(x, lens)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * y.size(0)\n        n_seen += y.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"MeanPoolEncoder\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"MeanPoolEncoder\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"MeanPoolEncoder\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f\"Epoch {epoch}: val_loss={val_stats['loss']:.4f} HCSA={val_stats['HCSA']:.3f} SNWA={val_stats['SNWA']:.3f}\"\n    )\n\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs = val_stats[\"HCSA\"]\n        best_state = copy.deepcopy(model.state_dict())\n        since_best = 0\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state:\n    model.load_state_dict(best_state)\n\n# ----------------------- 8. final evaluation -------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\ned = experiment_data[\"MeanPoolEncoder\"][\"SPR_BENCH\"]\ned[\"predictions\"][\"dev\"] = dev_final[\"preds\"]\ned[\"ground_truth\"][\"dev\"] = dev_final[\"gts\"]\ned[\"predictions\"][\"test\"] = test_final[\"preds\"]\ned[\"ground_truth\"][\"test\"] = test_final[\"gts\"]\n\nprint(\n    f\"Final Dev - CWA:{dev_final['CWA']:.3f} SWA:{dev_final['SWA']:.3f} \"\n    f\"HCSA:{dev_final['HCSA']:.3f} SNWA:{dev_final['SNWA']:.3f}\"\n)\nprint(\n    f\"Final Test - CWA:{test_final['CWA']:.3f} SWA:{test_final['SWA']:.3f} \"\n    f\"HCSA:{test_final['HCSA']:.3f} SNWA:{test_final['SNWA']:.3f}\"\n)\n\n# ----------------------- 9. save ------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", working_dir + \"/experiment_data.npy\")\n", "import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\n# ---------------------------------------------------- #\n# 0. house-keeping                                      #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ---------------------------------------------------- #\n# 1. locate & load SPR_BENCH                            #\n# ---------------------------------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for p in [cwd] + list(cwd.parents):\n        cands.append(p / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({split: _load(split) for split in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------------------------------- #\n# 2. helper metrics                                     #\n# ---------------------------------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(t[0] for t in seq.split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------------------------------- #\n# 3. glyph clustering                                   #\n# ---------------------------------------------------- #\ndef glyph_vector(g: str):\n    return [ord(g[0]) - 65, ord(g[1]) - 48] if len(g) >= 2 else [ord(g[0]) - 65, 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ---------------------------------------------------- #\n# 4. sequence-to-tensor dataset                         #\n# ---------------------------------------------------- #\nPAD_IDX = k_clusters\n\n\ndef seq_to_clusters(seq: str):\n    return [glyph_to_cluster.get(t, 0) for t in seq.split()]\n\n\ndef build_known_pairs(seqs):\n    return {(tok, glyph_to_cluster.get(tok, 0)) for seq in seqs for tok in seq.split()}\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str):\n    total = novel = 0\n    for tok in seq.split():\n        total += 1\n        novel += (tok, glyph_to_cluster.get(tok, 0)) not in train_known_pairs\n    return 1.0 + (novel / total if total else 0.0)\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs = [seq_to_clusters(s) for s in seqs]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lens = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lens)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : lens[i]] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lens),\n        \"label\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------- #\n# 5. model with FROZEN cluster embeddings              #\n# ---------------------------------------------------- #\nclass GRUClassifier(nn.Module):\n    def __init__(\n        self, n_clusters, pad_idx, emb_dim=64, hid=128, num_classes=10, frozen=True\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        if frozen:\n            self.emb.weight.requires_grad_(False)  # key line for ablation\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)\n        return self.out(h)\n\n\nmodel = GRUClassifier(k_clusters, PAD_IDX, num_classes=num_classes, frozen=True).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n\n\n# ---------------------------------------------------- #\n# 6. evaluation helper                                 #\n# ---------------------------------------------------- #\ndef evaluate(model, loader, seq_raw):\n    model.eval()\n    preds = []\n    gts = []\n    tot_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            logits = model(batch[\"seq\"].to(device), batch[\"len\"].to(device))\n            loss = criterion(logits, batch[\"label\"].to(device))\n            tot_loss += loss.item() * batch[\"label\"].size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(batch[\"label\"].tolist())\n    avg_loss = tot_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": harmonic_csa(cwa, swa),\n        \"SNWA\": snwa(seq_raw, gts, preds),\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------------------------------- #\n# 7. training loop with early stopping                 #\n# ---------------------------------------------------- #\nmax_epochs, patience = 25, 5\nbest_hcs, best_state = -1.0, None\nsince_best = 0\n\nexperiment_data = {\n    \"Frozen-Cluster-Embeddings\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    model.train()\n    tot_loss = 0.0\n    n_seen = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        logits = model(batch[\"seq\"].to(device), batch[\"len\"].to(device))\n        loss = criterion(logits, batch[\"label\"].to(device))\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"label\"].size(0)\n        n_seen += batch[\"label\"].size(0)\n    tr_loss = tot_loss / n_seen\n    experiment_data[\"Frozen-Cluster-Embeddings\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, tr_loss)\n    )\n\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"Frozen-Cluster-Embeddings\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"Frozen-Cluster-Embeddings\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f\"Epoch {epoch}: val_loss={val_stats['loss']:.4f} HCSA={val_stats['HCSA']:.3f} SNWA={val_stats['SNWA']:.3f}\"\n    )\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs = val_stats[\"HCSA\"]\n        best_state = copy.deepcopy(model.state_dict())\n        since_best = 0\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state:\n    model.load_state_dict(best_state)\n\n# ---------------------------------------------------- #\n# 8. final evaluation                                  #\n# ---------------------------------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\nexp = experiment_data[\"Frozen-Cluster-Embeddings\"][\"SPR_BENCH\"]\nexp[\"predictions\"][\"dev\"], exp[\"ground_truth\"][\"dev\"] = (\n    dev_final[\"preds\"],\n    dev_final[\"gts\"],\n)\nexp[\"predictions\"][\"test\"], exp[\"ground_truth\"][\"test\"] = (\n    test_final[\"preds\"],\n    test_final[\"gts\"],\n)\n\nprint(\n    f\"Final Dev - CWA:{dev_final['CWA']:.3f} SWA:{dev_final['SWA']:.3f} \"\n    f\"HCSA:{dev_final['HCSA']:.3f} SNWA:{dev_final['SNWA']:.3f}\"\n)\nprint(\n    f\"Final Test - CWA:{test_final['CWA']:.3f} SWA:{test_final['SWA']:.3f} \"\n    f\"HCSA:{test_final['HCSA']:.3f} SNWA:{test_final['SNWA']:.3f}\"\n)\n\n# ---------------------------------------------------- #\n# 9. save experiment data                              #\n# ---------------------------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\n# ---------------------------------------------------- #\n# 0. house-keeping & reproducibility                    #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# experiment data container (required format)\nexperiment_data = {\n    \"RandomClusterAssignment\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\n\n# ---------------------------------------------------- #\n# 1. locate and load SPR_BENCH                          #\n# ---------------------------------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for p in [cwd] + list(cwd.parents):\n        cands.append(p / \"SPR_BENCH\")\n    cands += [\n        pathlib.Path.home() / \"SPR_BENCH\",\n        pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n        pathlib.Path(\"/workspace/SPR_BENCH\"),\n        pathlib.Path(\"/data/SPR_BENCH\"),\n    ]\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{split}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(s) for s in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------------------------------- #\n# 2. helper metrics                                     #\n# ---------------------------------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------------------------------- #\n# 3. RANDOM glyph\u2192cluster mapping (ablation)            #\n# ---------------------------------------------------- #\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split())\nk_clusters = 16\nglyph_list = list(all_glyphs)\nrng = np.random.RandomState(SEED)\nrng.shuffle(glyph_list)\nglyph_to_cluster = {g: i % k_clusters for i, g in enumerate(glyph_list)}\nprint(f\"(Random) Assigned {len(all_glyphs)} glyphs to {k_clusters} clusters.\")\n\n# ---------------------------------------------------- #\n# 4. Sequence \u2192 tensor dataset                          #\n# ---------------------------------------------------- #\nPAD_IDX = k_clusters  # padding after 0..k-1\n\n\ndef seq_to_clusters(seq: str) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq.strip().split()]\n\n\ndef build_known_pairs(seqs) -> set:\n    return {\n        (tok, glyph_to_cluster.get(tok, 0))\n        for seq in seqs\n        for tok in seq.strip().split()\n    }\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total, novel = 0, 0\n    for tok in seq.strip().split():\n        pair = (tok, glyph_to_cluster.get(tok, 0))\n        total += 1\n        if pair not in train_known_pairs:\n            novel += 1\n    return 1.0 + (novel / total if total else 0.0)\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.seqs = [seq_to_clusters(s) for s in sequences]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lengths, dtype=torch.long),\n        \"label\": labels,\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------- #\n# 5. model                                              #\n# ---------------------------------------------------- #\nclass GRUClassifier(nn.Module):\n    def __init__(self, n_clusters, pad_idx, emb_dim=64, hid=128, num_classes=10):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)\n        return self.out(h)\n\n\nmodel = GRUClassifier(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------- #\n# 6. evaluation helper                                  #\n# ---------------------------------------------------- #\ndef evaluate(model, loader, seq_raw):\n    model.eval()\n    preds, gts, total_loss = [], [], 0.0\n    with torch.no_grad():\n        for batch in loader:\n            bseq = batch[\"seq\"].to(device)\n            blen = batch[\"len\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(bseq, blen)\n            loss = criterion(logits, lbl)\n            total_loss += loss.item() * lbl.size(0)\n            p = logits.argmax(1)\n            preds.extend(p.cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------------------------------- #\n# 7. training loop with early stopping                  #\n# ---------------------------------------------------- #\nmax_epochs, patience = 25, 5\nbest_hcs, since_best, best_state = -1.0, 0, None\n\nfor epoch in range(1, max_epochs + 1):\n    # training\n    model.train()\n    total_loss, n_seen = 0.0, 0\n    for batch in train_loader:\n        bseq, blen, lbl = (\n            batch[\"seq\"].to(device),\n            batch[\"len\"].to(device),\n            batch[\"label\"].to(device),\n        )\n        optimizer.zero_grad()\n        logits = model(bseq, blen)\n        loss = criterion(logits, lbl)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * lbl.size(0)\n        n_seen += lbl.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    # validation\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f'Epoch {epoch}: val_loss={val_stats[\"loss\"]:.4f} HCSA={val_stats[\"HCSA\"]:.3f} SNWA={val_stats[\"SNWA\"]:.3f}'\n    )\n\n    # early stopping\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs, best_state, since_best = (\n            val_stats[\"HCSA\"],\n            copy.deepcopy(model.state_dict()),\n            0,\n        )\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state:\n    model.load_state_dict(best_state)\n\n# ---------------------------------------------------- #\n# 8. final evaluation                                   #\n# ---------------------------------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\n\nexp = experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"]\nexp[\"predictions\"][\"dev\"], exp[\"ground_truth\"][\"dev\"] = (\n    dev_final[\"preds\"],\n    dev_final[\"gts\"],\n)\nexp[\"predictions\"][\"test\"], exp[\"ground_truth\"][\"test\"] = (\n    test_final[\"preds\"],\n    test_final[\"gts\"],\n)\n\nprint(\n    f'Dev  : CWA={dev_final[\"CWA\"]:.3f} SWA={dev_final[\"SWA\"]:.3f} HCSA={dev_final[\"HCSA\"]:.3f} SNWA={dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'Test : CWA={test_final[\"CWA\"]:.3f} SWA={test_final[\"SWA\"]:.3f} HCSA={test_final[\"HCSA\"]:.3f} SNWA={test_final[\"SNWA\"]:.3f}'\n)\n\n# ---------------------------------------------------- #\n# 9. save experiment data                               #\n# ---------------------------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\n# ----------------------------- 0. house-keeping ----------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ----------------------------- 1. locate SPR_BENCH -------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({split: _load(split) for split in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ----------------------------- 2. helper metrics ---------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ----------------------------- 3. glyph clustering -------------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    if len(g) >= 2:\n        return [ord(g[0]) - 65, ord(g[1]) - 48]\n    return [ord(g[0]) - 65, 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ----------------------------- 4. data utilities ---------------------------- #\nPAD_IDX = k_clusters  # padding index\n\n\ndef seq_to_clusters(seq_tokens: List[str]) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq_tokens]\n\n\ndef build_known_pairs(seqs) -> set:\n    s = set()\n    for seq in seqs:\n        for tok in seq.strip().split():\n            s.add((tok, glyph_to_cluster.get(tok, 0)))\n    return s\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total, novel = 0, 0\n    for tok in seq.strip().split():\n        total += 1\n        if (tok, glyph_to_cluster.get(tok, 0)) not in train_known_pairs:\n            novel += 1\n    return 1.0 + (novel / total if total else 0.0)\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\n# ---------------------- 4b. Shuffled-Order Dataset -------------------------- #\nclass SPRSeqDatasetShuf(Dataset):\n    \"\"\"\n    Dataset that shuffles glyph order on every access \u2013 destroys sequential order.\n    \"\"\"\n\n    def __init__(self, sequences, labels):\n        self.raw_seqs = list(sequences)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        tokens = self.raw_seqs[idx].strip().split()\n        random.shuffle(tokens)  # crucial shuffle\n        clusters = seq_to_clusters(tokens)\n        return {\"seq\": clusters, \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    return {\"seq\": seq_tensor, \"len\": lengths, \"label\": labels}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDatasetShuf(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDatasetShuf(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDatasetShuf(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ----------------------------- 5. model  ------------------------------------ #\nclass GRUClassifier(nn.Module):\n    def __init__(self, n_clusters, pad_idx, emb_dim=64, hid=128, num_classes=10):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)\n        return self.out(h)\n\n\nmodel = GRUClassifier(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ----------------------------- 6. evaluation helper ------------------------ #\ndef evaluate(model, loader, seq_raw):\n    model.eval()\n    preds, gts = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            bseq = batch[\"seq\"].to(device)\n            blen = batch[\"len\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(bseq, blen)\n            loss = criterion(logits, lbl)\n            total_loss += loss.item() * lbl.size(0)\n            pred = logits.argmax(1)\n            preds.extend(pred.cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ----------------------------- 7. training loop ---------------------------- #\nmax_epochs = 25\npatience = 5\nbest_hcs, since_best, best_state = -1.0, 0, None\n\nexperiment_data = {\n    \"Shuffled-Sequence-Order\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    model.train()\n    total_loss, n_seen = 0.0, 0\n    for batch in train_loader:\n        bseq = batch[\"seq\"].to(device)\n        blen = batch[\"len\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(bseq, blen)\n        loss = criterion(logits, lbl)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * lbl.size(0)\n        n_seen += lbl.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"Shuffled-Sequence-Order\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"Shuffled-Sequence-Order\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"Shuffled-Sequence-Order\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f'Epoch {epoch}: val_loss={val_stats[\"loss\"]:.4f} '\n        f'HCSA={val_stats[\"HCSA\"]:.3f} SNWA={val_stats[\"SNWA\"]:.3f}'\n    )\n\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs = val_stats[\"HCSA\"]\n        best_state = copy.deepcopy(model.state_dict())\n        since_best = 0\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state:\n    model.load_state_dict(best_state)\n\n# ----------------------------- 8. final evaluation ------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\nexp = experiment_data[\"Shuffled-Sequence-Order\"][\"SPR_BENCH\"]\nexp[\"predictions\"][\"dev\"] = dev_final[\"preds\"]\nexp[\"ground_truth\"][\"dev\"] = dev_final[\"gts\"]\nexp[\"predictions\"][\"test\"] = test_final[\"preds\"]\nexp[\"ground_truth\"][\"test\"] = test_final[\"gts\"]\n\nprint(\n    f'DEV  \u2013 CWA:{dev_final[\"CWA\"]:.3f} SWA:{dev_final[\"SWA\"]:.3f} '\n    f'HCSA:{dev_final[\"HCSA\"]:.3f} SNWA:{dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'TEST \u2013 CWA:{test_final[\"CWA\"]:.3f} SWA:{test_final[\"SWA\"]:.3f} '\n    f'HCSA:{test_final[\"HCSA\"]:.3f} SNWA:{test_final[\"SNWA\"]:.3f}'\n)\n\n# ----------------------------- 9. save results ----------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "# No-Early-Stopping ablation for SPR_BENCH\nimport os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\n# ---------------------------------------------------- #\n# 0. house-keeping                                      #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\n\n# ---------------------------------------------------- #\n# 1. locate SPR_BENCH                                   #\n# ---------------------------------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({split: _load(split) for split in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------------------------------- #\n# 2. helper metrics                                     #\n# ---------------------------------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------------------------------- #\n# 3. glyph clustering                                   #\n# ---------------------------------------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    if len(g) >= 2:\n        return [ord(g[0]) - 65, ord(g[1]) - 48]\n    return [ord(g[0]) - 65, 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ---------------------------------------------------- #\n# 4. sequence\u2192tensor dataset                            #\n# ---------------------------------------------------- #\nPAD_IDX = k_clusters  # padding index after clusters 0..k-1\n\n\ndef seq_to_clusters(seq: str) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq.strip().split()]\n\n\ndef build_known_pairs(seqs) -> set:\n    s = set()\n    for seq in seqs:\n        for tok in seq.strip().split():\n            s.add((tok, glyph_to_cluster.get(tok, 0)))\n    return s\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total = 0\n    novel = 0\n    for tok in seq.strip().split():\n        pair = (tok, glyph_to_cluster.get(tok, 0))\n        total += 1\n        if pair not in train_known_pairs:\n            novel += 1\n    novelty_ratio = novel / total if total else 0.0\n    return 1.0 + novelty_ratio\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.seqs = [seq_to_clusters(s) for s in sequences]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lengths, dtype=torch.long),\n        \"label\": labels,\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------------------------------- #\n# 5. model                                             #\n# ---------------------------------------------------- #\nclass GRUClassifier(nn.Module):\n    def __init__(self, n_clusters, pad_idx, emb_dim=64, hid=128, num_classes=10):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens):\n        x = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)\n        return self.out(h)\n\n\nmodel = GRUClassifier(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------------------------------- #\n# 6. evaluation helper                                 #\n# ---------------------------------------------------- #\ndef evaluate(model, loader, seq_raw) -> Dict[str, float]:\n    model.eval()\n    preds, gts = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            bseq = batch[\"seq\"].to(device)\n            blen = batch[\"len\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(bseq, blen)\n            loss = criterion(logits, lbl)\n            total_loss += loss.item() * lbl.size(0)\n            pred = logits.argmax(1)\n            preds.extend(pred.cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------------------------------- #\n# 7. training loop WITHOUT early stopping              #\n# ---------------------------------------------------- #\nmax_epochs = 25\n\nexperiment_data = {\n    \"no_early_stopping\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    # --- train ---\n    model.train()\n    total_loss = 0.0\n    n_seen = 0\n    for batch in train_loader:\n        bseq = batch[\"seq\"].to(device)\n        blen = batch[\"len\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(bseq, blen)\n        loss = criterion(logits, lbl)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * lbl.size(0)\n        n_seen += lbl.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    # --- validate ---\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n\n    print(\n        f\"Epoch {epoch}/{max_epochs} - train_loss:{train_loss:.4f} \"\n        f\"val_loss:{val_stats['loss']:.4f} HCSA:{val_stats['HCSA']:.3f} SNWA:{val_stats['SNWA']:.3f}\"\n    )\n\n# ---------------------------------------------------- #\n# 8. final evaluation with last epoch weights          #\n# ---------------------------------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\n\nexperiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"predictions\"][\"dev\"] = dev_final[\n    \"preds\"\n]\nexperiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"ground_truth\"][\"dev\"] = dev_final[\n    \"gts\"\n]\nexperiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"predictions\"][\"test\"] = test_final[\n    \"preds\"\n]\nexperiment_data[\"no_early_stopping\"][\"SPR_BENCH\"][\"ground_truth\"][\"test\"] = test_final[\n    \"gts\"\n]\n\nprint(\n    f'Final Dev  - CWA:{dev_final[\"CWA\"]:.3f} SWA:{dev_final[\"SWA\"]:.3f} '\n    f'HCSA:{dev_final[\"HCSA\"]:.3f} SNWA:{dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'Final Test - CWA:{test_final[\"CWA\"]:.3f} SWA:{test_final[\"SWA\"]:.3f} '\n    f'HCSA:{test_final[\"HCSA\"]:.3f} SNWA:{test_final[\"SNWA\"]:.3f}'\n)\n\n# ---------------------------------------------------- #\n# 9. save experiment data                              #\n# ---------------------------------------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os, pathlib, random, copy, math, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom datasets import load_dataset, DatasetDict\nfrom typing import List, Dict\n\n# ---------------------------- 0. house-keeping ---------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# ---------------------------- 1. locate SPR_BENCH ------------------------- #\ndef find_spr_bench_root() -> pathlib.Path:\n    env = os.getenv(\"SPR_BENCH_ROOT\")\n    cands = [pathlib.Path(env)] if env else []\n    cwd = pathlib.Path.cwd()\n    for parent in [cwd] + list(cwd.parents):\n        cands.append(parent / \"SPR_BENCH\")\n    cands.extend(\n        [\n            pathlib.Path.home() / \"SPR_BENCH\",\n            pathlib.Path.home() / \"AI-Scientist-v2\" / \"SPR_BENCH\",\n            pathlib.Path(\"/workspace/SPR_BENCH\"),\n            pathlib.Path(\"/data/SPR_BENCH\"),\n        ]\n    )\n    for p in cands:\n        if (\n            (p / \"train.csv\").exists()\n            and (p / \"dev.csv\").exists()\n            and (p / \"test.csv\").exists()\n        ):\n            print(f\"Found SPR_BENCH at {p}\")\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH dataset not found.\")\n\n\nDATA_PATH = find_spr_bench_root()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({split: _load(split) for split in [\"train\", \"dev\", \"test\"]})\n\n\nspr = load_spr_bench(DATA_PATH)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ---------------------------- 2. helper metrics --------------------------- #\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.strip().split()))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [cw if t == p else 0 for cw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [sw if t == p else 0 for sw, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\ndef harmonic_csa(cwa, swa):\n    return 2 * cwa * swa / (cwa + swa + 1e-8)\n\n\n# ---------------------------- 3. glyph clustering ------------------------- #\ndef glyph_vector(g: str) -> List[int]:\n    return [ord(g[0]) - 65, ord(g[1]) - 48] if len(g) >= 2 else [ord(g[0]) - 65, 0]\n\n\nall_glyphs = set(tok for seq in spr[\"train\"][\"sequence\"] for tok in seq.strip().split())\nvecs = np.array([glyph_vector(g) for g in all_glyphs])\nk_clusters = 16\nkmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10)\nglyph_to_cluster = {g: c for g, c in zip(all_glyphs, kmeans.fit_predict(vecs))}\nprint(f\"Clustered {len(all_glyphs)} glyphs into {k_clusters} clusters.\")\n\n# ---------------------------- 4. sequence\u2192tensor dataset ------------------ #\nPAD_IDX = k_clusters  # padding index\n\n\ndef seq_to_clusters(seq: str) -> List[int]:\n    return [glyph_to_cluster.get(tok, 0) for tok in seq.strip().split()]\n\n\ndef build_known_pairs(seqs) -> set:\n    s = set()\n    for seq in seqs:\n        for tok in seq.strip().split():\n            s.add((tok, glyph_to_cluster.get(tok, 0)))\n    return s\n\n\ntrain_known_pairs = build_known_pairs(spr[\"train\"][\"sequence\"])\n\n\ndef sequence_novelty_weight(seq: str) -> float:\n    total = novel = 0\n    for tok in seq.strip().split():\n        total += 1\n        if (tok, glyph_to_cluster.get(tok, 0)) not in train_known_pairs:\n            novel += 1\n    return 1.0 + (novel / total if total else 0.0)\n\n\ndef snwa(seqs, y_true, y_pred):\n    w = [sequence_novelty_weight(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) else 0.0\n\n\nclass SPRSeqDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.seqs = [seq_to_clusters(s) for s in sequences]\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\"seq\": self.seqs[idx], \"label\": self.labels[idx]}\n\n\ndef collate(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    seq_tensor = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n    for i, b in enumerate(batch):\n        seq_tensor[i, : len(b[\"seq\"])] = torch.tensor(b[\"seq\"], dtype=torch.long)\n    return {\n        \"seq\": seq_tensor,\n        \"len\": torch.tensor(lengths, dtype=torch.long),\n        \"label\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRSeqDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRSeqDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRSeqDataset(spr[\"test\"][\"sequence\"], spr[\"test\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ---------------------------- 5. model (NO PACKING) ----------------------- #\nclass GRUClassifierNoPack(nn.Module):\n    def __init__(self, n_clusters, pad_idx, emb_dim=64, hid=128, num_classes=10):\n        super().__init__()\n        self.emb = nn.Embedding(n_clusters + 1, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n        self.out = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens=None):\n        x = self.emb(x)  # [B,T,E]\n        _, h = self.gru(x)  # h: [2, B, H]\n        h = torch.cat([h[-2], h[-1]], dim=1)  # [B, 2H]\n        return self.out(h)\n\n\nmodel = GRUClassifierNoPack(k_clusters, PAD_IDX, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------------------------- 6. evaluation helper ------------------------ #\ndef evaluate(model, loader, seq_raw) -> Dict[str, float]:\n    model.eval()\n    preds, gts = [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            bseq = batch[\"seq\"].to(device)\n            lbl = batch[\"label\"].to(device)\n            logits = model(bseq, batch[\"len\"].to(device))\n            total_loss += criterion(logits, lbl).item() * lbl.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(lbl.cpu().tolist())\n    avg_loss = total_loss / len(gts)\n    cwa = color_weighted_accuracy(seq_raw, gts, preds)\n    swa = shape_weighted_accuracy(seq_raw, gts, preds)\n    hcs = harmonic_csa(cwa, swa)\n    snw = snwa(seq_raw, gts, preds)\n    return {\n        \"loss\": avg_loss,\n        \"CWA\": cwa,\n        \"SWA\": swa,\n        \"HCSA\": hcs,\n        \"SNWA\": snw,\n        \"preds\": preds,\n        \"gts\": gts,\n    }\n\n\n# ---------------------------- 7. training loop ---------------------------- #\nmax_epochs, patience = 25, 5\nbest_hcs, since_best, best_state = -1.0, 0, None\n\nexperiment_data = {\n    \"no_sequence_packing\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": {\"dev\": [], \"test\": []},\n            \"ground_truth\": {\"dev\": [], \"test\": []},\n        }\n    }\n}\n\nfor epoch in range(1, max_epochs + 1):\n    model.train()\n    total_loss, n_seen = 0.0, 0\n    for batch in train_loader:\n        bseq = batch[\"seq\"].to(device)\n        lbl = batch[\"label\"].to(device)\n        optimizer.zero_grad()\n        logits = model(bseq, batch[\"len\"].to(device))\n        loss = criterion(logits, lbl)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * lbl.size(0)\n        n_seen += lbl.size(0)\n    train_loss = total_loss / n_seen\n    experiment_data[\"no_sequence_packing\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        (epoch, train_loss)\n    )\n\n    val_stats = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\n    experiment_data[\"no_sequence_packing\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        (epoch, val_stats[\"loss\"])\n    )\n    experiment_data[\"no_sequence_packing\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        (\n            epoch,\n            val_stats[\"CWA\"],\n            val_stats[\"SWA\"],\n            val_stats[\"HCSA\"],\n            val_stats[\"SNWA\"],\n        )\n    )\n    print(\n        f\"Epoch {epoch}: val_loss={val_stats['loss']:.4f} HCSA={val_stats['HCSA']:.3f} SNWA={val_stats['SNWA']:.3f}\"\n    )\n\n    if val_stats[\"HCSA\"] > best_hcs + 1e-6:\n        best_hcs = val_stats[\"HCSA\"]\n        best_state = copy.deepcopy(model.state_dict())\n        since_best = 0\n    else:\n        since_best += 1\n    if since_best >= patience:\n        print(\"Early stopping.\")\n        break\n\nif best_state:\n    model.load_state_dict(best_state)\n\n# ---------------------------- 8. final evaluation ------------------------- #\ndev_final = evaluate(model, dev_loader, spr[\"dev\"][\"sequence\"])\ntest_final = evaluate(model, test_loader, spr[\"test\"][\"sequence\"])\nexp = experiment_data[\"no_sequence_packing\"][\"SPR_BENCH\"]\nexp[\"predictions\"][\"dev\"], exp[\"ground_truth\"][\"dev\"] = (\n    dev_final[\"preds\"],\n    dev_final[\"gts\"],\n)\nexp[\"predictions\"][\"test\"], exp[\"ground_truth\"][\"test\"] = (\n    test_final[\"preds\"],\n    test_final[\"gts\"],\n)\n\nprint(\n    f'Dev  - CWA:{dev_final[\"CWA\"]:.3f} SWA:{dev_final[\"SWA\"]:.3f} HCSA:{dev_final[\"HCSA\"]:.3f} SNWA:{dev_final[\"SNWA\"]:.3f}'\n)\nprint(\n    f'Test - CWA:{test_final[\"CWA\"]:.3f} SWA:{test_final[\"SWA\"]:.3f} HCSA:{test_final[\"HCSA\"]:.3f} SNWA:{test_final[\"SNWA\"]:.3f}'\n)\n\n# ---------------------------- 9. save experiment data --------------------- #\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n"], "term_out": ["['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 296541.94\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 576188.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 357056.24\nexamples/s]', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n', 'Epoch 1:\nvalidation_loss = 0.0997 HCSA=0.970 SNWA=0.970', '\\n', 'Epoch 2: validation_loss\n= 0.0377 HCSA=0.993 SNWA=0.993', '\\n', 'Epoch 3: validation_loss = 0.0164\nHCSA=0.995 SNWA=0.995', '\\n', 'Epoch 4: validation_loss = 0.0102 HCSA=0.999\nSNWA=0.999', '\\n', 'Epoch 5: validation_loss = 0.0029 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 6: validation_loss = 0.0022 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 7:\nvalidation_loss = 0.0020 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 8: validation_loss\n= 0.0017 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 9: validation_loss = 0.0016\nHCSA=0.999 SNWA=0.999', '\\n', 'Epoch 10: validation_loss = 0.0014 HCSA=1.000\nSNWA=1.000', '\\n', 'Epoch 11: validation_loss = 0.0015 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 12: validation_loss = 0.0013 HCSA=1.000 SNWA=1.000', '\\n', 'Early\nstopping.', '\\n', 'Final Dev - CWA:1.000 SWA:1.000 HCSA:1.000 SNWA:1.000', '\\n',\n'Final Test - CWA:0.635 SWA:0.699 HCSA:0.666 SNWA:0.700', '\\n', 'Saved\nexperiment data to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-31_02-26-\n58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n15/working/experiment_data.npy', '\\n', 'Execution time: 38 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 233298.14\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 162861.56\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 261888.66\nexamples/s]', '\\n', 'Clustered 16 glyphs into 16 clusters for metrics.', '\\n',\n'Vocabulary size (no clustering) = 16', '\\n', 'Epoch 1: val_loss=0.0705\nHCSA=0.984 SNWA=0.983', '\\n', 'Epoch 2: val_loss=0.0353 HCSA=0.993 SNWA=0.993',\n'\\n', 'Epoch 3: val_loss=0.0117 HCSA=0.996 SNWA=0.996', '\\n', 'Epoch 4:\nval_loss=0.0049 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 5: val_loss=0.0027\nHCSA=0.999 SNWA=0.999', '\\n', 'Epoch 6: val_loss=0.0021 HCSA=1.000 SNWA=1.000',\n'\\n', 'Epoch 7: val_loss=0.0020 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 8:\nval_loss=0.0019 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 9: val_loss=0.0018\nHCSA=1.000 SNWA=1.000', '\\n', 'Epoch 10: val_loss=0.0018 HCSA=1.000 SNWA=1.000',\n'\\n', 'Epoch 11: val_loss=0.0018 HCSA=1.000 SNWA=1.000', '\\n', 'Early\nstopping.', '\\n', 'Final Dev  - CWA:1.000 SWA:1.000 HCSA:1.000 SNWA:1.000',\n'\\n', 'Final Test - CWA:0.635 SWA:0.700 HCSA:0.666 SNWA:0.700', '\\n', 'Saved\nexperiment data to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-31_02-26-\n58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n20/working/experiment_data.npy', '\\n', 'Execution time: 17 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 647553.94\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 644424.91\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 480777.62\nexamples/s]', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n', 'Epoch 1:\nval_loss=0.1396 HCSA=0.958 SNWA=0.956', '\\n', 'Epoch 2: val_loss=0.0714\nHCSA=0.981 SNWA=0.981', '\\n', 'Epoch 3: val_loss=0.0295 HCSA=0.990 SNWA=0.990',\n'\\n', 'Epoch 4: val_loss=0.0181 HCSA=0.997 SNWA=0.998', '\\n', 'Epoch 5:\nval_loss=0.0102 HCSA=0.998 SNWA=0.998', '\\n', 'Epoch 6: val_loss=0.0042\nHCSA=0.999 SNWA=0.999', '\\n', 'Epoch 7: val_loss=0.0033 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 8: val_loss=0.0021 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 9:\nval_loss=0.0019 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 10: val_loss=0.0018\nHCSA=0.999 SNWA=0.999', '\\n', 'Epoch 11: val_loss=0.0016 HCSA=1.000 SNWA=1.000',\n'\\n', 'Epoch 12: val_loss=0.0018 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 13:\nval_loss=0.0017 HCSA=1.000 SNWA=1.000', '\\n', 'Early stopping.', '\\n', 'Final\nDev  - CWA:1.000 SWA:1.000 HCSA:1.000 SNWA:1.000', '\\n', 'Final Test - CWA:0.635\nSWA:0.700 HCSA:0.666 SNWA:0.700', '\\n', 'Saved experiment data to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-31_02-26-\n58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n22/working/experiment_data.npy', '\\n', 'Execution time: 13 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Found SPR_BENCH at', ' ',\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0\nexamples [00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples\n[00:00, 318996.08 examples/s]', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 5000 examples [00:00,\n119482.90 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 243991.06\nexamples/s]', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n', 'Epoch 1:\nval_loss=0.5369 HCSA=0.738 SNWA=0.745', '\\n', 'Epoch 2: val_loss=0.5224\nHCSA=0.736 SNWA=0.743', '\\n', 'Epoch 3: val_loss=0.5210 HCSA=0.738 SNWA=0.744',\n'\\n', 'Epoch 4: val_loss=0.5211 HCSA=0.743 SNWA=0.748', '\\n', 'Epoch 5:\nval_loss=0.5209 HCSA=0.747 SNWA=0.752', '\\n', 'Epoch 6: val_loss=0.5211\nHCSA=0.743 SNWA=0.748', '\\n', 'Epoch 7: val_loss=0.5213 HCSA=0.760 SNWA=0.762',\n'\\n', 'Epoch 8: val_loss=0.5207 HCSA=0.746 SNWA=0.751', '\\n', 'Epoch 9:\nval_loss=0.5213 HCSA=0.738 SNWA=0.744', '\\n', 'Epoch 10: val_loss=0.5211\nHCSA=0.736 SNWA=0.743', '\\n', 'Epoch 11: val_loss=0.5216 HCSA=0.755 SNWA=0.758',\n'\\n', 'Epoch 12: val_loss=0.5210 HCSA=0.736 SNWA=0.742', '\\n', 'Early\nstopping.', '\\n', 'Final Dev - CWA:0.756 SWA:0.764 HCSA:0.760 SNWA:0.762', '\\n',\n'Final Test - CWA:0.598 SWA:0.634 HCSA:0.616 SNWA:0.632', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n31_02-26-58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n21/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '\\rGenerating train split: 20000 examples [00:00, 158092.47\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 157618.09\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 349467.09\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 412894.28\nexamples/s]', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n', 'Epoch 1:\nval_loss=0.1038 HCSA=0.969 SNWA=0.968', '\\n', 'Epoch 2: val_loss=0.0385\nHCSA=0.992 SNWA=0.992', '\\n', 'Epoch 3: val_loss=0.0184 HCSA=0.994 SNWA=0.994',\n'\\n', 'Epoch 4: val_loss=0.0121 HCSA=0.998 SNWA=0.998', '\\n', 'Epoch 5:\nval_loss=0.0029 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 6: val_loss=0.0027\nHCSA=0.999 SNWA=0.999', '\\n', 'Epoch 7: val_loss=0.0024 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 8: val_loss=0.0020 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 9:\nval_loss=0.0019 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 10: val_loss=0.0017\nHCSA=1.000 SNWA=1.000', '\\n', 'Epoch 11: val_loss=0.0018 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 12: val_loss=0.0016 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 13:\nval_loss=0.0013 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 14: val_loss=0.0015\nHCSA=1.000 SNWA=1.000', '\\n', 'Epoch 15: val_loss=0.0013 HCSA=1.000 SNWA=1.000',\n'\\n', 'Early stopping.', '\\n', 'Final Dev - CWA:1.000 SWA:1.000 HCSA:1.000\nSNWA:1.000', '\\n', 'Final Test - CWA:0.635 SWA:0.699 HCSA:0.666 SNWA:0.700',\n'\\n', 'Saved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-23/working/experiment_data.npy', '\\n', 'Execution time:\n20 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '(Random) Assigned 16 glyphs to 16 clusters.',\n'\\n', 'Epoch 1: val_loss=0.0931 HCSA=0.975 SNWA=0.974', '\\n', 'Epoch 2:\nval_loss=0.0356 HCSA=0.995 SNWA=0.995', '\\n', 'Epoch 3: val_loss=0.0125\nHCSA=0.996 SNWA=0.997', '\\n', 'Epoch 4: val_loss=0.0051 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 5: val_loss=0.0027 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 6:\nval_loss=0.0023 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 7: val_loss=0.0025\nHCSA=1.000 SNWA=1.000', '\\n', 'Epoch 8: val_loss=0.0023 HCSA=1.000 SNWA=1.000',\n'\\n', 'Epoch 9: val_loss=0.0024 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 10:\nval_loss=0.0023 HCSA=1.000 SNWA=1.000', '\\n', 'Early stopping.', '\\n', 'Dev  :\nCWA=1.000 SWA=1.000 HCSA=1.000 SNWA=1.000', '\\n', 'Test : CWA=0.635 SWA=0.700\nHCSA=0.666 SNWA=0.700', '\\n', 'Saved experiment data to /home/zxl240011/AI-Scien\ntist-v2/experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-22/working/experiment_data.npy', '\\n', 'Execution time:\n18 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n',\n'Epoch 1: val_loss=0.2966 HCSA=0.892 SNWA=0.895', '\\n', 'Epoch 2:\nval_loss=0.2271 HCSA=0.923 SNWA=0.926', '\\n', 'Epoch 3: val_loss=0.1994\nHCSA=0.939 SNWA=0.942', '\\n', 'Epoch 4: val_loss=0.1882 HCSA=0.943 SNWA=0.946',\n'\\n', 'Epoch 5: val_loss=0.1886 HCSA=0.939 SNWA=0.943', '\\n', 'Epoch 6:\nval_loss=0.1782 HCSA=0.945 SNWA=0.948', '\\n', 'Epoch 7: val_loss=0.1797\nHCSA=0.946 SNWA=0.949', '\\n', 'Epoch 8: val_loss=0.1773 HCSA=0.945 SNWA=0.948',\n'\\n', 'Epoch 9: val_loss=0.1849 HCSA=0.940 SNWA=0.944', '\\n', 'Epoch 10:\nval_loss=0.1748 HCSA=0.944 SNWA=0.947', '\\n', 'Epoch 11: val_loss=0.1686\nHCSA=0.946 SNWA=0.949', '\\n', 'Epoch 12: val_loss=0.1691 HCSA=0.946 SNWA=0.949',\n'\\n', 'Epoch 13: val_loss=0.1730 HCSA=0.946 SNWA=0.949', '\\n', 'Epoch 14:\nval_loss=0.1751 HCSA=0.946 SNWA=0.949', '\\n', 'Epoch 15: val_loss=0.1685\nHCSA=0.946 SNWA=0.949', '\\n', 'Epoch 16: val_loss=0.1699 HCSA=0.946 SNWA=0.949',\n'\\n', 'Early stopping.', '\\n', 'DEV  \u2013 CWA:0.947 SWA:0.944 HCSA:0.946\nSNWA:0.949', '\\n', 'TEST \u2013 CWA:0.629 SWA:0.686 HCSA:0.656 SNWA:0.687', '\\n',\n'Saved experiment data to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n31_02-26-58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n21/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n',\n'Epoch 1/25 - train_loss:0.1953 val_loss:0.0997 HCSA:0.970 SNWA:0.970', '\\n',\n'Epoch 2/25 - train_loss:0.0539 val_loss:0.0377 HCSA:0.993 SNWA:0.993', '\\n',\n'Epoch 3/25 - train_loss:0.0206 val_loss:0.0164 HCSA:0.995 SNWA:0.995', '\\n',\n'Epoch 4/25 - train_loss:0.0069 val_loss:0.0102 HCSA:0.999 SNWA:0.999', '\\n',\n'Epoch 5/25 - train_loss:0.0021 val_loss:0.0029 HCSA:0.999 SNWA:0.999', '\\n',\n'Epoch 6/25 - train_loss:0.0008 val_loss:0.0022 HCSA:0.999 SNWA:0.999', '\\n',\n'Epoch 7/25 - train_loss:0.0005 val_loss:0.0020 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 8/25 - train_loss:0.0003 val_loss:0.0017 HCSA:0.999 SNWA:0.999', '\\n',\n'Epoch 9/25 - train_loss:0.0002 val_loss:0.0016 HCSA:0.999 SNWA:0.999', '\\n',\n'Epoch 10/25 - train_loss:0.0002 val_loss:0.0014 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 11/25 - train_loss:0.0001 val_loss:0.0015 HCSA:0.999 SNWA:0.999', '\\n',\n'Epoch 12/25 - train_loss:0.0001 val_loss:0.0013 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 13/25 - train_loss:0.0001 val_loss:0.0011 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 14/25 - train_loss:0.0001 val_loss:0.0012 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 15/25 - train_loss:0.0001 val_loss:0.0010 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 16/25 - train_loss:0.0001 val_loss:0.0012 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 17/25 - train_loss:0.0001 val_loss:0.0010 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 18/25 - train_loss:0.0000 val_loss:0.0009 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 19/25 - train_loss:0.0000 val_loss:0.0009 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 20/25 - train_loss:0.0000 val_loss:0.0010 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 21/25 - train_loss:0.0000 val_loss:0.0008 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 22/25 - train_loss:0.0000 val_loss:0.0009 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 23/25 - train_loss:0.0000 val_loss:0.0008 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 24/25 - train_loss:0.0000 val_loss:0.0007 HCSA:1.000 SNWA:1.000', '\\n',\n'Epoch 25/25 - train_loss:0.0000 val_loss:0.0007 HCSA:1.000 SNWA:1.000', '\\n',\n'Final Dev  - CWA:1.000 SWA:1.000 HCSA:1.000 SNWA:1.000', '\\n', 'Final Test -\nCWA:0.635 SWA:0.700 HCSA:0.666 SNWA:0.700', '\\n', 'Saved experiment data to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-31_02-26-\n58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n20/working/experiment_data.npy', '\\n', 'Execution time: 19 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Clustered 16 glyphs into 16 clusters.', '\\n',\n'Epoch 1: val_loss=0.0997 HCSA=0.970 SNWA=0.970', '\\n', 'Epoch 2:\nval_loss=0.0377 HCSA=0.993 SNWA=0.993', '\\n', 'Epoch 3: val_loss=0.0164\nHCSA=0.995 SNWA=0.995', '\\n', 'Epoch 4: val_loss=0.0102 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 5: val_loss=0.0029 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 6:\nval_loss=0.0022 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 7: val_loss=0.0020\nHCSA=1.000 SNWA=1.000', '\\n', 'Epoch 8: val_loss=0.0017 HCSA=0.999 SNWA=0.999',\n'\\n', 'Epoch 9: val_loss=0.0016 HCSA=0.999 SNWA=0.999', '\\n', 'Epoch 10:\nval_loss=0.0014 HCSA=1.000 SNWA=1.000', '\\n', 'Epoch 11: val_loss=0.0015\nHCSA=0.999 SNWA=0.999', '\\n', 'Epoch 12: val_loss=0.0013 HCSA=1.000 SNWA=1.000',\n'\\n', 'Early stopping.', '\\n', 'Dev  - CWA:1.000 SWA:1.000 HCSA:1.000\nSNWA:1.000', '\\n', 'Test - CWA:0.635 SWA:0.699 HCSA:0.666 SNWA:0.700', '\\n',\n'Saved experiment data to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n31_02-26-58_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n23/working/experiment_data.npy', '\\n', 'Execution time: 18 seconds seconds (time\nlimit is 30 minutes).']"], "analysis": ["", "", "", "The execution of the training script completed successfully without any errors\nor bugs. The model achieved a final validation HCSA of 0.760 and SNWA of 0.762,\nand a test HCSA of 0.616 and SNWA of 0.632. While the test metrics did not\nsurpass the SOTA benchmarks, the script functioned as intended, and the results\nare reasonable for this stage of experimentation. Further refinements to the\nclustering or model architecture could be explored to improve performance.", "", "The script executed successfully without any bugs. The training and evaluation\nprocesses ran smoothly, and the results were saved correctly. The Color-Weighted\nAccuracy (CWA) and Shape-Weighted Accuracy (SWA) on the test set were 63.5% and\n70.0%, respectively, which are below the SOTA benchmarks (70.0% for CWA and\n65.0% for SWA). However, this is expected as the experiment used a random glyph-\nto-cluster assignment as part of an ablation study. The script demonstrated\nproper early stopping and saved the experimental data as intended.", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final loss value during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.000121, "best_value": 0.000121}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.001303, "best_value": 0.001303}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation dataset metric for CWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999634, "best_value": 0.999634}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation dataset metric for SWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999593, "best_value": 0.999593}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "Validation dataset metric for HCSA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999614, "best_value": 0.999614}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "Validation dataset metric for SNWA", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated during training, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0002, "best_value": 0.0002}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset, indicating how well the model generalizes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0018, "best_value": 0.0018}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correct Word Accuracy metric calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9997, "best_value": 0.9997}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Sentence Word Accuracy metric calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "The Human Character Sequence Accuracy metric calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "The Sentence Normalized Word Accuracy metric calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0002, "best_value": 0.0002}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0016, "best_value": 0.0016}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation Correct Weighted Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation Simple Weighted Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "Validation Harmonic Correct Subset Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "Validation Simple Non-Weighted Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.51968, "best_value": 0.51968}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.520683, "best_value": 0.520683}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correct Weighted Accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.755964, "best_value": 0.755964}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Smoothed Weighted Accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.763923, "best_value": 0.763923}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "The Harmonic Class-Specific Accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.759922, "best_value": 0.759922}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "The Smoothed Normalized Weighted Accuracy during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7618, "best_value": 0.7618}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0001, "best_value": 0.0001}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0013, "best_value": 0.0013}]}, {"metric_name": "validation Color Weighted Accuracy", "lower_is_better": false, "description": "The weighted accuracy for color predictions during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation Shape Weighted Accuracy", "lower_is_better": false, "description": "The weighted accuracy for shape predictions during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation Harmonic Color-Shape Accuracy", "lower_is_better": false, "description": "The harmonic mean of color and shape accuracies during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation Sequence Novelty Weighted Accuracy", "lower_is_better": false, "description": "The weighted accuracy for sequence novelty predictions during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "development accuracy", "lower_is_better": false, "description": "The accuracy on the development set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6995, "best_value": 0.6995}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating model performance.", "data": [{"dataset_name": "Training", "final_value": 0.0002, "best_value": 0.0002}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, indicating model performance.", "data": [{"dataset_name": "Validation", "final_value": 0.0023, "best_value": 0.0023}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation metric CWA, indicating weighted average performance.", "data": [{"dataset_name": "Validation", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation metric SWA, another weighted average performance indicator.", "data": [{"dataset_name": "Validation", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "Validation metric HCSA, indicating a specific performance measure.", "data": [{"dataset_name": "Validation", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "Validation metric SNWA, indicating another specific performance measure.", "data": [{"dataset_name": "Validation", "final_value": 0.9998, "best_value": 0.9998}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, measuring how well the model fits the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1715, "best_value": 0.1715}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, measuring how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1699, "best_value": 0.1699}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Correct Weighted Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9477, "best_value": 0.9477}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Simple Weighted Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9447, "best_value": 0.9447}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "The Harmonic Correct Sequence Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9462, "best_value": 0.9462}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "The Sequence Normalized Weighted Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9492, "best_value": 0.9492}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value computed during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value computed during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0007, "best_value": 0.0007}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Validation metric for Correct Weighted Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9999, "best_value": 0.9999}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Validation metric for Simple Weighted Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation HCSA", "lower_is_better": false, "description": "Validation metric for Hierarchical Correct Semantic Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9999, "best_value": 0.9999}]}, {"metric_name": "validation SNWA", "lower_is_better": false, "description": "Validation metric for Semantic Normalized Weighted Accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9998, "best_value": 0.9998}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final training loss value for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.000121, "best_value": 0.000121}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final validation loss value for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.001302, "best_value": 0.001302}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The final validation color-weighted accuracy value for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999634, "best_value": 0.999634}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The final validation shape-weighted accuracy value for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999593, "best_value": 0.999593}]}, {"metric_name": "validation harmonic CSA", "lower_is_better": false, "description": "The final validation harmonic CSA value for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999614, "best_value": 0.999614}]}, {"metric_name": "validation sequence-novelty weighted accuracy", "lower_is_better": false, "description": "The final validation sequence-novelty weighted accuracy value for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}]}], "is_best_node": [false, false, false, false, false, false, false, true, false], "plots": [["../../logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_val_metric_curves.png", "../../logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_dev_vs_test_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_loss.png", "../../logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_CWA.png", "../../logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_SWA.png", "../../logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_HCSA.png", "../../logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_SNWA.png"], ["../../logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_conf_mat_Dev.png", "../../logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_conf_mat_Test.png"], ["../../logs/0-run/experiment_results/experiment_3b478b9922e749bb98a7276122bb9792_proc_1615734/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3b478b9922e749bb98a7276122bb9792_proc_1615734/SPR_BENCH_metrics_curves.png"], ["../../logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_metrics_curve.png", "../../logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_HCSA_curve.png", "../../logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_SNWA_curve.png", "../../logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_CWA_SWA_curve.png"], ["../../logs/0-run/experiment_results/experiment_c85f045ea7de4cc9be1cb5ae073f517f_proc_1615734/SPR_BENCH_Shuffled-Sequence-Order_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c85f045ea7de4cc9be1cb5ae073f517f_proc_1615734/SPR_BENCH_Shuffled-Sequence-Order_val_metrics.png"], ["../../logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_loss_curve_no_early_stopping.png", "../../logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_val_metrics_no_early_stopping.png", "../../logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_confusion_matrix_no_early_stopping.png"], ["../../logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_val_metrics.png", "../../logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_confusion_matrix_dev.png", "../../logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_confusion_matrix_test.png"]], "plot_paths": [["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_loss_curves.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_val_metric_curves.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_dev_vs_test_accuracy.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_loss.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_CWA.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_SWA.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_HCSA.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_SNWA.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_loss_curves.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_validation_metrics.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_conf_mat_Dev.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_conf_mat_Test.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_3b478b9922e749bb98a7276122bb9792_proc_1615734/SPR_BENCH_loss_curves.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_3b478b9922e749bb98a7276122bb9792_proc_1615734/SPR_BENCH_metrics_curves.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_metrics_curve.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_HCSA_curve.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_SNWA_curve.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_CWA_SWA_curve.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_c85f045ea7de4cc9be1cb5ae073f517f_proc_1615734/SPR_BENCH_Shuffled-Sequence-Order_loss_curves.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_c85f045ea7de4cc9be1cb5ae073f517f_proc_1615734/SPR_BENCH_Shuffled-Sequence-Order_val_metrics.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_loss_curve_no_early_stopping.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_val_metrics_no_early_stopping.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_confusion_matrix_no_early_stopping.png"], ["experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_loss_curves.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_val_metrics.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_confusion_matrix_dev.png", "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_confusion_matrix_test.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation loss over 12 epochs. Both curves decrease steadily, with the training loss slightly lower than the validation loss throughout the training process. By the 4th epoch, the losses converge to near-zero values, indicating rapid learning and minimal overfitting. The model training appears to be stable and effective.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot presents the four evaluation metrics (CWA, SWA, HCSA, and SNWA) over 12 epochs. All metrics show a sharp increase in performance during the initial epochs and stabilize at nearly 100% by the 4th epoch. This suggests that the model generalizes well across all metrics, achieving near-perfect accuracy in validation. The consistency across metrics highlights the robustness of the model.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_val_metric_curves.png"}, {"analysis": "This bar chart compares the accuracy on the dev and test datasets. While the dev accuracy is close to 100%, the test accuracy is significantly lower, around 70%. This indicates a potential overfitting issue or a distribution shift between the dev and test datasets. Further analysis or regularization techniques may be needed to address this discrepancy.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b3a7aca055a2450a97402dc98ed6bf18_proc_1610517/SPR_BENCH_dev_vs_test_accuracy.png"}], [{"analysis": "The loss curve shows a steady decrease in both training and validation loss over the epochs, with the validation loss closely following the training loss. This indicates that the model is learning effectively without overfitting, as the validation loss does not diverge significantly from the training loss. The loss stabilizes around epoch 5, suggesting that the model converges quickly.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_loss.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) curve demonstrates a rapid increase in validation accuracy during the initial epochs, followed by stabilization around epoch 5. The near-perfect accuracy achieved suggests that the model performs exceptionally well in identifying patterns based on color-weighted criteria. However, this could indicate potential overfitting to the dataset if the task is inherently challenging.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_CWA.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) curve follows a similar trend to the CWA curve, with rapid improvement in the initial epochs and stabilization around epoch 5. The high accuracy achieved suggests that the model is highly effective in recognizing shape-weighted patterns. The similarity in trends between CWA and SWA indicates balanced performance across both metrics.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_SWA.png"}, {"analysis": "The HCSA curve shows a pattern consistent with the CWA and SWA curves, with rapid improvement and stabilization around epoch 5. The high accuracy achieved suggests that the model generalizes well across hidden cluster-based symbolic attributes, even without explicit glyph clustering. This consistency reinforces the model's robustness in symbolic reasoning tasks.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_HCSA.png"}, {"analysis": "The SNWA curve exhibits the same trends as the other accuracy metrics, with rapid initial improvement and stabilization around epoch 5. The near-perfect accuracy achieved indicates that the model is adept at recognizing symbolic sequences with shape and color nuances, even without glyph clustering. This consistency across metrics highlights the model's comprehensive understanding of the task.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_70f75bb111704a5b8f1a47333c47a030_proc_1615733/no_glyph_clustering_SPR_BENCH_SNWA.png"}], [{"analysis": "The training and validation loss curves show a rapid decrease in loss during the initial epochs, stabilizing near zero after approximately 5 epochs. This indicates effective learning and convergence of the model without signs of overfitting, as the validation loss closely follows the training loss.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation metrics for Color-Weighted Accuracy (CWA), Shape-Weighted Accuracy (SWA), and other related metrics demonstrate consistent improvement across epochs, reaching near-perfect scores around epoch 6. This suggests that the model performs exceptionally well on the validation set, achieving high accuracy and robustness in symbolic glyph clustering and reasoning tasks.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_validation_metrics.png"}, {"analysis": "The confusion matrix for the development set shows near-perfect classification, with only 2 misclassifications out of 5000 samples. This indicates excellent performance on the development set and suggests that the model generalizes well to unseen data within the same distribution.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_conf_mat_Dev.png"}, {"analysis": "The confusion matrix for the test set reveals a higher misclassification rate compared to the development set, with approximately 1500 misclassified samples in each category. This highlights a potential generalization gap when applied to the test set, suggesting that the model may struggle with certain unseen patterns or distributions in the test data.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_aedc4406de7a47af85a9f739966c4222_proc_1615735/SPR_BENCH_conf_mat_Test.png"}], [{"analysis": "The plot shows the training and validation loss over 12 epochs. Initially, both the training and validation losses decrease sharply, indicating effective learning during the early epochs. After epoch 3, the loss values stabilize, with minimal fluctuations. The validation loss remains consistently close to the training loss, suggesting that the model does not suffer from significant overfitting. This behavior indicates that the model is well-regularized and has achieved a good balance between fitting the training data and generalizing to unseen validation data.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_3b478b9922e749bb98a7276122bb9792_proc_1615734/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot presents the validation metrics for HCSA and SNWA over 12 epochs. The SNWA metric shows higher variability compared to HCSA, with noticeable peaks at epochs 6 and 10. HCSA, while more stable, exhibits a slight decline towards the end. The higher and more variable SNWA scores suggest that the model's performance is more sensitive to shape-weighted accuracy. The peaks in SNWA may indicate epochs where the model effectively captures shape-related features, but the subsequent decline suggests potential overfitting or instability in shape-related generalization. The performance trends highlight areas for further tuning, particularly to stabilize and enhance SNWA.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_3b478b9922e749bb98a7276122bb9792_proc_1615734/SPR_BENCH_metrics_curves.png"}], [{"analysis": "The loss curves indicate that both training and validation losses decrease rapidly in the first few epochs, stabilizing near zero after approximately 5 epochs. This suggests that the model converges effectively without significant overfitting, as the validation loss mirrors the training loss closely throughout the training process.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_loss_curve.png"}, {"analysis": "The validation metrics plot shows that both Color-Weighted Accuracy (HCSA) and Shape-Weighted Accuracy (SNWA) approach a near-perfect score of 1.0 after a few epochs, with minimal fluctuations. This indicates strong performance and generalization on the validation set, suggesting that the proposed symbolic glyph clustering approach is highly effective in improving model accuracy.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_metrics_curve.png"}, {"analysis": "The confusion matrix for the test set reveals a balanced performance across all classes, as indicated by the evenly distributed diagonal elements. This suggests that the model performs well on unseen data and does not exhibit significant bias towards any particular class.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b5cd2068aa3f48bd814e7e9b410477e2_proc_1615736/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training and validation loss curves indicate a rapid convergence of the model. Both losses decrease significantly within the first few epochs and stabilize close to zero. This suggests that the model is effectively learning the underlying patterns without signs of overfitting, as the validation loss closely follows the training loss.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_loss_curve.png"}, {"analysis": "The Harmonic CSA (HCSA) metric shows a consistent improvement over epochs, reaching near-perfect values by epoch 4 and stabilizing thereafter. This indicates that the model achieves high accuracy in balancing the contributions of different evaluation components early in training and maintains this performance.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_HCSA_curve.png"}, {"analysis": "The Sequence Novelty-Weighted Accuracy (SNWA) metric follows a similar trend to HCSA, starting with a high value and improving rapidly within the first few epochs. The stabilization of SNWA at near-perfect values suggests that the model generalizes well to novel sequences.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_SNWA_curve.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) metrics show nearly identical trends, with both improving rapidly and stabilizing at near-perfect values by epoch 4. This indicates that the model is equally effective at handling both color and shape variations in the sequences, achieving balanced performance across these two dimensions.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0e15e424ff2c42afac58e99c1a3037da_proc_1615735/SPR_BENCH_CWA_SWA_curve.png"}], [{"analysis": "The loss curves for both training and validation datasets indicate a steady decrease in loss over the epochs, demonstrating effective learning. The convergence of the training and validation loss curves suggests minimal overfitting, which is a positive outcome. However, the slight fluctuations in the validation loss after epoch 8 might indicate some instability or sensitivity to the validation set. This could be explored further to ensure robustness of the model.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_c85f045ea7de4cc9be1cb5ae073f517f_proc_1615734/SPR_BENCH_Shuffled-Sequence-Order_loss_curves.png"}, {"analysis": "The validation metric curves for CWA, SWA, HCSA, and SNWA show a consistent improvement over the epochs, with all metrics stabilizing at high values after epoch 8. This indicates strong performance across the different evaluation metrics. The slight dip and recovery around epoch 8 might suggest a temporary overfitting or a challenging validation batch. Overall, the metrics surpass the stated SOTA benchmarks of 70% CWA and 65% SWA, showcasing the effectiveness of the proposed approach.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_c85f045ea7de4cc9be1cb5ae073f517f_proc_1615734/SPR_BENCH_Shuffled-Sequence-Order_val_metrics.png"}], [{"analysis": "The loss curve indicates a rapid convergence of the model, with both training and validation losses decreasing sharply in the initial epochs and stabilizing near zero after approximately 5 epochs. This suggests the model is learning effectively and generalizing well, as the validation curve closely follows the training curve without significant divergence, indicating minimal overfitting.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_loss_curve_no_early_stopping.png"}, {"analysis": "The validation metrics (CWA, SWA, HCSA, SNWA) demonstrate consistently high performance, achieving near-perfect scores (close to 1.0) after the initial few epochs. This indicates the model's robustness and its ability to accurately evaluate sequences based on the defined metrics. The stability of these metrics over epochs further reinforces the model's reliability.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_val_metrics_no_early_stopping.png"}, {"analysis": "The confusion matrix for the test split shows a strong diagonal dominance, indicating high accuracy in predictions. There are minimal misclassifications, as evidenced by the sparse off-diagonal values. This confirms the model's effectiveness in correctly classifying symbolic sequences in the test dataset.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b0381cb0f4f54d0ba0000faa2d39d203_proc_1615733/SPR_BENCH_confusion_matrix_no_early_stopping.png"}], [{"analysis": "The loss curves indicate that the model is well-trained. Both the training and validation losses decrease rapidly during the initial epochs and converge to near-zero values, suggesting that the model effectively learns the underlying patterns in the data. The absence of divergence between the training and validation losses implies that overfitting is not a concern in this experiment.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_loss_curves.png"}, {"analysis": "The validation metrics plot shows that the model achieves near-perfect scores across all metrics (CWA, SWA, HCSA, SNWA) after the first few epochs. This suggests that the proposed symbolic glyph clustering method is highly effective in improving model accuracy and generalization for SPR tasks. The steady and consistent performance across metrics demonstrates robustness and reliability.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_val_metrics.png"}, {"analysis": "The confusion matrix for the development set reveals a strong classification performance, with most predictions aligning with the ground truth. The high density in the diagonal elements indicates that the model accurately predicts the correct classes, with minimal misclassifications.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_confusion_matrix_dev.png"}, {"analysis": "The confusion matrix for the test set mirrors the performance observed in the development set. The strong diagonal dominance indicates that the model generalizes well to unseen data, effectively maintaining high classification accuracy.", "plot_path": "experiments/2025-08-31_02-26-58_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7d840f36ece440d38ab83a9cbfcb16cf_proc_1615736/spr_bench_confusion_matrix_test.png"}]], "vlm_feedback_summary": ["The plots indicate that the model trains effectively and generalizes well on the\nvalidation set, achieving near-perfect performance across metrics. However, the\ndrop in test accuracy compared to dev accuracy suggests possible overfitting or\ndataset distribution issues, warranting further investigation.", "The plots indicate that the model performs exceptionally well across all\nevaluation metrics, with rapid convergence and high accuracy. The results\nsuggest that even without glyph clustering, the model demonstrates strong\ngeneralization and pattern recognition capabilities. However, the consistently\nhigh accuracy could indicate potential overfitting or that the dataset may not\nbe sufficiently challenging.", "The model demonstrates strong performance on the validation and development\ndatasets, with near-perfect accuracy and minimal loss. However, a noticeable\ngeneralization gap is observed on the test dataset, indicating areas for\nimprovement in handling unseen data distributions.", "The plots reveal effective training with well-balanced loss reduction and\nhighlight variability in shape-weighted accuracy, suggesting opportunities to\nimprove model generalization on shape-related features.", "The plots collectively demonstrate strong model convergence, high validation\naccuracy, and balanced performance on the test set. This suggests that the\nproposed symbolic glyph clustering approach is effective in enhancing model\naccuracy and generalization for Synthetic PolyRule Reasoning.", "The provided plots demonstrate effective model training and evaluation. The\nrapid convergence of loss metrics and near-perfect stabilization of accuracy\nmetrics (HCSA, SNWA, CWA, SWA) suggest that the proposed symbolic glyph\nclustering approach is highly effective in enhancing model performance for SPR\ntasks.", "The plots demonstrate effective learning and strong performance across multiple\nmetrics, with results surpassing the SOTA benchmarks. The model shows minimal\noverfitting and robust generalization, though minor instabilities in validation\nloss and metrics around epoch 8 warrant further investigation.", "The experimental results are highly promising. The loss curve reveals effective\nlearning and generalization, the validation metrics confirm robust and stable\nperformance, and the confusion matrix highlights high classification accuracy\nwith minimal errors. These findings suggest that the proposed symbolic glyph\nclustering approach significantly enhances the model's reasoning capabilities in\nthe SPR_BENCH benchmark.", "The plots collectively indicate that the symbolic glyph clustering method is\nhighly effective. The loss curves show rapid convergence without overfitting,\nthe validation metrics demonstrate near-perfect scores across all evaluated\nmetrics, and the confusion matrices confirm strong classification performance\nand generalization to unseen data."], "exec_time": [38.09321093559265, 17.544676065444946, 13.223817586898804, 9.592260122299194, 20.467358112335205, 18.05087947845459, 15.079076290130615, 19.75246834754944, 18.119630813598633], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["All datasets in the provided experiment data are successfully tested."], ["[\"validation\"", "\"development\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['CWA'", "'SWA'", "'HCSA'", "'SNWA']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------ #\n# 1. Load experiment data                                            #\n# ------------------------------------------------------------------ #\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n\ndef unpack(list_of_tuples, idx):\n    return [t[idx] for t in list_of_tuples]\n\n\n# ------------------------------------------------------------------ #\n# 2. Iterate through datasets and plot                               #\n# ------------------------------------------------------------------ #\nfor dname, dct in exp.items():\n    # -------------------- a. loss curves --------------------------- #\n    try:\n        plt.figure()\n        tr_epochs = unpack(dct[\"losses\"][\"train\"], 0)\n        tr_loss = unpack(dct[\"losses\"][\"train\"], 1)\n        v_epochs = unpack(dct[\"losses\"][\"val\"], 0)\n        v_loss = unpack(dct[\"losses\"][\"val\"], 1)\n        plt.plot(tr_epochs, tr_loss, \"--\", label=\"Train\")\n        plt.plot(v_epochs, v_loss, \"-\", label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy loss\")\n        plt.title(f\"{dname}: Train vs. Val Loss\\n(Standard sequence classification)\")\n        plt.legend(fontsize=7)\n        fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n        plt.savefig(fname, dpi=150)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # -------------------- b. metric curves ------------------------- #\n    try:\n        metrics_val = dct[\"metrics\"][\"val\"]\n        if metrics_val:\n            epochs = unpack(metrics_val, 0)\n            cwa = unpack(metrics_val, 1)\n            swa = unpack(metrics_val, 2)\n            hcs = unpack(metrics_val, 3)\n            snwa = unpack(metrics_val, 4)\n\n            fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n            axs = axs.flatten()\n            for ax, data, ttl in zip(\n                axs, [cwa, swa, hcs, snwa], [\"CWA\", \"SWA\", \"HCSA\", \"SNWA\"]\n            ):\n                ax.plot(epochs, data, \"-o\", ms=3)\n                ax.set_xlabel(\"Epoch\")\n                ax.set_title(ttl)\n            fig.suptitle(\n                f\"{dname}: Validation Metrics\\n(Left-Top\u2192Right-Bottom: CWA, SWA, HCSA, SNWA)\"\n            )\n            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n            fname = os.path.join(working_dir, f\"{dname}_val_metric_curves.png\")\n            plt.savefig(fname, dpi=150)\n            plt.close()\n            print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating metric plot for {dname}: {e}\")\n        plt.close()\n\n    # -------------------- c. dev vs. test accuracy ----------------- #\n    try:\n        for split in [\"dev\", \"test\"]:\n            preds = np.array(dct[\"predictions\"].get(split, []))\n            gts = np.array(dct[\"ground_truth\"].get(split, []))\n            acc = (preds == gts).mean() if preds.size else np.nan\n            dct.setdefault(\"acc\", {})[split] = acc\n        acc_dev, acc_test = dct[\"acc\"][\"dev\"], dct[\"acc\"][\"test\"]\n\n        plt.figure()\n        plt.bar([\"Dev\", \"Test\"], [acc_dev, acc_test], color=[\"steelblue\", \"orange\"])\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{dname}: Dev vs. Test Accuracy\\n(Simple class agreement)\")\n        fname = os.path.join(working_dir, f\"{dname}_dev_vs_test_accuracy.png\")\n        plt.savefig(fname, dpi=150)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating accuracy bar chart for {dname}: {e}\")\n        plt.close()\n\n# ------------------------------------------------------------------ #\n# 3. Print summary accuracies                                        #\n# ------------------------------------------------------------------ #\nfor dname, dct in exp.items():\n    dev_acc = dct.get(\"acc\", {}).get(\"dev\", float(\"nan\"))\n    test_acc = dct.get(\"acc\", {}).get(\"test\", float(\"nan\"))\n    print(f\"{dname}: Dev Accuracy={dev_acc:.3f}, Test Accuracy={test_acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------- load data ------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper: safe close even on error\ndef save_plot(fig, fname):\n    try:\n        fig.savefig(os.path.join(working_dir, fname))\n    except Exception as e:\n        print(f\"Error saving {fname}: {e}\")\n    finally:\n        plt.close(fig)\n\n\n# ------------------- iterate and plot ------------------- #\nfor exp_name, datasets in experiment_data.items():\n    for dset_name, d in datasets.items():\n        # unpack\n        losses_tr = (\n            np.array(d[\"losses\"][\"train\"]) if d[\"losses\"][\"train\"] else np.empty((0, 2))\n        )\n        losses_val = np.array(d[\"losses\"][\"val\"])\n        metrics_val = np.array(d[\"metrics\"][\"val\"])  # (epoch,CWA,SWA,HCSA,SNWA)\n\n        # 1) loss curve\n        try:\n            fig = plt.figure()\n            if losses_tr.size:\n                plt.plot(losses_tr[:, 0], losses_tr[:, 1], label=\"train\")\n            plt.plot(losses_val[:, 0], losses_val[:, 1], label=\"val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dset_name} Loss Curves ({exp_name})\")\n            plt.legend()\n            save_plot(fig, f\"{exp_name}_{dset_name}_loss.png\")\n        except Exception as e:\n            print(f\"Error creating loss plot: {e}\")\n            plt.close()\n\n        # helper for metric plots\n        metric_names = [\"CWA\", \"SWA\", \"HCSA\", \"SNWA\"]\n        for idx, mname in enumerate(metric_names, start=1):\n            try:\n                fig = plt.figure()\n                plt.plot(metrics_val[:, 0], metrics_val[:, idx], label=f\"val {mname}\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(mname)\n                plt.title(f\"{dset_name} {mname} Curve ({exp_name})\")\n                plt.legend()\n                save_plot(fig, f\"{exp_name}_{dset_name}_{mname}.png\")\n            except Exception as e:\n                print(f\"Error creating {mname} plot: {e}\")\n                plt.close()\n\n        # ------------------- final accuracies ------------------- #\n        for split in [\"dev\", \"test\"]:\n            try:\n                preds = np.array(d[\"predictions\"][split])\n                gts = np.array(d[\"ground_truth\"][split])\n                acc = (preds == gts).mean() if len(gts) else float(\"nan\")\n                print(f\"{exp_name}-{dset_name} {split} accuracy: {acc:.3f}\")\n            except Exception as e:\n                print(f\"Error computing accuracy for {split}: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nmodel_key = \"No-Bidirectional-GRU\"\ndataset_key = \"SPR_BENCH\"\ned = experiment_data.get(model_key, {}).get(dataset_key, {})\n\n\n# Helper to downsample epochs to at most 5 points\ndef downsample(arr_list, max_pts=5):\n    if len(arr_list) <= max_pts:\n        return arr_list\n    step = max(1, len(arr_list) // max_pts)\n    return arr_list[::step]\n\n\n# 1. Loss curves\ntry:\n    train_loss = downsample(ed[\"losses\"][\"train\"])\n    val_loss = downsample(ed[\"losses\"][\"val\"])\n    tr_epochs, tr_vals = zip(*train_loss)\n    va_epochs, va_vals = zip(*val_loss)\n    plt.figure()\n    plt.plot(tr_epochs, tr_vals, label=\"Train Loss\")\n    plt.plot(va_epochs, va_vals, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. Validation metric curves\ntry:\n    metrics = downsample(ed[\"metrics\"][\"val\"])\n    epochs, cwa, swa, hcs, snwa = zip(*metrics)\n    plt.figure()\n    plt.plot(epochs, cwa, label=\"CWA\")\n    plt.plot(epochs, swa, label=\"SWA\")\n    plt.plot(epochs, hcs, label=\"HCSA\")\n    plt.plot(epochs, snwa, label=\"SNWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH Validation Metrics\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_validation_metrics.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating metric curves: {e}\")\n    plt.close()\n\n\n# Confusion-matrix helper\ndef plot_conf_mat(gts, preds, split_name):\n    try:\n        num_cls = int(max(max(gts), max(preds))) + 1\n        cm = np.zeros((num_cls, num_cls), dtype=int)\n        for g, p in zip(gts, preds):\n            cm[g, p] += 1\n        plt.figure(figsize=(5, 4))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.title(f\"SPR_BENCH Confusion Matrix - {split_name}\")\n        plt.ylabel(\"True\")\n        plt.xlabel(\"Predicted\")\n        # annotate counts\n        for i in range(num_cls):\n            for j in range(num_cls):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    fontsize=6,\n                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                )\n        fname = os.path.join(working_dir, f\"SPR_BENCH_conf_mat_{split_name}.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix ({split_name}): {e}\")\n        plt.close()\n\n\n# 3. Dev confusion matrix\nplot_conf_mat(\n    ed.get(\"ground_truth\", {}).get(\"dev\", []),\n    ed.get(\"predictions\", {}).get(\"dev\", []),\n    \"Dev\",\n)\n\n# 4. Test confusion matrix\nplot_conf_mat(\n    ed.get(\"ground_truth\", {}).get(\"test\", []),\n    ed.get(\"predictions\", {}).get(\"test\", []),\n    \"Test\",\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- Load data ---------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Navigate to the block we need\nmodel_key, dataset_key = \"MeanPoolEncoder\", \"SPR_BENCH\"\ndata = experiment_data.get(model_key, {}).get(dataset_key, {})\n\nloss_train = data.get(\"losses\", {}).get(\"train\", [])  # list of (epoch, loss)\nloss_val = data.get(\"losses\", {}).get(\"val\", [])  # list of (epoch, loss)\nmetrics_val = data.get(\"metrics\", {}).get(\n    \"val\", []\n)  # list of (epoch,CWA,SWA,HCSA,SNWA)\n\n\n# Helper to convert list-of-tuples -> np arrays (may be empty)\ndef to_xy(arr, idx=1):\n    if not arr:\n        return np.array([]), np.array([])\n    arr = np.array(arr)\n    return arr[:, 0], arr[:, idx]\n\n\n# ---------------- Plot 1: Loss curves ---------------- #\ntry:\n    ep_tr, loss_tr = to_xy(loss_train)\n    ep_val, loss_v = to_xy(loss_val)\n    if ep_tr.size and ep_val.size:\n        plt.figure()\n        plt.plot(ep_tr, loss_tr, label=\"Train\")\n        plt.plot(ep_val, loss_v, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    else:\n        print(\"Loss data missing, skipping loss plot.\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------------- Plot 2: Metric curves ---------------- #\ntry:\n    ep, hcs = to_xy(metrics_val, 3)  # HCSA index 3\n    _, snw = to_xy(metrics_val, 4)  # SNWA  index 4\n    if ep.size:\n        plt.figure()\n        plt.plot(ep, hcs, label=\"HCSA\")\n        plt.plot(ep, snw, label=\"SNWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH: Validation Metrics (HCSA & SNWA)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_metrics_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    else:\n        print(\"Metric data missing, skipping metric plot.\")\nexcept Exception as e:\n    print(f\"Error creating metric plot: {e}\")\n    plt.close()\n\n# ---------------- Print final metrics ---------------- #\nif metrics_val:\n    last_epoch, _, _, last_hcs, last_snw = metrics_val[-1]\n    print(\n        f\"Final validation epoch {last_epoch} -> HCSA={last_hcs:.3f}, SNWA={last_snw:.3f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------- #\n# Load experiment data\n# ---------------------------------------------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to fetch nested dict safely\ndef get_nested(d, keys, default=None):\n    for k in keys:\n        if isinstance(d, dict) and k in d:\n            d = d[k]\n        else:\n            return default\n    return d\n\n\nexp_path = [\"Frozen-Cluster-Embeddings\", \"SPR_BENCH\"]\nloss_train = get_nested(experiment_data, exp_path + [\"losses\", \"train\"], [])\nloss_val = get_nested(experiment_data, exp_path + [\"losses\", \"val\"], [])\nmetrics_val = get_nested(experiment_data, exp_path + [\"metrics\", \"val\"], [])\npreds_test = get_nested(experiment_data, exp_path + [\"predictions\", \"test\"], [])\ngts_test = get_nested(experiment_data, exp_path + [\"ground_truth\", \"test\"], [])\n\n# ---------------------------------------------------- #\n# 1. Loss curves\n# ---------------------------------------------------- #\ntry:\n    if loss_train and loss_val:\n        ep_t, l_t = zip(*loss_train)\n        ep_v, l_v = zip(*loss_val)\n        plt.figure()\n        plt.plot(ep_t, l_t, label=\"Train\")\n        plt.plot(ep_v, l_v, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n    else:\n        print(\"Loss data missing, skipping loss curve.\")\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\nfinally:\n    plt.close()\n\n# ---------------------------------------------------- #\n# 2. Validation metric curves (HCSA & SNWA)\n# ---------------------------------------------------- #\ntry:\n    if metrics_val:\n        ep, cwa, swa, hcs, snwa = zip(*metrics_val)\n        plt.figure()\n        plt.plot(ep, hcs, label=\"HCSA\")\n        plt.plot(ep, snwa, label=\"SNWA\")\n        plt.title(\"SPR_BENCH Validation Metrics\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_metrics_curve.png\"))\n    else:\n        print(\"Metric data missing, skipping metric curve.\")\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\nfinally:\n    plt.close()\n\n# ---------------------------------------------------- #\n# 3. Confusion matrix on test set\n# ---------------------------------------------------- #\ntry:\n    if preds_test and gts_test:\n        preds = np.array(preds_test)\n        gts = np.array(gts_test)\n        n_cls = max(preds.max(), gts.max()) + 1\n        cm, _, _ = np.histogram2d(gts, preds, bins=[np.arange(n_cls + 1)] * 2)\n        plt.figure()\n        im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"viridis\")\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    else:\n        print(\"Prediction data missing, skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\nfinally:\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nexp = experiment_data.get(\"RandomClusterAssignment\", {}).get(\"SPR_BENCH\", {})\nloss_train = exp.get(\"losses\", {}).get(\"train\", [])\nloss_val = exp.get(\"losses\", {}).get(\"val\", [])\nmetrics_val = exp.get(\"metrics\", {}).get(\"val\", [])\n\n\n# Helper to split epoch/value pairs\ndef split_xy(pairs, idx=1):\n    if not pairs:\n        return [], []\n    x, y = zip(*[(p[0], p[idx]) for p in pairs])\n    return list(x), list(y)\n\n\n# ---------- Plot 1: Loss curves ----------\ntry:\n    e1, y1 = split_xy(loss_train)\n    e2, y2 = split_xy(loss_val)\n    plt.figure()\n    if e1:\n        plt.plot(e1, y1, label=\"Train\")\n    if e2:\n        plt.plot(e2, y2, label=\"Validation\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n\n# ---------- helper to fetch metric column ----------\ndef metric_curve(col):\n    return split_xy(metrics_val, idx=col)\n\n\n# columns: 1=CWA, 2=SWA, 3=HCSA, 4=SNWA\n# ---------- Plot 2: HCSA ----------\ntry:\n    e, hcs = metric_curve(3)\n    plt.figure()\n    if e:\n        plt.plot(e, hcs, marker=\"o\")\n    plt.title(\"SPR_BENCH: Harmonic CSA (HCSA) over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HCSA\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_HCSA_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HCSA curve: {e}\")\n    plt.close()\n\n# ---------- Plot 3: SNWA ----------\ntry:\n    e, sn = metric_curve(4)\n    plt.figure()\n    if e:\n        plt.plot(e, sn, marker=\"o\", color=\"green\")\n    plt.title(\"SPR_BENCH: Sequence Novelty-Weighted Acc. (SNWA) over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"SNWA\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_SNWA_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating SNWA curve: {e}\")\n    plt.close()\n\n# ---------- Plot 4: CWA & SWA ----------\ntry:\n    e, cwa = metric_curve(1)\n    _, swa = metric_curve(2)\n    plt.figure()\n    if e:\n        plt.plot(e, cwa, label=\"CWA\")\n    if e:\n        plt.plot(e, swa, label=\"SWA\")\n    plt.title(\"SPR_BENCH: Color & Shape Weighted Acc. over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_CWA_SWA_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA/SWA curve: {e}\")\n    plt.close()\n\n# ---------- print last-epoch key metrics ----------\nif metrics_val:\n    last = metrics_val[-1]\n    print(\n        f\"Final Val Metrics -> Epoch {last[0]}: HCSA={last[3]:.3f}, SNWA={last[4]:.3f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper for safe extraction\ndef split_xy(pairs):\n    xs, ys = zip(*pairs) if pairs else ([], [])\n    return list(xs), list(ys)\n\n\n# iterate over experiments and datasets\nfor exp_name, datasets in experiment_data.items():\n    for dname, content in datasets.items():\n        # ------------------ LOSS CURVES ------------------ #\n        try:\n            plt.figure()\n            # training losses\n            x_train, y_train = split_xy(content.get(\"losses\", {}).get(\"train\", []))\n            if x_train:\n                plt.plot(x_train, y_train, label=\"train\")\n            # validation losses\n            x_val, y_val = split_xy(content.get(\"losses\", {}).get(\"val\", []))\n            if x_val:\n                plt.plot(x_val, y_val, label=\"val\")\n            plt.xlabel(\"epoch\")\n            plt.ylabel(\"loss\")\n            plt.title(f\"{dname} Loss Curves ({exp_name})\")\n            plt.legend()\n            fname = f\"{dname}_{exp_name}_loss_curves.png\".replace(\" \", \"_\")\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dname}: {e}\")\n            plt.close()\n\n        # ------------------ METRIC CURVES ---------------- #\n        try:\n            plt.figure()\n            metrics = content.get(\"metrics\", {}).get(\"val\", [])\n            if metrics:\n                epochs, cwa, swa, hcs, snwa = zip(*metrics)\n                plt.plot(epochs, cwa, label=\"CWA\")\n                plt.plot(epochs, swa, label=\"SWA\")\n                plt.plot(epochs, hcs, label=\"HCSA\")\n                plt.plot(epochs, snwa, label=\"SNWA\")\n            plt.xlabel(\"epoch\")\n            plt.ylabel(\"score\")\n            plt.title(f\"{dname} Validation Metrics ({exp_name})\")\n            plt.legend()\n            fname = f\"{dname}_{exp_name}_val_metrics.png\".replace(\" \", \"_\")\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating metric plot for {dname}: {e}\")\n            plt.close()\n\n        # --------------- PRINT FINAL METRICS -------------- #\n        if content.get(\"metrics\", {}).get(\"val\"):\n            last = content[\"metrics\"][\"val\"][-1]\n            print(\n                f\"{dname} final metrics (epoch {last[0]}): \"\n                f\"CWA={last[1]:.3f}, SWA={last[2]:.3f}, \"\n                f\"HCSA={last[3]:.3f}, SNWA={last[4]:.3f}\"\n            )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------ #\n# setup\n# ------------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# convenient pointer\ndname = (\"no_early_stopping\", \"SPR_BENCH\")\ndata = experiment_data.get(dname[0], {}).get(dname[1], {}) if experiment_data else {}\n\n# ------------------------------------------------------------------ #\n# 1. train / val loss curve\n# ------------------------------------------------------------------ #\ntry:\n    train = np.array(data[\"losses\"][\"train\"])\n    val = np.array(data[\"losses\"][\"val\"])\n    epochs_t, loss_t = train[:, 0], train[:, 1]\n    epochs_v, loss_v = val[:, 0], val[:, 1]\n\n    plt.figure()\n    plt.plot(epochs_t, loss_t, label=\"Train\")\n    plt.plot(epochs_v, loss_v, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curve\\nTrain vs. Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve_no_early_stopping.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------ #\n# 2. validation metrics over epochs\n# ------------------------------------------------------------------ #\ntry:\n    metrics = np.array(data[\"metrics\"][\"val\"])\n    epochs = metrics[:, 0]\n    cwa, swa, hcs, snwa = metrics[:, 1], metrics[:, 2], metrics[:, 3], metrics[:, 4]\n\n    plt.figure()\n    plt.plot(epochs, cwa, label=\"CWA\")\n    plt.plot(epochs, swa, label=\"SWA\")\n    plt.plot(epochs, hcs, label=\"HCSA\")\n    plt.plot(epochs, snwa, label=\"SNWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH Validation Metrics Over Epochs\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_metrics_no_early_stopping.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating metrics plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------ #\n# 3. confusion matrix on test split\n# ------------------------------------------------------------------ #\ntry:\n    preds = np.array(data[\"predictions\"][\"test\"])\n    gts = np.array(data[\"ground_truth\"][\"test\"])\n    n_cls = int(max(preds.max(), gts.max())) + 1\n    cm = np.zeros((n_cls, n_cls), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"SPR_BENCH Confusion Matrix \u2013 Test Split\")\n    plt.tight_layout()\n    fname = os.path.join(\n        working_dir, \"SPR_BENCH_confusion_matrix_no_early_stopping.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------- load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    algo = next(iter(experiment_data))  # 'no_sequence_packing'\n    dset = next(iter(experiment_data[algo]))  # 'SPR_BENCH'\n    record = experiment_data[algo][dset]\n\n    # ---------- Helper to unpack (epoch, v) tuples -------------------------\n    def tup2arr(tups):\n        ep, val = zip(*tups) if tups else ([], [])\n        return np.array(ep), np.array(val)\n\n    # ------------------------- FIGURE 1: Loss curves -----------------------\n    try:\n        tr_ep, tr_loss = tup2arr(record[\"losses\"][\"train\"])\n        va_ep, va_loss = tup2arr(record[\"losses\"][\"val\"])\n\n        plt.figure()\n        plt.plot(tr_ep, tr_loss, label=\"Train\")\n        plt.plot(va_ep, va_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dset} Loss Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dset.lower()}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # -------------- FIGURE 2: Validation metric trajectories ---------------\n    try:\n        if record[\"metrics\"][\"val\"]:\n            vals = np.array(record[\"metrics\"][\"val\"])\n            ep = vals[:, 0]\n            labels = [\"CWA\", \"SWA\", \"HCSA\", \"SNWA\"]\n            for i, lab in enumerate(labels, start=1):\n                plt.plot(ep, vals[:, i], label=lab)\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{dset} Validation Metrics\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset.lower()}_val_metrics.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot: {e}\")\n        plt.close()\n\n    # -------------- Helper to create confusion matrix plots ----------------\n    def plot_cm(split):\n        preds = np.array(record[\"predictions\"][split])\n        gts = np.array(record[\"ground_truth\"][split])\n        if preds.size == 0 or gts.size == 0:\n            return\n        n_cls = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for p, g in zip(preds, gts):\n            cm[g, p] += 1\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"{dset} Confusion Matrix ({split})\")\n        fname = os.path.join(\n            working_dir, f\"{dset.lower()}_confusion_matrix_{split}.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n\n    # --------------------- FIGURE 3 & 4: Confusion matrices ---------------\n    for split in [\"dev\", \"test\"]:\n        try:\n            plot_cm(split)\n        except Exception as e:\n            print(f\"Error creating confusion matrix ({split}): {e}\")\n            plt.close()\n"], "plot_plan": ["We load the saved numpy dictionary, iterate through each dataset entry (only \u201cSPR_BENCH\u201d here), and for every dataset we: 1) draw train vs validation cross-entropy loss, 2) draw the four validation metrics (CWA, SWA, HCSA, SNWA) in one 2\u00d72 subplot figure, and 3) draw a bar chart that compares simple accuracy on dev and test splits computed from the stored prediction/ground-truth lists.  All figures are placed in independent try-except blocks, saved under descriptive names inside working_dir, and closed immediately afterwards.  A helper unpack() function extracts columns from the tuple lists.  After plotting, the script prints the computed dev/test accuracies for each dataset.  The code uses only matplotlib, numpy and os; no made-up data are introduced, and a maximum of three figures are produced per dataset, satisfying the \u22645-figure rule.  Everything is done with concise, readable code wrapped in a single self-contained file.", "We load experiment_data.npy from the working directory, detect every model\u2013dataset pair, and extract arrays for epochs, training losses, validation losses and the four validation metrics (CWA, SWA, HCSA, SNWA). For each statistic we build a dedicated line-plot in a separate try/except block, drawing training and/or validation curves where data exists; each figure is titled clearly and saved with a descriptive file name that contains the dataset and plot type. We guard every plot with plt.close() inside finally/except to prevent memory leakage. When metrics are only available for validation (as in the provided example) we plot just that series. After visualisations are completed we compute simple accuracies for the stored dev and test predictions (number of correct labels divided by total) and print them so the user can verify final model quality. We deliberately limit ourselves to at most five figures: loss, CWA, SWA, HCSA and SNWA. All work is done with plain matplotlib; no external styles or fake data are introduced. The code is self-contained and concise, fulfilling the plotting and metric-printing requirements.", "We will load the saved experiment_data.npy, extract the section corresponding to the \u201cNo-Bidirectional-GRU\u201d run on SPR_BENCH, and unpack losses, validation metrics, and final predictions. Four figures will be produced: (1) training vs validation loss curves; (2) validation metric curves (CWA, SWA, HCSA, SNWA); (3) confusion matrix on the dev split; and (4) confusion matrix on the test split. Each figure is wrapped in its own try-except to guard against missing data and is saved to working_dir with descriptive filenames. We limit epoch-based plots to at most 5 points by subsampling if necessary, and every figure is closed after saving. Titles clearly state dataset and plot type, with subtitles clarifying axes where helpful. Only values present in experiment_data.npy are used\u2014no synthetic data are generated. Finally, the script prints a short confirmation of saved paths.", "We first load the saved experiment_data dictionary, then pull out the lists of (epoch, value) tuples for training/validation loss and for validation metrics (CWA, SWA, HCSA, SNWA).  Using simple NumPy slicing we separate epochs from the numeric values.  Two figures are generated: (1) a line plot comparing training vs. validation loss over epochs, and (2) a line plot of the two most informative scores (HCSA and SNWA) over epochs.  Each figure creation is wrapped in its own try-except block, saved under a descriptive name that contains the dataset and plot type, and the figure is always closed.  We respect the five-figure limit by producing only these two essential plots.  Basic matplotlib defaults are kept; no extra styling is added.  After plotting, the script prints the last-epoch HCSA and SNWA so users can read the final evaluation directly from the console.  All artifacts are stored in the predefined working_dir folder.", "We load experiment_data.npy, extract the stored losses, per-epoch validation metrics and final predictions/ground-truth arrays. Three plots will be generated: (1) training vs. validation loss curves, (2) validation HCSA & SNWA curves, and (3) a confusion-matrix heat-map on the test set; each plot is wrapped in its own try-except, saved to working_dir with descriptive names, then closed. We rely solely on numpy and matplotlib, plot at most these three figures, use clear titles/subtitles, and never fabricate data. The code adheres to the provided plotting guideline template, beginning with the required imports, ensuring working_dir exists, and gracefully skipping any missing data. Finally, each figure includes axis labels and legends where appropriate.", "We will load experiment_data.npy, pull out the SPR_BENCH entry, and unpack the lists of (epoch, value) pairs already stored during training. Four separate plots will be produced: (1) training vs. validation loss, (2) HCSA over epochs, (3) SNWA over epochs, and (4) CWA & SWA together for comparison. Each plot sits in its own try-except block, uses clear titles that mention \u201cSPR_BENCH\u201d and the content, and is saved to working_dir with descriptive filenames. We keep the number of figures \u2264 5 and close every figure whether an error occurs or not. After plotting we print the last-epoch values for HCSA and SNWA as a quick sanity check of the experiment\u2019s final performance. No new data are fabricated\u2014everything comes from the npy file. The code is concise, begins with the required imports, creates working_dir if it doesn\u2019t exist, and adheres strictly to basic matplotlib usage.", "We load the persisted experiment_data.npy, walk through each experiment/dataset pair, and extract per-epoch tuples for training/validation loss as well as validation metrics (CWA, SWA, HCSA, SNWA). Using basic matplotlib we create at most two figures per dataset: one line plot comparing training and validation losses, and one line plot showing the four validation metrics over epochs. Each figure is wrapped in a try-except block, saved with a descriptive filename that includes both dataset and plot type, and closed afterward. Paths are resolved relative to the given working_dir, which is created if necessary. We also guard against missing keys or empty lists so the script never crashes even if some data are absent. After plotting, the script prints the final (last-epoch) validation metrics so users can inspect them directly in the console. All operations rely solely on the data present inside experiment_data.npy without generating any synthetic information.", "We load experiment_data.npy, focus on the \u201cno_early_stopping / SPR_BENCH\u201d entry, and extract (epoch, value) tuples for training loss, validation loss, and the four validation metrics (CWA, SWA, HCSA, SNWA).  \nPlot 1 shows training vs. validation loss across epochs; Plot 2 overlays the four validation metrics across epochs so we can see convergence trends; Plot 3 visualizes the final test-set confusion matrix derived from stored ground-truth and prediction lists.  \nEach figure is generated inside its own try-except block, is titled descriptively (including the dataset name), saved into working_dir with an informative filename, and closed afterward.  \nWe guard against missing data and limit ourselves to what is present in experiment_data.npy\u2014no extra simulation.  \nAt the end the script prints the location of each saved file so users can inspect them.  \nThis gives three concise, standard diagnostic visualizations while respecting the plotting guidelines and the available data.", "We first load experiment_data.npy, grab the single algorithm (\u2018no_sequence_packing\u2019) and dataset (\u2018SPR_BENCH\u2019) entries, and unpack the lists of (epoch, value) tuples into NumPy arrays.  \nFigure 1 shows training vs validation loss across epochs; Figure 2 stacks four validation metrics (CWA, SWA, HCSA, SNWA) on one axis to compare their trajectories.  \nUsing the final prediction arrays already stored, we build integer-indexed confusion matrices for the dev and test splits and visualise them as heat-maps in Figures 3 and 4.  \nEach figure is wrapped in an independent try-except block so one failure will not stop the others, and every figure is saved inside working_dir with descriptive names that include the dataset and plot type.  \nAll figures are closed after saving to free memory.  \nIf experiment_data.npy cannot be found or its structure is unexpected, the script prints an error and skips plotting gracefully.  \nOnly matplotlib and NumPy are used, satisfying the requirement for basic plotting, and at most four figures are produced, complying with the 5-figure limit."], "ablation_name": [null, "No-Glyph-Clustering", "No-Bidirectional-GRU", "Mean-Pooling-Encoder", "Frozen-Cluster-Embeddings", "Random-Cluster-Assignments", "Shuffled-Sequence-Order", "No-Early-Stopping", "No-Sequence-Packing"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["The script will load the saved NumPy file from the \u201cworking\u201d directory, turn it\nback into a Python dictionary, and inspect the contents for every dataset it\ncontains (e.g., \u201cSPR_BENCH\u201d).  For each dataset it will\u2028(a) pick the final\ntraining-loss value,\u2028(b) find the lowest validation-loss value, and\u2028(c) select\nthe validation-metrics row that achieves the highest HCSA score; the individual\nCWA, SWA, HCSA, and SNWA values from that row are then reported.  All\ninformation is printed clearly with explicit metric names so that the output is\nself-explanatory, and no figures are generated.  Everything is executed at the\ntop level, so running the file immediately produces the requested summaries.", "Below is a concise loader that reads the saved NumPy file, walks through its\nnested structure, and prints the final recorded value for every available\nmetric, clearly labelling both the dataset and the metric names.", "The script will load the experiment_data.npy file from the working directory,\ntraverse every model and dataset it contains, extract the stored loss and metric\nhistories, pick the best (minimum for losses, maximum for other metrics) value\nfor each, and print them in a clear, descriptive format. All operations run\nimmediately at import time\u2014no special entry point is required.", "The code will immediately load the saved experiment_data.npy file from the\nworking directory, traverse its nested dictionary to reach the SPR_BENCH\nresults, and then compute the best (minimum or maximum, as appropriate) value\nfor every stored metric. Specifically, it will pick the lowest training loss,\nlowest validation loss, and the highest values of the validation CWA, SWA, HCSA,\nand SNWA, since larger values denote better performance for those metrics. After\ncalculating these, it prints the dataset name followed by each metric name and\nits best value, keeping the output concise and clearly labelled. No plots or\nspecial entry-point guards are used, so the script executes as soon as it is\nrun.", "The script first locates the working directory, loads the saved numpy\ndictionary, and then drills down into the nested structure (model \u2192 dataset \u2192\nmetrics / losses).   For every dataset it: \u2022 extracts the list of (epoch, value)\ntuples for training-loss and validation-loss, reporting the final training loss\nand the best (minimum) validation loss.   \u2022 scans the validation metrics list to\nfind the best (maximum) value for each of CWA, SWA, HCSA and SNWA.   \u2022 computes\nsimple accuracies for the stored development and test predictions to give\ndevelopment accuracy and test accuracy.   Finally, it prints the dataset name\nfollowed by clearly-labelled metric names and their corresponding values.", "Below is a compact script that immediately loads the saved NumPy dictionary,\nextracts the stored losses and validation metrics for the SPR-BENCH experiment,\nidentifies the final training loss and the best (HCSA-maximising) validation\nscores, and prints them with explicit, descriptive labels.", "The script will load the saved NumPy dictionary from the working directory, walk\nthrough each experiment and its contained datasets, pull out the last\u2010recorded\n(i.e., final) values for training loss, validation loss, and the four validation\nmetrics (CWA, SWA, HCSA, SNWA), and then print them in a clear, labeled form. It\nassumes the nested structure produced by the training code: experiment \u279c dataset\n\u279c losses/metrics. The code runs immediately on import\u2014no special entry point is\nused.", "The script loads the saved numpy file from the \u201cworking\u201d directory, pulls out\nthe nested dictionaries for the \u201cSPR_BENCH\u201d experiment, retrieves the last-epoch\n(i.e., final) entries from the training/validation loss lists and the validation\nmetric list, and prints them with fully-qualified metric names so there is no\nambiguity. Only textual output is produced\u2014no figures or extra files\u2014and\neverything is executed immediately at import time.", "We will load the saved NumPy dictionary from working/experiment_data.npy,\nnavigate through its nested structure (technique \u2192 dataset \u2192 metrics/losses),\nand retrieve the last recorded value in each list, which corresponds to the\nfinal epoch. For every dataset found (here, SPR_BENCH), we will print the\ndataset name first, followed by explicit metric names such as \u201cFinal training\nloss,\u201d \u201cFinal validation loss,\u201d and the four validation metrics (CWA, SWA, HCSA,\nSNWA). The script executes immediately\u2014nothing is placed under a special entry\npoint."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------- #\n# 0. locate experiment_data.npy                         #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Cannot find experiment_data.npy at {exp_path}\")\n\n# ---------------------------------------------------- #\n# 1. load data                                         #\n# ---------------------------------------------------- #\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------- #\n# 2. helper to print best/final metrics                #\n# ---------------------------------------------------- #\ndef print_dataset_metrics(name: str, data: dict):\n    # --- losses ---\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    final_train_loss = train_losses[-1][1] if train_losses else None\n    best_val_loss = min(val_losses, key=lambda x: x[1])[1] if val_losses else None\n\n    # --- validation metrics (tuple: epoch, CWA, SWA, HCSA, SNWA) ---\n    val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        # choose the row with the highest HCSA\n        best_row = max(val_metrics, key=lambda x: x[3])\n        _, best_cwa, best_swa, best_hcsa, best_snwa = best_row\n    else:\n        best_cwa = best_swa = best_hcsa = best_snwa = None\n\n    # --- printing ---\n    print(f\"\\nDataset: {name}\")\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n    if val_metrics:\n        print(f\"Best validation CWA:  {best_cwa:.6f}\")\n        print(f\"Best validation SWA:  {best_swa:.6f}\")\n        print(f\"Best validation HCSA: {best_hcsa:.6f}\")\n        print(f\"Best validation SNWA: {best_snwa:.6f}\")\n\n\n# ---------------------------------------------------- #\n# 3. iterate through datasets                          #\n# ---------------------------------------------------- #\nfor dataset_name, dataset_val in experiment_data.items():\n    print_dataset_metrics(dataset_name, dataset_val)\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------- #\n# 0. locate and load stored experiment data            #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------- #\n# 1. helper for pretty printing                        #\n# ---------------------------------------------------- #\ndef _fmt(v, ndigits=4):\n    \"\"\"Format floats uniformly; leave other types untouched.\"\"\"\n    return f\"{v:.{ndigits}f}\" if isinstance(v, float) else v\n\n\n# ---------------------------------------------------- #\n# 2. iterate and report                                #\n# ---------------------------------------------------- #\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"\\nDataset: {dataset_name}  |  Experiment tag: {experiment_name}\")\n\n        # ---- losses ----\n        for split, losses in data.get(\"losses\", {}).items():\n            if losses:  # list of tuples: (epoch, value)\n                epoch, value = losses[-1]\n                split_name = \"training\" if split == \"train\" else \"validation\"\n                print(f\"final {split_name} loss: {_fmt(value)} (epoch {epoch})\")\n\n        # ---- metrics ----\n        for split, metrics in data.get(\"metrics\", {}).items():\n            if metrics:  # list of tuples: (epoch, CWA, SWA, HCSA, SNWA)\n                epoch, cwa, swa, hcs, snwa = metrics[-1]\n                split_name = \"training\" if split == \"train\" else \"validation\"\n                print(f\"final {split_name} CWA:  {_fmt(cwa)} (epoch {epoch})\")\n                print(f\"final {split_name} SWA:  {_fmt(swa)} (epoch {epoch})\")\n                print(f\"final {split_name} HCSA: {_fmt(hcs)} (epoch {epoch})\")\n                print(f\"final {split_name} SNWA: {_fmt(snwa)} (epoch {epoch})\")\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------\n# 0. Locate experiment data\n# ----------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------------\n# 1. Helper function to print nicely\n# ----------------------------------------------------\ndef print_metric(name: str, value: float, precision: int = 4):\n    \"\"\"Print a metric name followed by its value with given precision.\"\"\"\n    print(f\"  {name}: {value:.{precision}f}\")\n\n\n# ----------------------------------------------------\n# 2. Extract and display the best metrics\n# ----------------------------------------------------\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # --- losses ---\n        train_losses = [loss for _, loss in content[\"losses\"].get(\"train\", [])]\n        val_losses = [loss for _, loss in content[\"losses\"].get(\"val\", [])]\n\n        if train_losses:\n            best_train_loss = min(train_losses)\n            print_metric(\"Best training loss\", best_train_loss)\n        if val_losses:\n            best_val_loss = min(val_losses)\n            print_metric(\"Best validation loss\", best_val_loss)\n\n        # --- validation metrics ---\n        # Each entry is (epoch, CWA, SWA, HCSA, SNWA)\n        val_metrics = content[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            _, cwa_list, swa_list, hcs_list, snwa_list = zip(*val_metrics)\n            print_metric(\"Best validation CWA\", max(cwa_list))\n            print_metric(\"Best validation SWA\", max(swa_list))\n            print_metric(\"Best validation HCSA\", max(hcs_list))\n            print_metric(\"Best validation SNWA\", max(snwa_list))\n", "import os\nimport numpy as np\n\n# Locate file and load the saved dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the metrics we need\nmodel_key = \"MeanPoolEncoder\"\ndataset_key = \"SPR_BENCH\"\n\ndata = experiment_data[model_key][dataset_key]\n\ntrain_losses = data[\"losses\"][\"train\"]  # list of (epoch, loss)\nval_losses = data[\"losses\"][\"val\"]  # list of (epoch, loss)\nval_metrics = data[\"metrics\"][\"val\"]  # list of (epoch, CWA, SWA, HCSA, SNWA)\n\n\n# Helper to extract best values ------------------------------------------------\ndef best_loss(loss_list):\n    # Returns the minimum loss value\n    return min(loss_list, key=lambda x: x[1])[1] if loss_list else None\n\n\ndef best_metric(metric_list, col):\n    # Returns the maximum value in the given column (1=CWA,2=SWA,3=HCSA,4=SNWA)\n    return max(metric_list, key=lambda x: x[col])[col] if metric_list else None\n\n\n# Compute best statistics ------------------------------------------------------\nbest_train_loss = best_loss(train_losses)\nbest_val_loss = best_loss(val_losses)\n\nbest_val_cwa = best_metric(val_metrics, 1)\nbest_val_swa = best_metric(val_metrics, 2)\nbest_val_hcs = best_metric(val_metrics, 3)\nbest_val_snwa = best_metric(val_metrics, 4)\n\n# Print results ---------------------------------------------------------------\nprint(\"SPR_BENCH dataset\")\nif best_train_loss is not None:\n    print(f\"lowest training loss: {best_train_loss:.6f}\")\nif best_val_loss is not None:\n    print(f\"lowest validation loss: {best_val_loss:.6f}\")\nif best_val_cwa is not None:\n    print(f\"best validation CWA: {best_val_cwa:.6f}\")\nif best_val_swa is not None:\n    print(f\"best validation SWA: {best_val_swa:.6f}\")\nif best_val_hcs is not None:\n    print(f\"best validation HCSA: {best_val_hcs:.6f}\")\nif best_val_snwa is not None:\n    print(f\"best validation SNWA: {best_val_snwa:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------- #\n# 0. locate experiment file                             #\n# ---------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nif not os.path.exists(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------- #\n# 1. helper for accuracy                               #\n# ---------------------------------------------------- #\ndef simple_accuracy(preds, gts):\n    if len(preds) == 0:\n        return float(\"nan\")\n    return sum(int(p == t) for p, t in zip(preds, gts)) / len(preds)\n\n\n# ---------------------------------------------------- #\n# 2. traverse and print metrics                        #\n# ---------------------------------------------------- #\nfor model_name, model_dict in experiment_data.items():\n    for dataset_name, data in model_dict.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---------- losses ----------\n        train_losses = data.get(\"losses\", {}).get(\"train\", [])\n        val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n        if train_losses:\n            final_train_loss = train_losses[-1][1]\n            print(f\"training loss: {final_train_loss:.4f}\")\n\n        if val_losses:\n            best_val_loss = min(val_losses, key=lambda x: x[1])[1]\n            print(f\"validation loss (best): {best_val_loss:.4f}\")\n\n        # ---------- validation metrics ----------\n        val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            # unpack metrics per index\n            _, cwa_vals, swa_vals, hcs_vals, snwa_vals = zip(*val_metrics)\n            print(f\"validation Color Weighted Accuracy (best): {max(cwa_vals):.4f}\")\n            print(f\"validation Shape Weighted Accuracy (best): {max(swa_vals):.4f}\")\n            print(\n                f\"validation Harmonic Color-Shape Accuracy (best): {max(hcs_vals):.4f}\"\n            )\n            print(\n                f\"validation Sequence Novelty Weighted Accuracy (best): {max(snwa_vals):.4f}\"\n            )\n\n        # ---------- dev / test accuracies ----------\n        dev_preds = data.get(\"predictions\", {}).get(\"dev\", [])\n        dev_gts = data.get(\"ground_truth\", {}).get(\"dev\", [])\n        test_preds = data.get(\"predictions\", {}).get(\"test\", [])\n        test_gts = data.get(\"ground_truth\", {}).get(\"test\", [])\n\n        if dev_preds and dev_gts:\n            dev_acc = simple_accuracy(dev_preds, dev_gts)\n            print(f\"development accuracy: {dev_acc:.4f}\")\n\n        if test_preds and test_gts:\n            test_acc = simple_accuracy(test_preds, test_gts)\n            print(f\"test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# 0. load the experiment data\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------\n# 1. navigate the nested structure\n# ------------------------------------------------------------\nexp = experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"]\n\ntrain_losses = exp[\"losses\"][\"train\"]  # [(epoch, loss), ...]\nval_losses = exp[\"losses\"][\"val\"]  # [(epoch, loss), ...]\nval_metrics = exp[\"metrics\"][\"val\"]  # [(epoch, CWA, SWA, HCSA, SNWA), ...]\n\n# ------------------------------------------------------------\n# 2. compute the required summary numbers\n# ------------------------------------------------------------\n# Training \u00ad\u2013 final loss (last epoch)\nif train_losses:\n    final_train_loss = train_losses[-1][1]\nelse:\n    final_train_loss = None\n\n# Validation \u00ad\u2013 best values according to highest HCSA\nif val_metrics:\n    best_val_tuple = max(val_metrics, key=lambda t: t[3])  # t[3] == HCSA\n    _, best_cwa, best_swa, best_hcs, best_snwa = best_val_tuple\nelse:\n    best_cwa = best_swa = best_hcs = best_snwa = None\n\n# Validation \u00ad\u2013 lowest loss\nif val_losses:\n    best_val_loss = min(val_losses, key=lambda t: t[1])[1]\nelse:\n    best_val_loss = None\n\n# ------------------------------------------------------------\n# 3. print results\n# ------------------------------------------------------------\nprint(\"Dataset: Training\")\nif final_train_loss is not None:\n    print(f\"Training loss: {final_train_loss:.4f}\")\nelse:\n    print(\"Training loss: N/A\")\n\nprint(\"\\nDataset: Validation\")\nif best_val_loss is not None:\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\nelse:\n    print(\"Best validation loss: N/A\")\n\nif best_hcs is not None:\n    print(f\"Best validation CWA:  {best_cwa:.4f}\")\n    print(f\"Best validation SWA:  {best_swa:.4f}\")\n    print(f\"Best validation HCSA: {best_hcs:.4f}\")\n    print(f\"Best validation SNWA: {best_snwa:.4f}\")\nelse:\n    print(\"Validation metrics: N/A\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# 0. locate experiment_data.npy in the working directory                        #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# 1. helper to fetch the final element of a list of (epoch, value, \u2026) tuples    #\n# -----------------------------------------------------------------------------#\ndef last_record(records):\n    \"\"\"\n    Returns the last tuple stored in a list of epoch-tracked records.\n    If the list is empty it returns None.\n    \"\"\"\n    return records[-1] if records else None\n\n\n# -----------------------------------------------------------------------------#\n# 2. iterate over experiments and datasets, printing clear metric names         #\n# -----------------------------------------------------------------------------#\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(f\"\\nDataset: {dataset_name}  (Experiment: {experiment_name})\")\n\n        # ---------------- losses ---------------- #\n        train_loss_entry = last_record(content[\"losses\"].get(\"train\", []))\n        val_loss_entry = last_record(content[\"losses\"].get(\"val\", []))\n\n        if train_loss_entry is not None:\n            _, train_loss = train_loss_entry\n            print(f\"training loss: {train_loss:.4f}\")\n\n        if val_loss_entry is not None:\n            _, val_loss = val_loss_entry\n            print(f\"validation loss: {val_loss:.4f}\")\n\n        # ---------------- validation metrics ---------------- #\n        val_metrics_entry = last_record(content[\"metrics\"].get(\"val\", []))\n        # tuple format: (epoch, CWA, SWA, HCSA, SNWA)\n        if val_metrics_entry is not None and len(val_metrics_entry) == 5:\n            _, cwa, swa, hcs, snwa = val_metrics_entry\n            print(f\"validation CWA: {cwa:.4f}\")\n            print(f\"validation SWA: {swa:.4f}\")\n            print(f\"validation HCSA: {hcs:.4f}\")\n            print(f\"validation SNWA: {snwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. locate file and load\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# 1. helper for \"last\" tuple element of a list\n# ------------------------------------------------------------------\ndef last_value(lst, idx):\n    \"\"\"\n    Return the value at position `idx` from the last tuple in `lst`.\n    If the list is empty, return None.\n    \"\"\"\n    return lst[-1][idx] if lst else None\n\n\n# ------------------------------------------------------------------\n# 2. iterate through datasets and print metrics\n# ------------------------------------------------------------------\nfor exp_name, datasets in experiment_data.items():  # e.g. 'no_early_stopping'\n    for dataset_name, data in datasets.items():  # e.g. 'SPR_BENCH'\n        print(f\"Dataset: {dataset_name}\")\n\n        # ---- losses ----\n        train_losses = data[\"losses\"][\"train\"]  # list of (epoch, loss)\n        val_losses = data[\"losses\"][\"val\"]  # list of (epoch, loss)\n        final_train_loss = last_value(train_losses, 1)\n        final_val_loss = last_value(val_losses, 1)\n\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"final validation loss: {final_val_loss:.4f}\")\n\n        # ---- validation metrics ----\n        # each entry: (epoch, CWA, SWA, HCSA, SNWA)\n        val_metrics = data[\"metrics\"][\"val\"]\n        final_cwa = last_value(val_metrics, 1)\n        final_swa = last_value(val_metrics, 2)\n        final_hcsa = last_value(val_metrics, 3)\n        final_snwa = last_value(val_metrics, 4)\n\n        if final_cwa is not None:\n            print(f\"final validation CWA: {final_cwa:.4f}\")\n            print(f\"final validation SWA: {final_swa:.4f}\")\n            print(f\"final validation HCSA: {final_hcsa:.4f}\")\n            print(f\"final validation SNWA: {final_snwa:.4f}\")\n\n        print()  # blank line between datasets (if more appear)\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# Iterate through methods and datasets\nfor method_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # Retrieve final (last recorded) losses\n        train_losses = content.get(\"losses\", {}).get(\"train\", [])\n        val_losses = content.get(\"losses\", {}).get(\"val\", [])\n        final_train_loss = train_losses[-1][1] if train_losses else None\n        final_val_loss = val_losses[-1][1] if val_losses else None\n\n        if final_train_loss is not None:\n            print(f\"Final training loss: {final_train_loss:.6f}\")\n        if final_val_loss is not None:\n            print(f\"Final validation loss: {final_val_loss:.6f}\")\n\n        # Retrieve final (last recorded) validation metrics\n        val_metrics = content.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            _, cwa, swa, hcs, snwa = val_metrics[-1]\n            print(f\"Final validation color-weighted accuracy: {cwa:.6f}\")\n            print(f\"Final validation shape-weighted accuracy: {swa:.6f}\")\n            print(f\"Final validation harmonic CSA: {hcs:.6f}\")\n            print(f\"Final validation sequence-novelty weighted accuracy: {snwa:.6f}\")\n"], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.000121', '\\n', 'Best\nvalidation loss: 0.001303', '\\n', 'Best validation CWA:  0.999634', '\\n', 'Best\nvalidation SWA:  0.999593', '\\n', 'Best validation HCSA: 0.999614', '\\n', 'Best\nvalidation SNWA: 0.999600', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['\\nDataset: SPR_BENCH  |  Experiment tag: no_glyph_clustering', '\\n', 'final\ntraining loss: 0.0002 (epoch 11)', '\\n', 'final validation loss: 0.0018 (epoch\n11)', '\\n', 'final validation CWA:  0.9997 (epoch 11)', '\\n', 'final validation\nSWA:  0.9996 (epoch 11)', '\\n', 'final validation HCSA: 0.9996 (epoch 11)',\n'\\n', 'final validation SNWA: 0.9996 (epoch 11)', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Best training loss: 0.0002', '\\n', '  Best\nvalidation loss: 0.0016', '\\n', '  Best validation CWA: 0.9996', '\\n', '  Best\nvalidation SWA: 0.9996', '\\n', '  Best validation HCSA: 0.9996', '\\n', '  Best\nvalidation SNWA: 0.9996', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['SPR_BENCH dataset', '\\n', 'lowest training loss: 0.519680', '\\n', 'lowest\nvalidation loss: 0.520683', '\\n', 'best validation CWA: 0.755964', '\\n', 'best\nvalidation SWA: 0.763923', '\\n', 'best validation HCSA: 0.759922', '\\n', 'best\nvalidation SNWA: 0.761800', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'training loss: 0.0001', '\\n', 'validation loss\n(best): 0.0013', '\\n', 'validation Color Weighted Accuracy (best): 0.9996',\n'\\n', 'validation Shape Weighted Accuracy (best): 0.9996', '\\n', 'validation\nHarmonic Color-Shape Accuracy (best): 0.9996', '\\n', 'validation Sequence\nNovelty Weighted Accuracy (best): 0.9996', '\\n', 'development accuracy: 0.9996',\n'\\n', 'test accuracy: 0.6995', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: Training', '\\n', 'Training loss: 0.0002', '\\n', '\\nDataset:\nValidation', '\\n', 'Best validation loss: 0.0023', '\\n', 'Best validation CWA:\n0.9998', '\\n', 'Best validation SWA:  0.9998', '\\n', 'Best validation HCSA:\n0.9998', '\\n', 'Best validation SNWA: 0.9998', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  (Experiment: Shuffled-Sequence-Order)', '\\n', 'training\nloss: 0.1715', '\\n', 'validation loss: 0.1699', '\\n', 'validation CWA: 0.9477',\n'\\n', 'validation SWA: 0.9447', '\\n', 'validation HCSA: 0.9462', '\\n',\n'validation SNWA: 0.9492', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'final training loss: 0.0000', '\\n', 'final\nvalidation loss: 0.0007', '\\n', 'final validation CWA: 0.9999', '\\n', 'final\nvalidation SWA: 0.9998', '\\n', 'final validation HCSA: 0.9999', '\\n', 'final\nvalidation SNWA: 0.9998', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.000121', '\\n', 'Final\nvalidation loss: 0.001302', '\\n', 'Final validation color-weighted accuracy:\n0.999634', '\\n', 'Final validation shape-weighted accuracy: 0.999593', '\\n',\n'Final validation harmonic CSA: 0.999614', '\\n', 'Final validation sequence-\nnovelty weighted accuracy: 0.999600', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"], "current_stage": "Stage_4"};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
