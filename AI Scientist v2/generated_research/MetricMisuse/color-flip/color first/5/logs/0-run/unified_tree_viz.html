<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 3], [0, 4], [0, 1], [0, 2], [0, 6], [0, 8], [0, 5], [0, 7]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0], [1.0, 1.0]], "plan": ["We first embed every unique glyph-token from the training split with BERT-base\n(CLS vector, only a few dozen forward passes, so it is fast).   K-Means is then\napplied to these embeddings, yielding cluster assignments for tokens and\nsilhouette coefficients for every cluster.   Each SPR sequence is converted into\na \u201ccluster string\u201d (e.g.\u2006\u201cc2 c2 c7 \u2026\u201d) and a CountVectorizer is trained over\nthese cluster strings (1-2-gram).   The cluster-ngrams + original token n-grams\nare concatenated and fed to a small MLP.   During training we track loss,\nAccuracy, CWA, SWA and the proposed Cluster-Consistency-Weighted Accuracy\n(CCWA), whose silhouette weights are pre-computed from the token clusters.   All\nmetrics, losses and predictions are stored in experiment_data and saved to\n./working/experiment_data.npy.   The script is self-contained, GPU-aware and\nfinishes in a few minutes on a single GPU/CPU.", "Ablation name: Remove-Cluster-Feature Ablation. Below I prepare a variant of the\nbaseline that completely removes the cluster feature branch (X2), the K-means\nstep, silhouette weighting and majority\u2013cluster logic.  The model is now trained\nonly on raw token n-gram counts (X1); CCWA is recomputed as a plain un-weighted\naccuracy surrogate so that the metric is still logged even though no cluster\ninformation is available.  All plottable tensors are stored in the\n`experiment_data.npy` file under the ablation key `RemoveClusterFeat`.", "Ablation name: Random-Cluster-Assignment Ablation. The ablation simply replaces\nthe k-means derived cluster IDs with uniformly random IDs (while keeping the\nsame number of clusters and still providing the cluster-count feature channel).\nEverything else\u2014feature vectorisation, model, training loop, metric computations\n(ACC, CWA, SWA, CCWA), and saving\u2014remains unchanged, allowing direct comparison\nwith the baseline to see how destroying cluster semantics affects CCWA. The code\nbelow is a single, self-contained file that carries out this Random-Cluster-\nAssignment ablation and stores all plottable data in the required\n`experiment_data.npy`.", "Ablation name: Remove-Token-Feature Ablation. The ablation keeps every part of\nthe original pipeline unchanged except that the raw CountVectorizer token\nfeatures (X1) are removed: build_features now returns only the cluster-based\nfeatures (X2).  We keep the token vectorizer instantiation so the script runs\neven if other pieces still reference it, but its output is ignored.  All\nlogging/saving follows the required experiment_data format under the key\n'remove_token_feature', and the script trains/evaluates exactly as before using\nthe reduced input dimensionality.  The rest of the code (data loading,\nclustering, model, metrics, training loop, saving) is copied from the baseline\nso the file is completely self-contained and executable.", "Ablation name: ORD-Embedding-Cluster Ablation. This ablation forces the pipeline\nto rely solely on lightweight 2-D ordinal vectors when clustering tokens,\neliminating any contribution from BERT. All subsequent steps (k-means, cluster-\nstring generation, vectorisation, model, training and evaluation) remain\nunchanged so that any performance drop can be attributed to the missing semantic\nsignal. The script below is fully self-contained, reproduces the baseline logic\nwith the embedding alteration, logs metrics/losses across epochs, and finally\nsaves everything into the required experiment_data.npy file.", "Ablation name: No-Bigram-Feature Ablation. The ablation simply re-runs the whole\nbaseline pipeline but forces both CountVectorizers to unigrams only\n(ngram_range=(1,1)). Everything else\u2014data loading (or synthetic fallback),\ntoken-level BERT/ord embeddings, k-means clustering, feature concatenation, MLP\ntraining, metric computation and saving\u2014remains unchanged. Results are stored\nunder the ablation key \"no_bigram\" inside experiment_data and written to\nworking/experiment_data.npy. Below is the complete runnable script.", "Ablation name: Token-Order-Shuffle Ablation. We first load SPR-BENCH (or a tiny\nsynthetic fallback).   Before any feature extraction we randomly permute each\nsequence\u2019s token order once and keep these shuffled versions for all subsequent\nsteps (vectorizer fitting, feature building, model training/evaluation).   Token\nembeddings, k-means clustering and all evaluation metrics remain identical to\nthe baseline, isolating the influence of true sequential order.   Results,\nlosses and predictions are stored under the ablation key \u201cTokenOrderShuffle\u201d in\nexperiment_data.npy.", "Ablation name: Two-Cluster-Granularity Ablation. Below we set k = 2 for the\nclustering step and log everything under the ablation tag\n\u201cTwoClusterGranularity\u201d. All downstream code (cluster strings, vectorizers,\nCCWA, etc.) remains unchanged, allowing direct comparison with the baseline. The\nscript is completely self-contained and saves results in experiment_data.npy.", "Ablation name: Binary-Count-Feature Ablation. Below is a compact re-\nimplementation of the baseline that performs the \u201cBinary-Count-Feature\nAblation\u201d: it keeps the same vocabularies and n-gram ranges but binarises every\nnon-zero token/cluster feature. All other components (clustering, MLP,\nevaluation pipeline, saving) are unchanged so the impact of removing term-\nfrequency information can be gauged directly."], "code": ["import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List, Dict\n\n# ---------- working dir & device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ---------- data loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, building synthetic tiny split for demo\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# ---------- helpers ----------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------- BERT embedding for glyph tokens ----------\n# try Transformers, fallback to simple ord vectors if offline\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n        mdl.eval()\n        with torch.no_grad():\n            embs = []\n            for t in tokens:\n                inp = tok(t, return_tensors=\"pt\").to(device)\n                out = mdl(**inp).last_hidden_state[:, 0]  # CLS\n                embs.append(out.squeeze(0).cpu().numpy())\n        return np.vstack(embs)\n    except Exception as e:\n        print(\"Transformer load failed, using simple ord-embedding:\", e)\n        vec = []\n        for t in tokens:\n            a = ord(t[0])\n            b = ord(t[1]) if len(t) > 1 else 0\n            vec.append([a / 1000, b / 1000])  # small scale\n        return np.array(vec, dtype=np.float32)\n\n\n# ---------- build token clusters ----------\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\n\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Clustering {len(uniq_tokens)} tokens into {n_clusters} clusters\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = (\n    silhouette_samples(token_embs, kmeans.labels_)\n    if n_clusters > 1\n    else np.ones(len(uniq_tokens))\n)\ncluster_silh = {i: np.mean(silh_vals[kmeans.labels_ == i]) for i in range(n_clusters)}\nprint(\n    \"Mean silhouette:\",\n    silhouette_score(token_embs, kmeans.labels_) if n_clusters > 1 else 1.0,\n)\n\n\n# ---------- dataset transformation ----------\ndef seq_to_cluster_string(seq: str) -> str:\n    return \" \".join(f\"c{tok2cluster.get(tok,-1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    split: [seq_to_cluster_string(s) for s in dsets[split][\"sequence\"]]\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n# labels ids\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    split: np.array([lid[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- vectorizers ----------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\n\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\ndef build_features(split: str) -> np.ndarray:\n    X1 = vec_token.transform(dsets[split][\"sequence\"]).toarray().astype(np.float32)\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dim:\", X[\"train\"].shape[1])\n\n\n# ---------- model ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- dataloaders ----------\ndef make_loader(split: str, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------- CCWA ----------\ndef majority_cluster(seq_clusters: str) -> int:\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split: str, preds: np.ndarray) -> float:\n    maj_clusters = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    ccwa_num, ccwa_den = 0.0, 0.0\n    for cid in range(n_clusters):\n        mask = maj_clusters == cid\n        if not mask.any():\n            continue\n        Ai = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0)\n        ccwa_num += Si * Ai\n        ccwa_den += Si\n    return ccwa_num / ccwa_den if ccwa_den > 0 else 0.0\n\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    model.eval()\n    val_loss, val_preds = [], []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            val_loss.append(loss.item() * xb.size(0))\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss = sum(val_loss) / len(loaders[\"dev\"].dataset)\n    val_preds = np.array(val_preds)\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} validation_loss = {val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------- test evaluation ----------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\nprint(\n    f\"\\nTEST  ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y[\"test\"]\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Remove-Cluster-Feature Ablation \u2013 single-file runnable script\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# ---------- working dir & device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data dict ----------\nexperiment_data = {\n    \"RemoveClusterFeat\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ---------- data loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, generating synthetic toy data\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# ---------- helpers ----------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------- labels ----------\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    sp: np.array([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- vectorizer (ONLY raw token n-grams) ----------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_token.fit(dsets[\"train\"][\"sequence\"])\n\n\ndef build_features(split):\n    return vec_token.transform(dsets[split][\"sequence\"]).toarray().astype(np.float32)\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dim (X1 only):\", X[\"train\"].shape[1])\n\n\n# ---------- simple MLP ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- dataloaders ----------\ndef make_loader(split, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------- CCWA surrogate (cluster info removed) ----------\ndef compute_ccwa(split, preds):\n    # Without clusters we fallback to plain accuracy so the metric is still defined.\n    return (preds == y[split]).mean()\n\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    # training\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    # validation\n    model.eval()\n    val_loss, val_preds = 0.0, []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            val_loss += loss.item() * xb.size(0)\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss /= len(loaders[\"dev\"].dataset)\n    val_preds = np.array(val_preds)\n\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    # log\n    ed = experiment_data[\"RemoveClusterFeat\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------- test evaluation ----------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\n\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\n\nprint(\n    f\"\\nTEST  ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\ned = experiment_data[\"RemoveClusterFeat\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\ned[\"predictions\"] = preds\ned[\"ground_truth\"] = y[\"test\"]\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List\n\n# ---------- directories / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------- experiment dict ----------\nexperiment_data = {\n    \"RandomClusterAssignment\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ---------- dataset loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    out = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        out[sp] = _ld(f\"{sp}.csv\")\n    return out\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    # synthetic fallback\n    print(\"Dataset not found, building synthetic tiny split for demo.\")\n    shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n\n    def synth(n):\n        seq = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seq, \"label\": labels}\n\n    dsets = DatasetDict()\n    for sp, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[sp] = load_dataset(\"json\", data_files={\"train\": synth(n)}, split=\"train\")\n\n\n# ---------- helper metrics ----------\ndef count_color_variety(seq):\n    return len({t[1] for t in seq.split() if len(t) > 1})\n\n\ndef count_shape_variety(seq):\n    return len({t[0] for t in seq.split() if t})\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------- embeddings ----------\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(device).eval()\n        out = []\n        with torch.no_grad():\n            for t in tokens:\n                inp = tok(t, return_tensors=\"pt\").to(device)\n                out.append(mdl(**inp).last_hidden_state[:, 0].squeeze(0).cpu().numpy())\n        return np.vstack(out)\n    except Exception as e:\n        print(\"Falling back to simple embedding:\", e)\n        vec = []\n        for t in tokens:\n            a = ord(t[0])\n            b = ord(t[1]) if len(t) > 1 else 0\n            vec.append([a / 1000.0, b / 1000.0])\n        return np.array(vec, dtype=np.float32)\n\n\n# ---------- build random clusters ----------\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\n\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Assigning {len(uniq_tokens)} unique tokens to {n_clusters} random clusters\")\n\n# ensure at least one token per cluster (and preferably \u22652 for silhouette)\nlabels = np.arange(n_clusters).repeat((len(uniq_tokens) // n_clusters) + 1)\nnp.random.shuffle(labels)\nrand_labels = labels[: len(uniq_tokens)]\n\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, rand_labels)}\n\n# silhouette values for CCWA (may be low because of randomness)\nif n_clusters > 1 and len(set(rand_labels)) > 1 and min(np.bincount(rand_labels)) > 1:\n    silh_vals = silhouette_samples(token_embs, rand_labels)\n    mean_silh = silhouette_score(token_embs, rand_labels)\nelse:\n    silh_vals, mean_silh = np.ones(len(uniq_tokens)), 1.0\ncluster_silh = {\n    i: np.mean(silh_vals[np.array(rand_labels) == i]) for i in range(n_clusters)\n}\nprint(\"Mean silhouette (random clusters):\", mean_silh)\n\n\n# ---------- sequences \u2192 cluster token strings ----------\ndef seq_to_cluster_string(seq):\n    return \" \".join(f\"c{tok2cluster.get(tok, -1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    sp: [seq_to_cluster_string(s) for s in dsets[sp][\"sequence\"]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- label to id ----------\nlabels_all = sorted(set(dsets[\"train\"][\"label\"]))\nlid = {l: i for i, l in enumerate(labels_all)}\ny = {\n    sp: np.array([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- vectorisation ----------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\ndef build_features(split):\n    X1 = vec_token.transform(dsets[split][\"sequence\"]).toarray().astype(np.float32)\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dimension:\", X[\"train\"].shape[1])\n\n\n# ---------- model ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels_all)).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- dataloaders ----------\ndef make_loader(split, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------- CCWA ----------\ndef majority_cluster(cs):\n    ids = [int(t[1:]) for t in cs.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split, preds):\n    maj = np.array([majority_cluster(s) for s in cluster_strings[split]])\n    num = den = 0.0\n    for cid in range(n_clusters):\n        mask = maj == cid\n        if not mask.any():\n            continue\n        Ai = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0)\n        num += Si * Ai\n        den += Si\n    return num / den if den else 0.0\n\n\n# ---------- training loop ----------\nepochs = 5\nfor ep in range(1, epochs + 1):\n    # train\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        opt.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        opt.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    # validation\n    model.eval()\n    vloss, preds = 0.0, []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            vloss += criterion(out, yb).item() * xb.size(0)\n            preds.extend(out.argmax(1).cpu().numpy())\n    vloss /= len(loaders[\"dev\"].dataset)\n    preds = np.array(preds)\n    acc = (preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], preds)\n    ccwa = compute_ccwa(\"dev\", preds)\n\n    # store\n    ed = experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(vloss)\n    ed[\"metrics\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n    ed[\"metrics\"][\"val\"].append(\n        {\"epoch\": ep, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n    print(\n        f\"Epoch {ep}: train_loss={tr_loss:.4f} val_loss={vloss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------- test evaluation ----------\nmodel.eval()\nwith torch.no_grad():\n    test_pred = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\nt_acc = (test_pred == y[\"test\"]).mean()\nt_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], test_pred)\nt_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], test_pred)\nt_ccwa = compute_ccwa(\"test\", test_pred)\nprint(f\"\\nTEST ACC={t_acc:.3f} CWA={t_cwa:.3f} SWA={t_swa:.3f} CCWA={t_ccwa:.3f}\")\n\ned = experiment_data[\"RandomClusterAssignment\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test\"] = {\"acc\": t_acc, \"cwa\": t_cwa, \"swa\": t_swa, \"ccwa\": t_ccwa}\ned[\"predictions\"] = test_pred\ned[\"ground_truth\"] = y[\"test\"]\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List, Dict\n\n# ---------- working dir & device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"remove_token_feature\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ---------- data loading ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, building synthetic tiny split for demo\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# ---------- helpers ----------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------- BERT embedding for glyph tokens ----------\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n        mdl.eval()\n        with torch.no_grad():\n            embs = []\n            for t in tokens:\n                inp = tok(t, return_tensors=\"pt\").to(device)\n                out = mdl(**inp).last_hidden_state[:, 0]  # CLS\n                embs.append(out.squeeze(0).cpu().numpy())\n        return np.vstack(embs)\n    except Exception as e:\n        print(\"Transformer load failed, using simple ord-embedding:\", e)\n        vec = []\n        for t in tokens:\n            a = ord(t[0])\n            b = ord(t[1]) if len(t) > 1 else 0\n            vec.append([a / 1000, b / 1000])\n        return np.array(vec, dtype=np.float32)\n\n\n# ---------- build token clusters ----------\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\n\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Clustering {len(uniq_tokens)} tokens into {n_clusters} clusters\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = (\n    silhouette_samples(token_embs, kmeans.labels_)\n    if n_clusters > 1\n    else np.ones(len(uniq_tokens))\n)\ncluster_silh = {i: np.mean(silh_vals[kmeans.labels_ == i]) for i in range(n_clusters)}\nprint(\n    \"Mean silhouette:\",\n    silhouette_score(token_embs, kmeans.labels_) if n_clusters > 1 else 1.0,\n)\n\n\n# ---------- dataset transformation ----------\ndef seq_to_cluster_string(seq: str) -> str:\n    return \" \".join(f\"c{tok2cluster.get(tok,-1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    split: [seq_to_cluster_string(s) for s in dsets[split][\"sequence\"]]\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n# labels ids\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    split: np.array([lid[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    for split in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- vectorizers ----------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))  # unused later\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\n\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\n# ---------- ablation build_features (X2 only) ----------\ndef build_features(split: str) -> np.ndarray:\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return X2  # omit token-level features\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dim after ablation:\", X[\"train\"].shape[1])\n\n\n# ---------- model ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- dataloaders ----------\ndef make_loader(split: str, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------- CCWA ----------\ndef majority_cluster(seq_clusters: str) -> int:\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split: str, preds: np.ndarray) -> float:\n    maj_clusters = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    ccwa_num, ccwa_den = 0.0, 0.0\n    for cid in range(n_clusters):\n        mask = maj_clusters == cid\n        if not mask.any():\n            continue\n        Ai = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0)\n        ccwa_num += Si * Ai\n        ccwa_den += Si\n    return ccwa_num / ccwa_den if ccwa_den > 0 else 0.0\n\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    model.eval()\n    val_loss, val_preds = [], []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            val_loss.append(loss.item() * xb.size(0))\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss = sum(val_loss) / len(loaders[\"dev\"].dataset)\n    val_preds = np.array(val_preds)\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    experiment_data[\"remove_token_feature\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        tr_loss\n    )\n    experiment_data[\"remove_token_feature\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        val_loss\n    )\n    experiment_data[\"remove_token_feature\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} validation_loss={val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------- test evaluation ----------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\nprint(\n    f\"\\nTEST  ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\nexperiment_data[\"remove_token_feature\"][\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\nexperiment_data[\"remove_token_feature\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"remove_token_feature\"][\"SPR_BENCH\"][\"ground_truth\"] = y[\"test\"]\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# ORD-Embedding-Cluster Ablation \u2500 self-contained single-file implementation\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n# ---------- experiment bookkeeping ----------\nexperiment_data = {\n    \"ORD_EMB_CLUSTER_ABLATION\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\nexp_ref = experiment_data[\"ORD_EMB_CLUSTER_ABLATION\"][\"SPR_BENCH\"]\n\n# ---------- working dir & device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- dataset loading (SPR_BENCH or synthetic) ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, creating tiny synthetic data for demo\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# ---------- evaluation helpers ----------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------- ORD token embedding (no BERT) ----------\ndef embed_tokens(tokens):\n    vec = []\n    for tok in tokens:\n        a = ord(tok[0]) / 1000.0\n        b = ord(tok[1]) / 1000.0 if len(tok) > 1 else 0.0\n        vec.append([a, b])\n    return np.asarray(vec, dtype=np.float32)\n\n\n# ---------- build token clusters ----------\ntrain_tokens = [tk for seq in dsets[\"train\"][\"sequence\"] for tk in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Clustering {len(uniq_tokens)} unique tokens into {n_clusters} clusters\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = (\n    silhouette_samples(token_embs, kmeans.labels_)\n    if n_clusters > 1\n    else np.ones(len(uniq_tokens))\n)\ncluster_silh = {\n    i: float(np.mean(silh_vals[kmeans.labels_ == i])) for i in range(n_clusters)\n}\nprint(\n    \"Mean silhouette:\",\n    float(silhouette_score(token_embs, kmeans.labels_)) if n_clusters > 1 else 1.0,\n)\n\n\ndef seq_to_cluster_string(seq):\n    return \" \".join(f\"c{tok2cluster.get(tok,-1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    sp: [seq_to_cluster_string(s) for s in dsets[sp][\"sequence\"]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- label indexing ----------\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    sp: np.asarray([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------- vectorisation ----------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\ndef build_features(split):\n    X1 = vec_token.transform(dsets[split][\"sequence\"]).toarray().astype(np.float32)\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dimension:\", X[\"train\"].shape[1])\n\n\n# ---------- simple MLP ----------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------- dataloaders ----------\ndef make_loader(split, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------- CCWA ----------\ndef majority_cluster(seq_clusters):\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split, preds):\n    maj_clusters = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    num = den = 0.0\n    for cid in range(n_clusters):\n        mask = maj_clusters == cid\n        if not mask.any():\n            continue\n        Ai = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0.0)\n        num += Si * Ai\n        den += Si\n    return num / den if den > 0 else 0.0\n\n\n# ---------- training loop ----------\nepochs = 5\nfor ep in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(xb), yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    model.eval()\n    val_loss, val_preds = 0.0, []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            val_loss += criterion(out, yb).item() * xb.size(0)\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss /= len(loaders[\"dev\"].dataset)\n    val_preds = np.asarray(val_preds)\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    exp_ref[\"losses\"][\"train\"].append(tr_loss)\n    exp_ref[\"losses\"][\"val\"].append(val_loss)\n    exp_ref[\"metrics\"][\"val\"].append(\n        {\"epoch\": ep, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {ep}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------- final test evaluation ----------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\n\nprint(\n    f\"\\nTEST  ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\nexp_ref[\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\nexp_ref[\"predictions\"] = preds\nexp_ref[\"ground_truth\"] = y[\"test\"]\n\n# ---------- save experiment ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List\n\n# ---------------- experiment bookkeeping ----------------\nexperiment_data = {\n    \"no_bigram\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\nablation_key = \"no_bigram\"\nds_key = \"SPR_BENCH\"\n\n# ---------------- working dir & device ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- data loading --------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, building synthetic tiny split for demo\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# ---------------- helper metrics ------------------------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------------- token embedding -----------------------\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n        mdl.eval()\n        embs = []\n        with torch.no_grad():\n            for t in tokens:\n                inp = tok(t, return_tensors=\"pt\").to(device)\n                embs.append(mdl(**inp).last_hidden_state[:, 0].squeeze(0).cpu().numpy())\n        return np.vstack(embs)\n    except Exception as e:\n        print(\"Transformer load failed, using simple ord-embedding:\", e)\n        return np.array(\n            [[ord(t[0]) / 1000, ord(t[1]) / 1000 if len(t) > 1 else 0] for t in tokens],\n            dtype=np.float32,\n        )\n\n\n# ---------------- build token clusters ------------------\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Clustering {len(uniq_tokens)} tokens into {n_clusters} clusters\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = (\n    silhouette_samples(token_embs, kmeans.labels_)\n    if n_clusters > 1\n    else np.ones(len(uniq_tokens))\n)\ncluster_silh = {i: np.mean(silh_vals[kmeans.labels_ == i]) for i in range(n_clusters)}\nprint(\n    \"Mean silhouette:\",\n    silhouette_score(token_embs, kmeans.labels_) if n_clusters > 1 else 1.0,\n)\n\n\ndef seq_to_cluster_string(seq: str) -> str:\n    return \" \".join(f\"c{tok2cluster.get(tok,-1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    sp: [seq_to_cluster_string(s) for s in dsets[sp][\"sequence\"]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------------- labels -------------------------------\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    sp: np.array([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------------- CountVectorizers (UNIGRAM ONLY) -------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 1))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 1))\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\ndef build_features(split: str) -> np.ndarray:\n    X1 = vec_token.transform(dsets[split][\"sequence\"]).toarray().astype(np.float32)\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dimension:\", X[\"train\"].shape[1])\n\n\n# ---------------- model -------------------------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------------- data loaders ------------------------\ndef make_loader(split, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------------- CCWA metric -------------------------\ndef majority_cluster(seq_clusters: str) -> int:\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split: str, preds: np.ndarray) -> float:\n    maj_clusters = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    ccwa_num = ccwa_den = 0.0\n    for cid in range(n_clusters):\n        mask = maj_clusters == cid\n        if not mask.any():\n            continue\n        Ai = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0)\n        ccwa_num += Si * Ai\n        ccwa_den += Si\n    return ccwa_num / ccwa_den if ccwa_den > 0 else 0.0\n\n\n# ---------------- training loop -----------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        loss = criterion(model(xb), yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    model.eval()\n    val_loss, val_preds = 0.0, []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            val_loss += criterion(out, yb).item() * xb.size(0)\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss /= len(loaders[\"dev\"].dataset)\n    val_preds = np.array(val_preds)\n\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    ed = experiment_data[ablation_key][ds_key]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------------- test evaluation ---------------------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\nprint(\n    f\"\\nTEST  ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\ned[\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\ned[\"predictions\"] = preds\ned[\"ground_truth\"] = y[\"test\"]\n\n# ---------------- save everything ---------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, torch.nn as nn, random\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List, Dict\n\n# -------------------- experiment bookkeeping --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nablation_name = \"TokenOrderShuffle\"\nexperiment_data = {\n    ablation_name: {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------- data loading --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, building synthetic tiny split for demo.\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# -------------------- token-order shuffle ablation --------------------\ndef shuffle_sequence(seq: str) -> str:\n    toks = seq.split()\n    random.shuffle(toks)\n    return \" \".join(toks)\n\n\nshuf_sequences = {\n    sp: [shuffle_sequence(s) for s in dsets[sp][\"sequence\"]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n\n# -------------------- helper metrics (based on original sequences) --------------------\ndef count_color_variety(seq):  # color is second char\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):  # shape is first char\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# -------------------- token embedding & clustering (unchanged) --------------------\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n        mdl.eval()\n        with torch.no_grad():\n            embs = []\n            for t in tokens:\n                inp = tok(t, return_tensors=\"pt\").to(device)\n                out = mdl(**inp).last_hidden_state[:, 0]\n                embs.append(out.squeeze(0).cpu().numpy())\n        return np.vstack(embs)\n    except Exception as e:\n        print(\"Transformer unavailable, using ord embedding.\", e)\n        vec = []\n        for t in tokens:\n            a = ord(t[0])\n            b = ord(t[1]) if len(t) > 1 else 0\n            vec.append([a / 1000, b / 1000])\n        return np.array(vec, dtype=np.float32)\n\n\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Clustering {len(uniq_tokens)} tokens into {n_clusters} clusters.\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = (\n    silhouette_samples(token_embs, kmeans.labels_)\n    if n_clusters > 1\n    else np.ones(len(uniq_tokens))\n)\ncluster_silh = {i: np.mean(silh_vals[kmeans.labels_ == i]) for i in range(n_clusters)}\nprint(\n    \"Mean silhouette:\",\n    silhouette_score(token_embs, kmeans.labels_) if n_clusters > 1 else 1.0,\n)\n\n\ndef seq_to_cluster_string(seq: str) -> str:\n    return \" \".join(f\"c{tok2cluster.get(tok, -1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    sp: [seq_to_cluster_string(s) for s in shuf_sequences[sp]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# -------------------- labels --------------------\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    sp: np.array([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# -------------------- feature extraction --------------------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_token.fit(shuf_sequences[\"train\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\ndef build_features(split: str) -> np.ndarray:\n    X1 = vec_token.transform(shuf_sequences[split]).toarray().astype(np.float32)\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dim:\", X[\"train\"].shape[1])\n\n\n# -------------------- simple MLP model --------------------\nclass MLP(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(d_in, 256), nn.ReLU(), nn.Linear(256, d_out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\ndef make_loader(split, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# -------------------- CCWA --------------------\ndef majority_cluster(seq_clusters: str) -> int:\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split: str, preds: np.ndarray) -> float:\n    maj_clusters = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    ccwa_num = ccwa_den = 0.0\n    for cid in range(n_clusters):\n        mask = maj_clusters == cid\n        if not mask.any():\n            continue\n        acc_i = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0)\n        ccwa_num += Si * acc_i\n        ccwa_den += Si\n    return ccwa_num / ccwa_den if ccwa_den else 0.0\n\n\n# -------------------- training --------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    model.eval()\n    val_loss = 0.0\n    val_preds = []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            val_loss += loss.item() * xb.size(0)\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss /= len(loaders[\"dev\"].dataset)\n    val_preds = np.array(val_preds)\n\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    experiment_data[ablation_name][\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[ablation_name][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[ablation_name][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# -------------------- test evaluation --------------------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\n\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\n\nprint(\n    f\"\\nTEST  ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\nexperiment_data[ablation_name][\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\nexperiment_data[ablation_name][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[ablation_name][\"SPR_BENCH\"][\"ground_truth\"] = y[\"test\"]\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Two-Cluster-Granularity Ablation \u2500 self-contained script\nimport os, pathlib, numpy as np, torch, torch.nn as nn, warnings\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List, Dict\n\n# ---------------- working dir & device ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- experiment data ----------------\nexperiment_data = {\n    \"TwoClusterGranularity\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\nexp_key = experiment_data[\"TwoClusterGranularity\"][\"SPR_BENCH\"]  # shortcut\n\n\n# ---------------- data loading ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:\n    print(\"Dataset not found, building synthetic tiny split for demo\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for split, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[split] = load_dataset(\n            \"json\", data_files={\"train\": synth(n)}, split=\"train\"\n        )\n\n\n# ---------------- helpers ----------------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# ---------------- BERT embedding for glyph tokens ----------------\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n        mdl.eval()\n        with torch.no_grad():\n            embs = []\n            for t in tokens:\n                inp = tok(t, return_tensors=\"pt\").to(device)\n                embs.append(mdl(**inp).last_hidden_state[:, 0].squeeze(0).cpu().numpy())\n        return np.vstack(embs)\n    except Exception as e:\n        warnings.warn(f\"Transformer load failed, using simple ord embeddings: {e}\")\n        vec = [\n            [ord(t[0]) / 1000, ord(t[1]) / 1000 if len(t) > 1 else 0] for t in tokens\n        ]\n        return np.array(vec, dtype=np.float32)\n\n\n# ---------------- build token clusters (k = 2) ----------------\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\n\nn_clusters = 2  # enforced by ablation\nprint(f\"Ablation: clustering {len(uniq_tokens)} tokens into {n_clusters} clusters\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = silhouette_samples(token_embs, kmeans.labels_)\ncluster_silh = {i: np.mean(silh_vals[kmeans.labels_ == i]) for i in range(n_clusters)}\nprint(\"Mean silhouette:\", silhouette_score(token_embs, kmeans.labels_))\n\n\n# ---------------- dataset transformation ----------------\ndef seq_to_cluster_string(seq: str) -> str:\n    return \" \".join(f\"c{tok2cluster.get(tok,-1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    sp: [seq_to_cluster_string(s) for s in dsets[sp][\"sequence\"]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# label ids\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    sp: np.array([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# ---------------- vectorizers ----------------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\ndef build_features(split: str) -> np.ndarray:\n    X1 = vec_token.transform(dsets[split][\"sequence\"]).toarray().astype(np.float32)\n    X2 = vec_clust.transform(cluster_strings[split]).toarray().astype(np.float32)\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Feature dim:\", X[\"train\"].shape[1])\n\n\n# ---------------- model ----------------\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Linear(256, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------------- dataloaders ----------------\ndef make_loader(split: str, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# ---------------- CCWA ----------------\ndef majority_cluster(seq_clusters: str) -> int:\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split: str, preds: np.ndarray) -> float:\n    maj_clusters = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    ccwa_num = ccwa_den = 0.0\n    for cid in range(n_clusters):\n        mask = maj_clusters == cid\n        if not mask.any():\n            continue\n        Ai = (preds[mask] == y[split][mask]).mean()\n        Si = cluster_silh.get(cid, 0)\n        ccwa_num += Si * Ai\n        ccwa_den += Si\n    return ccwa_num / ccwa_den if ccwa_den > 0 else 0.0\n\n\n# ---------------- training loop ----------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n\n    model.eval()\n    val_loss, val_preds = [], []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            val_loss.append(loss.item() * xb.size(0))\n            val_preds.extend(out.argmax(1).cpu().numpy())\n    val_loss = sum(val_loss) / len(loaders[\"dev\"].dataset)\n    val_preds = np.array(val_preds)\n    acc = (val_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], val_preds)\n    ccwa = compute_ccwa(\"dev\", val_preds)\n\n    exp_key[\"losses\"][\"train\"].append(tr_loss)\n    exp_key[\"losses\"][\"val\"].append(val_loss)\n    exp_key[\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# ---------------- test evaluation ----------------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\ntest_acc = (preds == y[\"test\"]).mean()\ntest_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\ntest_ccwa = compute_ccwa(\"test\", preds)\nprint(\n    f\"\\nTEST ACC={test_acc:.3f} CWA={test_cwa:.3f} SWA={test_swa:.3f} CCWA={test_ccwa:.3f}\"\n)\n\nexp_key[\"metrics\"][\"test\"] = {\n    \"acc\": test_acc,\n    \"cwa\": test_cwa,\n    \"swa\": test_swa,\n    \"ccwa\": test_ccwa,\n}\nexp_key[\"predictions\"] = preds\nexp_key[\"ground_truth\"] = y[\"test\"]\n\n# ---------------- save ----------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom typing import List, Dict\n\n# --------- working dir & experiment dict ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"BinaryCountAblation\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": {}},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# --------- load SPR_BENCH or synthetic tiny version ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(name):  # helper\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for sp in [\"train\", \"dev\", \"test\"]:\n        d[sp] = _ld(f\"{sp}.csv\")\n    return d\n\n\nDATA_ENV = os.getenv(\"SPR_BENCH_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif pathlib.Path(DATA_ENV).exists():\n    dsets = load_spr_bench(pathlib.Path(DATA_ENV))\nelse:  # tiny synthetic fallback\n    print(\"Dataset not found \u2013 using synthetic tiny split for demo\")\n\n    def synth(n):\n        shapes, colors = [\"\u25b2\", \"\u25cf\", \"\u25a0\"], [\"r\", \"g\", \"b\"]\n        seqs = [\n            \" \".join(\n                np.random.choice(\n                    [s + c for s in shapes for c in colors],\n                    size=np.random.randint(3, 8),\n                )\n            )\n            for _ in range(n)\n        ]\n        labels = np.random.choice([\"A\", \"B\", \"C\"], size=n).tolist()\n        return {\"sequence\": seqs, \"label\": labels}\n\n    dsets = DatasetDict()\n    for sp, n in zip([\"train\", \"dev\", \"test\"], [400, 100, 100]):\n        dsets[sp] = load_dataset(\"json\", data_files={\"train\": synth(n)}, split=\"train\")\n\n\n# --------- misc helpers ----------\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split() if len(t) > 1))\n\n\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split() if t))\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\n# --------- token embedding (BERT if available else ord) ----------\ndef embed_tokens(tokens: List[str]) -> np.ndarray:\n    try:\n        from transformers import AutoTokenizer, AutoModel\n\n        tok, mdl = (\n            AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n            AutoModel.from_pretrained(\"bert-base-uncased\").to(device).eval(),\n        )\n        with torch.no_grad():\n            return np.vstack(\n                [\n                    mdl(**tok(t, return_tensors=\"pt\").to(device))\n                    .last_hidden_state[:, 0]\n                    .squeeze(0)\n                    .cpu()\n                    .numpy()\n                    for t in tokens\n                ]\n            )\n    except Exception as e:\n        print(\"BERT unavailable, using ord embedding:\", e)\n        return np.array(\n            [\n                [ord(t[0]) / 1000, (ord(t[1]) if len(t) > 1 else 0) / 1000]\n                for t in tokens\n            ],\n            dtype=np.float32,\n        )\n\n\n# --------- build token clusters ----------\ntrain_tokens = [tok for seq in dsets[\"train\"][\"sequence\"] for tok in seq.split()]\nuniq_tokens = sorted(set(train_tokens))\ntoken_embs = embed_tokens(uniq_tokens)\nn_clusters = min(8, max(2, len(uniq_tokens) // 3))\nprint(f\"Clustering {len(uniq_tokens)} tokens into {n_clusters} clusters\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(token_embs)\ntok2cluster = {t: int(c) for t, c in zip(uniq_tokens, kmeans.labels_)}\nsilh_vals = (\n    silhouette_samples(token_embs, kmeans.labels_)\n    if n_clusters > 1\n    else np.ones(len(uniq_tokens))\n)\ncluster_silh = {i: silh_vals[kmeans.labels_ == i].mean() for i in range(n_clusters)}\n\n\ndef seq_to_cluster_string(seq: str) -> str:\n    return \" \".join(f\"c{tok2cluster.get(tok,-1)}\" for tok in seq.split())\n\n\ncluster_strings = {\n    sp: [seq_to_cluster_string(s) for s in dsets[sp][\"sequence\"]]\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# --------- label encoding ----------\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlid = {l: i for i, l in enumerate(labels)}\ny = {\n    sp: np.array([lid[l] for l in dsets[sp][\"label\"]], dtype=np.int64)\n    for sp in [\"train\", \"dev\", \"test\"]\n}\n\n# --------- vectorisers (keep vocabulary) ----------\nvec_token = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_clust = CountVectorizer(token_pattern=r\"[^ ]+\", ngram_range=(1, 2))\nvec_token.fit(dsets[\"train\"][\"sequence\"])\nvec_clust.fit(cluster_strings[\"train\"])\n\n\n# --------- Binary Count Ablation: convert counts>0 to 1 ----------\ndef build_features(split: str) -> np.ndarray:\n    X1 = (\n        (vec_token.transform(dsets[split][\"sequence\"]) > 0).astype(np.float32).toarray()\n    )\n    X2 = (vec_clust.transform(cluster_strings[split]) > 0).astype(np.float32).toarray()\n    return np.hstack([X1, X2])\n\n\nX = {sp: build_features(sp) for sp in [\"train\", \"dev\", \"test\"]}\nprint(\"Binary feature dim:\", X[\"train\"].shape[1])\n\n\n# --------- MLP ----------\nclass MLP(nn.Module):\n    def __init__(self, inp, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, 256), nn.ReLU(), nn.Linear(256, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = MLP(X[\"train\"].shape[1], len(labels)).to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\n# --------- dataloaders ----------\ndef make_loader(split, bs=64):\n    ds = TensorDataset(torch.from_numpy(X[split]), torch.from_numpy(y[split]))\n    return DataLoader(ds, batch_size=bs, shuffle=(split == \"train\"))\n\n\nloaders = {sp: make_loader(sp) for sp in [\"train\", \"dev\"]}\n\n\n# --------- CCWA ----------\ndef majority_cluster(seq_clusters: str) -> int:\n    ids = [int(t[1:]) for t in seq_clusters.split() if t != \"c-1\"]\n    return max(set(ids), key=ids.count) if ids else -1\n\n\ndef compute_ccwa(split: str, preds: np.ndarray) -> float:\n    maj = np.array([majority_cluster(cs) for cs in cluster_strings[split]])\n    num = den = 0.0\n    for cid in range(n_clusters):\n        m = maj == cid\n        if not m.any():\n            continue\n        Ai = (preds[m] == y[split][m]).mean()\n        Si = cluster_silh.get(cid, 0)\n        num += Si * Ai\n        den += Si\n    return num / den if den else 0.0\n\n\n# --------- training loop ----------\nepochs = 5\nfor ep in range(1, epochs + 1):\n    # train\n    model.train()\n    tr_loss = 0.0\n    for xb, yb in loaders[\"train\"]:\n        xb, yb = xb.to(device), yb.to(device)\n        optim.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optim.step()\n        tr_loss += loss.item() * xb.size(0)\n    tr_loss /= len(loaders[\"train\"].dataset)\n    # val\n    model.eval()\n    v_loss = []\n    v_preds = []\n    with torch.no_grad():\n        for xb, yb in loaders[\"dev\"]:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            v_loss.append(loss.item() * xb.size(0))\n            v_preds.extend(out.argmax(1).cpu().numpy())\n    v_loss = sum(v_loss) / len(loaders[\"dev\"].dataset)\n    v_preds = np.array(v_preds)\n    acc = (v_preds == y[\"dev\"]).mean()\n    cwa = color_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], v_preds)\n    swa = shape_weighted_accuracy(dsets[\"dev\"][\"sequence\"], y[\"dev\"], v_preds)\n    ccwa = compute_ccwa(\"dev\", v_preds)\n\n    ed = experiment_data[\"BinaryCountAblation\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(v_loss)\n    ed[\"metrics\"][\"val\"].append(\n        {\"epoch\": ep, \"acc\": acc, \"cwa\": cwa, \"swa\": swa, \"ccwa\": ccwa}\n    )\n\n    print(\n        f\"E{ep}: train_loss={tr_loss:.4f} val_loss={v_loss:.4f} ACC={acc:.3f} CWA={cwa:.3f} SWA={swa:.3f} CCWA={ccwa:.3f}\"\n    )\n\n# --------- test evaluation ----------\nmodel.eval()\nwith torch.no_grad():\n    preds = model(torch.from_numpy(X[\"test\"]).to(device)).argmax(1).cpu().numpy()\nt_acc = (preds == y[\"test\"]).mean()\nt_cwa = color_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\nt_swa = shape_weighted_accuracy(dsets[\"test\"][\"sequence\"], y[\"test\"], preds)\nt_ccwa = compute_ccwa(\"test\", preds)\nprint(f\"\\nTEST ACC={t_acc:.3f} CWA={t_cwa:.3f} SWA={t_swa:.3f} CCWA={t_ccwa:.3f}\")\n\ned = experiment_data[\"BinaryCountAblation\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test\"] = {\"acc\": t_acc, \"cwa\": t_cwa, \"swa\": t_swa, \"ccwa\": t_ccwa}\ned[\"predictions\"] = preds\ned[\"ground_truth\"] = y[\"test\"]\n\n# --------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n"], "term_out": ["['Using device: cuda', '\\n', 'Clustering 16 tokens into 5 clusters', '\\n', 'Mean\nsilhouette:', ' ', '0.53673553', '\\n', 'Feature dim:', ' ', '302', '\\n', 'Epoch\n1: train_loss=0.2624 validation_loss = 0.1114 ACC=0.972 CWA=0.972 SWA=0.971\nCCWA=0.974', '\\n', 'Epoch 2: train_loss=0.0645 validation_loss = 0.0525\nACC=0.987 CWA=0.987 SWA=0.987 CCWA=0.990', '\\n', 'Epoch 3: train_loss=0.0345\nvalidation_loss = 0.0355 ACC=0.992 CWA=0.992 SWA=0.992 CCWA=0.993', '\\n', 'Epoch\n4: train_loss=0.0221 validation_loss = 0.0353 ACC=0.989 CWA=0.989 SWA=0.988\nCCWA=0.991', '\\n', 'Epoch 5: train_loss=0.0160 validation_loss = 0.0274\nACC=0.992 CWA=0.993 SWA=0.992 CCWA=0.993', '\\n', '\\nTEST  ACC=0.697 CWA=0.633\nSWA=0.697 CCWA=0.696', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-31_03-13-\n33_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n15/working/experiment_data.npy', '\\n', 'Execution time: 13 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 221164.32\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 190573.95\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 327936.20\nexamples/s]', '\\n', 'Feature dim (X1 only):', ' ', '272', '\\n', 'Epoch 1:\ntrain_loss=0.3049 val_loss=0.1246 ACC=0.962 CWA=0.962 SWA=0.962 CCWA=0.962',\n'\\n', 'Epoch 2: train_loss=0.0804 val_loss=0.0594 ACC=0.984 CWA=0.985 SWA=0.984\nCCWA=0.984', '\\n', 'Epoch 3: train_loss=0.0436 val_loss=0.0447 ACC=0.987\nCWA=0.987 SWA=0.987 CCWA=0.987', '\\n', 'Epoch 4: train_loss=0.0301\nval_loss=0.0393 ACC=0.989 CWA=0.989 SWA=0.988 CCWA=0.989', '\\n', 'Epoch 5:\ntrain_loss=0.0213 val_loss=0.0326 ACC=0.992 CWA=0.992 SWA=0.991 CCWA=0.992',\n'\\n', '\\nTEST  ACC=0.696 CWA=0.633 SWA=0.696 CCWA=0.696', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n31_03-13-33_symbol_glyph_clustering_attempt_0/0-run/process_ForkProcess-\n20/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n272569.79 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 264324.68\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 199541.57\nexamples/s]', '\\n', 'Assigning 16 unique tokens to 5 random clusters', '\\n',\n'Mean silhouette (random clusters):', ' ', '-0.30251196', '\\n', 'Feature\ndimension:', ' ', '302', '\\n', 'Epoch 1: train_loss=0.3130 val_loss=0.1636\nACC=0.946 CWA=0.946 SWA=0.948 CCWA=0.954', '\\n', 'Epoch 2: train_loss=0.1050\nval_loss=0.0782 ACC=0.979 CWA=0.980 SWA=0.980 CCWA=0.984', '\\n', 'Epoch 3:\ntrain_loss=0.0562 val_loss=0.0496 ACC=0.986 CWA=0.986 SWA=0.985 CCWA=0.987',\n'\\n', 'Epoch 4: train_loss=0.0372 val_loss=0.0451 ACC=0.985 CWA=0.986 SWA=0.985\nCCWA=0.989', '\\n', 'Epoch 5: train_loss=0.0263 val_loss=0.0346 ACC=0.990\nCWA=0.990 SWA=0.989 CCWA=0.992', '\\n', '\\nTEST ACC=0.695 CWA=0.632 SWA=0.695\nCCWA=0.706', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-21/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 445884.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 133557.85\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 703175.97\nexamples/s]', '\\n', 'Clustering 16 tokens into 5 clusters', '\\n', 'Mean\nsilhouette:', ' ', '0.53673553', '\\n', 'Feature dim after ablation:', ' ', '30',\n'\\n', 'Epoch 1: train_loss=0.5106 validation_loss=0.4127 ACC=0.835 CWA=0.834\nSWA=0.825 CCWA=0.848', '\\n', 'Epoch 2: train_loss=0.3846 validation_loss=0.3710\nACC=0.842 CWA=0.842 SWA=0.831 CCWA=0.856', '\\n', 'Epoch 3: train_loss=0.3568\nvalidation_loss=0.3649 ACC=0.851 CWA=0.851 SWA=0.839 CCWA=0.865', '\\n', 'Epoch\n4: train_loss=0.3441 validation_loss=0.3521 ACC=0.856 CWA=0.856 SWA=0.844\nCCWA=0.868', '\\n', 'Epoch 5: train_loss=0.3370 validation_loss=0.3456 ACC=0.858\nCWA=0.858 SWA=0.846 CCWA=0.871', '\\n', '\\nTEST  ACC=0.643 CWA=0.596 SWA=0.639\nCCWA=0.645', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-23/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n231254.22 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 147453.12\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 394917.85\nexamples/s]', '\\n', 'Clustering 16 unique tokens into 5 clusters', '\\n', 'Mean\nsilhouette:', ' ', '0.4839850664138794', '\\n', 'Feature dimension:', ' ', '302',\n'\\n', 'Epoch 1: train_loss=0.2347 val_loss=0.0970 ACC=0.977 CWA=0.977 SWA=0.977\nCCWA=0.981', '\\n', 'Epoch 2: train_loss=0.0589 val_loss=0.0506 ACC=0.988\nCWA=0.988 SWA=0.988 CCWA=0.992', '\\n', 'Epoch 3: train_loss=0.0349\nval_loss=0.0387 ACC=0.989 CWA=0.990 SWA=0.989 CCWA=0.991', '\\n', 'Epoch 4:\ntrain_loss=0.0232 val_loss=0.0368 ACC=0.990 CWA=0.990 SWA=0.989 CCWA=0.992',\n'\\n', 'Epoch 5: train_loss=0.0158 val_loss=0.0278 ACC=0.994 CWA=0.994 SWA=0.993\nCCWA=0.996', '\\n', '\\nTEST  ACC=0.696 CWA=0.633 SWA=0.696 CCWA=0.720', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-22/working/experiment_data.npy', '\\n', 'Execution time:\n9 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Clustering 16 tokens into 5 clusters', '\\n', 'Mean\nsilhouette:', ' ', '0.53673553', '\\n', 'Feature dimension:', ' ', '21', '\\n',\n'Epoch 1: train_loss=0.4105 val_loss=0.2745 ACC=0.920 CWA=0.918 SWA=0.916\nCCWA=0.931', '\\n', 'Epoch 2: train_loss=0.2298 val_loss=0.2002 ACC=0.945\nCWA=0.943 SWA=0.940 CCWA=0.951', '\\n', 'Epoch 3: train_loss=0.1888\nval_loss=0.1823 ACC=0.948 CWA=0.946 SWA=0.943 CCWA=0.955', '\\n', 'Epoch 4:\ntrain_loss=0.1762 val_loss=0.1748 ACC=0.948 CWA=0.947 SWA=0.943 CCWA=0.956',\n'\\n', 'Epoch 5: train_loss=0.1714 val_loss=0.1708 ACC=0.949 CWA=0.948 SWA=0.945\nCCWA=0.957', '\\n', '\\nTEST  ACC=0.687 CWA=0.629 SWA=0.686 CCWA=0.687', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-21/working/experiment_data.npy', '\\n', 'Execution time:\n8 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Clustering 16 tokens into 5 clusters.',\n'\\n', 'Mean silhouette:', ' ', '0.53673553', '\\n', 'Feature dim:', ' ', '302',\n'\\n', 'Epoch 1: train_loss=0.3901 val_loss=0.2550 ACC=0.917 CWA=0.914 SWA=0.912\nCCWA=0.928', '\\n', 'Epoch 2: train_loss=0.2161 val_loss=0.2000 ACC=0.941\nCWA=0.940 SWA=0.936 CCWA=0.949', '\\n', 'Epoch 3: train_loss=0.1765\nval_loss=0.1865 ACC=0.945 CWA=0.944 SWA=0.940 CCWA=0.953', '\\n', 'Epoch 4:\ntrain_loss=0.1597 val_loss=0.1870 ACC=0.940 CWA=0.937 SWA=0.934 CCWA=0.948',\n'\\n', 'Epoch 5: train_loss=0.1448 val_loss=0.1843 ACC=0.946 CWA=0.944 SWA=0.941\nCCWA=0.953', '\\n', '\\nTEST  ACC=0.686 CWA=0.628 SWA=0.684 CCWA=0.686', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-20/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Ablation: clustering 16 tokens into 2 clusters',\n'\\n', 'Mean silhouette:', ' ', '0.6132444', '\\n', 'Feature dim:', ' ', '278',\n'\\n', 'Epoch 1: train_loss=0.2877 val_loss=0.1161 ACC=0.968 CWA=0.967 SWA=0.968\nCCWA=0.964', '\\n', 'Epoch 2: train_loss=0.0773 val_loss=0.0574 ACC=0.984\nCWA=0.984 SWA=0.984 CCWA=0.981', '\\n', 'Epoch 3: train_loss=0.0443\nval_loss=0.0416 ACC=0.989 CWA=0.989 SWA=0.989 CCWA=0.987', '\\n', 'Epoch 4:\ntrain_loss=0.0285 val_loss=0.0354 ACC=0.992 CWA=0.992 SWA=0.992 CCWA=0.991',\n'\\n', 'Epoch 5: train_loss=0.0210 val_loss=0.0448 ACC=0.986 CWA=0.986 SWA=0.986\nCCWA=0.986', '\\n', '\\nTEST ACC=0.695 CWA=0.632 SWA=0.694 CCWA=0.695', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-23/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Clustering 16 tokens into 5 clusters',\n'\\n', 'Binary feature dim:', ' ', '302', '\\n', 'E1: train_loss=0.3128\nval_loss=0.1947 ACC=0.930 CWA=0.929 SWA=0.927 CCWA=0.932', '\\n', 'E2:\ntrain_loss=0.1417 val_loss=0.1231 ACC=0.960 CWA=0.960 SWA=0.957 CCWA=0.960',\n'\\n', 'E3: train_loss=0.0946 val_loss=0.0947 ACC=0.968 CWA=0.969 SWA=0.967\nCCWA=0.966', '\\n', 'E4: train_loss=0.0696 val_loss=0.0835 ACC=0.972 CWA=0.972\nSWA=0.970 CCWA=0.970', '\\n', 'E5: train_loss=0.0552 val_loss=0.0780 ACC=0.974\nCWA=0.975 SWA=0.972 CCWA=0.970', '\\n', '\\nTEST ACC=0.687 CWA=0.626 SWA=0.686\nCCWA=0.687', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/0-\nrun/process_ForkProcess-22/working/experiment_data.npy', '\\n', 'Execution time:\n10 seconds seconds (time limit is 30 minutes).']"], "analysis": ["", "", "", "", "The execution of the script was successful with no bugs observed. The training\nloop completed as expected, and the final test results were produced. The model\nachieved promising validation metrics (ACC=0.994, CWA=0.994, SWA=0.993,\nCCWA=0.996) and reasonable test metrics (ACC=0.696, CWA=0.633, SWA=0.696,\nCCWA=0.720). The clustering process and silhouette score calculation also worked\ncorrectly. No issues were detected in the code or its execution.", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.016, "best_value": 0.016}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0274, "best_value": 0.0274}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9926, "best_value": 0.9926}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9917, "best_value": 0.9917}]}, {"metric_name": "validation cluster-consistency weighted accuracy", "lower_is_better": false, "description": "The cluster-consistency weighted accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9933, "best_value": 0.9933}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.6969}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6331, "best_value": 0.6331}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6966, "best_value": 0.6966}]}, {"metric_name": "test cluster-consistency weighted accuracy", "lower_is_better": false, "description": "The cluster-consistency weighted accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6955, "best_value": 0.6955}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0213, "best_value": 0.0213}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0326, "best_value": 0.0326}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.992, "best_value": 0.992}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.992, "best_value": 0.992}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.991, "best_value": 0.991}]}, {"metric_name": "validation cluster-corrected weighted accuracy", "lower_is_better": false, "description": "The cluster-corrected weighted accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.992, "best_value": 0.992}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.633, "best_value": 0.633}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test cluster-corrected weighted accuracy", "lower_is_better": false, "description": "The cluster-corrected weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, which indicates how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0263, "best_value": 0.0263}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating how well the model performs on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0346, "best_value": 0.0346}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9898, "best_value": 0.9898}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9901, "best_value": 0.9901}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9892, "best_value": 0.9892}]}, {"metric_name": "validation cluster-consistency weighted accuracy", "lower_is_better": false, "description": "The cluster-consistency weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9922, "best_value": 0.9922}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6954, "best_value": 0.6954}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6322, "best_value": 0.6322}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6951, "best_value": 0.6951}]}, {"metric_name": "test cluster-consistency weighted accuracy", "lower_is_better": false, "description": "The cluster-consistency weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7057, "best_value": 0.7057}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.337, "best_value": 0.337}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3456, "best_value": 0.3456}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8578, "best_value": 0.8578}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Measures the class-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8585, "best_value": 0.8585}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Measures the sample-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8457, "best_value": 0.8457}]}, {"metric_name": "validation CCWA", "lower_is_better": false, "description": "Measures the cumulative class-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8713, "best_value": 0.8713}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6429, "best_value": 0.6429}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "Measures the class-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5962, "best_value": 0.5962}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "Measures the sample-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6393, "best_value": 0.6393}]}, {"metric_name": "test CCWA", "lower_is_better": false, "description": "Measures the cumulative class-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6449, "best_value": 0.6449}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training, lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.015792, "best_value": 0.015792}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.027773, "best_value": 0.027773}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9938, "best_value": 0.9938}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during validation, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.994021, "best_value": 0.994021}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.993373, "best_value": 0.993373}]}, {"metric_name": "validation cluster-consistency-weighted accuracy", "lower_is_better": false, "description": "The cluster-consistency-weighted accuracy during validation, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.995558, "best_value": 0.995558}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6961, "best_value": 0.6961}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during testing, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.632577, "best_value": 0.632577}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during testing, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695754, "best_value": 0.695754}]}, {"metric_name": "test cluster-consistency-weighted accuracy", "lower_is_better": false, "description": "The cluster-consistency-weighted accuracy during testing, higher is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.71964, "best_value": 0.71964}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final loss during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1714, "best_value": 0.1714}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final loss during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1708, "best_value": 0.1708}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The best accuracy achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9492, "best_value": 0.9492}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The best color-weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9477, "best_value": 0.9477}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The best shape-weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9447, "best_value": 0.9447}]}, {"metric_name": "validation cluster-conditional weighted accuracy", "lower_is_better": false, "description": "The best cluster-conditional weighted accuracy achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.957, "best_value": 0.957}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6871, "best_value": 0.6871}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.629, "best_value": 0.629}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6858, "best_value": 0.6858}]}, {"metric_name": "test cluster-conditional weighted accuracy", "lower_is_better": false, "description": "The cluster-conditional weighted accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.687, "best_value": 0.687}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1448, "best_value": 0.1448}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1843, "best_value": 0.1843}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9458, "best_value": 0.9458}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9439, "best_value": 0.9439}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9411, "best_value": 0.9411}]}, {"metric_name": "validation cluster silhouette weighted accuracy", "lower_is_better": false, "description": "Cluster silhouette-weighted accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9531, "best_value": 0.9531}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6856, "best_value": 0.6856}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6282, "best_value": 0.6282}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6844, "best_value": 0.6844}]}, {"metric_name": "test cluster silhouette weighted accuracy", "lower_is_better": false, "description": "Cluster silhouette-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6857, "best_value": 0.6857}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures how well the model is learning during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.021, "best_value": 0.021}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model generalizes to unseen validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0448, "best_value": 0.0448}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9864, "best_value": 0.9864}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9865, "best_value": 0.9865}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9861, "best_value": 0.9861}]}, {"metric_name": "validation cluster-consistency weighted accuracy", "lower_is_better": false, "description": "Cluster-consistency weighted accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9856, "best_value": 0.9856}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6946, "best_value": 0.6946}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.632, "best_value": 0.632}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6943, "best_value": 0.6943}]}, {"metric_name": "test cluster-consistency weighted accuracy", "lower_is_better": false, "description": "Cluster-consistency weighted accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6954, "best_value": 0.6954}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0552, "best_value": 0.0552}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.078, "best_value": 0.078}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Class-wise accuracy on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9746, "best_value": 0.9746}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Sample-weighted accuracy on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9723, "best_value": 0.9723}]}, {"metric_name": "validation CCWA", "lower_is_better": false, "description": "Category-wise accuracy on the validation dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.97, "best_value": 0.97}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6866, "best_value": 0.6866}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "Class-wise accuracy on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6259, "best_value": 0.6259}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "Sample-weighted accuracy on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6855, "best_value": 0.6855}]}, {"metric_name": "test CCWA", "lower_is_better": false, "description": "Category-wise accuracy on the test dataset. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6869, "best_value": 0.6869}]}]}], "is_best_node": [false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_val_acc_curve.png", "../../logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_val_cwa_swa_curve.png", "../../logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_test_metrics.png", "../../logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_loss_curves_remove_token_feature.png", "../../logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_val_metrics_remove_token_feature.png", "../../logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_test_metrics_remove_token_feature.png"], ["../../logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_test_metrics.png", "../../logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_test_metrics.png", "../../logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_val_metrics_curve.png", "../../logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_test_metrics_bar.png"], ["../../logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_weighted_metrics.png", "../../logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_test_metrics.png", "../../logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_confusion_matrix.png"]], "plot_paths": [["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_loss_curves.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_val_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_val_acc_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_val_cwa_swa_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_test_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_val_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_loss_curves_remove_token_feature.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_val_metrics_remove_token_feature.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_test_metrics_remove_token_feature.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_val_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_test_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_loss_curves.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_validation_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_validation_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_test_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_val_metrics_curve.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_test_metrics_bar.png"], ["experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_loss_curves.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_val_accuracy.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_weighted_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_test_metrics.png", "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_confusion_matrix.png"]], "plot_analyses": [[{"analysis": "The plot shows the training and validation loss decreasing steadily over the epochs, with the training loss reducing more sharply. This indicates that the model is learning effectively and converging well. However, the validation loss stabilizes at a slightly higher level than the training loss, suggesting some potential overfitting. Despite this, the gap between the losses is small, indicating that the model generalizes reasonably well to unseen data.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot demonstrates the performance of various metrics (Accuracy, Color-Weighted Accuracy, Shape-Weighted Accuracy, and Combined Color-Weighted Accuracy) across epochs. All metrics show a consistent improvement over the epochs and stabilize at high values, indicating that the model performs well on the validation set. The close alignment of the metrics suggests that the model performs uniformly across different evaluation criteria.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_val_metrics.png"}, {"analysis": "The bar chart summarizes the test metrics, showing an accuracy of 70%, which matches the SOTA benchmark. However, the Color-Weighted Accuracy (CWA) is slightly lower at 63%, falling short of the SOTA benchmark of 70%. Shape-Weighted Accuracy (SWA) and Combined Color-Weighted Accuracy (CCWA) both reach 70%, which is on par with the SOTA. This indicates that while the model performs well on shape-related tasks, there is room for improvement in handling color-related complexities.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_7c8c9cc926b14c96968d28e479b6cc03_proc_1653756/SPR_BENCH_test_metrics.png"}], [{"analysis": "The plot shows that both the training and validation loss decrease steadily over epochs, with the validation loss closely following the training loss. This indicates that the model generalizes well to unseen data without overfitting. By the end of the training, both losses converge to very low values, suggesting effective learning and optimization.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_loss_curve.png"}, {"analysis": "The validation accuracy improves consistently over epochs, demonstrating that the model's performance on unseen data improves with training. The accuracy reaches above 99%, indicating strong model performance and high alignment with the validation set.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_val_acc_curve.png"}, {"analysis": "The weighted accuracy metrics (CWA and SWA) show a steady increase over epochs, closely tracking each other. This suggests that the model is performing well across both color and shape-based weighted evaluations. The near-identical trends for CWA and SWA imply balanced performance across these metrics.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_val_cwa_swa_curve.png"}, {"analysis": "The test metrics indicate that the model achieves high accuracy across all evaluation metrics. While the exact values for Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) are slightly below the overall accuracy (ACC), they remain competitive and indicate strong performance. The Combined Color-Weighted Accuracy (CCWA) is also high, reinforcing the model's generalization capabilities.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_test_metrics.png"}, {"analysis": "The confusion matrix reveals class-wise performance. The high values along the diagonal indicate that the model correctly predicts the majority of the samples. However, there are some misclassifications, as shown by the off-diagonal values. The balance between true positives and false positives/negatives suggests opportunities for further optimization in specific cases.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_4cdddbe6194f4bc4a6f325cb11da9fb4_proc_1691878/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the cross-entropy loss for both training and validation sets over 5 epochs. The training loss decreases steadily and converges to a very low value, indicating effective learning. The validation loss follows a similar trend and remains closely aligned with the training loss, suggesting no overfitting and good generalization. The rapid decrease in loss within the first two epochs highlights efficient learning dynamics early in training.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot depicts the progression of various validation metrics (ACC, CWA, SWA, CCWA) over 5 epochs. All metrics show consistent improvement and converge to high values, with CCWA slightly outperforming the others. This indicates that the model not only achieves high overall accuracy but also performs well on color- and shape-weighted evaluations. The steady increase in scores suggests that the clustering-based approach contributes positively to the model's reasoning capabilities.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_val_metrics.png"}, {"analysis": "The bar chart compares performance metrics (ACC, CWA, SWA, CCWA) on the test set. While ACC, SWA, and CCWA achieve around 70-71%, CWA lags slightly at 63%. This discrepancy indicates that the model struggles more with color-weighted reasoning compared to other metrics. However, the overall performance surpasses the stated benchmarks for SWA and approaches the SOTA for CWA, affirming the effectiveness of the proposed approach.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_092512c7cbf549629232fe4003797d13_proc_1691879/SPR_BENCH_test_metrics.png"}], [{"analysis": "The loss curves indicate consistent convergence for both training and validation sets. The training loss decreases steadily across epochs, with no signs of overfitting as the validation loss follows a similar trend. This suggests that the model is learning effectively and generalizing well to the validation data.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_loss_curves_remove_token_feature.png"}, {"analysis": "The validation metrics (ACC, CWA, SWA, CCWA) show a steady improvement across epochs. The Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) scores improve consistently, which aligns with the hypothesis that removing the token feature aids in better glyph clustering and reasoning. The Composite Color-Weighted Accuracy (CCWA) metric achieves the highest score, indicating a balanced performance across multiple aspects of the task.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_val_metrics_remove_token_feature.png"}, {"analysis": "The test metrics show relatively uniform performance across all evaluated metrics (ACC, CWA, SWA, CCWA), with scores around 0.6 to 0.65. While these scores are below the validation metrics, they still indicate a reasonable level of generalization. However, there may be room for improvement to close the gap between validation and test performance.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fdb79f2c205a4224b439cb5c4df5067c_proc_1691881/SPR_BENCH_test_metrics_remove_token_feature.png"}], [{"analysis": "This plot shows the training and validation loss trends over epochs. Both losses decrease consistently, indicating effective learning by the model. The validation loss closely follows the training loss, suggesting minimal overfitting. The convergence after epoch 3 implies the model has stabilized and further training may not yield significant improvements.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot compares various validation metrics (ACC, CWA, SWA, CCWA) across epochs. All metrics improve steadily, with CCWA achieving the highest score. The close alignment of ACC, CWA, and SWA indicates balanced performance across different evaluation criteria. The upward trend suggests the model is learning effectively and generalizing well to the validation set.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart summarizes test metrics (ACC, CWA, SWA, CCWA). While ACC and SWA reach 0.70, CWA lags slightly at 0.63, indicating the model struggles more with color-weighted accuracy. CCWA is the highest at 0.72, reflecting strong overall performance when considering combined criteria. These results suggest room for improvement in color-specific tasks.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_test_metrics.png"}, {"analysis": "This confusion matrix visualizes the model's predictions against ground truth labels. The high density along the diagonal indicates accurate predictions for both classes. However, some misclassifications are visible, suggesting potential areas for improvement in handling certain edge cases or ambiguous sequences.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b301fa4fca314b3d874b295d1dd1f751_proc_1691880/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the cross-entropy loss for both the training and validation datasets over five epochs. The training loss decreases steeply in the first two epochs and then gradually flattens out, indicating that the model is learning effectively. The validation loss follows a similar trend, suggesting that the model generalizes well to unseen data. The convergence of both curves towards a low loss value indicates stability and no signs of overfitting.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the progression of various validation metrics (ACC, CWA, SWA, and CCWA) over five epochs. All metrics show an increasing trend, with CCWA achieving the highest score, followed by ACC, CWA, and SWA. This indicates that the model is improving in accuracy and weighted performance metrics as training progresses. The consistent improvement across all metrics suggests that the model is successfully learning the underlying rules and patterns in the data.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_validation_metrics.png"}, {"analysis": "The confusion matrix illustrates the model's performance on the test set. It shows a true positive rate of 74% and a true negative rate of 64%. However, there is a notable false positive rate (36%) and false negative rate (26%), indicating areas where the model could improve. While the diagonal dominance suggests the model is reasonably effective at distinguishing between the two classes, the off-diagonal values highlight the need for further optimization to reduce misclassifications.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_fb7c7069c061416fa2dd060dad24e11e_proc_1691879/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curve shows a steady decrease in training loss over the epochs, which indicates that the model is learning effectively. The validation loss decreases initially but plateaus after a few epochs, suggesting that the model's performance on unseen data stabilizes. The lack of overfitting is evident, as there is no divergence between training and validation losses.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_loss_curve.png"}, {"analysis": "The validation metrics plot illustrates that all metrics (ACC, CWA, SWA, CCWA) improve over the epochs. While there is some fluctuation in the curves, particularly for CCWA, the overall trend is upward, demonstrating that the model's performance on validation data improves consistently across different evaluation criteria.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_validation_metrics.png"}, {"analysis": "The test metrics bar chart reveals that the model achieves comparable performance across all metrics (ACC, CWA, SWA, CCWA). However, CWA appears to be slightly lower than the others, indicating that the model might be less effective at capturing color-weighted patterns compared to other features.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_test_metrics.png"}, {"analysis": "The confusion matrix indicates that the model performs reasonably well in distinguishing between the two classes. However, there is a noticeable number of misclassifications in both directions, with slightly more false positives than false negatives. This suggests room for improvement in reducing classification errors and enhancing the model's discriminative ability.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b8926c4f7ec3486680728520c5787b70_proc_1691878/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over five epochs. The training loss decreases sharply and stabilizes, indicating effective learning of the training data. The validation loss also decreases initially but shows a slight increase at the last epoch, which may indicate the onset of overfitting. To mitigate this, early stopping or regularization techniques could be considered.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot illustrates the validation metrics (ACC, CWA, SWA, CCWA) across epochs. All metrics show consistent improvement and converge to high values, suggesting that the model generalizes well to the validation set. The close proximity of the metrics indicates balanced performance across different evaluation criteria.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_val_metrics_curve.png"}, {"analysis": "This plot presents the test metrics for ACC, CWA, SWA, and CCWA. The model achieves 0.69 for ACC and SWA, 0.63 for CWA, and 0.70 for CCWA. While the results are promising, the CWA metric is slightly lower, indicating that the model may struggle more with color-weighted aspects. Further tuning or adjustments to the clustering algorithm may improve this metric.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_9eab43b2d7b34868a2ad4b8d6ae3d839_proc_1691881/SPR_BENCH_test_metrics_bar.png"}], [{"analysis": "The plot shows the training and validation loss decreasing steadily over epochs, which indicates that the model is learning effectively. The training loss decreases slightly faster than the validation loss, which could suggest a good generalization capability. The convergence of both curves at later epochs suggests that the model is not overfitting.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation accuracy improves consistently over epochs, reaching a value close to 0.98. This indicates that the model is learning to generalize well to unseen data, and the performance is nearing saturation, suggesting that further training may not yield significant improvements.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_val_accuracy.png"}, {"analysis": "The validation metrics (CWA, SWA, and CCWA) show a similar upward trend, with CWA slightly outperforming the other metrics. This indicates that the model is better at capturing color-weighted patterns compared to shape-weighted ones. The close alignment of the metrics also suggests that the model is robust across different evaluation criteria.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_weighted_metrics.png"}, {"analysis": "The test metrics show a slight drop compared to validation metrics, with an accuracy of 0.69 and CWA at 0.63. This drop might indicate a slight overfitting to the validation set or inherent difficulty in the test set. However, the performance is still close to the SOTA benchmarks, suggesting that the model is competitive.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_test_metrics.png"}, {"analysis": "The confusion matrix shows a relatively balanced performance across the two classes, with a slight bias towards one class. The number of false positives and false negatives is moderate, suggesting room for improvement in the decision boundary or feature representation.", "plot_path": "experiments/2025-08-31_03-13-33_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_479284fee8714e2a94499310cdd4833b_proc_1691880/SPR_BENCH_confusion_matrix.png"}]], "vlm_feedback_summary": ["The plots indicate that the model shows strong performance overall, achieving\nSOTA-level results in most metrics except for Color-Weighted Accuracy (CWA),\nwhich falls slightly short. The training and validation losses suggest good\nconvergence with minimal overfitting, and the consistent improvement across\nvalidation metrics highlights the robustness of the approach. However,\nimprovements in handling color-related features could further enhance the\nmodel's performance.", "The plots collectively demonstrate that the proposed symbolic glyph clustering\napproach is effective. The decreasing loss curves, increasing validation\naccuracy, and high test metrics validate the hypothesis. The confusion matrix\nhighlights areas where further optimization can enhance performance.", "The experimental results demonstrate effective learning, with rapid convergence\nof losses and high validation metrics. The clustering-based approach\nsignificantly enhances performance, particularly in SWA and CCWA, while CWA\nremains a challenge.", "The plots collectively indicate that the model is learning effectively, with\nconsistent improvement in validation metrics and reasonable generalization on\nthe test set. The removal of the token feature appears to positively impact the\nmodel's ability to cluster symbolic glyphs and reason about patterns.", "The plots demonstrate consistent training and validation trends, effective\ngeneralization, and strong performance across various metrics. However, there is\nroom for improvement in color-weighted accuracy and specific edge cases.", "The plots collectively indicate that the model is learning effectively and\ngeneralizing well, as evidenced by decreasing loss curves and improving\nvalidation metrics. However, the confusion matrix reveals some room for\nimprovement in reducing misclassification rates. Overall, the results are\npromising and align with the research objectives of enhancing model performance\nin symbolic pattern recognition.", "The plots collectively indicate that the model is learning effectively and\ngeneralizing well, as evidenced by the consistent improvement in validation\nmetrics and stable test performance. While the loss curve shows no signs of\noverfitting, the confusion matrix highlights areas for improvement in\nclassification accuracy. Overall, the results are promising, but further\noptimization is needed to enhance performance on specific metrics like CWA.", "The plots indicate effective training with minor overfitting concerns, strong\ngeneralization to validation data, and promising test results. However, the\nslightly lower CWA metric suggests room for improvement in handling color-\nweighted aspects.", "The plots indicate effective learning with steady loss reduction and high\nvalidation accuracy nearing saturation. The validation metrics confirm\nrobustness, though test metrics show a slight drop, possibly due to overfitting\nor test set complexity. The confusion matrix reveals balanced performance but\nhighlights areas for improvement in classification."], "exec_time": [13.772527694702148, 9.809523344039917, 7.244777202606201, 7.9106199741363525, 9.223686218261719, 8.548150539398193, 7.256197214126587, 7.63222336769104, 10.912141561508179], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['dataset_name_placeholder']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['ds_key']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- Load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    ds_names = list(experiment_data.keys())\n    test_accs = {}\n\n    for ds in ds_names:\n        ds_data = experiment_data.get(ds, {})\n        losses = ds_data.get(\"losses\", {})\n        metrics_val = ds_data.get(\"metrics\", {}).get(\"val\", [])\n        test_metrics = ds_data.get(\"metrics\", {}).get(\"test\", {})\n\n        epochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n        # ---- 1. Loss curves ----\n        try:\n            plt.figure(figsize=(6, 4))\n            if losses.get(\"train\"):\n                plt.plot(epochs, losses[\"train\"], label=\"train\")\n            if losses.get(\"val\"):\n                plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=\"val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds} \u2014 Train vs Val Loss\\n(Left: train, Right: val)\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve plot for {ds}: {e}\")\n            plt.close()\n\n        # ---- 2. Validation metric curves ----\n        try:\n            if metrics_val:\n                plt.figure(figsize=(6, 4))\n                accs = [m.get(\"acc\") for m in metrics_val]\n                cwas = [m.get(\"cwa\") for m in metrics_val]\n                swas = [m.get(\"swa\") for m in metrics_val]\n                ccwas = [m.get(\"ccwa\") for m in metrics_val]\n                for arr, lbl in zip(\n                    [accs, cwas, swas, ccwas], [\"ACC\", \"CWA\", \"SWA\", \"CCWA\"]\n                ):\n                    if any(a is not None for a in arr):\n                        plt.plot(epochs, arr, label=lbl)\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Score\")\n                plt.ylim(0, 1)\n                plt.title(f\"{ds} \u2014 Validation Metrics Across Epochs\")\n                plt.legend()\n                fname = os.path.join(working_dir, f\"{ds}_val_metrics.png\")\n                plt.savefig(fname)\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating val metric plot for {ds}: {e}\")\n            plt.close()\n\n        # ---- 3. Test metric bar chart ----\n        try:\n            if test_metrics:\n                plt.figure(figsize=(6, 4))\n                metric_names = [\"acc\", \"cwa\", \"swa\", \"ccwa\"]\n                values = [test_metrics.get(m, np.nan) for m in metric_names]\n                plt.bar(metric_names, values, color=\"skyblue\")\n                plt.ylim(0, 1)\n                plt.title(f\"{ds} \u2014 Test Metrics Summary\")\n                for i, v in enumerate(values):\n                    if not np.isnan(v):\n                        plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n                fname = os.path.join(working_dir, f\"{ds}_test_metrics.png\")\n                plt.savefig(fname)\n                plt.close()\n            test_accs[ds] = test_metrics.get(\"acc\", np.nan)\n        except Exception as e:\n            print(f\"Error creating test metric plot for {ds}: {e}\")\n            plt.close()\n\n        # ---- Print metrics ----\n        if test_metrics:\n            print(f\"\\n{ds} TEST METRICS:\")\n            for k, v in test_metrics.items():\n                print(f\"  {k.upper():5s}: {v:.3f}\")\n\n    # ---- 4. Inter-dataset comparison ----\n    if len(test_accs) > 1:\n        try:\n            plt.figure(figsize=(6, 4))\n            names = list(test_accs.keys())\n            vals = [test_accs[n] for n in names]\n            plt.bar(names, vals, color=\"lightgreen\")\n            plt.ylim(0, 1)\n            plt.title(\"Test Accuracy Comparison Across Datasets\")\n            for i, v in enumerate(vals):\n                if not np.isnan(v):\n                    plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n            fname = os.path.join(working_dir, \"dataset_test_accuracy_comparison.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating cross-dataset plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ load data ------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\ned = experiment_data[\"RemoveClusterFeat\"][\"SPR_BENCH\"]\nloss_tr = np.asarray(ed[\"losses\"][\"train\"])\nloss_val = np.asarray(ed[\"losses\"][\"val\"])\nval_metrics = ed[\"metrics\"][\"val\"]  # list of dicts\nepochs = np.arange(1, len(loss_tr) + 1)\n\nval_acc = np.array([m[\"acc\"] for m in val_metrics])\nval_cwa = np.array([m[\"cwa\"] for m in val_metrics])\nval_swa = np.array([m[\"swa\"] for m in val_metrics])\nval_ccwa = np.array([m[\"ccwa\"] for m in val_metrics])\n\ntest_metrics = ed[\"metrics\"][\"test\"]\ny_true = np.asarray(ed[\"ground_truth\"])\ny_pred = np.asarray(ed[\"predictions\"])\nnum_labels = len(np.unique(y_true))\n\n# ------------------ plot 1: loss curves ------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_tr, label=\"Train Loss\")\n    plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH \u2013 Train vs. Val Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------ plot 2: validation accuracy ------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, val_acc, marker=\"o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH \u2013 Validation Accuracy over Epochs\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_acc_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val accuracy plot: {e}\")\n    plt.close()\n\n# ------------------ plot 3: validation CWA & SWA ------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, val_cwa, marker=\"s\", label=\"CWA\")\n    plt.plot(epochs, val_swa, marker=\"^\", label=\"SWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Weighted Accuracy\")\n    plt.title(\"SPR_BENCH \u2013 CWA & SWA on Validation Set\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_cwa_swa_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating weighted accuracy plot: {e}\")\n    plt.close()\n\n# ------------------ plot 4: test metrics bar plot ------------------\ntry:\n    plt.figure()\n    names = [\"ACC\", \"CWA\", \"SWA\", \"CCWA\"]\n    values = [\n        test_metrics[\"acc\"],\n        test_metrics[\"cwa\"],\n        test_metrics[\"swa\"],\n        test_metrics[\"ccwa\"],\n    ]\n    plt.bar(names, values, color=[\"steelblue\", \"orange\", \"green\", \"red\"])\n    plt.ylim(0, 1)\n    plt.title(\"SPR_BENCH \u2013 Test Metrics\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metrics bar plot: {e}\")\n    plt.close()\n\n# ------------------ plot 5: confusion matrix ------------------\ntry:\n    cm = np.zeros((num_labels, num_labels), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im, fraction=0.046, pad=0.04)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\n        \"SPR_BENCH \u2013 Confusion Matrix\\nLeft: Ground Truth, Right: Generated Predictions\"\n    )\n    for i in range(num_labels):\n        for j in range(num_labels):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ------------------ print final test metrics ------------------\nprint(\"TEST RESULTS\")\nfor k, v in test_metrics.items():\n    print(f\"{k.upper():>5}: {v:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# -------- load data --------\ntry:\n    edict = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    run = edict[\"RandomClusterAssignment\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    run = None\n\nif run is not None:\n    epochs = range(1, len(run[\"losses\"][\"train\"]) + 1)\n\n    # -------- 1) Loss curve --------\n    try:\n        plt.figure()\n        plt.plot(epochs, run[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, run[\"losses\"][\"val\"], label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss vs. Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # -------- 2) Validation metrics over epochs --------\n    try:\n        val_metrics = run[\"metrics\"][\"val\"]\n        keys = [\"acc\", \"cwa\", \"swa\", \"ccwa\"]\n        for k in keys:\n            plt.plot(epochs, [m[k] for m in val_metrics], label=k.upper())\n        plt.title(\"SPR_BENCH Validation Metrics over Epochs\\n(ACC, CWA, SWA, CCWA)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_metrics.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metric plot: {e}\")\n        plt.close()\n\n    # -------- 3) Test set summary --------\n    try:\n        test = run[\"metrics\"][\"test\"]\n        plt.figure()\n        bars = plt.bar(list(test.keys()), list(test.values()), color=\"skyblue\")\n        for b in bars:\n            plt.text(\n                b.get_x() + b.get_width() / 2,\n                b.get_height(),\n                f\"{b.get_height():.2f}\",\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=8,\n            )\n        plt.title(\"SPR_BENCH Test Metrics (ACC / CWA / SWA / CCWA)\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------- prepare paths & load data --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper to safely drill into dict\ndef get_run(edict, exp, dataset):\n    return edict.get(exp, {}).get(dataset, {}) if edict else {}\n\n\nrun = get_run(experiment_data, \"remove_token_feature\", \"SPR_BENCH\")\nlosses = run.get(\"losses\", {})\nmetrics_val = run.get(\"metrics\", {}).get(\"val\", [])\nmetrics_test = run.get(\"metrics\", {}).get(\"test\", {})\n\n# -------- plot 1: loss curves --------\ntry:\n    epochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train Loss\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curves (remove_token_feature)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_remove_token_feature.png\")\n    plt.savefig(fname)\n    print(\"Saved:\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# -------- plot 2: validation metrics over epochs --------\ntry:\n    if metrics_val:\n        epochs = [m[\"epoch\"] for m in metrics_val]\n        for key in [\"acc\", \"cwa\", \"swa\", \"ccwa\"]:\n            plt.plot(epochs, [m[key] for m in metrics_val], label=key.upper())\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH \u2013 Validation Metrics (remove_token_feature)\")\n        plt.legend()\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_val_metrics_remove_token_feature.png\"\n        )\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metrics plot: {e}\")\n    plt.close()\n\n# -------- plot 3: test metrics bar chart --------\ntry:\n    if metrics_test:\n        keys = [\"acc\", \"cwa\", \"swa\", \"ccwa\"]\n        values = [metrics_test.get(k, 0) for k in keys]\n        plt.figure()\n        plt.bar(keys, values, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH \u2013 Test Metrics (remove_token_feature)\")\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_test_metrics_remove_token_feature.png\"\n        )\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metrics plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"ORD_EMB_CLUSTER_ABLATION\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nsaved_plots = []\n\nif exp:\n    # ---------- pre-extract common arrays ----------\n    tr_loss = np.asarray(exp[\"losses\"][\"train\"], dtype=float)\n    val_loss = np.asarray(exp[\"losses\"][\"val\"], dtype=float)\n    val_metrics = exp[\"metrics\"][\"val\"]\n    epochs = np.arange(1, len(tr_loss) + 1)\n\n    # prepare val metric arrays if they exist\n    def get_metric(m):\n        return np.asarray([d.get(m, np.nan) for d in val_metrics], dtype=float)\n\n    acc, cwa, swa, ccwa = map(get_metric, [\"acc\", \"cwa\", \"swa\", \"ccwa\"])\n\n    # ---------- Plot 1: loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        saved_plots.append(fname)\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n    finally:\n        plt.close()\n\n    # ---------- Plot 2: validation metrics over epochs ----------\n    try:\n        plt.figure()\n        for arr, lab in zip([acc, cwa, swa, ccwa], [\"ACC\", \"CWA\", \"SWA\", \"CCWA\"]):\n            if not np.all(np.isnan(arr)):\n                plt.plot(epochs, arr, label=lab)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH: Validation Metrics across Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_metrics.png\")\n        plt.savefig(fname)\n        saved_plots.append(fname)\n    except Exception as e:\n        print(f\"Error creating val metrics plot: {e}\")\n    finally:\n        plt.close()\n\n    # ---------- Plot 3: test metrics bar chart ----------\n    try:\n        test_m = exp[\"metrics\"][\"test\"]\n        labels = list(test_m.keys())\n        values = [test_m[k] for k in labels]\n        plt.figure()\n        plt.bar(labels, values, color=\"skyblue\")\n        plt.ylim(0, 1)\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.title(\"SPR_BENCH: Test Metrics Summary\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n        saved_plots.append(fname)\n    except Exception as e:\n        print(f\"Error creating test metrics bar: {e}\")\n    finally:\n        plt.close()\n\n    # ---------- Plot 4: confusion matrix ----------\n    try:\n        gt = np.asarray(exp[\"ground_truth\"]).ravel()\n        pr = np.asarray(exp[\"predictions\"]).ravel()\n        if gt.size and pr.size and gt.shape == pr.shape:\n            num_classes = int(max(gt.max(), pr.max())) + 1\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for g, p in zip(gt, pr):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\"SPR_BENCH: Confusion Matrix\\nLeft: GT, Right: Predicted\")\n            plt.xticks(range(num_classes))\n            plt.yticks(range(num_classes))\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            saved_plots.append(fname)\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n    finally:\n        plt.close()\n\nprint(\"Saved plots:\", saved_plots)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns  # lightweight; if unavailable we fallback to plt.imshow\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    ablation_key = next(iter(experiment_data))\n    ds_key = next(iter(experiment_data[ablation_key]))\n    ed = experiment_data[ablation_key][ds_key]\n\n    # helper\n    epochs = np.arange(1, len(ed[\"losses\"][\"train\"]) + 1)\n\n    # ------------- Plot 1: loss curves -------------\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, ed[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{ds_key} Loss Curves\\nLeft: Train, Right: Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_key}_loss_curves.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ------------- Plot 2: validation metrics -------------\n    try:\n        metrics_per_epoch = ed[\"metrics\"][\"val\"]\n        if metrics_per_epoch:\n            acc = [m[\"acc\"] for m in metrics_per_epoch]\n            cwa = [m[\"cwa\"] for m in metrics_per_epoch]\n            swa = [m[\"swa\"] for m in metrics_per_epoch]\n            ccwa = [m[\"ccwa\"] for m in metrics_per_epoch]\n            plt.figure()\n            plt.plot(epochs, acc, label=\"ACC\")\n            plt.plot(epochs, cwa, label=\"CWA\")\n            plt.plot(epochs, swa, label=\"SWA\")\n            plt.plot(epochs, ccwa, label=\"CCWA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(\n                f\"{ds_key} Validation Metrics Over Epochs\\nLeft: ACC, Right: Weighted Accuracies\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_key}_validation_metrics.png\")\n            plt.savefig(fname)\n            print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot: {e}\")\n        plt.close()\n\n    # ------------- Plot 3: confusion matrix -------------\n    try:\n        y_true = np.array(ed[\"ground_truth\"])\n        y_pred = np.array(ed[\"predictions\"])\n        if y_true.size and y_pred.size:\n            cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n            plt.figure()\n            try:\n                sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n            except Exception:\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_key} Test Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            fname = os.path.join(working_dir, f\"{ds_key}_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(\"Saved\", fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# pick the only ablation/dataset we have\nabl = \"TokenOrderShuffle\"\ndset = \"SPR_BENCH\"\nd = experiment_data.get(abl, {}).get(dset, {})\n\nloss_train = d.get(\"losses\", {}).get(\"train\", [])\nloss_val = d.get(\"losses\", {}).get(\"val\", [])\nmetrics_val = d.get(\"metrics\", {}).get(\"val\", [])\nmetrics_test = d.get(\"metrics\", {}).get(\"test\", {})\npreds = np.array(d.get(\"predictions\", []))\ngts = np.array(d.get(\"ground_truth\", []))\n\nepochs = list(range(1, len(loss_train) + 1))\n\n# -------- 1. loss curve --------\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH Loss Curve\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# -------- 2. validation metric curves --------\ntry:\n    if metrics_val:\n        acc = [m[\"acc\"] for m in metrics_val]\n        cwa = [m[\"cwa\"] for m in metrics_val]\n        swa = [m[\"swa\"] for m in metrics_val]\n        ccwa = [m[\"ccwa\"] for m in metrics_val]\n        plt.figure()\n        plt.plot(epochs, acc, label=\"ACC\")\n        plt.plot(epochs, cwa, label=\"CWA\")\n        plt.plot(epochs, swa, label=\"SWA\")\n        plt.plot(epochs, ccwa, label=\"CCWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH Validation Metrics\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_validation_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metrics plot: {e}\")\n    plt.close()\n\n# -------- 3. test metric bar chart --------\ntry:\n    if metrics_test:\n        names = [\"ACC\", \"CWA\", \"SWA\", \"CCWA\"]\n        vals = [metrics_test.get(k.lower(), 0) for k in names]\n        plt.figure()\n        plt.bar(names, vals, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH Test Metrics\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metrics bar chart: {e}\")\n    plt.close()\n\n# -------- 4. confusion matrix --------\ntry:\n    if preds.size and gts.size and preds.shape == gts.shape:\n        n_cls = len(np.unique(np.concatenate([preds, gts])))\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH Confusion Matrix\")\n        for i in range(n_cls):\n            for j in range(n_cls):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\nprint(\"Plotting complete. Files saved to\", working_dir)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# shortcut to dataset dict\nexp = experiment_data.get(\"TwoClusterGranularity\", {}).get(\"SPR_BENCH\", {})\n\nlosses = exp.get(\"losses\", {})\nmetrics_val = exp.get(\"metrics\", {}).get(\"val\", [])\nmetrics_test = exp.get(\"metrics\", {}).get(\"test\", {})\n\n# ----- Plot 1: loss curves ----------------------------------------------------\ntry:\n    epochs = range(1, len(losses.get(\"train\", [])) + 1)\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train Loss\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ----- Plot 2: validation metrics --------------------------------------------\ntry:\n    if metrics_val:\n        epochs = [m[\"epoch\"] for m in metrics_val]\n        acc = [m[\"acc\"] for m in metrics_val]\n        cwa = [m[\"cwa\"] for m in metrics_val]\n        swa = [m[\"swa\"] for m in metrics_val]\n        ccwa = [m[\"ccwa\"] for m in metrics_val]\n\n        plt.figure()\n        plt.plot(epochs, acc, label=\"ACC\")\n        plt.plot(epochs, cwa, label=\"CWA\")\n        plt.plot(epochs, swa, label=\"SWA\")\n        plt.plot(epochs, ccwa, label=\"CCWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH: Validation Metrics Across Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_metrics_curve.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric curve: {e}\")\n    plt.close()\n\n# ----- Plot 3: test metrics ---------------------------------------------------\ntry:\n    if metrics_test:\n        names = [\"ACC\", \"CWA\", \"SWA\", \"CCWA\"]\n        values = [\n            metrics_test.get(\"acc\", 0),\n            metrics_test.get(\"cwa\", 0),\n            metrics_test.get(\"swa\", 0),\n            metrics_test.get(\"ccwa\", 0),\n        ]\n        plt.figure()\n        plt.bar(names, values, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH: Test Metrics\")\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_bar.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metric bar chart: {e}\")\n    plt.close()\n\n# ----- Print test metrics -----------------------------------------------------\nif metrics_test:\n    print(\"FINAL TEST METRICS:\")\n    for k, v in metrics_test.items():\n        print(f\"  {k.upper():4s}: {v:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# convenience getter\ndef get_ed():\n    try:\n        return experiment_data[\"BinaryCountAblation\"][\"SPR_BENCH\"]\n    except KeyError:\n        return {}\n\n\ned = get_ed()\n\n# 1) Loss curves ---------------------------------------------------------------\ntry:\n    tr_loss = np.asarray(ed[\"losses\"][\"train\"])\n    vl_loss = np.asarray(ed[\"losses\"][\"val\"])\n    if tr_loss.size and vl_loss.size:\n        epochs = np.arange(1, len(tr_loss) + 1)\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, vl_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting loss curves: {e}\")\n    plt.close()\n\n# 2) Validation accuracy -------------------------------------------------------\ntry:\n    val_m = ed[\"metrics\"][\"val\"]\n    if val_m:\n        acc = np.array([m[\"acc\"] for m in val_m])\n        epochs = np.arange(1, len(acc) + 1)\n        plt.figure()\n        plt.plot(epochs, acc, marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH \u2013 Validation Accuracy\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_accuracy.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting validation accuracy: {e}\")\n    plt.close()\n\n# 3) Weighted metrics curves ---------------------------------------------------\ntry:\n    if val_m:\n        cwa = [m[\"cwa\"] for m in val_m]\n        swa = [m[\"swa\"] for m in val_m]\n        ccwa = [m[\"ccwa\"] for m in val_m]\n        epochs = np.arange(1, len(cwa) + 1)\n        plt.figure()\n        plt.plot(epochs, cwa, label=\"CWA\")\n        plt.plot(epochs, swa, label=\"SWA\")\n        plt.plot(epochs, ccwa, label=\"CCWA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH \u2013 Validation Weighted Metrics\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_weighted_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting weighted metrics: {e}\")\n    plt.close()\n\n# 4) Test metrics bar chart ----------------------------------------------------\ntry:\n    tst = ed.get(\"metrics\", {}).get(\"test\", {})\n    if tst:\n        names = [\"ACC\", \"CWA\", \"SWA\", \"CCWA\"]\n        vals = [tst.get(k.lower(), np.nan) for k in names]\n        plt.figure()\n        plt.bar(names, vals, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH \u2013 Test Metrics\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting test metrics: {e}\")\n    plt.close()\n\n# 5) Confusion matrix ----------------------------------------------------------\ntry:\n    gt = np.asarray(ed.get(\"ground_truth\", []))\n    pr = np.asarray(ed.get(\"predictions\", []))\n    if gt.size and pr.size and gt.shape == pr.shape:\n        n_cls = int(max(gt.max(), pr.max()) + 1)\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for g, p in zip(gt, pr):\n            cm[g, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"SPR_BENCH \u2013 Confusion Matrix\")\n        for i in range(n_cls):\n            for j in range(n_cls):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                )\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting confusion matrix: {e}\")\n    plt.close()\n"], "plot_plan": ["The script loads the saved experiment_data.npy, identifies every dataset present, and for each dataset it extracts training/validation loss arrays, per-epoch validation metrics, and the final test metrics.  It then generates three standard visualizations per dataset: (1) a train-vs-val loss curve, (2) per-epoch validation metric curves (ACC, CWA, SWA, CCWA) plotted together, and (3) a bar chart summarizing the four test metrics.  Each plot is wrapped in its own try-except block, saved to the working directory with descriptive names, and the figure is closed afterwards.  After the per-dataset plots, the script checks whether more than one dataset exists; if so, it adds an extra comparison bar chart of test accuracies across datasets.  All plots include clear titles that mention the dataset and the subplot purpose, and filenames always reflect the dataset name and plot type.  The code respects the guideline of using only data present in experiment_data.npy and never fabricates values.  It finishes by printing the test metrics for quick inspection.  The entire routine is encapsulated in one concise, self-contained script ready to run in the working environment.", "We will first load the saved experiment_data.npy, extract the relevant arrays (training/validation losses, per-epoch validation metrics, final test metrics and prediction/label arrays) and convert them into simple NumPy vectors.  Five independent try-except blocks will be used to generate: (1) a train vs. validation loss curve, (2) validation accuracy curve, (3) validation CWA & SWA comparison, (4) a grouped bar plot of the final test metrics (ACC, CWA, SWA, CCWA), and (5) a confusion-matrix heat-map for the test split.  Each figure will have a descriptive title that mentions \u201cSPR_BENCH\u201d and the plot type, and will be saved into working_dir with an informative file name.  Figures are always closed in finally/except clauses to avoid memory leaks.  When plotting multiple metrics that share the same epoch axis we limit ourselves to the five training epochs already present, so no extra sub-sampling is needed.  After plotting, the script prints the numeric test metrics for quick inspection.  All plotting relies solely on the information stored inside experiment_data.npy; no synthetic data are fabricated.  The code is concise, imports the minimum required libraries, and is fully self-contained.  Finally, if loading the experiment data fails, a clear error is reported and no plotting is attempted.", "We first load experiment_data.npy from the working directory and extract the sub-dictionary for the RandomClusterAssignment / SPR_BENCH run. From this we pull epoch-wise training and validation losses, plus the validation metrics (overall accuracy, CWA, SWA, CCWA). We then build three simple visualizations: \u2460 a line plot comparing training vs. validation loss across epochs, \u2461 a multi-line plot showing the four validation metrics over epochs, and \u2462 a bar chart summarising the final test-set metrics. Each plot is wrapped in its own try-except block as required, saved with clear filenames that include the dataset name and the plot type, given informative titles/subtitles, and the figure is closed after saving to avoid memory leaks. No synthetic or missing values are generated\u2014only keys found in experiment_data are used. At most five figures are produced (here only three). All plots are written to the existing working directory. Finally, the script prints the saved filenames for convenience.", "We will load experiment_data.npy, locate the single run remove_token_feature\u2006\u2192\u2006SPR_BENCH, and extract the epoch-level losses and validation metrics plus the final test metrics. Three figures will be produced: (1) training vs. validation loss curves over epochs; (2) validation ACC, CWA, SWA and CCWA curves over epochs; (3) a bar chart summarising the same four metrics on the test split. Each figure is wrapped in a try-except block, saved into the predefined working_dir with descriptive filenames, titled clearly and then closed. We respect the five-figure limit by producing only these three plots. The code begins with the mandated imports, recreates working_dir if necessary, and prints out the filenames of the saved plots so users can locate them. All plotting uses basic matplotlib without styling, and no synthetic data are generated\u2014only what exists in the loaded experiment_data structure. Finally the script can be executed standalone from the repository root.", "We will load the stored experiment_data.npy, navigate to the ORD_EMB_CLUSTER_ABLATION \u2192 SPR_BENCH branch, and pull out the per-epoch lists for train/val loss and the list of validation-metric dictionaries. From these we create epoch indices, assemble NumPy arrays for loss curves, and build separate arrays for ACC, CWA, SWA and CCWA. Four figures will be produced: (1) train vs. validation loss across epochs, (2) each validation metric across epochs, (3) a bar chart summarising the final test metrics, and (4) a confusion-matrix style heat-map comparing ground-truth vs. predictions. Each plotting block is wrapped in its own try-except, figures are titled clearly, saved to working_dir with descriptive names, and always closed afterward. We guard against any missing keys by using .get with defaults to prevent crashes. Only data present in experiment_data.npy are used; no synthetic points are generated. Finally, the script prints the paths of all successfully saved figures so users know where to look.", "We first load the saved experiment_data.npy and identify the single ablation/dataset pair present. From this we extract (1) the epoch-wise train/validation losses, (2) the epoch-wise validation metrics (ACC, CWA, SWA, CCWA) and (3) the final test metrics together with predicted and ground-truth labels. Three standard visualizations are produced: a loss curve, a multi-metric accuracy curve and a normalized confusion-matrix heatmap on the test split. Each plot is wrapped in a separate try-except block, saved to the working directory with descriptive file names that contain the dataset name and plot type, titled accordingly and immediately closed. Only the values that exist in experiment_data are used; no synthetic data are generated. The plotting cadence is one figure per category, so we remain below the five-figure limit. Finally the script prints the path of every saved figure so the user can inspect them.", "We will load the saved experiment_data.npy, locate the TokenOrderShuffle \u2192 SPR_BENCH entry, and extract losses, per-epoch validation metrics, and final test metrics/predictions.  \nFour plots will be produced (all inside independent try-except blocks, each closed after saving):  \n1. Training vs. validation loss curve across epochs.  \n2. Validation metric curves (ACC, CWA, SWA, CCWA) across epochs.  \n3. Bar chart summarising final test metrics.  \n4. Confusion-matrix heatmap between test predictions and ground-truth labels (if both arrays exist and are 1-D).  \nAll figures will be titled clearly, use the dataset name in filenames, and be written to the working directory.  \nThe script begins with required imports, creates working_dir, loads the numpy file safely, generates the plots, and prints a short confirmation of saved filenames.", "We will load experiment_data.npy, navigate to the nested dict for the only dataset (TwoClusterGranularity / SPR_BENCH) and pull out the stored training & validation losses, epoch-wise validation metrics, and final test metrics. Three plots are produced: (1) Train vs Val loss across epochs; (2) Four validation metrics (ACC, CWA, SWA, CCWA) across epochs; (3) A bar chart summarising the same four metrics on the test split. Each plot is generated inside its own try-except block, saved into working_dir with descriptive names, then closed. Titles clearly indicate dataset, split and what is on each axis. After plotting, the script prints the final test metrics so the user sees quantitative results in the console. No synthetic data are created\u2014everything comes directly from experiment_data.npy. At most three figures are saved, complying with the \u201c\u2264 5 plots\u201d rule.", "The script will load the saved experiment_data.npy, fetch the BinaryCountAblation/SPR_BENCH entry and convert the per-epoch lists into numpy arrays. Five figures are attempted: (1) training vs validation loss, (2) validation accuracy, (3) validation weighted metrics (CWA, SWA, CCWA), (4) a bar chart of final test metrics, and (5) a confusion matrix of ground-truth vs predictions. Each figure is wrapped in a try-except so that missing keys simply skip that plot; on success the plot is titled, saved to working_dir with a descriptive filename, and closed. Only data present in the file are used\u2014no synthetic values are generated\u2014and at most one figure per category is produced. All matplotlib resources are freed after saving, preventing memory leaks. The code is concise, uses basic Matplotlib, and can be dropped into any environment where experiment_data.npy is present."], "ablation_name": [null, "Remove-Cluster-Feature Ablation", "Random-Cluster-Assignment Ablation", "Remove-Token-Feature Ablation", "ORD-Embedding-Cluster Ablation", "No-Bigram-Feature Ablation", "Token-Order-Shuffle Ablation", "Two-Cluster-Granularity Ablation", "Binary-Count-Feature Ablation"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["We will load experiment_data.npy from the \u201cworking\u201d directory, extract the\nstored dictionaries, and for every dataset iterate through the loss and metric\nsub-lists/dicts.  We simply take the last (i.e., final) element in each list as\nthe \u201cfinal\u201d value, then print each value with an explicit, descriptive label so\nnothing is ambiguous.  The script runs immediately at import time and produces\nhuman-readable output without any plotting.  All logic is kept at the top level\nor in helper functions\u2014no special entry point is used.", "Below is a small script that immediately loads the saved numpy file, retrieves\nthe recorded losses and metrics, selects the final-epoch losses and the best\n(highest-accuracy) validation metrics, and then prints them together with the\ntest metrics\u2014all clearly labelled. The code assumes the file lives in the\nworking sub-directory created by the original training script.", "Below is a short script that immediately loads the saved numpy file, extracts\nthe metric dictionaries, selects the best (or final when appropriate) values for\neach split, and prints them with precise labels for the \u201cSPR_BENCH\u201d dataset.", "The script will load experiment_data.npy from the working sub-directory, recover\nthe nested dictionary, and then iterate through every technique and dataset\nstored inside.   For each dataset it will (1) find the lowest training loss, (2)\nfind the lowest validation loss, (3) identify the validation epoch that achieved\nthe highest validation accuracy and report all its metrics, and (4) print the\nfinal test metrics.   Every number is printed with an explicit metric name to\nsatisfy the formatting rules, and the code is written at global scope so it runs\nimmediately when executed.", "The script will load the NumPy file that stores the experiment dictionary,\niterate through every experiment and every dataset, pull out the final values\nrecorded for each metric and loss, and print them with explicit, self-describing\nlabels. For training we only have the loss, while for validation we have both\nthe loss and several accuracy-type measures; test metrics are stored once at the\nend. Everything runs immediately without any special entry-point guard.", "The script will locate the working directory, load the saved numpy file into a\nPython dictionary, and iterate through every ablation\u2013dataset pair it finds. For\neach dataset it prints the final training and validation losses, the best\nvalidation metrics (chosen by the highest validation accuracy), and the final\ntest metrics, always labeling each value explicitly. Everything lives in the\nglobal scope so the code executes immediately when run, satisfying all\nstructural requirements.", "The script will locate the working directory, load the saved NumPy file, and\niterate through every ablation and dataset contained in the dictionary.   For\neach dataset it will (1) find the best training and validation losses (minimum\nvalue), (2) identify the epoch with the highest validation accuracy and report\nall accompanying validation metrics from that epoch, and (3) print the final\ntest metrics already stored.   Each metric is printed with an explicit,\ndescriptive name so the output is self-explanatory, and no plots or extra entry-\npoint guards are used.", "Below is the plan:   Load the saved NumPy dictionary from the working directory,\nnavigate through its hierarchical structure (experiment \u2192 dataset), and for each\ndataset print (1) the final training loss, (2) the final validation loss, (3)\nthe final/last validation metrics (accuracy, CWA, SWA, CCWA), and (4) the test-\nset metrics. Each printed item is prefixed with an explicit, human-readable\nmetric name.", "The script below loads the saved experiment dictionary from the working folder,\ndrills down to each dataset, and prints the final value recorded for every loss\nand evaluation metric. For validation metrics it uses the last epoch entry, and\nfor the test split it uses the single stored dictionary. All metric names are\nprinted explicitly (e.g., \u201cvalidation accuracy\u201d) and the code executes\nimmediately without any special entry-point block."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper ----------\ndef safe_last(lst, default=None):\n    return lst[-1] if lst else default\n\n\n# ---------- print metrics ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # ----- losses -----\n    final_train_loss = safe_last(content.get(\"losses\", {}).get(\"train\", []))\n    final_val_loss = safe_last(content.get(\"losses\", {}).get(\"val\", []))\n\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n    # ----- validation metrics -----\n    val_metrics_list = content.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics_list:\n        final_val_metrics = val_metrics_list[-1]\n        print(f\"Final validation accuracy: {final_val_metrics['acc']:.4f}\")\n        print(\n            f\"Final validation color-weighted accuracy: {final_val_metrics['cwa']:.4f}\"\n        )\n        print(\n            f\"Final validation shape-weighted accuracy: {final_val_metrics['swa']:.4f}\"\n        )\n        print(\n            f\"Final validation cluster-consistency weighted accuracy: {final_val_metrics['ccwa']:.4f}\"\n        )\n\n    # ----- test metrics -----\n    test_metrics = content.get(\"metrics\", {}).get(\"test\", {})\n    if test_metrics:\n        print(f\"Test accuracy: {test_metrics['acc']:.4f}\")\n        print(f\"Test color-weighted accuracy: {test_metrics['cwa']:.4f}\")\n        print(f\"Test shape-weighted accuracy: {test_metrics['swa']:.4f}\")\n        print(f\"Test cluster-consistency weighted accuracy: {test_metrics['ccwa']:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate over stored results ----------\nfor exp_name, datasets in experiment_data.items():  # exp_name == 'RemoveClusterFeat'\n    for dataset_name, data in datasets.items():  # dataset_name == 'SPR_BENCH'\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---- losses (take the final epoch\u2019s value) ----\n        if data[\"losses\"][\"train\"]:\n            final_train_loss = data[\"losses\"][\"train\"][-1]\n            print(f\"final training loss: {final_train_loss:.4f}\")\n\n        if data[\"losses\"][\"val\"]:\n            final_val_loss = data[\"losses\"][\"val\"][-1]\n            print(f\"final validation loss: {final_val_loss:.4f}\")\n\n        # ---- validation metrics (pick best by accuracy) ----\n        val_metrics_list = data[\"metrics\"][\"val\"]\n        if val_metrics_list:\n            best_val = max(val_metrics_list, key=lambda d: d[\"acc\"])\n            print(f\"best validation accuracy: {best_val['acc']:.3f}\")\n            print(f\"best validation color-weighted accuracy: {best_val['cwa']:.3f}\")\n            print(f\"best validation shape-weighted accuracy: {best_val['swa']:.3f}\")\n            print(\n                f\"best validation cluster-corrected weighted accuracy: {best_val['ccwa']:.3f}\"\n            )\n\n        # ---- test metrics (single dict) ----\n        test_metrics = data[\"metrics\"][\"test\"]\n        if test_metrics:\n            print(f\"test accuracy: {test_metrics['acc']:.3f}\")\n            print(f\"test color-weighted accuracy: {test_metrics['cwa']:.3f}\")\n            print(f\"test shape-weighted accuracy: {test_metrics['swa']:.3f}\")\n            print(\n                f\"test cluster-corrected weighted accuracy: {test_metrics['ccwa']:.3f}\"\n            )\n", "import os\nimport numpy as np\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- iterate through stored results ----------\nfor method_name, datasets in experiment_data.items():\n    for dataset_name, ed in datasets.items():  # e.g. \u201cSPR_BENCH\u201d\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---------- training ----------\n        train_losses = ed.get(\"losses\", {}).get(\"train\", [])\n        if train_losses:\n            best_train_loss = min(train_losses)\n            print(f\"best training loss: {best_train_loss:.4f}\")\n\n        # ---------- validation ----------\n        val_losses = ed.get(\"losses\", {}).get(\"val\", [])\n        val_metrics = ed.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            # choose the epoch with the highest validation accuracy\n            best_idx = int(np.argmax([m[\"acc\"] for m in val_metrics]))\n            best_val_dict = val_metrics[best_idx]\n\n            if val_losses:\n                best_val_loss = val_losses[best_idx]\n                print(f\"validation loss (at best accuracy epoch): {best_val_loss:.4f}\")\n\n            print(f\"validation accuracy: {best_val_dict['acc']:.4f}\")\n            print(f\"validation color-weighted accuracy: {best_val_dict['cwa']:.4f}\")\n            print(f\"validation shape-weighted accuracy: {best_val_dict['swa']:.4f}\")\n            print(\n                f\"validation cluster-consistency weighted accuracy: {best_val_dict['ccwa']:.4f}\"\n            )\n\n        # ---------- test ----------\n        test_metrics = ed.get(\"metrics\", {}).get(\"test\", {})\n        if test_metrics:\n            print(f\"test accuracy: {test_metrics.get('acc', float('nan')):.4f}\")\n            print(\n                f\"test color-weighted accuracy: {test_metrics.get('cwa', float('nan')):.4f}\"\n            )\n            print(\n                f\"test shape-weighted accuracy: {test_metrics.get('swa', float('nan')):.4f}\"\n            )\n            print(\n                f\"test cluster-consistency weighted accuracy: {test_metrics.get('ccwa', float('nan')):.4f}\"\n            )\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to pick best ----------\ndef best_index(values, higher_is_better=True):\n    if not values:\n        return None\n    return (np.argmax if higher_is_better else np.argmin)(values)\n\n\n# ---------- iterate and print ----------\nfor technique_name, technique_block in experiment_data.items():\n    for dataset_name, data in technique_block.items():\n        print(dataset_name)\n\n        # losses\n        train_losses = data.get(\"losses\", {}).get(\"train\", [])\n        val_losses = data.get(\"losses\", {}).get(\"val\", [])\n\n        best_train_loss = min(train_losses) if train_losses else None\n        best_val_loss = min(val_losses) if val_losses else None\n\n        if best_train_loss is not None:\n            print(f\"best training loss: {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"best validation loss: {best_val_loss:.4f}\")\n\n        # validation metrics (list of dicts)\n        val_metrics = data.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            val_accs = [m[\"acc\"] for m in val_metrics]\n            idx_best = best_index(val_accs, higher_is_better=True)\n            best_val = val_metrics[idx_best]\n\n            print(f\"best validation accuracy: {best_val['acc']:.4f}\")\n            print(f\"best validation CWA: {best_val['cwa']:.4f}\")\n            print(f\"best validation SWA: {best_val['swa']:.4f}\")\n            print(f\"best validation CCWA: {best_val['ccwa']:.4f}\")\n\n        # test metrics (single dict)\n        test_metrics = data.get(\"metrics\", {}).get(\"test\", {})\n        if test_metrics:\n            print(f\"test accuracy: {test_metrics.get('acc', np.nan):.4f}\")\n            print(f\"test CWA: {test_metrics.get('cwa', np.nan):.4f}\")\n            print(f\"test SWA: {test_metrics.get('swa', np.nan):.4f}\")\n            print(f\"test CCWA: {test_metrics.get('ccwa', np.nan):.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the saved experiment data\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to safely fetch the last element of a list\n# ---------------------------------------------------------------------\ndef last_or_none(lst):\n    return lst[-1] if isinstance(lst, list) and lst else None\n\n\n# ---------------------------------------------------------------------\n# Iterate over experiments and datasets, printing final metrics\n# ---------------------------------------------------------------------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, records in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---------- losses ----------\n        train_loss_final = last_or_none(records.get(\"losses\", {}).get(\"train\", []))\n        if train_loss_final is not None:\n            print(f\"train loss: {train_loss_final:.6f}\")\n\n        val_loss_final = last_or_none(records.get(\"losses\", {}).get(\"val\", []))\n        if val_loss_final is not None:\n            print(f\"validation loss: {val_loss_final:.6f}\")\n\n        # ---------- validation metrics ----------\n        val_metrics_final = last_or_none(records.get(\"metrics\", {}).get(\"val\", []))\n        if val_metrics_final:\n            if \"acc\" in val_metrics_final:\n                print(f\"validation accuracy: {val_metrics_final['acc']:.6f}\")\n            if \"cwa\" in val_metrics_final:\n                print(\n                    f\"validation color-weighted accuracy: {val_metrics_final['cwa']:.6f}\"\n                )\n            if \"swa\" in val_metrics_final:\n                print(\n                    f\"validation shape-weighted accuracy: {val_metrics_final['swa']:.6f}\"\n                )\n            if \"ccwa\" in val_metrics_final:\n                print(\n                    f\"validation cluster-consistency-weighted accuracy: {val_metrics_final['ccwa']:.6f}\"\n                )\n\n        # ---------- test metrics ----------\n        test_metrics = records.get(\"metrics\", {}).get(\"test\", {})\n        if test_metrics:\n            if \"acc\" in test_metrics:\n                print(f\"test accuracy: {test_metrics['acc']:.6f}\")\n            if \"cwa\" in test_metrics:\n                print(f\"test color-weighted accuracy: {test_metrics['cwa']:.6f}\")\n            if \"swa\" in test_metrics:\n                print(f\"test shape-weighted accuracy: {test_metrics['swa']:.6f}\")\n            if \"ccwa\" in test_metrics:\n                print(\n                    f\"test cluster-consistency-weighted accuracy: {test_metrics['ccwa']:.6f}\"\n                )\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------\n# locate and load experiment data\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------------\n# helper for pretty printing with consistent precision\n# -------------------------------------------------------\ndef p(label, value):\n    print(f\"{label}: {value:.4f}\")\n\n\n# -------------------------------------------------------\n# iterate over all stored experiments / datasets\n# -------------------------------------------------------\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # --------------- losses --------------------------\n        train_losses = content[\"losses\"].get(\"train\", [])\n        if train_losses:\n            p(\"final training loss\", train_losses[-1])\n\n        val_losses = content[\"losses\"].get(\"val\", [])\n        if val_losses:\n            p(\"final validation loss\", val_losses[-1])\n\n        # --------------- validation metrics --------------\n        val_metrics = content[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            # select best epoch by highest validation accuracy\n            best_val = max(val_metrics, key=lambda m: m.get(\"acc\", -np.inf))\n            p(\"best validation accuracy\", best_val[\"acc\"])\n            p(\"best validation color weighted accuracy\", best_val[\"cwa\"])\n            p(\"best validation shape weighted accuracy\", best_val[\"swa\"])\n            p(\"best validation cluster-conditional weighted accuracy\", best_val[\"ccwa\"])\n\n        # --------------- test metrics --------------------\n        test_metrics = content[\"metrics\"].get(\"test\", {})\n        if test_metrics:\n            p(\"test accuracy\", test_metrics[\"acc\"])\n            p(\"test color weighted accuracy\", test_metrics[\"cwa\"])\n            p(\"test shape weighted accuracy\", test_metrics[\"swa\"])\n            p(\"test cluster-conditional weighted accuracy\", test_metrics[\"ccwa\"])\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# -------- locate and load the saved experiment data --------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# -------- helper to safely format a metric value --------\ndef fmt(val):\n    return f\"{val:.4f}\" if isinstance(val, (int, float, np.floating)) else str(val)\n\n\n# -------- iterate through ablations and datasets --------\nfor ablation_name, ablation_dict in experiment_data.items():\n    for dataset_name, data in ablation_dict.items():\n        print(dataset_name)  # dataset header\n\n        # 1. Training & validation losses (report best / minimum)\n        train_losses = data.get(\"losses\", {}).get(\"train\", [])\n        if train_losses:\n            print(\"training loss:\", fmt(min(train_losses)))\n\n        val_losses = data.get(\"losses\", {}).get(\"val\", [])\n        if val_losses:\n            print(\"validation loss:\", fmt(min(val_losses)))\n\n        # 2. Validation metrics (select epoch with best accuracy)\n        val_metrics_list = data.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics_list:\n            best_epoch_idx = int(np.argmax([m[\"acc\"] for m in val_metrics_list]))\n            best_val_metrics = val_metrics_list[best_epoch_idx]\n\n            print(\"validation accuracy:\", fmt(best_val_metrics[\"acc\"]))\n            print(\"validation color weighted accuracy:\", fmt(best_val_metrics[\"cwa\"]))\n            print(\"validation shape weighted accuracy:\", fmt(best_val_metrics[\"swa\"]))\n            print(\n                \"validation cluster silhouette weighted accuracy:\",\n                fmt(best_val_metrics[\"ccwa\"]),\n            )\n\n        # 3. Test metrics (single final set)\n        test_metrics = data.get(\"metrics\", {}).get(\"test\", {})\n        if test_metrics:\n            print(\"test accuracy:\", fmt(test_metrics.get(\"acc\", \"N/A\")))\n            print(\"test color weighted accuracy:\", fmt(test_metrics.get(\"cwa\", \"N/A\")))\n            print(\"test shape weighted accuracy:\", fmt(test_metrics.get(\"swa\", \"N/A\")))\n            print(\n                \"test cluster silhouette weighted accuracy:\",\n                fmt(test_metrics.get(\"ccwa\", \"N/A\")),\n            )\n\n        # Blank line for readability between datasets\n        print()\n", "import os\nimport numpy as np\n\n# ---------------- load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ---------------- helpers ----------------\ndef _fmt(v):\n    \"Pretty-format floats / ints.\"\n    return f\"{v:.4f}\" if isinstance(v, float) else str(v)\n\n\ndef _print(name, value):\n    print(f\"{name}: {_fmt(value)}\")\n\n\n# ---------------- parse & display ----------------\nfor exp_name, datasets in experiment_data.items():  # e.g. \"TwoClusterGranularity\"\n    for ds_name, ds_dict in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(ds_name)  # dataset header\n\n        # losses ------------------------------------------------------------\n        losses = ds_dict.get(\"losses\", {})\n        if losses:\n            if losses.get(\"train\"):\n                _print(\"training loss\", losses[\"train\"][-1])\n            if losses.get(\"val\"):\n                _print(\"validation loss\", losses[\"val\"][-1])\n\n        # validation metrics -------------------------------------------------\n        val_metrics = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n        if val_metrics:\n            best = val_metrics[-1]  # final epoch entry\n            _print(\"validation accuracy\", best.get(\"acc\"))\n            _print(\"validation color-weighted accuracy\", best.get(\"cwa\"))\n            _print(\"validation shape-weighted accuracy\", best.get(\"swa\"))\n            _print(\"validation cluster-consistency weighted accuracy\", best.get(\"ccwa\"))\n\n        # test metrics -------------------------------------------------------\n        test_metrics = ds_dict.get(\"metrics\", {}).get(\"test\", {})\n        if test_metrics:\n            _print(\"test accuracy\", test_metrics.get(\"acc\"))\n            _print(\"test color-weighted accuracy\", test_metrics.get(\"cwa\"))\n            _print(\"test shape-weighted accuracy\", test_metrics.get(\"swa\"))\n            _print(\n                \"test cluster-consistency weighted accuracy\", test_metrics.get(\"ccwa\")\n            )\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the saved experiment dictionary\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# iterate over experiments & datasets, printing final / best metrics\n# ------------------------------------------------------------------\nfor experiment in experiment_data.values():  # e.g. \"BinaryCountAblation\"\n    for dataset_name, ds in experiment.items():  # e.g. \"SPR_BENCH\"\n        print(dataset_name)  # dataset header\n\n        # ---- losses ----\n        if ds[\"losses\"][\"train\"]:\n            final_train_loss = ds[\"losses\"][\"train\"][-1]\n            print(f\"train loss: {final_train_loss:.4f}\")\n\n        if ds[\"losses\"][\"val\"]:\n            final_val_loss = ds[\"losses\"][\"val\"][-1]\n            print(f\"validation loss: {final_val_loss:.4f}\")\n\n        # ---- validation metrics (take last epoch recorded) ----\n        if ds[\"metrics\"][\"val\"]:\n            last_val_metrics = ds[\"metrics\"][\"val\"][-1]\n            print(\n                f\"validation accuracy: {last_val_metrics.get('acc', float('nan')):.4f}\"\n            )\n            print(f\"validation CWA: {last_val_metrics.get('cwa', float('nan')):.4f}\")\n            print(f\"validation SWA: {last_val_metrics.get('swa', float('nan')):.4f}\")\n            print(f\"validation CCWA: {last_val_metrics.get('ccwa', float('nan')):.4f}\")\n\n        # ---- test metrics ----\n        test_metrics = ds[\"metrics\"][\"test\"]\n        if test_metrics:\n            print(f\"test accuracy: {test_metrics.get('acc', float('nan')):.4f}\")\n            print(f\"test CWA: {test_metrics.get('cwa', float('nan')):.4f}\")\n            print(f\"test SWA: {test_metrics.get('swa', float('nan')):.4f}\")\n            print(f\"test CCWA: {test_metrics.get('ccwa', float('nan')):.4f}\")\n"], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', 'Final training loss: 0.0160', '\\n', 'Final\nvalidation loss: 0.0274', '\\n', 'Final validation accuracy: 0.9924', '\\n',\n'Final validation color-weighted accuracy: 0.9926', '\\n', 'Final validation\nshape-weighted accuracy: 0.9917', '\\n', 'Final validation cluster-consistency\nweighted accuracy: 0.9933', '\\n', 'Test accuracy: 0.6969', '\\n', 'Test color-\nweighted accuracy: 0.6331', '\\n', 'Test shape-weighted accuracy: 0.6966', '\\n',\n'Test cluster-consistency weighted accuracy: 0.6955', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.0213', '\\n', 'final\nvalidation loss: 0.0326', '\\n', 'best validation accuracy: 0.992', '\\n', 'best\nvalidation color-weighted accuracy: 0.992', '\\n', 'best validation shape-\nweighted accuracy: 0.991', '\\n', 'best validation cluster-corrected weighted\naccuracy: 0.992', '\\n', 'test accuracy: 0.696', '\\n', 'test color-weighted\naccuracy: 0.633', '\\n', 'test shape-weighted accuracy: 0.696', '\\n', 'test\ncluster-corrected weighted accuracy: 0.696', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best training loss: 0.0263', '\\n', 'validation\nloss (at best accuracy epoch): 0.0346', '\\n', 'validation accuracy: 0.9898',\n'\\n', 'validation color-weighted accuracy: 0.9901', '\\n', 'validation shape-\nweighted accuracy: 0.9892', '\\n', 'validation cluster-consistency weighted\naccuracy: 0.9922', '\\n', 'test accuracy: 0.6954', '\\n', 'test color-weighted\naccuracy: 0.6322', '\\n', 'test shape-weighted accuracy: 0.6951', '\\n', 'test\ncluster-consistency weighted accuracy: 0.7057', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best training loss: 0.3370', '\\n', 'best validation loss:\n0.3456', '\\n', 'best validation accuracy: 0.8578', '\\n', 'best validation CWA:\n0.8585', '\\n', 'best validation SWA: 0.8457', '\\n', 'best validation CCWA:\n0.8713', '\\n', 'test accuracy: 0.6429', '\\n', 'test CWA: 0.5962', '\\n', 'test\nSWA: 0.6393', '\\n', 'test CCWA: 0.6449', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'train loss: 0.015792', '\\n', 'validation loss:\n0.027773', '\\n', 'validation accuracy: 0.993800', '\\n', 'validation color-\nweighted accuracy: 0.994021', '\\n', 'validation shape-weighted accuracy:\n0.993373', '\\n', 'validation cluster-consistency-weighted accuracy: 0.995558',\n'\\n', 'test accuracy: 0.696100', '\\n', 'test color-weighted accuracy: 0.632577',\n'\\n', 'test shape-weighted accuracy: 0.695754', '\\n', 'test cluster-consistency-\nweighted accuracy: 0.719640', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'final training loss: 0.1714', '\\n', 'final\nvalidation loss: 0.1708', '\\n', 'best validation accuracy: 0.9492', '\\n', 'best\nvalidation color weighted accuracy: 0.9477', '\\n', 'best validation shape\nweighted accuracy: 0.9447', '\\n', 'best validation cluster-conditional weighted\naccuracy: 0.9570', '\\n', 'test accuracy: 0.6871', '\\n', 'test color weighted\naccuracy: 0.6290', '\\n', 'test shape weighted accuracy: 0.6858', '\\n', 'test\ncluster-conditional weighted accuracy: 0.6870', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'training loss:', ' ', '0.1448', '\\n', 'validation loss:', '\n', '0.1843', '\\n', 'validation accuracy:', ' ', '0.9458', '\\n', 'validation\ncolor weighted accuracy:', ' ', '0.9439', '\\n', 'validation shape weighted\naccuracy:', ' ', '0.9411', '\\n', 'validation cluster silhouette weighted\naccuracy:', ' ', '0.9531', '\\n', 'test accuracy:', ' ', '0.6856', '\\n', 'test\ncolor weighted accuracy:', ' ', '0.6282', '\\n', 'test shape weighted accuracy:',\n' ', '0.6844', '\\n', 'test cluster silhouette weighted accuracy:', ' ',\n'0.6857', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'training loss: 0.0210', '\\n', 'validation loss: 0.0448',\n'\\n', 'validation accuracy: 0.9864', '\\n', 'validation color-weighted accuracy:\n0.9865', '\\n', 'validation shape-weighted accuracy: 0.9861', '\\n', 'validation\ncluster-consistency weighted accuracy: 0.9856', '\\n', 'test accuracy: 0.6946',\n'\\n', 'test color-weighted accuracy: 0.6320', '\\n', 'test shape-weighted\naccuracy: 0.6943', '\\n', 'test cluster-consistency weighted accuracy: 0.6954',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train loss: 0.0552', '\\n', 'validation loss: 0.0780', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation CWA: 0.9746', '\\n', 'validation\nSWA: 0.9723', '\\n', 'validation CCWA: 0.9700', '\\n', 'test accuracy: 0.6866',\n'\\n', 'test CWA: 0.6259', '\\n', 'test SWA: 0.6855', '\\n', 'test CCWA: 0.6869',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"], "current_stage": "Stage_4"};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
