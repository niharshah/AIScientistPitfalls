\relax 
\citation{chen2020deep,gulrajani2021}
\citation{jin2018deep,rezende2014stochastic,burgess2019monet}
\citation{locatello2019challenging}
\citation{wu2020unsupervised}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method Discussion}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{1}{}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance metrics for the baseline approach. Higher K-values improve factor granularity but can destabilize training.\relax }}{2}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:baseline}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary Material}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces In the proposed method, higher complexity weighting leads to more stable performance, yet some trials produce anomalous logs.\relax }}{3}{}\protected@file@percent }
\newlabel{fig:research}{{2}{3}}
\gdef \@abspage@last{3}
