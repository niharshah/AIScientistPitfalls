\documentclass{article} % For LaTeX2e
\usepackage{iclr2025,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Custom
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Fix chktex warnings about references:
\def\secref#1{Section~\ref{#1}}
\def\chapref#1{Chapter~\ref{#1}}

\graphicspath{{../figures/}}

\begin{filecontents}{references.bib}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{hartigan1979akc,
 author = {J. Hartigan and M. A. Wong},
 title = {A k-means clustering algorithm},
 year = {1979}
}

@article{devlin2019bertpo,
 author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 pages = {4171-4186},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

@article{deng2020dbscanca,
 author = {Dingsheng Deng},
 booktitle = {2020 7th International Forum on Electrical Engineering and Automation (IFEEA)},
 journal = {2020 7th International Forum on Electrical Engineering and Automation (IFEEA)},
 pages = {949-953},
 title = {DBSCAN Clustering Algorithm Based on Density},
 year = {2020}
}

@article{alotaibi2024graphol,
 author = {Fatimah Alotaibi and Adithya Kulkarni and Dawei Zhou},
 booktitle = {BigData Congress [Services Society]},
 journal = {2024 IEEE International Conference on Big Data (BigData)},
 pages = {5926-5935},
 title = {Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic},
 year = {2024}
}

@article{yu2024reasonagainue,
 author = {Xiaodong Yu and Ben Zhou and Hao Cheng and Dan Roth},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning},
 volume = {abs/2410.19056},
 year = {2024}
}

@article{snell2017prototypicalnf,
 author = {Jake Snell and Kevin Swersky and R. Zemel},
 booktitle = {Neural Information Processing Systems},
 pages = {4077-4087},
 title = {Prototypical Networks for Few-shot Learning},
 year = {2017}
}

@article{sreedhar2017clusteringld,
 author = {C. Sreedhar and N. Kasiviswanath and P. C. Reddy},
 booktitle = {Journal of Big Data},
 journal = {Journal of Big Data},
 pages = {1-19},
 title = {Clustering large datasets using K-means modified inter and intra clustering (KM-I2C) in Hadoop},
 volume = {4},
 year = {2017}
}

@article{lee2023interactivegs,
 author = {Han-Eum Lee and Cheonghwan Hur and Bunyodbek Ibrokhimov and Sanggil Kang},
 booktitle = {Applied Sciences},
 journal = {Applied Sciences},
 title = {Interactive Guiding Sparse Auto-Encoder with Wasserstein Regularization for Efficient Classification},
 year = {2023}
}

@article{mondorf2024beyondae,
 author = {Philipp Mondorf and Barbara Plank},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey},
 volume = {abs/2404.01869},
 year = {2024}
}
\end{filecontents}

\title{Unveiling Hidden Patterns: Symbolic Glyph Clustering for Enhanced PolyRule Reasoning}

\author{Anonymous}

\begin{document}

\maketitle

\begin{abstract}
Symbolic Pattern Recognition (SPR) requires models to uncover rules governing sequences of abstract glyphs. We explore clustering glyphs via auto-encoder embeddings and show that cluster labels alongside token embeddings can boost Weighted Accuracy from about 0.04 to near-perfect levels on SPR\_BENCH. However, these inflated scores may not generalize when glyphs deviate from training distributions, suggesting caution in relying on clustering alone.
\end{abstract}

\section{Introduction}
Machine learning systems often struggle to extrapolate from small or abstract symbol sets, where latent attributes drive inferences \citep{goodfellow2016deep, devlin2019bertpo, mondorf2024beyondae}. We investigate Symbolic PolyRule Reasoning (SPR), where shape-color glyphs form sequences mapped to labels. Traditional token embeddings sometimes climb from near 0.04 Weighted Accuracy toward 0.9 or higher, indicating partial capability. We study whether clustering symbolic glyphs prior to inference can improve performance. Our analysis suggests that near-perfect metrics may arise from memorizing ephemeral patterns, underscoring the gap between in-distribution accuracy and real-world robustness \citep{alotaibi2024graphol, yu2024reasonagainue}.

\section{Related Work}
Clustering has long aided pattern discovery in data \citep{hartigan1979akc, sreedhar2017clusteringld, deng2020dbscanca}, while auto-encoders capture compressed representations \citep{lee2023interactivegs}. Hybrid symbolic approaches often rely on embeddings \citep{snell2017prototypicalnf}, but few systematically cluster glyphs. Our method merges these concepts to gauge if grouping tokens by latent similarity clarifies symbolic tasks.

\section{Method and Discussion}
A 4D-bottleneck auto-encoder produces glyph representations. K-means with $K\in\{4,8,16\}$ assigns clusters. The downstream model then processes token and cluster IDs. Results can soar to near-1.0 Weighted Accuracy on SPR\_BENCH, though random clustering can also achieve surprisingly high scores. This outcome indicates that superficial correlations or shape-color biases inflate metrics rather than reflect genuine rule learning.

\section{Experiments}
We measure Weighted Accuracy covering shape/color complexities on SPR\_BENCH. \cref{fig:main_comparison} tracks baseline and clustering performance. Subfigure (a) shows a baseline stepping from about 0.04 to above 0.9. Subfigure (b) shows clustering quickly reaching nearly 1.0, nevertheless sensitive to slight glyph variations.

\begin{figure}[t!]
\centering
\subfigure[Baseline]{\includegraphics[width=0.45\textwidth]{Baseline_Val_Weighted_Accuracy.png}}
\hfill
\subfigure[Clustering]{\includegraphics[width=0.45\textwidth]{Research_Aggregated.png}}
\caption{\textbf{Validation Weighted Accuracy on SPR\_BENCH.} (a) Baseline transitions from ~0.04 to >0.9. (b) Clustering nearly saturates the metric.}
\label{fig:main_comparison}
\end{figure}

\cref{fig:bagofembeddings} shows that even a Bag-of-Embeddings approach (no RNN) can memorize distributions. \cref{fig:cluster_only} indicates cluster IDs alone can sustain high scores. \cref{fig:no_ae_clustering} confirms that raw glyph clustering is similarly effective. These observations reinforce the notion that SPR\_BENCH can be gamed with limited true generalization.

\begin{figure}[t!]
\centering
\includegraphics[width=0.52\textwidth]{Bag_of_Embeddings_No_RNN_Aggregated.png}
\caption{Bag-of-Embeddings achieves high accuracy despite ignoring sequence order, signaling strong distribution memorization.}
\label{fig:bagofembeddings}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=0.52\textwidth]{Cluster_Only_Representation_Aggregated.png}
\caption{Cluster-only inputs can perform nearly as well as standard embeddings, reflecting potential overfitting to cluster IDs.}
\label{fig:cluster_only}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=0.52\textwidth]{No_AE_Clustering_Aggregated.png}
\caption{Clustering raw glyph embeddings yields similar gains, confirming SPR\_BENCH's limited complexity.}
\label{fig:no_ae_clustering}
\end{figure}

\section{Conclusion}
Though clustering-based strategies can deliver striking results on SPR\_BENCH, they may fail under distribution shifts. We conclude that high in-distribution metrics do not necessarily reflect robust, generalized rule learning. Future steps should incorporate domain-shift evaluations and adversarial glyph alterations to ensure performance extends beyond curated scenarios.

\bibliography{references}
\bibliographystyle{iclr2025}

\appendix

\section*{\LARGE Supplementary Material}

\section{Additional Figures}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{Random_Cluster_Assignment_Aggregated.png}
\caption{Random cluster assignment can still reach high metrics, underscoring the dataset's susceptibility to memorization.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{Shape_Based_Clustering_Aggregated.png}
\caption{Pure shape-based clustering can excel in shape-focused tasks but lacks color granularity.}
\end{figure}

\end{document}