
% The foundational work on the K-means clustering algorithm by J. Hartigan and M. A. Wong, 1979. This paper introduces the K-means algorithm, which is employed in the symbolic glyph clustering process for grouping latent features. It should be cited when discussing the methodology of clustering techniques in the context of the proposed model.
@inproceedings{hartigan1979akc,
 author = {J. Hartigan and M. A. Wong},
 title = {A k-means clustering algorithm},
 year = {1979}
}

% The foundational paper on BERT, introducing the Bidirectional Encoder Representations from Transformers model. It provides the basis for extracting latent features during the data preprocessing stage for symbolic glyph clustering. This paper should be cited when discussing the methodology of using pre-trained language models in the proposed pipeline.
@article{devlin2019bertpo,
 author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 pages = {4171-4186},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

% The foundational work on DBSCAN clustering algorithm, which explains its density-based approach and ability to handle datasets with non-uniform density. This paper is relevant to the symbolic glyph clustering process and should be cited when discussing the methodology of using DBSCAN for clustering latent features in symbolic reasoning tasks.
@article{deng2020dbscanca,
 author = {Dingsheng Deng},
 booktitle = {2020 7th International Forum on Electrical Engineering and Automation (IFEEA)},
 journal = {2020 7th International Forum on Electrical Engineering and Automation (IFEEA)},
 pages = {949-953},
 title = {DBSCAN Clustering Algorithm Based on Density},
 year = {2020}
}

% Paper 1 discusses the Graph of Logic (GoL) framework, which combines symbolic logic and graph structures to improve reasoning capabilities of LLMs, aligning with the challenges of symbolic reasoning tasks. It will be cited when discussing the integration of symbolic logic for reasoning improvements. Paper 2 introduces the use of symbolic programs for evaluating mathematical reasoning, providing insights into evaluating reasoning tasks. It will be cited in the context of discussing benchmarks and evaluation methodologies for symbolic reasoning.
@article{alotaibi2024graphol,
 author = {Fatimah Alotaibi and Adithya Kulkarni and Dawei Zhou},
 booktitle = {BigData Congress [Services Society]},
 journal = {2024 IEEE International Conference on Big Data (BigData)},
 pages = {5926-5935},
 title = {Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic},
 year = {2024}
}

@article{yu2024reasonagainue,
 author = {Xiaodong Yu and Ben Zhou and Hao Cheng and Dan Roth},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning},
 volume = {abs/2410.19056},
 year = {2024}
}

% This foundational work introduces Prototypical Networks for few-shot classification, which learn a metric space where classification is performed by computing distances to prototype representations of each class. It is relevant to the proposed symbolic glyph clustering approach as it highlights the power of clustering techniques in small data regimes. This paper should be cited when discussing the theoretical basis and adaptation of such clustering techniques for symbolic reasoning tasks.
@article{snell2017prototypicalnf,
 author = {Jake Snell and Kevin Swersky and R. Zemel},
 booktitle = {Neural Information Processing Systems},
 pages = {4077-4087},
 title = {Prototypical Networks for Few-shot Learning},
 year = {2017}
}

% The selected paper introduces a modified K-means clustering algorithm called KM-I2C, implemented using Hadoop MapReduce, to address scalability challenges for large datasets. This method improves efficiency in terms of execution times and cluster quality by optimizing intra-cluster and inter-cluster distances. It is relevant to mitigating the risk of computational costs in symbolic glyph clustering and should be cited in the section discussing scalability and clustering methodologies.
@article{sreedhar2017clusteringld,
 author = {C. Sreedhar and N. Kasiviswanath and P. C. Reddy},
 booktitle = {Journal of Big Data},
 journal = {Journal of Big Data},
 pages = {1-19},
 title = {Clustering large datasets using K-means modified inter and intra clustering (KM-I2C) in Hadoop},
 volume = {4},
 year = {2017}
}

% The paper 'Interactive Guiding Sparse Auto-Encoder with Wasserstein Regularization for Efficient Classification' introduces a sparse auto-encoder method enhanced with Wasserstein distance for guiding feature extraction. This work is relevant for the proposed symbolic glyph clustering pipeline, which uses auto-encoders for dimensionality reduction and latent feature extraction. It should be cited when discussing the role of auto-encoders in preprocessing and feature engineering for symbolic reasoning tasks.
@article{lee2023interactivegs,
 author = {Han-Eum Lee and Cheonghwan Hur and Bunyodbek Ibrokhimov and Sanggil Kang},
 booktitle = {Applied Sciences},
 journal = {Applied Sciences},
 title = {Interactive Guiding Sparse Auto-Encoder with Wasserstein Regularization for Efficient Classification},
 year = {2023}
}

% The selected paper provides a comprehensive review of evaluation methodologies for reasoning in large language models, emphasizing the limitations of accuracy-based metrics and the need for deeper insights into reasoning behavior. This work is relevant for supporting the use of Color-Weighted Accuracy (CWA) and Shape-Weighted Accuracy (SWA) as evaluation metrics in the study, and it should be cited in the evaluation methodology section to contextualize the chosen metrics in the broader landscape of reasoning assessments in AI.
@article{mondorf2024beyondae,
 author = {Philipp Mondorf and Barbara Plank},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey},
 volume = {abs/2404.01869},
 year = {2024}
}
