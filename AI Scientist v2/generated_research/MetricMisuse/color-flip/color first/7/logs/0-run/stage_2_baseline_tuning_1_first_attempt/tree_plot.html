<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 3], [0, 4], [0, 1], [0, 2]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.3333333333333333, 1.0], [0.6666666666666666, 1.0], [1.0, 1.0]], "plan": ["We start with a plain sequence-classification baseline: every glyph (e.g. \u201cA3\u201d)\nis mapped to an integer id, embedded, averaged across the sequence, and passed\nthrough a two-layer MLP trained with cross-entropy. This simple bag-of-glyph\nrepresentation is quick, GPU-friendly, and gives us a strong first reference\npoint. We load SPR_BENCH with the helper from SPR.py; if the folder is missing\nwe fall back on a small synthetic dataset so the script is always runnable. At\nevery epoch we compute the three required metrics\u2014CWA, SWA, and our new\nComplexity-Weighted Accuracy (CpxWA)\u2014on both train and dev splits, print\nvalidation loss, and store everything in an experiment_data dict that is finally\nsaved to disk. All tensors, the model, and the criterion are moved to CUDA when\navailable, following the mandatory device-handling rules. A concise plot of\nCpxWA over epochs is produced and saved in the working directory. This\nestablishes a fully working, end-to-end pipeline from data preparation to\nevaluation that we can later refine with glyph-clustering ideas.", "Hyperparam tuning name: EPOCHS. We will extend training to 30 epochs and add a\nsimple early-stopping routine (patience = 5) that monitors validation\ncomplexity-weighted accuracy, stopping when no improvement is seen for five\nconsecutive epochs. All metrics, losses, predictions and ground-truth labels are\nlogged every epoch in the required experiment_data structure (top-level key\n\u201cepochs_tuning\u201d). The rest of the pipeline (data loading / model / plotting /\nsaving) remains unchanged.", "Hyperparam tuning name: learning_rate. We sweep four learning-rates (1e-4, 3e-4,\n1e-3, 3e-3).   For each rate a fresh model is trained for five epochs, recording\nper-epoch losses and colour/shape/complexity-weighted accuracies on both train\nand dev splits.   Results are stored in\nexperiment_data['learning_rate'][str(lr)][dataset]\u2026; after all runs the\nlearning-rate with the highest dev complexity-weighted accuracy is reported and\nits curve is plotted.   Finally the whole experiment_data object is saved to\nworking/experiment_data.npy.", "Hyperparam tuning name: batch_size. The solution iterates over three batch sizes\n(32, 64, 128).   For each batch size it builds fresh data-loaders, re-\ninitialises model and optimiser, trains for five epochs, evaluates after every\nepoch, and stores losses and weighted-accuracy metrics in a structured\nexperiment_data dictionary keyed by batch size.   All collected data are saved\nto working/experiment_data.npy and a validation Complexity-Weighted-Accuracy\nlearning-curve is also plotted for every batch size.", "Hyperparam tuning name: emb_dim. We keep the original pipeline but wrap the\nwhole training-and-evaluation routine inside a loop that iterates over the\nembedding dimensions {16, 32, 64, 128}.   For every emb_dim we re-initialise the\nmodel, optimiser, train for 5 epochs, collect the per-epoch losses and weighted-\naccuracy metrics, and finally store predictions from the last validation pass.\nAll results are written into the mandatory experiment_data dictionary under the\ntop-level key \"emb_dim\", then persisted with np.save(\"experiment_data.npy\")."], "code": ["import os, pathlib, random, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Dataset loading helpers (adapted from SPR.py) ------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------------------------------------------------------------------\n# -------------------- Synthetic fallback -----------------------------\ndef make_synthetic_split(\n    n: int, vocab_shapes=5, vocab_colors=4, max_len=8, num_labels=3\n):\n    rng = random.Random(42 + n)\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n):\n        L = rng.randint(3, max_len)\n        seq = []\n        for _ in range(L):\n            s = chr(ord(\"A\") + rng.randint(0, vocab_shapes - 1))\n            c = str(rng.randint(0, vocab_colors - 1))\n            seq.append(s + c)\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(\" \".join(seq))\n        data[\"label\"].append(rng.randint(0, num_labels - 1))\n    return data\n\n\ndef load_data():\n    spr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        if not spr_root.exists():\n            raise FileNotFoundError\n        dset = load_spr_bench(spr_root)\n    except Exception as e:\n        print(\"SPR_BENCH not found \u2013 using synthetic data.\")\n        train = make_synthetic_split(3000)\n        dev = make_synthetic_split(600)\n        test = make_synthetic_split(600)\n        dset = DatasetDict(\n            {\n                \"train\": load_dataset(\n                    \"json\", data_files={\"train\": train}, split=\"train\"\n                ),\n                \"dev\": load_dataset(\"json\", data_files={\"train\": dev}, split=\"train\"),\n                \"test\": load_dataset(\"json\", data_files={\"train\": test}, split=\"train\"),\n            }\n        )\n    return dset\n\n\ndset = load_data()\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nprint(f\"Classes: {num_classes}, Train size: {len(dset['train'])}\")\n\n\n# ---------------------------------------------------------------------\n# --------------------- Tokenisation & Vocab --------------------------\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {}\n    for seq in sequences:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab) + 1  # reserve 0 for padding\n    return vocab\n\n\nvocab = build_vocab(dset[\"train\"][\"sequence\"])\nvocab_size = len(vocab) + 1\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(sequence: str) -> List[int]:\n    return [vocab.get(tok, 0) for tok in sequence.split()]\n\n\n# ---------------------------------------------------------------------\n# -------------------- Torch Dataset wrapper --------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.ids = hf_split[\"id\"]\n        self.seqs = [encode_sequence(s) for s in hf_split[\"sequence\"]]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": (\n                dset[\"train\" if hf_split is dset[\"train\"] else \"dev\"][\"sequence\"][idx]\n                if False\n                else \"\"\n            ),\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        padded[i, : lengths[i]] = b[\"seq\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"seq\": padded, \"lengths\": torch.tensor(lengths), \"label\": labels}\n\n\ntrain_ds = SPRTorchDataset(dset[\"train\"])\ndev_ds = SPRTorchDataset(dset[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\n\n\n# ---------------------------------------------------------------------\n# -------------------------- Model ------------------------------------\nclass AvgEmbClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        mask = (x != 0).float().unsqueeze(-1)\n        summed = (self.embed(x) * mask).sum(1)\n        lengths = mask.sum(1).clamp(min=1e-6)\n        avg = summed / lengths\n        return self.fc(avg)\n\n\nmodel = AvgEmbClassifier(vocab_size, 32, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------------------------------------------------------------------\n# -------------- Experiment data tracking structure -------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------------------------------------------------------------------\n# ------------------------- Training loop -----------------------------\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0\n    n = 0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"seq\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"label\"].size(0)\n        n += batch[\"label\"].size(0)\n    train_loss = total_loss / n\n\n    # ----- Evaluation\n    def evaluate(loader):\n        model.eval()\n        all_preds = []\n        all_labels = []\n        all_seqs = []\n        with torch.no_grad():\n            for batch in loader:\n                seqs_raw = None\n                batch = {\n                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n                    for k, v in batch.items()\n                }\n                logits = model(batch[\"seq\"])\n                preds = logits.argmax(1).cpu().tolist()\n                labels = batch[\"label\"].cpu().tolist()\n                all_preds.extend(preds)\n                all_labels.extend(labels)\n        return all_preds, all_labels\n\n    train_preds, train_labels = evaluate(train_loader)\n    val_preds, val_labels = evaluate(dev_loader)\n\n    # Need raw sequences for metrics\n    train_raw = dset[\"train\"][\"sequence\"]\n    val_raw = dset[\"dev\"][\"sequence\"]\n\n    train_cwa = color_weighted_accuracy(train_raw, train_labels, train_preds)\n    val_cwa = color_weighted_accuracy(val_raw, val_labels, val_preds)\n    train_swa = shape_weighted_accuracy(train_raw, train_labels, train_preds)\n    val_swa = shape_weighted_accuracy(val_raw, val_labels, val_preds)\n    train_cpx = complexity_weighted_accuracy(train_raw, train_labels, train_preds)\n    val_cpx = complexity_weighted_accuracy(val_raw, val_labels, val_preds)\n\n    # logging\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"cwa\": train_cwa, \"swa\": train_swa, \"cpx\": train_cpx}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"cwa\": val_cwa, \"swa\": val_swa, \"cpx\": val_cpx}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        None\n    )  # val loss not computed here\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}  Val CpxWA={val_cpx:.4f}\")\n\n# ---------------------------------------------------------------------\n# -------------------- Save metrics & plot ----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\ncpx_vals = [m[\"cpx\"] for m in experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"]]\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"epochs\"], cpx_vals, marker=\"o\")\nplt.title(\"Validation Complexity-Weighted Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"CpxWA\")\nplt.savefig(os.path.join(working_dir, \"cpxwa_curve.png\"))\nprint(\"Finished. Results saved in working/.\")\n", "import os, pathlib, random, time, json, math\nfrom typing import List, Dict\nimport numpy as np\nimport torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------- experiment store --------------------------\nexperiment_data = {\n    \"epochs_tuning\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_ref = experiment_data[\"epochs_tuning\"][\"SPR_BENCH\"]\n\n# ------------------------- working dir -------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------------- device --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- Dataset helpers ------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# --------------------- synthetic fallback ----------------------------\ndef make_synthetic_split(n, vocab_shapes=5, vocab_colors=4, max_len=8, num_labels=3):\n    rng = random.Random(42 + n)\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n):\n        L = rng.randint(3, max_len)\n        seq = [\n            chr(ord(\"A\") + rng.randint(0, vocab_shapes - 1))\n            + str(rng.randint(0, vocab_colors - 1))\n            for _ in range(L)\n        ]\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(\" \".join(seq))\n        data[\"label\"].append(rng.randint(0, num_labels - 1))\n    return data\n\n\ndef load_data():\n    spr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        if not spr_root.exists():\n            raise FileNotFoundError\n        return load_spr_bench(spr_root)\n    except Exception:\n        print(\"SPR_BENCH not found \u2013 using synthetic data.\")\n        return DatasetDict(\n            {\n                \"train\": load_dataset(\n                    \"json\",\n                    data_files={\"train\": make_synthetic_split(3000)},\n                    split=\"train\",\n                ),\n                \"dev\": load_dataset(\n                    \"json\",\n                    data_files={\"train\": make_synthetic_split(600)},\n                    split=\"train\",\n                ),\n                \"test\": load_dataset(\n                    \"json\",\n                    data_files={\"train\": make_synthetic_split(600)},\n                    split=\"train\",\n                ),\n            }\n        )\n\n\ndset = load_data()\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nprint(f'Classes:{num_classes}, Train size:{len(dset[\"train\"])}')\n\n\n# ------------------------ tokenisation -------------------------------\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    vocab = {}\n    for s in seqs:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab) + 1\n    return vocab\n\n\nvocab = build_vocab(dset[\"train\"][\"sequence\"])\nvocab_size = len(vocab) + 1\n\n\ndef encode_sequence(seq: str) -> List[int]:\n    return [vocab.get(tok, 0) for tok in seq.split()]\n\n\n# ------------------------ torch dataset ------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split, raw_sequences):\n        self.seqs = [encode_sequence(s) for s in hf_split[\"sequence\"]]\n        self.labels = hf_split[\"label\"]\n        self.raw = raw_sequences\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw\": self.raw[idx],\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        padded[i, : lengths[i]] = b[\"seq\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raws = [b[\"raw\"] for b in batch]\n    return {\n        \"seq\": padded,\n        \"lengths\": torch.tensor(lengths),\n        \"label\": labels,\n        \"raw\": raws,\n    }\n\n\ntrain_ds = SPRTorchDataset(dset[\"train\"], dset[\"train\"][\"sequence\"])\ndev_ds = SPRTorchDataset(dset[\"dev\"], dset[\"dev\"][\"sequence\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\n\n\n# ---------------------------- model ----------------------------------\nclass AvgEmbClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        mask = (x != 0).float().unsqueeze(-1)\n        summed = (self.embed(x) * mask).sum(1)\n        avg = summed / mask.sum(1).clamp(min=1e-6)\n        return self.fc(avg)\n\n\nmodel = AvgEmbClassifier(vocab_size, 32, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------------------- training --------------------------------\nMAX_EPOCHS = 30\npatience = 5\nbest_val = -float(\"inf\")\npatience_ctr = 0\n\n\ndef run_eval(loader):\n    model.eval()\n    preds, labels, raws = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"seq\"])\n            preds.extend(logits.argmax(1).cpu().tolist())\n            labels.extend(batch[\"label\"].cpu().tolist())\n            raws.extend(batch[\"raw\"])\n    return preds, labels, raws\n\n\nfor epoch in range(1, MAX_EPOCHS + 1):\n    model.train()\n    tot_loss = 0\n    n = 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"seq\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"label\"].size(0)\n        n += batch[\"label\"].size(0)\n    train_loss = tot_loss / n\n\n    # evaluation\n    tr_preds, tr_labels, _ = run_eval(train_loader)\n    val_preds, val_labels, val_raw = run_eval(dev_loader)\n    train_raw = dset[\"train\"][\"sequence\"]\n\n    train_cwa = color_weighted_accuracy(train_raw, tr_labels, tr_preds)\n    val_cwa = color_weighted_accuracy(val_raw, val_labels, val_preds)\n    train_swa = shape_weighted_accuracy(train_raw, tr_labels, tr_preds)\n    val_swa = shape_weighted_accuracy(val_raw, val_labels, val_preds)\n    train_cpx = complexity_weighted_accuracy(train_raw, tr_labels, tr_preds)\n    val_cpx = complexity_weighted_accuracy(val_raw, val_labels, val_preds)\n\n    # logging\n    exp_ref[\"metrics\"][\"train\"].append(\n        {\"cwa\": train_cwa, \"swa\": train_swa, \"cpx\": train_cpx}\n    )\n    exp_ref[\"metrics\"][\"val\"].append({\"cwa\": val_cwa, \"swa\": val_swa, \"cpx\": val_cpx})\n    exp_ref[\"losses\"][\"train\"].append(train_loss)\n    exp_ref[\"losses\"][\"val\"].append(None)\n    exp_ref[\"predictions\"].append(val_preds)\n    exp_ref[\"ground_truth\"].append(val_labels)\n    exp_ref[\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch:02d}/{MAX_EPOCHS}  train_loss={train_loss:.4f}  Val CpxWA={val_cpx:.4f}\"\n    )\n\n    # early stopping\n    if val_cpx > best_val + 1e-6:\n        best_val = val_cpx\n        patience_ctr = 0\n    else:\n        patience_ctr += 1\n        if patience_ctr >= patience:\n            print(f\"No improvement for {patience} epochs \u2013 early stopping.\")\n            break\n\n# -------------------------- save & plot ------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\ncpx_vals = [m[\"cpx\"] for m in exp_ref[\"metrics\"][\"val\"]]\nplt.figure()\nplt.plot(exp_ref[\"epochs\"], cpx_vals, marker=\"o\")\nplt.title(\"Validation Complexity-Weighted Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"CpxWA\")\nplt.savefig(os.path.join(working_dir, \"cpxwa_curve.png\"))\nprint(\"Finished. All artifacts saved in working/.\")\n", "import os, pathlib, random, time, json, warnings\nfrom typing import List, Dict\nimport numpy as np\nimport torch, math\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------\n# experiment data structure ------------------------------------------------\nexperiment_data = {\"learning_rate\": {}}  # will be filled with one sub-dict per lr value\n\n# ---------------------------------------------------------------------\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Dataset loading helpers (adapted from SPR.py) ------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------------------------------------------------------------------\n# -------------------- Synthetic fallback -----------------------------\ndef make_synthetic_split(\n    n: int, vocab_shapes=5, vocab_colors=4, max_len=8, num_labels=3\n):\n    rng = random.Random(42 + n)\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n):\n        L = rng.randint(3, max_len)\n        seq = []\n        for _ in range(L):\n            s = chr(ord(\"A\") + rng.randint(0, vocab_shapes - 1))\n            c = str(rng.randint(0, vocab_colors - 1))\n            seq.append(s + c)\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(\" \".join(seq))\n        data[\"label\"].append(rng.randint(0, num_labels - 1))\n    return data\n\n\ndef load_data():\n    spr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        if not spr_root.exists():\n            raise FileNotFoundError\n        dset = load_spr_bench(spr_root)\n    except Exception as e:\n        print(\"SPR_BENCH not found \u2013 using synthetic data.\")\n        train = make_synthetic_split(3000)\n        dev = make_synthetic_split(600)\n        test = make_synthetic_split(600)\n        dset = DatasetDict(\n            {\n                \"train\": load_dataset(\n                    \"json\", data_files={\"train\": train}, split=\"train\"\n                ),\n                \"dev\": load_dataset(\"json\", data_files={\"train\": dev}, split=\"train\"),\n                \"test\": load_dataset(\"json\", data_files={\"train\": test}, split=\"train\"),\n            }\n        )\n    return dset\n\n\ndset = load_data()\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nprint(f\"Classes: {num_classes}, Train size: {len(dset['train'])}\")\n\n\n# ---------------------------------------------------------------------\n# --------------------- Tokenisation & Vocab --------------------------\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {}\n    for seq in sequences:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab) + 1  # reserve 0 for padding\n    return vocab\n\n\nvocab = build_vocab(dset[\"train\"][\"sequence\"])\nvocab_size = len(vocab) + 1\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(sequence: str) -> List[int]:\n    return [vocab.get(tok, 0) for tok in sequence.split()]\n\n\n# ---------------------------------------------------------------------\n# -------------------- Torch Dataset wrapper --------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.hf_split = hf_split\n        self.ids = hf_split[\"id\"]\n        self.seqs = [encode_sequence(s) for s in hf_split[\"sequence\"]]\n        self.labels = hf_split[\"label\"]\n        self.raw_sequences = hf_split[\"sequence\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.raw_sequences[idx],\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        padded[i, : lengths[i]] = b[\"seq\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    return {\n        \"seq\": padded,\n        \"lengths\": torch.tensor(lengths),\n        \"label\": labels,\n        \"raw\": raw,\n    }\n\n\ntrain_ds = SPRTorchDataset(dset[\"train\"])\ndev_ds = SPRTorchDataset(dset[\"dev\"])\ntrain_loader_base = DataLoader(\n    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\n\n\n# ---------------------------------------------------------------------\n# -------------------------- Model ------------------------------------\nclass AvgEmbClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        mask = (x != 0).float().unsqueeze(-1)\n        summed = (self.embed(x) * mask).sum(1)\n        lengths = mask.sum(1).clamp(min=1e-6)\n        avg = summed / lengths\n        return self.fc(avg)\n\n\n# ---------------------------------------------------------------------\n# ------------------------- Training utils ----------------------------\ndef evaluate(model, loader):\n    model.eval()\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"seq\"])\n            preds = logits.argmax(1).cpu().tolist()\n            labels = batch[\"label\"].cpu().tolist()\n            seqs = batch[\"raw\"]\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    return all_preds, all_labels, all_seqs\n\n\n# ---------------------------------------------------------------------\n# ----------------------- Hyper-parameter sweep -----------------------\ncandidates = [1e-4, 3e-4, 1e-3, 3e-3]\nEPOCHS = 5\nbest_lr, best_score = None, -float(\"inf\")\n\nfor lr in candidates:\n    print(f\"\\n=== Training with learning rate={lr} ===\")\n    lr_key = f\"{lr:.0e}\" if lr < 1e-3 else f\"{lr}\"\n    # prepare containers\n    experiment_data[\"learning_rate\"][lr_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n    model = AvgEmbClassifier(vocab_size, 32, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # create new shuffled loader each run\n    train_loader = DataLoader(\n        train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n    )\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss, n = 0.0, 0\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"seq\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            n += batch[\"label\"].size(0)\n        train_loss = total_loss / n\n\n        # evaluation\n        train_preds, train_labels, train_raw = evaluate(model, train_loader)\n        val_preds, val_labels, val_raw = evaluate(model, dev_loader)\n\n        train_cwa = color_weighted_accuracy(train_raw, train_labels, train_preds)\n        val_cwa = color_weighted_accuracy(val_raw, val_labels, val_preds)\n        train_swa = shape_weighted_accuracy(train_raw, train_labels, train_preds)\n        val_swa = shape_weighted_accuracy(val_raw, val_labels, val_preds)\n        train_cpx = complexity_weighted_accuracy(train_raw, train_labels, train_preds)\n        val_cpx = complexity_weighted_accuracy(val_raw, val_labels, val_preds)\n\n        # logging\n        log_slot = experiment_data[\"learning_rate\"][lr_key][\"SPR_BENCH\"]\n        log_slot[\"metrics\"][\"train\"].append(\n            {\"cwa\": train_cwa, \"swa\": train_swa, \"cpx\": train_cpx}\n        )\n        log_slot[\"metrics\"][\"val\"].append(\n            {\"cwa\": val_cwa, \"swa\": val_swa, \"cpx\": val_cpx}\n        )\n        log_slot[\"losses\"][\"train\"].append(train_loss)\n        log_slot[\"losses\"][\"val\"].append(None)\n        log_slot[\"epochs\"].append(epoch)\n        print(\n            f\"LR {lr_key} Epoch {epoch}: train_loss={train_loss:.4f} Val CpxWA={val_cpx:.4f}\"\n        )\n\n    # save final predictions & ground truth (dev set)\n    log_slot[\"predictions\"] = val_preds\n    log_slot[\"ground_truth\"] = val_labels\n\n    # update best lr\n    if val_cpx > best_score:\n        best_score = val_cpx\n        best_lr = lr\n        best_lr_key = lr_key\n\nprint(f\"\\nBest learning rate: {best_lr} with dev Complexity-WA={best_score:.4f}\")\n\n# ---------------------------------------------------------------------\n# -------------------- Save metrics & plot ----------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n\n# Plot CpxWA curve of best lr\nbest_epochs = experiment_data[\"learning_rate\"][best_lr_key][\"SPR_BENCH\"][\"epochs\"]\nbest_cpx_vals = [\n    m[\"cpx\"]\n    for m in experiment_data[\"learning_rate\"][best_lr_key][\"SPR_BENCH\"][\"metrics\"][\n        \"val\"\n    ]\n]\nplt.figure()\nplt.plot(best_epochs, best_cpx_vals, marker=\"o\")\nplt.title(f\"Validation Complexity-Weighted Accuracy (lr={best_lr})\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"CpxWA\")\nplt.savefig(os.path.join(working_dir, \"cpxwa_curve.png\"))\nprint(\"Finished. Results saved in working/.\")\n", "import os, pathlib, random, time, json, math\nfrom typing import List, Dict\nimport numpy as np\nimport torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------\n# experiment bookkeeping container\nexperiment_data = {\n    \"batch_size\": {  # hyper-parameter tuned\n        \"SPR_BENCH\": {\n            # each batch size (32/64/128) will be added here\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------------- Dataset utilities ----------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    data = DatasetDict()\n    data[\"train\"] = _load(\"train.csv\")\n    data[\"dev\"] = _load(\"dev.csv\")\n    data[\"test\"] = _load(\"test.csv\")\n    return data\n\n\ndef make_synthetic_split(\n    n: int, vocab_shapes=5, vocab_colors=4, max_len=8, num_labels=3\n):\n    rng = random.Random(42 + n)\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n):\n        L = rng.randint(3, max_len)\n        seq = []\n        for _ in range(L):\n            s = chr(ord(\"A\") + rng.randint(0, vocab_shapes - 1))\n            c = str(rng.randint(0, vocab_colors - 1))\n            seq.append(s + c)\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(\" \".join(seq))\n        data[\"label\"].append(rng.randint(0, num_labels - 1))\n    return data\n\n\ndef load_data():\n    spr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        if not spr_root.exists():\n            raise FileNotFoundError\n        dset = load_spr_bench(spr_root)\n    except Exception:\n        print(\"SPR_BENCH not found \u2013 generating synthetic toy data.\")\n        train = make_synthetic_split(3000)\n        dev = make_synthetic_split(600)\n        test = make_synthetic_split(600)\n        dset = DatasetDict(\n            {\n                \"train\": load_dataset(\n                    \"json\", data_files={\"train\": train}, split=\"train\"\n                ),\n                \"dev\": load_dataset(\"json\", data_files={\"train\": dev}, split=\"train\"),\n                \"test\": load_dataset(\"json\", data_files={\"train\": test}, split=\"train\"),\n            }\n        )\n    return dset\n\n\ndset = load_data()\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nprint(f\"Classes={num_classes}  Train size={len(dset['train'])}\")\n\n\n# --------------------------- Vocab -----------------------------------\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    vocab = {}\n    for s in seqs:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab) + 1  # 0 reserved for padding\n    return vocab\n\n\nvocab = build_vocab(dset[\"train\"][\"sequence\"])\nvocab_size = len(vocab) + 1\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode_sequence(seq: str):\n    return [vocab.get(tok, 0) for tok in seq.split()]\n\n\n# --------------------- Metrics ---------------------------------------\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------------ Torch Dataset ------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = [encode_sequence(s) for s in hf_split[\"sequence\"]]\n        self.labels = hf_split[\"label\"]\n        self.raw = hf_split[\"sequence\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.raw[idx],\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        padded[i, : lengths[i]] = b[\"seq\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw_seq = [b[\"raw_seq\"] for b in batch]\n    return {\n        \"seq\": padded,\n        \"lengths\": torch.tensor(lengths),\n        \"label\": labels,\n        \"raw_seq\": raw_seq,\n    }\n\n\ntrain_ds = SPRTorchDataset(dset[\"train\"])\ndev_ds = SPRTorchDataset(dset[\"dev\"])\n\n\n# -------------------------- Model ------------------------------------\nclass AvgEmbClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        mask = (x != 0).float().unsqueeze(-1)\n        summed = (self.embed(x) * mask).sum(1)\n        lens = mask.sum(1).clamp(min=1e-6)\n        avg = summed / lens\n        return self.fc(avg)\n\n\n# ----------------- Training/eval routines ----------------------------\ndef evaluate(model, loader):\n    model.eval()\n    preds, labels, raws = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"seq\"])\n            preds.extend(logits.argmax(1).cpu().tolist())\n            labels.extend(batch[\"label\"].cpu().tolist())\n            raws.extend(batch[\"raw_seq\"])\n    return preds, labels, raws\n\n\ndef run_experiment(batch_size, epochs=5):\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n    )\n    dev_loader = DataLoader(\n        dev_ds, batch_size=256, shuffle=False, collate_fn=collate_fn\n    )\n    model = AvgEmbClassifier(vocab_size, 32, num_classes).to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics_train, metrics_val = [], []\n    losses_train, losses_val = [], []\n\n    for ep in range(1, epochs + 1):\n        model.train()\n        tot_loss, n = 0.0, 0\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optim.zero_grad()\n            logits = model(batch[\"seq\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optim.step()\n            bs = batch[\"label\"].size(0)\n            tot_loss += loss.item() * bs\n            n += bs\n        train_loss = tot_loss / n\n\n        # evaluation\n        tr_preds, tr_lbls, tr_raw = evaluate(model, train_loader)\n        dv_preds, dv_lbls, dv_raw = evaluate(model, dev_loader)\n\n        tr_cwa = color_weighted_accuracy(tr_raw, tr_lbls, tr_preds)\n        dv_cwa = color_weighted_accuracy(dv_raw, dv_lbls, dv_preds)\n        tr_swa = shape_weighted_accuracy(tr_raw, tr_lbls, tr_preds)\n        dv_swa = shape_weighted_accuracy(dv_raw, dv_lbls, dv_preds)\n        tr_cpx = complexity_weighted_accuracy(tr_raw, tr_lbls, tr_preds)\n        dv_cpx = complexity_weighted_accuracy(dv_raw, dv_lbls, dv_preds)\n\n        metrics_train.append({\"cwa\": tr_cwa, \"swa\": tr_swa, \"cpx\": tr_cpx})\n        metrics_val.append({\"cwa\": dv_cwa, \"swa\": dv_swa, \"cpx\": dv_cpx})\n        losses_train.append(train_loss)\n        losses_val.append(None)\n        print(\n            f\"[bs={batch_size}] Epoch {ep}: \"\n            f\"train_loss={train_loss:.4f}  Val CpxWA={dv_cpx:.4f}\"\n        )\n\n    return {\n        \"metrics\": {\"train\": metrics_train, \"val\": metrics_val},\n        \"losses\": {\"train\": losses_train, \"val\": losses_val},\n        \"epochs\": list(range(1, epochs + 1)),\n    }\n\n\n# ------------------ Hyper-parameter sweep ----------------------------\nfor bs in [32, 64, 128]:\n    result = run_experiment(bs)\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][bs] = result\n\n# ------------------ Save & plot --------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# plot validation CpxWA curves\nplt.figure()\nfor bs, res in experiment_data[\"batch_size\"][\"SPR_BENCH\"].items():\n    cpx = [m[\"cpx\"] for m in res[\"metrics\"][\"val\"]]\n    plt.plot(res[\"epochs\"], cpx, marker=\"o\", label=f\"bs={bs}\")\nplt.title(\"Validation Complexity-Weighted Accuracy vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"CpxWA\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"cpxwa_curves.png\"))\nprint(\"Finished. All results saved to 'working/'.\")\n", "import os, pathlib, random, time, json, math, warnings\nfrom typing import List, Dict\nimport numpy as np\nimport torch, matplotlib\n\nmatplotlib.use(\"Agg\")\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# ---------- Dataset loading helpers (adapted from SPR.py) ------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------------------------------------------------------------------\n# -------------------- Synthetic fallback -----------------------------\ndef make_synthetic_split(\n    n: int, vocab_shapes=5, vocab_colors=4, max_len=8, num_labels=3\n):\n    rng = random.Random(42 + n)\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n):\n        L = rng.randint(3, max_len)\n        seq = []\n        for _ in range(L):\n            s = chr(ord(\"A\") + rng.randint(0, vocab_shapes - 1))\n            c = str(rng.randint(0, vocab_colors - 1))\n            seq.append(s + c)\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(\" \".join(seq))\n        data[\"label\"].append(rng.randint(0, num_labels - 1))\n    return data\n\n\ndef load_data():\n    spr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    try:\n        if not spr_root.exists():\n            raise FileNotFoundError\n        dset = load_spr_bench(spr_root)\n    except Exception:\n        print(\"SPR_BENCH not found \u2013 using synthetic data.\")\n        train = make_synthetic_split(3000)\n        dev = make_synthetic_split(600)\n        test = make_synthetic_split(600)\n        dset = DatasetDict(\n            {\n                \"train\": load_dataset(\n                    \"json\", data_files={\"train\": train}, split=\"train\"\n                ),\n                \"dev\": load_dataset(\"json\", data_files={\"dev\": dev}, split=\"dev\"),\n                \"test\": load_dataset(\"json\", data_files={\"test\": test}, split=\"test\"),\n            }\n        )\n    return dset\n\n\ndset = load_data()\nnum_classes = len(set(dset[\"train\"][\"label\"]))\nprint(f\"Classes: {num_classes}, Train size: {len(dset['train'])}\")\n\n\n# ---------------------------------------------------------------------\n# --------------------- Tokenisation & Vocab --------------------------\ndef build_vocab(sequences: List[str]) -> Dict[str, int]:\n    vocab = {}\n    for seq in sequences:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab) + 1  # reserve 0 for padding\n    return vocab\n\n\nvocab = build_vocab(dset[\"train\"][\"sequence\"])\nvocab_size = len(vocab) + 1\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(sequence: str) -> List[int]:\n    return [vocab.get(tok, 0) for tok in sequence.split()]\n\n\n# ---------------------------------------------------------------------\n# -------------------- Torch Dataset wrapper --------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.ids = hf_split[\"id\"]\n        self.seqs = [encode_sequence(s) for s in hf_split[\"sequence\"]]\n        self.labels = hf_split[\"label\"]\n        self.rawseq = hf_split[\"sequence\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"seq\": torch.tensor(self.seqs[idx], dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw\": self.rawseq[idx],\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(b[\"seq\"]) for b in batch]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        padded[i, : lengths[i]] = b[\"seq\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raws = [b[\"raw\"] for b in batch]\n    return {\n        \"seq\": padded,\n        \"lengths\": torch.tensor(lengths),\n        \"label\": labels,\n        \"raw\": raws,\n    }\n\n\ntrain_ds = SPRTorchDataset(dset[\"train\"])\ndev_ds = SPRTorchDataset(dset[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\n\n\n# ---------------------------------------------------------------------\n# -------------------------- Model ------------------------------------\nclass AvgEmbClassifier(nn.Module):\n    def __init__(self, vocab_size: int, emb_dim: int, num_classes: int):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.fc = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        mask = (x != 0).float().unsqueeze(-1)\n        summed = (self.embed(x) * mask).sum(1)\n        lengths = mask.sum(1).clamp(min=1e-6)\n        avg = summed / lengths\n        return self.fc(avg)\n\n\n# ---------------------------------------------------------------------\n# -------------- Experiment data tracking structure -------------------\nexperiment_data = {\"emb_dim\": {}}  # top-level key is tuning type\n\n# ---------------------------------------------------------------------\n# ------------------------- Training loop -----------------------------\nEPOCHS = 5\nemb_grid = [16, 32, 64, 128]\n\nfor emb_dim in emb_grid:\n    print(f\"\\n===== Training with emb_dim={emb_dim} =====\")\n    model = AvgEmbClassifier(vocab_size, emb_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    log = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n\n    # lists of raw sequences for metric computation\n    train_raw = dset[\"train\"][\"sequence\"]\n    val_raw = dset[\"dev\"][\"sequence\"]\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        total_loss, n = 0.0, 0\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"seq\"])\n            loss = criterion(logits, batch[\"label\"])\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            n += batch[\"label\"].size(0)\n        train_loss = total_loss / n\n\n        # evaluation helper\n        def evaluate(loader):\n            model.eval()\n            preds, labs, raws = [], [], []\n            with torch.no_grad():\n                for b in loader:\n                    raw_seqs = b[\"raw\"]\n                    b = {\n                        k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                        for k, v in b.items()\n                    }\n                    logits = model(b[\"seq\"])\n                    preds.extend(logits.argmax(1).cpu().tolist())\n                    labs.extend(b[\"label\"].cpu().tolist())\n                    raws.extend(raw_seqs)\n            return preds, labs, raws\n\n        train_preds, train_labels, _ = evaluate(train_loader)\n        val_preds, val_labels, _ = evaluate(dev_loader)\n\n        # compute metrics\n        train_cwa = color_weighted_accuracy(train_raw, train_labels, train_preds)\n        val_cwa = color_weighted_accuracy(val_raw, val_labels, val_preds)\n        train_swa = shape_weighted_accuracy(train_raw, train_labels, train_preds)\n        val_swa = shape_weighted_accuracy(val_raw, val_labels, val_preds)\n        train_cpx = complexity_weighted_accuracy(train_raw, train_labels, train_preds)\n        val_cpx = complexity_weighted_accuracy(val_raw, val_labels, val_preds)\n\n        # logging\n        log[\"metrics\"][\"train\"].append(\n            {\"cwa\": train_cwa, \"swa\": train_swa, \"cpx\": train_cpx}\n        )\n        log[\"metrics\"][\"val\"].append({\"cwa\": val_cwa, \"swa\": val_swa, \"cpx\": val_cpx})\n        log[\"losses\"][\"train\"].append(train_loss)\n        log[\"losses\"][\"val\"].append(None)\n        log[\"epochs\"].append(epoch)\n\n        print(\n            f\"[emb {emb_dim}] Epoch {epoch} | train_loss={train_loss:.4f} | Val CpxWA={val_cpx:.4f}\"\n        )\n\n    # store last epoch predictions / ground truth\n    log[\"predictions\"] = val_preds\n    log[\"ground_truth\"] = val_labels\n\n    experiment_data[\"emb_dim\"][str(emb_dim)] = {\"SPR_BENCH\": log}\n\n    # plot curve for this emb_dim\n    cpx_vals = [m[\"cpx\"] for m in log[\"metrics\"][\"val\"]]\n    plt.figure()\n    plt.plot(log[\"epochs\"], cpx_vals, marker=\"o\")\n    plt.title(f\"Val Complexity-WA (emb_dim={emb_dim})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CpxWA\")\n    plt.savefig(os.path.join(working_dir, f\"cpxwa_curve_emb{emb_dim}.png\"))\n    plt.close()\n\n    # free memory\n    del model, optimizer, criterion\n    torch.cuda.empty_cache()\n\n# ---------------------------------------------------------------------\n# -------------------- Save metrics & plot ----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All experiments finished. Data saved to working/experiment_data.npy\")\n"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 377492.83\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 623224.96\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 703494.41\nexamples/s]', '\\n', 'Classes: 2, Train size: 20000', '\\n', 'Vocab size: 17',\n'\\n', 'Epoch 1: train_loss=0.4531  Val CpxWA=0.9027', '\\n', 'Epoch 2:\ntrain_loss=0.2438  Val CpxWA=0.9250', '\\n', 'Epoch 3: train_loss=0.2035  Val\nCpxWA=0.9396', '\\n', 'Epoch 4: train_loss=0.1883  Val CpxWA=0.9399', '\\n',\n'Epoch 5: train_loss=0.1808  Val CpxWA=0.9402', '\\n', 'Finished. Results saved\nin working/.', '\\n', 'Execution time: 36 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 624157.02\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 454233.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 919541.36\nexamples/s]', '\\n', 'Classes:2, Train size:20000', '\\n', 'Epoch 01/30\ntrain_loss=0.4531  Val CpxWA=0.9027', '\\n', 'Epoch 02/30  train_loss=0.2438  Val\nCpxWA=0.9250', '\\n', 'Epoch 03/30  train_loss=0.2035  Val CpxWA=0.9396', '\\n',\n'Epoch 04/30  train_loss=0.1883  Val CpxWA=0.9399', '\\n', 'Epoch 05/30\ntrain_loss=0.1808  Val CpxWA=0.9402', '\\n', 'Epoch 06/30  train_loss=0.1762  Val\nCpxWA=0.9416', '\\n', 'Epoch 07/30  train_loss=0.1730  Val CpxWA=0.9428', '\\n',\n'Epoch 08/30  train_loss=0.1705  Val CpxWA=0.9422', '\\n', 'Epoch 09/30\ntrain_loss=0.1688  Val CpxWA=0.9430', '\\n', 'Epoch 10/30  train_loss=0.1669  Val\nCpxWA=0.9429', '\\n', 'Epoch 11/30  train_loss=0.1661  Val CpxWA=0.9432', '\\n',\n'Epoch 12/30  train_loss=0.1651  Val CpxWA=0.9432', '\\n', 'Epoch 13/30\ntrain_loss=0.1647  Val CpxWA=0.9432', '\\n', 'Epoch 14/30  train_loss=0.1637  Val\nCpxWA=0.9432', '\\n', 'Epoch 15/30  train_loss=0.1631  Val CpxWA=0.9432', '\\n',\n'Epoch 16/30  train_loss=0.1629  Val CpxWA=0.9429', '\\n', 'No improvement for 5\nepochs \u2013 early stopping.', '\\n', 'Finished. All artifacts saved in working/.',\n'\\n', 'Execution time: 55 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 474557.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 610737.96\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 748929.36\nexamples/s]', '\\n', 'Classes: 2, Train size: 20000', '\\n', 'Vocab size: 17',\n'\\n', '\\n=== Training with learning rate=0.0001 ===', '\\n', 'LR 1e-04 Epoch 1:\ntrain_loss=0.6604 Val CpxWA=0.7388', '\\n', 'LR 1e-04 Epoch 2: train_loss=0.5817\nVal CpxWA=0.7715', '\\n', 'LR 1e-04 Epoch 3: train_loss=0.5112 Val CpxWA=0.7795',\n'\\n', 'LR 1e-04 Epoch 4: train_loss=0.4703 Val CpxWA=0.7940', '\\n', 'LR 1e-04\nEpoch 5: train_loss=0.4375 Val CpxWA=0.8102', '\\n', '\\n=== Training with\nlearning rate=0.0003 ===', '\\n', 'LR 3e-04 Epoch 1: train_loss=0.5960 Val\nCpxWA=0.7384', '\\n', 'LR 3e-04 Epoch 2: train_loss=0.4946 Val CpxWA=0.7818',\n'\\n', 'LR 3e-04 Epoch 3: train_loss=0.4267 Val CpxWA=0.8317', '\\n', 'LR 3e-04\nEpoch 4: train_loss=0.3417 Val CpxWA=0.8875', '\\n', 'LR 3e-04 Epoch 5:\ntrain_loss=0.2751 Val CpxWA=0.9202', '\\n', '\\n=== Training with learning\nrate=0.001 ===', '\\n', 'LR 0.001 Epoch 1: train_loss=0.4422 Val CpxWA=0.8900',\n'\\n', 'LR 0.001 Epoch 2: train_loss=0.2404 Val CpxWA=0.9259', '\\n', 'LR 0.001\nEpoch 3: train_loss=0.1994 Val CpxWA=0.9386', '\\n', 'LR 0.001 Epoch 4:\ntrain_loss=0.1846 Val CpxWA=0.9403', '\\n', 'LR 0.001 Epoch 5: train_loss=0.1775\nVal CpxWA=0.9411', '\\n', '\\n=== Training with learning rate=0.003 ===', '\\n',\n'LR 0.003 Epoch 1: train_loss=0.3129 Val CpxWA=0.9353', '\\n', 'LR 0.003 Epoch 2:\ntrain_loss=0.1865 Val CpxWA=0.9419', '\\n', 'LR 0.003 Epoch 3: train_loss=0.1740\nVal CpxWA=0.9424', '\\n', 'LR 0.003 Epoch 4: train_loss=0.1697 Val CpxWA=0.9432',\n'\\n', 'LR 0.003 Epoch 5: train_loss=0.1679 Val CpxWA=0.9432', '\\n', '\\nBest\nlearning rate: 0.003 with dev Complexity-WA=0.9432', '\\n', 'Finished. Results\nsaved in working/.', '\\n', 'Execution time: 23 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n374003.89 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 214502.90\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 618574.17\nexamples/s]', '\\n', 'Classes=2  Train size=20000', '\\n', 'Vocab size:', ' ',\n'17', '\\n', '[bs=32] Epoch 1: train_loss=0.3836  Val CpxWA=0.9168', '\\n',\n'[bs=32] Epoch 2: train_loss=0.2137  Val CpxWA=0.9358', '\\n', '[bs=32] Epoch 3:\ntrain_loss=0.1878  Val CpxWA=0.9410', '\\n', '[bs=32] Epoch 4: train_loss=0.1788\nVal CpxWA=0.9395', '\\n', '[bs=32] Epoch 5: train_loss=0.1738  Val CpxWA=0.9408',\n'\\n', '[bs=64] Epoch 1: train_loss=0.5031  Val CpxWA=0.8422', '\\n', '[bs=64]\nEpoch 2: train_loss=0.2761  Val CpxWA=0.9206', '\\n', '[bs=64] Epoch 3:\ntrain_loss=0.2082  Val CpxWA=0.9383', '\\n', '[bs=64] Epoch 4: train_loss=0.1887\nVal CpxWA=0.9416', '\\n', '[bs=64] Epoch 5: train_loss=0.1797  Val CpxWA=0.9422',\n'\\n', '[bs=128] Epoch 1: train_loss=0.5209  Val CpxWA=0.8321', '\\n', '[bs=128]\nEpoch 2: train_loss=0.3083  Val CpxWA=0.9195', '\\n', '[bs=128] Epoch 3:\ntrain_loss=0.2322  Val CpxWA=0.9279', '\\n', '[bs=128] Epoch 4: train_loss=0.2034\nVal CpxWA=0.9387', '\\n', '[bs=128] Epoch 5: train_loss=0.1896  Val\nCpxWA=0.9388', '\\n', \"Finished. All results saved to 'working/'.\", '\\n',\n'Execution time: 19 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 255876.62\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 146885.10\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 157798.65\nexamples/s]', '\\n', 'Classes: 2, Train size: 20000', '\\n', 'Vocab size: 17',\n'\\n', '\\n===== Training with emb_dim=16 =====', '\\n', '[emb 16] Epoch 1 |\ntrain_loss=0.5099 | Val CpxWA=0.8000', '\\n', '[emb 16] Epoch 2 |\ntrain_loss=0.3305 | Val CpxWA=0.9106', '\\n', '[emb 16] Epoch 3 |\ntrain_loss=0.2450 | Val CpxWA=0.9251', '\\n', '[emb 16] Epoch 4 |\ntrain_loss=0.2187 | Val CpxWA=0.9330', '\\n', '[emb 16] Epoch 5 |\ntrain_loss=0.2029 | Val CpxWA=0.9381', '\\n', '\\n===== Training with emb_dim=32\n=====', '\\n', '[emb 32] Epoch 1 | train_loss=0.4866 | Val CpxWA=0.8544', '\\n',\n'[emb 32] Epoch 2 | train_loss=0.2708 | Val CpxWA=0.9249', '\\n', '[emb 32] Epoch\n3 | train_loss=0.2103 | Val CpxWA=0.9326', '\\n', '[emb 32] Epoch 4 |\ntrain_loss=0.1932 | Val CpxWA=0.9384', '\\n', '[emb 32] Epoch 5 |\ntrain_loss=0.1837 | Val CpxWA=0.9416', '\\n', '\\n===== Training with emb_dim=64\n=====', '\\n', '[emb 64] Epoch 1 | train_loss=0.4058 | Val CpxWA=0.9257', '\\n',\n'[emb 64] Epoch 2 | train_loss=0.2037 | Val CpxWA=0.9411', '\\n', '[emb 64] Epoch\n3 | train_loss=0.1801 | Val CpxWA=0.9417', '\\n', '[emb 64] Epoch 4 |\ntrain_loss=0.1735 | Val CpxWA=0.9419', '\\n', '[emb 64] Epoch 5 |\ntrain_loss=0.1703 | Val CpxWA=0.9424', '\\n', '\\n===== Training with emb_dim=128\n=====', '\\n', '[emb 128] Epoch 1 | train_loss=0.3441 | Val CpxWA=0.9307', '\\n',\n'[emb 128] Epoch 2 | train_loss=0.1932 | Val CpxWA=0.9416', '\\n', '[emb 128]\nEpoch 3 | train_loss=0.1775 | Val CpxWA=0.9420', '\\n', '[emb 128] Epoch 4 |\ntrain_loss=0.1715 | Val CpxWA=0.9432', '\\n', '[emb 128] Epoch 5 |\ntrain_loss=0.1694 | Val CpxWA=0.9427', '\\n', 'All experiments finished. Data\nsaved to working/experiment_data.npy', '\\n', 'Execution time: a minute seconds\n(time limit is 30 minutes).']"], "analysis": ["", "The training script executed successfully without any errors or bugs. The\nmodel's performance improved across epochs, with validation Complexity-Weighted\nAccuracy (CpxWA) reaching a peak of 0.9432. Early stopping was correctly\ntriggered after no improvement in validation CpxWA for 5 epochs. The script also\nsaved all artifacts in the 'working/' directory as expected. No issues were\nobserved.", "", "", "The execution output indicates that the training script ran successfully without\nany errors or bugs. The model was trained with different embedding dimensions\n(16, 32, 64, 128), and the validation Complexity-Weighted Accuracy (CpxWA)\nimproved steadily across epochs for each embedding dimension. The results were\nsaved successfully, and the script executed within the time limit. No issues\nwere observed."], "exc_type": [null, null, null, null, null], "exc_info": [null, null, null, null, null], "exc_stack": [null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy based on color attributes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9447, "best_value": 0.947}]}]}, {"metric_names": [{"metric_name": "complexity-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by complexity of the task.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9432, "best_value": 0.9494}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by color attributes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9477, "best_value": 0.9491}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by shape attributes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9447, "best_value": 0.949}]}]}, {"metric_names": [{"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by color in predictions.", "data": [{"dataset_name": "training", "final_value": 0.9476, "best_value": 0.9476}, {"dataset_name": "validation", "final_value": 0.9477, "best_value": 0.9477}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by shape in predictions.", "data": [{"dataset_name": "training", "final_value": 0.9446, "best_value": 0.9446}, {"dataset_name": "validation", "final_value": 0.9447, "best_value": 0.9447}]}, {"metric_name": "complexity-weighted accuracy", "lower_is_better": false, "description": "Accuracy weighted by complexity in predictions.", "data": [{"dataset_name": "training", "final_value": 0.9436, "best_value": 0.9436}, {"dataset_name": "validation", "final_value": 0.9432, "best_value": 0.9432}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase.", "data": [{"dataset_name": "training", "final_value": 0.1679, "best_value": 0.1679}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss during training, where lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.1738, "best_value": 0.1738}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.1797, "best_value": 0.1797}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.1896, "best_value": 0.1896}]}, {"metric_name": "training color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during training, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.9458, "best_value": 0.9458}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.9472, "best_value": 0.9472}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.9448, "best_value": 0.9448}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during validation, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.945, "best_value": 0.945}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.9468, "best_value": 0.9468}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.9434, "best_value": 0.9434}]}, {"metric_name": "training shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during training, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.943, "best_value": 0.943}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.9442, "best_value": 0.9442}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.9419, "best_value": 0.9419}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during validation, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.9423, "best_value": 0.9423}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.9437, "best_value": 0.9437}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.9408, "best_value": 0.9408}]}, {"metric_name": "training complexity weighted accuracy", "lower_is_better": false, "description": "The complexity-weighted accuracy during training, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.942, "best_value": 0.942}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.9431, "best_value": 0.9431}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.9407, "best_value": 0.9407}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "The complexity-weighted accuracy during validation, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH (batch_size=32)", "final_value": 0.9408, "best_value": 0.9408}, {"dataset_name": "SPR_BENCH (batch_size=64)", "final_value": 0.9422, "best_value": 0.9422}, {"dataset_name": "SPR_BENCH (batch_size=128)", "final_value": 0.9388, "best_value": 0.9388}]}]}, {"metric_names": [{"metric_name": "color weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy for color classification.", "data": [{"dataset_name": "SPR_BENCH (embedding dimension = 16)", "final_value": 0.9421, "best_value": 0.9421}, {"dataset_name": "SPR_BENCH (embedding dimension = 32)", "final_value": 0.9463, "best_value": 0.9463}, {"dataset_name": "SPR_BENCH (embedding dimension = 64)", "final_value": 0.9469, "best_value": 0.9469}, {"dataset_name": "SPR_BENCH (embedding dimension = 128)", "final_value": 0.9472, "best_value": 0.9472}]}, {"metric_name": "shape weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy for shape classification.", "data": [{"dataset_name": "SPR_BENCH (embedding dimension = 16)", "final_value": 0.9393, "best_value": 0.9393}, {"dataset_name": "SPR_BENCH (embedding dimension = 32)", "final_value": 0.9433, "best_value": 0.9433}, {"dataset_name": "SPR_BENCH (embedding dimension = 64)", "final_value": 0.9438, "best_value": 0.9438}, {"dataset_name": "SPR_BENCH (embedding dimension = 128)", "final_value": 0.9442, "best_value": 0.9442}]}, {"metric_name": "complexity weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy for complexity classification.", "data": [{"dataset_name": "SPR_BENCH (embedding dimension = 16)", "final_value": 0.9381, "best_value": 0.9381}, {"dataset_name": "SPR_BENCH (embedding dimension = 32)", "final_value": 0.9416, "best_value": 0.9416}, {"dataset_name": "SPR_BENCH (embedding dimension = 64)", "final_value": 0.9424, "best_value": 0.9424}, {"dataset_name": "SPR_BENCH (embedding dimension = 128)", "final_value": 0.9427, "best_value": 0.9427}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, used to evaluate model performance.", "data": [{"dataset_name": "SPR_BENCH (embedding dimension = 16)", "final_value": 0.2029, "best_value": 0.2029}, {"dataset_name": "SPR_BENCH (embedding dimension = 32)", "final_value": 0.1837, "best_value": 0.1837}, {"dataset_name": "SPR_BENCH (embedding dimension = 64)", "final_value": 0.1703, "best_value": 0.1703}, {"dataset_name": "SPR_BENCH (embedding dimension = 128)", "final_value": 0.1694, "best_value": 0.1694}]}]}], "is_best_node": [false, true, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/cpxwa_curve.png", "../../logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_training_loss.png", "../../logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_val_weighted_acc.png", "../../logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_cpxwa_train_val.png"], ["../../logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/cpxwa_curve.png", "../../logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_cpxwa_curve.png", "../../logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_cwa_curve.png", "../../logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_swa_curve.png"], ["../../logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/cpxwa_curve.png", "../../logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_val_cpxwa_all_lrs.png", "../../logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_train_loss_all_lrs.png", "../../logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_bestlr_0.003_train_vs_val_cpxwa.png"], ["../../logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/cpxwa_curves.png", "../../logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_loss_comparison.png", "../../logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_val_cpxwa_comparison.png", "../../logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_cpxwa_train_val.png", "../../logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_swa_train_val.png", "../../logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_cwa_train_val.png"], ["../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb16.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb32.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb64.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb128.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_val_cpxwa_curves.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_cpxwa_bar.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_cwa_bar.png", "../../logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_swa_bar.png"]], "plot_paths": [["experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/cpxwa_curve.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_training_loss.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_val_weighted_acc.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_cpxwa_train_val.png"], ["experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/cpxwa_curve.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_loss_curve.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_cpxwa_curve.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_cwa_curve.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_swa_curve.png"], ["experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/cpxwa_curve.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_val_cpxwa_all_lrs.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_train_loss_all_lrs.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_bestlr_0.003_train_vs_val_cpxwa.png"], ["experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/cpxwa_curves.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_loss_comparison.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_val_cpxwa_comparison.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_cpxwa_train_val.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_swa_train_val.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_cwa_train_val.png"], ["experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb16.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb32.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb64.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb128.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_val_cpxwa_curves.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_cpxwa_bar.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_cwa_bar.png", "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_swa_bar.png"]], "plot_analyses": [[{"analysis": "This plot demonstrates the improvement in Complexity-Weighted Accuracy (CpxWA) on the validation set over training epochs. The metric increases steadily from epoch 1 to epoch 3, reaching a plateau around 0.94 at epoch 3. This suggests that the model quickly learns the patterns in the data and achieves stable performance after a few epochs. The lack of significant fluctuation after epoch 3 indicates good convergence and no overfitting issues.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/cpxwa_curve.png"}, {"analysis": "The training loss decreases consistently across epochs, starting from approximately 0.45 and dropping to below 0.2 by epoch 5. This steady decline indicates that the model is effectively learning from the training data without encountering major issues like overfitting or underfitting. The sharp drop in the initial epochs suggests that the model quickly captures the underlying patterns in the data.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_training_loss.png"}, {"analysis": "This plot compares the performance of three metrics\u2014Color-Weighted Accuracy (CWA), Shape-Weighted Accuracy (SWA), and Complexity-Weighted Accuracy (CpxWA)\u2014on the validation set. All three metrics show a similar upward trend, with CWA slightly outperforming SWA and CpxWA. The metrics plateau around epoch 3, suggesting stable and consistent performance across different evaluation criteria. This supports the hypothesis that the model generalizes well to various aspects of the task.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_val_weighted_acc.png"}, {"analysis": "This plot compares Complexity-Weighted Accuracy (CpxWA) between the training and validation sets. Both curves show a similar trend, with rapid improvement in the first three epochs followed by a plateau. The validation CpxWA is slightly lower than the training CpxWA throughout, which is expected and indicates no significant overfitting. The close alignment of the two curves suggests that the model generalizes well from training to validation data.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_0f22f16b890d47f79f71f98524557919_proc_1723010/spr_bench_cpxwa_train_val.png"}], [{"analysis": "This plot illustrates the validation complexity-weighted accuracy (CpxWA) over epochs. The rapid improvement in the initial epochs suggests that the model quickly learns the underlying patterns in the data. After epoch 6, the accuracy plateaus at approximately 94.4%, indicating that the model has likely reached its optimal performance for the current hyperparameter settings. There is no sign of overfitting, as the validation accuracy remains stable.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/cpxwa_curve.png"}, {"analysis": "The loss curve for training shows a steep decrease in the first few epochs, followed by a slower decline as training progresses. The final cross-entropy loss stabilizes around 0.15, which is indicative of convergence. This trend is typical for well-tuned models and suggests that the learning rate and number of epochs are appropriate for the task.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_loss_curve.png"}, {"analysis": "The complexity-weighted accuracy (CpxWA) for both training and validation sets shows a rapid increase in the initial epochs, followed by a plateau. The gap between training and validation accuracy is minimal, indicating that the model generalizes well without significant overfitting. Both metrics stabilize at around 94.5%, which is a strong result.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_cpxwa_curve.png"}, {"analysis": "The color-weighted accuracy (CWA) plot demonstrates similar trends for training and validation. Both curves show rapid improvement in the early epochs, with a slight plateau around epoch 6. The small gap between the training and validation curves further confirms that the model is not overfitting and is performing consistently across datasets.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_cwa_curve.png"}, {"analysis": "The shape-weighted accuracy (SWA) plot follows a similar pattern to the CWA plot. Both training and validation accuracies increase rapidly initially and stabilize around epoch 6. The final accuracy values are approximately 94.5%, with a minimal gap between the training and validation curves, again suggesting good generalization and robust model performance.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_56e5a02fcf6c4877a5b295fa24ee2f2b_proc_1726518/SPR_BENCH_swa_curve.png"}], [{"analysis": "This plot shows the validation Complexity-Weighted Accuracy (CpxWA) over epochs for a learning rate of 0.003. The accuracy improves significantly from epoch 1 to epoch 2, after which the improvement becomes more gradual and plateaus around epoch 4. This indicates that the model quickly learns the patterns in the data and stabilizes. The chosen learning rate appears to be effective in achieving high accuracy within a few epochs.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/cpxwa_curve.png"}, {"analysis": "This plot compares the validation CpxWA for multiple learning rates (1e-4, 3e-4, 0.001, and 0.003) over epochs. The learning rate of 0.003 achieves the highest accuracy, followed by 0.001 and 3e-4. The learning rate of 1e-4 lags significantly behind, indicating that it is too small for effective learning within 5 epochs. This suggests that higher learning rates are more effective for this task, up to a certain threshold.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_val_cpxwa_all_lrs.png"}, {"analysis": "This plot shows the training loss over epochs for different learning rates. The learning rate of 0.003 achieves the fastest and most significant reduction in loss, stabilizing at a low value by epoch 3. Similarly, 0.001 and 3e-4 also show good convergence but at a slower rate. The learning rate of 1e-4 converges much more slowly and retains a higher loss throughout, indicating that it is suboptimal for this task. The results reinforce that 0.003 is the best-performing learning rate in terms of minimizing training loss.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_train_loss_all_lrs.png"}, {"analysis": "This plot compares the train and validation CpxWA for the best learning rate (0.003). Both training and validation accuracies improve rapidly up to epoch 2 and then stabilize, with only minor differences between them. This indicates that the model generalizes well without overfitting, as the validation accuracy closely follows the training accuracy. The learning rate of 0.003 is well-suited for this task, achieving high accuracy and good generalization.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_b637933455464ad9a960f9014a00f2d3_proc_1726519/SPR_BENCH_bestlr_0.003_train_vs_val_cpxwa.png"}], [{"analysis": "This plot demonstrates the relationship between batch size and validation complexity-weighted accuracy (CpxWA) across epochs. Smaller batch sizes (bs=32) achieve higher validation accuracy earlier compared to larger batch sizes (bs=64 and bs=128). However, all batch sizes converge to similar levels of accuracy by epoch 5. This suggests that smaller batch sizes may be more effective in capturing patterns during early training stages, though the final performance is not significantly affected by batch size.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/cpxwa_curves.png"}, {"analysis": "This plot shows the training loss as a function of epochs for different batch sizes. Smaller batch sizes (bs=32) lead to faster loss reduction compared to larger batch sizes (bs=64 and bs=128). By epoch 5, the training loss for all batch sizes converges to similar low values. The faster convergence of smaller batch sizes indicates that they might be more effective during the early training phase, but the overall training efficiency is comparable across batch sizes by the end of training.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_loss_comparison.png"}, {"analysis": "This plot highlights the validation complexity-weighted accuracy (CpxWA) for different batch sizes over epochs. Smaller batch sizes (bs=32) demonstrate higher accuracy in the initial epochs, but all batch sizes converge to a similar accuracy level by epoch 5. This reinforces the idea that smaller batch sizes are advantageous for early-stage learning but do not provide a significant edge in the final performance.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_val_cpxwa_comparison.png"}, {"analysis": "This plot compares the training and validation complexity-weighted accuracy (CpxWA) for a batch size of 32. Both training and validation accuracies improve steadily and converge closely by epoch 5, indicating minimal overfitting and good generalization. The close alignment of training and validation curves suggests a well-balanced model with effective training dynamics for this batch size.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_cpxwa_train_val.png"}, {"analysis": "This plot compares the training and validation shape-weighted accuracy (SWA) for a batch size of 32. Both metrics improve steadily and align closely by epoch 5, indicating effective learning and good generalization. The minimal gap between training and validation SWA curves suggests that the model is not overfitting and is performing consistently across the training and validation datasets.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_swa_train_val.png"}, {"analysis": "This plot compares the training and validation color-weighted accuracy (CWA) for a batch size of 32. Both training and validation accuracies improve steadily, converging by epoch 5. The close match between the curves indicates that the model generalizes well and does not overfit to the training data. The consistent performance across epochs suggests stable training dynamics.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_83d195c90f3140e1ab454e8afc0267a0_proc_1726520/SPR_BENCH_bs32_cwa_train_val.png"}], [{"analysis": "The plot shows the validation Complexity-Weighted Accuracy (CpxWA) over epochs for an embedding dimension of 16. The accuracy increases consistently from epoch 1 to epoch 5, with the most significant improvement occurring between epochs 1 and 2. The curve flattens after epoch 3, indicating diminishing returns in performance improvement. This suggests that early epochs are crucial for learning, and further training beyond epoch 3 provides marginal benefits.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb16.png"}, {"analysis": "This plot represents the validation Complexity-Weighted Accuracy (CpxWA) over epochs for an embedding dimension of 32. There is a rapid increase in accuracy from epoch 1 to epoch 2, followed by a gradual improvement up to epoch 5. The accuracy plateaus after epoch 3, similar to the case with an embedding dimension of 16. This indicates that the model converges early and additional training yields minimal gains.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb32.png"}, {"analysis": "The plot demonstrates the validation Complexity-Weighted Accuracy (CpxWA) over epochs for an embedding dimension of 64. The accuracy improves rapidly between epochs 1 and 2, with a noticeable flattening of the curve after epoch 2. This indicates that the model reaches near-optimal performance early during training, and further epochs provide minimal improvement.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb64.png"}, {"analysis": "This plot shows the validation Complexity-Weighted Accuracy (CpxWA) over epochs for an embedding dimension of 128. The accuracy increases significantly from epoch 1 to epoch 2, and the curve flattens after epoch 3. Interestingly, there is a slight dip in accuracy at epoch 5, which could indicate overfitting or instability in training. This suggests that careful monitoring of training progress is needed to avoid overtraining.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/cpxwa_curve_emb128.png"}, {"analysis": "This plot compares the validation Complexity-Weighted Accuracy (CpxWA) across different embedding dimensions (16, 32, 64, 128) over epochs. All configurations show rapid improvement between epochs 1 and 2, followed by a plateau after epoch 3. Higher embedding dimensions (64 and 128) achieve slightly better final accuracy compared to lower dimensions (16 and 32). However, the differences are minimal, suggesting that increasing embedding dimensions beyond 64 might not provide significant benefits.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_val_cpxwa_curves.png"}, {"analysis": "The bar chart illustrates the final validation Complexity-Weighted Accuracy (CpxWA) for different embedding dimensions. All embedding dimensions achieve similar final accuracy, with only marginal differences. This implies that embedding dimension has minimal impact on the final CpxWA, and smaller dimensions might be preferred for computational efficiency without sacrificing performance.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_cpxwa_bar.png"}, {"analysis": "The bar chart shows the final validation Color-Weighted Accuracy (CWA) for different embedding dimensions. All configurations achieve nearly identical accuracy, indicating that embedding dimension does not significantly affect the model's ability to capture color-related patterns. This suggests that the model's performance on CWA is robust to changes in embedding dimension.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_cwa_bar.png"}, {"analysis": "The bar chart displays the final validation Shape-Weighted Accuracy (SWA) for different embedding dimensions. Similar to the Color-Weighted Accuracy, all embedding dimensions achieve comparable performance. This indicates that the model's ability to capture shape-related patterns is not significantly influenced by the embedding dimension.", "plot_path": "experiments/2025-08-31_14-11-51_symbol_glyph_clustering_attempt_0/logs/0-run/experiment_results/experiment_51c91a0747244169b3db597f04fce1c3_proc_1726521/spr_bench_final_swa_bar.png"}]], "vlm_feedback_summary": ["The plots collectively indicate strong model performance, with rapid learning in\nthe initial epochs and stable accuracy and loss curves thereafter. The model\nachieves consistent results across multiple metrics, with no signs of\noverfitting or underfitting. These results support the hypothesis that the\nproposed approach is effective in improving accuracy and generalization for\nSynthetic PolyRule Reasoning tasks.", "The experimental results show consistent improvement across all metrics, with\nrapid learning in the initial epochs followed by stabilization. The minimal gap\nbetween training and validation metrics indicates good generalization. The model\nachieves strong performance, with accuracies stabilizing at approximately 94.5%\nfor all metrics, suggesting that the current hyperparameter settings are\neffective.", "The plots demonstrate that a learning rate of 0.003 is optimal for achieving\nhigh accuracy and low loss in both training and validation. The model learns\neffectively and generalizes well, with no signs of overfitting. Other learning\nrates, particularly 1e-4, perform significantly worse, indicating that they are\ntoo small for effective learning.", "The plots provide insights into the impact of batch size on training and\nvalidation performance metrics (CpxWA, SWA, and CWA). Smaller batch sizes show\nfaster convergence during early epochs but do not significantly affect final\nperformance. The close alignment of training and validation curves indicates\ngood generalization and minimal overfitting across all metrics.", "The plots reveal consistent trends in validation accuracy metrics across\ndifferent embedding dimensions. The model achieves rapid improvement in early\nepochs, with diminishing returns after epoch 3. Higher embedding dimensions\nprovide marginal benefits, but the differences are minimal, suggesting that\nsmaller dimensions suffice for optimal performance. The results highlight\nrobustness in both color- and shape-weighted accuracy metrics across embedding\ndimensions."], "exec_time": [36.14336061477661, 55.390777826309204, 23.07490849494934, 19.99769639968872, 70.95214772224426], "exec_time_feedback": ["", "", "", "", ""], "datasets_successfully_tested": [["['Synthetic PolyRule Reasoning']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------------------------------------------------------------\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n    train_metrics = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_metrics = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n\n    # ------------ 1) Training loss curve -----------------------\n    try:\n        if train_losses:\n            plt.figure()\n            plt.plot(epochs, train_losses, marker=\"o\", label=\"Train\")\n            plt.title(f\"{ds_name} Dataset \u2013 Training Loss Curve\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name.lower()}_training_loss.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating training-loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ------------ 2) Validation weighted-accuracy metrics ------\n    try:\n        if val_metrics:\n            cwa = [m[\"cwa\"] for m in val_metrics]\n            swa = [m[\"swa\"] for m in val_metrics]\n            cpx = [m[\"cpx\"] for m in val_metrics]\n            plt.figure()\n            plt.plot(epochs, cwa, marker=\"o\", label=\"CWA\")\n            plt.plot(epochs, swa, marker=\"s\", label=\"SWA\")\n            plt.plot(epochs, cpx, marker=\"^\", label=\"CpxWA\")\n            plt.title(f\"{ds_name} Dataset \u2013 Validation Weighted-Accuracy Curves\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name.lower()}_val_weighted_acc.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating validation-metrics plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ------------ 3) Train vs Val CpxWA ------------------------\n    try:\n        if train_metrics and val_metrics:\n            train_cpx = [m[\"cpx\"] for m in train_metrics]\n            val_cpx = [m[\"cpx\"] for m in val_metrics]\n            plt.figure()\n            plt.plot(epochs, train_cpx, marker=\"o\", label=\"Train CpxWA\")\n            plt.plot(epochs, val_cpx, marker=\"s\", label=\"Val CpxWA\")\n            plt.title(f\"{ds_name} Dataset \u2013 CpxWA Train vs. Val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"CpxWA\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name.lower()}_cpxwa_train_val.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating CpxWA comparison plot for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment data ---------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"epochs_tuning\"][\"SPR_BENCH\"]\n    epochs = exp[\"epochs\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit(0)\n\n\n# helper to fetch series safely\ndef get_series(key, split):\n    return (\n        [m[key] if m is not None else None for m in exp[\"metrics\"][split]]\n        if key in exp[\"metrics\"][split][0]\n        else []\n    )\n\n\n# --------- 1. Loss curve ---------------\ntry:\n    plt.figure()\n    tr_loss = exp[\"losses\"][\"train\"]\n    val_loss = exp[\"losses\"][\"val\"]\n    plt.plot(epochs, tr_loss, label=\"Train\")\n    if any(v is not None for v in val_loss):\n        plt.plot(\n            epochs, [v if v is not None else np.nan for v in val_loss], label=\"Val\"\n        )\n    plt.title(\"SPR_BENCH: Loss Curve (Train vs Validation)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss curve: {e}\")\n    plt.close()\n\n# --------- 2. Complexity-Weighted Acc ----------------\ntry:\n    plt.figure()\n    tr = [m[\"cpx\"] for m in exp[\"metrics\"][\"train\"]]\n    val = [m[\"cpx\"] for m in exp[\"metrics\"][\"val\"]]\n    plt.plot(epochs, tr, label=\"Train\")\n    plt.plot(epochs, val, label=\"Val\")\n    plt.title(\"SPR_BENCH: Complexity-Weighted Accuracy (CpxWA)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CpxWA\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_cpxwa_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CpxWA curve: {e}\")\n    plt.close()\n\n# --------- 3. Color-Weighted Acc ---------------------\ntry:\n    plt.figure()\n    tr = [m[\"cwa\"] for m in exp[\"metrics\"][\"train\"]]\n    val = [m[\"cwa\"] for m in exp[\"metrics\"][\"val\"]]\n    plt.plot(epochs, tr, label=\"Train\")\n    plt.plot(epochs, val, label=\"Val\")\n    plt.title(\"SPR_BENCH: Color-Weighted Accuracy (CWA)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CWA\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_cwa_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA curve: {e}\")\n    plt.close()\n\n# --------- 4. Shape-Weighted Acc ---------------------\ntry:\n    plt.figure()\n    tr = [m[\"swa\"] for m in exp[\"metrics\"][\"train\"]]\n    val = [m[\"swa\"] for m in exp[\"metrics\"][\"val\"]]\n    plt.plot(epochs, tr, label=\"Train\")\n    plt.plot(epochs, val, label=\"Val\")\n    plt.title(\"SPR_BENCH: Shape-Weighted Accuracy (SWA)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"SWA\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_swa_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating SWA curve: {e}\")\n    plt.close()\n\nprint(f\"Finished plotting. Figures saved to {working_dir}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------------------------------------------------------------------\n# gather lr keys and compute best lr via final val cpx\nlr_dict = experiment_data.get(\"learning_rate\", {})\nbest_lr_key, best_cpx = None, -float(\"inf\")\nfor lr_key, lr_data in lr_dict.items():\n    val_cpx_curve = [m[\"cpx\"] for m in lr_data[\"SPR_BENCH\"][\"metrics\"][\"val\"]]\n    if val_cpx_curve and val_cpx_curve[-1] > best_cpx:\n        best_cpx = val_cpx_curve[-1]\n        best_lr_key = lr_key\n\n# ---------------------------------------------------------------------\n# PLOT 1: Validation CpxWA curves for all lrs\ntry:\n    plt.figure()\n    for lr_key, lr_data in lr_dict.items():\n        epochs = lr_data[\"SPR_BENCH\"][\"epochs\"]\n        val_cpx = [m[\"cpx\"] for m in lr_data[\"SPR_BENCH\"][\"metrics\"][\"val\"]]\n        plt.plot(epochs, val_cpx, marker=\"o\", label=f\"lr={lr_key}\")\n    plt.title(\n        \"SPR_BENCH \u2013 Validation Complexity-WA\\nLeft: Multiple LRs, Right: Accuracy over Epochs\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CpxWA\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_cpxwa_all_lrs.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Validation CpxWA plot: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# PLOT 2: Training loss curves for all lrs\ntry:\n    plt.figure()\n    for lr_key, lr_data in lr_dict.items():\n        epochs = lr_data[\"SPR_BENCH\"][\"epochs\"]\n        train_loss = lr_data[\"SPR_BENCH\"][\"losses\"][\"train\"]\n        plt.plot(epochs, train_loss, marker=\"o\", label=f\"lr={lr_key}\")\n    plt.title(\n        \"SPR_BENCH \u2013 Training Loss\\nLeft: All Learning Rates, Right: Loss over Epochs\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_train_loss_all_lrs.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Training Loss plot: {e}\")\n    plt.close()\n\n# ---------------------------------------------------------------------\n# PLOT 3: Train vs Val CpxWA for the best lr\nif best_lr_key is not None:\n    try:\n        best_data = lr_dict[best_lr_key][\"SPR_BENCH\"]\n        epochs = best_data[\"epochs\"]\n        val_cpx = [m[\"cpx\"] for m in best_data[\"metrics\"][\"val\"]]\n        train_cpx = [m[\"cpx\"] for m in best_data[\"metrics\"][\"train\"]]\n        plt.figure()\n        plt.plot(epochs, train_cpx, marker=\"o\", label=\"Train CpxWA\")\n        plt.plot(epochs, val_cpx, marker=\"s\", label=\"Val CpxWA\")\n        plt.title(f\"SPR_BENCH \u2013 Train vs Val CpxWA (Best lr={best_lr_key})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CpxWA\")\n        plt.legend()\n        fname = os.path.join(\n            working_dir, f\"SPR_BENCH_bestlr_{best_lr_key}_train_vs_val_cpxwa.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Best-LR CpxWA plot: {e}\")\n        plt.close()\n\n# ---------------------------------------------------------------------\n# Print key metric\nif best_lr_key is not None:\n    print(f\"Best learning rate: {best_lr_key}, final Val CpxWA: {best_cpx:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    bench = experiment_data[\"batch_size\"][\"SPR_BENCH\"]\n    bs_list = sorted(bench.keys())\n    epochs_dict = {bs: bench[bs][\"epochs\"] for bs in bs_list}\n\n    # 1) Training loss comparison\n    try:\n        plt.figure()\n        for bs in bs_list:\n            plt.plot(\n                epochs_dict[bs],\n                bench[bs][\"losses\"][\"train\"],\n                marker=\"o\",\n                label=f\"bs={bs}\",\n            )\n        plt.title(\n            \"SPR_BENCH \u2013 Training Loss vs Epochs\\nLeft: Smaller bs, Right: Larger bs\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_comparison.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss comparison: {e}\")\n        plt.close()\n\n    # 2) Validation CpxWA across batch sizes\n    try:\n        plt.figure()\n        for bs in bs_list:\n            cpx = [m[\"cpx\"] for m in bench[bs][\"metrics\"][\"val\"]]\n            plt.plot(epochs_dict[bs], cpx, marker=\"o\", label=f\"bs={bs}\")\n        plt.title(\n            \"SPR_BENCH \u2013 Validation Complexity-Weighted Accuracy\\nHigher is better\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CpxWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_cpxwa_comparison.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CpxWA comparison: {e}\")\n        plt.close()\n\n    # choose one batch size for detailed train/val plots\n    focus_bs = 32 if 32 in bs_list else bs_list[0]\n    focus_epochs = epochs_dict[focus_bs]\n    tr_metrics = bench[focus_bs][\"metrics\"][\"train\"]\n    val_metrics = bench[focus_bs][\"metrics\"][\"val\"]\n\n    # helper to extract metric series\n    def series(key, split):\n        return [m[key] for m in (tr_metrics if split == \"train\" else val_metrics)]\n\n    # 3) Train vs Val CpxWA\n    try:\n        plt.figure()\n        plt.plot(focus_epochs, series(\"cpx\", \"train\"), marker=\"o\", label=\"Train\")\n        plt.plot(focus_epochs, series(\"cpx\", \"val\"), marker=\"s\", label=\"Validation\")\n        plt.title(f\"SPR_BENCH \u2013 Train vs Val CpxWA (bs={focus_bs})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CpxWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_bs{focus_bs}_cpxwa_train_val.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CpxWA train/val: {e}\")\n        plt.close()\n\n    # 4) Train vs Val SWA\n    try:\n        plt.figure()\n        plt.plot(focus_epochs, series(\"swa\", \"train\"), marker=\"o\", label=\"Train\")\n        plt.plot(focus_epochs, series(\"swa\", \"val\"), marker=\"s\", label=\"Validation\")\n        plt.title(f\"SPR_BENCH \u2013 Train vs Val Shape-Weighted Acc (bs={focus_bs})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"SWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_bs{focus_bs}_swa_train_val.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA train/val: {e}\")\n        plt.close()\n\n    # 5) Train vs Val CWA\n    try:\n        plt.figure()\n        plt.plot(focus_epochs, series(\"cwa\", \"train\"), marker=\"o\", label=\"Train\")\n        plt.plot(focus_epochs, series(\"cwa\", \"val\"), marker=\"s\", label=\"Validation\")\n        plt.title(f\"SPR_BENCH \u2013 Train vs Val Color-Weighted Acc (bs={focus_bs})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"SPR_BENCH_bs{focus_bs}_cwa_train_val.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA train/val: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper: collect per-emb_dim info\nruns = experiment_data.get(\"emb_dim\", {})\nemb_dims = sorted(runs.keys(), key=lambda x: int(x))\nepochs_dict, val_cpx_curves = {}, {}\nfinal_cpx, final_cwa, final_swa = [], [], []\n\nfor emb in emb_dims:\n    log = runs[emb][\"SPR_BENCH\"]\n    epochs = log[\"epochs\"]\n    val_metrics = log[\"metrics\"][\"val\"]\n    cpx_curve = [m[\"cpx\"] for m in val_metrics]\n    epochs_dict[emb] = epochs\n    val_cpx_curves[emb] = cpx_curve\n    final_cpx.append(cpx_curve[-1])\n    final_cwa.append(val_metrics[-1][\"cwa\"])\n    final_swa.append(val_metrics[-1][\"swa\"])\n\n# 1) Validation CpxWA curves\ntry:\n    plt.figure()\n    for emb in emb_dims:\n        plt.plot(epochs_dict[emb], val_cpx_curves[emb], marker=\"o\", label=f\"emb={emb}\")\n    plt.title(\"SPR_BENCH: Validation Complexity-WA vs Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CpxWA\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"spr_bench_val_cpxwa_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CpxWA curve plot: {e}\")\n    plt.close()\n\n# 2) Final CpxWA bar chart\ntry:\n    plt.figure()\n    plt.bar(emb_dims, final_cpx)\n    plt.title(\"SPR_BENCH: Final Validation Complexity-WA by Embedding Dim\")\n    plt.xlabel(\"Embedding Dim\")\n    plt.ylabel(\"Final CpxWA\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"spr_bench_final_cpxwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final CpxWA bar plot: {e}\")\n    plt.close()\n\n# 3) Final CWA bar chart\ntry:\n    plt.figure()\n    plt.bar(emb_dims, final_cwa, color=\"orange\")\n    plt.title(\"SPR_BENCH: Final Validation Color-WA by Embedding Dim\")\n    plt.xlabel(\"Embedding Dim\")\n    plt.ylabel(\"Final CWA\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"spr_bench_final_cwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final CWA bar plot: {e}\")\n    plt.close()\n\n# 4) Final SWA bar chart\ntry:\n    plt.figure()\n    plt.bar(emb_dims, final_swa, color=\"green\")\n    plt.title(\"SPR_BENCH: Final Validation Shape-WA by Embedding Dim\")\n    plt.xlabel(\"Embedding Dim\")\n    plt.ylabel(\"Final SWA\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"spr_bench_final_swa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final SWA bar plot: {e}\")\n    plt.close()\n\nprint(\"Plotting complete.\")\n"], "plot_plan": ["We will load the saved experiment_data.npy, loop through the dataset keys (only \u201cSPR_BENCH\u201d here), and extract epochs, loss arrays, and metric dictionaries. Three concise visualizations are produced (\u2264 5 total): (1) training-loss curve, (2) validation curves for the three weighted-accuracy metrics (CWA, SWA, CpxWA) on one figure, and (3) a comparison plot of train vs. val CpxWA. Each plot lives inside its own try-except so errors never break the script, and every figure is saved to working_dir with descriptive names that include the dataset and plot type. We respect the requirement to close figures after saving, to use only the data that actually exists, and to avoid overcrowding by limiting the number of figures. All logic is wrapped in a single compact Python snippet that can be run as-is.", "We will load the stored numpy dictionary, focus on the single run saved under epochs_tuning \u2192 SPR_BENCH, and pull out the epoch list, per-epoch losses, and per-epoch weighted accuracies (cwa, swa, cpx) for both train and validation. Then, in up to four independent try-except blocks, we will generate: (1) the loss curve, (2) CpxWA curve, (3) CWA curve, and (4) SWA curve. Each figure will show both train and validation series when the data exist, include axis labels, a legend, and a descriptive title containing the dataset name and metric. After saving to the working directory with a descriptive filename, we will always close the figure to free resources. Should any plot fail to render because some data are missing (e.g., validation loss list is all None), the error will be caught and printed while the script proceeds to the next plot. We respect the guideline to create no more than five figures and use only data present in experiment_data.npy. Finally, the script prints a short completion message so users know where the artifacts live.", "We first load the saved experiment_data.npy, locate every learning-rate sub-dictionary, and extract the common fields: epochs, training losses, and per-epoch validation metrics (cwa, swa, cpx). We then identify the best learning rate by looking at the last epoch\u2019s validation cpx value. To stay within the \u201c\u22645 plots\u201d rule, we create three figures: (1) one multi-line plot comparing validation Complexity-Weighted Accuracy across learning rates, (2) one multi-line plot comparing training loss across learning rates, and (3) a two-line plot of the best learning rate\u2019s train vs. validation cpx curves. Each plot is wrapped in an independent try-except to avoid cascading failures, explicitly closed afterwards, titled with the dataset name and a clear subtitle, and saved to working_dir with descriptive filenames. After plotting, we print the best learning rate and its final validation CpxWA so the notebook/user can immediately see the key quantitative result. No synthetic numbers are generated\u2014everything is read directly from experiment_data.npy. The entire script is short, self-contained, and begins with the mandated imports and working_dir definition.", "We first load experiment_data.npy, ensure working_dir exists, and pick out the only dataset (SPR_BENCH) and the three tried batch sizes. We then create up to five standard figures: (1) a comparison of training loss across batch sizes, (2) validation complexity-weighted accuracy across batch sizes, and for one illustrative batch size (bs=32) we show (3) train vs validation CpxWA, (4) train vs validation shape-weighted accuracy, and (5) train vs validation color-weighted accuracy. Each figure is wrapped in its own try-except block, given a descriptive title and subtitle, saved to working_dir with self-describing file names, and the figure is closed whether or not an error occurs. We never fabricate data\u2014everything is read directly from the numpy file. We sample only one batch size for the per-metric train/val plots so that the total number of figures does not exceed five. The code starts with the required imports, follows basic matplotlib usage, and prints a short confirmation for each saved plot. Finally, nothing beyond the plots is produced.", "We will first load experiment_data.npy and gather the list of embedding-dimension experiments. For each run we extract the epoch list, the per-epoch training/validation complexity-weighted accuracy (CpxWA), and the final\u2010epoch values of CpxWA, color-weighted accuracy (CWA) and shape-weighted accuracy (SWA). We then create at most four plots: (1) a multi-line figure showing the validation CpxWA curve versus epoch for every emb_dim; (2) a bar chart of the final-epoch CpxWA for each emb_dim; (3) a bar chart of final-epoch CWA; and (4) a bar chart of final-epoch SWA. Every plotting section is wrapped in its own try/except block, each figure is titled clearly (mentioning dataset \u201cSPR_BENCH\u201d), saved with descriptive names in working_dir, and closed afterwards. No data are fabricated\u2014everything comes directly from experiment_data.npy. The code remains concise, obeys the required imports/working_dir setup, and prints a short confirmation after saving each plot."], "ablation_name": [null, null, null, null, null], "hyperparam_name": [null, "EPOCHS", "learning_rate", "batch_size", "emb_dim"], "is_seed_node": [false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false], "parse_metrics_plan": ["The script will locate the working directory that the original training script\nused, load experiment_data.npy, and iterate over each stored dataset (e.g.,\n\u201cSPR_BENCH\u201d).   For every dataset it will look at the recorded lists of metrics\nand losses, determine the best value for each metric (highest for accuracies,\nlowest for losses), and then print them with explicit names such as \u201ctrain\ncolor-weighted accuracy\u201d or \u201cvalidation loss\u201d.   No figures are generated, no\nspecial entry-point guard is used, and all code runs immediately upon execution.", "The solution loads the saved numpy file from the working directory, navigates\nthe nested dictionary to fetch metric histories, derives the best (maximum for\nevery accuracy\u2010type metric and minimum for loss) or, where appropriate, final\nvalues, and prints them in a clear, labelled manner for the single dataset\n(SPR_BENCH). It follows the requested structure\u2014no entry-point guard, no plots,\nand explicit metric labels.", "We will load the numpy file, convert it back to a Python dict, and then inspect\nthe nested structure (learning-rate \u2192 dataset \u2192 metrics / losses).   For each\nlearning-rate configuration we will extract the training and validation lists of\nmetrics and losses, pick the best value for each metric (maximum for accuracies,\nminimum for losses), and print them with explicit, fully-qualified names.\nDataset names are printed first, followed by the clearly-labeled metric values.\nNo plotting or special entry points are used; the script runs immediately at\nimport.", "The script loads the saved experiment_data.npy dictionary, iterates through each\nbatch-size run contained in the SPR_BENCH entry, and fetches the final-epoch\nstatistics. For every run it prints the dataset name (annotated with its batch\nsize) followed by clearly labelled final values for training loss,\ntraining/validation color-weighted accuracy, shape-weighted accuracy, and\ncomplexity-weighted accuracy. The code executes immediately\u2014nothing is hidden\nbehind an `if __name__ == \"__main__\":` guard, and no plots are generated.", "The script will load the saved NumPy file from the working directory, decode it\nback to a regular Python dictionary, and then iterate over each embedding-\ndimension run stored under the \u201cSPR_BENCH\u201d dataset. For every run it simply\ngrabs the last (i.e., final) epoch entry for each logged metric and prints them\nwith explicit, descriptive labels such as \u201ctrain color weighted accuracy\u201d or\n\u201cvalidation complexity weighted accuracy.\u201d Because the validation loss was not\nstored, only the training loss is reported. All code executes immediately at\nimport time, with no special entry point."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper functions\ndef best(values, higher_is_better=True):\n    \"\"\"Return best (max or min) value from a list, ignoring Nones.\"\"\"\n    values = [v for v in values if v is not None]\n    if not values:  # if list is empty after removing None\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# ---------------------------------------------------------------------\nfor dset_name, dset_info in experiment_data.items():\n    print(f\"\\nDataset: {dset_name}\")\n\n    # --- Accuracy-type metrics -------------------------------------------------\n    for split, split_name in [(\"train\", \"train\"), (\"val\", \"validation\")]:\n        cwa_vals = [m[\"cwa\"] for m in dset_info[\"metrics\"][split]]\n        swa_vals = [m[\"swa\"] for m in dset_info[\"metrics\"][split]]\n        cpx_vals = [m[\"cpx\"] for m in dset_info[\"metrics\"][split]]\n\n        best_cwa = best(cwa_vals, higher_is_better=True)\n        best_swa = best(swa_vals, higher_is_better=True)\n        best_cpx = best(cpx_vals, higher_is_better=True)\n\n        print(f\"{split_name} color-weighted accuracy: {best_cwa:.4f}\")\n        print(f\"{split_name} shape-weighted  accuracy: {best_swa:.4f}\")\n        print(f\"{split_name} complexity-weighted accuracy: {best_cpx:.4f}\")\n\n    # --- Losses ----------------------------------------------------------------\n    train_losses = dset_info[\"losses\"][\"train\"]\n    val_losses = dset_info[\"losses\"][\"val\"]\n\n    best_train_loss = best(train_losses, higher_is_better=False)\n    best_val_loss = best(val_losses, higher_is_better=False)\n\n    if best_train_loss is not None:\n        print(f\"train loss: {best_train_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment results\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to extract \u201cbest\u201d or \u201cfinal\u201d metric values\n# ---------------------------------------------------------------------\ndef best_val(values, maximize=True):\n    \"\"\"Return the best (max or min) non-None value from a list.\"\"\"\n    values = [v for v in values if v is not None]\n    if not values:  # Fallback if list is empty / all None\n        return None\n    return max(values) if maximize else min(values)\n\n\ndef final_val(values):\n    \"\"\"Return the last non-None value from a list.\"\"\"\n    for v in reversed(values):\n        if v is not None:\n            return v\n    return None\n\n\n# ---------------------------------------------------------------------\n# Iterate through experiments and print metrics\n# ---------------------------------------------------------------------\nfor exp_group, datasets in experiment_data.items():  # \"epochs_tuning\"\n    for dataset_name, record in datasets.items():  # \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # Accuracy-type metrics ------------------------------------------------\n        train_metrics = record[\"metrics\"][\"train\"]\n        val_metrics = record[\"metrics\"][\"val\"]\n\n        # Collect lists of individual metric values\n        train_cwa = [m[\"cwa\"] for m in train_metrics]\n        val_cwa = [m[\"cwa\"] for m in val_metrics]\n        train_swa = [m[\"swa\"] for m in train_metrics]\n        val_swa = [m[\"swa\"] for m in val_metrics]\n        train_cpx = [m[\"cpx\"] for m in train_metrics]\n        val_cpx = [m[\"cpx\"] for m in val_metrics]\n\n        # Best accuracies (higher is better)\n        print(f\"Best train complexity-weighted accuracy: {best_val(train_cpx):.4f}\")\n        print(f\"Best validation complexity-weighted accuracy: {best_val(val_cpx):.4f}\")\n\n        print(f\"Best train color-weighted accuracy: {best_val(train_cwa):.4f}\")\n        print(f\"Best validation color-weighted accuracy: {best_val(val_cwa):.4f}\")\n\n        print(f\"Best train shape-weighted accuracy: {best_val(train_swa):.4f}\")\n        print(f\"Best validation shape-weighted accuracy: {best_val(val_swa):.4f}\")\n\n        # Losses --------------------------------------------------------------\n        train_losses = record[\"losses\"][\"train\"]\n        print(f\"Final train loss: {final_val(train_losses):.4f}\")\n\n        # Validation loss was not recorded in the original script (all None),\n        # so it is intentionally omitted.\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# helper functions ----------------------------------------------------\ndef best(series, higher_is_better=True):\n    \"\"\"Return the best (max or min) value in a list, skipping None.\"\"\"\n    series = [v for v in series if v is not None]\n    if not series:\n        return None\n    return max(series) if higher_is_better else min(series)\n\n\n# mapping from short metric keys to verbose descriptions + direction\nmetric_info = {\n    \"cwa\": (\"color-weighted accuracy\", True),\n    \"swa\": (\"shape-weighted accuracy\", True),\n    \"cpx\": (\"complexity-weighted accuracy\", True),\n}\n\n# ---------------------------------------------------------------------\n# iterate over learning-rate configurations ---------------------------\nfor lr_key, lr_block in experiment_data[\"learning_rate\"].items():\n    for dset_name, dset_block in lr_block.items():\n        print(f\"{dset_name} (learning rate {lr_key})\")\n\n        # process weighted-accuracy metrics\n        for split in (\"train\", \"val\"):\n            split_name = \"training\" if split == \"train\" else \"validation\"\n            metrics_list = dset_block[\"metrics\"][split]\n            # collect per-metric series\n            series_by_metric = {\n                m: [step[m] for step in metrics_list] for m in metric_info\n            }\n            for m_key, values in series_by_metric.items():\n                m_desc, higher = metric_info[m_key]\n                value = best(values, higher_is_better=higher)\n                print(f\"{split_name} {m_desc}: {value:.4f}\")\n\n        # process losses\n        for split in (\"train\", \"val\"):\n            split_name = \"training\" if split == \"train\" else \"validation\"\n            losses = dset_block[\"losses\"][split]\n            best_loss = best(losses, higher_is_better=False)\n            # skip if no loss recorded (e.g., validation loss list may be all None)\n            if best_loss is not None:\n                print(f\"{split_name} loss: {best_loss:.4f}\")\n\n        print()  # blank line between learning-rate sections\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the experiment results\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper for nice number formatting\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Traverse structure and print final metrics\nfor dataset_name, dataset_runs in experiment_data[\"batch_size\"].items():\n    for batch_size, run in dataset_runs.items():\n        # Dataset identifier with batch-size context\n        run_id = f\"{dataset_name} (batch_size={batch_size})\"\n        print(run_id)\n\n        # Final-epoch metrics\n        final_train_metrics = run[\"metrics\"][\"train\"][-1]\n        final_val_metrics = run[\"metrics\"][\"val\"][-1]\n        final_train_loss = run[\"losses\"][\"train\"][-1]\n\n        # Print loss\n        print(\"  training loss:\", fmt(final_train_loss))\n\n        # Print weighted accuracies\n        print(\"  training color weighted accuracy:\", fmt(final_train_metrics[\"cwa\"]))\n        print(\"  validation color weighted accuracy:\", fmt(final_val_metrics[\"cwa\"]))\n\n        print(\"  training shape weighted accuracy:\", fmt(final_train_metrics[\"swa\"]))\n        print(\"  validation shape weighted accuracy:\", fmt(final_val_metrics[\"swa\"]))\n\n        print(\n            \"  training complexity weighted accuracy:\", fmt(final_train_metrics[\"cpx\"])\n        )\n        print(\n            \"  validation complexity weighted accuracy:\", fmt(final_val_metrics[\"cpx\"])\n        )\n\n        # Blank line between runs for readability\n        print()\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(\n        f\"Could not find '{exp_file}'. Make sure the experiments have been run.\"\n    )\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# Extract and print final metrics\n# The structure is: experiment_data[\"emb_dim\"][emb_dim_str][\"SPR_BENCH\"] -> log dict\nfor emb_dim_str, dataset_dict in experiment_data.get(\"emb_dim\", {}).items():\n    for dataset_name, log in dataset_dict.items():\n        # Get final (last epoch) metrics\n        final_train_metrics = log[\"metrics\"][\"train\"][-1]\n        final_val_metrics = log[\"metrics\"][\"val\"][-1]\n        final_train_loss = log[\"losses\"][\"train\"][-1]\n\n        # Print results\n        print(f\"{dataset_name} (embedding dimension = {emb_dim_str})\")\n        print(f\"train color weighted accuracy: {final_train_metrics['cwa']:.4f}\")\n        print(f\"validation color weighted accuracy: {final_val_metrics['cwa']:.4f}\")\n        print(f\"train shape weighted accuracy: {final_train_metrics['swa']:.4f}\")\n        print(f\"validation shape weighted accuracy: {final_val_metrics['swa']:.4f}\")\n        print(f\"train complexity weighted accuracy: {final_train_metrics['cpx']:.4f}\")\n        print(\n            f\"validation complexity weighted accuracy: {final_val_metrics['cpx']:.4f}\"\n        )\n        print(f\"training loss: {final_train_loss:.4f}\\n\")\n"], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'train color-weighted accuracy: 0.9470', '\\n',\n'train shape-weighted  accuracy: 0.9472', '\\n', 'train complexity-weighted\naccuracy: 0.9467', '\\n', 'validation color-weighted accuracy: 0.9447', '\\n',\n'validation shape-weighted  accuracy: 0.9419', '\\n', 'validation complexity-\nweighted accuracy: 0.9402', '\\n', 'train loss: 0.1808', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best train complexity-weighted accuracy: 0.9494',\n'\\n', 'Best validation complexity-weighted accuracy: 0.9432', '\\n', 'Best train\ncolor-weighted accuracy: 0.9491', '\\n', 'Best validation color-weighted\naccuracy: 0.9477', '\\n', 'Best train shape-weighted accuracy: 0.9490', '\\n',\n'Best validation shape-weighted accuracy: 0.9447', '\\n', 'Final train loss:\n0.1629', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH (learning rate 1e-04)', '\\n', 'training color-weighted accuracy:\n0.8079', '\\n', 'training shape-weighted accuracy: 0.8087', '\\n', 'training\ncomplexity-weighted accuracy: 0.8038', '\\n', 'validation color-weighted\naccuracy: 0.8145', '\\n', 'validation shape-weighted accuracy: 0.8153', '\\n',\n'validation complexity-weighted accuracy: 0.8102', '\\n', 'training loss:\n0.4375', '\\n', '\\n', 'SPR_BENCH (learning rate 3e-04)', '\\n', 'training color-\nweighted accuracy: 0.9214', '\\n', 'training shape-weighted accuracy: 0.9211',\n'\\n', 'training complexity-weighted accuracy: 0.9207', '\\n', 'validation color-\nweighted accuracy: 0.9218', '\\n', 'validation shape-weighted accuracy: 0.9206',\n'\\n', 'validation complexity-weighted accuracy: 0.9202', '\\n', 'training loss:\n0.2751', '\\n', '\\n', 'SPR_BENCH (learning rate 0.001)', '\\n', 'training color-\nweighted accuracy: 0.9466', '\\n', 'training shape-weighted accuracy: 0.9434',\n'\\n', 'training complexity-weighted accuracy: 0.9424', '\\n', 'validation color-\nweighted accuracy: 0.9459', '\\n', 'validation shape-weighted accuracy: 0.9428',\n'\\n', 'validation complexity-weighted accuracy: 0.9411', '\\n', 'training loss:\n0.1775', '\\n', '\\n', 'SPR_BENCH (learning rate 0.003)', '\\n', 'training color-\nweighted accuracy: 0.9476', '\\n', 'training shape-weighted accuracy: 0.9446',\n'\\n', 'training complexity-weighted accuracy: 0.9436', '\\n', 'validation color-\nweighted accuracy: 0.9477', '\\n', 'validation shape-weighted accuracy: 0.9447',\n'\\n', 'validation complexity-weighted accuracy: 0.9432', '\\n', 'training loss:\n0.1679', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH (batch_size=32)', '\\n', '  training loss:', ' ', '0.1738', '\\n', '\ntraining color weighted accuracy:', ' ', '0.9458', '\\n', '  validation color\nweighted accuracy:', ' ', '0.9450', '\\n', '  training shape weighted accuracy:',\n' ', '0.9430', '\\n', '  validation shape weighted accuracy:', ' ', '0.9423',\n'\\n', '  training complexity weighted accuracy:', ' ', '0.9420', '\\n', '\nvalidation complexity weighted accuracy:', ' ', '0.9408', '\\n', '\\n', 'SPR_BENCH\n(batch_size=64)', '\\n', '  training loss:', ' ', '0.1797', '\\n', '  training\ncolor weighted accuracy:', ' ', '0.9472', '\\n', '  validation color weighted\naccuracy:', ' ', '0.9468', '\\n', '  training shape weighted accuracy:', ' ',\n'0.9442', '\\n', '  validation shape weighted accuracy:', ' ', '0.9437', '\\n', '\ntraining complexity weighted accuracy:', ' ', '0.9431', '\\n', '  validation\ncomplexity weighted accuracy:', ' ', '0.9422', '\\n', '\\n', 'SPR_BENCH\n(batch_size=128)', '\\n', '  training loss:', ' ', '0.1896', '\\n', '  training\ncolor weighted accuracy:', ' ', '0.9448', '\\n', '  validation color weighted\naccuracy:', ' ', '0.9434', '\\n', '  training shape weighted accuracy:', ' ',\n'0.9419', '\\n', '  validation shape weighted accuracy:', ' ', '0.9408', '\\n', '\ntraining complexity weighted accuracy:', ' ', '0.9407', '\\n', '  validation\ncomplexity weighted accuracy:', ' ', '0.9388', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH (embedding dimension = 16)', '\\n', 'train color weighted accuracy:\n0.9427', '\\n', 'validation color weighted accuracy: 0.9421', '\\n', 'train shape\nweighted accuracy: 0.9431', '\\n', 'validation shape weighted accuracy: 0.9393',\n'\\n', 'train complexity weighted accuracy: 0.9428', '\\n', 'validation complexity\nweighted accuracy: 0.9381', '\\n', 'training loss: 0.2029\\n', '\\n', 'SPR_BENCH\n(embedding dimension = 32)', '\\n', 'train color weighted accuracy: 0.9474',\n'\\n', 'validation color weighted accuracy: 0.9463', '\\n', 'train shape weighted\naccuracy: 0.9477', '\\n', 'validation shape weighted accuracy: 0.9433', '\\n',\n'train complexity weighted accuracy: 0.9473', '\\n', 'validation complexity\nweighted accuracy: 0.9416', '\\n', 'training loss: 0.1837\\n', '\\n', 'SPR_BENCH\n(embedding dimension = 64)', '\\n', 'train color weighted accuracy: 0.9481',\n'\\n', 'validation color weighted accuracy: 0.9469', '\\n', 'train shape weighted\naccuracy: 0.9481', '\\n', 'validation shape weighted accuracy: 0.9438', '\\n',\n'train complexity weighted accuracy: 0.9482', '\\n', 'validation complexity\nweighted accuracy: 0.9424', '\\n', 'training loss: 0.1703\\n', '\\n', 'SPR_BENCH\n(embedding dimension = 128)', '\\n', 'train color weighted accuracy: 0.9486',\n'\\n', 'validation color weighted accuracy: 0.9472', '\\n', 'train shape weighted\naccuracy: 0.9486', '\\n', 'validation shape weighted accuracy: 0.9442', '\\n',\n'train complexity weighted accuracy: 0.9488', '\\n', 'validation complexity\nweighted accuracy: 0.9427', '\\n', 'training loss: 0.1694\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null], "parse_exc_info": [null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
