{"edges": [[0, 3], [0, 1], [0, 8], [0, 2], [0, 4], [2, 7], [3, 5], [3, 6]], "layout": [[0.5111111111111111, 0.0], [0.0, 0.5], [0.2222222222222222, 0.5], [0.5555555555555556, 0.5], [0.7777777777777778, 0.5], [0.4444444444444444, 1.0], [0.6666666666666666, 1.0], [0.2222222222222222, 1.0], [1.0, 0.5]], "plan": ["This baseline tokenizes each SPR sequence, builds a vocabulary, and encodes\nsequences with an embedding layer followed by a bi-directional LSTM whose mean-\npooled hidden state represents the whole sequence.  During a brief contrastive\npre-training stage we feed two stochastic augmentations of every\nsequence\u2014generated by random masking and local shuffling\u2014through the encoder and\noptimise an InfoNCE loss so that paired views stay close while others repel.  A\ntwo-layer projection head is used only for this stage.  We then attach a linear\nclassifier, fine-tune the entire network with cross-entropy, and evaluate after\nevery epoch on the dev split, tracking validation loss plus Shape-Weighted\nAccuracy, Color-Weighted Accuracy and the target Complexity-Weighted Composite\nAccuracy (CWCA).  All metrics, losses, predictions and ground-truth labels are\naccumulated in the required experiment_data dictionary and saved to ./working.\nA t-SNE visualisation of dev-set embeddings is produced for qualitative\ninspection.  The script is fully self-contained, automatically falls back to a\nsmall synthetic dataset if SPR_BENCH is not found, abides by the specified\nGPU/CPU handling rules, and finishes within minutes for the default hyper-\nparameters.", "Ablation name: NoContrastivePretraining. Below we remove Stage-1 entirely,\ninitialise the encoder randomly and go straight to the supervised fine-tuning\nloop while keeping every other hyper-parameter, data-loader and architecture\nidentical to the baseline. All bookkeeping (losses, CWCA metrics, predictions)\nis stored in the required experiment_data structure and saved to\nworking/experiment_data.npy together with a t-SNE plot of dev embeddings.", "Ablation name: NoProjectionHeadContrastive. The ablation simply skips the MLP\nprojection head during SimCLR-style pre-training: the encoder\u2019s averaged bi-LSTM\nsentence embedding is L2-normalised and fed straight into the NT-Xent loss.\nDuring contrastive training we therefore optimise only the encoder parameters;\nthe classification head is added and fine-tuned afterwards exactly as in the\nbaseline.  All logs, metrics and artefacts are stored in the required\n`experiment_data.npy` file under the key `NoProjHeadContrastive \u2192 SPR_BENCH`.", "Ablation name: MultiSyntheticDatasetTraining. First, create three synthetic SPR-\nlike datasets that differ in shape/color alphabets, sequence-length ranges and\nlabeling rules.   Build one unified vocabulary from the union of their tokens so\nthe same encoder can process every set.   (1)\u2003Contrastive-pre-train and\nsupervised-train on Synthetic1.   (2)\u2003Load that checkpoint, switch to a smaller\nlearning-rate, and fine-tune on Synthetic2.   (3)\u2003Freeze nothing, evaluate the\nresulting model on the held-out Synthetic3 test split.   Log CWCA scores/losses\nfor every phase inside the required experiment_data structure and save\neverything as experiment_data.npy.", "Ablation name: BagOfTokensEncoder. Below is the single-file script that realises\nthe Bag-of-Tokens ablation.   The BiLSTM in the original Encoder is replaced\nwith BagOfTokensEncoder that simply mean-pools token embeddings (ignoring\npadding) before passing them to the classifier / projection head.  All training,\nevaluation, logging and visualisation code remains unchanged; results are stored\nin the prescribed experiment_data.npy file.", "Fixed the bug causing incorrect positive-pair indexing in the SimCLR contrastive\nphase: the two augmented views of each sample are now arranged as [v1_0 \u2026 v1_N |\nv2_0 \u2026 v2_N], so nt_xent correctly matches positives with the +N offset.  We\nalso correct color-variety counting (token[1:] instead of token[1]) and add the\nAugmentation Consistency Rate (ACR) metric that is tracked every epoch.\nEverything else (GPU handling, data saving, plots, etc.) follows the required\nguidelines.", "The constant high contrastive loss stemmed from an incorrect NT-Xent\nimplementation \u2013 the positive-pair similarity was mistakenly retained in the\ndenominator, making the loss incapable of decreasing.  We patch NT-Xent so that\npositives are removed from the normalisation term (SimCLR style) and tidy the\ntraining loops to guarantee every tensor is moved to the correct device.  At the\nsame time we replace the ad-hoc \u201ccwca\u201d metric with the correct Shape-Weighted\nAccuracy (SWA) and Color-Weighted Accuracy (CWA) and add the new Augmentation\nConsistency Rate (ACR) requested by the evaluation protocol.  All metrics and\nlosses are stored in experiment_data and saved at the end.  The rest of the\npipeline (contrastive pre-train \u2192 supervised fine-tune \u2192 zero-shot evaluation)\nis kept intact.", "The previous experiment used the encoder output itself for the contrastive loss.\nBecause the same representation is later sent directly to the classifier, the\ngradients coming from NT-Xent make the encoder focus on pairwise similarity\nrather than class-separable structure, hurting downstream accuracy and (in our\nablation stage) giving misleading metrics.   The fix is to add a small non-\nlinear projection head that is used only during contrastive pre-training;\nafterwards we throw it away and fine-tune the encoder + classifier. In addition,\nwe replace the ad-hoc CWCA metric with the correct Shape-Weighted and Color-\nWeighted accuracies that exist in SPR.py and we track the required Augmentation\nConsistency Rate (ACR).   The code below implements these changes, follows all\nGPU/CPU handling rules, logs every metric/epoch, and saves everything to\n./working.", "Ablation name: FreezeEncoderFineTuning. The implementation keeps the existing\ncontrastive\u2010pre-training phase unchanged, but after it finishes every encoder\nparameter\u2019s requires_grad flag is set to False. During the subsequent supervised\nstage only the final linear classifier is optimised, allowing us to isolate the\neffect of updating the encoder. All losses, metrics and predictions are stored\nunder the ablation key \u201cFreezeEncoderFineTuning\u201d so they can be compared against\nthe baseline run saved previously."], "code": ["import os, random, math, pathlib, time, itertools\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------------------\n# mandatory working dir + gpu setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# Try SPR_BENCH, else synthetic data\ndef safe_load_spr():\n    try:\n        from SPR import load_spr_bench\n\n        DATA_PATH = pathlib.Path(\"SPR_BENCH\")\n        if DATA_PATH.exists():\n            dset = load_spr_bench(DATA_PATH)\n            return dset\n    except Exception as e:\n        print(\"Could not load real SPR_BENCH, falling back to synthetic data.\", e)\n\n    # synthetic toy dataset -------------------------------------------------\n    shapes = list(\"ABCDEFG\")\n    colors = list(\"123456\")\n\n    def random_token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def random_seq():\n        return \" \".join(random_token() for _ in range(random.randint(4, 10)))\n\n    def label_fn(seq):\n        return 1 if sum(tok[0] in \"ABC\" for tok in seq.split()) % 2 == 0 else 0\n\n    synthetic = {\"train\": [], \"dev\": [], \"test\": []}\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        for i in range(n):\n            seq = random_seq()\n            synthetic[split].append({\"id\": i, \"sequence\": seq, \"label\": label_fn(seq)})\n    return synthetic\n\n\ndset = safe_load_spr()\nprint({k: len(v) for k, v in dset.items()})\n\n\n# ------------------------------------------------------------\n# utility metrics\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef cwca(seqs, y_true, y_pred):\n    weights = [(count_shape_variety(s) + count_color_variety(s)) / 2 for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\n# ------------------------------------------------------------\n# vocab + tokenisation\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(sequences):\n    vocab = set(itertools.chain.from_iterable(seq.split() for seq in sequences))\n    vocab = [PAD, MASK, UNK] + sorted(vocab)\n    stoi = {tok: i for i, tok in enumerate(vocab)}\n    return vocab, stoi\n\n\nvocab, stoi = build_vocab([ex[\"sequence\"] for ex in dset[\"train\"]])\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# ------------------------------------------------------------\n# datasets\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.records = records\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        out = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            out[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return out\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = [len(s) for s in seqs]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\n        \"input_ids\": padded.to(device),\n        \"label\": labels.to(device),\n        \"lengths\": torch.tensor(lengths).to(device),\n    }\n\n\n# ------------------------------------------------------------\n# contrastive augmentation\ndef augment(ids: List[int]) -> List[int]:\n    ids = ids.copy()\n    # random masking\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    # local shuffle\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    # random dropout\n    if len(ids) > 4 and random.random() < 0.3:\n        drop_idx = random.randint(0, len(ids) - 1)\n        del ids[drop_idx]\n    return ids\n\n\ndef collate_contrastive(batch):\n    base = [b[\"input_ids\"] for b in batch]\n    views = []\n    for ids in base:\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n    lengths = [len(v) for v in views]\n    max_len = max(lengths)\n    padded = torch.zeros(len(views), max_len, dtype=torch.long)\n    for i, v in enumerate(views):\n        padded[i, : len(v)] = v\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lengths).to(device)}\n\n\n# ------------------------------------------------------------\n# model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # (B,L,D)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.linear(h_mean)\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, dim, proj_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, proj_dim)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_labels):\n        super().__init__()\n        self.encoder = Encoder(vocab_size)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lengths):\n        rep = self.encoder(x, lengths)\n        logits = self.classifier(rep)\n        return logits, rep\n\n\n# ------------------------------------------------------------\n# contrastive loss (SimCLR style)\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = F.normalize(z, dim=1)\n    sim = torch.exp(torch.matmul(z, z.T) / temperature)\n    mask = ~torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], dim=0)\n    denom = sim.sum(dim=1)\n    loss = -torch.log(pos / denom)\n    return loss.mean()\n\n\n# ------------------------------------------------------------\n# data loaders\nbatch_c = 128\ntrain_contrastive_loader = DataLoader(\n    SPRDataset(dset[\"train\"], with_label=False),\n    batch_size=batch_c // 2,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n    drop_last=True,\n)\ntrain_loader = DataLoader(\n    SPRDataset(dset[\"train\"]),\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_classification,\n)\ndev_loader = DataLoader(\n    SPRDataset(dset[\"dev\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\ntest_loader = DataLoader(\n    SPRDataset(dset[\"test\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\n\n# ------------------------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": [], \"test\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------------------------------------------------\n# build model / optim\nmodel = SPRModel(len(vocab), num_labels=len(set(r[\"label\"] for r in dset[\"train\"]))).to(\n    device\n)\nproj_head = ProjectionHead(128).to(device)\nopt_contrastive = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(proj_head.parameters()), lr=1e-3\n)\n\n# ------------------------------------------------------------\n# 1. quick contrastive pretraining\nepochs_ct = 3\nfor epoch in range(1, epochs_ct + 1):\n    model.train()\n    proj_head.train()\n    losses = []\n    for batch in train_contrastive_loader:\n        reps = model.encoder(batch[\"input_ids\"], batch[\"lengths\"])\n        feats = proj_head(reps)\n        loss = nt_xent(feats)\n        opt_contrastive.zero_grad()\n        loss.backward()\n        opt_contrastive.step()\n        losses.append(loss.item())\n    print(f\"[Pretrain] Epoch {epoch}/{epochs_ct} - loss={np.mean(losses):.4f}\")\n\n# ------------------------------------------------------------\n# 2. classification fine-tune\nclf_epochs = 5\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\n\ndef run_epoch(loader, train=True):\n    (model.train() if train else model.eval())\n    total_loss, preds, trues, seqs = 0.0, [], [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            pred = logits.argmax(1).detach().cpu().numpy()\n            true = batch[\"label\"].detach().cpu().numpy()\n            preds.extend(pred)\n            trues.extend(true)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    cwca_val = cwca(seqs, trues, preds)\n    return total_loss / len(loader.dataset), cwca_val, preds, trues\n\n\nbest_val = 0\nfor epoch in range(1, clf_epochs + 1):\n    train_loss, train_cwca, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_cwca, _, _ = run_epoch(dev_loader, train=False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_cwca)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_cwca)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_CWCA={val_cwca:.4f}\"\n    )\n    if val_cwca > best_val:\n        best_val = val_cwca\n\n# ------------------------------------------------------------\n# 3. final test evaluation\ntest_loss, test_cwca, preds, trues = run_epoch(test_loader, train=False)\nprint(f\"Test CWCA = {test_cwca:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"].append(test_cwca)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = trues\n\n# ------------------------------------------------------------\n# 4. t-SNE visualisation on dev set\nmodel.eval()\nembeddings, labels = [], []\nwith torch.no_grad():\n    for batch in dev_loader:\n        _, reps = model(batch[\"input_ids\"], batch[\"lengths\"])\n        embeddings.append(reps.cpu().numpy())\n        labels.extend(batch[\"label\"].cpu().numpy())\nembeddings = np.concatenate(embeddings, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embeddings\n)\nplt.figure(figsize=(6, 5))\nscatter = plt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE of dev embeddings\")\nplt.savefig(os.path.join(working_dir, \"tsne_dev.png\"))\n\n# ------------------------------------------------------------\n# save experiment_data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data and plot saved to ./working/\")\n", "# NoContrastivePretraining \u2013 single-file ablation\nimport os, random, math, pathlib, time, itertools, warnings\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# Try to load SPR_BENCH, else build synthetic toy data\ndef safe_load_spr():\n    try:\n        from SPR import load_spr_bench\n\n        DATA_PATH = pathlib.Path(\"SPR_BENCH\")\n        if DATA_PATH.exists():\n            return load_spr_bench(DATA_PATH)\n    except Exception as e:\n        print(\"Could not load real SPR_BENCH; falling back to synthetic.\", e)\n\n    shapes, colors = list(\"ABCDEFG\"), list(\"123456\")\n\n    def random_token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def random_seq():\n        return \" \".join(random_token() for _ in range(random.randint(4, 10)))\n\n    def label_fn(seq):\n        return 1 if sum(tok[0] in \"ABC\" for tok in seq.split()) % 2 == 0 else 0\n\n    synthetic = {\"train\": [], \"dev\": [], \"test\": []}\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        for i in range(n):\n            seq = random_seq()\n            synthetic[split].append({\"id\": i, \"sequence\": seq, \"label\": label_fn(seq)})\n    return synthetic\n\n\ndset = safe_load_spr()\nprint({k: len(v) for k, v in dset.items()})\n\n\n# ------------------------------------------------------------\n# metrics\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split() if len(tok) > 1))\n\n\ndef cwca(seqs, y_true, y_pred):\n    weights = [(count_shape_variety(s) + count_color_variety(s)) / 2 for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\n# ------------------------------------------------------------\n# vocab\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(seqs):\n    vocab = set(itertools.chain.from_iterable(s.split() for s in seqs))\n    vocab = [PAD, MASK, UNK] + sorted(vocab)\n    stoi = {tok: i for i, tok in enumerate(vocab)}\n    return vocab, stoi\n\n\nvocab, stoi = build_vocab([ex[\"sequence\"] for ex in dset[\"train\"]])\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# ------------------------------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, recs, with_label=True):\n        self.recs = recs\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.recs)\n\n    def __getitem__(self, idx):\n        rec = self.recs[idx]\n        item = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            item[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return item\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = [len(s) for s in seqs]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\n        \"input_ids\": padded.to(device),\n        \"label\": labels.to(device),\n        \"lengths\": torch.tensor(lengths).to(device),\n    }\n\n\n# ------------------------------------------------------------\n# model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x, lens):\n        emb = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.linear(h_mean)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_labels):\n        super().__init__()\n        self.encoder = Encoder(vocab_size)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lens):\n        rep = self.encoder(x, lens)\n        return self.classifier(rep), rep\n\n\n# ------------------------------------------------------------\n# data loaders\ntrain_loader = DataLoader(\n    SPRDataset(dset[\"train\"]),\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_classification,\n)\ndev_loader = DataLoader(\n    SPRDataset(dset[\"dev\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\ntest_loader = DataLoader(\n    SPRDataset(dset[\"test\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\n\n# ------------------------------------------------------------\n# experiment logging dict\nexperiment_data = {\n    \"NoContrastivePretraining\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------\n# initialise model (random) \u2013 no contrastive pre-training\nmodel = SPRModel(len(vocab), num_labels=len(set(r[\"label\"] for r in dset[\"train\"]))).to(\n    device\n)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\nclf_epochs = 5\n\n\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    total_loss, preds, trues, seqs = 0.0, [], [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            pred = logits.argmax(1).detach().cpu().numpy()\n            true = batch[\"label\"].detach().cpu().numpy()\n            preds.extend(pred)\n            trues.extend(true)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    cwca_val = cwca(seqs, trues, preds)\n    return total_loss / len(loader.dataset), cwca_val, preds, trues\n\n\nbest_val = 0\nfor epoch in range(1, clf_epochs + 1):\n    tr_loss, tr_cwca, _, _ = run_epoch(train_loader, True)\n    val_loss, val_cwca, _, _ = run_epoch(dev_loader, False)\n    experiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        tr_loss\n    )\n    experiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        val_loss\n    )\n    experiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        tr_cwca\n    )\n    experiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        val_cwca\n    )\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}  val_CWCA={val_cwca:.4f}\"\n    )\n    best_val = max(best_val, val_cwca)\n\n# ------------------------------------------------------------\n# final test evaluation\ntest_loss, test_cwca, preds, trues = run_epoch(test_loader, False)\nprint(f\"Test CWCA = {test_cwca:.4f}\")\nexperiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"metrics\"][\"test\"].append(\n    test_cwca\n)\nexperiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"NoContrastivePretraining\"][\"SPR_BENCH\"][\"ground_truth\"] = trues\n\n# ------------------------------------------------------------\n# t-SNE visualisation on dev embeddings\nmodel.eval()\nembs, labels = [], []\nwith torch.no_grad():\n    for batch in dev_loader:\n        _, reps = model(batch[\"input_ids\"], batch[\"lengths\"])\n        embs.append(reps.cpu().numpy())\n        labels.extend(batch[\"label\"].cpu().numpy())\nembs = np.concatenate(embs, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embs\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE of dev embeddings (NoContrastivePretraining)\")\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, \"tsne_dev.png\"))\n\n# ------------------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data and plot to ./working/\")\n", "# NoProjectionHeadContrastive \u2013 complete, self-contained script\nimport os, random, math, pathlib, time, itertools, json, warnings\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------------------------------------------------\ndef safe_load_spr():\n    try:\n        from SPR import load_spr_bench\n\n        DATA_PATH = pathlib.Path(\"SPR_BENCH\")\n        if DATA_PATH.exists():\n            return load_spr_bench(DATA_PATH)\n    except Exception as e:\n        print(\"Could not load real SPR_BENCH, using synthetic data.\", e)\n\n    shapes, colors = list(\"ABCDEFG\"), list(\"123456\")\n\n    def random_token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def random_seq():\n        return \" \".join(random_token() for _ in range(random.randint(4, 10)))\n\n    def label_fn(s):\n        return 1 if sum(tok[0] in \"ABC\" for tok in s.split()) % 2 == 0 else 0\n\n    toy = {sp: [] for sp in [\"train\", \"dev\", \"test\"]}\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        for i in range(n):\n            seq = random_seq()\n            toy[split].append({\"id\": i, \"sequence\": seq, \"label\": label_fn(seq)})\n    return toy\n\n\ndset = safe_load_spr()\nprint({k: len(v) for k, v in dset.items()})\n\n# ------------------------------------------------------------\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(seqs):\n    vocab = [PAD, MASK, UNK] + sorted(\n        set(itertools.chain.from_iterable(s.split() for s in seqs))\n    )\n    return vocab, {tok: i for i, tok in enumerate(vocab)}\n\n\nvocab, stoi = build_vocab([r[\"sequence\"] for r in dset[\"train\"]])\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq):\n    return [stoi.get(t, stoi[UNK]) for t in seq.split()]\n\n\n# ------------------------------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.records, self.with_label = records, with_label\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        item = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            item[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return item\n\n\ndef pad_and_stack(seqs, lengths, max_len, dtype=torch.long):\n    padded = torch.zeros(len(seqs), max_len, dtype=dtype)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return padded\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    lengths = [len(s) for s in seqs]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    padded = pad_and_stack(seqs, lengths, max(lengths))\n    return {\n        \"input_ids\": padded.to(device),\n        \"lengths\": torch.tensor(lengths).to(device),\n        \"label\": labels.to(device),\n    }\n\n\ndef augment(ids: List[int]) -> List[int]:\n    ids = ids.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    if len(ids) > 4 and random.random() < 0.3:\n        del ids[random.randint(0, len(ids) - 1)]\n    return ids\n\n\ndef collate_contrastive(batch):\n    bases = [b[\"input_ids\"] for b in batch]\n    views = []\n    for ids in bases:\n        views += [\n            torch.tensor(augment(ids.tolist()), dtype=torch.long),\n            torch.tensor(augment(ids.tolist()), dtype=torch.long),\n        ]\n    lengths = [len(v) for v in views]\n    padded = pad_and_stack(views, lengths, max(lengths))\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lengths).to(device)}\n\n\n# ------------------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, d_model, padding_idx=0)\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.linear(h_mean)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_sz, num_labels):\n        super().__init__()\n        self.encoder = Encoder(vocab_sz)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lengths):\n        rep = self.encoder(x, lengths)\n        return self.classifier(rep), rep\n\n\n# ------------------------------------------------------------\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = F.normalize(z, dim=1)\n    sim = torch.exp(z @ z.T / temperature)\n    mask = ~torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1)\n    return (-torch.log(pos / denom)).mean()\n\n\n# ------------------------------------------------------------\nbatch_c = 128\ntrain_contrastive_loader = DataLoader(\n    SPRDataset(dset[\"train\"], with_label=False),\n    batch_size=batch_c // 2,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n    drop_last=True,\n)\ntrain_loader = DataLoader(\n    SPRDataset(dset[\"train\"]),\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_classification,\n)\ndev_loader = DataLoader(\n    SPRDataset(dset[\"dev\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\ntest_loader = DataLoader(\n    SPRDataset(dset[\"test\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\n\n# ------------------------------------------------------------\nexperiment_data = {\n    \"NoProjHeadContrastive\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------\nmodel = SPRModel(len(vocab), num_labels=len(set(r[\"label\"] for r in dset[\"train\"]))).to(\n    device\n)\n\n# Contrastive pre-train (without projection head)\nopt_contrastive = torch.optim.Adam(model.encoder.parameters(), lr=1e-3)\nepochs_ct = 3\nfor ep in range(1, epochs_ct + 1):\n    model.encoder.train()\n    losses = []\n    for batch in train_contrastive_loader:\n        reps = model.encoder(batch[\"input_ids\"], batch[\"lengths\"])\n        loss = nt_xent(reps)\n        opt_contrastive.zero_grad()\n        loss.backward()\n        opt_contrastive.step()\n        losses.append(loss.item())\n    print(f\"[Pretrain] Epoch {ep}/{epochs_ct} loss={np.mean(losses):.4f}\")\n\n# ------------------------------------------------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\n\ndef cwca(seqs, y_true, y_pred):\n    def shape_var(s):\n        return len(set(tok[0] for tok in s.split()))\n\n    def color_var(s):\n        return len(set(tok[1] for tok in s.split()))\n\n    weights = [(shape_var(s) + color_var(s)) / 2 for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss, preds, truths, seqs = 0.0, [], [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits, reps = model(batch[\"input_ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"label\"].size(0)\n            pred = logits.argmax(1).cpu().numpy()\n            true = batch[\"label\"].cpu().numpy()\n            preds.extend(pred)\n            truths.extend(true)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    cwca_val = cwca(seqs, truths, preds)\n    return tot_loss / len(loader.dataset), cwca_val, preds, truths\n\n\nbest_val = 0\nclf_epochs = 5\nfor ep in range(1, clf_epochs + 1):\n    tr_loss, tr_cwca, _, _ = run_epoch(train_loader, True)\n    val_loss, val_cwca, _, _ = run_epoch(dev_loader, False)\n    experiment_data[\"NoProjHeadContrastive\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        tr_loss\n    )\n    experiment_data[\"NoProjHeadContrastive\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        val_loss\n    )\n    experiment_data[\"NoProjHeadContrastive\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        tr_cwca\n    )\n    experiment_data[\"NoProjHeadContrastive\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        val_cwca\n    )\n    print(\n        f\"Epoch {ep}: train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}  val_CWCA={val_cwca:.4f}\"\n    )\n    best_val = max(best_val, val_cwca)\n\n# ------------------------------------------------------------\ntest_loss, test_cwca, preds, trues = run_epoch(test_loader, False)\nprint(\"Test CWCA =\", test_cwca)\nexp = experiment_data[\"NoProjHeadContrastive\"][\"SPR_BENCH\"]\nexp[\"metrics\"][\"test\"].append(test_cwca)\nexp[\"predictions\"] = preds\nexp[\"ground_truth\"] = trues\n\n# ------------------------------------------------------------\n# t-SNE on dev\nmodel.eval()\nembeds, labels = [], []\nwith torch.no_grad():\n    for b in dev_loader:\n        _, rep = model(b[\"input_ids\"], b[\"lengths\"])\n        embeds.append(rep.cpu().numpy())\n        labels.extend(b[\"label\"].cpu().numpy())\nembeds = np.concatenate(embeds, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embeds\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE dev embeddings (NoProjHead)\")\nplt.savefig(os.path.join(working_dir, \"tsne_dev.png\"))\nplt.close()\n\n# ------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved metrics & plot to\", working_dir)\n", "import os, random, math, pathlib, time, itertools\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------------------\n# mandatory working dir + gpu setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# 1. build three distinct synthetic datasets -----------------\ndef make_synthetic(\n    shapes: List[str],\n    colors: List[str],\n    seq_len_range=(4, 10),\n    rule: str = \"parity\",\n    n_train=2000,\n    n_dev=400,\n    n_test=400,\n):\n    def random_token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def random_seq():\n        L = random.randint(*seq_len_range)\n        return \" \".join(random_token() for _ in range(L))\n\n    # define different label rules\n    if rule == \"parity\":\n\n        def label_fn(seq):\n            return (\n                1\n                if sum(tok[0] in shapes[: len(shapes) // 2] for tok in seq.split()) % 2\n                == 0\n                else 0\n            )\n\n    elif rule == \"color_even\":\n        target_colors = set(colors[: len(colors) // 2])\n\n        def label_fn(seq):\n            return (\n                1 if sum(tok[1] in target_colors for tok in seq.split()) % 2 == 0 else 0\n            )\n\n    elif rule == \"mod3\":\n\n        def label_fn(seq):\n            return 1 if len(seq.split()) % 3 == 0 else 0\n\n    else:\n        raise ValueError(rule)\n\n    out = {\"train\": [], \"dev\": [], \"test\": []}\n    for split, n in [(\"train\", n_train), (\"dev\", n_dev), (\"test\", n_test)]:\n        for i in range(n):\n            s = random_seq()\n            out[split].append({\"id\": i, \"sequence\": s, \"label\": label_fn(s)})\n    return out\n\n\nset1 = make_synthetic(list(\"ABCDEF\"), list(\"1234\"), rule=\"parity\")\nset2 = make_synthetic(list(\"GHIJKL\"), list(\"5678\"), rule=\"color_even\")\nset3 = make_synthetic(list(\"MNOPQR\"), list(\"90\"), rule=\"mod3\")\n\ndatasets = {\"Synthetic1\": set1, \"Synthetic2\": set2, \"Synthetic3\": set3}\nprint({k: {s: len(v[s]) for s in v} for k, v in datasets.items()})\n\n\n# ------------------------------------------------------------\n# 2. utility metrics -----------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef cwca(seqs, y_true, y_pred):\n    weights = [(count_shape_variety(s) + count_color_variety(s)) / 2 for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\n# ------------------------------------------------------------\n# 3. vocab + tokenisation (union of all sets) ----------------\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(datasets):\n    seqs = []\n    for d in datasets.values():\n        for split in d.values():\n            seqs.extend(ex[\"sequence\"] for ex in split)\n    vocab = set(itertools.chain.from_iterable(seq.split() for seq in seqs))\n    vocab = [PAD, MASK, UNK] + sorted(vocab)\n    stoi = {tok: i for i, tok in enumerate(vocab)}\n    return vocab, stoi\n\n\nvocab, stoi = build_vocab(datasets)\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# ------------------------------------------------------------\n# 4. dataset & loader helpers --------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.records = records\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        out = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            out[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return out\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = [len(s) for s in seqs]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\n        \"input_ids\": padded.to(device),\n        \"label\": labels.to(device),\n        \"lengths\": torch.tensor(lengths).to(device),\n    }\n\n\n# contrastive augmentation\ndef augment(ids: List[int]) -> List[int]:\n    ids = ids.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    if len(ids) > 4 and random.random() < 0.3:\n        del ids[random.randint(0, len(ids) - 1)]\n    return ids\n\n\ndef collate_contrastive(batch):\n    base = [b[\"input_ids\"] for b in batch]\n    views = []\n    for ids in base:\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n    lengths = [len(v) for v in views]\n    max_len = max(lengths)\n    padded = torch.zeros(len(views), max_len, dtype=torch.long)\n    for i, v in enumerate(views):\n        padded[i, : len(v)] = v\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lengths).to(device)}\n\n\n# ------------------------------------------------------------\n# 5. model ----------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True, bidirectional=True)\n        self.linear = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.linear(h_mean)\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, dim, proj_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, proj_dim)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_labels=2):\n        super().__init__()\n        self.encoder = Encoder(vocab_size)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lengths):\n        rep = self.encoder(x, lengths)\n        logits = self.classifier(rep)\n        return logits, rep\n\n\n# contrastive loss\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.exp(torch.matmul(z, z.T) / temperature)\n    mask = ~torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1)\n    loss = -torch.log(pos / denom)\n    return loss.mean()\n\n\n# ------------------------------------------------------------\n# 6. experiment_data skeleton --------------------------------\nexperiment_data = {\n    \"MultiSyntheticDatasetTraining\": {\n        \"Synthetic1\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"Synthetic2\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"Synthetic3\": {\n            \"metrics\": {\"test\": []},\n            \"losses\": {},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------------------------------------\n# 7. loaders for every dataset -------------------------------\ndef build_loaders(dset, contrastive=False, batch_size=64):\n    if contrastive:\n        loader = DataLoader(\n            SPRDataset(dset[\"train\"], with_label=False),\n            batch_size=batch_size // 2,\n            shuffle=True,\n            collate_fn=collate_contrastive,\n            drop_last=True,\n        )\n        return loader\n    else:\n        train_loader = DataLoader(\n            SPRDataset(dset[\"train\"]),\n            batch_size=batch_size,\n            shuffle=True,\n            collate_fn=collate_classification,\n        )\n        dev_loader = DataLoader(\n            SPRDataset(dset[\"dev\"]),\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=collate_classification,\n        )\n        test_loader = DataLoader(\n            SPRDataset(dset[\"test\"]),\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=collate_classification,\n        )\n        return train_loader, dev_loader, test_loader\n\n\ncontrastive_loader_1 = build_loaders(set1, contrastive=True, batch_size=128)\ntrain1_loader, dev1_loader, test1_loader = build_loaders(set1)\ntrain2_loader, dev2_loader, test2_loader = build_loaders(set2)\ntrain3_loader, dev3_loader, test3_loader = build_loaders(set3)\n\n# ------------------------------------------------------------\n# 8. instantiate model & optim -------------------------------\nmodel = SPRModel(len(vocab)).to(device)\nproj_head = ProjectionHead(128).to(device)\nopt_contrastive = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(proj_head.parameters()), lr=1e-3\n)\n\n# ------------------------------------------------------------\n# 9. phase 1: contrastive pretrain on Synthetic1 -------------\nepochs_ct = 3\nfor epoch in range(1, epochs_ct + 1):\n    model.train()\n    proj_head.train()\n    losses = []\n    for batch in contrastive_loader_1:\n        reps = model.encoder(batch[\"input_ids\"], batch[\"lengths\"])\n        feats = proj_head(reps)\n        loss = nt_xent(feats)\n        opt_contrastive.zero_grad()\n        loss.backward()\n        opt_contrastive.step()\n        losses.append(loss.item())\n    print(f\"[Contrastive] Epoch {epoch}/{epochs_ct} loss={np.mean(losses):.4f}\")\n\n# ------------------------------------------------------------\n# helper: run epoch ------------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, train=True, optimizer=None):\n    (model.train() if train else model.eval())\n    total_loss, preds, trues, seqs = 0.0, [], [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            p = logits.argmax(1).detach().cpu().numpy()\n            t = batch[\"label\"].detach().cpu().numpy()\n            preds.extend(p)\n            trues.extend(t)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    cwca_val = cwca(seqs, trues, preds)\n    return total_loss / len(loader.dataset), cwca_val, preds, trues\n\n\n# ------------------------------------------------------------\n# 10. phase 1: supervised train on Synthetic1 ----------------\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\nbest_val = 0\nclf_epochs = 5\nfor epoch in range(1, clf_epochs + 1):\n    tr_loss, tr_cwca, _, _ = run_epoch(train1_loader, True, optimizer)\n    val_loss, val_cwca, _, _ = run_epoch(dev1_loader, False)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic1\"][\"losses\"][\n        \"train\"\n    ].append(tr_loss)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic1\"][\"losses\"][\n        \"val\"\n    ].append(val_loss)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic1\"][\"metrics\"][\n        \"train\"\n    ].append(tr_cwca)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic1\"][\"metrics\"][\n        \"val\"\n    ].append(val_cwca)\n    print(\n        f\"[S1] Ep{epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_CWCA={val_cwca:.4f}\"\n    )\n    if val_cwca > best_val:\n        best_val = val_cwca\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_s1.pt\"))\n\n# store preds on dev once\n_, _, preds_dev1, trues_dev1 = run_epoch(dev1_loader, False)\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic1\"][\n    \"predictions\"\n] = preds_dev1\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic1\"][\n    \"ground_truth\"\n] = trues_dev1\n\n# ------------------------------------------------------------\n# 11. phase 2: fine-tune on Synthetic2 -----------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_s1.pt\")))\noptimizer_ft = torch.optim.Adam(model.parameters(), lr=1e-4)\nft_epochs = 3\nfor epoch in range(1, ft_epochs + 1):\n    tr_loss, tr_cwca, _, _ = run_epoch(train2_loader, True, optimizer_ft)\n    val_loss, val_cwca, _, _ = run_epoch(dev2_loader, False)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic2\"][\"losses\"][\n        \"train\"\n    ].append(tr_loss)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic2\"][\"losses\"][\n        \"val\"\n    ].append(val_loss)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic2\"][\"metrics\"][\n        \"train\"\n    ].append(tr_cwca)\n    experiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic2\"][\"metrics\"][\n        \"val\"\n    ].append(val_cwca)\n    print(\n        f\"[S2 fine-tune] Ep{epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_CWCA={val_cwca:.4f}\"\n    )\n\n_, _, preds_dev2, trues_dev2 = run_epoch(dev2_loader, False)\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic2\"][\n    \"predictions\"\n] = preds_dev2\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic2\"][\n    \"ground_truth\"\n] = trues_dev2\n\n# ------------------------------------------------------------\n# 12. phase 3: evaluate on Synthetic3 ------------------------\ntest_loss3, test_cwca3, preds3, trues3 = run_epoch(test3_loader, False)\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic3\"][\"metrics\"][\n    \"test\"\n].append(test_cwca3)\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic3\"][\"predictions\"] = preds3\nexperiment_data[\"MultiSyntheticDatasetTraining\"][\"Synthetic3\"][\"ground_truth\"] = trues3\nprint(f\"[S3 Test] loss={test_loss3:.4f} CWCA={test_cwca3:.4f}\")\n\n# ------------------------------------------------------------\n# 13. t-SNE visualisation on Synthetic3 dev set --------------\nmodel.eval()\nembeds, labels = [], []\nwith torch.no_grad():\n    for batch in dev3_loader:\n        _, reps = model(batch[\"input_ids\"], batch[\"lengths\"])\n        embeds.append(reps.cpu().numpy())\n        labels.extend(batch[\"label\"].cpu().numpy())\nembeds = np.concatenate(embeds, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embeds\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE of Synthetic3 dev embeddings\")\nplt.savefig(os.path.join(working_dir, \"tsne_s3_dev.png\"))\n\n# ------------------------------------------------------------\n# 14. save experiment data -----------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All done. Results stored in ./working/\")\n", "# Bag-of-Tokens ablation of the SPR baseline -------------------------------\n# (self-contained single-file programme)\n\nimport os, random, math, pathlib, time, itertools\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# --------------------------------------------------------------------------\n# mandatory working dir + gpu setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------------------------------------------------------\n# SPR dataset (real or synthetic fallback)\ndef safe_load_spr():\n    try:\n        from SPR import load_spr_bench\n\n        DATA_PATH = pathlib.Path(\"SPR_BENCH\")\n        if DATA_PATH.exists():\n            return load_spr_bench(DATA_PATH)\n    except Exception as e:\n        print(\"Could not load real SPR_BENCH, falling back to synthetic.\", e)\n\n    shapes, colors = list(\"ABCDEFG\"), list(\"123456\")\n\n    def random_token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def random_seq():\n        return \" \".join(random_token() for _ in range(random.randint(4, 10)))\n\n    def label_fn(seq):\n        return 1 if sum(tok[0] in \"ABC\" for tok in seq.split()) % 2 == 0 else 0\n\n    data = {\"train\": [], \"dev\": [], \"test\": []}\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        for i in range(n):\n            seq = random_seq()\n            data[split].append({\"id\": i, \"sequence\": seq, \"label\": label_fn(seq)})\n    return data\n\n\ndset = safe_load_spr()\nprint({k: len(v) for k, v in dset.items()})\n\n\n# --------------------------------------------------------------------------\n# metrics\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef cwca(seqs, y_true, y_pred):\n    w = [(count_shape_variety(s) + count_color_variety(s)) / 2 for s in seqs]\n    correct = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\n# --------------------------------------------------------------------------\n# vocab + tokenisation\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(sequences):\n    vocab = set(itertools.chain.from_iterable(s.split() for s in sequences))\n    vocab = [PAD, MASK, UNK] + sorted(vocab)\n    stoi = {tok: i for i, tok in enumerate(vocab)}\n    return vocab, stoi\n\n\nvocab, stoi = build_vocab([r[\"sequence\"] for r in dset[\"train\"]])\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# --------------------------------------------------------------------------\n# datasets / dataloaders\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.records = records\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        out = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            out[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return out\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = [len(s) for s in seqs]\n    max_len = max(lengths)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\n        \"input_ids\": padded.to(device),\n        \"label\": labels.to(device),\n        \"lengths\": torch.tensor(lengths).to(device),\n    }\n\n\n# contrastive augmentation\ndef augment(ids: List[int]) -> List[int]:\n    ids = ids.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    if len(ids) > 4 and random.random() < 0.3:\n        del ids[random.randint(0, len(ids) - 1)]\n    return ids\n\n\ndef collate_contrastive(batch):\n    base = [b[\"input_ids\"] for b in batch]\n    views = []\n    for ids in base:\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n    lengths = [len(v) for v in views]\n    max_len = max(lengths)\n    padded = torch.zeros(len(views), max_len, dtype=torch.long)\n    for i, v in enumerate(views):\n        padded[i, : len(v)] = v\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lengths).to(device)}\n\n\nbatch_c = 128\ntrain_contrastive_loader = DataLoader(\n    SPRDataset(dset[\"train\"], with_label=False),\n    batch_size=batch_c // 2,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n    drop_last=True,\n)\ntrain_loader = DataLoader(\n    SPRDataset(dset[\"train\"]),\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_classification,\n)\ndev_loader = DataLoader(\n    SPRDataset(dset[\"dev\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\ntest_loader = DataLoader(\n    SPRDataset(dset[\"test\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\n\n\n# --------------------------------------------------------------------------\n# Bag-of-Tokens encoder (ablation)\nclass BagOfTokensEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.linear = nn.Linear(d_model, d_model)  # optional projection\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # (B,L,D)\n        mask = (x != 0).unsqueeze(-1)  # (B,L,1)\n        summed = (emb * mask).sum(1)  # (B,D)\n        mean = summed / ((mask.sum(1)) + 1e-6)\n        return self.linear(mean)  # (B,D)\n\n\n# projection head and full model\nclass ProjectionHead(nn.Module):\n    def __init__(self, dim, proj_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, proj_dim)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_labels):\n        super().__init__()\n        self.encoder = BagOfTokensEncoder(vocab_size)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lengths):\n        rep = self.encoder(x, lengths)\n        logits = self.classifier(rep)\n        return logits, rep\n\n\n# --------------------------------------------------------------------------\n# contrastive loss (NT-Xent as in SimCLR)\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = F.normalize(z, dim=1)\n    sim = torch.exp(torch.matmul(z, z.T) / temperature)\n    mask = ~torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1)\n    loss = -torch.log(pos / denom)\n    return loss.mean()\n\n\n# --------------------------------------------------------------------------\n# experiment logging skeleton\nexperiment_data = {\n    \"BagOfTokensEncoder\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# --------------------------------------------------------------------------\n# build model & optimisers\nmodel = SPRModel(len(vocab), num_labels=len(set(r[\"label\"] for r in dset[\"train\"]))).to(\n    device\n)\nproj_head = ProjectionHead(128).to(device)\nopt_contrastive = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(proj_head.parameters()), lr=1e-3\n)\n\n# --------------------------------------------------------------------------\n# 1) contrastive pre-training\nepochs_ct = 3\nfor epoch in range(1, epochs_ct + 1):\n    model.train()\n    proj_head.train()\n    losses = []\n    for batch in train_contrastive_loader:\n        reps = model.encoder(batch[\"input_ids\"], batch[\"lengths\"])\n        feats = proj_head(reps)\n        loss = nt_xent(feats)\n        opt_contrastive.zero_grad()\n        loss.backward()\n        opt_contrastive.step()\n        losses.append(loss.item())\n    print(f\"[Pretrain] Epoch {epoch}/{epochs_ct} loss={np.mean(losses):.4f}\")\n\n# --------------------------------------------------------------------------\n# 2) supervised fine-tuning\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\n\ndef run_epoch(loader, train=True):\n    (model.train() if train else model.eval())\n    total_loss, preds, trues, seqs = 0.0, [], [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits, rep = model(batch[\"input_ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            p = logits.argmax(1).detach().cpu().numpy()\n            t = batch[\"label\"].detach().cpu().numpy()\n            preds.extend(p)\n            trues.extend(t)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    cwca_val = cwca(seqs, trues, preds)\n    return total_loss / len(loader.dataset), cwca_val, preds, trues\n\n\nbest_val = 0\nclf_epochs = 5\nfor epoch in range(1, clf_epochs + 1):\n    train_loss, train_cwca, _, _ = run_epoch(train_loader, True)\n    val_loss, val_cwca, _, _ = run_epoch(dev_loader, False)\n    experiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        train_loss\n    )\n    experiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        train_cwca\n    )\n    experiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        val_cwca\n    )\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_CWCA={val_cwca:.4f}\"\n    )\n    if val_cwca > best_val:\n        best_val = val_cwca\n\n# --------------------------------------------------------------------------\n# 3) final test\ntest_loss, test_cwca, preds, trues = run_epoch(test_loader, False)\nprint(f\"Test CWCA = {test_cwca:.4f}\")\nexperiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"metrics\"][\"test\"].append(test_cwca)\nexperiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"BagOfTokensEncoder\"][\"SPR_BENCH\"][\"ground_truth\"] = trues\n\n# --------------------------------------------------------------------------\n# 4) t-SNE visualisation\nmodel.eval()\nembeddings, labels = [], []\nwith torch.no_grad():\n    for batch in dev_loader:\n        _, reps = model(batch[\"input_ids\"], batch[\"lengths\"])\n        embeddings.append(reps.cpu().numpy())\n        labels.extend(batch[\"label\"].cpu().numpy())\nembeddings = np.concatenate(embeddings, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embeddings\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE of dev embeddings (Bag-of-Tokens)\")\nplt.savefig(os.path.join(working_dir, \"tsne_dev.png\"))\n\n# --------------------------------------------------------------------------\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data and plot to ./working/\")\n", "import os, random, math, pathlib, time, itertools\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------------------\n# mandatory working dir + gpu setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# 1. build three distinct synthetic datasets -----------------\ndef make_synthetic(\n    shapes,\n    colors,\n    seq_len_range=(4, 10),\n    rule=\"parity\",\n    n_train=2000,\n    n_dev=400,\n    n_test=400,\n):\n    def random_token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def random_seq():\n        L = random.randint(*seq_len_range)\n        return \" \".join(random_token() for _ in range(L))\n\n    if rule == \"parity\":\n\n        def label_fn(seq):\n            return (\n                1\n                if sum(tok[0] in shapes[: len(shapes) // 2] for tok in seq.split()) % 2\n                == 0\n                else 0\n            )\n\n    elif rule == \"color_even\":\n        tgt = set(colors[: len(colors) // 2])\n\n        def label_fn(seq):\n            return 1 if sum(tok[1:] in tgt for tok in seq.split()) % 2 == 0 else 0\n\n    elif rule == \"mod3\":\n\n        def label_fn(seq):\n            return 1 if len(seq.split()) % 3 == 0 else 0\n\n    else:\n        raise ValueError(rule)\n\n    out = {sp: [] for sp in [\"train\", \"dev\", \"test\"]}\n    for split, n in [(\"train\", n_train), (\"dev\", n_dev), (\"test\", n_test)]:\n        for i in range(n):\n            s = random_seq()\n            out[split].append({\"id\": i, \"sequence\": s, \"label\": label_fn(s)})\n    return out\n\n\nset1 = make_synthetic(list(\"ABCDEF\"), list(\"1234\"), rule=\"parity\")\nset2 = make_synthetic(list(\"GHIJKL\"), list(\"5678\"), rule=\"color_even\")\nset3 = make_synthetic(list(\"MNOPQR\"), [\"90\"], rule=\"mod3\")\ndatasets = {\"Synthetic1\": set1, \"Synthetic2\": set2, \"Synthetic3\": set3}\nprint({k: {sp: len(v[sp]) for sp in v} for k, v in datasets.items()})\n\n\n# ------------------------------------------------------------\n# 2. metrics helpers -----------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1:] for tok in sequence.split() if len(tok) > 1))\n\n\ndef cwca(seqs, y_true, y_pred):\n    weights = [(count_shape_variety(s) + count_color_variety(s)) / 2 for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) else 0.0\n\n\n# ------------------------------------------------------------\n# 3. vocab ----------------------------------------------------\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(dsets):\n    seqs = list(\n        itertools.chain.from_iterable(\n            ex[\"sequence\"].split()\n            for ds in dsets.values()\n            for split in ds.values()\n            for ex in split\n        )\n    )\n    vocab = [PAD, MASK, UNK] + sorted(set(seqs))\n    return vocab, {tok: i for i, tok in enumerate(vocab)}\n\n\nvocab, stoi = build_vocab(datasets)\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq: str):\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# ------------------------------------------------------------\n# 4. datasets -------------------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.data = records\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        rec = self.data[idx]\n        out = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            out[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return out\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    lengths = torch.tensor([len(s) for s in seqs])\n    max_len = lengths.max()\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\n        \"input_ids\": padded.to(device),\n        \"label\": labels.to(device),\n        \"lengths\": lengths.to(device),\n    }\n\n\n# contrastive augmentation & collate\ndef augment(ids):\n    ids = ids.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    if len(ids) > 4 and random.random() < 0.3:\n        del ids[random.randint(0, len(ids) - 1)]\n    return ids\n\n\ndef collate_contrastive(batch):\n    base_ids = [b[\"input_ids\"] for b in batch]\n    view1, view2 = [], []\n    for ids in base_ids:\n        view1.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n        view2.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n    views = view1 + view2  # <-- fixed ordering\n    lengths = [len(v) for v in views]\n    max_len = max(lengths)\n    padded = torch.zeros(len(views), max_len, dtype=torch.long)\n    for i, v in enumerate(views):\n        padded[i, : len(v)] = v\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lengths).to(device)}\n\n\ndef build_loaders(dset, contrastive=False, batch_size=64):\n    if contrastive:\n        return DataLoader(\n            SPRDataset(dset[\"train\"], with_label=False),\n            batch_size=batch_size // 2,\n            shuffle=True,\n            collate_fn=collate_contrastive,\n            drop_last=True,\n        )\n    tr = DataLoader(\n        SPRDataset(dset[\"train\"]),\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_classification,\n    )\n    dv = DataLoader(\n        SPRDataset(dset[\"dev\"]),\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_classification,\n    )\n    te = DataLoader(\n        SPRDataset(dset[\"test\"]),\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_classification,\n    )\n    return tr, dv, te\n\n\ncontrastive_loader_1 = build_loaders(set1, True, 128)\ntrain1_loader, dev1_loader, test1_loader = build_loaders(set1)\ntrain2_loader, dev2_loader, test2_loader = build_loaders(set2)\ntrain3_loader, dev3_loader, test3_loader = build_loaders(set3)\n\n\n# ------------------------------------------------------------\n# 5. model ----------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, d=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, d, padding_idx=0)\n        self.lstm = nn.LSTM(d, d, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(d * 2, d)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.lin(h_mean)\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, dim, proj_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, proj_dim)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_sz, num_labels=2):\n        super().__init__()\n        self.encoder = Encoder(vocab_sz)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lengths):\n        rep = self.encoder(x, lengths)\n        return self.classifier(rep), rep\n\n\n# contrastive loss\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = F.normalize(z, dim=1)\n    sim = torch.exp(torch.mm(z, z.t()) / temperature)\n    mask = ~torch.eye(2 * N, device=z.device, dtype=torch.bool)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1)\n    loss = -torch.log(pos / denom)\n    return loss.mean()\n\n\n# ------------------------------------------------------------\n# 6. experiment data dict ------------------------------------\nexperiment_data = {\n    \"Synthetic1\": {\n        \"metrics\": {\"train_cwca\": [], \"val_cwca\": [], \"train_acr\": [], \"val_acr\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"Synthetic2\": {\n        \"metrics\": {\"train_cwca\": [], \"val_cwca\": [], \"train_acr\": [], \"val_acr\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"Synthetic3\": {\n        \"metrics\": {\"test_cwca\": [], \"test_acr\": []},\n        \"losses\": {\"test\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n}\n\n\n# ------------------------------------------------------------\n# 7. helper: ACR computation ---------------------------------\ndef compute_acr(model, seq_strs, orig_preds, k=3):\n    model.eval()\n    consistent = 0\n    for seq, pr in zip(seq_strs, orig_preds):\n        ok = True\n        ids = encode(seq)\n        for _ in range(k):\n            aug_ids = augment(ids.copy())\n            t = torch.tensor(aug_ids, dtype=torch.long).unsqueeze(0).to(device)\n            l = torch.tensor([t.size(1)]).to(device)\n            with torch.no_grad():\n                logit, _ = model(t, l)\n            if logit.argmax(1).item() != pr:\n                ok = False\n                break\n        consistent += int(ok)\n    return consistent / len(seq_strs) if seq_strs else 0.0\n\n\n# ------------------------------------------------------------\n# 8. helper: run epoch ---------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, train=True, optimizer=None):\n    (model.train() if train else model.eval())\n    tot_loss, preds, trues, seqs = 0.0, [], [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"label\"]) if \"label\" in batch else None\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        if loss is not None:\n            tot_loss += loss.item() * batch[\"label\"].size(0)\n            t = batch[\"label\"].detach().cpu().numpy()\n            p = logits.argmax(1).detach().cpu().numpy()\n            trues.extend(t)\n            preds.extend(p)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    cwca_val = cwca(seqs, trues, preds) if preds else 0.0\n    avg_loss = tot_loss / len(loader.dataset) if preds else 0.0\n    return avg_loss, cwca_val, preds, trues, seqs\n\n\n# ------------------------------------------------------------\n# 9. instantiate models --------------------------------------\nmodel = SPRModel(len(vocab)).to(device)\nproj_head = ProjectionHead(128).to(device)\nopt_contrastive = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(proj_head.parameters()), lr=1e-3\n)\n\n# ------------------------------------------------------------\n# 10. contrastive pretraining -------------------------------\nepochs_ct = 3\nfor ep in range(1, epochs_ct + 1):\n    model.train()\n    proj_head.train()\n    c_losses = []\n    for batch in contrastive_loader_1:\n        feats = proj_head(model.encoder(batch[\"input_ids\"], batch[\"lengths\"]))\n        loss = nt_xent(feats)\n        opt_contrastive.zero_grad()\n        loss.backward()\n        opt_contrastive.step()\n        c_losses.append(loss.item())\n    print(f\"[Contrastive] Epoch {ep}/{epochs_ct} loss={np.mean(c_losses):.4f}\")\n\n# ------------------------------------------------------------\n# 11. supervised training on Synthetic1 ----------------------\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\nbest_val = 0\nclf_epochs = 5\nfor ep in range(1, clf_epochs + 1):\n    tr_loss, tr_cwca, tr_pred, _, tr_seq = run_epoch(train1_loader, True, optimizer)\n    acr_train = compute_acr(model, tr_seq, tr_pred)\n    val_loss, val_cwca, val_pred, val_true, val_seq = run_epoch(dev1_loader, False)\n    acr_val = compute_acr(model, val_seq, val_pred)\n    experiment_data[\"Synthetic1\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"Synthetic1\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"Synthetic1\"][\"metrics\"][\"train_cwca\"].append(tr_cwca)\n    experiment_data[\"Synthetic1\"][\"metrics\"][\"val_cwca\"].append(val_cwca)\n    experiment_data[\"Synthetic1\"][\"metrics\"][\"train_acr\"].append(acr_train)\n    experiment_data[\"Synthetic1\"][\"metrics\"][\"val_acr\"].append(acr_val)\n    print(\n        f\"[S1] Ep{ep}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_CWCA={val_cwca:.4f} val_ACR={acr_val:.4f}\"\n    )\n    if val_cwca > best_val:\n        best_val = val_cwca\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_s1.pt\"))\nexperiment_data[\"Synthetic1\"][\"predictions\"] = val_pred\nexperiment_data[\"Synthetic1\"][\"ground_truth\"] = val_true\n\n# ------------------------------------------------------------\n# 12. fine-tune on Synthetic2 --------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_s1.pt\")))\noptimizer_ft = torch.optim.Adam(model.parameters(), lr=1e-4)\nft_epochs = 3\nfor ep in range(1, ft_epochs + 1):\n    tr_loss, tr_cwca, tr_pred, _, tr_seq = run_epoch(train2_loader, True, optimizer_ft)\n    acr_train = compute_acr(model, tr_seq, tr_pred)\n    val_loss, val_cwca, val_pred, val_true, val_seq = run_epoch(dev2_loader, False)\n    acr_val = compute_acr(model, val_seq, val_pred)\n    experiment_data[\"Synthetic2\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"Synthetic2\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"Synthetic2\"][\"metrics\"][\"train_cwca\"].append(tr_cwca)\n    experiment_data[\"Synthetic2\"][\"metrics\"][\"val_cwca\"].append(val_cwca)\n    experiment_data[\"Synthetic2\"][\"metrics\"][\"train_acr\"].append(acr_train)\n    experiment_data[\"Synthetic2\"][\"metrics\"][\"val_acr\"].append(acr_val)\n    print(\n        f\"[S2 FT] Ep{ep}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_CWCA={val_cwca:.4f} val_ACR={acr_val:.4f}\"\n    )\nexperiment_data[\"Synthetic2\"][\"predictions\"] = val_pred\nexperiment_data[\"Synthetic2\"][\"ground_truth\"] = val_true\n\n# ------------------------------------------------------------\n# 13. final test on Synthetic3 -------------------------------\ntest_loss, test_cwca, test_pred, test_true, test_seq = run_epoch(test3_loader, False)\ntest_acr = compute_acr(model, test_seq, test_pred)\nexperiment_data[\"Synthetic3\"][\"losses\"][\"test\"].append(test_loss)\nexperiment_data[\"Synthetic3\"][\"metrics\"][\"test_cwca\"].append(test_cwca)\nexperiment_data[\"Synthetic3\"][\"metrics\"][\"test_acr\"].append(test_acr)\nexperiment_data[\"Synthetic3\"][\"predictions\"] = test_pred\nexperiment_data[\"Synthetic3\"][\"ground_truth\"] = test_true\nprint(f\"[S3 TEST] loss={test_loss:.4f} CWCA={test_cwca:.4f} ACR={test_acr:.4f}\")\n\n# ------------------------------------------------------------\n# 14. t-SNE visualisation ------------------------------------\nmodel.eval()\nembeds, labels = [], []\nwith torch.no_grad():\n    for batch in dev3_loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        _, rep = model(batch[\"input_ids\"], batch[\"lengths\"])\n        embeds.append(rep.cpu().numpy())\n        labels.extend(batch[\"label\"].cpu().numpy())\nembeds = np.concatenate(embeds, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embeds\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE Synthetic3 dev embeddings\")\nplt.savefig(os.path.join(working_dir, \"tsne_s3_dev_fixed.png\"))\n\n# ------------------------------------------------------------\n# 15. save experiment data -----------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All done. Results stored in ./working/\")\n", "import os, random, itertools, pathlib, time\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------------------------------\n# 0. house-keeping --------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------------\n# 1. synthetic data generation (same as before) --------------------------\ndef make_synthetic(\n    shapes,\n    colors,\n    seq_len_range=(4, 10),\n    rule=\"parity\",\n    n_train=2000,\n    n_dev=400,\n    n_test=400,\n):\n    def rand_tok():\n        return random.choice(shapes) + random.choice(colors)\n\n    def rand_seq():\n        return \" \".join(rand_tok() for _ in range(random.randint(*seq_len_range)))\n\n    if rule == \"parity\":\n\n        def label_fn(seq):\n            return (\n                1\n                if sum(tok[0] in shapes[: len(shapes) // 2] for tok in seq.split()) % 2\n                == 0\n                else 0\n            )\n\n    elif rule == \"color_even\":\n        target = set(colors[: len(colors) // 2])\n\n        def label_fn(seq):\n            return 1 if sum(tok[1] in target for tok in seq.split()) % 2 == 0 else 0\n\n    elif rule == \"mod3\":\n\n        def label_fn(seq):\n            return 1 if len(seq.split()) % 3 == 0 else 0\n\n    else:\n        raise ValueError(rule)\n\n    out = {sp: [] for sp in [\"train\", \"dev\", \"test\"]}\n    for split, N in [(\"train\", n_train), (\"dev\", n_dev), (\"test\", n_test)]:\n        for i in range(N):\n            s = rand_seq()\n            out[split].append({\"id\": i, \"sequence\": s, \"label\": label_fn(s)})\n    return out\n\n\nset1 = make_synthetic(list(\"ABCDEF\"), list(\"1234\"), rule=\"parity\")\nset2 = make_synthetic(list(\"GHIJKL\"), list(\"5678\"), rule=\"color_even\")\nset3 = make_synthetic(list(\"MNOPQR\"), list(\"90\"), rule=\"mod3\")\ndatasets = {\"Synthetic1\": set1, \"Synthetic2\": set2, \"Synthetic3\": set3}\nprint({k: {s: len(v[s]) for s in v} for k, v in datasets.items()})\n\n\n# ------------------------------------------------------------------------\n# 2. metrics -------------------------------------------------------------\ndef count_shape_variety(sequence):\n    return len(set(tok[0] for tok in sequence.split()))\n\n\ndef count_color_variety(sequence):\n    return len(set(tok[1] for tok in sequence.split()))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# Augmentation Consistency Rate -----------------------------------------\ndef _augment_tokens(ids, stoi, MASK=\"<MASK>\"):\n    ids = ids.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    return ids\n\n\ndef compute_acr(model, loader, stoi, n_views=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs = batch[\"seq_raw\"]  # list[str]\n            inp = batch[\"input_ids\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            logits, _ = model(inp, lens)\n            base_pred = logits.argmax(1).cpu().tolist()\n            for idx, seq in enumerate(seqs):\n                tok_ids = [stoi.get(t, stoi[\"<UNK>\"]) for t in seq.split()]\n                all_same = True\n                for _ in range(n_views):\n                    aug = torch.tensor(\n                        _augment_tokens(tok_ids, stoi), dtype=torch.long, device=device\n                    ).unsqueeze(0)\n                    aug_len = torch.tensor([aug.size(1)], device=device)\n                    p = model(aug, aug_len)[0].argmax(1).item()\n                    if p != base_pred[idx]:\n                        all_same = False\n                        break\n                consistent += int(all_same)\n                total += 1\n    return consistent / total if total else 0.0\n\n\n# ------------------------------------------------------------------------\n# 3. vocab & tokenisation ------------------------------------------------\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(datasets):\n    vocab = set()\n    for d in datasets.values():\n        for split in d.values():\n            vocab.update(\n                itertools.chain.from_iterable(ex[\"sequence\"].split() for ex in split)\n            )\n    vocab = [PAD, MASK, UNK] + sorted(vocab)\n    return vocab, {t: i for i, t in enumerate(vocab)}\n\n\nvocab, stoi = build_vocab(datasets)\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq):\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# ------------------------------------------------------------------------\n# 4. torch Dataset / DataLoader -----------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, recs, with_label=True):\n        self.recs = recs\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.recs)\n\n    def __getitem__(self, idx):\n        r = self.recs[idx]\n        item = {\n            \"input_ids\": torch.tensor(encode(r[\"sequence\"]), dtype=torch.long),\n            \"seq_raw\": r[\"sequence\"],\n        }\n        if self.with_label:\n            item[\"label\"] = torch.tensor(r[\"label\"], dtype=torch.long)\n        return item\n\n\ndef collate_cls(batch):\n    lens = [len(b[\"input_ids\"]) for b in batch]\n    mx = max(lens)\n    ids = torch.zeros(len(batch), mx, dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids[i, : lens[i]] = b[\"input_ids\"]\n    out = {\n        \"input_ids\": ids,\n        \"lengths\": torch.tensor(lens),\n        \"seq_raw\": [b[\"seq_raw\"] for b in batch],\n    }\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.stack([b[\"label\"] for b in batch])\n    return out\n\n\ndef collate_ctr(batch):\n    views = []\n    for b in batch:\n        base = b[\"input_ids\"].tolist()\n        views.append(torch.tensor(_augment_tokens(base, stoi), dtype=torch.long))\n        views.append(torch.tensor(_augment_tokens(base, stoi), dtype=torch.long))\n    lens = [len(v) for v in views]\n    mx = max(lens)\n    ids = torch.zeros(len(views), mx, dtype=torch.long)\n    for i, v in enumerate(views):\n        ids[i, : len(v)] = v\n    return {\"input_ids\": ids, \"lengths\": torch.tensor(lens)}\n\n\ndef build_loaders(dset, batch=64, contrastive=False):\n    if contrastive:\n        return DataLoader(\n            SPRDataset(dset[\"train\"], with_label=False),\n            batch_size=batch // 2,\n            shuffle=True,\n            collate_fn=collate_ctr,\n            drop_last=True,\n        )\n    else:\n        tr = DataLoader(\n            SPRDataset(dset[\"train\"]),\n            batch_size=batch,\n            shuffle=True,\n            collate_fn=collate_cls,\n        )\n        dv = DataLoader(\n            SPRDataset(dset[\"dev\"]),\n            batch_size=batch,\n            shuffle=False,\n            collate_fn=collate_cls,\n        )\n        te = DataLoader(\n            SPRDataset(dset[\"test\"]),\n            batch_size=batch,\n            shuffle=False,\n            collate_fn=collate_cls,\n        )\n        return tr, dv, te\n\n\ncontrastive_loader_1 = build_loaders(set1, batch=128, contrastive=True)\ntrain1, dev1, test1 = build_loaders(set1)\ntrain2, dev2, test2 = build_loaders(set2)\ntrain3, dev3, test3 = build_loaders(set3)\n\n\n# ------------------------------------------------------------------------\n# 5. model ---------------------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x, lens):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.lin(h_mean)\n\n\nclass Projection(nn.Module):\n    def __init__(self, dim, proj=128):\n        super().__init__()\n        self.mlp = nn.Sequential(nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, proj))\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_labels=2):\n        super().__init__()\n        self.encoder = Encoder(vocab_size)\n        self.cls = nn.Linear(128, num_labels)\n\n    def forward(self, x, lens):\n        rep = self.encoder(x, lens)\n        return self.cls(rep), rep\n\n\n# Corrected NT-Xent -------------------------------------------------------\ndef nt_xent(z, temperature=0.5):\n    z = F.normalize(z, dim=1)\n    N = z.size(0) // 2\n    sim = torch.exp(torch.matmul(z, z.T) / temperature)\n    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim.masked_fill(mask, 0)  # remove self-similarity\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1) - pos  # exclude positives from denom\n    loss = -torch.log(pos / denom)\n    return loss.mean()\n\n\n# ------------------------------------------------------------------------\n# 6. experiment log structure -------------------------------------------\nexperiment_data = {\n    \"Synthetic1\": {\n        \"metrics\": {\"train\": [], \"val\": [], \"ACR\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"Synthetic2\": {\n        \"metrics\": {\"train\": [], \"val\": [], \"ACR\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"Synthetic3\": {\n        \"metrics\": {\"test\": []},\n        \"losses\": {},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n}\n\n\n# ------------------------------------------------------------------------\n# 7. training utilities --------------------------------------------------\ndef move_to_device(batch):\n    return {\n        k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()\n    }\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, model, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    total_loss, seqs, preds, trues = 0.0, [], [], []\n    for batch in loader:\n        batch = move_to_device(batch)\n        logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"label\"]) if \"label\" in batch else None\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        total_loss += loss.item() * batch[\"label\"].size(0)\n        p = logits.argmax(1).detach().cpu().numpy()\n        t = batch[\"label\"].detach().cpu().numpy()\n        preds.extend(p)\n        trues.extend(t)\n        seqs.extend(batch[\"seq_raw\"])\n    swa = shape_weighted_accuracy(seqs, trues, preds)\n    cwa = color_weighted_accuracy(seqs, trues, preds)\n    return total_loss / len(loader.dataset), (swa, cwa), preds, trues\n\n\n# ------------------------------------------------------------------------\n# 8. instantiate model/optim --------------------------------------------\nmodel = SPRModel(len(vocab)).to(device)\nproj = Projection(128).to(device)\nopt_ctr = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\n# ------------------------------------------------------------------------\n# 9. contrastive pre-training -------------------------------------------\nfor ep in range(1, 4):\n    model.train()\n    proj.train()\n    losses = []\n    for batch in contrastive_loader_1:\n        batch = move_to_device(batch)\n        reps = model.encoder(batch[\"input_ids\"], batch[\"lengths\"])\n        z = proj(reps)\n        loss = nt_xent(z)\n        opt_ctr.zero_grad()\n        loss.backward()\n        opt_ctr.step()\n        losses.append(loss.item())\n    print(f\"[Contrastive] epoch {ep} loss={np.mean(losses):.4f}\")\n\n# ------------------------------------------------------------------------\n# 10. supervised training on Synthetic1 ---------------------------------\nopt_cls = torch.optim.Adam(model.parameters(), lr=2e-4)\nbest_val = -1\nfor ep in range(1, 6):\n    tr_loss, (tr_swa, tr_cwa), _, _ = run_epoch(train1, model, opt_cls)\n    val_loss, (val_swa, val_cwa), pD, tD = run_epoch(dev1, model)\n    acr = compute_acr(model, dev1, stoi)\n    experiment_data[\"Synthetic1\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"Synthetic1\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"Synthetic1\"][\"metrics\"][\"train\"].append((tr_swa, tr_cwa))\n    experiment_data[\"Synthetic1\"][\"metrics\"][\"val\"].append((val_swa, val_cwa))\n    experiment_data[\"Synthetic1\"][\"ACR\"].append(acr)\n    print(\n        f\"[S1] Ep{ep} tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"SWA={val_swa:.3f} CWA={val_cwa:.3f} ACR={acr:.3f}\"\n    )\n    if val_cwa > best_val:\n        best_val = val_cwa\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_s1.pt\"))\nexperiment_data[\"Synthetic1\"][\"predictions\"] = pD\nexperiment_data[\"Synthetic1\"][\"ground_truth\"] = tD\n\n# ------------------------------------------------------------------------\n# 11. fine-tune on Synthetic2 -------------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_s1.pt\")))\nopt_ft = torch.optim.Adam(model.parameters(), lr=1e-4)\nfor ep in range(1, 4):\n    tr_loss, (tr_swa, tr_cwa), _, _ = run_epoch(train2, model, opt_ft)\n    val_loss, (val_swa, val_cwa), pD, tD = run_epoch(dev2, model)\n    acr = compute_acr(model, dev2, stoi)\n    experiment_data[\"Synthetic2\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"Synthetic2\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"Synthetic2\"][\"metrics\"][\"train\"].append((tr_swa, tr_cwa))\n    experiment_data[\"Synthetic2\"][\"metrics\"][\"val\"].append((val_swa, val_cwa))\n    experiment_data[\"Synthetic2\"][\"ACR\"].append(acr)\n    print(\n        f\"[S2] Ep{ep} val_loss={val_loss:.4f} SWA={val_swa:.3f} CWA={val_cwa:.3f} ACR={acr:.3f}\"\n    )\nexperiment_data[\"Synthetic2\"][\"predictions\"] = pD\nexperiment_data[\"Synthetic2\"][\"ground_truth\"] = tD\n\n# ------------------------------------------------------------------------\n# 12. zero-shot evaluation on Synthetic3 --------------------------------\ntest_loss, (test_swa, test_cwa), p3, t3 = run_epoch(test3, model)\nexperiment_data[\"Synthetic3\"][\"metrics\"][\"test\"].append((test_swa, test_cwa))\nexperiment_data[\"Synthetic3\"][\"predictions\"] = p3\nexperiment_data[\"Synthetic3\"][\"ground_truth\"] = t3\nprint(f\"[S3] test_loss={test_loss:.4f} SWA={test_swa:.3f} CWA={test_cwa:.3f}\")\n\n# ------------------------------------------------------------------------\n# 13. t-SNE visualisation -----------------------------------------------\nemb, lab = [], []\nmodel.eval()\nwith torch.no_grad():\n    for batch in dev3:\n        batch = move_to_device(batch)\n        _, reps = model(batch[\"input_ids\"], batch[\"lengths\"])\n        emb.append(reps.cpu().numpy())\n        lab.extend(batch[\"label\"].cpu().numpy())\nemb = np.concatenate(emb, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    emb\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=lab, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE Synthetic3 dev\")\nplt.savefig(os.path.join(working_dir, \"tsne_s3_dev.png\"))\n\n# ------------------------------------------------------------------------\n# 14. save experiment data ----------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Finished. All artefacts saved to ./working/\")\n", "import os, random, itertools, time, pathlib, warnings, math, json\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\n# ----------------- mandatory working folder -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- reproducibility & device -----------------\nwarnings.filterwarnings(\"ignore\")\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------- experiment dict --------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\n            \"train_SWA\": [],\n            \"val_SWA\": [],\n            \"train_CWA\": [],\n            \"val_CWA\": [],\n            \"train_ACR\": [],\n            \"val_ACR\": [],\n            \"test_SWA\": [],\n            \"test_CWA\": [],\n            \"test_ACR\": [],\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ----------------- helpers to load (or mock) dataset --------\ndef safe_load_spr():\n    \"\"\"\n    Try to load the real SPR_BENCH folder via SPR.py; otherwise fabricate\n    a tiny synthetic benchmark so that the script is runnable everywhere.\n    \"\"\"\n    try:\n        from SPR import load_spr_bench\n\n        DATA_PATH = pathlib.Path(\"SPR_BENCH\")\n        if DATA_PATH.exists():\n            d = load_spr_bench(DATA_PATH)\n            print(\"Loaded real SPR_BENCH.\")\n            return {k: [dict(r) for r in d[k]] for k in d}\n    except Exception as e:\n        print(\"Could not load real SPR_BENCH \u2013 falling back to synthetic toy set.\", e)\n\n    shapes, colors = list(\"ABCDEFG\"), list(\"123456\")\n\n    def token():\n        return random.choice(shapes) + random.choice(colors)\n\n    def rand_seq():\n        return \" \".join(token() for _ in range(random.randint(4, 10)))\n\n    def label_fn(s):\n        return 1 if sum(tok[0] in \"ABC\" for tok in s.split()) % 2 == 0 else 0\n\n    toy = {sp: [] for sp in [\"train\", \"dev\", \"test\"]}\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        for i in range(n):\n            seq = rand_seq()\n            toy[split].append({\"id\": i, \"sequence\": seq, \"label\": label_fn(seq)})\n    return toy\n\n\ndset = safe_load_spr()\nprint({k: len(v) for k, v in dset.items()})\n\n# ----------------- vocabulary & encoding --------------------\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(seqs):\n    vocab = [PAD, MASK, UNK] + sorted(\n        set(itertools.chain.from_iterable(s.split() for s in seqs))\n    )\n    return vocab, {tok: i for i, tok in enumerate(vocab)}\n\n\nvocab, stoi = build_vocab([r[\"sequence\"] for r in dset[\"train\"]])\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq):\n    return [stoi.get(t, stoi[UNK]) for t in seq.split()]\n\n\n# ----------------- metrics as defined by benchmark ----------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ----------------- dataset & collators ----------------------\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.records, self.with_label = records, with_label\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        out = {\n            \"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long),\n            \"sequence\": rec[\"sequence\"],\n        }\n        if self.with_label:\n            out[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return out\n\n\ndef pad_and_stack(seqs, max_len, dtype=torch.long):\n    out = torch.zeros(len(seqs), max_len, dtype=dtype)\n    for i, s in enumerate(seqs):\n        out[i, : len(s)] = s\n    return out\n\n\ndef collate_classification(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    lens = [len(x) for x in ids]\n    padded = pad_and_stack(ids, max(lens))\n    res = {\n        \"input_ids\": padded.to(device),\n        \"lengths\": torch.tensor(lens).to(device),\n        \"sequence\": [b[\"sequence\"] for b in batch],\n        \"label\": torch.stack([b[\"label\"] for b in batch]).to(device),\n    }\n    return res\n\n\ndef augment_ids(id_list):\n    ids = id_list.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    if len(ids) > 4 and random.random() < 0.3:\n        del ids[random.randint(0, len(ids) - 1)]\n    return ids\n\n\ndef collate_contrastive(batch):\n    bases = [b[\"input_ids\"] for b in batch]\n    views = []\n    for base in bases:\n        views.append(torch.tensor(augment_ids(base.tolist()), dtype=torch.long))\n        views.append(torch.tensor(augment_ids(base.tolist()), dtype=torch.long))\n    lens = [len(v) for v in views]\n    padded = pad_and_stack(views, max(lens))\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lens).to(device)}\n\n\n# ----------------- model ------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, d_model=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, d_model, padding_idx=0)\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x, lengths):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        pooled = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.lin(pooled)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_sz, num_labels):\n        super().__init__()\n        self.encoder = Encoder(vocab_sz)\n        self.projection = nn.Sequential(\n            nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 64)\n        )\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lengths, use_projection=False):\n        rep = self.encoder(x, lengths)\n        if use_projection:\n            return self.projection(rep)\n        return self.classifier(rep), rep\n\n\n# ----------------- NT-Xent loss ------------------------------\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = F.normalize(z, dim=1)\n    sim = torch.exp(z @ z.t() / temperature)\n    mask = ~torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1)\n    return (-torch.log(pos / denom)).mean()\n\n\n# ----------------- dataloaders ------------------------------\nbatch_c = 128\ntrain_con_loader = DataLoader(\n    SPRDataset(dset[\"train\"], with_label=False),\n    batch_size=batch_c // 2,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n    drop_last=True,\n)\ntrain_loader = DataLoader(\n    SPRDataset(dset[\"train\"]),\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_classification,\n)\nval_loader = DataLoader(\n    SPRDataset(dset[\"dev\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\ntest_loader = DataLoader(\n    SPRDataset(dset[\"test\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\n\n# ----------------- model init & optimizers ------------------\nmodel = SPRModel(len(vocab), num_labels=len(set(r[\"label\"] for r in dset[\"train\"]))).to(\n    device\n)\nopt_contrastive = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(model.projection.parameters()), lr=1e-3\n)\n\n# ----------------- contrastive pre-training -----------------\nepochs_ct = 3\nfor ep in range(1, epochs_ct + 1):\n    model.train()\n    losses = []\n    for batch in train_con_loader:\n        z = model(batch[\"input_ids\"], batch[\"lengths\"], use_projection=True)\n        loss = nt_xent(z)\n        opt_contrastive.zero_grad()\n        loss.backward()\n        opt_contrastive.step()\n        losses.append(loss.item())\n    print(f\"[Pretrain] epoch {ep}/{epochs_ct} loss={np.mean(losses):.4f}\")\n\n# ----------------- fine-tuning classifier -------------------\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\n\ndef compute_acr(seqs, preds_fn, k=3):\n    \"\"\"\n    Augmentation Consistency Rate: fraction whose predictions remain identical\n    over k augmented views.\n    \"\"\"\n    consistent = 0\n    with torch.no_grad():\n        for s in seqs:\n            base_ids = torch.tensor(encode(s), dtype=torch.long).unsqueeze(0).to(device)\n            base_len = torch.tensor([base_ids.size(1)]).to(device)\n            base_pred = preds_fn(base_ids, base_len)\n            ok = True\n            for _ in range(k):\n                aug = (\n                    torch.tensor(augment_ids(encode(s)), dtype=torch.long)\n                    .unsqueeze(0)\n                    .to(device)\n                )\n                alen = torch.tensor([aug.size(1)]).to(device)\n                if preds_fn(aug, alen) != base_pred:\n                    ok = False\n                    break\n            consistent += int(ok)\n    return consistent / len(seqs) if seqs else 0.0\n\n\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    total_loss, all_preds, all_truths, all_seqs = 0.0, [], [], []\n    for batch in loader:\n        logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"label\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"label\"].size(0)\n        preds = logits.argmax(1).cpu().numpy()\n        truths = batch[\"label\"].cpu().numpy()\n        all_preds.extend(preds)\n        all_truths.extend(truths)\n        all_seqs.extend(batch[\"sequence\"])\n    swa = shape_weighted_accuracy(all_seqs, all_truths, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_truths, all_preds)\n    acr = compute_acr(\n        all_seqs, lambda x, l: model(x, l)[0].argmax(1).item(), k=2\n    )  # k=2 for speed\n    return (\n        total_loss / len(loader.dataset),\n        swa,\n        cwa,\n        acr,\n        all_preds,\n        all_truths,\n        all_seqs,\n    )\n\n\nbest_val_swa = 0\nclf_epochs = 5\nfor ep in range(1, clf_epochs + 1):\n    tr_loss, tr_swa, tr_cwa, tr_acr, *_ = run_epoch(train_loader, True)\n    val_loss, val_swa, val_cwa, val_acr, *_ = run_epoch(val_loader, False)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_SWA\"].append(tr_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_SWA\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_CWA\"].append(tr_cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_CWA\"].append(val_cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_ACR\"].append(tr_acr)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_ACR\"].append(val_acr)\n\n    print(\n        f\"Epoch {ep}: \"\n        f\"train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"val_SWA={val_swa:.4f} val_CWA={val_cwa:.4f} val_ACR={val_acr:.4f}\"\n    )\n\n    best_val_swa = max(best_val_swa, val_swa)\n\n# ----------------- final test evaluation --------------------\ntest_loss, test_swa, test_cwa, test_acr, preds, trues, seqs = run_epoch(\n    test_loader, False\n)\nprint(f\"Test: SWA={test_swa:.4f}  CWA={test_cwa:.4f}  ACR={test_acr:.4f}\")\n\nexp = experiment_data[\"SPR_BENCH\"]\nexp[\"metrics\"][\"test_SWA\"].append(test_swa)\nexp[\"metrics\"][\"test_CWA\"].append(test_cwa)\nexp[\"metrics\"][\"test_ACR\"].append(test_acr)\nexp[\"predictions\"] = preds\nexp[\"ground_truth\"] = trues\n\n# ----------------- save everything --------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", working_dir)\n", "import os, random, math, pathlib, itertools, time\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom typing import List\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------------------\n# mandatory working dir + gpu setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# dataset -----------------------------------------------------\ndef safe_load_spr():\n    try:\n        from SPR import load_spr_bench\n\n        DATA_PATH = pathlib.Path(\"SPR_BENCH\")\n        if DATA_PATH.exists():\n            return load_spr_bench(DATA_PATH)\n    except Exception as e:\n        print(\"Could not load real SPR_BENCH, falling back to synthetic data.\", e)\n\n    # synthetic fallback\n    shapes, colors = list(\"ABCDEFG\"), list(\"123456\")\n\n    def rand_tok():\n        return random.choice(shapes) + random.choice(colors)\n\n    def rand_seq():\n        return \" \".join(rand_tok() for _ in range(random.randint(4, 10)))\n\n    def label_fn(seq):\n        return 1 if sum(t[0] in \"ABC\" for t in seq.split()) % 2 == 0 else 0\n\n    data = {\"train\": [], \"dev\": [], \"test\": []}\n    for split, n in [(\"train\", 2000), (\"dev\", 400), (\"test\", 400)]:\n        for i in range(n):\n            seq = rand_seq()\n            data[split].append({\"id\": i, \"sequence\": seq, \"label\": label_fn(seq)})\n    return data\n\n\ndset = safe_load_spr()\nprint({k: len(v) for k, v in dset.items()})\n\n\n# ------------------------------------------------------------\n# metrics -----------------------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.split()))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.split()))\n\n\ndef cwca(seqs, y_true, y_pred):\n    w = [(count_shape_variety(s) + count_color_variety(s)) / 2 for s in seqs]\n    correct = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ------------------------------------------------------------\n# vocab / encode ---------------------------------------------\nPAD, MASK, UNK = \"<PAD>\", \"<MASK>\", \"<UNK>\"\n\n\ndef build_vocab(seqs):\n    vocab = set(itertools.chain.from_iterable(s.split() for s in seqs))\n    vocab = [PAD, MASK, UNK] + sorted(vocab)\n    stoi = {tok: i for i, tok in enumerate(vocab)}\n    return vocab, stoi\n\n\nvocab, stoi = build_vocab([ex[\"sequence\"] for ex in dset[\"train\"]])\nitos = {i: s for s, i in stoi.items()}\n\n\ndef encode(seq: str) -> List[int]:\n    return [stoi.get(tok, stoi[UNK]) for tok in seq.split()]\n\n\n# ------------------------------------------------------------\n# datasets ----------------------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, records, with_label=True):\n        self.records = records\n        self.with_label = with_label\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        out = {\"input_ids\": torch.tensor(encode(rec[\"sequence\"]), dtype=torch.long)}\n        if self.with_label:\n            out[\"label\"] = torch.tensor(rec[\"label\"], dtype=torch.long)\n        return out\n\n\ndef collate_classification(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    lens = [len(s) for s in seqs]\n    mx = max(lens)\n    padded = torch.zeros(len(batch), mx, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\n        \"input_ids\": padded.to(device),\n        \"lengths\": torch.tensor(lens).to(device),\n        \"label\": labels.to(device),\n    }\n\n\ndef augment(ids):\n    ids = ids.copy()\n    for i in range(len(ids)):\n        if random.random() < 0.15:\n            ids[i] = stoi[MASK]\n    for i in range(len(ids) - 1):\n        if random.random() < 0.1:\n            ids[i], ids[i + 1] = ids[i + 1], ids[i]\n    if len(ids) > 4 and random.random() < 0.3:\n        del ids[random.randint(0, len(ids) - 1)]\n    return ids\n\n\ndef collate_contrastive(batch):\n    base = [b[\"input_ids\"] for b in batch]\n    views = []\n    for ids in base:\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n        views.append(torch.tensor(augment(ids.tolist()), dtype=torch.long))\n    lens = [len(v) for v in views]\n    mx = max(lens)\n    padded = torch.zeros(len(views), mx, dtype=torch.long)\n    for i, v in enumerate(views):\n        padded[i, : len(v)] = v\n    return {\"input_ids\": padded.to(device), \"lengths\": torch.tensor(lens).to(device)}\n\n\n# ------------------------------------------------------------\n# model -------------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d, padding_idx=0)\n        self.lstm = nn.LSTM(d, d, batch_first=True, bidirectional=True)\n        self.lin = nn.Linear(d * 2, d)\n\n    def forward(self, x, lens):\n        emb = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        h, _ = self.lstm(packed)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n        mask = (x != 0).unsqueeze(-1)\n        h_mean = (h * mask).sum(1) / (mask.sum(1) + 1e-6)\n        return self.lin(h_mean)\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, dim, proj=128):\n        super().__init__()\n        self.mlp = nn.Sequential(nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, proj))\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_labels):\n        super().__init__()\n        self.encoder = Encoder(vocab_size)\n        self.classifier = nn.Linear(128, num_labels)\n\n    def forward(self, x, lens):\n        rep = self.encoder(x, lens)\n        return self.classifier(rep), rep\n\n\n# ------------------------------------------------------------\n# contrastive loss -------------------------------------------\ndef nt_xent(z, temperature=0.5):\n    N = z.size(0) // 2\n    z = F.normalize(z, dim=1)\n    sim = torch.exp(torch.matmul(z, z.T) / temperature)\n    mask = ~torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim = sim * mask\n    pos = torch.cat([torch.diag(sim, N), torch.diag(sim, -N)], 0)\n    denom = sim.sum(1)\n    return (-torch.log(pos / denom)).mean()\n\n\n# ------------------------------------------------------------\n# loaders -----------------------------------------------------\nbatch_c = 128\ntrain_contrastive_loader = DataLoader(\n    SPRDataset(dset[\"train\"], with_label=False),\n    batch_size=batch_c // 2,\n    shuffle=True,\n    drop_last=True,\n    collate_fn=collate_contrastive,\n)\n\ntrain_loader = DataLoader(\n    SPRDataset(dset[\"train\"]),\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_classification,\n)\ndev_loader = DataLoader(\n    SPRDataset(dset[\"dev\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\ntest_loader = DataLoader(\n    SPRDataset(dset[\"test\"]),\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_classification,\n)\n\n# ------------------------------------------------------------\n# experiment data dict ---------------------------------------\nexperiment_data = {\n    \"FreezeEncoderFineTuning\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------\n# build model & pretrain -------------------------------------\nmodel = SPRModel(len(vocab), num_labels=len(set(r[\"label\"] for r in dset[\"train\"]))).to(\n    device\n)\nproj = ProjectionHead(128).to(device)\nopt_ct = torch.optim.Adam(\n    list(model.encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\nepochs_ct = 3\nfor ep in range(1, epochs_ct + 1):\n    model.train()\n    proj.train()\n    losses = []\n    for b in train_contrastive_loader:\n        rep = model.encoder(b[\"input_ids\"], b[\"lengths\"])\n        z = proj(rep)\n        loss = nt_xent(z)\n        opt_ct.zero_grad()\n        loss.backward()\n        opt_ct.step()\n        losses.append(loss.item())\n    print(f\"[Pretrain] {ep}/{epochs_ct} loss={np.mean(losses):.4f}\")\n\n# ------------------------------------------------------------\n# freeze encoder for ablation -------------------------------\nfor p in model.encoder.parameters():\n    p.requires_grad = False\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.classifier.parameters(), lr=2e-4)\n\n\ndef run_epoch(loader, train=True):\n    (model.train() if train else model.eval())\n    tot_loss, preds, trues, seqs = 0.0, [], [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            logits, _ = model(batch[\"input_ids\"], batch[\"lengths\"])\n            loss = criterion(logits, batch[\"label\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"label\"].size(0)\n            pred = logits.argmax(1).cpu().numpy()\n            true = batch[\"label\"].cpu().numpy()\n            preds.extend(pred)\n            trues.extend(true)\n            seqs.extend(\n                [\n                    \" \".join(itos[idx.item()] for idx in row if idx.item() != 0)\n                    for row in batch[\"input_ids\"].cpu()\n                ]\n            )\n    return tot_loss / len(loader.dataset), cwca(seqs, trues, preds), preds, trues\n\n\nbest_val = 0\nepochs_clf = 5\nfor ep in range(1, epochs_clf + 1):\n    tr_loss, tr_cwca, _, _ = run_epoch(train_loader, True)\n    v_loss, v_cwca, _, _ = run_epoch(dev_loader, False)\n    experiment_data[\"FreezeEncoderFineTuning\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        tr_loss\n    )\n    experiment_data[\"FreezeEncoderFineTuning\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        v_loss\n    )\n    experiment_data[\"FreezeEncoderFineTuning\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        tr_cwca\n    )\n    experiment_data[\"FreezeEncoderFineTuning\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        v_cwca\n    )\n    print(\n        f\"[Finetune] Ep{ep}: train_loss={tr_loss:.4f} val_loss={v_loss:.4f} val_CWCA={v_cwca:.4f}\"\n    )\n    best_val = max(best_val, v_cwca)\n\n# ------------------------------------------------------------\n# final test --------------------------------------------------\ntest_loss, test_cwca, preds, trues = run_epoch(test_loader, False)\nprint(f\"Test CWCA = {test_cwca:.4f}\")\ned = experiment_data[\"FreezeEncoderFineTuning\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test\"].append(test_cwca)\ned[\"predictions\"] = preds\ned[\"ground_truth\"] = trues\n\n# ------------------------------------------------------------\n# t-SNE -------------------------------------------------------\nmodel.eval()\nembs, lbls = [], []\nwith torch.no_grad():\n    for batch in dev_loader:\n        _, rep = model(batch[\"input_ids\"], batch[\"lengths\"])\n        embs.append(rep.cpu().numpy())\n        lbls.extend(batch[\"label\"].cpu().numpy())\nembs = np.concatenate(embs, 0)\ntsne = TSNE(n_components=2, init=\"random\", perplexity=30, random_state=0).fit_transform(\n    embs\n)\nplt.figure(figsize=(6, 5))\nplt.scatter(tsne[:, 0], tsne[:, 1], c=lbls, cmap=\"tab10\", s=10)\nplt.title(\"t-SNE (encoder frozen)\")\nplt.savefig(os.path.join(working_dir, \"tsne_dev_frozen.png\"))\n\n# ------------------------------------------------------------\n# save data ---------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to ./working/experiment_data.npy\")\n"], "term_out": ["['Using device: cuda', '\\n', 'Could not load real SPR_BENCH, falling back to\nsynthetic data.', ' ', \"No module named 'SPR'\", '\\n', \"{'train': 2000, 'dev':\n400, 'test': 400}\", '\\n', '[Pretrain] Epoch 1/3 - loss=4.8471', '\\n',\n'[Pretrain] Epoch 2/3 - loss=4.8449', '\\n', '[Pretrain] Epoch 3/3 -\nloss=4.8431', '\\n', 'Epoch 1: train_loss=0.6953  val_loss=0.6938\nval_CWCA=0.4763', '\\n', 'Epoch 2: train_loss=0.6904  val_loss=0.6943\nval_CWCA=0.4847', '\\n', 'Epoch 3: train_loss=0.6881  val_loss=0.6946\nval_CWCA=0.5049', '\\n', 'Epoch 4: train_loss=0.6853  val_loss=0.6956\nval_CWCA=0.5257', '\\n', 'Epoch 5: train_loss=0.6834  val_loss=0.6978\nval_CWCA=0.5144', '\\n', 'Test CWCA = 0.4764', '\\n', 'Experiment data and plot\nsaved to ./working/', '\\n', 'Execution time: a minute seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Could not load real SPR_BENCH; falling back to\nsynthetic.', ' ', \"No module named 'SPR'\", '\\n', \"{'train': 2000, 'dev': 400,\n'test': 400}\", '\\n', 'Epoch 1: train_loss=0.6949  val_loss=0.6943\nval_CWCA=0.4902', '\\n', 'Epoch 2: train_loss=0.6896  val_loss=0.6946\nval_CWCA=0.5007', '\\n', 'Epoch 3: train_loss=0.6877  val_loss=0.6943\nval_CWCA=0.4921', '\\n', 'Epoch 4: train_loss=0.6852  val_loss=0.6951\nval_CWCA=0.4896', '\\n', 'Epoch 5: train_loss=0.6830  val_loss=0.6976\nval_CWCA=0.4961', '\\n', 'Test CWCA = 0.5107', '\\n', 'Saved experiment data and\nplot to ./working/', '\\n', 'Execution time: 3 minutes seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Could not load real SPR_BENCH, using\nsynthetic data.', ' ', \"No module named 'SPR'\", '\\n', \"{'train': 2000, 'dev':\n400, 'test': 400}\", '\\n', '[Pretrain] Epoch 1/3 loss=4.8619', '\\n', '[Pretrain]\nEpoch 2/3 loss=4.8462', '\\n', '[Pretrain] Epoch 3/3 loss=4.8457', '\\n', 'Epoch\n1: train_loss=0.6961  val_loss=0.6943  val_CWCA=0.5337', '\\n', 'Epoch 2:\ntrain_loss=0.6905  val_loss=0.6961  val_CWCA=0.5157', '\\n', 'Epoch 3:\ntrain_loss=0.6873  val_loss=0.6981  val_CWCA=0.5075', '\\n', 'Epoch 4:\ntrain_loss=0.6845  val_loss=0.7024  val_CWCA=0.4987', '\\n', 'Epoch 5:\ntrain_loss=0.6821  val_loss=0.7045  val_CWCA=0.4965', '\\n', 'Test CWCA =', ' ',\n'0.5192697768762677', '\\n', 'Saved metrics & plot to', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-16_00-46-\n17_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n21/working', '\\n', 'Execution time: 3 minutes seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', \"{'Synthetic1': {'train': 2000, 'dev': 400, 'test':\n400}, 'Synthetic2': {'train': 2000, 'dev': 400, 'test': 400}, 'Synthetic3':\n{'train': 2000, 'dev': 400, 'test': 400}}\", '\\n', '[Contrastive] Epoch 1/3\nloss=4.8462', '\\n', '[Contrastive] Epoch 2/3 loss=4.8451', '\\n', '[Contrastive]\nEpoch 3/3 loss=4.8445', '\\n', '[S1] Ep1: train_loss=0.6934 val_loss=0.6913\nval_CWCA=0.5327', '\\n', '[S1] Ep2: train_loss=0.6906 val_loss=0.6920\nval_CWCA=0.5557', '\\n', '[S1] Ep3: train_loss=0.6888 val_loss=0.6924\nval_CWCA=0.5418', '\\n', '[S1] Ep4: train_loss=0.6874 val_loss=0.6928\nval_CWCA=0.5304', '\\n', '[S1] Ep5: train_loss=0.6870 val_loss=0.6963\nval_CWCA=0.4917', '\\n', '[S2 fine-tune] Ep1: train_loss=0.6952 val_loss=0.6932\nval_CWCA=0.5262', '\\n', '[S2 fine-tune] Ep2: train_loss=0.6921 val_loss=0.6925\nval_CWCA=0.5321', '\\n', '[S2 fine-tune] Ep3: train_loss=0.6905 val_loss=0.6928\nval_CWCA=0.5184', '\\n', '[S3 Test] loss=0.6695 CWCA=0.7042', '\\n', 'All done.\nResults stored in ./working/', '\\n', 'Execution time: 3 minutes seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Could not load real SPR_BENCH, falling back to\nsynthetic.', ' ', \"No module named 'SPR'\", '\\n', \"{'train': 2000, 'dev': 400,\n'test': 400}\", '\\n', '[Pretrain] Epoch 1/3 loss=4.8539', '\\n', '[Pretrain] Epoch\n2/3 loss=4.8483', '\\n', '[Pretrain] Epoch 3/3 loss=4.8458', '\\n', 'Epoch 1:\ntrain_loss=0.6982 val_loss=0.6949 val_CWCA=0.5038', '\\n', 'Epoch 2:\ntrain_loss=0.6903 val_loss=0.6968 val_CWCA=0.5169', '\\n', 'Epoch 3:\ntrain_loss=0.6879 val_loss=0.6989 val_CWCA=0.4945', '\\n', 'Epoch 4:\ntrain_loss=0.6868 val_loss=0.7030 val_CWCA=0.4894', '\\n', 'Epoch 5:\ntrain_loss=0.6850 val_loss=0.7043 val_CWCA=0.5016', '\\n', 'Test CWCA = 0.4897',\n'\\n', 'Saved experiment data and plot to ./working/', '\\n', 'Execution time: 3\nminutes seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'Synthetic1': {'train': 2000, 'dev': 400, 'test':\n400}, 'Synthetic2': {'train': 2000, 'dev': 400, 'test': 400}, 'Synthetic3':\n{'train': 2000, 'dev': 400, 'test': 400}}\", '\\n', '[Contrastive] Epoch 1/3\nloss=3.8219', '\\n', '[Contrastive] Epoch 2/3 loss=3.4818', '\\n', '[Contrastive]\nEpoch 3/3 loss=3.4466', '\\n', '[S1] Ep1: train_loss=0.6971 val_loss=0.6885\nval_CWCA=0.5668 val_ACR=0.8175', '\\n', '[S1] Ep2: train_loss=0.6921\nval_loss=0.6900 val_CWCA=0.5339 val_ACR=0.7850', '\\n', '[S1] Ep3:\ntrain_loss=0.6902 val_loss=0.6905 val_CWCA=0.5277 val_ACR=0.7600', '\\n', '[S1]\nEp4: train_loss=0.6889 val_loss=0.6933 val_CWCA=0.5101 val_ACR=0.6975', '\\n',\n'[S1] Ep5: train_loss=0.6869 val_loss=0.6915 val_CWCA=0.5368 val_ACR=0.7075',\n'\\n', '[S2 FT] Ep1: train_loss=0.6943 val_loss=0.6934 val_CWCA=0.4764\nval_ACR=0.6500', '\\n', '[S2 FT] Ep2: train_loss=0.6922 val_loss=0.6942\nval_CWCA=0.4939 val_ACR=0.7125', '\\n', '[S2 FT] Ep3: train_loss=0.6910\nval_loss=0.6946 val_CWCA=0.4955 val_ACR=0.7150', '\\n', '[S3 TEST] loss=0.6721\nCWCA=0.6958 ACR=1.0000', '\\n', 'All done. Results stored in ./working/', '\\n',\n'Execution time: 15 minutes seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'Synthetic1': {'train': 2000, 'dev': 400, 'test':\n400}, 'Synthetic2': {'train': 2000, 'dev': 400, 'test': 400}, 'Synthetic3':\n{'train': 2000, 'dev': 400, 'test': 400}}\", '\\n', '[Contrastive] epoch 1\nloss=4.8380', '\\n', '[Contrastive] epoch 2 loss=4.8371', '\\n', '[Contrastive]\nepoch 3 loss=4.8372', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 395, in <module>\\n\nexperiment_data[\"Synthetic1\"][\"ACR\"].append(acr)\\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\\nKeyError: \\'ACR\\'\\n', 'Execution time: 4\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Could not load real SPR_BENCH \u2013 falling back to\nsynthetic toy set.', ' ', \"No module named 'SPR'\", '\\n', \"{'train': 2000, 'dev':\n400, 'test': 400}\", '\\n', '[Pretrain] epoch 1/3 loss=4.8460', '\\n', '[Pretrain]\nepoch 2/3 loss=4.8455', '\\n', '[Pretrain] epoch 3/3 loss=4.8437', '\\n', 'Epoch\n1: train_loss=0.6930 val_loss=0.6939 val_SWA=0.5176 val_CWA=0.5153\nval_ACR=0.8775', '\\n', 'Epoch 2: train_loss=0.6894 val_loss=0.6968\nval_SWA=0.5019 val_CWA=0.4947 val_ACR=0.8450', '\\n', 'Epoch 3: train_loss=0.6866\nval_loss=0.6982 val_SWA=0.5208 val_CWA=0.5130 val_ACR=0.8525', '\\n', 'Epoch 4:\ntrain_loss=0.6839 val_loss=0.7014 val_SWA=0.5198 val_CWA=0.5106 val_ACR=0.8125',\n'\\n', 'Epoch 5: train_loss=0.6805 val_loss=0.7069 val_SWA=0.5295 val_CWA=0.5206\nval_ACR=0.7825', '\\n', 'Test: SWA=0.5158  CWA=0.5248  ACR=0.8125', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n16_00-46-17_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-22/working', '\\n', 'Execution time: 33 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Could not load real SPR_BENCH, falling back to\nsynthetic data.', ' ', \"No module named 'SPR'\", '\\n', \"{'train': 2000, 'dev':\n400, 'test': 400}\", '\\n', '[Pretrain] 1/3 loss=4.8459', '\\n', '[Pretrain] 2/3\nloss=4.8472', '\\n', '[Pretrain] 3/3 loss=4.8449', '\\n', '[Finetune] Ep1:\ntrain_loss=0.6924 val_loss=0.6924 val_CWCA=0.5242', '\\n', '[Finetune] Ep2:\ntrain_loss=0.6921 val_loss=0.6920 val_CWCA=0.5203', '\\n', '[Finetune] Ep3:\ntrain_loss=0.6918 val_loss=0.6916 val_CWCA=0.5208', '\\n', '[Finetune] Ep4:\ntrain_loss=0.6917 val_loss=0.6916 val_CWCA=0.5120', '\\n', '[Finetune] Ep5:\ntrain_loss=0.6915 val_loss=0.6914 val_CWCA=0.5088', '\\n', 'Test CWCA = 0.4834',\n'\\n', 'Experiment data saved to ./working/experiment_data.npy', '\\n', 'Execution\ntime: a minute seconds (time limit is 30 minutes).']"], "analysis": ["", "", "The execution log indicates that the real SPR_BENCH dataset could not be loaded\ndue to a missing module ('No module named SPR'). As a result, the script\ndefaulted to using synthetic data instead of the actual SPR_BENCH dataset. This\nimpacts the validity of the results since the synthetic data may not represent\nthe complexity of the real dataset. To fix this, ensure that the SPR_BENCH\ndataset is properly set up and the required 'SPR' module is installed or\naccessible in the environment before running the script.", "The execution output reveals that the contrastive pretraining phase did not\nconverge effectively. The loss values during the contrastive pretraining phase\nremained nearly constant across all epochs (loss=4.8462, 4.8451, 4.8445),\nindicating that the model is not learning meaningful representations. This issue\nlikely stems from an excessively high temperature parameter in the nt_xent loss\nfunction or suboptimal hyperparameters for the contrastive learning setup.\nProposed Fix: 1. Lower the temperature parameter in the nt_xent loss function\n(e.g., from 0.5 to 0.1) to increase the sensitivity of the similarity scores. 2.\nExperiment with different learning rates or optimizers for the contrastive\npretraining phase. 3. Add more diverse augmentations to create effective\npositive and negative pairs for contrastive learning. 4. Monitor gradient values\nto ensure they are not vanishing or exploding during training.", "", "", "The execution failed due to a KeyError: 'ACR'. This error occurred because the\nkey 'ACR' is not properly initialized in the 'Synthetic1' dictionary within the\n'experiment_data' structure. To fix this, ensure that the 'ACR' key is properly\ninitialized in the 'Synthetic1' dictionary before it is accessed. Specifically,\nin the initialization of 'experiment_data', add an empty list for the 'ACR' key\nunder 'Synthetic1'. For example: '\"Synthetic1\": {\"metrics\": {\"train\": [], \"val\":\n[], \"ACR\": []}, ...}'.", "", "The execution output reveals multiple issues:  1. **SPR_BENCH Dataset Loading\nIssue:** The script could not load the real SPR_BENCH dataset due to a missing\nmodule ('SPR'). As a result, the script fell back to using synthetic data, which\nmight not represent the actual SPR_BENCH dataset's characteristics.\n**Proposed Fix:** Ensure the 'SPR' module is correctly installed and accessible\nin the environment. Verify the path to the SPR_BENCH dataset and ensure it\nexists.  2. **Contrastive Pretraining Loss:** The pretraining loss did not show\nsignificant improvement over the epochs, indicating a possible issue with the\ncontrastive learning setup (e.g., ineffective augmentations, unsuitable\ntemperature parameter, or inadequate model capacity).     **Proposed Fix:**\nRevisit the contrastive learning setup. Experiment with different augmentation\nstrategies, temperature values, and model configurations to improve the\npretraining effectiveness.  3. **Fine-tuning Performance:** The fine-tuning\nphase showed poor CWCA performance on the validation and test sets, with CWCA\nvalues consistently below 0.53. This indicates that the model is not learning\neffectively from the frozen encoder representations.     **Proposed Fix:**\nInvestigate the quality of the frozen encoder representations. Consider allowing\nthe encoder to fine-tune partially or fully during this phase. Additionally,\nevaluate the classifier architecture and hyperparameters.  4. **t-SNE\nVisualization:** While the t-SNE plot was generated, its utility is unclear\nwithout analyzing the plot. Ensure the visualization provides meaningful\ninsights into the learned embeddings.     **Proposed Fix:** Examine the t-SNE\nplot to identify clustering patterns. If patterns are not evident, assess the\nencoder's ability to produce discriminative embeddings.  By addressing these\nissues, the experiment's outcomes can be improved significantly."], "exc_type": [null, null, null, null, null, null, "KeyError", null, null], "exc_info": [null, null, null, null, null, null, {"args": ["ACR"]}, null, null], "exc_stack": [null, null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 395, "<module>", "experiment_data[\"Synthetic1\"][\"ACR\"].append(acr)"]], null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "CWCA accuracy", "lower_is_better": false, "description": "CWCA accuracy measures the classification performance of the model.", "data": [{"dataset_name": "training", "final_value": 0.5629, "best_value": 0.5629}, {"dataset_name": "validation", "final_value": 0.5257, "best_value": 0.5257}, {"dataset_name": "test", "final_value": 0.4764, "best_value": 0.4764}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error in the model's predictions.", "data": [{"dataset_name": "training", "final_value": 0.6834, "best_value": 0.6834}, {"dataset_name": "validation", "final_value": 0.6938, "best_value": 0.6938}]}]}, {"metric_names": [{"metric_name": "CWCA", "lower_is_better": false, "description": "A metric for evaluating classification performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5107, "best_value": 0.5107}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the error between predictions and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.683, "best_value": 0.683}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, used to evaluate model performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6976, "best_value": 0.6976}]}]}, {"metric_names": [{"metric_name": "CWCA", "lower_is_better": false, "description": "Class Weighted Classification Accuracy", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5193, "best_value": 0.5337}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss value during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6943, "best_value": 0.6821}]}]}, {"metric_names": [{"metric_name": "CWCA", "lower_is_better": false, "description": "Cumulative Weighted Classification Accuracy", "data": [{"dataset_name": "Synthetic1", "final_value": 0.5436, "best_value": 0.5557}, {"dataset_name": "Synthetic2", "final_value": 0.5279, "best_value": 0.5321}, {"dataset_name": "Synthetic3", "final_value": 0.7042, "best_value": 0.7042}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "Loss value indicating model error", "data": [{"dataset_name": "Synthetic1", "final_value": 0.687, "best_value": 0.687}, {"dataset_name": "Synthetic2", "final_value": 0.6905, "best_value": 0.6905}]}]}, {"metric_names": [{"metric_name": "CWCA", "lower_is_better": false, "description": "Class-weighted classification accuracy", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4897, "best_value": 0.5436}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Cross-entropy loss", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6949, "best_value": 0.685}]}]}, {"metric_names": [{"metric_name": "CWCA", "lower_is_better": false, "description": "Class Weighted Classification Accuracy", "data": [{"dataset_name": "Synthetic1", "final_value": 0.5668, "best_value": 0.5668}, {"dataset_name": "Synthetic2", "final_value": 0.4955, "best_value": 0.4955}, {"dataset_name": "Synthetic3", "final_value": 0.6958, "best_value": 0.6958}]}, {"metric_name": "ACR", "lower_is_better": false, "description": "Area under the Curve of the Receiver operating characteristic", "data": [{"dataset_name": "Synthetic1", "final_value": 0.8175, "best_value": 0.8175}, {"dataset_name": "Synthetic2", "final_value": 0.715, "best_value": 0.715}, {"dataset_name": "Synthetic3", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss value during training or evaluation", "data": [{"dataset_name": "Synthetic1", "final_value": 0.6885, "best_value": 0.6869}, {"dataset_name": "Synthetic2", "final_value": 0.6934, "best_value": 0.691}, {"dataset_name": "Synthetic3", "final_value": 0.6721, "best_value": 0.6721}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the model's error.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6805, "best_value": 0.6805}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating the model's error.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "training shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy during training, weighted by shape.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5647, "best_value": 0.5647}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy during validation, weighted by shape.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5295, "best_value": 0.5295}]}, {"metric_name": "training color-weighted accuracy", "lower_is_better": false, "description": "The accuracy during training, weighted by color.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5646, "best_value": 0.5646}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The accuracy during validation, weighted by color.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5206, "best_value": 0.5206}]}, {"metric_name": "training augmentation consistency rate", "lower_is_better": false, "description": "The consistency rate of augmentations during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.884, "best_value": 0.884}]}, {"metric_name": "validation augmentation consistency rate", "lower_is_better": false, "description": "The consistency rate of augmentations during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8775, "best_value": 0.8775}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy on the test set, weighted by shape.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5158, "best_value": 0.5158}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The accuracy on the test set, weighted by color.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5248, "best_value": 0.5248}]}, {"metric_name": "test augmentation consistency rate", "lower_is_better": false, "description": "The consistency rate of augmentations on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8125, "best_value": 0.8125}]}]}, {"metric_names": [{"metric_name": "CWCA", "lower_is_better": false, "description": "Class-Weighted Classification Accuracy, a metric for evaluating classification performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4834, "best_value": 0.5242}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The loss function value, indicating the error of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6915, "best_value": 0.6914}]}]}], "is_best_node": [false, false, false, false, false, false, false, true, false], "plots": [["../../logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/tsne_dev.png", "../../logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_cwca_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_loss_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_test_cwca_bar.png"], ["../../logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/tsne_dev.png", "../../logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_cwca_curves.png", "../../logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_confusion_matrix.png"], [], [], ["../../logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/tsne_dev.png", "../../logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_loss_curve.png", "../../logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_cwca_curve.png", "../../logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/tsne_dev.png", "../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/tsne_s3_dev_fixed.png", "../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic1_loss_curve.png", "../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic1_metrics_curve.png", "../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic2_metrics_curve.png", "../../logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic3_test_metrics.png"], [], ["../../logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/tsne_s3_dev.png", "../../logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_SWA_curve.png", "../../logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_CWA_ACR_curve.png", "../../logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_confusion_matrix.png"], []], "plot_paths": [["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/tsne_dev.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_cwca_train_val_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_loss_train_val_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_test_cwca_bar.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/tsne_dev.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_cwca_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_confusion_matrix.png"], [], [], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/tsne_dev.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_loss_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_cwca_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_confusion_matrix.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/tsne_dev.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/tsne_s3_dev_fixed.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic1_loss_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic1_metrics_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic2_loss_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic2_metrics_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic3_test_metrics.png"], [], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/tsne_s3_dev.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_SWA_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_CWA_ACR_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_confusion_matrix.png"], []], "plot_analyses": [[{"analysis": "The t-SNE visualization of the development embeddings shows a scattered distribution with no clear clustering pattern. This indicates that the learned embeddings may lack sufficient structure or differentiation to group sequences based on their underlying patterns. This might suggest that the context-aware contrastive learning framework needs further refinement to better capture the symbolic sequence relationships.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/tsne_dev.png"}, {"analysis": "The CWCA curves show that while the training CWCA improves consistently over the epochs, the validation CWCA initially increases but then starts to decline after epoch 4. This suggests potential overfitting, where the model starts to perform well on the training set but loses generalization capability on the validation set. Regularization techniques or early stopping might help address this issue.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_cwca_train_val_curves.png"}, {"analysis": "The loss curves indicate that while the training loss steadily decreases, the validation loss increases slightly after epoch 3. This divergence further supports the hypothesis of overfitting and suggests that the model's capacity to generalize to unseen data is limited under the current setup.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_loss_train_val_curves.png"}, {"analysis": "The final test CWCA value of 0.476 is significantly below the current SOTA of 70.0%. This indicates that the proposed model and training framework are not yet competitive with the benchmark. Additional efforts are needed to improve the model's ability to capture and leverage the symbolic patterns in the dataset.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4095de4652b346919ac60564df5beb09_proc_3074526/SPR_BENCH_test_cwca_bar.png"}], [{"analysis": "The t-SNE visualization of the embeddings from the development set without contrastive pretraining demonstrates that the embeddings are not well-clustered. There is no clear separation between different classes, indicating that the current embedding space lacks discriminative power for the SPR task. This suggests that the absence of contrastive pretraining may have resulted in suboptimal feature representations.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/tsne_dev.png"}, {"analysis": "The loss curves indicate that while the training loss decreases steadily over the epochs, the validation loss remains relatively flat and even shows an increasing trend towards the end. This suggests that the model is overfitting to the training data and is unable to generalize well to the validation set. The lack of validation loss improvement may also point to limitations in the current training strategy or model architecture.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_loss_curves.png"}, {"analysis": "The CWCA curves show that the training performance improves consistently over the epochs, but the validation performance stagnates and even slightly decreases. The test performance, represented by the red dot, is relatively low at 0.511. This again highlights the model's inability to generalize effectively, which could be attributed to insufficient regularization or the lack of context-aware pretraining.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_cwca_curves.png"}, {"analysis": "The confusion matrix reveals a significant number of misclassifications, particularly in one of the classes (lower right quadrant). This imbalance in prediction accuracy across classes suggests that the model struggles to handle certain symbolic patterns, possibly due to inadequate feature representation or an imbalanced dataset.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_df0775e215054ec2a8865131b33b4ba6_proc_3078651/SPR_BENCH_confusion_matrix.png"}], [], [], [{"analysis": "The t-SNE visualization of the development set embeddings indicates that the embeddings generated by the Bag-of-Tokens encoder do not form well-separated clusters. This suggests that the learned embeddings lack strong discriminative power for the symbolic sequences in the SPR task. The lack of clear clusters might indicate insufficient contextual awareness in the learned representations.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/tsne_dev.png"}, {"analysis": "The loss curve shows a decreasing trend for the training loss, indicating that the model is learning from the training data. However, the validation loss increases steadily after the first epoch, suggesting overfitting. The model appears to generalize poorly to the validation set, which could be due to insufficient regularization or ineffective data augmentation strategies.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_loss_curve.png"}, {"analysis": "The CWCA curve reveals a concerning divergence between the training and validation performance. While the training CWCA improves consistently, the validation CWCA decreases after the second epoch. This further supports the observation of overfitting and indicates that the embeddings learned by the Bag-of-Tokens encoder are not generalizing well to unseen data.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_cwca_curve.png"}, {"analysis": "The confusion matrix for the test set shows a significant number of misclassifications, particularly in the off-diagonal cells. This indicates that the model struggles to correctly classify symbolic sequences, further emphasizing the need for improvements in the encoder's ability to capture context-aware features.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c078a069eb1b4509afe27acab2d633f8_proc_3078654/SPR_BENCH_BagOfTokensEncoder_confusion_matrix.png"}], [{"analysis": "The t-SNE visualization of the dev embeddings without a projection head suggests that the embeddings are not well-separated. The clusters are not distinct, which might indicate that the embeddings lack sufficient discriminative power for the SPR task. This could be a sign that the contrastive learning framework requires further refinement.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/tsne_dev.png"}, {"analysis": "The t-SNE visualization of the Synthetic3 dev embeddings shows slightly better clustering compared to the previous plot. However, the clusters are still not entirely distinct, indicating room for improvement in the embedding space. The embedding quality might benefit from better context-aware augmentations or an improved loss function.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/tsne_s3_dev_fixed.png"}, {"analysis": "The training loss decreases consistently across epochs, showing effective learning. However, the validation loss initially decreases but starts to increase after epoch 3, suggesting overfitting. This implies that regularization techniques or early stopping criteria might be needed to improve generalization.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic1_loss_curve.png"}, {"analysis": "The CWCA and ACR metrics show divergent trends between training and validation sets. While training metrics improve steadily, validation performance decreases initially and then stabilizes. This indicates overfitting and suggests a need for better generalization strategies, such as improved data augmentation or a more robust model architecture.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic1_metrics_curve.png"}, {"analysis": "The training loss decreases steadily, but the validation loss increases consistently, indicating severe overfitting. The model struggles to generalize to unseen data, pointing to potential issues in the training process or the need for regularization techniques.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic2_loss_curve.png"}, {"analysis": "Both CWCA and ACR metrics improve on the training set but plateau on the validation set. This suggests that while the model learns well on the training data, it does not generalize effectively to the validation set. Enhancements in the contrastive learning strategy or data preparation might be necessary.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic2_metrics_curve.png"}, {"analysis": "The test metrics for Synthetic3 show a significant gap between CWCA and ACR scores. While ACR achieves a high score, CWCA lags behind, indicating that the model is better at capturing color-related patterns than shape-related ones. This highlights an imbalance in the model's ability to learn different aspects of the task, which could be addressed by adjusting the loss function or augmentations to emphasize shape features.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_1ebb6da5f358483e96eaca31b629cb04_proc_3078652/Synthetic3_test_metrics.png"}], [], [{"analysis": "The t-SNE plot shows the embeddings for the Synthetic3 development set. The embeddings appear to be scattered without clear clusters, suggesting that the learned representations may not yet fully capture the inherent structure of the data. This could indicate a need for further tuning of the contrastive learning framework or data augmentation strategies to improve the separation of distinct classes.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/tsne_s3_dev.png"}, {"analysis": "The training and validation loss curves indicate divergent behavior. While the training loss decreases steadily, the validation loss increases over epochs. This suggests overfitting, where the model performs well on the training set but fails to generalize to unseen data. Regularization techniques or early stopping might be necessary to address this issue.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_loss_curve.png"}, {"analysis": "The Shape-Weighted Accuracy (SWA) plot shows an increasing trend for both training and validation sets, though the validation SWA lags behind the training SWA. This indicates that the model is learning to improve its performance but still struggles to generalize to the validation set. The gap between training and validation SWA supports the observation of overfitting from the loss curves.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_SWA_curve.png"}, {"analysis": "The Color-Weighted Accuracy (CWA) and Average Classification Rate (ACR) plot shows a consistent gap between the training and validation metrics. While training CWA and ACR improve steadily, the validation metrics show limited improvement. This further highlights the generalization issue and suggests that the model may be overfitting to the training data.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_CWA_ACR_curve.png"}, {"analysis": "The confusion matrix for the test set shows that the model has a significant number of misclassifications, with 126 false positives and 66 false negatives. This indicates that the model struggles to differentiate between the two classes effectively. Improving class-specific features in the embeddings or balancing the dataset might help reduce these errors.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f5ffcc662d5a44ba803b84c4106bd9fc_proc_3078653/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The experimental results highlight several challenges in the current setup. The\nt-SNE embeddings lack clear structure, indicating room for improvement in the\nlearned representations. The training and validation curves point to\noverfitting, suggesting the need for better regularization or architectural\nadjustments. Finally, the test CWCA performance is far below the SOTA,\nemphasizing the need for substantial enhancements to the proposed approach.", "The provided plots indicate that the current model setup struggles with\ngeneralization and effective feature representation. The t-SNE visualization\nhighlights weak class separation, the loss and CWCA curves suggest overfitting\nand limited validation performance, and the confusion matrix points to class-\nspecific prediction issues. These findings emphasize the need for improved\npretraining strategies, such as context-aware contrastive learning, and better\nregularization techniques.", "[]", "[]", "The results suggest that the Bag-of-Tokens encoder struggles to generalize\neffectively, as evidenced by poor clustering in the t-SNE plot, increasing\nvalidation loss, declining validation CWCA, and a high number of\nmisclassifications in the confusion matrix. These issues highlight the need for\nbetter regularization, improved data augmentation, and potentially a more\nsophisticated encoder architecture to enhance performance on the SPR task.", "The analysis highlights issues of overfitting, insufficient generalization, and\nimbalanced performance across metrics. There is a need for improved\nregularization, data augmentation, and enhancements to the contrastive learning\nframework to achieve better results.", "[]", "The plots reveal that while the model shows improvement on training metrics, it\nstruggles with generalization to validation and test sets. Overfitting is a key\nissue, as indicated by the divergence in training and validation loss, as well\nas the gaps in performance metrics like SWA and CWA. The confusion matrix\nhighlights classification challenges, suggesting a need for better embeddings or\ndataset balancing.", "[]"], "exec_time": [74.9491081237793, 213.08693265914917, 189.61123776435852, 197.5925440788269, 218.33656644821167, 940.9041812419891, 4.097210168838501, 33.10790777206421, 73.43418049812317], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["[]"], [], [], ["[]"], ["\"\""], [], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# Try to load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------\n# Extract metrics for SPR_BENCH\nspr_key = \"SPR_BENCH\"\nmetrics, losses, test_cwca = {}, {}, None\ntry:\n    metrics = experiment_data[spr_key][\"metrics\"]\n    losses = experiment_data[spr_key][\"losses\"]\n    test_cwca = (\n        experiment_data[spr_key][\"metrics\"][\"test\"][0]\n        if experiment_data[spr_key][\"metrics\"][\"test\"]\n        else None\n    )\n    epochs_axis = np.arange(1, len(metrics[\"train\"]) + 1)\nexcept Exception as e:\n    print(f\"Error extracting metrics: {e}\")\n    metrics, losses, test_cwca, epochs_axis = {}, {}, None, np.array([])\n\n# ------------------------------------------------------------\n# Plot 1: CWCA curves\ntry:\n    if len(epochs_axis):\n        plt.figure()\n        plt.plot(epochs_axis, metrics[\"train\"], label=\"Train CWCA\", color=\"steelblue\")\n        plt.plot(\n            epochs_axis,\n            metrics[\"val\"],\n            label=\"Validation CWCA\",\n            color=\"orange\",\n            linestyle=\"--\",\n        )\n        plt.title(\"SPR_BENCH \u2013 CWCA Curves\\nSolid: Train, Dashed: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWCA\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_cwca_train_val_curves.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWCA curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# Plot 2: Loss curves\ntry:\n    if len(epochs_axis):\n        plt.figure()\n        plt.plot(epochs_axis, losses[\"train\"], label=\"Train Loss\", color=\"green\")\n        plt.plot(\n            epochs_axis,\n            losses[\"val\"],\n            label=\"Validation Loss\",\n            color=\"red\",\n            linestyle=\"--\",\n        )\n        plt.title(\"SPR_BENCH \u2013 Loss Curves\\nSolid: Train, Dashed: Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_train_val_curves.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# Plot 3: Test CWCA bar\ntry:\n    if test_cwca is not None:\n        plt.figure()\n        plt.bar([\"Test\"], [test_cwca], color=\"purple\")\n        plt.title(\"SPR_BENCH \u2013 Final Test CWCA\")\n        plt.ylabel(\"CWCA\")\n        plt.ylim(0, 1.0)\n        plt.text(0, test_cwca + 0.01, f\"{test_cwca:.3f}\", ha=\"center\", va=\"bottom\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_cwca_bar.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test CWCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# Print evaluation metric\nif test_cwca is not None:\n    print(f\"Final Test CWCA: {test_cwca:.4f}\")\nelse:\n    print(\"Test CWCA not available.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nexp_key = \"NoContrastivePretraining\"\ndset_key = \"SPR_BENCH\"\nif exp_key in experiment_data and dset_key in experiment_data[exp_key]:\n    data = experiment_data[exp_key][dset_key]\n    losses_tr = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\n    cwca_tr = data[\"metrics\"][\"train\"]\n    cwca_val = data[\"metrics\"][\"val\"]\n    cwca_test = data[\"metrics\"][\"test\"][0] if data[\"metrics\"][\"test\"] else None\n    y_pred = np.array(data[\"predictions\"])\n    y_true = np.array(data[\"ground_truth\"])\nelse:\n    print(\"Required keys not found in experiment_data.\")\n    losses_tr = losses_val = cwca_tr = cwca_val = []\n    cwca_test = None\n    y_pred = y_true = np.array([])\n\n# ------------------------------------------------------------------\n# 1) Loss curves\ntry:\n    plt.figure()\n    epochs = range(1, len(losses_tr) + 1)\n    plt.plot(epochs, losses_tr, label=\"Train\")\n    plt.plot(epochs, losses_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Loss Curves \u2013 SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) CWCA curves (+ test point)\ntry:\n    plt.figure()\n    plt.plot(epochs, cwca_tr, label=\"Train\")\n    plt.plot(epochs, cwca_val, label=\"Validation\")\n    if cwca_test is not None:\n        plt.scatter(\n            [epochs[-1]], [cwca_test], color=\"red\", label=f\"Test ({cwca_test:.3f})\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CWCA\")\n    plt.title(\"CWCA Curves \u2013 SPR_BENCH\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_cwca_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWCA plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Confusion matrix heat-map\ntry:\n    if y_true.size and y_pred.size:\n        n_lbl = int(max(y_true.max(), y_pred.max()) + 1)\n        cm = np.zeros((n_lbl, n_lbl), int)\n        for t, p in zip(y_true, y_pred):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.title(\n            \"Confusion Matrix \u2013 SPR_BENCH\\nLeft: Ground Truth (rows), Right: Predictions (cols)\"\n        )\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        for i in range(n_lbl):\n            for j in range(n_lbl):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.colorbar()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\nif cwca_test is not None:\n    print(f\"Final Test CWCA: {cwca_test:.4f}\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# iterate over model/dataset combinations\nfor model_name, dsets in experiment_data.items():\n    for dset_name, content in dsets.items():\n        losses = content.get(\"losses\", {})\n        metrics = content.get(\"metrics\", {})\n        preds = content.get(\"predictions\", [])\n        gts = content.get(\"ground_truth\", [])\n\n        # 1) train/val loss curve -------------------------------------------------\n        try:\n            train_loss = losses.get(\"train\", [])\n            val_loss = losses.get(\"val\", [])\n            if train_loss and val_loss:\n                plt.figure()\n                epochs = range(1, len(train_loss) + 1)\n                plt.plot(epochs, train_loss, label=\"Train\")\n                plt.plot(epochs, val_loss, label=\"Validation\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Cross-Entropy Loss\")\n                plt.title(f\"{dset_name} - {model_name} - Loss Curve\")\n                plt.legend()\n                fname = f\"{dset_name}_{model_name}_loss_curve.png\".replace(\" \", \"_\")\n                plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve: {e}\")\n            plt.close()\n\n        # 2) train/val CWCA curve -------------------------------------------------\n        try:\n            train_cwca = metrics.get(\"train\", [])\n            val_cwca = metrics.get(\"val\", [])\n            if train_cwca and val_cwca:\n                plt.figure()\n                epochs = range(1, len(train_cwca) + 1)\n                plt.plot(epochs, train_cwca, label=\"Train\")\n                plt.plot(epochs, val_cwca, label=\"Validation\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"CWCA\")\n                plt.title(f\"{dset_name} - {model_name} - CWCA Curve\")\n                plt.legend()\n                fname = f\"{dset_name}_{model_name}_cwca_curve.png\".replace(\" \", \"_\")\n                plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating CWCA curve: {e}\")\n            plt.close()\n\n        # 3) confusion matrix on test set ----------------------------------------\n        try:\n            if preds and gts:\n                cm = confusion_matrix(gts, preds)\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im, fraction=0.046, pad=0.04)\n                plt.xlabel(\"Predicted label\")\n                plt.ylabel(\"True label\")\n                plt.title(f\"{dset_name} - {model_name} - Confusion Matrix (Test Set)\")\n                for (i, j), v in np.ndenumerate(cm):\n                    plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"black\")\n                fname = f\"{dset_name}_{model_name}_confusion_matrix.png\".replace(\n                    \" \", \"_\"\n                )\n                plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n\n        # print evaluation metric -------------------------------------------------\n        try:\n            test_cwca = metrics.get(\"test\", [None])[0]\n            print(f\"{model_name} on {dset_name} - Test CWCA: {test_cwca}\")\n        except Exception as e:\n            print(f\"Error printing test CWCA: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- Synthetic1 ----------\ntry:\n    s1 = experiment_data.get(\"Synthetic1\", {})\n    tr_loss, val_loss = s1[\"losses\"][\"train\"], s1[\"losses\"][\"val\"]\n    if tr_loss and val_loss:\n        epochs = np.arange(1, len(tr_loss) + 1)\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Synthetic1: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"Synthetic1_loss_curve.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Synthetic1 loss plot: {e}\")\n    plt.close()\n\ntry:\n    cwca_tr, cwca_val = s1[\"metrics\"][\"train_cwca\"], s1[\"metrics\"][\"val_cwca\"]\n    acr_tr, acr_val = s1[\"metrics\"][\"train_acr\"], s1[\"metrics\"][\"val_acr\"]\n    if cwca_tr and acr_tr:\n        epochs = np.arange(1, len(cwca_tr) + 1)\n        fig, axs = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n        axs[0].plot(epochs, cwca_tr, label=\"Train\")\n        axs[0].plot(epochs, cwca_val, label=\"Validation\")\n        axs[0].set_ylabel(\"CWCA\")\n        axs[0].legend()\n        axs[1].plot(epochs, acr_tr, label=\"Train\")\n        axs[1].plot(epochs, acr_val, label=\"Validation\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].set_ylabel(\"ACR\")\n        axs[1].legend()\n        fig.suptitle(\"Synthetic1: CWCA & ACR\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, \"Synthetic1_metrics_curve.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Synthetic1 metrics plot: {e}\")\n    plt.close()\n\n# ---------- Synthetic2 ----------\ntry:\n    s2 = experiment_data.get(\"Synthetic2\", {})\n    tr_loss, val_loss = s2[\"losses\"][\"train\"], s2[\"losses\"][\"val\"]\n    if tr_loss and val_loss:\n        epochs = np.arange(1, len(tr_loss) + 1)\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Synthetic2: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"Synthetic2_loss_curve.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Synthetic2 loss plot: {e}\")\n    plt.close()\n\ntry:\n    cwca_tr, cwca_val = s2[\"metrics\"][\"train_cwca\"], s2[\"metrics\"][\"val_cwca\"]\n    acr_tr, acr_val = s2[\"metrics\"][\"train_acr\"], s2[\"metrics\"][\"val_acr\"]\n    if cwca_tr and acr_tr:\n        epochs = np.arange(1, len(cwca_tr) + 1)\n        fig, axs = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n        axs[0].plot(epochs, cwca_tr, label=\"Train\")\n        axs[0].plot(epochs, cwca_val, label=\"Validation\")\n        axs[0].set_ylabel(\"CWCA\")\n        axs[0].legend()\n        axs[1].plot(epochs, acr_tr, label=\"Train\")\n        axs[1].plot(epochs, acr_val, label=\"Validation\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].set_ylabel(\"ACR\")\n        axs[1].legend()\n        fig.suptitle(\"Synthetic2: CWCA & ACR\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, \"Synthetic2_metrics_curve.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Synthetic2 metrics plot: {e}\")\n    plt.close()\n\n# ---------- Synthetic3 ----------\ntry:\n    s3 = experiment_data.get(\"Synthetic3\", {})\n    cwca_test = s3[\"metrics\"][\"test_cwca\"]\n    acr_test = s3[\"metrics\"][\"test_acr\"]\n    if cwca_test and acr_test:\n        plt.figure()\n        plt.bar(\n            [\"CWCA\", \"ACR\"], [cwca_test[0], acr_test[0]], color=[\"skyblue\", \"salmon\"]\n        )\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(\"Synthetic3: Test Metrics\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"Synthetic3_test_metrics.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Synthetic3 metrics plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------- setup & load -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nds_name = \"SPR_BENCH\"\nds = experiment_data.get(ds_name, {})\nlosses = ds.get(\"losses\", {})\nmetrics = ds.get(\"metrics\", {})\npreds = np.array(ds.get(\"predictions\", []))\ntruths = np.array(ds.get(\"ground_truth\", []))\n\n# x-axis (epochs)\nepochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n# ----------- plot 1: loss ------------\ntry:\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{ds_name} \u2013 Training & Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{ds_name}_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ----------- plot 2: SWA -------------\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_SWA\", []), label=\"Train SWA\")\n    plt.plot(epochs, metrics.get(\"val_SWA\", []), label=\"Val SWA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Shape-Weighted Accuracy\")\n    plt.title(f\"{ds_name} \u2013 SWA Over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"{ds_name}_SWA_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating SWA plot: {e}\")\n    plt.close()\n\n# ----------- plot 3: CWA & ACR -------\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_CWA\", []), label=\"Train CWA\")\n    plt.plot(epochs, metrics.get(\"val_CWA\", []), label=\"Val CWA\")\n    plt.plot(epochs, metrics.get(\"train_ACR\", []), \"--\", label=\"Train ACR\")\n    plt.plot(epochs, metrics.get(\"val_ACR\", []), \"--\", label=\"Val ACR\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric Value\")\n    plt.title(f\"{ds_name} \u2013 CWA & ACR Over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"{ds_name}_CWA_ACR_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA/ACR plot: {e}\")\n    plt.close()\n\n# ----------- plot 4: confusion matrix-\ntry:\n    if preds.size and truths.size:\n        num_labels = int(max(truths.max(), preds.max()) + 1)\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for t, p in zip(truths, preds):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(f\"{ds_name} \u2013 Confusion Matrix (Test)\")\n        for i in range(num_labels):\n            for j in range(num_labels):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ----------- print final test metrics -\ntry:\n    print(\n        \"Final Test Metrics:\",\n        \"SWA=\",\n        metrics.get(\"test_SWA\", [-1])[-1],\n        \"CWA=\",\n        metrics.get(\"test_CWA\", [-1])[-1],\n        \"ACR=\",\n        metrics.get(\"test_ACR\", [-1])[-1],\n    )\nexcept Exception as e:\n    print(f\"Error printing metrics: {e}\")\n", null], "plot_plan": ["The code will load the stored experiment_data.npy, safely extract the training/validation CWCA metrics and losses recorded during fine-tuning, and the single test CWCA value. It will then generate three plots: (1) CWCA curves for train versus validation across epochs, (2) loss curves for train versus validation across epochs, and (3) a bar chart showing the final test CWCA. Each plot is wrapped in its own try-except block, uses clear titles/subtitles, is saved to working_dir with descriptive filenames, and the figure is closed afterwards. Only data present in experiment_data.npy is accessed, avoiding any fabricated values. After plotting, the script prints the final test CWCA so the metric is visible in logs. If any part of the data is missing, the corresponding plot gracefully skips and reports the error without stopping the rest of the script. This keeps the visualization simple, reproducible, and compliant with the specified guidelines.", "We will load the saved NumPy dictionary, extract the loss arrays, CWCA scores, predictions and ground-truth labels, then generate at most three plots: (1) training/validation loss curves, (2) training/validation CWCA curves together with a single red dot for the final test CWCA, and (3) a confusion-matrix heat-map of the test set.  Each plot is wrapped in a try-except block, saved with a descriptive name that contains the dataset and plot type, and the figure is always closed.  After plotting, we print the final test CWCA so the notebook logs an easy-to-see metric.  The code respects all the given plotting guidelines, only uses data present in experiment_data.npy, and stores everything under working_dir.", null, null, "The script will load the stored experiment_data.npy, iterate over the available model\u2013dataset pair(s), and, if data exist, generate up-to-five standard diagnostic plots: (1) train/val loss curve, (2) train/val CWCA curve, and (3) a test-set confusion matrix built from the recorded predictions vs. ground-truth labels. Each figure is created inside its own try-except block, titled clearly, saved to working_dir with descriptive names, and then closed to free resources. Any missing data silently skips the corresponding plot. Finally, the code prints the test CWCA so users can see the main evaluation metric in the notebook/console alongside the figures.", "We will load the saved experiment_data dictionary, then generate up-to-five basic matplotlib figures that summarize the results.  \nFor Synthetic1 and Synthetic2 we plot (i) training vs. validation loss and (ii) CWCA/ACR curves across epochs.  \nFor Synthetic3 only a single test point exists, so we plot a simple bar chart of its CWCA and ACR.  \nEach figure is wrapped in its own try-except block, titled clearly, saved into working_dir with descriptive names, and the figure is always closed.  \nWe only plot when the relevant arrays are non-empty and restrict ourselves to the real data in experiment_data.npy.  \nTraining/validation curves share the same x-axis (epochs) and are displayed together to ease comparison; CWCA and ACR curves are placed in two stacked subplots inside one figure to keep the total number of figures \u22645.  \nAll requirements about imports, directory handling, and figure titling/saving strategy are followed.", null, "The script will first load the saved experiment dictionary, extract the entry under the single dataset (SPR_BENCH), and prepare arrays for the available losses and metrics.  \nWe create up to four plots: (1) training vs. validation loss, (2) training vs. validation Shape-Weighted Accuracy (SWA), (3) training vs. validation Color-Weighted Accuracy (CWA) together with Augmentation Consistency Rate (ACR), and (4) a simple 2\u00d72 confusion-matrix heat-map of the final test predictions.  \nEach plot is wrapped in its own try-except block to guarantee graceful failure and figure closing, and each figure is saved into the mandatory working folder with a descriptive filename.  \nFor time-series curves, we plot the whole history (only 5 epochs, well under the 5-figure limit).  \nAxes are labeled and every figure is titled, including subtitles that specify the split or metric shown.  \nAfter plotting, the script prints final test metrics (SWA/CWA/ACR) so they are visible in the notebook/stdout.  \nAll matplotlib resources are closed explicitly to avoid memory leaks, and no data are fabricated\u2014everything comes directly from experiment_data.npy.", null], "ablation_name": [null, "NoContrastivePretraining", "NoProjectionHeadContrastive", "MultiSyntheticDatasetTraining", "BagOfTokensEncoder", null, null, null, "FreezeEncoderFineTuning"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["We load the NumPy file from the working directory, recover the Python dictionary\nit contains and iterate over its top-level dataset keys (e.g. \u201cSPR_BENCH\u201d).\nFor every dataset we take:   \u2022 the maximum CWCA accuracy achieved on the\ntraining set,   \u2022 the maximum CWCA accuracy achieved on the validation set,   \u2022\nthe (only) CWCA accuracy stored for the test set,   \u2022 the minimum loss observed\non the training set, and   \u2022 the minimum loss observed on the validation set.\nEach of these values is then printed with an explicit, self-describing label so\nthe output is unambiguous.", "The script will locate the saved NumPy file in the ./working directory, load it\nas a regular Python dictionary, iterate over every experiment and its nested\ndataset(s), and then extract the stored lists of losses and CWCA scores. For\neach list we report either the last element (for training loss, validation loss,\ntraining CWCA, test CWCA) or the best element (for validation CWCA). The\ninformation is printed with explicit, self-descriptive labels so that each line\nclearly states both the split and the metric being reported.", "Below is a concise approach: load the saved NumPy dictionary from the working\ndirectory, iterate through each dataset contained in every experiment, and for\nevery metric/loss array choose either the best value (max for scores, min for\nlosses) or, when only one value is present (e.g., test CWCA), simply print it.\nAll results are printed with explicit metric names so they are self-describing.", "The script will load the saved NumPy dictionary, iterate over the three\nsynthetic datasets, compute the best (max CWCA) and lowest (min loss) values\nwhere applicable, and print them in a clear, labeled way. It takes care of\npossible missing keys/lists and runs immediately once executed, without relying\non any special entry point.", "The script will compute the working directory path, load the saved NumPy\ndictionary, iterate through each dataset stored under every model, and for each\ndataset print (a) the highest CWCA obtained during training, (b) the highest\nCWCA on the validation set, (c) the CWCA on the test set, (d) the lowest\ntraining loss, and (e) the lowest validation loss.   All printing is done with\nclear, explicit metric names exactly once per dataset, and no plots are\nproduced.", "The script will load the saved numpy dictionary from the working directory,\niterate over the three dataset entries, and compute either the best (max-\nimising) or final (last) values for each recorded metric. For the\ntraining/validation CWCA and ACR values we report the maximum, whereas for the\ncorresponding losses we report the minimum. For the test-only dataset\n(Synthetic3) we simply output the single stored value for each metric and loss.\nClear, descriptive print statements are provided exactly as required, and the\ncode sits at global scope so it runs immediately.", "", "We will load the saved NumPy dictionary from the working directory, extract the\nsingle dataset (\u201cSPR_BENCH\u201d), and then summarise its results.   For every list\nthat records values across epochs, we report the best value: the minimum for\nlosses and the maximum for the other metrics.   For the test metrics, which are\nonly recorded once, we simply print that final value.   Each printed line begins\nwith a clear dataset label followed by an explicit metric name and its value.\nNo plots or special entry-point guards are used, so the code runs immediately\nwhen executed.", "Below is a compact script that immediately loads the stored numpy file, extracts\nthe recorded numbers, chooses the most relevant (best validation / last-epoch\ntrain & test) values, and prints them with clear labels for every dataset found."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# helper to print nicely\ndef print_metric(label: str, value: float):\n    print(f\"    {label}: {value:.4f}\")\n\n\n# ------------------------------------------------------------\n# iterate over datasets and report metrics\nfor dataset_name, records in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # CWCA accuracies\n    train_cwca_vals = records[\"metrics\"].get(\"train\", [])\n    val_cwca_vals = records[\"metrics\"].get(\"val\", [])\n    test_cwca_vals = records[\"metrics\"].get(\"test\", [])\n\n    if train_cwca_vals:\n        best_train_cwca = max(train_cwca_vals)\n        print_metric(\"Best training CWCA accuracy\", best_train_cwca)\n\n    if val_cwca_vals:\n        best_val_cwca = max(val_cwca_vals)\n        print_metric(\"Best validation CWCA accuracy\", best_val_cwca)\n\n    if test_cwca_vals:\n        final_test_cwca = test_cwca_vals[-1]  # only one entry is expected\n        print_metric(\"Final test CWCA accuracy\", final_test_cwca)\n\n    # Losses\n    train_losses = records[\"losses\"].get(\"train\", [])\n    val_losses = records[\"losses\"].get(\"val\", [])\n\n    if train_losses:\n        min_train_loss = min(train_losses)\n        print_metric(\"Minimum training loss\", min_train_loss)\n\n    if val_losses:\n        min_val_loss = min(val_losses)\n        print_metric(\"Minimum validation loss\", min_val_loss)\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Traverse experiments \u2192 datasets \u2192 metrics / losses\nfor exp_name, exp_contents in experiment_data.items():\n    for dataset_name, data in exp_contents.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # ----- CWCA metrics -----\n        metrics = data.get(\"metrics\", {})\n        train_cwca = metrics.get(\"train\", [])\n        val_cwca = metrics.get(\"val\", [])\n        test_cwca = metrics.get(\"test\", [])\n\n        if train_cwca:\n            print(f\"Training CWCA: {train_cwca[-1]:.4f}\")\n        if val_cwca:\n            print(f\"Best validation CWCA: {max(val_cwca):.4f}\")\n        if test_cwca:\n            print(f\"Test CWCA: {test_cwca[-1]:.4f}\")\n\n        # ----- Loss values -----\n        losses = data.get(\"losses\", {})\n        train_losses = losses.get(\"train\", [])\n        val_losses = losses.get(\"val\", [])\n\n        if train_losses:\n            print(f\"Final training loss: {train_losses[-1]:.4f}\")\n        if val_losses:\n            print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n\n        # Blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# 0. Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\ndef best_score(values, higher_is_better=True):\n    \"\"\"Return best value from a list given its direction.\"\"\"\n    if not values:  # empty list guard\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# ------------------------------------------------------------\n# 1. Iterate over experiments and datasets\nfor exp_name, datasets in experiment_data.items():\n    for dset_name, dct in datasets.items():\n        print(f\"\\nDataset: {dset_name}\")\n\n        # Metrics ----------------------------------------------------------------\n        metrics = dct.get(\"metrics\", {})\n        if metrics:\n            train_cwca_best = best_score(metrics.get(\"train\", []), True)\n            val_cwca_best = best_score(metrics.get(\"val\", []), True)\n            test_cwca = metrics.get(\"test\", [None])[-1]\n\n            if train_cwca_best is not None:\n                print(f\"Best train CWCA: {train_cwca_best:.4f}\")\n            if val_cwca_best is not None:\n                print(f\"Best validation CWCA: {val_cwca_best:.4f}\")\n            if test_cwca is not None:\n                print(f\"Test CWCA: {test_cwca:.4f}\")\n\n        # Losses -----------------------------------------------------------------\n        losses = dct.get(\"losses\", {})\n        if losses:\n            train_loss_best = best_score(losses.get(\"train\", []), False)\n            val_loss_best = best_score(losses.get(\"val\", []), False)\n\n            if train_loss_best is not None:\n                print(f\"Lowest train loss: {train_loss_best:.4f}\")\n            if val_loss_best is not None:\n                print(f\"Lowest validation loss: {val_loss_best:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# 0. locate and load the experiment data ---------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# 1. helper to fetch best / final values ---------------------\ndef safe_best(values, key=\"max\"):\n    \"\"\"Return best (max or min) value from a list, or None if list is empty.\"\"\"\n    if not values:\n        return None\n    return max(values) if key == \"max\" else min(values)\n\n\n# ------------------------------------------------------------\n# 2. iterate and print metrics -------------------------------\nmain_key = \"MultiSyntheticDatasetTraining\"\nfor dataset_name, content in experiment_data.get(main_key, {}).items():\n    print(dataset_name)  # dataset header\n\n    # ---------- CWCA metrics ----------\n    # train\n    train_cwca = safe_best(content.get(\"metrics\", {}).get(\"train\", []), key=\"max\")\n    if train_cwca is not None:\n        print(f\"  best train CWCA: {train_cwca:.4f}\")\n\n    # validation\n    val_cwca = safe_best(content.get(\"metrics\", {}).get(\"val\", []), key=\"max\")\n    if val_cwca is not None:\n        print(f\"  best validation CWCA: {val_cwca:.4f}\")\n\n    # test (only for Synthetic3)\n    test_cwca_list = content.get(\"metrics\", {}).get(\"test\", [])\n    if test_cwca_list:\n        print(f\"  test CWCA: {test_cwca_list[-1]:.4f}\")\n\n    # ---------- Loss metrics ----------\n    train_losses = content.get(\"losses\", {}).get(\"train\", [])\n    min_train_loss = safe_best(train_losses, key=\"min\")\n    if min_train_loss is not None:\n        print(f\"  minimum train loss: {min_train_loss:.4f}\")\n\n    val_losses = content.get(\"losses\", {}).get(\"val\", [])\n    min_val_loss = safe_best(val_losses, key=\"min\")\n    if min_val_loss is not None:\n        print(f\"  minimum validation loss: {min_val_loss:.4f}\")\n\n    print()  # empty line between datasets\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------------------------\n# 0. Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------------------------------------------------------------------\n# 1. Helper to obtain best (min for loss, max for metric) value\ndef best(values, kind=\"metric\"):\n    if not values:  # safety check for empty list\n        return None\n    return max(values) if kind == \"metric\" else min(values)\n\n\n# ----------------------------------------------------------------------\n# 2. Iterate through models and their contained datasets\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(dataset_name)  # Dataset header\n\n        # ---- CWCA (metric) ----\n        train_cwca = best(results[\"metrics\"].get(\"train\", []), kind=\"metric\")\n        val_cwca = best(results[\"metrics\"].get(\"val\", []), kind=\"metric\")\n        # test CWCA list is length 1 by construction\n        test_cwca = results[\"metrics\"].get(\"test\", [None])[-1]\n\n        # ---- Losses ----\n        train_loss = best(results[\"losses\"].get(\"train\", []), kind=\"loss\")\n        val_loss = best(results[\"losses\"].get(\"val\", []), kind=\"loss\")\n\n        # ---- Print in required, explicit format ----\n        if train_cwca is not None:\n            print(f\"train CWCA: {train_cwca:.4f}\")\n        if val_cwca is not None:\n            print(f\"validation CWCA: {val_cwca:.4f}\")\n        if test_cwca is not None:\n            print(f\"test CWCA: {test_cwca:.4f}\")\n\n        if train_loss is not None:\n            print(f\"train loss: {train_loss:.4f}\")\n        if val_loss is not None:\n            print(f\"validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. locate and load data ------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# 1. helper functions ----------------------------------------------\ndef safe_best(values, mode=\"max\"):\n    \"\"\"Return best value according to mode; empty list returns None.\"\"\"\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\ndef maybe_print(label, value, precision=4):\n    \"\"\"Print only if value is not None.\"\"\"\n    if value is not None:\n        print(f\"  {label}: {value:.{precision}f}\")\n\n\n# ------------------------------------------------------------------\n# 2. parse & display -----------------------------------------------\nfor dataset_name, content in experiment_data.items():\n    print(dataset_name)  # dataset header\n    metrics = content.get(\"metrics\", {})\n    losses = content.get(\"losses\", {})\n\n    if dataset_name in (\"Synthetic1\", \"Synthetic2\"):\n        # CWCA ----------------------------------------------------------------\n        maybe_print(\n            \"best training CWCA\", safe_best(metrics.get(\"train_cwca\", []), \"max\")\n        )\n        maybe_print(\n            \"best validation CWCA\", safe_best(metrics.get(\"val_cwca\", []), \"max\")\n        )\n        # ACR -----------------------------------------------------------------\n        maybe_print(\"best training ACR\", safe_best(metrics.get(\"train_acr\", []), \"max\"))\n        maybe_print(\"best validation ACR\", safe_best(metrics.get(\"val_acr\", []), \"max\"))\n        # Losses --------------------------------------------------------------\n        maybe_print(\"minimum training loss\", safe_best(losses.get(\"train\", []), \"min\"))\n        maybe_print(\"minimum validation loss\", safe_best(losses.get(\"val\", []), \"min\"))\n\n    elif dataset_name == \"Synthetic3\":\n        # Single-shot test results -------------------------------------------\n        maybe_print(\"test CWCA\", metrics.get(\"test_cwca\", [None])[-1])\n        maybe_print(\"test ACR\", metrics.get(\"test_acr\", [None])[-1])\n        maybe_print(\"test loss\", losses.get(\"test\", [None])[-1])\n\n    print(\"\")  # blank line between datasets\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate the working directory and load the stored experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Helper dictionaries for pretty names\n# ------------------------------------------------------------------\nmetric_name_map = {\n    \"SWA\": \"shape-weighted accuracy\",\n    \"CWA\": \"color-weighted accuracy\",\n    \"ACR\": \"augmentation consistency rate\",\n}\n\nprefix_map = {\n    \"train\": \"training\",\n    \"val\": \"validation\",\n    \"test\": \"test\",\n}\n\n# ------------------------------------------------------------------\n# 2. Iterate over datasets and print best/final metrics\n# ------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\n=== Dataset: {dataset_name} ===\")\n\n    # ---- losses ----\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n\n    if train_losses:\n        best_train_loss = min(train_losses)\n        print(f\"{prefix_map['train']} loss: {best_train_loss:.4f}\")\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"{prefix_map['val']} loss: {best_val_loss:.4f}\")\n\n    # ---- other metrics ----\n    for key, values in data[\"metrics\"].items():\n        if not values:\n            continue\n\n        # Split key into prefix (train/val/test) and the real metric name\n        prefix, raw_metric = key.split(\"_\", 1)\n        nice_prefix = prefix_map.get(prefix, prefix)\n        nice_metric = metric_name_map.get(raw_metric, raw_metric)\n\n        # Decide whether to take best (multiple values) or final (single value)\n        if len(values) > 1 and prefix != \"test\":\n            # For accuracy-type metrics, higher is better\n            best_val = max(values)\n            print(f\"{nice_prefix} {nice_metric}: {best_val:.4f}\")\n        else:\n            print(f\"{nice_prefix} {nice_metric}: {values[-1]:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- load --------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helpers -----------------------------------------------------------\ndef best(values, higher=True):\n    \"\"\"Return the best value out of a list (max if higher=True else min).\"\"\"\n    return (max(values) if higher else min(values)) if values else None\n\n\n# ---------- iterate and report ------------------------------------------------\nfor exp_name, datasets in experiment_data.items():\n    for ds_name, ds_dict in datasets.items():\n        print(f\"{ds_name}:\")  # dataset header\n\n        # ---- CWCA (accuracy-like) -------------------------------------------\n        metrics = ds_dict.get(\"metrics\", {})\n        train_cwca = metrics.get(\"train\", [])\n        val_cwca = metrics.get(\"val\", [])\n        test_cwca = metrics.get(\"test\", [])\n\n        if train_cwca:\n            print(f\"training CWCA: {train_cwca[-1]:.4f}\")\n        if val_cwca:\n            print(f\"best validation CWCA: {best(val_cwca, higher=True):.4f}\")\n        if test_cwca:\n            print(f\"test CWCA: {test_cwca[-1]:.4f}\")\n\n        # ---- losses ----------------------------------------------------------\n        losses = ds_dict.get(\"losses\", {})\n        train_loss = losses.get(\"train\", [])\n        val_loss = losses.get(\"val\", [])\n\n        if train_loss:\n            print(f\"training loss: {train_loss[-1]:.4f}\")\n        if val_loss:\n            print(f\"best validation loss: {best(val_loss, higher=False):.4f}\")\n"], "parse_term_out": ["['SPR_BENCH', '\\n', '    Best training CWCA accuracy: 0.5629', '\\n', '    Best\nvalidation CWCA accuracy: 0.5257', '\\n', '    Final test CWCA accuracy: 0.4764',\n'\\n', '    Minimum training loss: 0.6834', '\\n', '    Minimum validation loss:\n0.6938', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Training CWCA: 0.5520', '\\n', 'Best validation\nCWCA: 0.5007', '\\n', 'Test CWCA: 0.5107', '\\n', 'Final training loss: 0.6830',\n'\\n', 'Final validation loss: 0.6976', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best train CWCA: 0.5616', '\\n', 'Best validation\nCWCA: 0.5337', '\\n', 'Test CWCA: 0.5193', '\\n', 'Lowest train loss: 0.6821',\n'\\n', 'Lowest validation loss: 0.6943', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Synthetic1', '\\n', '  best train CWCA: 0.5436', '\\n', '  best validation CWCA:\n0.5557', '\\n', '  minimum train loss: 0.6870', '\\n', '  minimum validation loss:\n0.6913', '\\n', '\\n', 'Synthetic2', '\\n', '  best train CWCA: 0.5279', '\\n', '\nbest validation CWCA: 0.5321', '\\n', '  minimum train loss: 0.6905', '\\n', '\nminimum validation loss: 0.6925', '\\n', '\\n', 'Synthetic3', '\\n', '  test CWCA:\n0.7042', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'train CWCA: 0.5436', '\\n', 'validation CWCA: 0.5169', '\\n',\n'test CWCA: 0.4897', '\\n', 'train loss: 0.6850', '\\n', 'validation loss:\n0.6949', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Synthetic1', '\\n', '  best training CWCA: 0.5383', '\\n', '  best validation\nCWCA: 0.5668', '\\n', '  best training ACR: 0.7525', '\\n', '  best validation\nACR: 0.8175', '\\n', '  minimum training loss: 0.6869', '\\n', '  minimum\nvalidation loss: 0.6885', '\\n', '', '\\n', 'Synthetic2', '\\n', '  best training\nCWCA: 0.5324', '\\n', '  best validation CWCA: 0.4955', '\\n', '  best training\nACR: 0.6785', '\\n', '  best validation ACR: 0.7150', '\\n', '  minimum training\nloss: 0.6910', '\\n', '  minimum validation loss: 0.6934', '\\n', '', '\\n',\n'Synthetic3', '\\n', '  test CWCA: 0.6958', '\\n', '  test ACR: 1.0000', '\\n', '\ntest loss: 0.6721', '\\n', '', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "", "['\\n=== Dataset: SPR_BENCH ===', '\\n', 'training loss: 0.6805', '\\n',\n'validation loss: 0.6939', '\\n', 'training shape-weighted accuracy: 0.5647',\n'\\n', 'validation shape-weighted accuracy: 0.5295', '\\n', 'training color-\nweighted accuracy: 0.5646', '\\n', 'validation color-weighted accuracy: 0.5206',\n'\\n', 'training augmentation consistency rate: 0.8840', '\\n', 'validation\naugmentation consistency rate: 0.8775', '\\n', 'test shape-weighted accuracy:\n0.5158', '\\n', 'test color-weighted accuracy: 0.5248', '\\n', 'test augmentation\nconsistency rate: 0.8125', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH:', '\\n', 'training CWCA: 0.5305', '\\n', 'best validation CWCA:\n0.5242', '\\n', 'test CWCA: 0.4834', '\\n', 'training loss: 0.6915', '\\n', 'best\nvalidation loss: 0.6914', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}