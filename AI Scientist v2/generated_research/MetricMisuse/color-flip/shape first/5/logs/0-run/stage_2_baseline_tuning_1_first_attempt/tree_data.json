{"edges": [[0, 4], [0, 8], [0, 2], [0, 5], [0, 1], [0, 6], [0, 3], [0, 7], [1, 12], [1, 11], [1, 10], [1, 9]], "layout": [[0.5882352941176471, 0.0], [0.17647058823529413, 0.5], [0.29411764705882354, 0.5], [0.4117647058823529, 0.5], [0.5294117647058824, 0.5], [0.6470588235294118, 0.5], [0.7647058823529411, 0.5], [0.8823529411764706, 0.5], [1.0, 0.5], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["A simple baseline is built around two stages: (i) a lightweight SimCLR-style\ncontrastive pre-training that learns context-aware sequence embeddings from the\nSPR sequences using random token-mask / deletion augmentations, and (ii) a\nsupervised fine-tuning with a linear classifier on top of the same encoder.\nSequences are tokenised at the symbol level, padded per batch, sent through an\nEmbedding + 1-layer GRU.  For contrastive learning we create two augmented views\nof every sequence in a batch and minimise NT-Xent loss.  After a few epochs the\nencoder is copied, a classification head is added, and the whole model is fine-\ntuned on the labelled data while monitoring validation loss together with SWA,\nCWA and their harmonic mean (HSCA).  All tensors and the model are placed on GPU\nwhen available, metrics are stored in the required experiment_data structure and\nfinally saved in the ./working folder.", "Hyperparam tuning name: supervised_finetuning_epochs. Below is an extended\nversion of the baseline that (1) runs contrastive pre-training once, (2) sweeps\nthe number of supervised fine-tuning epochs in {5, 10, 15, 20} with early-\nstopping, (3) keeps the best validation HSCA checkpoint for every sweep and\nevaluates it on the test set, and (4) logs/serialises every metric so it can be\nplotted later.", "Hyperparam tuning name: learning_rate. The solution performs a grid-search over\nlearning-rates {3e-4, 5e-4, 1e-3, 2e-3} for both the contrastive pre-training\nand the supervised fine-tuning optimizers.   For every (pre_lr, sup_lr) pair the\nscript (1) builds fresh models, (2) runs contrastive pre-training, (3) fine-\ntunes with supervision, (4) evaluates on dev (train key) and test (val key)\nsplits, and (5) stores the resulting metrics / losses in the required\nexperiment_data structure before saving it to \u201cexperiment_data.npy\u201d.   All\nutilities, data handling, model code and training loops remain self-contained in\na single executable file.", "Hyperparam tuning name: batch_size. The solution iterates over the candidate\nmini-batch sizes (64, 128, 256, 512).   For every batch-size setting it rebuilds\ndataloaders, re-instantiates a fresh encoder / projection head / classifier,\nperforms the same two-stage training (contrastive then supervised), evaluates on\nthe dev split each epoch, finally evaluates on the test split, and stores all\nplottable data inside an NP-savable `experiment_data` dictionary under the\n\u201cbatch_size\u201d tuning key.", "Hyperparam tuning name: contrastive_pretraining_epochs. Below is a compact re-\nimplementation of the baseline that adds a simple hyper-parameter sweep over\ncontrastive_pretraining_epochs (8-15).   For every epoch setting we retrain the\nwhole pipeline from scratch, log the dev (train key) and test (val key) HSCA,\nplus the average supervised training loss, and finally save everything in\nexperiment_data.npy.", "Hyperparam tuning name: encoder_hidden_dim. We loop over hidden dimensions [64,\n128, 256, 512], instantiating a fresh encoder/contrastive head/classifier for\neach size. For every setting we perform the same two-stage training (contrastive\npre-training then supervised fine-tuning), evaluate after every supervised epoch\non the dev set, and finally test on the held-out split. All per-epoch losses,\nHSCA scores, predictions and ground-truth labels are stored in the\nexperiment_data dictionary under the key \u201cencoder_hidden_dim\u201d, then saved to\nworking/experiment_data.npy. The rest of the pipeline (data creation, loaders,\naugmentation, loss functions) is unchanged.", "Hyperparam tuning name: contrastive_temperature. The solution performs a\nlightweight grid-search over the NT-Xent temperature \u03c4 \u2208 {0.05, 0.08, 0.10,\n0.15, 0.20}.   For every temperature the script:   1. re-initialises encoder,\nprojection head and classifier;   2. contrastively pre-trains the encoder with\nthe chosen \u03c4;   3. fine-tunes on the labelled data;   4. tracks losses / HSCA on\nthe dev split and finally evaluates on the test split.    Results for every \u03c4\nare stored under experiment_data['contrastive_temperature'][str(\u03c4)] and the\ncomplete structure is saved to experiment_data.npy.", "Hyperparam tuning name: weight_decay. Proposed solution   We loop over weight-\ndecay values {0, 1e-5, 1e-4, 5e-4, 1e-3}.   For every value we (re-)initialise\nthe encoder, projection head and classifier, then run the original 3-epoch\ncontrastive pre-training followed by 5-epoch supervised fine-tuning with Adam\nthat now receives the corresponding weight_decay parameter.   After each fine-\ntuning epoch we evaluate on the dev split (HSCA) and, once training finishes,\nevaluate on the test split.   All per-epoch losses/metrics together with the\nfinal test scores are stored in a numpy-serialisable dictionary that follows the\nrequired experiment_data layout and is saved to \u2018experiment_data.npy\u2019.", "Hyperparam tuning name: encoder_embedding_dim. We grid-search three values (64,\n128, 256) for Encoder\u2019s embedding size.   For every size we train exactly the\nsame two-stage pipeline (contrastive pre-training \u2192 supervised fine-tuning),\nrecord train/validation HSCA curves, and store them in\nexperiment_data['encoder_embedding_dim']['SPR_BENCH'][dim]. Finally we dump all\ncollected data to \u201cexperiment_data.npy\u201d.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, random, math, time, pathlib, csv\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# Utilities required by benchmark\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 use local SPR_BENCH if present, else create synthetic small one\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        tokens = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        ]\n        return \" \".join(tokens)\n\n    def gen_csv(filename, n):\n        with open(path / filename, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)  # simple rule\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [row for row in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synthetic(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocabulary\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[tok] for tok in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset objects\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.supervised = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        seq_ids = encode(row[\"sequence\"])\n        if self.supervised:\n            return {\"input\": seq_ids, \"label\": row[\"label\"], \"seq\": row[\"sequence\"]}\n        else:\n            return {\"input\": seq_ids, \"seq\": row[\"sequence\"]}\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp = []\n    labels = []\n    seqs = []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            labels.append(b[\"label\"])\n    inp = torch.tensor(inp, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": seqs}\n    if labels:\n        out[\"label\"] = torch.tensor(labels, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive learning\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue  # deletion\n        if r < 0.2:\n            new.append(1)  # mask token id=1\n        else:\n            new.append(tok)\n    if len(new) == 0:\n        new = ids\n    return new\n\n\n# ------------------------------------------------------------------\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hidden=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)  # [B,L,E]\n        lengths = (x != 0).sum(dim=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]  # [B,H]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, n_cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Contrastive loss (NT-Xent)\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t  # [2B,2B]\n    B = z.size(0) // 2\n    labels = torch.arange(B, device=z.device)\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ------------------------------------------------------------------\n# Prepare loaders\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ------------------------------------------------------------------\n# Training utils\ndef evaluate(model_enc, model_clf, loader):\n    model_enc.eval()\n    model_clf.eval()\n    y_true, y_pred, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = model_clf(model_enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_pred.extend(pred)\n            y_true.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    hsca = harmonic_sca(swa, cwa)\n    return swa, cwa, hsca\n\n\n# ------------------------------------------------------------------\n# Experiment data storage\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------------------------------------------------------\n# Instantiate models\nencoder = Encoder(vocab_size).to(device)\nproj = ProjectionHead().to(device)\noptim_enc = torch.optim.Adam(\n    list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\n# ------------------------------------------------------------------\n# Stage 1: Contrastive pre-training\nepochs_pre = 3\nfor epoch in range(1, epochs_pre + 1):\n    encoder.train()\n    proj.train()\n    tot_loss = 0\n    cnt = 0\n    for batch in contrast_loader:\n        ids = batch[\"input\"]\n        views1 = [augment(seq.tolist()) for seq in ids]\n        views2 = [augment(seq.tolist()) for seq in ids]\n\n        # encode and pad\n        def to_tensor(seqs):\n            maxlen = max(len(s) for s in seqs)\n            arr = [s + [0] * (maxlen - len(s)) for s in seqs]\n            return torch.tensor(arr, dtype=torch.long)\n\n        v1 = to_tensor(views1).to(device)\n        v2 = to_tensor(views2).to(device)\n        z1 = proj(encoder(v1))\n        z2 = proj(encoder(v2))\n        loss = nt_xent(torch.cat([z1, z2], dim=0))\n        optim_enc.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        tot_loss += loss.item()\n        cnt += 1\n    print(f\"[Contrastive] Epoch {epoch}: loss={tot_loss/cnt:.4f}\")\n\n# ------------------------------------------------------------------\n# Stage 2: Supervised fine-tuning\nclassifier = Classifier(128, 2).to(device)\noptim_all = torch.optim.Adam(\n    list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3\n)\ncriterion = nn.CrossEntropyLoss()\n\nepochs_sup = 5\nfor epoch in range(1, epochs_sup + 1):\n    encoder.train()\n    classifier.train()\n    tr_loss = 0\n    tr_cnt = 0\n    for batch in train_loader:\n        x = batch[\"input\"].to(device)\n        y = batch[\"label\"].to(device)\n        logits = classifier(encoder(x))\n        loss = criterion(logits, y)\n        optim_all.zero_grad()\n        loss.backward()\n        optim_all.step()\n        tr_loss += loss.item()\n        tr_cnt += 1\n    val_swa, val_cwa, val_hsca = evaluate(encoder, classifier, dev_loader)\n    print(\n        f\"Epoch {epoch}: validation_loss = {tr_loss/tr_cnt:.4f} | HSCA={val_hsca:.4f}\"\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(val_hsca)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss / tr_cnt)\n\n# ------------------------------------------------------------------\n# Final evaluation on test split\ntest_swa, test_cwa, test_hsca = evaluate(encoder, classifier, test_loader)\nprint(f\"TEST -> SWA: {test_swa:.4f} | CWA: {test_cwa:.4f} | HSCA: {test_hsca:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(test_hsca)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = []  # placeholder\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = []  # placeholder\n\n# ------------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, math, time, pathlib, csv, copy, warnings\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# Experiment-data dict (required format)\nexperiment_data = {\n    \"supervised_finetuning_epochs\": {\n        \"SPR_BENCH\": {\n            \"epochs_grid\": [5, 10, 15, 20],\n            \"metrics\": {\"train\": [], \"val\": []},  # list-of-lists per sweep\n            \"losses\": {\"train\": [], \"val\": []},\n            \"test_hsca\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ------------------------------------------------------------------\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 load or create synthetic\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(name, n):\n        with open(path / name, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    out = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            lst = [row for row in rdr]\n            for r in lst:\n                r[\"label\"] = int(r[\"label\"])\n            out[split] = lst\n    return out\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    DATA_PATH.mkdir(parents=True, exist_ok=True)\n    generate_synthetic(DATA_PATH)\n\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocab\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset & collate\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.sup = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        ids = encode(r[\"sequence\"])\n        item = {\"input\": ids, \"seq\": r[\"sequence\"]}\n        if self.sup:\n            item[\"label\"] = r[\"label\"]\n        return item\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    pads = [b[\"input\"] + [0] * (maxlen - len(b[\"input\"])) for b in batch]\n    inp = torch.tensor(pads, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": [b[\"seq\"] for b in batch]}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)  # MASK token id\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model components\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, dim=128, hid=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lens = (x != 0).sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, n_cls=2):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Losses / evaluation\ndef nt_xent(z, temp=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.mm(z, z.t()) / temp\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / torch.exp(denom).sum())\n        j = i + B\n        pos2 = sim[j, i]\n        denom2 = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos2) / torch.exp(denom2).sum())\n    return loss / (2 * B)\n\n\ndef evaluate(enc, clf, loader):\n    enc.eval()\n    clf.eval()\n    ys, ps, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = clf(enc(x))\n            ps.extend(logits.argmax(1).cpu().tolist())\n            ys.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, ys, ps)\n    cwa = color_weighted_accuracy(seqs, ys, ps)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# DataLoaders\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ------------------------------------------------------------------\n# Stage-1: Contrastive pre-training\nencoder = Encoder(vocab_size).to(device)\nproj = ProjectionHead().to(device)\noptim_enc = torch.optim.Adam(\n    list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\nepochs_pre = 3\nfor ep in range(1, epochs_pre + 1):\n    encoder.train()\n    proj.train()\n    tot = cnt = 0\n    for batch in contrast_loader:\n        ids = batch[\"input\"]\n        v1 = [augment(seq.tolist()) for seq in ids]\n        v2 = [augment(seq.tolist()) for seq in ids]\n\n        def pad(seqs):\n            ml = max(len(s) for s in seqs)\n            return torch.tensor(\n                [s + [0] * (ml - len(s)) for s in seqs], dtype=torch.long\n            )\n\n        z1 = proj(encoder(pad(v1).to(device)))\n        z2 = proj(encoder(pad(v2).to(device)))\n        loss = nt_xent(torch.cat([z1, z2], 0))\n        optim_enc.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"[Pre-train] epoch {ep}/{epochs_pre}  loss={tot/cnt:.4f}\")\n\npretrained_weights = copy.deepcopy(encoder.state_dict())  # save snapshot\n\n# ------------------------------------------------------------------\n# Stage-2 sweep: fine-tuning epochs grid\nepochs_grid = experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\n    \"epochs_grid\"\n]\ncriterion = nn.CrossEntropyLoss()\npatience = 3  # early stopping patience\n\nfor max_epochs in epochs_grid:\n    print(\n        f\"\\n=== Fine-tuning for up-to {max_epochs} epochs (early-stopping patience={patience}) ===\"\n    )\n    enc = Encoder(vocab_size).to(device)\n    enc.load_state_dict(pretrained_weights)\n    clf = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(enc.parameters()) + list(clf.parameters()), lr=1e-3\n    )\n\n    train_hsca_history, val_hsca_history = [], []\n    train_loss_history, val_loss_dummy = [], []\n\n    best_hsca, best_state, no_improve = -1.0, None, 0\n\n    for epoch in range(1, max_epochs + 1):\n        enc.train()\n        clf.train()\n        t_loss, steps = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = clf(enc(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            t_loss += loss.item()\n            steps += 1\n\n        swa, cwa, hsca = evaluate(enc, clf, train_loader)\n        train_hsca_history.append(hsca)\n        train_loss_history.append(t_loss / steps)\n\n        val_swa, val_cwa, val_hsca = evaluate(enc, clf, dev_loader)\n        val_hsca_history.append(val_hsca)\n        val_loss_dummy.append(0.0)  # placeholder if you want val loss later\n\n        print(f\"  Epoch {epoch:02d}: train HSCA={hsca:.4f} | dev HSCA={val_hsca:.4f}\")\n\n        # early stopping\n        if val_hsca > best_hsca + 1e-6:\n            best_hsca = val_hsca\n            best_state = (\n                copy.deepcopy(enc.state_dict()),\n                copy.deepcopy(clf.state_dict()),\n            )\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(\"  Early stopping!\")\n                break\n\n    # restore best & evaluate on test\n    enc.load_state_dict(best_state[0])\n    clf.load_state_dict(best_state[1])\n    test_swa, test_cwa, test_hsca = evaluate(enc, clf, test_loader)\n    print(f\"  >>> TEST HSCA = {test_hsca:.4f}\")\n\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train\"\n    ].append(train_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val\"\n    ].append(val_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"train\"\n    ].append(train_loss_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"val\"\n    ].append(val_loss_dummy)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"test_hsca\"].append(\n        test_hsca\n    )\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"predictions\"].append(\n        []\n    )  # placeholder\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"ground_truth\"].append(\n        []\n    )  # placeholder\n\n# ------------------------------------------------------------------\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "import os, random, math, time, pathlib, csv, itertools\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------\n# Reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# ------------------------------------------------------------------\n# Experiment container\nexperiment_data = {\n    \"lr_tuning\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},  # train=dev, val=test\n            \"losses\": {\"train\": [], \"val\": []},  # average losses\n            \"lr_pairs\": [],  # (pre_lr, sup_lr)\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# Utilities required by benchmark\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 use local SPR_BENCH if present, else create synthetic small one\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        tokens = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        ]\n        return \" \".join(tokens)\n\n    def gen_csv(filename, n):\n        with open(path / filename, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)  # simple rule\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [row for row in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synthetic(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocabulary\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[tok] for tok in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset objects\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.supervised = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        seq_ids = encode(row[\"sequence\"])\n        if self.supervised:\n            return {\"input\": seq_ids, \"label\": row[\"label\"], \"seq\": row[\"sequence\"]}\n        else:\n            return {\"input\": seq_ids, \"seq\": row[\"sequence\"]}\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp, labels, seqs = [], [], []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            labels.append(b[\"label\"])\n    inp = torch.tensor(inp, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": seqs}\n    if labels:\n        out[\"label\"] = torch.tensor(labels, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive learning\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue  # deletion\n        if r < 0.2:\n            new.append(1)  # mask token id=1\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hidden=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lengths = (x != 0).sum(dim=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, n_cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Contrastive loss (NT-Xent)\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t\n    B = z.size(0) // 2\n    labels = torch.arange(B, device=z.device)\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ------------------------------------------------------------------\n# DataLoaders (shared across runs)\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ------------------------------------------------------------------\n# Evaluation helper\ndef evaluate(model_enc, model_clf, loader):\n    model_enc.eval()\n    model_clf.eval()\n    y_true, y_pred, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = model_clf(model_enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_pred.extend(pred)\n            y_true.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# Training functions\ndef tensorize(seqs: List[List[int]]) -> torch.Tensor:\n    ml = max(len(s) for s in seqs)\n    arr = [s + [0] * (ml - len(s)) for s in seqs]\n    return torch.tensor(arr, dtype=torch.long)\n\n\ndef contrastive_pretrain(encoder, proj, lr, epochs=3):\n    opt = torch.optim.Adam(list(encoder.parameters()) + list(proj.parameters()), lr=lr)\n    encoder.train()\n    proj.train()\n    for ep in range(epochs):\n        tot, cnt = 0, 0\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            v1 = tensorize([augment(seq.tolist()) for seq in ids]).to(device)\n            v2 = tensorize([augment(seq.tolist()) for seq in ids]).to(device)\n            z1, z2 = proj(encoder(v1)), proj(encoder(v2))\n            loss = nt_xent(torch.cat([z1, z2], 0))\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            tot += loss.item()\n            cnt += 1\n        print(f\"  [Pre] lr={lr:.0e} ep={ep+1} loss={tot/cnt:.4f}\")\n\n\ndef supervised_finetune(encoder, clf, lr, epochs=5):\n    opt = torch.optim.Adam(list(encoder.parameters()) + list(clf.parameters()), lr=lr)\n    crit = nn.CrossEntropyLoss()\n    for ep in range(epochs):\n        encoder.train()\n        clf.train()\n        tot, cnt = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            loss = crit(clf(encoder(x)), y)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            tot += loss.item()\n            cnt += 1\n        swa, cwa, hsca = evaluate(encoder, clf, dev_loader)\n        print(\n            f\"  [Sup] lr={lr:.0e} ep={ep+1} train_loss={tot/cnt:.4f} dev_HSCA={hsca:.4f}\"\n        )\n    return tot / cnt  # last train loss\n\n\n# ------------------------------------------------------------------\n# Hyperparameter sweep\npre_lrs = [3e-4, 5e-4, 1e-3, 2e-3]\nsup_lrs = [3e-4, 5e-4, 1e-3, 2e-3]\n\nfor pre_lr, sup_lr in itertools.product(pre_lrs, sup_lrs):\n    print(f\"\\n=== Running combo: pre_lr={pre_lr:.0e} | sup_lr={sup_lr:.0e} ===\")\n    # fresh models\n    encoder = Encoder(vocab_size).to(device)\n    proj = ProjectionHead().to(device)\n    classifier = Classifier(128, 2).to(device)\n\n    # stage 1\n    contrastive_pretrain(encoder, proj, lr=pre_lr, epochs=3)\n\n    # stage 2\n    train_loss = supervised_finetune(encoder, classifier, lr=sup_lr, epochs=5)\n\n    # evaluations\n    dev_swa, dev_cwa, dev_hsca = evaluate(encoder, classifier, dev_loader)\n    test_swa, test_cwa, test_hsca = evaluate(encoder, classifier, test_loader)\n    print(f\"  DEV  HSCA={dev_hsca:.4f} | TEST HSCA={test_hsca:.4f}\")\n\n    # store\n    ed = experiment_data[\"lr_tuning\"][\"SPR_BENCH\"]\n    ed[\"metrics\"][\"train\"].append(dev_hsca)\n    ed[\"metrics\"][\"val\"].append(test_hsca)\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(0.0)  # placeholder\n    ed[\"lr_pairs\"].append((pre_lr, sup_lr))\n\n# ------------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, math, time, pathlib, csv\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------\n# Working dir, device & experiment dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\"batch_size\": {}}  # every entry will be \"SPR_BENCH_bs{size}\": {...}\n\n\n# ------------------------------------------------------------\n# Accuracy helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wt if t == p else 0 for wt, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------\n# Build / load data (synthetic fallback)\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\n\n\ndef _generate_synthetic(path: pathlib.Path):\n    shapes, colors = [\"A\", \"B\", \"C\", \"D\"], [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(name, n):\n        with open(path / name, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef _load_csv(path: pathlib.Path) -> Dict[str, List[Dict]]:\n    d = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(path / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            d[split] = [r for r in rdr]\n            for r in d[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return d\n\n\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    _generate_synthetic(DATA_PATH)\ndatasets = _load_csv(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------\n# Vocabulary / encoding\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------------------------------------------------\n# Torch Datasets / collate\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows, self.sup = rows, supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        ids = encode(r[\"sequence\"])\n        out = {\"input\": ids, \"seq\": r[\"sequence\"]}\n        if self.sup:\n            out[\"label\"] = r[\"label\"]\n        return out\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp, lab, seqs = [], [], []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            lab.append(b[\"label\"])\n    out = {\"input\": torch.tensor(inp, dtype=torch.long), \"seq\": seqs}\n    if lab:\n        out[\"label\"] = torch.tensor(lab, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------\n# Augmentation for contrastive views\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)  # MASK id\n        else:\n            new.append(tok)\n    return new or ids\n\n\n# ------------------------------------------------------------\n# Models\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, dim=128, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, dim, padding_idx=0)\n        self.rnn = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        lengths = (x != 0).sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.rnn(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, n_cls=2):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------\n# NT-Xent\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t\n    B = z.size(0) // 2\n    idx = torch.arange(B, device=z.device)\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ------------------------------------------------------------\n# Evaluation helpers\ndef evaluate(enc, clf, loader, return_preds=False):\n    enc.eval(), clf.eval()\n    y_t, y_p, seqs = [], [], []\n    with torch.no_grad():\n        for b in loader:\n            x = b[\"input\"].to(device)\n            logits = clf(enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_p.extend(pred)\n            y_t.extend(b[\"label\"].tolist())\n            seqs.extend(b[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_t, y_p)\n    cwa = color_weighted_accuracy(seqs, y_t, y_p)\n    hsca = harmonic_sca(swa, cwa)\n    if return_preds:\n        return swa, cwa, hsca, y_p, y_t\n    return swa, cwa, hsca\n\n\n# ------------------------------------------------------------\n# Full experiment over different batch sizes\nbatch_sizes = [64, 128, 256, 512]\nepochs_pre, epochs_sup = 3, 5\n\nfor bs in batch_sizes:\n    tag = f\"SPR_BENCH_bs{bs}\"\n    print(f\"\\n=== Running experiment: batch_size={bs} ===\")\n    # ---------------------- DataLoaders\n    contrast_loader = DataLoader(\n        SPRDataset(datasets[\"train\"], supervised=False),\n        batch_size=bs,\n        shuffle=True,\n        collate_fn=collate,\n    )\n    train_loader = DataLoader(\n        SPRDataset(datasets[\"train\"], supervised=True),\n        batch_size=bs,\n        shuffle=True,\n        collate_fn=collate,\n    )\n    dev_loader = DataLoader(\n        SPRDataset(datasets[\"dev\"], supervised=True),\n        batch_size=bs,\n        shuffle=False,\n        collate_fn=collate,\n    )\n    test_loader = DataLoader(\n        SPRDataset(datasets[\"test\"], supervised=True),\n        batch_size=bs,\n        shuffle=False,\n        collate_fn=collate,\n    )\n\n    # ---------------------- Models & optimizers\n    encoder = Encoder(vocab_size).to(device)\n    proj = ProjectionHead().to(device)\n    optim_enc = torch.optim.Adam(\n        list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n    )\n\n    # ---------- Stage 1: contrastive\n    for ep in range(1, epochs_pre + 1):\n        encoder.train(), proj.train()\n        tot, cnt = 0, 0\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            v1 = [augment(s.tolist()) for s in ids]\n            v2 = [augment(s.tolist()) for s in ids]\n\n            def to_tensor(lst):\n                ml = max(len(s) for s in lst)\n                return torch.tensor(\n                    [s + [0] * (ml - len(s)) for s in lst], dtype=torch.long\n                ).to(device)\n\n            z1 = proj(encoder(to_tensor(v1)))\n            z2 = proj(encoder(to_tensor(v2)))\n            loss = nt_xent(torch.cat([z1, z2], 0))\n            optim_enc.zero_grad()\n            loss.backward()\n            optim_enc.step()\n            tot += loss.item()\n            cnt += 1\n        print(f\"[Contrastive] epoch {ep} loss={tot/cnt:.4f}\")\n\n    # ---------- Stage 2: supervised fine-tuning\n    clf = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(encoder.parameters()) + list(clf.parameters()), lr=1e-3\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    train_losses, val_metrics = [], []\n    for ep in range(1, epochs_sup + 1):\n        encoder.train(), clf.train()\n        tloss, tcnt = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = clf(encoder(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            tloss += loss.item()\n            tcnt += 1\n        swa, cwa, hsca = evaluate(encoder, clf, dev_loader)\n        train_losses.append(tloss / tcnt)\n        val_metrics.append(hsca)\n        print(f\"[Supervised] epoch {ep} loss={tloss/tcnt:.4f} | dev HSCA={hsca:.4f}\")\n\n    # ---------- Final test\n    swa, cwa, hsca, preds, gts = evaluate(encoder, clf, test_loader, True)\n    print(f\"TEST  -> SWA {swa:.4f} | CWA {cwa:.4f} | HSCA {hsca:.4f}\")\n\n    # ---------- Store\n    experiment_data[\"batch_size\"][tag] = {\n        \"metrics\": {\"train\": val_metrics, \"val\": [hsca]},\n        \"losses\": {\"train\": train_losses, \"val\": []},\n        \"predictions\": preds,\n        \"ground_truth\": gts,\n    }\n\n# ------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, math, time, pathlib, csv, sys\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------\n# Reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n# ---------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------\n# Metrics helpers (identical to baseline)\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(t[0] for t in sequence.strip().split() if t))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(t[1] for t in sequence.strip().split() if len(t) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ---------------------------------------------------------------\n# Data (create tiny synthetic set if SPR_BENCH missing)\ndef generate_synth(path: pathlib.Path):\n    shapes, colors = [\"A\", \"B\", \"C\", \"D\"], [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(fn, n):\n        with open(path / fn, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [r for r in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synth(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ---------------------------------------------------------------\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ---------------------------------------------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows, self.sup = rows, supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        ids = encode(r[\"sequence\"])\n        out = {\"input\": ids, \"seq\": r[\"sequence\"]}\n        if self.sup:\n            out[\"label\"] = r[\"label\"]\n        return out\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    pad = lambda x: x + [0] * (maxlen - len(x))\n    inp = torch.tensor([pad(b[\"input\"]) for b in batch], dtype=torch.long)\n    seqs = [b[\"seq\"] for b in batch]\n    out = {\"input\": inp, \"seq\": seqs}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return out\n\n\n# ---------------------------------------------------------------\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)\n        else:\n            new.append(tok)\n    return new or ids\n\n\n# ---------------------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hid=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lens = (x != 0).sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ---------------------------------------------------------------\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ---------------------------------------------------------------\ndef evaluate(enc, clf, loader):\n    enc.eval()\n    clf.eval()\n    y_t, y_p, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = clf(enc(x))\n            y_p.extend(logits.argmax(1).cpu().tolist())\n            y_t.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_t, y_p)\n    cwa = color_weighted_accuracy(seqs, y_t, y_p)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ---------------------------------------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ---------------------------------------------------------------\n# Hyper-parameter sweep: contrastive_pretraining_epochs\npretrain_grid = [8, 11, 15]  # explore 8-15 epochs\nexperiment_data = {\n    \"contrastive_pretraining_epochs\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": pretrain_grid,\n        }\n    }\n}\n\nfor epochs_pre in pretrain_grid:\n    print(f\"\\n=== Running setting: contrastive_pretraining_epochs={epochs_pre} ===\")\n    # ---------- model instantiation ----------\n    encoder = Encoder(vocab_size).to(device)\n    proj = ProjectionHead().to(device)\n    optim_enc = torch.optim.Adam(\n        list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n    )\n\n    # ---------- contrastive pre-training ----------\n    for ep in range(1, epochs_pre + 1):\n        encoder.train()\n        proj.train()\n        tot = cnt = 0\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            v1 = [augment(s.tolist()) for s in ids]\n            v2 = [augment(s.tolist()) for s in ids]\n\n            def to_t(seqs):\n                L = max(len(s) for s in seqs)\n                return torch.tensor(\n                    [s + [0] * (L - len(s)) for s in seqs], dtype=torch.long\n                )\n\n            z1 = proj(encoder(to_t(v1).to(device)))\n            z2 = proj(encoder(to_t(v2).to(device)))\n            loss = nt_xent(torch.cat([z1, z2], 0))\n            optim_enc.zero_grad()\n            loss.backward()\n            optim_enc.step()\n            tot += loss.item()\n            cnt += 1\n        print(f\"[Contrastive] epoch {ep}/{epochs_pre} loss={tot/cnt:.4f}\")\n\n    # ---------- supervised fine-tuning ----------\n    classifier = Classifier(128, 2).to(device)\n    optim_sup = torch.optim.Adam(\n        list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3\n    )\n    crit = nn.CrossEntropyLoss()\n    sup_epochs = 5\n    sup_losses = []\n    for ep in range(1, sup_epochs + 1):\n        encoder.train()\n        classifier.train()\n        tl = tc = 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = classifier(encoder(x))\n            loss = crit(logits, y)\n            optim_sup.zero_grad()\n            loss.backward()\n            optim_sup.step()\n            tl += loss.item()\n            tc += 1\n        swa, cwa, hsca_dev = evaluate(encoder, classifier, dev_loader)\n        sup_losses.append(tl / tc)\n        print(f\"[Sup] epoch {ep}/{sup_epochs} loss={tl/tc:.4f} Dev-HSCA={hsca_dev:.4f}\")\n    # store dev metric as last epoch HSCA\n    experiment_data[\"contrastive_pretraining_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train\"\n    ].append(hsca_dev)\n    experiment_data[\"contrastive_pretraining_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"train\"\n    ].append(np.mean(sup_losses))\n\n    # ---------- final test ----------\n    swa_t, cwa_t, hsca_test = evaluate(encoder, classifier, test_loader)\n    print(f\"TEST HSCA={hsca_test:.4f} (SWA={swa_t:.4f}, CWA={cwa_t:.4f})\")\n    experiment_data[\"contrastive_pretraining_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val\"\n    ].append(hsca_test)\n\n# ---------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random, math, time, pathlib, csv, copy\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------\n# Reproducibility\ntorch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# Utilities required by benchmark\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 use local SPR_BENCH if present, else create synthetic small one\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        tokens = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        ]\n        return \" \".join(tokens)\n\n    def gen_csv(filename, n):\n        with open(path / filename, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)  # simple rule\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [row for row in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synthetic(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocabulary\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[tok] for tok in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset objects\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.supervised = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        seq_ids = encode(row[\"sequence\"])\n        if self.supervised:\n            return {\"input\": seq_ids, \"label\": row[\"label\"], \"seq\": row[\"sequence\"]}\n        else:\n            return {\"input\": seq_ids, \"seq\": row[\"sequence\"]}\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp, labels, seqs = [], [], []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            labels.append(b[\"label\"])\n    inp = torch.tensor(inp, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": seqs}\n    if labels:\n        out[\"label\"] = torch.tensor(labels, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive learning\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue  # deletion\n        if r < 0.2:\n            new.append(1)  # mask token id=1\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hidden=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lengths = (x != 0).sum(dim=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, n_cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Contrastive loss (NT-Xent)\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t\n    B = z.size(0) // 2\n    loss = []\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss.append(-torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8)))\n        j = i + B\n        pos2 = sim[j, i]\n        denom2 = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss.append(-torch.log(torch.exp(pos2) / (torch.exp(denom2).sum() + 1e-8)))\n    return torch.stack(loss).mean()\n\n\n# ------------------------------------------------------------------\n# Evaluation helper\ndef evaluate(model_enc, model_clf, loader, return_preds=False):\n    model_enc.eval()\n    model_clf.eval()\n    y_true, y_pred, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = model_clf(model_enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_pred.extend(pred)\n            y_true.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    hsca = harmonic_sca(swa, cwa)\n    if return_preds:\n        return swa, cwa, hsca, y_pred, y_true\n    return swa, cwa, hsca\n\n\n# ------------------------------------------------------------------\n# DataLoaders (static across experiments)\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ------------------------------------------------------------------\n# Experiment data container\nexperiment_data = {\"encoder_hidden_dim\": {\"SPR_BENCH\": {}}}\n\n# ------------------------------------------------------------------\n# Hyperparameter search\nhidden_dims = [64, 128, 256, 512]\nepochs_pre, epochs_sup = 3, 5\n\nfor hdim in hidden_dims:\n    run_key = f\"hid_{hdim}\"\n    experiment_data[\"encoder_hidden_dim\"][\"SPR_BENCH\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    print(f\"\\n=== Running experiment with encoder hidden dim = {hdim} ===\")\n\n    # Instantiate models\n    encoder = Encoder(vocab_size, dim=128, hidden=hdim).to(device)\n    proj = ProjectionHead(in_dim=hdim, out_dim=64).to(device)\n    classifier = Classifier(hdim, 2).to(device)\n\n    # Optimizers\n    optim_enc = torch.optim.Adam(\n        list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n    )\n\n    # ---- Stage 1: Contrastive pre-training ----\n    for epoch in range(1, epochs_pre + 1):\n        encoder.train()\n        proj.train()\n        tot_loss = 0\n        cnt = 0\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            views1 = [augment(seq.tolist()) for seq in ids]\n            views2 = [augment(seq.tolist()) for seq in ids]\n\n            def to_tensor(seqs):\n                maxlen = max(len(s) for s in seqs)\n                arr = [s + [0] * (maxlen - len(s)) for s in seqs]\n                return torch.tensor(arr, dtype=torch.long)\n\n            v1 = to_tensor(views1).to(device)\n            v2 = to_tensor(views2).to(device)\n            z1 = proj(encoder(v1))\n            z2 = proj(encoder(v2))\n            loss = nt_xent(torch.cat([z1, z2], dim=0))\n            optim_enc.zero_grad()\n            loss.backward()\n            optim_enc.step()\n            tot_loss += loss.item()\n            cnt += 1\n        print(f\"[Contrastive] Epoch {epoch}: loss={tot_loss/cnt:.4f}\")\n\n    # ---- Stage 2: Supervised fine-tuning ----\n    optim_all = torch.optim.Adam(\n        list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3\n    )\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(1, epochs_sup + 1):\n        encoder.train()\n        classifier.train()\n        tr_loss = 0\n        tr_cnt = 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = classifier(encoder(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            tr_loss += loss.item()\n            tr_cnt += 1\n        swa, cwa, hsca = evaluate(encoder, classifier, dev_loader)\n        print(f\"[FineTune] Epoch {epoch}: loss={tr_loss/tr_cnt:.4f} | HSCA={hsca:.4f}\")\n        # Store metrics\n        experiment_data[\"encoder_hidden_dim\"][\"SPR_BENCH\"][run_key][\"metrics\"][\n            \"train\"\n        ].append(hsca)\n        experiment_data[\"encoder_hidden_dim\"][\"SPR_BENCH\"][run_key][\"losses\"][\n            \"train\"\n        ].append(tr_loss / tr_cnt)\n\n    # ---- Final test evaluation ----\n    test_swa, test_cwa, test_hsca, preds, gts = evaluate(\n        encoder, classifier, test_loader, return_preds=True\n    )\n    print(f\"[TEST] SWA={test_swa:.4f} | CWA={test_cwa:.4f} | HSCA={test_hsca:.4f}\")\n    edict = experiment_data[\"encoder_hidden_dim\"][\"SPR_BENCH\"][run_key]\n    edict[\"metrics\"][\"val\"].append(test_hsca)\n    edict[\"losses\"][\"val\"].append(None)  # placeholder\n    edict[\"predictions\"] = preds\n    edict[\"ground_truth\"] = gts\n\n    # free memory\n    del encoder, proj, classifier, optim_enc, optim_all\n    torch.cuda.empty_cache()\n\n# ------------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, math, time, pathlib, csv\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------\n# Experiment data structure\nexperiment_data = {\"contrastive_temperature\": {}}  # results filled per temperature\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# Repro helper\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\n# ------------------------------------------------------------------\n# Utilities required by benchmark\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 use local SPR_BENCH if present, else create synthetic small one\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        tokens = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        ]\n        return \" \".join(tokens)\n\n    def gen_csv(filename, n):\n        with open(path / filename, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)  # simple rule\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [row for row in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synthetic(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocabulary\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[tok] for tok in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset objects\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.supervised = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        seq_ids = encode(row[\"sequence\"])\n        if self.supervised:\n            return {\"input\": seq_ids, \"label\": row[\"label\"], \"seq\": row[\"sequence\"]}\n        else:\n            return {\"input\": seq_ids, \"seq\": row[\"sequence\"]}\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp, labels, seqs = [], [], []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            labels.append(b[\"label\"])\n    inp = torch.tensor(inp, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": seqs}\n    if labels:\n        out[\"label\"] = torch.tensor(labels, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive learning\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue  # deletion\n        if r < 0.2:\n            new.append(1)  # mask token id=1\n        else:\n            new.append(tok)\n    if len(new) == 0:\n        new = ids\n    return new\n\n\n# ------------------------------------------------------------------\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hidden=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lengths = (x != 0).sum(dim=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, n_cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Contrastive loss (NT-Xent) with temperature parameter\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t  # [2B,2B]\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ------------------------------------------------------------------\n# DataLoaders (shared across experiments)\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ------------------------------------------------------------------\n# Evaluation helper\ndef evaluate(model_enc, model_clf, loader):\n    model_enc.eval()\n    model_clf.eval()\n    y_true, y_pred, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = model_clf(model_enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_pred.extend(pred)\n            y_true.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    hsca = harmonic_sca(swa, cwa)\n    return swa, cwa, hsca\n\n\n# ------------------------------------------------------------------\n# Training routine for one temperature\ndef run_experiment(temp_val: float):\n    set_seed(42)\n    encoder = Encoder(vocab_size).to(device)\n    proj = ProjectionHead().to(device)\n    optim_enc = torch.optim.Adam(\n        list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n    )\n\n    # Contrastive pre-training\n    epochs_pre = 3\n    pre_losses = []\n    for epoch in range(1, epochs_pre + 1):\n        encoder.train()\n        proj.train()\n        tot_loss = 0\n        cnt = 0\n\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            views1 = [augment(seq.tolist()) for seq in ids]\n            views2 = [augment(seq.tolist()) for seq in ids]\n\n            def to_tensor(seqs):\n                maxlen = max(len(s) for s in seqs)\n                arr = [s + [0] * (maxlen - len(s)) for s in seqs]\n                return torch.tensor(arr, dtype=torch.long)\n\n            v1 = to_tensor(views1).to(device)\n            v2 = to_tensor(views2).to(device)\n            z1 = proj(encoder(v1))\n            z2 = proj(encoder(v2))\n            loss = nt_xent(torch.cat([z1, z2], dim=0), t=temp_val)\n            optim_enc.zero_grad()\n            loss.backward()\n            optim_enc.step()\n            tot_loss += loss.item()\n            cnt += 1\n        pre_losses.append(tot_loss / cnt)\n        print(f\"[\u03c4={temp_val}] Contrastive Epoch {epoch}: loss={tot_loss / cnt:.4f}\")\n\n    # Supervised fine-tuning\n    classifier = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    epochs_sup = 5\n    train_losses, dev_hsca = [], []\n    for epoch in range(1, epochs_sup + 1):\n        encoder.train()\n        classifier.train()\n        tr_loss = 0\n        tr_cnt = 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = classifier(encoder(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            tr_loss += loss.item()\n            tr_cnt += 1\n        train_losses.append(tr_loss / tr_cnt)\n        _, _, hsca_val = evaluate(encoder, classifier, dev_loader)\n        dev_hsca.append(hsca_val)\n        print(\n            f\"[\u03c4={temp_val}] Supervised Epoch {epoch}: train_loss={tr_loss / tr_cnt:.4f} | Dev HSCA={hsca_val:.4f}\"\n        )\n\n    # Final test evaluation with the final model\n    swa_t, cwa_t, hsca_test = evaluate(encoder, classifier, test_loader)\n    print(\n        f\"[\u03c4={temp_val}] TEST -> SWA: {swa_t:.4f} | CWA: {cwa_t:.4f} | HSCA: {hsca_test:.4f}\"\n    )\n\n    # Store data\n    data_dict = {\n        \"metrics\": {\"train\": np.array(dev_hsca), \"val\": np.array([hsca_test])},\n        \"losses\": {\n            \"train\": np.array(train_losses),\n            \"val\": np.array(pre_losses),  # store pre-training losses for reference\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    return data_dict, hsca_test\n\n\n# ------------------------------------------------------------------\n# Grid search over temperatures\ntemperature_grid = [0.05, 0.08, 0.1, 0.15, 0.2]\nbest_temp, best_score = None, -1\nfor temp in temperature_grid:\n    res_dict, test_score = run_experiment(temp)\n    experiment_data[\"contrastive_temperature\"][str(temp)] = res_dict\n    if test_score > best_score:\n        best_score, best_temp = test_score, temp\n\nprint(f\"Best \u03c4 = {best_temp} | Test HSCA = {best_score:.4f}\")\n\n# ------------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, math, time, pathlib, csv, json\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------\n# Reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------------------------------------------------------\n# Experiment data container\nexperiment_data = {\n    \"weight_decay\": {\n        \"SPR_BENCH\": {\n            \"decays\": [],  # list of decay values tried\n            \"metrics\": {\"train\": [], \"val\": []},  # list-of-lists per decay\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],  # kept empty \u2013 placeholder\n            \"ground_truth\": [],  # kept empty \u2013 placeholder\n        }\n    }\n}\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# Utilities required by benchmark\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 use local SPR_BENCH if present, else create synthetic small one\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        tokens = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        ]\n        return \" \".join(tokens)\n\n    def gen_csv(filename, n):\n        with open(path / filename, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)  # simple rule\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [row for row in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synthetic(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocabulary\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[tok] for tok in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset objects\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.supervised = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        seq_ids = encode(row[\"sequence\"])\n        if self.supervised:\n            return {\"input\": seq_ids, \"label\": row[\"label\"], \"seq\": row[\"sequence\"]}\n        else:\n            return {\"input\": seq_ids, \"seq\": row[\"sequence\"]}\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp, labels, seqs = [], [], []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            labels.append(b[\"label\"])\n    inp = torch.tensor(inp, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": seqs}\n    if labels:\n        out[\"label\"] = torch.tensor(labels, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive learning\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue  # deletion\n        if r < 0.2:\n            new.append(1)  # mask token id=1\n        else:\n            new.append(tok)\n    if len(new) == 0:\n        new = ids\n    return new\n\n\n# ------------------------------------------------------------------\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hidden=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lengths = (x != 0).sum(dim=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, n_cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Contrastive loss (NT-Xent)\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ------------------------------------------------------------------\n# Evaluation\ndef evaluate(model_enc, model_clf, loader):\n    model_enc.eval()\n    model_clf.eval()\n    y_true, y_pred, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = model_clf(model_enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_pred.extend(pred)\n            y_true.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# DataLoaders (static across runs)\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], supervised=True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ------------------------------------------------------------------\n# Hyperparameter sweep\ndecay_grid = [0.0, 1e-5, 1e-4, 5e-4, 1e-3]\nfor wd in decay_grid:\n    print(f\"\\n========== Weight Decay = {wd} ==========\")\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"decays\"].append(wd)\n\n    # instantiate fresh models\n    encoder = Encoder(vocab_size).to(device)\n    proj = ProjectionHead().to(device)\n    classifier = Classifier(128, 2).to(device)\n\n    optim_enc = torch.optim.Adam(\n        list(encoder.parameters()) + list(proj.parameters()),\n        lr=1e-3,\n        weight_decay=wd,\n    )\n\n    # ----------------- Stage 1: contrastive pre-training -----------------\n    epochs_pre = 3\n    for epoch in range(1, epochs_pre + 1):\n        encoder.train()\n        proj.train()\n        tot_loss, cnt = 0, 0\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            views1 = [augment(seq.tolist()) for seq in ids]\n            views2 = [augment(seq.tolist()) for seq in ids]\n\n            def to_tensor(seqs):\n                m = max(len(s) for s in seqs)\n                return torch.tensor(\n                    [s + [0] * (m - len(s)) for s in seqs], dtype=torch.long\n                )\n\n            v1, v2 = to_tensor(views1).to(device), to_tensor(views2).to(device)\n            z1, z2 = proj(encoder(v1)), proj(encoder(v2))\n            loss = nt_xent(torch.cat([z1, z2], dim=0))\n            optim_enc.zero_grad()\n            loss.backward()\n            optim_enc.step()\n            tot_loss += loss.item()\n            cnt += 1\n        print(f\"[Contrastive] Epoch {epoch}: loss={tot_loss/cnt:.4f}\")\n\n    # ----------------- Stage 2: supervised fine-tuning -----------------\n    optim_all = torch.optim.Adam(\n        list(encoder.parameters()) + list(classifier.parameters()),\n        lr=1e-3,\n        weight_decay=wd,\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    tr_metrics, tr_losses = [], []\n    epochs_sup = 5\n    for epoch in range(1, epochs_sup + 1):\n        encoder.train()\n        classifier.train()\n        tr_loss_sum, tr_cnt = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = classifier(encoder(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            tr_loss_sum += loss.item()\n            tr_cnt += 1\n        val_swa, val_cwa, val_hsca = evaluate(encoder, classifier, dev_loader)\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss_sum/tr_cnt:.4f} | Dev HSCA={val_hsca:.4f}\"\n        )\n        tr_losses.append(tr_loss_sum / tr_cnt)\n        tr_metrics.append(val_hsca)\n\n    # record per-decay training curves\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_losses)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_metrics)\n\n    # ----------------- Final test evaluation -----------------\n    test_swa, test_cwa, test_hsca = evaluate(encoder, classifier, test_loader)\n    print(f\"TEST -> SWA: {test_swa:.4f} | CWA: {test_cwa:.4f} | HSCA: {test_hsca:.4f}\")\n\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(test_hsca)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        None\n    )  # placeholder\n\n# ------------------------------------------------------------------\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Hyper-parameter tuning: encoder_embedding_dim\nimport os, random, math, time, pathlib, csv, copy\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------\n# Experiment data dict\nexperiment_data = {\"encoder_embedding_dim\": {\"SPR_BENCH\": {}}}\n\n\n# ------------------------------------------------------------------\n# Reproducibility helpers\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed()\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# Utilities required by benchmark\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(token[0] for token in sequence.strip().split() if token))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(token[1] for token in sequence.strip().split() if len(token) > 1))\n\n\ndef shape_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_shape_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef color_weighted_accuracy(sequences, y_true, y_pred):\n    weights = [count_color_variety(seq) for seq in sequences]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / max(sum(weights), 1)\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 load or generate synthetic\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(fname, n):\n        with open(path / fname, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    data = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            data[split] = [row for row in rdr]\n            for r in data[split]:\n                r[\"label\"] = int(r[\"label\"])\n    return data\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    os.makedirs(DATA_PATH, exist_ok=True)\n    generate_synthetic(DATA_PATH)\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocabulary\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[tok] for tok in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset objects\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.supervised = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        seq_ids = encode(row[\"sequence\"])\n        if self.supervised:\n            return {\"input\": seq_ids, \"label\": row[\"label\"], \"seq\": row[\"sequence\"]}\n        return {\"input\": seq_ids, \"seq\": row[\"sequence\"]}\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    inp, labels, seqs = [], [], []\n    for b in batch:\n        seqs.append(b[\"seq\"])\n        pad = b[\"input\"] + [0] * (maxlen - len(b[\"input\"]))\n        inp.append(pad)\n        if \"label\" in b:\n            labels.append(b[\"label\"])\n    inp = torch.tensor(inp, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": seqs}\n    if labels:\n        out[\"label\"] = torch.tensor(labels, dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive learning\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dim=128, hidden=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lengths = (x != 0).sum(dim=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc_out, n_cls):\n        super().__init__()\n        self.fc = nn.Linear(enc_out, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Contrastive loss\ndef nt_xent(z, t=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / t\n    B = z.size(0) // 2\n    idx = torch.arange(B, device=z.device)\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n        j = i + B\n        pos = sim[j, i]\n        denom = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos) / (torch.exp(denom).sum() + 1e-8))\n    return loss / (2 * B)\n\n\n# ------------------------------------------------------------------\ndef evaluate(model_enc, model_clf, loader):\n    model_enc.eval()\n    model_clf.eval()\n    y_true, y_pred, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = model_clf(model_enc(x))\n            pred = logits.argmax(1).cpu().tolist()\n            y_pred.extend(pred)\n            y_true.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# Loaders (shared)\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"], True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"], True),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n\n# ------------------------------------------------------------------\ndef run_experiment(embed_dim):\n    print(f\"\\n=== Running experiment: embed_dim={embed_dim} ===\")\n    # storage\n    exp_store = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # models\n    encoder = Encoder(vocab_size, dim=embed_dim, hidden=128).to(device)\n    proj = ProjectionHead(128).to(device)\n    optim_enc = torch.optim.Adam(\n        list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n    )\n\n    # Stage 1: contrastive\n    epochs_pre = 3\n    for ep in range(1, epochs_pre + 1):\n        encoder.train()\n        proj.train()\n        tot_loss = cnt = 0\n        for batch in contrast_loader:\n            ids = batch[\"input\"]\n            views1 = [augment(seq.tolist()) for seq in ids]\n            views2 = [augment(seq.tolist()) for seq in ids]\n\n            def to_tensor(seqs):\n                maxlen = max(len(s) for s in seqs)\n                return torch.tensor(\n                    [s + [0] * (maxlen - len(s)) for s in seqs], dtype=torch.long\n                )\n\n            v1 = to_tensor(views1).to(device)\n            v2 = to_tensor(views2).to(device)\n            z1 = proj(encoder(v1))\n            z2 = proj(encoder(v2))\n            loss = nt_xent(torch.cat([z1, z2], 0))\n            optim_enc.zero_grad()\n            loss.backward()\n            optim_enc.step()\n            tot_loss += loss.item()\n            cnt += 1\n        print(f\"[Contrastive] ep{ep} loss={tot_loss/cnt:.4f}\")\n\n    # Stage 2: supervised\n    classifier = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3\n    )\n    criterion = nn.CrossEntropyLoss()\n    epochs_sup = 5\n    for ep in range(1, epochs_sup + 1):\n        encoder.train()\n        classifier.train()\n        tr_loss = tr_cnt = 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = classifier(encoder(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            tr_loss += loss.item()\n            tr_cnt += 1\n        swa, cwa, hsca = evaluate(encoder, classifier, dev_loader)\n        print(f\"[Supervised] ep{ep} train_loss={tr_loss/tr_cnt:.4f} HSCA={hsca:.4f}\")\n        exp_store[\"metrics\"][\"train\"].append(hsca)\n        exp_store[\"losses\"][\"train\"].append(tr_loss / tr_cnt)\n\n    # final test\n    swa, cwa, hsca = evaluate(encoder, classifier, test_loader)\n    print(f\"TEST (embed_dim={embed_dim}) -> HSCA={hsca:.4f}\")\n    exp_store[\"metrics\"][\"val\"].append(hsca)\n\n    return exp_store\n\n\n# ------------------------------------------------------------------\ndims_to_try = [64, 128, 256]\nfor d in dims_to_try:\n    set_seed()  # reset seeds for fair comparison\n    result = run_experiment(d)\n    experiment_data[\"encoder_embedding_dim\"][\"SPR_BENCH\"][d] = result\n    torch.cuda.empty_cache()\n\n# ------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, math, time, pathlib, csv, copy, warnings\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# Experiment-data dict (required format)\nexperiment_data = {\n    \"supervised_finetuning_epochs\": {\n        \"SPR_BENCH\": {\n            \"epochs_grid\": [5, 10, 15, 20],\n            \"metrics\": {\"train\": [], \"val\": []},  # list-of-lists per sweep\n            \"losses\": {\"train\": [], \"val\": []},\n            \"test_hsca\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ------------------------------------------------------------------\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 load or create synthetic\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(name, n):\n        with open(path / name, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    out = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            lst = [row for row in rdr]\n            for r in lst:\n                r[\"label\"] = int(r[\"label\"])\n            out[split] = lst\n    return out\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    DATA_PATH.mkdir(parents=True, exist_ok=True)\n    generate_synthetic(DATA_PATH)\n\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocab\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset & collate\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.sup = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        ids = encode(r[\"sequence\"])\n        item = {\"input\": ids, \"seq\": r[\"sequence\"]}\n        if self.sup:\n            item[\"label\"] = r[\"label\"]\n        return item\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    pads = [b[\"input\"] + [0] * (maxlen - len(b[\"input\"])) for b in batch]\n    inp = torch.tensor(pads, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": [b[\"seq\"] for b in batch]}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)  # MASK token id\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model components\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, dim=128, hid=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lens = (x != 0).sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, n_cls=2):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Losses / evaluation\ndef nt_xent(z, temp=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.mm(z, z.t()) / temp\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / torch.exp(denom).sum())\n        j = i + B\n        pos2 = sim[j, i]\n        denom2 = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos2) / torch.exp(denom2).sum())\n    return loss / (2 * B)\n\n\ndef evaluate(enc, clf, loader):\n    enc.eval()\n    clf.eval()\n    ys, ps, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = clf(enc(x))\n            ps.extend(logits.argmax(1).cpu().tolist())\n            ys.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, ys, ps)\n    cwa = color_weighted_accuracy(seqs, ys, ps)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# DataLoaders\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ------------------------------------------------------------------\n# Stage-1: Contrastive pre-training\nencoder = Encoder(vocab_size).to(device)\nproj = ProjectionHead().to(device)\noptim_enc = torch.optim.Adam(\n    list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\nepochs_pre = 3\nfor ep in range(1, epochs_pre + 1):\n    encoder.train()\n    proj.train()\n    tot = cnt = 0\n    for batch in contrast_loader:\n        ids = batch[\"input\"]\n        v1 = [augment(seq.tolist()) for seq in ids]\n        v2 = [augment(seq.tolist()) for seq in ids]\n\n        def pad(seqs):\n            ml = max(len(s) for s in seqs)\n            return torch.tensor(\n                [s + [0] * (ml - len(s)) for s in seqs], dtype=torch.long\n            )\n\n        z1 = proj(encoder(pad(v1).to(device)))\n        z2 = proj(encoder(pad(v2).to(device)))\n        loss = nt_xent(torch.cat([z1, z2], 0))\n        optim_enc.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"[Pre-train] epoch {ep}/{epochs_pre}  loss={tot/cnt:.4f}\")\n\npretrained_weights = copy.deepcopy(encoder.state_dict())  # save snapshot\n\n# ------------------------------------------------------------------\n# Stage-2 sweep: fine-tuning epochs grid\nepochs_grid = experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\n    \"epochs_grid\"\n]\ncriterion = nn.CrossEntropyLoss()\npatience = 3  # early stopping patience\n\nfor max_epochs in epochs_grid:\n    print(\n        f\"\\n=== Fine-tuning for up-to {max_epochs} epochs (early-stopping patience={patience}) ===\"\n    )\n    enc = Encoder(vocab_size).to(device)\n    enc.load_state_dict(pretrained_weights)\n    clf = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(enc.parameters()) + list(clf.parameters()), lr=1e-3\n    )\n\n    train_hsca_history, val_hsca_history = [], []\n    train_loss_history, val_loss_dummy = [], []\n\n    best_hsca, best_state, no_improve = -1.0, None, 0\n\n    for epoch in range(1, max_epochs + 1):\n        enc.train()\n        clf.train()\n        t_loss, steps = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = clf(enc(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            t_loss += loss.item()\n            steps += 1\n\n        swa, cwa, hsca = evaluate(enc, clf, train_loader)\n        train_hsca_history.append(hsca)\n        train_loss_history.append(t_loss / steps)\n\n        val_swa, val_cwa, val_hsca = evaluate(enc, clf, dev_loader)\n        val_hsca_history.append(val_hsca)\n        val_loss_dummy.append(0.0)  # placeholder if you want val loss later\n\n        print(f\"  Epoch {epoch:02d}: train HSCA={hsca:.4f} | dev HSCA={val_hsca:.4f}\")\n\n        # early stopping\n        if val_hsca > best_hsca + 1e-6:\n            best_hsca = val_hsca\n            best_state = (\n                copy.deepcopy(enc.state_dict()),\n                copy.deepcopy(clf.state_dict()),\n            )\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(\"  Early stopping!\")\n                break\n\n    # restore best & evaluate on test\n    enc.load_state_dict(best_state[0])\n    clf.load_state_dict(best_state[1])\n    test_swa, test_cwa, test_hsca = evaluate(enc, clf, test_loader)\n    print(f\"  >>> TEST HSCA = {test_hsca:.4f}\")\n\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train\"\n    ].append(train_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val\"\n    ].append(val_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"train\"\n    ].append(train_loss_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"val\"\n    ].append(val_loss_dummy)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"test_hsca\"].append(\n        test_hsca\n    )\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"predictions\"].append(\n        []\n    )  # placeholder\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"ground_truth\"].append(\n        []\n    )  # placeholder\n\n# ------------------------------------------------------------------\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, math, time, pathlib, csv, copy, warnings\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# Experiment-data dict (required format)\nexperiment_data = {\n    \"supervised_finetuning_epochs\": {\n        \"SPR_BENCH\": {\n            \"epochs_grid\": [5, 10, 15, 20],\n            \"metrics\": {\"train\": [], \"val\": []},  # list-of-lists per sweep\n            \"losses\": {\"train\": [], \"val\": []},\n            \"test_hsca\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ------------------------------------------------------------------\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 load or create synthetic\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(name, n):\n        with open(path / name, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    out = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            lst = [row for row in rdr]\n            for r in lst:\n                r[\"label\"] = int(r[\"label\"])\n            out[split] = lst\n    return out\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    DATA_PATH.mkdir(parents=True, exist_ok=True)\n    generate_synthetic(DATA_PATH)\n\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocab\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset & collate\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.sup = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        ids = encode(r[\"sequence\"])\n        item = {\"input\": ids, \"seq\": r[\"sequence\"]}\n        if self.sup:\n            item[\"label\"] = r[\"label\"]\n        return item\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    pads = [b[\"input\"] + [0] * (maxlen - len(b[\"input\"])) for b in batch]\n    inp = torch.tensor(pads, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": [b[\"seq\"] for b in batch]}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)  # MASK token id\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model components\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, dim=128, hid=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lens = (x != 0).sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, n_cls=2):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Losses / evaluation\ndef nt_xent(z, temp=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.mm(z, z.t()) / temp\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / torch.exp(denom).sum())\n        j = i + B\n        pos2 = sim[j, i]\n        denom2 = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos2) / torch.exp(denom2).sum())\n    return loss / (2 * B)\n\n\ndef evaluate(enc, clf, loader):\n    enc.eval()\n    clf.eval()\n    ys, ps, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = clf(enc(x))\n            ps.extend(logits.argmax(1).cpu().tolist())\n            ys.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, ys, ps)\n    cwa = color_weighted_accuracy(seqs, ys, ps)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# DataLoaders\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ------------------------------------------------------------------\n# Stage-1: Contrastive pre-training\nencoder = Encoder(vocab_size).to(device)\nproj = ProjectionHead().to(device)\noptim_enc = torch.optim.Adam(\n    list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\nepochs_pre = 3\nfor ep in range(1, epochs_pre + 1):\n    encoder.train()\n    proj.train()\n    tot = cnt = 0\n    for batch in contrast_loader:\n        ids = batch[\"input\"]\n        v1 = [augment(seq.tolist()) for seq in ids]\n        v2 = [augment(seq.tolist()) for seq in ids]\n\n        def pad(seqs):\n            ml = max(len(s) for s in seqs)\n            return torch.tensor(\n                [s + [0] * (ml - len(s)) for s in seqs], dtype=torch.long\n            )\n\n        z1 = proj(encoder(pad(v1).to(device)))\n        z2 = proj(encoder(pad(v2).to(device)))\n        loss = nt_xent(torch.cat([z1, z2], 0))\n        optim_enc.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"[Pre-train] epoch {ep}/{epochs_pre}  loss={tot/cnt:.4f}\")\n\npretrained_weights = copy.deepcopy(encoder.state_dict())  # save snapshot\n\n# ------------------------------------------------------------------\n# Stage-2 sweep: fine-tuning epochs grid\nepochs_grid = experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\n    \"epochs_grid\"\n]\ncriterion = nn.CrossEntropyLoss()\npatience = 3  # early stopping patience\n\nfor max_epochs in epochs_grid:\n    print(\n        f\"\\n=== Fine-tuning for up-to {max_epochs} epochs (early-stopping patience={patience}) ===\"\n    )\n    enc = Encoder(vocab_size).to(device)\n    enc.load_state_dict(pretrained_weights)\n    clf = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(enc.parameters()) + list(clf.parameters()), lr=1e-3\n    )\n\n    train_hsca_history, val_hsca_history = [], []\n    train_loss_history, val_loss_dummy = [], []\n\n    best_hsca, best_state, no_improve = -1.0, None, 0\n\n    for epoch in range(1, max_epochs + 1):\n        enc.train()\n        clf.train()\n        t_loss, steps = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = clf(enc(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            t_loss += loss.item()\n            steps += 1\n\n        swa, cwa, hsca = evaluate(enc, clf, train_loader)\n        train_hsca_history.append(hsca)\n        train_loss_history.append(t_loss / steps)\n\n        val_swa, val_cwa, val_hsca = evaluate(enc, clf, dev_loader)\n        val_hsca_history.append(val_hsca)\n        val_loss_dummy.append(0.0)  # placeholder if you want val loss later\n\n        print(f\"  Epoch {epoch:02d}: train HSCA={hsca:.4f} | dev HSCA={val_hsca:.4f}\")\n\n        # early stopping\n        if val_hsca > best_hsca + 1e-6:\n            best_hsca = val_hsca\n            best_state = (\n                copy.deepcopy(enc.state_dict()),\n                copy.deepcopy(clf.state_dict()),\n            )\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(\"  Early stopping!\")\n                break\n\n    # restore best & evaluate on test\n    enc.load_state_dict(best_state[0])\n    clf.load_state_dict(best_state[1])\n    test_swa, test_cwa, test_hsca = evaluate(enc, clf, test_loader)\n    print(f\"  >>> TEST HSCA = {test_hsca:.4f}\")\n\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train\"\n    ].append(train_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val\"\n    ].append(val_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"train\"\n    ].append(train_loss_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"val\"\n    ].append(val_loss_dummy)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"test_hsca\"].append(\n        test_hsca\n    )\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"predictions\"].append(\n        []\n    )  # placeholder\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"ground_truth\"].append(\n        []\n    )  # placeholder\n\n# ------------------------------------------------------------------\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, math, time, pathlib, csv, copy, warnings\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ------------------------------------------------------------------\n# Working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# Experiment-data dict (required format)\nexperiment_data = {\n    \"supervised_finetuning_epochs\": {\n        \"SPR_BENCH\": {\n            \"epochs_grid\": [5, 10, 15, 20],\n            \"metrics\": {\"train\": [], \"val\": []},  # list-of-lists per sweep\n            \"losses\": {\"train\": [], \"val\": []},\n            \"test_hsca\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ------------------------------------------------------------------\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum((wi if yt == yp else 0) for wi, yt, yp in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef harmonic_sca(swa, cwa, eps=1e-8):\n    return 2 * swa * cwa / (swa + cwa + eps)\n\n\n# ------------------------------------------------------------------\n# Data \u2013 load or create synthetic\ndef generate_synthetic(path: pathlib.Path):\n    shapes = [\"A\", \"B\", \"C\", \"D\"]\n    colors = [\"1\", \"2\", \"3\"]\n\n    def gen_seq():\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(5, 10))\n        )\n\n    def gen_csv(name, n):\n        with open(path / name, \"w\", newline=\"\") as f:\n            w = csv.writer(f)\n            w.writerow([\"id\", \"sequence\", \"label\"])\n            for i in range(n):\n                seq = gen_seq()\n                label = int(count_shape_variety(seq) % 2 == 0)\n                w.writerow([i, seq, label])\n\n    gen_csv(\"train.csv\", 2000)\n    gen_csv(\"dev.csv\", 500)\n    gen_csv(\"test.csv\", 500)\n\n\ndef load_csv_dataset(folder: pathlib.Path) -> Dict[str, List[Dict]]:\n    out = {}\n    for split in [\"train\", \"dev\", \"test\"]:\n        with open(folder / f\"{split}.csv\") as f:\n            rdr = csv.DictReader(f)\n            lst = [row for row in rdr]\n            for r in lst:\n                r[\"label\"] = int(r[\"label\"])\n            out[split] = lst\n    return out\n\n\nDATA_PATH = pathlib.Path(os.environ.get(\"SPR_PATH\", \"./SPR_BENCH\"))\nif not DATA_PATH.exists():\n    DATA_PATH.mkdir(parents=True, exist_ok=True)\n    generate_synthetic(DATA_PATH)\n\ndatasets = load_csv_dataset(DATA_PATH)\nprint({k: len(v) for k, v in datasets.items()})\n\n# ------------------------------------------------------------------\n# Vocab\nPAD, MASK = \"<PAD>\", \"<MASK>\"\n\n\ndef build_vocab(samples):\n    vocab = {PAD: 0, MASK: 1}\n    idx = 2\n    for s in samples:\n        for tok in s.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([r[\"sequence\"] for r in datasets[\"train\"]])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------------------------------------------------------\n# Dataset & collate\nclass SPRDataset(Dataset):\n    def __init__(self, rows, supervised=True):\n        self.rows = rows\n        self.sup = supervised\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        ids = encode(r[\"sequence\"])\n        item = {\"input\": ids, \"seq\": r[\"sequence\"]}\n        if self.sup:\n            item[\"label\"] = r[\"label\"]\n        return item\n\n\ndef collate(batch):\n    maxlen = max(len(b[\"input\"]) for b in batch)\n    pads = [b[\"input\"] + [0] * (maxlen - len(b[\"input\"])) for b in batch]\n    inp = torch.tensor(pads, dtype=torch.long)\n    out = {\"input\": inp, \"seq\": [b[\"seq\"] for b in batch]}\n    if \"label\" in batch[0]:\n        out[\"label\"] = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return out\n\n\n# ------------------------------------------------------------------\n# Augmentation for contrastive\ndef augment(ids: List[int]) -> List[int]:\n    new = []\n    for tok in ids:\n        r = random.random()\n        if r < 0.1:\n            continue\n        if r < 0.2:\n            new.append(1)  # MASK token id\n        else:\n            new.append(tok)\n    return new if new else ids\n\n\n# ------------------------------------------------------------------\n# Model components\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, dim=128, hid=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, dim, padding_idx=0)\n        self.gru = nn.GRU(dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        lens = (x != 0).sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lens, batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        return h[-1]\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim=128, out_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Linear(in_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, n_cls=2):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# ------------------------------------------------------------------\n# Losses / evaluation\ndef nt_xent(z, temp=0.1):\n    z = F.normalize(z, dim=1)\n    sim = torch.mm(z, z.t()) / temp\n    B = z.size(0) // 2\n    loss = 0.0\n    for i in range(B):\n        pos = sim[i, i + B]\n        denom = torch.cat([sim[i, :i], sim[i, i + 1 :]])\n        loss += -torch.log(torch.exp(pos) / torch.exp(denom).sum())\n        j = i + B\n        pos2 = sim[j, i]\n        denom2 = torch.cat([sim[j, :j], sim[j, j + 1 :]])\n        loss += -torch.log(torch.exp(pos2) / torch.exp(denom2).sum())\n    return loss / (2 * B)\n\n\ndef evaluate(enc, clf, loader):\n    enc.eval()\n    clf.eval()\n    ys, ps, seqs = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"input\"].to(device)\n            logits = clf(enc(x))\n            ps.extend(logits.argmax(1).cpu().tolist())\n            ys.extend(batch[\"label\"].tolist())\n            seqs.extend(batch[\"seq\"])\n    swa = shape_weighted_accuracy(seqs, ys, ps)\n    cwa = color_weighted_accuracy(seqs, ys, ps)\n    return swa, cwa, harmonic_sca(swa, cwa)\n\n\n# ------------------------------------------------------------------\n# DataLoaders\nbatch_size = 128\ncontrast_loader = DataLoader(\n    SPRDataset(datasets[\"train\"], supervised=False),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ntrain_loader = DataLoader(\n    SPRDataset(datasets[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate,\n)\ndev_loader = DataLoader(\n    SPRDataset(datasets[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\ntest_loader = DataLoader(\n    SPRDataset(datasets[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate,\n)\n\n# ------------------------------------------------------------------\n# Stage-1: Contrastive pre-training\nencoder = Encoder(vocab_size).to(device)\nproj = ProjectionHead().to(device)\noptim_enc = torch.optim.Adam(\n    list(encoder.parameters()) + list(proj.parameters()), lr=1e-3\n)\n\nepochs_pre = 3\nfor ep in range(1, epochs_pre + 1):\n    encoder.train()\n    proj.train()\n    tot = cnt = 0\n    for batch in contrast_loader:\n        ids = batch[\"input\"]\n        v1 = [augment(seq.tolist()) for seq in ids]\n        v2 = [augment(seq.tolist()) for seq in ids]\n\n        def pad(seqs):\n            ml = max(len(s) for s in seqs)\n            return torch.tensor(\n                [s + [0] * (ml - len(s)) for s in seqs], dtype=torch.long\n            )\n\n        z1 = proj(encoder(pad(v1).to(device)))\n        z2 = proj(encoder(pad(v2).to(device)))\n        loss = nt_xent(torch.cat([z1, z2], 0))\n        optim_enc.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"[Pre-train] epoch {ep}/{epochs_pre}  loss={tot/cnt:.4f}\")\n\npretrained_weights = copy.deepcopy(encoder.state_dict())  # save snapshot\n\n# ------------------------------------------------------------------\n# Stage-2 sweep: fine-tuning epochs grid\nepochs_grid = experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\n    \"epochs_grid\"\n]\ncriterion = nn.CrossEntropyLoss()\npatience = 3  # early stopping patience\n\nfor max_epochs in epochs_grid:\n    print(\n        f\"\\n=== Fine-tuning for up-to {max_epochs} epochs (early-stopping patience={patience}) ===\"\n    )\n    enc = Encoder(vocab_size).to(device)\n    enc.load_state_dict(pretrained_weights)\n    clf = Classifier(128, 2).to(device)\n    optim_all = torch.optim.Adam(\n        list(enc.parameters()) + list(clf.parameters()), lr=1e-3\n    )\n\n    train_hsca_history, val_hsca_history = [], []\n    train_loss_history, val_loss_dummy = [], []\n\n    best_hsca, best_state, no_improve = -1.0, None, 0\n\n    for epoch in range(1, max_epochs + 1):\n        enc.train()\n        clf.train()\n        t_loss, steps = 0, 0\n        for batch in train_loader:\n            x = batch[\"input\"].to(device)\n            y = batch[\"label\"].to(device)\n            logits = clf(enc(x))\n            loss = criterion(logits, y)\n            optim_all.zero_grad()\n            loss.backward()\n            optim_all.step()\n            t_loss += loss.item()\n            steps += 1\n\n        swa, cwa, hsca = evaluate(enc, clf, train_loader)\n        train_hsca_history.append(hsca)\n        train_loss_history.append(t_loss / steps)\n\n        val_swa, val_cwa, val_hsca = evaluate(enc, clf, dev_loader)\n        val_hsca_history.append(val_hsca)\n        val_loss_dummy.append(0.0)  # placeholder if you want val loss later\n\n        print(f\"  Epoch {epoch:02d}: train HSCA={hsca:.4f} | dev HSCA={val_hsca:.4f}\")\n\n        # early stopping\n        if val_hsca > best_hsca + 1e-6:\n            best_hsca = val_hsca\n            best_state = (\n                copy.deepcopy(enc.state_dict()),\n                copy.deepcopy(clf.state_dict()),\n            )\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(\"  Early stopping!\")\n                break\n\n    # restore best & evaluate on test\n    enc.load_state_dict(best_state[0])\n    clf.load_state_dict(best_state[1])\n    test_swa, test_cwa, test_hsca = evaluate(enc, clf, test_loader)\n    print(f\"  >>> TEST HSCA = {test_hsca:.4f}\")\n\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train\"\n    ].append(train_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val\"\n    ].append(val_hsca_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"train\"\n    ].append(train_loss_history)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"losses\"][\n        \"val\"\n    ].append(val_loss_dummy)\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"test_hsca\"].append(\n        test_hsca\n    )\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"predictions\"].append(\n        []\n    )  # placeholder\n    experiment_data[\"supervised_finetuning_epochs\"][\"SPR_BENCH\"][\"ground_truth\"].append(\n        []\n    )  # placeholder\n\n# ------------------------------------------------------------------\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '[Contrastive] Epoch 1: loss=3.1907', '\\n',\n'[Contrastive] Epoch 2: loss=2.3584', '\\n', '[Contrastive] Epoch 3:\nloss=1.8872', '\\n', 'Epoch 1: validation_loss = 0.6780 | HSCA=0.6396', '\\n',\n'Epoch 2: validation_loss = 0.6493 | HSCA=0.6450', '\\n', 'Epoch 3:\nvalidation_loss = 0.6286 | HSCA=0.6589', '\\n', 'Epoch 4: validation_loss =\n0.6004 | HSCA=0.6882', '\\n', 'Epoch 5: validation_loss = 0.5580 | HSCA=0.7071',\n'\\n', 'TEST -> SWA: 0.7061 | CWA: 0.6944 | HSCA: 0.7002', '\\n', 'Execution time:\n6 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '[Pre-train] epoch 1/3  loss=3.1832', '\\n', '[Pre-train]\nepoch 2/3  loss=2.2957', '\\n', '[Pre-train] epoch 3/3  loss=1.7953', '\\n',\n'\\n=== Fine-tuning for up-to 5 epochs (early-stopping patience=3) ===', '\\n', '\nEpoch 01: train HSCA=0.6312 | dev HSCA=0.6241', '\\n', '  Epoch 02: train\nHSCA=0.6570 | dev HSCA=0.6285', '\\n', '  Epoch 03: train HSCA=0.7058 | dev\nHSCA=0.6690', '\\n', '  Epoch 04: train HSCA=0.7593 | dev HSCA=0.7025', '\\n', '\nEpoch 05: train HSCA=0.8217 | dev HSCA=0.7567', '\\n', '  >>> TEST HSCA =\n0.7530', '\\n', '\\n=== Fine-tuning for up-to 10 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6295 | dev HSCA=0.6234', '\\n',\n'  Epoch 02: train HSCA=0.6809 | dev HSCA=0.6418', '\\n', '  Epoch 03: train\nHSCA=0.7204 | dev HSCA=0.6803', '\\n', '  Epoch 04: train HSCA=0.7711 | dev\nHSCA=0.7153', '\\n', '  Epoch 05: train HSCA=0.7876 | dev HSCA=0.7256', '\\n', '\nEpoch 06: train HSCA=0.8608 | dev HSCA=0.7892', '\\n', '  Epoch 07: train\nHSCA=0.8948 | dev HSCA=0.8174', '\\n', '  Epoch 08: train HSCA=0.9456 | dev\nHSCA=0.8862', '\\n', '  Epoch 09: train HSCA=0.9559 | dev HSCA=0.9177', '\\n', '\nEpoch 10: train HSCA=0.9588 | dev HSCA=0.9226', '\\n', '  >>> TEST HSCA =\n0.9419', '\\n', '\\n=== Fine-tuning for up-to 15 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6322 | dev HSCA=0.6309', '\\n',\n'  Epoch 02: train HSCA=0.6502 | dev HSCA=0.6492', '\\n', '  Epoch 03: train\nHSCA=0.7094 | dev HSCA=0.6631', '\\n', '  Epoch 04: train HSCA=0.7804 | dev\nHSCA=0.7141', '\\n', '  Epoch 05: train HSCA=0.8191 | dev HSCA=0.7408', '\\n', '\nEpoch 06: train HSCA=0.8574 | dev HSCA=0.7801', '\\n', '  Epoch 07: train\nHSCA=0.9005 | dev HSCA=0.8297', '\\n', '  Epoch 08: train HSCA=0.9235 | dev\nHSCA=0.8641', '\\n', '  Epoch 09: train HSCA=0.9496 | dev HSCA=0.8940', '\\n', '\nEpoch 10: train HSCA=0.9560 | dev HSCA=0.8985', '\\n', '  Epoch 11: train\nHSCA=0.9610 | dev HSCA=0.9035', '\\n', '  Epoch 12: train HSCA=0.9639 | dev\nHSCA=0.9071', '\\n', '  Epoch 13: train HSCA=0.9686 | dev HSCA=0.9177', '\\n', '\nEpoch 14: train HSCA=0.9759 | dev HSCA=0.9301', '\\n', '  Epoch 15: train\nHSCA=0.9784 | dev HSCA=0.9229', '\\n', '  >>> TEST HSCA = 0.9488', '\\n', '\\n===\nFine-tuning for up-to 20 epochs (early-stopping patience=3) ===', '\\n', '  Epoch\n01: train HSCA=0.6278 | dev HSCA=0.6192', '\\n', '  Epoch 02: train HSCA=0.6792 |\ndev HSCA=0.6614', '\\n', '  Epoch 03: train HSCA=0.7125 | dev HSCA=0.6674', '\\n',\n'  Epoch 04: train HSCA=0.7129 | dev HSCA=0.6710', '\\n', '  Epoch 05: train\nHSCA=0.8153 | dev HSCA=0.7418', '\\n', '  Epoch 06: train HSCA=0.8476 | dev\nHSCA=0.7891', '\\n', '  Epoch 07: train HSCA=0.8917 | dev HSCA=0.8381', '\\n', '\nEpoch 08: train HSCA=0.9306 | dev HSCA=0.8730', '\\n', '  Epoch 09: train\nHSCA=0.9520 | dev HSCA=0.9126', '\\n', '  Epoch 10: train HSCA=0.9568 | dev\nHSCA=0.9219', '\\n', '  Epoch 11: train HSCA=0.9580 | dev HSCA=0.9129', '\\n', '\nEpoch 12: train HSCA=0.9585 | dev HSCA=0.9142', '\\n', '  Epoch 13: train\nHSCA=0.9648 | dev HSCA=0.9265', '\\n', '  Epoch 14: train HSCA=0.9690 | dev\nHSCA=0.9234', '\\n', '  Epoch 15: train HSCA=0.9780 | dev HSCA=0.9261', '\\n', '\nEpoch 16: train HSCA=0.9850 | dev HSCA=0.9323', '\\n', '  Epoch 17: train\nHSCA=0.9898 | dev HSCA=0.9412', '\\n', '  Epoch 18: train HSCA=0.9927 | dev\nHSCA=0.9445', '\\n', '  Epoch 19: train HSCA=0.9956 | dev HSCA=0.9510', '\\n', '\nEpoch 20: train HSCA=0.9962 | dev HSCA=0.9630', '\\n', '  >>> TEST HSCA =\n0.9709', '\\n', '\\nSaved experiment_data.npy', '\\n', 'Execution time: 30 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '\\n=== Running combo: pre_lr=3e-04 | sup_lr=3e-04 ===',\n'\\n', '  [Pre] lr=3e-04 ep=1 loss=3.6004', '\\n', '  [Pre] lr=3e-04 ep=2\nloss=2.9549', '\\n', '  [Pre] lr=3e-04 ep=3 loss=2.5472', '\\n', '  [Sup] lr=3e-04\nep=1 train_loss=0.6871 dev_HSCA=0.6360', '\\n', '  [Sup] lr=3e-04 ep=2\ntrain_loss=0.6762 dev_HSCA=0.6376', '\\n', '  [Sup] lr=3e-04 ep=3\ntrain_loss=0.6720 dev_HSCA=0.6368', '\\n', '  [Sup] lr=3e-04 ep=4\ntrain_loss=0.6681 dev_HSCA=0.6441', '\\n', '  [Sup] lr=3e-04 ep=5\ntrain_loss=0.6624 dev_HSCA=0.6427', '\\n', '  DEV  HSCA=0.6427 | TEST\nHSCA=0.6513', '\\n', '\\n=== Running combo: pre_lr=3e-04 | sup_lr=5e-04 ===',\n'\\n', '  [Pre] lr=3e-04 ep=1 loss=3.6601', '\\n', '  [Pre] lr=3e-04 ep=2\nloss=3.1323', '\\n', '  [Pre] lr=3e-04 ep=3 loss=2.6513', '\\n', '  [Sup] lr=5e-04\nep=1 train_loss=0.6910 dev_HSCA=0.6344', '\\n', '  [Sup] lr=5e-04 ep=2\ntrain_loss=0.6755 dev_HSCA=0.6371', '\\n', '  [Sup] lr=5e-04 ep=3\ntrain_loss=0.6677 dev_HSCA=0.6468', '\\n', '  [Sup] lr=5e-04 ep=4\ntrain_loss=0.6584 dev_HSCA=0.6350', '\\n', '  [Sup] lr=5e-04 ep=5\ntrain_loss=0.6427 dev_HSCA=0.6657', '\\n', '  DEV  HSCA=0.6657 | TEST\nHSCA=0.6785', '\\n', '\\n=== Running combo: pre_lr=3e-04 | sup_lr=1e-03 ===',\n'\\n', '  [Pre] lr=3e-04 ep=1 loss=3.6213', '\\n', '  [Pre] lr=3e-04 ep=2\nloss=3.0641', '\\n', '  [Pre] lr=3e-04 ep=3 loss=2.5493', '\\n', '  [Sup] lr=1e-03\nep=1 train_loss=0.6942 dev_HSCA=0.6149', '\\n', '  [Sup] lr=1e-03 ep=2\ntrain_loss=0.6736 dev_HSCA=0.6295', '\\n', '  [Sup] lr=1e-03 ep=3\ntrain_loss=0.6605 dev_HSCA=0.6528', '\\n', '  [Sup] lr=1e-03 ep=4\ntrain_loss=0.6371 dev_HSCA=0.6680', '\\n', '  [Sup] lr=1e-03 ep=5\ntrain_loss=0.5995 dev_HSCA=0.6654', '\\n', '  DEV  HSCA=0.6654 | TEST\nHSCA=0.6700', '\\n', '\\n=== Running combo: pre_lr=3e-04 | sup_lr=2e-03 ===',\n'\\n', '  [Pre] lr=3e-04 ep=1 loss=3.5376', '\\n', '  [Pre] lr=3e-04 ep=2\nloss=2.9908', '\\n', '  [Pre] lr=3e-04 ep=3 loss=2.5802', '\\n', '  [Sup] lr=2e-03\nep=1 train_loss=0.7007 dev_HSCA=0.6473', '\\n', '  [Sup] lr=2e-03 ep=2\ntrain_loss=0.6678 dev_HSCA=0.6331', '\\n', '  [Sup] lr=2e-03 ep=3\ntrain_loss=0.6479 dev_HSCA=0.6424', '\\n', '  [Sup] lr=2e-03 ep=4\ntrain_loss=0.5850 dev_HSCA=0.6786', '\\n', '  [Sup] lr=2e-03 ep=5\ntrain_loss=0.4578 dev_HSCA=0.8666', '\\n', '  DEV  HSCA=0.8666 | TEST\nHSCA=0.8809', '\\n', '\\n=== Running combo: pre_lr=5e-04 | sup_lr=3e-04 ===',\n'\\n', '  [Pre] lr=5e-04 ep=1 loss=3.3983', '\\n', '  [Pre] lr=5e-04 ep=2\nloss=2.6939', '\\n', '  [Pre] lr=5e-04 ep=3 loss=2.2405', '\\n', '  [Sup] lr=3e-04\nep=1 train_loss=0.6849 dev_HSCA=0.6360', '\\n', '  [Sup] lr=3e-04 ep=2\ntrain_loss=0.6750 dev_HSCA=0.6363', '\\n', '  [Sup] lr=3e-04 ep=3\ntrain_loss=0.6691 dev_HSCA=0.6392', '\\n', '  [Sup] lr=3e-04 ep=4\ntrain_loss=0.6605 dev_HSCA=0.6394', '\\n', '  [Sup] lr=3e-04 ep=5\ntrain_loss=0.6540 dev_HSCA=0.6310', '\\n', '  DEV  HSCA=0.6310 | TEST\nHSCA=0.6537', '\\n', '\\n=== Running combo: pre_lr=5e-04 | sup_lr=5e-04 ===',\n'\\n', '  [Pre] lr=5e-04 ep=1 loss=3.4423', '\\n', '  [Pre] lr=5e-04 ep=2\nloss=2.7375', '\\n', '  [Pre] lr=5e-04 ep=3 loss=2.2442', '\\n', '  [Sup] lr=5e-04\nep=1 train_loss=0.6888 dev_HSCA=0.6393', '\\n', '  [Sup] lr=5e-04 ep=2\ntrain_loss=0.6747 dev_HSCA=0.6491', '\\n', '  [Sup] lr=5e-04 ep=3\ntrain_loss=0.6650 dev_HSCA=0.6363', '\\n', '  [Sup] lr=5e-04 ep=4\ntrain_loss=0.6533 dev_HSCA=0.6566', '\\n', '  [Sup] lr=5e-04 ep=5\ntrain_loss=0.6370 dev_HSCA=0.6582', '\\n', '  DEV  HSCA=0.6582 | TEST\nHSCA=0.6720', '\\n', '\\n=== Running combo: pre_lr=5e-04 | sup_lr=1e-03 ===',\n'\\n', '  [Pre] lr=5e-04 ep=1 loss=3.5202', '\\n', '  [Pre] lr=5e-04 ep=2\nloss=2.7355', '\\n', '  [Pre] lr=5e-04 ep=3 loss=2.3452', '\\n', '  [Sup] lr=1e-03\nep=1 train_loss=0.6885 dev_HSCA=0.6310', '\\n', '  [Sup] lr=1e-03 ep=2\ntrain_loss=0.6658 dev_HSCA=0.6718', '\\n', '  [Sup] lr=1e-03 ep=3\ntrain_loss=0.6457 dev_HSCA=0.6764', '\\n', '  [Sup] lr=1e-03 ep=4\ntrain_loss=0.6127 dev_HSCA=0.7091', '\\n', '  [Sup] lr=1e-03 ep=5\ntrain_loss=0.5573 dev_HSCA=0.7002', '\\n', '  DEV  HSCA=0.7002 | TEST\nHSCA=0.7148', '\\n', '\\n=== Running combo: pre_lr=5e-04 | sup_lr=2e-03 ===',\n'\\n', '  [Pre] lr=5e-04 ep=1 loss=3.3597', '\\n', '  [Pre] lr=5e-04 ep=2\nloss=2.7377', '\\n', '  [Pre] lr=5e-04 ep=3 loss=2.1541', '\\n', '  [Sup] lr=2e-03\nep=1 train_loss=0.6917 dev_HSCA=0.6173', '\\n', '  [Sup] lr=2e-03 ep=2\ntrain_loss=0.6618 dev_HSCA=0.6719', '\\n', '  [Sup] lr=2e-03 ep=3\ntrain_loss=0.6148 dev_HSCA=0.7159', '\\n', '  [Sup] lr=2e-03 ep=4\ntrain_loss=0.5095 dev_HSCA=0.7873', '\\n', '  [Sup] lr=2e-03 ep=5\ntrain_loss=0.3833 dev_HSCA=0.9027', '\\n', '  DEV  HSCA=0.9027 | TEST\nHSCA=0.9017', '\\n', '\\n=== Running combo: pre_lr=1e-03 | sup_lr=3e-04 ===',\n'\\n', '  [Pre] lr=1e-03 ep=1 loss=3.3543', '\\n', '  [Pre] lr=1e-03 ep=2\nloss=2.4738', '\\n', '  [Pre] lr=1e-03 ep=3 loss=1.9882', '\\n', '  [Sup] lr=3e-04\nep=1 train_loss=0.6868 dev_HSCA=0.6398', '\\n', '  [Sup] lr=3e-04 ep=2\ntrain_loss=0.6698 dev_HSCA=0.6461', '\\n', '  [Sup] lr=3e-04 ep=3\ntrain_loss=0.6629 dev_HSCA=0.6431', '\\n', '  [Sup] lr=3e-04 ep=4\ntrain_loss=0.6547 dev_HSCA=0.6380', '\\n', '  [Sup] lr=3e-04 ep=5\ntrain_loss=0.6444 dev_HSCA=0.6336', '\\n', '  DEV  HSCA=0.6336 | TEST\nHSCA=0.6701', '\\n', '\\n=== Running combo: pre_lr=1e-03 | sup_lr=5e-04 ===',\n'\\n', '  [Pre] lr=1e-03 ep=1 loss=3.2572', '\\n', '  [Pre] lr=1e-03 ep=2\nloss=2.3454', '\\n', '  [Pre] lr=1e-03 ep=3 loss=1.8656', '\\n', '  [Sup] lr=5e-04\nep=1 train_loss=0.6847 dev_HSCA=0.6229', '\\n', '  [Sup] lr=5e-04 ep=2\ntrain_loss=0.6667 dev_HSCA=0.6418', '\\n', '  [Sup] lr=5e-04 ep=3\ntrain_loss=0.6549 dev_HSCA=0.6387', '\\n', '  [Sup] lr=5e-04 ep=4\ntrain_loss=0.6389 dev_HSCA=0.6332', '\\n', '  [Sup] lr=5e-04 ep=5\ntrain_loss=0.6168 dev_HSCA=0.7001', '\\n', '  DEV  HSCA=0.7001 | TEST\nHSCA=0.7044', '\\n', '\\n=== Running combo: pre_lr=1e-03 | sup_lr=1e-03 ===',\n'\\n', '  [Pre] lr=1e-03 ep=1 loss=3.1126', '\\n', '  [Pre] lr=1e-03 ep=2\nloss=2.3921', '\\n', '  [Pre] lr=1e-03 ep=3 loss=1.9492', '\\n', '  [Sup] lr=1e-03\nep=1 train_loss=0.6862 dev_HSCA=0.6316', '\\n', '  [Sup] lr=1e-03 ep=2\ntrain_loss=0.6552 dev_HSCA=0.6288', '\\n', '  [Sup] lr=1e-03 ep=3\ntrain_loss=0.6276 dev_HSCA=0.6883', '\\n', '  [Sup] lr=1e-03 ep=4\ntrain_loss=0.5724 dev_HSCA=0.7134', '\\n', '  [Sup] lr=1e-03 ep=5\ntrain_loss=0.4927 dev_HSCA=0.8010', '\\n', '  DEV  HSCA=0.8010 | TEST\nHSCA=0.8194', '\\n', '\\n=== Running combo: pre_lr=1e-03 | sup_lr=2e-03 ===',\n'\\n', '  [Pre] lr=1e-03 ep=1 loss=3.2179', '\\n', '  [Pre] lr=1e-03 ep=2\nloss=2.4080', '\\n', '  [Pre] lr=1e-03 ep=3 loss=2.0114', '\\n', '  [Sup] lr=2e-03\nep=1 train_loss=0.6868 dev_HSCA=0.6564', '\\n', '  [Sup] lr=2e-03 ep=2\ntrain_loss=0.6443 dev_HSCA=0.6786', '\\n', '  [Sup] lr=2e-03 ep=3\ntrain_loss=0.5814 dev_HSCA=0.7539', '\\n', '  [Sup] lr=2e-03 ep=4\ntrain_loss=0.4709 dev_HSCA=0.8428', '\\n', '  [Sup] lr=2e-03 ep=5\ntrain_loss=0.3680 dev_HSCA=0.9249', '\\n', '  DEV  HSCA=0.9249 | TEST\nHSCA=0.9278', '\\n', '\\n=== Running combo: pre_lr=2e-03 | sup_lr=3e-04 ===',\n'\\n', '  [Pre] lr=2e-03 ep=1 loss=3.0102', '\\n', '  [Pre] lr=2e-03 ep=2\nloss=2.0946', '\\n', '  [Pre] lr=2e-03 ep=3 loss=1.7051', '\\n', '  [Sup] lr=3e-04\nep=1 train_loss=0.6810 dev_HSCA=0.6358', '\\n', '  [Sup] lr=3e-04 ep=2\ntrain_loss=0.6695 dev_HSCA=0.6340', '\\n', '  [Sup] lr=3e-04 ep=3\ntrain_loss=0.6607 dev_HSCA=0.6355', '\\n', '  [Sup] lr=3e-04 ep=4\ntrain_loss=0.6514 dev_HSCA=0.6281', '\\n', '  [Sup] lr=3e-04 ep=5\ntrain_loss=0.6427 dev_HSCA=0.6326', '\\n', '  DEV  HSCA=0.6326 | TEST\nHSCA=0.6579', '\\n', '\\n=== Running combo: pre_lr=2e-03 | sup_lr=5e-04 ===',\n'\\n', '  [Pre] lr=2e-03 ep=1 loss=2.9761', '\\n', '  [Pre] lr=2e-03 ep=2\nloss=2.0969', '\\n', '  [Pre] lr=2e-03 ep=3 loss=1.6819', '\\n', '  [Sup] lr=5e-04\nep=1 train_loss=0.6787 dev_HSCA=0.6360', '\\n', '  [Sup] lr=5e-04 ep=2\ntrain_loss=0.6613 dev_HSCA=0.6582', '\\n', '  [Sup] lr=5e-04 ep=3\ntrain_loss=0.6470 dev_HSCA=0.6608', '\\n', '  [Sup] lr=5e-04 ep=4\ntrain_loss=0.6329 dev_HSCA=0.6903', '\\n', '  [Sup] lr=5e-04 ep=5\ntrain_loss=0.6099 dev_HSCA=0.7267', '\\n', '  DEV  HSCA=0.7267 | TEST\nHSCA=0.7132', '\\n', '\\n=== Running combo: pre_lr=2e-03 | sup_lr=1e-03 ===',\n'\\n', '  [Pre] lr=2e-03 ep=1 loss=2.9940', '\\n', '  [Pre] lr=2e-03 ep=2\nloss=2.1045', '\\n', '  [Pre] lr=2e-03 ep=3 loss=1.6775', '\\n', '  [Sup] lr=1e-03\nep=1 train_loss=0.7071 dev_HSCA=0.6449', '\\n', '  [Sup] lr=1e-03 ep=2\ntrain_loss=0.6654 dev_HSCA=0.6339', '\\n', '  [Sup] lr=1e-03 ep=3\ntrain_loss=0.6406 dev_HSCA=0.6834', '\\n', '  [Sup] lr=1e-03 ep=4\ntrain_loss=0.6091 dev_HSCA=0.7147', '\\n', '  [Sup] lr=1e-03 ep=5\ntrain_loss=0.5557 dev_HSCA=0.7286', '\\n', '  DEV  HSCA=0.7286 | TEST\nHSCA=0.7656', '\\n', '\\n=== Running combo: pre_lr=2e-03 | sup_lr=2e-03 ===',\n'\\n', '  [Pre] lr=2e-03 ep=1 loss=3.1207', '\\n', '  [Pre] lr=2e-03 ep=2\nloss=2.1620', '\\n', '  [Pre] lr=2e-03 ep=3 loss=1.7242', '\\n', '  [Sup] lr=2e-03\nep=1 train_loss=0.6840 dev_HSCA=0.6435', '\\n', '  [Sup] lr=2e-03 ep=2\ntrain_loss=0.6366 dev_HSCA=0.6751', '\\n', '  [Sup] lr=2e-03 ep=3\ntrain_loss=0.5607 dev_HSCA=0.7764', '\\n', '  [Sup] lr=2e-03 ep=4\ntrain_loss=0.4484 dev_HSCA=0.8564', '\\n', '  [Sup] lr=2e-03 ep=5\ntrain_loss=0.3339 dev_HSCA=0.9313', '\\n', '  DEV  HSCA=0.9313 | TEST\nHSCA=0.9351', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scienti\nst-v2/experiments/2025-08-16_00-46-\n17_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size:', ' ', '14', '\\n', '\\n=== Running experiment: batch_size=64 ===',\n'\\n', '[Contrastive] epoch 1 loss=2.3181', '\\n', '[Contrastive] epoch 2\nloss=1.5918', '\\n', '[Contrastive] epoch 3 loss=1.2146', '\\n', '[Supervised]\nepoch 1 loss=0.6713 | dev HSCA=0.5974', '\\n', '[Supervised] epoch 2 loss=0.6399\n| dev HSCA=0.6510', '\\n', '[Supervised] epoch 3 loss=0.5780 | dev HSCA=0.7090',\n'\\n', '[Supervised] epoch 4 loss=0.4794 | dev HSCA=0.8282', '\\n', '[Supervised]\nepoch 5 loss=0.3730 | dev HSCA=0.9104', '\\n', 'TEST  -> SWA 0.9486 | CWA 0.9288\n| HSCA 0.9386', '\\n', '\\n=== Running experiment: batch_size=128 ===', '\\n',\n'[Contrastive] epoch 1 loss=3.1599', '\\n', '[Contrastive] epoch 2 loss=2.3523',\n'\\n', '[Contrastive] epoch 3 loss=1.9397', '\\n', '[Supervised] epoch 1\nloss=0.6868 | dev HSCA=0.6036', '\\n', '[Supervised] epoch 2 loss=0.6531 | dev\nHSCA=0.6132', '\\n', '[Supervised] epoch 3 loss=0.6278 | dev HSCA=0.6569', '\\n',\n'[Supervised] epoch 4 loss=0.5852 | dev HSCA=0.7146', '\\n', '[Supervised] epoch\n5 loss=0.5105 | dev HSCA=0.7506', '\\n', 'TEST  -> SWA 0.7685 | CWA 0.7253 | HSCA\n0.7462', '\\n', '\\n=== Running experiment: batch_size=256 ===', '\\n',\n'[Contrastive] epoch 1 loss=4.1925', '\\n', '[Contrastive] epoch 2 loss=3.4473',\n'\\n', '[Contrastive] epoch 3 loss=2.8874', '\\n', '[Supervised] epoch 1\nloss=0.6857 | dev HSCA=0.5847', '\\n', '[Supervised] epoch 2 loss=0.6644 | dev\nHSCA=0.5777', '\\n', '[Supervised] epoch 3 loss=0.6533 | dev HSCA=0.5870', '\\n',\n'[Supervised] epoch 4 loss=0.6393 | dev HSCA=0.6153', '\\n', '[Supervised] epoch\n5 loss=0.6189 | dev HSCA=0.6255', '\\n', 'TEST  -> SWA 0.6628 | CWA 0.6249 | HSCA\n0.6433', '\\n', '\\n=== Running experiment: batch_size=512 ===', '\\n',\n'[Contrastive] epoch 1 loss=5.2161', '\\n', '[Contrastive] epoch 2 loss=4.3889',\n'\\n', '[Contrastive] epoch 3 loss=4.0687', '\\n', '[Supervised] epoch 1\nloss=0.6790 | dev HSCA=0.6245', '\\n', '[Supervised] epoch 2 loss=0.6669 | dev\nHSCA=0.5464', '\\n', '[Supervised] epoch 3 loss=0.6591 | dev HSCA=0.6070', '\\n',\n'[Supervised] epoch 4 loss=0.6494 | dev HSCA=0.6010', '\\n', '[Supervised] epoch\n5 loss=0.6398 | dev HSCA=0.6074', '\\n', 'TEST  -> SWA 0.6455 | CWA 0.5986 | HSCA\n0.6212', '\\n', '\\nSaved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-16_00-46-\n17_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 23 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'\\n=== Running setting: contrastive_pretraining_epochs=8 ===', '\\n',\n'[Contrastive] epoch 1/8 loss=3.1585', '\\n', '[Contrastive] epoch 2/8\nloss=2.2787', '\\n', '[Contrastive] epoch 3/8 loss=1.8909', '\\n', '[Contrastive]\nepoch 4/8 loss=1.5401', '\\n', '[Contrastive] epoch 5/8 loss=1.5284', '\\n',\n'[Contrastive] epoch 6/8 loss=1.4434', '\\n', '[Contrastive] epoch 7/8\nloss=1.3666', '\\n', '[Contrastive] epoch 8/8 loss=1.3306', '\\n', '[Sup] epoch\n1/5 loss=0.6761 Dev-HSCA=0.6431', '\\n', '[Sup] epoch 2/5 loss=0.6477 Dev-\nHSCA=0.6517', '\\n', '[Sup] epoch 3/5 loss=0.6205 Dev-HSCA=0.6822', '\\n', '[Sup]\nepoch 4/5 loss=0.5876 Dev-HSCA=0.7422', '\\n', '[Sup] epoch 5/5 loss=0.5234 Dev-\nHSCA=0.7288', '\\n', 'TEST HSCA=0.7402 (SWA=0.7702, CWA=0.7124)', '\\n', '\\n===\nRunning setting: contrastive_pretraining_epochs=11 ===', '\\n', '[Contrastive]\nepoch 1/11 loss=3.1964', '\\n', '[Contrastive] epoch 2/11 loss=2.3155', '\\n',\n'[Contrastive] epoch 3/11 loss=1.8605', '\\n', '[Contrastive] epoch 4/11\nloss=1.7089', '\\n', '[Contrastive] epoch 5/11 loss=1.5313', '\\n', '[Contrastive]\nepoch 6/11 loss=1.4470', '\\n', '[Contrastive] epoch 7/11 loss=1.4107', '\\n',\n'[Contrastive] epoch 8/11 loss=1.3524', '\\n', '[Contrastive] epoch 9/11\nloss=1.3791', '\\n', '[Contrastive] epoch 10/11 loss=1.2480', '\\n',\n'[Contrastive] epoch 11/11 loss=1.2716', '\\n', '[Sup] epoch 1/5 loss=0.6774 Dev-\nHSCA=0.6497', '\\n', '[Sup] epoch 2/5 loss=0.6518 Dev-HSCA=0.6427', '\\n', '[Sup]\nepoch 3/5 loss=0.6215 Dev-HSCA=0.6637', '\\n', '[Sup] epoch 4/5 loss=0.5806 Dev-\nHSCA=0.7403', '\\n', '[Sup] epoch 5/5 loss=0.5181 Dev-HSCA=0.7676', '\\n', 'TEST\nHSCA=0.7689 (SWA=0.7948, CWA=0.7446)', '\\n', '\\n=== Running setting:\ncontrastive_pretraining_epochs=15 ===', '\\n', '[Contrastive] epoch 1/15\nloss=3.3166', '\\n', '[Contrastive] epoch 2/15 loss=2.4607', '\\n', '[Contrastive]\nepoch 3/15 loss=1.9934', '\\n', '[Contrastive] epoch 4/15 loss=1.6976', '\\n',\n'[Contrastive] epoch 5/15 loss=1.4896', '\\n', '[Contrastive] epoch 6/15\nloss=1.4464', '\\n', '[Contrastive] epoch 7/15 loss=1.4308', '\\n', '[Contrastive]\nepoch 8/15 loss=1.3468', '\\n', '[Contrastive] epoch 9/15 loss=1.2992', '\\n',\n'[Contrastive] epoch 10/15 loss=1.3059', '\\n', '[Contrastive] epoch 11/15\nloss=1.2631', '\\n', '[Contrastive] epoch 12/15 loss=1.2578', '\\n',\n'[Contrastive] epoch 13/15 loss=1.3137', '\\n', '[Contrastive] epoch 14/15\nloss=1.2035', '\\n', '[Contrastive] epoch 15/15 loss=1.1993', '\\n', '[Sup] epoch\n1/5 loss=0.6743 Dev-HSCA=0.6303', '\\n', '[Sup] epoch 2/5 loss=0.6454 Dev-\nHSCA=0.6604', '\\n', '[Sup] epoch 3/5 loss=0.6229 Dev-HSCA=0.6779', '\\n', '[Sup]\nepoch 4/5 loss=0.5818 Dev-HSCA=0.6901', '\\n', '[Sup] epoch 5/5 loss=0.5310 Dev-\nHSCA=0.7821', '\\n', 'TEST HSCA=0.8087 (SWA=0.8287, CWA=0.7897)', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: a minute seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '\\n=== Running experiment with encoder hidden dim = 64\n===', '\\n', '[Contrastive] Epoch 1: loss=3.4262', '\\n', '[Contrastive] Epoch 2:\nloss=2.7433', '\\n', '[Contrastive] Epoch 3: loss=2.2371', '\\n', '[FineTune]\nEpoch 1: loss=0.6819 | HSCA=0.6253', '\\n', '[FineTune] Epoch 2: loss=0.6614 |\nHSCA=0.6217', '\\n', '[FineTune] Epoch 3: loss=0.6462 | HSCA=0.6265', '\\n',\n'[FineTune] Epoch 4: loss=0.6307 | HSCA=0.6165', '\\n', '[FineTune] Epoch 5:\nloss=0.6049 | HSCA=0.6493', '\\n', '[TEST] SWA=0.6612 | CWA=0.6106 |\nHSCA=0.6349', '\\n', '\\n=== Running experiment with encoder hidden dim = 128\n===', '\\n', '[Contrastive] Epoch 1: loss=3.2290', '\\n', '[Contrastive] Epoch 2:\nloss=2.3636', '\\n', '[Contrastive] Epoch 3: loss=1.8660', '\\n', '[FineTune]\nEpoch 1: loss=0.6734 | HSCA=0.6217', '\\n', '[FineTune] Epoch 2: loss=0.6433 |\nHSCA=0.6534', '\\n', '[FineTune] Epoch 3: loss=0.6129 | HSCA=0.6717', '\\n',\n'[FineTune] Epoch 4: loss=0.5787 | HSCA=0.7026', '\\n', '[FineTune] Epoch 5:\nloss=0.5240 | HSCA=0.7268', '\\n', '[TEST] SWA=0.7414 | CWA=0.6890 |\nHSCA=0.7143', '\\n', '\\n=== Running experiment with encoder hidden dim = 256\n===', '\\n', '[Contrastive] Epoch 1: loss=2.8538', '\\n', '[Contrastive] Epoch 2:\nloss=2.0203', '\\n', '[Contrastive] Epoch 3: loss=1.7022', '\\n', '[FineTune]\nEpoch 1: loss=0.6894 | HSCA=0.6237', '\\n', '[FineTune] Epoch 2: loss=0.6376 |\nHSCA=0.6402', '\\n', '[FineTune] Epoch 3: loss=0.5778 | HSCA=0.7176', '\\n',\n'[FineTune] Epoch 4: loss=0.5054 | HSCA=0.7800', '\\n', '[FineTune] Epoch 5:\nloss=0.4023 | HSCA=0.8445', '\\n', '[TEST] SWA=0.8664 | CWA=0.8345 |\nHSCA=0.8501', '\\n', '\\n=== Running experiment with encoder hidden dim = 512\n===', '\\n', '[Contrastive] Epoch 1: loss=2.8813', '\\n', '[Contrastive] Epoch 2:\nloss=1.9482', '\\n', '[Contrastive] Epoch 3: loss=1.6426', '\\n', '[FineTune]\nEpoch 1: loss=0.6772 | HSCA=0.6427', '\\n', '[FineTune] Epoch 2: loss=0.6049 |\nHSCA=0.6938', '\\n', '[FineTune] Epoch 3: loss=0.4982 | HSCA=0.7919', '\\n',\n'[FineTune] Epoch 4: loss=0.3433 | HSCA=0.9067', '\\n', '[FineTune] Epoch 5:\nloss=0.2309 | HSCA=0.9129', '\\n', '[TEST] SWA=0.9524 | CWA=0.9265 |\nHSCA=0.9393', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scienti\nst-v2/experiments/2025-08-16_00-46-\n17_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 27 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '[\u03c4=0.05] Contrastive Epoch 1: loss=3.1109', '\\n',\n'[\u03c4=0.05] Contrastive Epoch 2: loss=2.3000', '\\n', '[\u03c4=0.05] Contrastive Epoch\n3: loss=1.6492', '\\n', '[\u03c4=0.05] Supervised Epoch 1: train_loss=0.6798 | Dev\nHSCA=0.5496', '\\n', '[\u03c4=0.05] Supervised Epoch 2: train_loss=0.6520 | Dev\nHSCA=0.6254', '\\n', '[\u03c4=0.05] Supervised Epoch 3: train_loss=0.6290 | Dev\nHSCA=0.6552', '\\n', '[\u03c4=0.05] Supervised Epoch 4: train_loss=0.5857 | Dev\nHSCA=0.7159', '\\n', '[\u03c4=0.05] Supervised Epoch 5: train_loss=0.5100 | Dev\nHSCA=0.7733', '\\n', '[\u03c4=0.05] TEST -> SWA: 0.7979 | CWA: 0.7715 | HSCA: 0.7845',\n'\\n', '[\u03c4=0.08] Contrastive Epoch 1: loss=3.1290', '\\n', '[\u03c4=0.08] Contrastive\nEpoch 2: loss=2.3150', '\\n', '[\u03c4=0.08] Contrastive Epoch 3: loss=1.7156', '\\n',\n'[\u03c4=0.08] Supervised Epoch 1: train_loss=0.6806 | Dev HSCA=0.5658', '\\n',\n'[\u03c4=0.08] Supervised Epoch 2: train_loss=0.6532 | Dev HSCA=0.6214', '\\n',\n'[\u03c4=0.08] Supervised Epoch 3: train_loss=0.6313 | Dev HSCA=0.6405', '\\n',\n'[\u03c4=0.08] Supervised Epoch 4: train_loss=0.5885 | Dev HSCA=0.7144', '\\n',\n'[\u03c4=0.08] Supervised Epoch 5: train_loss=0.5146 | Dev HSCA=0.7787', '\\n',\n'[\u03c4=0.08] TEST -> SWA: 0.7910 | CWA: 0.7630 | HSCA: 0.7767', '\\n', '[\u03c4=0.1]\nContrastive Epoch 1: loss=3.1808', '\\n', '[\u03c4=0.1] Contrastive Epoch 2:\nloss=2.3834', '\\n', '[\u03c4=0.1] Contrastive Epoch 3: loss=1.8599', '\\n', '[\u03c4=0.1]\nSupervised Epoch 1: train_loss=0.6800 | Dev HSCA=0.5619', '\\n', '[\u03c4=0.1]\nSupervised Epoch 2: train_loss=0.6525 | Dev HSCA=0.6108', '\\n', '[\u03c4=0.1]\nSupervised Epoch 3: train_loss=0.6301 | Dev HSCA=0.6319', '\\n', '[\u03c4=0.1]\nSupervised Epoch 4: train_loss=0.5886 | Dev HSCA=0.7070', '\\n', '[\u03c4=0.1]\nSupervised Epoch 5: train_loss=0.5162 | Dev HSCA=0.7789', '\\n', '[\u03c4=0.1] TEST ->\nSWA: 0.7898 | CWA: 0.7609 | HSCA: 0.7751', '\\n', '[\u03c4=0.15] Contrastive Epoch 1:\nloss=3.3951', '\\n', '[\u03c4=0.15] Contrastive Epoch 2: loss=2.7484', '\\n', '[\u03c4=0.15]\nContrastive Epoch 3: loss=2.4043', '\\n', '[\u03c4=0.15] Supervised Epoch 1:\ntrain_loss=0.6802 | Dev HSCA=0.5630', '\\n', '[\u03c4=0.15] Supervised Epoch 2:\ntrain_loss=0.6528 | Dev HSCA=0.6176', '\\n', '[\u03c4=0.15] Supervised Epoch 3:\ntrain_loss=0.6306 | Dev HSCA=0.6312', '\\n', '[\u03c4=0.15] Supervised Epoch 4:\ntrain_loss=0.5877 | Dev HSCA=0.7131', '\\n', '[\u03c4=0.15] Supervised Epoch 5:\ntrain_loss=0.5118 | Dev HSCA=0.7809', '\\n', '[\u03c4=0.15] TEST -> SWA: 0.7887 | CWA:\n0.7566 | HSCA: 0.7723', '\\n', '[\u03c4=0.2] Contrastive Epoch 1: loss=3.6393', '\\n',\n'[\u03c4=0.2] Contrastive Epoch 2: loss=3.1386', '\\n', '[\u03c4=0.2] Contrastive Epoch 3:\nloss=2.9117', '\\n', '[\u03c4=0.2] Supervised Epoch 1: train_loss=0.6800 | Dev\nHSCA=0.5624', '\\n', '[\u03c4=0.2] Supervised Epoch 2: train_loss=0.6522 | Dev\nHSCA=0.6241', '\\n', '[\u03c4=0.2] Supervised Epoch 3: train_loss=0.6294 | Dev\nHSCA=0.6202', '\\n', '[\u03c4=0.2] Supervised Epoch 4: train_loss=0.5864 | Dev\nHSCA=0.7132', '\\n', '[\u03c4=0.2] Supervised Epoch 5: train_loss=0.5125 | Dev\nHSCA=0.7834', '\\n', '[\u03c4=0.2] TEST -> SWA: 0.8054 | CWA: 0.7715 | HSCA: 0.7881',\n'\\n', 'Best \u03c4 = 0.2 | Test HSCA = 0.7881', '\\n', 'Saved experiment data to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-16_00-46-\n17_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 24 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '\\n========== Weight Decay = 0.0 ==========', '\\n',\n'[Contrastive] Epoch 1: loss=3.1792', '\\n', '[Contrastive] Epoch 2:\nloss=2.3612', '\\n', '[Contrastive] Epoch 3: loss=1.8947', '\\n', 'Epoch 1:\ntrain_loss=0.6814 | Dev HSCA=0.6427', '\\n', 'Epoch 2: train_loss=0.6573 | Dev\nHSCA=0.6704', '\\n', 'Epoch 3: train_loss=0.6271 | Dev HSCA=0.6816', '\\n', 'Epoch\n4: train_loss=0.5892 | Dev HSCA=0.7342', '\\n', 'Epoch 5: train_loss=0.5219 | Dev\nHSCA=0.8170', '\\n', 'TEST -> SWA: 0.8338 | CWA: 0.8019 | HSCA: 0.8175', '\\n',\n'\\n========== Weight Decay = 1e-05 ==========', '\\n', '[Contrastive] Epoch 1:\nloss=3.1636', '\\n', '[Contrastive] Epoch 2: loss=2.3951', '\\n', '[Contrastive]\nEpoch 3: loss=1.9291', '\\n', 'Epoch 1: train_loss=0.6847 | Dev HSCA=0.6411',\n'\\n', 'Epoch 2: train_loss=0.6552 | Dev HSCA=0.6491', '\\n', 'Epoch 3:\ntrain_loss=0.6312 | Dev HSCA=0.6374', '\\n', 'Epoch 4: train_loss=0.6066 | Dev\nHSCA=0.7056', '\\n', 'Epoch 5: train_loss=0.5512 | Dev HSCA=0.7513', '\\n', 'TEST\n-> SWA: 0.7971 | CWA: 0.7568 | HSCA: 0.7764', '\\n', '\\n========== Weight Decay =\n0.0001 ==========', '\\n', '[Contrastive] Epoch 1: loss=3.1843', '\\n',\n'[Contrastive] Epoch 2: loss=2.2839', '\\n', '[Contrastive] Epoch 3:\nloss=1.8824', '\\n', 'Epoch 1: train_loss=0.6862 | Dev HSCA=0.6547', '\\n', 'Epoch\n2: train_loss=0.6632 | Dev HSCA=0.6586', '\\n', 'Epoch 3: train_loss=0.6377 | Dev\nHSCA=0.6599', '\\n', 'Epoch 4: train_loss=0.6030 | Dev HSCA=0.6763', '\\n', 'Epoch\n5: train_loss=0.5498 | Dev HSCA=0.7839', '\\n', 'TEST -> SWA: 0.7931 | CWA:\n0.7532 | HSCA: 0.7727', '\\n', '\\n========== Weight Decay = 0.0005 ==========',\n'\\n', '[Contrastive] Epoch 1: loss=3.2502', '\\n', '[Contrastive] Epoch 2:\nloss=2.3128', '\\n', '[Contrastive] Epoch 3: loss=1.9157', '\\n', 'Epoch 1:\ntrain_loss=0.6836 | Dev HSCA=0.6366', '\\n', 'Epoch 2: train_loss=0.6554 | Dev\nHSCA=0.6746', '\\n', 'Epoch 3: train_loss=0.6257 | Dev HSCA=0.6886', '\\n', 'Epoch\n4: train_loss=0.5773 | Dev HSCA=0.7272', '\\n', 'Epoch 5: train_loss=0.5084 | Dev\nHSCA=0.8304', '\\n', 'TEST -> SWA: 0.8648 | CWA: 0.8355 | HSCA: 0.8499', '\\n',\n'\\n========== Weight Decay = 0.001 ==========', '\\n', '[Contrastive] Epoch 1:\nloss=3.0848', '\\n', '[Contrastive] Epoch 2: loss=2.2453', '\\n', '[Contrastive]\nEpoch 3: loss=1.8102', '\\n', 'Epoch 1: train_loss=0.6886 | Dev HSCA=0.6505',\n'\\n', 'Epoch 2: train_loss=0.6717 | Dev HSCA=0.6443', '\\n', 'Epoch 3:\ntrain_loss=0.6514 | Dev HSCA=0.6427', '\\n', 'Epoch 4: train_loss=0.6370 | Dev\nHSCA=0.6651', '\\n', 'Epoch 5: train_loss=0.6125 | Dev HSCA=0.6928', '\\n', 'TEST\n-> SWA: 0.6980 | CWA: 0.6724 | HSCA: 0.6850', '\\n', '\\nSaved experiment data\nto', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-16_00-46-\n17_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 28 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size:', ' ', '14', '\\n', '\\n=== Running experiment: embed_dim=64 ===',\n'\\n', '[Contrastive] ep1 loss=3.3034', '\\n', '[Contrastive] ep2 loss=2.4554',\n'\\n', '[Contrastive] ep3 loss=1.9354', '\\n', '[Supervised] ep1 train_loss=0.6851\nHSCA=0.6249', '\\n', '[Supervised] ep2 train_loss=0.6672 HSCA=0.6488', '\\n',\n'[Supervised] ep3 train_loss=0.6456 HSCA=0.6691', '\\n', '[Supervised] ep4\ntrain_loss=0.6206 HSCA=0.6727', '\\n', '[Supervised] ep5 train_loss=0.5843\nHSCA=0.7309', '\\n', 'TEST (embed_dim=64) -> HSCA=0.7263', '\\n', '\\n=== Running\nexperiment: embed_dim=128 ===', '\\n', '[Contrastive] ep1 loss=3.1260', '\\n',\n'[Contrastive] ep2 loss=2.3275', '\\n', '[Contrastive] ep3 loss=1.8700', '\\n',\n'[Supervised] ep1 train_loss=0.6873 HSCA=0.6501', '\\n', '[Supervised] ep2\ntrain_loss=0.6592 HSCA=0.6379', '\\n', '[Supervised] ep3 train_loss=0.6372\nHSCA=0.6588', '\\n', '[Supervised] ep4 train_loss=0.5998 HSCA=0.7239', '\\n',\n'[Supervised] ep5 train_loss=0.5396 HSCA=0.8032', '\\n', 'TEST (embed_dim=128) ->\nHSCA=0.7616', '\\n', '\\n=== Running experiment: embed_dim=256 ===', '\\n',\n'[Contrastive] ep1 loss=3.1397', '\\n', '[Contrastive] ep2 loss=2.2998', '\\n',\n'[Contrastive] ep3 loss=1.8137', '\\n', '[Supervised] ep1 train_loss=0.6815\nHSCA=0.6464', '\\n', '[Supervised] ep2 train_loss=0.6334 HSCA=0.6708', '\\n',\n'[Supervised] ep3 train_loss=0.5834 HSCA=0.7182', '\\n', '[Supervised] ep4\ntrain_loss=0.5165 HSCA=0.8044', '\\n', '[Supervised] ep5 train_loss=0.4402\nHSCA=0.8684', '\\n', 'TEST (embed_dim=256) -> HSCA=0.8816', '\\n', '\\nSaved\nexperiment_data.npy', '\\n', 'Execution time: 20 seconds seconds (time limit is\n30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '[Pre-train] epoch 1/3  loss=3.1634', '\\n', '[Pre-train]\nepoch 2/3  loss=2.3942', '\\n', '[Pre-train] epoch 3/3  loss=1.9443', '\\n',\n'\\n=== Fine-tuning for up-to 5 epochs (early-stopping patience=3) ===', '\\n', '\nEpoch 01: train HSCA=0.6228 | dev HSCA=0.6376', '\\n', '  Epoch 02: train\nHSCA=0.6612 | dev HSCA=0.6501', '\\n', '  Epoch 03: train HSCA=0.6370 | dev\nHSCA=0.6424', '\\n', '  Epoch 04: train HSCA=0.7522 | dev HSCA=0.7224', '\\n', '\nEpoch 05: train HSCA=0.8482 | dev HSCA=0.7885', '\\n', '  >>> TEST HSCA =\n0.7935', '\\n', '\\n=== Fine-tuning for up-to 10 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6539 | dev HSCA=0.6404', '\\n',\n'  Epoch 02: train HSCA=0.6910 | dev HSCA=0.6642', '\\n', '  Epoch 03: train\nHSCA=0.6972 | dev HSCA=0.6727', '\\n', '  Epoch 04: train HSCA=0.7611 | dev\nHSCA=0.7199', '\\n', '  Epoch 05: train HSCA=0.7809 | dev HSCA=0.7514', '\\n', '\nEpoch 06: train HSCA=0.8859 | dev HSCA=0.8407', '\\n', '  Epoch 07: train\nHSCA=0.9260 | dev HSCA=0.8871', '\\n', '  Epoch 08: train HSCA=0.9538 | dev\nHSCA=0.9214', '\\n', '  Epoch 09: train HSCA=0.9578 | dev HSCA=0.9393', '\\n', '\nEpoch 10: train HSCA=0.9583 | dev HSCA=0.9364', '\\n', '  >>> TEST HSCA =\n0.9423', '\\n', '\\n=== Fine-tuning for up-to 15 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6154 | dev HSCA=0.6268', '\\n',\n'  Epoch 02: train HSCA=0.6080 | dev HSCA=0.6335', '\\n', '  Epoch 03: train\nHSCA=0.6394 | dev HSCA=0.6431', '\\n', '  Epoch 04: train HSCA=0.6868 | dev\nHSCA=0.6628', '\\n', '  Epoch 05: train HSCA=0.8154 | dev HSCA=0.7750', '\\n', '\nEpoch 06: train HSCA=0.8526 | dev HSCA=0.8240', '\\n', '  Epoch 07: train\nHSCA=0.9341 | dev HSCA=0.8957', '\\n', '  Epoch 08: train HSCA=0.9536 | dev\nHSCA=0.9206', '\\n', '  Epoch 09: train HSCA=0.9559 | dev HSCA=0.9347', '\\n', '\nEpoch 10: train HSCA=0.9588 | dev HSCA=0.9344', '\\n', '  Epoch 11: train\nHSCA=0.9602 | dev HSCA=0.9302', '\\n', '  Epoch 12: train HSCA=0.9654 | dev\nHSCA=0.9363', '\\n', '  Epoch 13: train HSCA=0.9728 | dev HSCA=0.9357', '\\n', '\nEpoch 14: train HSCA=0.9853 | dev HSCA=0.9462', '\\n', '  Epoch 15: train\nHSCA=0.9888 | dev HSCA=0.9468', '\\n', '  >>> TEST HSCA = 0.9622', '\\n', '\\n===\nFine-tuning for up-to 20 epochs (early-stopping patience=3) ===', '\\n', '  Epoch\n01: train HSCA=0.6236 | dev HSCA=0.6517', '\\n', '  Epoch 02: train HSCA=0.6637 |\ndev HSCA=0.6587', '\\n', '  Epoch 03: train HSCA=0.7121 | dev HSCA=0.6831', '\\n',\n'  Epoch 04: train HSCA=0.7518 | dev HSCA=0.7102', '\\n', '  Epoch 05: train\nHSCA=0.8189 | dev HSCA=0.7841', '\\n', '  Epoch 06: train HSCA=0.7661 | dev\nHSCA=0.7223', '\\n', '  Epoch 07: train HSCA=0.9364 | dev HSCA=0.9017', '\\n', '\nEpoch 08: train HSCA=0.9519 | dev HSCA=0.9261', '\\n', '  Epoch 09: train\nHSCA=0.9573 | dev HSCA=0.9370', '\\n', '  Epoch 10: train HSCA=0.9580 | dev\nHSCA=0.9354', '\\n', '  Epoch 11: train HSCA=0.9583 | dev HSCA=0.9337', '\\n', '\nEpoch 12: train HSCA=0.9620 | dev HSCA=0.9354', '\\n', '  Early stopping!', '\\n',\n'  >>> TEST HSCA = 0.9397', '\\n', '\\nSaved experiment_data.npy', '\\n',\n'Execution time: 28 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '[Pre-train] epoch 1/3  loss=3.2926', '\\n', '[Pre-train]\nepoch 2/3  loss=2.2967', '\\n', '[Pre-train] epoch 3/3  loss=1.9253', '\\n',\n'\\n=== Fine-tuning for up-to 5 epochs (early-stopping patience=3) ===', '\\n', '\nEpoch 01: train HSCA=0.6467 | dev HSCA=0.6202', '\\n', '  Epoch 02: train\nHSCA=0.6579 | dev HSCA=0.6302', '\\n', '  Epoch 03: train HSCA=0.7045 | dev\nHSCA=0.6624', '\\n', '  Epoch 04: train HSCA=0.7331 | dev HSCA=0.6892', '\\n', '\nEpoch 05: train HSCA=0.8222 | dev HSCA=0.7570', '\\n', '  >>> TEST HSCA =\n0.7633', '\\n', '\\n=== Fine-tuning for up-to 10 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6291 | dev HSCA=0.6255', '\\n',\n'  Epoch 02: train HSCA=0.6290 | dev HSCA=0.6242', '\\n', '  Epoch 03: train\nHSCA=0.6382 | dev HSCA=0.6266', '\\n', '  Epoch 04: train HSCA=0.7781 | dev\nHSCA=0.7169', '\\n', '  Epoch 05: train HSCA=0.8676 | dev HSCA=0.7899', '\\n', '\nEpoch 06: train HSCA=0.9019 | dev HSCA=0.8389', '\\n', '  Epoch 07: train\nHSCA=0.9550 | dev HSCA=0.9190', '\\n', '  Epoch 08: train HSCA=0.9574 | dev\nHSCA=0.9300', '\\n', '  Epoch 09: train HSCA=0.9575 | dev HSCA=0.9296', '\\n', '\nEpoch 10: train HSCA=0.9626 | dev HSCA=0.9270', '\\n', '  >>> TEST HSCA =\n0.9412', '\\n', '\\n=== Fine-tuning for up-to 15 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6269 | dev HSCA=0.6237', '\\n',\n'  Epoch 02: train HSCA=0.6402 | dev HSCA=0.6316', '\\n', '  Epoch 03: train\nHSCA=0.7227 | dev HSCA=0.6863', '\\n', '  Epoch 04: train HSCA=0.8023 | dev\nHSCA=0.7315', '\\n', '  Epoch 05: train HSCA=0.8635 | dev HSCA=0.8132', '\\n', '\nEpoch 06: train HSCA=0.8986 | dev HSCA=0.8567', '\\n', '  Epoch 07: train\nHSCA=0.9536 | dev HSCA=0.9186', '\\n', '  Epoch 08: train HSCA=0.9580 | dev\nHSCA=0.9335', '\\n', '  Epoch 09: train HSCA=0.9580 | dev HSCA=0.9341', '\\n', '\nEpoch 10: train HSCA=0.9602 | dev HSCA=0.9348', '\\n', '  Epoch 11: train\nHSCA=0.9645 | dev HSCA=0.9245', '\\n', '  Epoch 12: train HSCA=0.9682 | dev\nHSCA=0.9295', '\\n', '  Epoch 13: train HSCA=0.9777 | dev HSCA=0.9447', '\\n', '\nEpoch 14: train HSCA=0.9871 | dev HSCA=0.9445', '\\n', '  Epoch 15: train\nHSCA=0.9920 | dev HSCA=0.9535', '\\n', '  >>> TEST HSCA = 0.9711', '\\n', '\\n===\nFine-tuning for up-to 20 epochs (early-stopping patience=3) ===', '\\n', '  Epoch\n01: train HSCA=0.6387 | dev HSCA=0.6197', '\\n', '  Epoch 02: train HSCA=0.6668 |\ndev HSCA=0.6229', '\\n', '  Epoch 03: train HSCA=0.6846 | dev HSCA=0.6439', '\\n',\n'  Epoch 04: train HSCA=0.8185 | dev HSCA=0.7187', '\\n', '  Epoch 05: train\nHSCA=0.8392 | dev HSCA=0.7563', '\\n', '  Epoch 06: train HSCA=0.9092 | dev\nHSCA=0.8438', '\\n', '  Epoch 07: train HSCA=0.9391 | dev HSCA=0.8779', '\\n', '\nEpoch 08: train HSCA=0.9561 | dev HSCA=0.9007', '\\n', '  Epoch 09: train\nHSCA=0.9571 | dev HSCA=0.9182', '\\n', '  Epoch 10: train HSCA=0.9617 | dev\nHSCA=0.9213', '\\n', '  Epoch 11: train HSCA=0.9676 | dev HSCA=0.9231', '\\n', '\nEpoch 12: train HSCA=0.9742 | dev HSCA=0.9281', '\\n', '  Epoch 13: train\nHSCA=0.9799 | dev HSCA=0.9414', '\\n', '  Epoch 14: train HSCA=0.9852 | dev\nHSCA=0.9446', '\\n', '  Epoch 15: train HSCA=0.9915 | dev HSCA=0.9485', '\\n', '\nEpoch 16: train HSCA=0.9935 | dev HSCA=0.9619', '\\n', '  Epoch 17: train\nHSCA=0.9967 | dev HSCA=0.9622', '\\n', '  Epoch 18: train HSCA=0.9976 | dev\nHSCA=0.9671', '\\n', '  Epoch 19: train HSCA=0.9989 | dev HSCA=0.9688', '\\n', '\nEpoch 20: train HSCA=0.9997 | dev HSCA=0.9747', '\\n', '  >>> TEST HSCA =\n0.9800', '\\n', '\\nSaved experiment_data.npy', '\\n', 'Execution time: 10 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 500}\", '\\n',\n'Vocab size: 14', '\\n', '[Pre-train] epoch 1/3  loss=3.2043', '\\n', '[Pre-train]\nepoch 2/3  loss=2.2567', '\\n', '[Pre-train] epoch 3/3  loss=1.8360', '\\n',\n'\\n=== Fine-tuning for up-to 5 epochs (early-stopping patience=3) ===', '\\n', '\nEpoch 01: train HSCA=0.6053 | dev HSCA=0.6360', '\\n', '  Epoch 02: train\nHSCA=0.6366 | dev HSCA=0.6273', '\\n', '  Epoch 03: train HSCA=0.6972 | dev\nHSCA=0.6694', '\\n', '  Epoch 04: train HSCA=0.7572 | dev HSCA=0.7176', '\\n', '\nEpoch 05: train HSCA=0.7310 | dev HSCA=0.7232', '\\n', '  >>> TEST HSCA =\n0.7114', '\\n', '\\n=== Fine-tuning for up-to 10 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6041 | dev HSCA=0.6363', '\\n',\n'  Epoch 02: train HSCA=0.6382 | dev HSCA=0.6440', '\\n', '  Epoch 03: train\nHSCA=0.6915 | dev HSCA=0.6686', '\\n', '  Epoch 04: train HSCA=0.7372 | dev\nHSCA=0.6940', '\\n', '  Epoch 05: train HSCA=0.7806 | dev HSCA=0.7373', '\\n', '\nEpoch 06: train HSCA=0.8246 | dev HSCA=0.7786', '\\n', '  Epoch 07: train\nHSCA=0.9082 | dev HSCA=0.8477', '\\n', '  Epoch 08: train HSCA=0.9470 | dev\nHSCA=0.9181', '\\n', '  Epoch 09: train HSCA=0.9570 | dev HSCA=0.9352', '\\n', '\nEpoch 10: train HSCA=0.9584 | dev HSCA=0.9307', '\\n', '  >>> TEST HSCA =\n0.9446', '\\n', '\\n=== Fine-tuning for up-to 15 epochs (early-stopping\npatience=3) ===', '\\n', '  Epoch 01: train HSCA=0.6224 | dev HSCA=0.6419', '\\n',\n'  Epoch 02: train HSCA=0.6591 | dev HSCA=0.6685', '\\n', '  Epoch 03: train\nHSCA=0.6902 | dev HSCA=0.6557', '\\n', '  Epoch 04: train HSCA=0.7329 | dev\nHSCA=0.6971', '\\n', '  Epoch 05: train HSCA=0.7609 | dev HSCA=0.7089', '\\n', '\nEpoch 06: train HSCA=0.8210 | dev HSCA=0.7538', '\\n', '  Epoch 07: train\nHSCA=0.9192 | dev HSCA=0.8751', '\\n', '  Epoch 08: train HSCA=0.9472 | dev\nHSCA=0.9113', '\\n', '  Epoch 09: train HSCA=0.9567 | dev HSCA=0.9307', '\\n', '\nEpoch 10: train HSCA=0.9578 | dev HSCA=0.9271', '\\n', '  Epoch 11: train\nHSCA=0.9586 | dev HSCA=0.9285', '\\n', '  Epoch 12: train HSCA=0.9610 | dev\nHSCA=0.9324', '\\n', '  Epoch 13: train HSCA=0.9719 | dev HSCA=0.9410', '\\n', '\nEpoch 14: train HSCA=0.9807 | dev HSCA=0.9420', '\\n', '  Epoch 15: train\nHSCA=0.9885 | dev HSCA=0.9521', '\\n', '  >>> TEST HSCA = 0.9566', '\\n', '\\n===\nFine-tuning for up-to 20 epochs (early-stopping patience=3) ===', '\\n', '  Epoch\n01: train HSCA=0.6057 | dev HSCA=0.6380', '\\n', '  Epoch 02: train HSCA=0.6648 |\ndev HSCA=0.6699', '\\n', '  Epoch 03: train HSCA=0.6913 | dev HSCA=0.6780', '\\n',\n'  Epoch 04: train HSCA=0.7448 | dev HSCA=0.6986', '\\n', '  Epoch 05: train\nHSCA=0.8079 | dev HSCA=0.7546', '\\n', '  Epoch 06: train HSCA=0.8425 | dev\nHSCA=0.8119', '\\n', '  Epoch 07: train HSCA=0.9061 | dev HSCA=0.8513', '\\n', '\nEpoch 08: train HSCA=0.9401 | dev HSCA=0.9079', '\\n', '  Epoch 09: train\nHSCA=0.9540 | dev HSCA=0.9370', '\\n', '  Epoch 10: train HSCA=0.9565 | dev\nHSCA=0.9217', '\\n', '  Epoch 11: train HSCA=0.9591 | dev HSCA=0.9321', '\\n', '\nEpoch 12: train HSCA=0.9639 | dev HSCA=0.9337', '\\n', '  Early stopping!', '\\n',\n'  >>> TEST HSCA = 0.9265', '\\n', '\\nSaved experiment_data.npy', '\\n',\n'Execution time: 9 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "", "", "The execution of the training script was successful without any bugs. The script\nexplored the impact of different contrastive pretraining epochs (8, 11, and 15)\non the Synthetic PolyRule Reasoning (SPR) task. The results showed that\nincreasing the number of pretraining epochs improved the harmonic shape-color\naccuracy (HSCA) on the test set, with the best performance achieved at 15 epochs\n(HSCA=0.8087). The script effectively saved the experimental data for further\nanalysis. No issues were detected.", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "harmonic SCA", "lower_is_better": false, "description": "Harmonic SCA measures the harmonic mean of precision and recall for semantic class accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7002, "best_value": 0.7071}]}, {"metric_name": "cross-entropy loss", "lower_is_better": true, "description": "Cross-entropy loss measures the difference between predicted and actual distributions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.558, "best_value": 0.558}]}]}, {"metric_names": [{"metric_name": "train HSCA", "lower_is_better": false, "description": "Final training HSCA (Higher is better)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8217, "best_value": 0.8217}, {"dataset_name": "SPR_BENCH", "final_value": 0.9588, "best_value": 0.9588}, {"dataset_name": "SPR_BENCH", "final_value": 0.9784, "best_value": 0.9784}, {"dataset_name": "SPR_BENCH", "final_value": 0.9962, "best_value": 0.9962}]}, {"metric_name": "validation HSCA", "lower_is_better": false, "description": "Best validation HSCA (Higher is better)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7567, "best_value": 0.7567}, {"dataset_name": "SPR_BENCH", "final_value": 0.9226, "best_value": 0.9226}, {"dataset_name": "SPR_BENCH", "final_value": 0.9301, "best_value": 0.9301}, {"dataset_name": "SPR_BENCH", "final_value": 0.963, "best_value": 0.963}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss (Lower is better)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5278, "best_value": 0.5278}, {"dataset_name": "SPR_BENCH", "final_value": 0.2392, "best_value": 0.2392}, {"dataset_name": "SPR_BENCH", "final_value": 0.1187, "best_value": 0.1187}, {"dataset_name": "SPR_BENCH", "final_value": 0.0468, "best_value": 0.0468}]}, {"metric_name": "test HSCA", "lower_is_better": false, "description": "Test HSCA (Higher is better)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.753, "best_value": 0.753}, {"dataset_name": "SPR_BENCH", "final_value": 0.9419, "best_value": 0.9419}, {"dataset_name": "SPR_BENCH", "final_value": 0.9488, "best_value": 0.9488}, {"dataset_name": "SPR_BENCH", "final_value": 0.9709, "best_value": 0.9709}]}]}, {"metric_names": [{"metric_name": "HSCA", "lower_is_better": false, "description": "Harmonic Similarity Coefficient Accuracy (HSCA) measures the accuracy of predictions relative to a harmonic similarity benchmark.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9351, "best_value": 0.9351}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, where a lower value indicates better model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3339, "best_value": 0.3339}]}]}, {"metric_names": [{"metric_name": "validation HSCA", "lower_is_better": false, "description": "The best validation HSCA score achieved during training.", "data": [{"dataset_name": "SPR_BENCH_bs64", "final_value": 0.9104, "best_value": 0.9104}, {"dataset_name": "SPR_BENCH_bs128", "final_value": 0.7506, "best_value": 0.7506}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6255, "best_value": 0.6255}, {"dataset_name": "SPR_BENCH_bs512", "final_value": 0.6245, "best_value": 0.6245}]}, {"metric_name": "test HSCA", "lower_is_better": false, "description": "The HSCA score on the test dataset.", "data": [{"dataset_name": "SPR_BENCH_bs64", "final_value": 0.9386, "best_value": 0.9386}, {"dataset_name": "SPR_BENCH_bs128", "final_value": 0.7462, "best_value": 0.7462}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6433, "best_value": 0.6433}, {"dataset_name": "SPR_BENCH_bs512", "final_value": 0.6212, "best_value": 0.6212}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The final loss value during training.", "data": [{"dataset_name": "SPR_BENCH_bs64", "final_value": 0.373, "best_value": 0.373}, {"dataset_name": "SPR_BENCH_bs128", "final_value": 0.5105, "best_value": 0.5105}, {"dataset_name": "SPR_BENCH_bs256", "final_value": 0.6189, "best_value": 0.6189}, {"dataset_name": "SPR_BENCH_bs512", "final_value": 0.6398, "best_value": 0.6398}]}]}, {"metric_names": [{"metric_name": "harmonic SCA", "lower_is_better": false, "description": "Harmonic mean of the scaled classification accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8087, "best_value": 0.8087}]}, {"metric_name": "cross-entropy loss", "lower_is_better": true, "description": "Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6099, "best_value": 0.6099}]}]}, {"metric_names": [{"metric_name": "train harmonic SCA", "lower_is_better": false, "description": "Harmonic mean of some classification accuracy during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9129, "best_value": 0.9129}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss function value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.2309, "best_value": 0.2309}]}, {"metric_name": "test harmonic SCA", "lower_is_better": false, "description": "Harmonic mean of some classification accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9393, "best_value": 0.9393}]}]}, {"metric_names": [{"metric_name": "harmonic shape-color accuracy (HSCA)", "lower_is_better": false, "description": "Measures the accuracy of correctly identifying the harmonic shape-color combinations.", "data": [{"dataset_name": "Development Set, \u03c4 = 0.05", "final_value": 0.7733, "best_value": 0.7733}, {"dataset_name": "Test Set, \u03c4 = 0.05", "final_value": 0.7845, "best_value": 0.7845}, {"dataset_name": "Development Set, \u03c4 = 0.08", "final_value": 0.7787, "best_value": 0.7787}, {"dataset_name": "Test Set, \u03c4 = 0.08", "final_value": 0.7767, "best_value": 0.7767}, {"dataset_name": "Development Set, \u03c4 = 0.1", "final_value": 0.7789, "best_value": 0.7789}, {"dataset_name": "Test Set, \u03c4 = 0.1", "final_value": 0.7751, "best_value": 0.7751}, {"dataset_name": "Development Set, \u03c4 = 0.15", "final_value": 0.7809, "best_value": 0.7809}, {"dataset_name": "Test Set, \u03c4 = 0.15", "final_value": 0.7723, "best_value": 0.7723}, {"dataset_name": "Development Set, \u03c4 = 0.2", "final_value": 0.7834, "best_value": 0.7834}, {"dataset_name": "Test Set, \u03c4 = 0.2", "final_value": 0.7881, "best_value": 0.7881}]}, {"metric_name": "final supervised training loss", "lower_is_better": true, "description": "The final loss value after supervised training.", "data": [{"dataset_name": "Development Set, \u03c4 = 0.05", "final_value": 0.51, "best_value": 0.51}, {"dataset_name": "Development Set, \u03c4 = 0.08", "final_value": 0.5146, "best_value": 0.5146}, {"dataset_name": "Development Set, \u03c4 = 0.1", "final_value": 0.5162, "best_value": 0.5162}, {"dataset_name": "Development Set, \u03c4 = 0.15", "final_value": 0.5118, "best_value": 0.5118}, {"dataset_name": "Development Set, \u03c4 = 0.2", "final_value": 0.5125, "best_value": 0.5125}]}, {"metric_name": "final contrastive pre-training loss", "lower_is_better": true, "description": "The final loss value after contrastive pre-training.", "data": [{"dataset_name": "Test Set, \u03c4 = 0.05", "final_value": 1.6492, "best_value": 1.6492}, {"dataset_name": "Test Set, \u03c4 = 0.08", "final_value": 1.7156, "best_value": 1.7156}, {"dataset_name": "Test Set, \u03c4 = 0.1", "final_value": 1.8599, "best_value": 1.8599}, {"dataset_name": "Test Set, \u03c4 = 0.15", "final_value": 2.4043, "best_value": 2.4043}, {"dataset_name": "Test Set, \u03c4 = 0.2", "final_value": 2.9117, "best_value": 2.9117}]}]}, {"metric_names": [{"metric_name": "final training loss", "lower_is_better": true, "description": "The final loss value after training, indicating the model's performance on the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5084, "best_value": 0.5084}]}, {"metric_name": "final validation harmonic SCA", "lower_is_better": false, "description": "The harmonic mean of precision and recall on the validation set, indicating model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8304, "best_value": 0.8304}]}, {"metric_name": "test harmonic SCA", "lower_is_better": false, "description": "The harmonic mean of precision and recall on the test set, indicating model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8499, "best_value": 0.8499}]}]}, {"metric_names": [{"metric_name": "HSCA", "lower_is_better": false, "description": "A metric indicating the hierarchical structure clustering accuracy.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7263, "best_value": 0.8816}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4402, "best_value": 0.4402}]}]}, {"metric_names": [{"metric_name": "train HSCA", "lower_is_better": false, "description": "Final train HSCA metric value after training.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.8482, "best_value": 0.8482}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9583, "best_value": 0.9583}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9888, "best_value": 0.9888}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.962, "best_value": 0.962}]}, {"metric_name": "validation HSCA", "lower_is_better": false, "description": "Best validation HSCA metric value during training.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.7885, "best_value": 0.7885}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9393, "best_value": 0.9393}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9468, "best_value": 0.9468}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.937, "best_value": 0.937}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final train loss metric value after training.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.528, "best_value": 0.528}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.2271, "best_value": 0.2271}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.0822, "best_value": 0.0822}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.1754, "best_value": 0.1754}]}, {"metric_name": "test HSCA", "lower_is_better": false, "description": "Test HSCA metric value after training.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.7935, "best_value": 0.7935}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9423, "best_value": 0.9423}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9622, "best_value": 0.9622}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.9397, "best_value": 0.9397}]}]}, {"metric_names": [{"metric_name": "train HSCA", "lower_is_better": false, "description": "The final training HSCA metric value.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.8222, "best_value": 0.8222}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9626, "best_value": 0.9626}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.9997, "best_value": 0.9997}]}, {"metric_name": "validation HSCA", "lower_is_better": false, "description": "The best validation HSCA metric value.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.757, "best_value": 0.757}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.93, "best_value": 0.93}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9535, "best_value": 0.9535}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.9747, "best_value": 0.9747}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The final training loss metric value.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.522, "best_value": 0.522}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.1882, "best_value": 0.1882}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.0756, "best_value": 0.0756}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.0281, "best_value": 0.0281}]}, {"metric_name": "test HSCA", "lower_is_better": false, "description": "The test HSCA metric value.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.7633, "best_value": 0.7633}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9412, "best_value": 0.9412}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9711, "best_value": 0.9711}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.98, "best_value": 0.98}]}]}, {"metric_names": [{"metric_name": "train HSCA", "lower_is_better": false, "description": "Final train HSCA metric value for the dataset.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.731, "best_value": 0.731}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9584, "best_value": 0.9584}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.9639, "best_value": 0.9639}]}, {"metric_name": "validation HSCA", "lower_is_better": false, "description": "Best validation HSCA metric value for the dataset.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.7232, "best_value": 0.7232}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9352, "best_value": 0.9352}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9521, "best_value": 0.9521}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.937, "best_value": 0.937}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final train loss metric value for the dataset.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.5497, "best_value": 0.5497}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.2417, "best_value": 0.2417}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.0841, "best_value": 0.0841}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.174, "best_value": 0.174}]}, {"metric_name": "test HSCA", "lower_is_better": false, "description": "Test HSCA metric value for the dataset.", "data": [{"dataset_name": "SPR_BENCH (fine-tuning max_epochs=5)", "final_value": 0.7114, "best_value": 0.7114}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=10)", "final_value": 0.9446, "best_value": 0.9446}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=15)", "final_value": 0.9566, "best_value": 0.9566}, {"dataset_name": "SPR_BENCH (fine-tuning max_epochs=20)", "final_value": 0.9265, "best_value": 0.9265}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_0df10d0b90ec40899e48e2196512a032_proc_3065839/SPR_BENCH_HSCA_curve.png", "../../logs/0-run/experiment_results/experiment_0df10d0b90ec40899e48e2196512a032_proc_3065839/SPR_BENCH_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_69cc842955bc4716a578b76635796f3e_proc_3069201/SPR_BENCH_hsca_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_69cc842955bc4716a578b76635796f3e_proc_3069201/SPR_BENCH_test_hsca_bar.png"], ["../../logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_HSCA_curves.png", "../../logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_Loss_curves.png", "../../logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_LR_scatter.png"], ["../../logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_training_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_dev_HSCA_curves.png", "../../logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_final_test_HSCA.png", "../../logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_confusion_best_bs64.png"], ["../../logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_dev_hsca_vs_pretrain_epochs.png", "../../logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_test_hsca_vs_pretrain_epochs.png", "../../logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_loss_vs_pretrain_epochs.png"], ["../../logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_HSCA_vs_Epoch.png", "../../logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_TrainLoss_vs_Epoch.png", "../../logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_Test_HSCA_Bar.png", "../../logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_ConfusionMatrix_hid512.png"], ["../../logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_dev_hsca_vs_epoch.png", "../../logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_test_hsca_vs_temperature.png", "../../logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_train_loss_best_tau_0.2.png"], ["../../logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_train_loss_weight_decay.png", "../../logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_train_HSCA_weight_decay.png", "../../logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_test_HSCA_vs_decay.png"], ["../../logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_train_HSCA_vs_epoch.png", "../../logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_train_loss_vs_epoch.png", "../../logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_final_test_HSCA.png"], ["../../logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/SPR_BENCH_hsca_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/SPR_BENCH_test_hsca_bar.png"], ["../../logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/SPR_BENCH_hsca_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/SPR_BENCH_test_hsca_bar.png"], ["../../logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/SPR_BENCH_hsca_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/SPR_BENCH_test_hsca_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_0c938dfc3d7d4ff0a18d4d1816b56113/SPR_BENCH_hsca_train_val_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_0c938dfc3d7d4ff0a18d4d1816b56113/SPR_BENCH_test_hsca_bar_aggregated.png"]], "plot_paths": [["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0df10d0b90ec40899e48e2196512a032_proc_3065839/SPR_BENCH_HSCA_curve.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0df10d0b90ec40899e48e2196512a032_proc_3065839/SPR_BENCH_loss_curve.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_69cc842955bc4716a578b76635796f3e_proc_3069201/SPR_BENCH_hsca_train_val_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_69cc842955bc4716a578b76635796f3e_proc_3069201/SPR_BENCH_test_hsca_bar.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_HSCA_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_Loss_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_LR_scatter.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_training_loss_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_dev_HSCA_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_final_test_HSCA.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_confusion_best_bs64.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_dev_hsca_vs_pretrain_epochs.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_test_hsca_vs_pretrain_epochs.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_loss_vs_pretrain_epochs.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_HSCA_vs_Epoch.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_TrainLoss_vs_Epoch.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_Test_HSCA_Bar.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_ConfusionMatrix_hid512.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_dev_hsca_vs_epoch.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_test_hsca_vs_temperature.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_train_loss_best_tau_0.2.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_train_loss_weight_decay.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_train_HSCA_weight_decay.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_test_HSCA_vs_decay.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_train_HSCA_vs_epoch.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_train_loss_vs_epoch.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_final_test_HSCA.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/SPR_BENCH_hsca_train_val_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/SPR_BENCH_test_hsca_bar.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/SPR_BENCH_hsca_train_val_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/SPR_BENCH_test_hsca_bar.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/SPR_BENCH_hsca_train_val_curves.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/SPR_BENCH_test_hsca_bar.png"], ["experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_0c938dfc3d7d4ff0a18d4d1816b56113/SPR_BENCH_hsca_train_val_aggregated.png", "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_0c938dfc3d7d4ff0a18d4d1816b56113/SPR_BENCH_test_hsca_bar_aggregated.png"]], "plot_analyses": [[{"analysis": "This plot demonstrates a consistent increase in the Harmonic Shape-Color Accuracy (HSCA) metric over training epochs. The steady upward trend indicates that the model is progressively learning and improving its ability to recognize patterns in the SPR task. By epoch 5, the HSCA reaches approximately 0.71, which is a promising result for early-stage experiments. This suggests that the enhancements in the contrastive learning framework are effectively contributing to better feature representations.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0df10d0b90ec40899e48e2196512a032_proc_3065839/SPR_BENCH_HSCA_curve.png"}, {"analysis": "This plot shows a steady decline in the cross-entropy loss during training over successive epochs. The consistent reduction in loss, from around 0.68 at epoch 1 to approximately 0.56 at epoch 5, indicates that the model is converging well. This is a strong sign of effective learning, as the model is minimizing the error in its predictions on the training data. Combined with the improvement in HSCA, this suggests that the training process is functioning as intended.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0df10d0b90ec40899e48e2196512a032_proc_3065839/SPR_BENCH_loss_curve.png"}], [{"analysis": "The first plot displays the HSCA (Hypothetical Symbolic Classification Accuracy) curves for both training and validation sets across different epochs. The solid lines represent the training accuracy, while the dashed lines represent the validation accuracy. The results indicate that as the number of epochs increases, both training and validation accuracies improve significantly. Notably, the training accuracy reaches near-perfect levels for all configurations, while the validation accuracy shows a slight plateauing effect after 10 epochs. This suggests that increasing the number of training epochs beyond 10 yields diminishing returns for validation performance. Additionally, the consistent gap between training and validation curves indicates potential overfitting, especially for higher epochs like 20.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_69cc842955bc4716a578b76635796f3e_proc_3069201/SPR_BENCH_hsca_train_val_curves.png"}, {"analysis": "The second plot illustrates the Test HSCA values achieved after fine-tuning the model for different maximum numbers of epochs. It is evident that increasing the fine-tuning epochs improves the Test HSCA, with the highest score of 0.971 achieved at 20 epochs. However, the improvement diminishes as the number of epochs increases beyond 10, with only marginal gains between 15 and 20 epochs. This suggests that while additional fine-tuning epochs can improve performance, there may be an optimal range (e.g., 10-15 epochs) where the trade-off between computational cost and performance gain is most efficient.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_69cc842955bc4716a578b76635796f3e_proc_3069201/SPR_BENCH_test_hsca_bar.png"}], [{"analysis": "This plot shows the Hyper-parameter Sweep results for HSCA (Hypothetical Symbolic Contextual Accuracy). The train and validation HSCA values are plotted for different hyperparameter configurations indexed by the run index. The results show a significant variance in both training and validation HSCA across the runs, indicating that the hyperparameter settings have a strong influence on model performance. Notably, some runs achieve HSCA values near 0.95, suggesting that the model can achieve high performance with optimal hyperparameters. However, the alignment between train and validation HSCA varies, with some runs showing overfitting (validation HSCA significantly lower than train HSCA). Further investigation into the hyperparameter configurations for the best-performing runs is recommended.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_HSCA_curves.png"}, {"analysis": "This plot represents the average cross-entropy loss for training and validation sets across different runs. While the training loss decreases in a fluctuating manner, the validation loss remains constant at zero throughout. This indicates a potential issue with the validation loss computation or data, as it is highly unusual for validation loss to remain constant and at zero. This behavior suggests a need to debug the validation loss evaluation process to ensure it is correctly reflecting the model's performance on the validation set.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_Loss_curves.png"}, {"analysis": "This plot visualizes the results of a grid search over learning rates for pre-training and supervised fine-tuning, with the color intensity representing the validation HSCA. The results indicate that higher pre-training learning rates (e.g., 2 \u00d7 10^-3) and supervised learning rates (e.g., 2 \u00d7 10^-3) yield the best validation HSCA values, reaching near 0.90. Conversely, lower learning rates result in significantly poorer performance. This suggests that the model benefits from more aggressive learning rates during both pre-training and supervised fine-tuning for this task. Further fine-grained tuning of learning rates around the high-performing regions is recommended to maximize performance.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c0f90f8a5c70496b9c6e024279798527_proc_3069202/SPR_BENCH_LR_scatter.png"}], [{"analysis": "This plot shows the training loss for different batch sizes over epochs. Smaller batch sizes (e.g., bs64) result in faster convergence and lower final training loss compared to larger batch sizes (e.g., bs512). This suggests that smaller batch sizes may allow the model to capture finer-grained details in the data more effectively, leading to better optimization during training.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_training_loss_curves.png"}, {"analysis": "This plot illustrates the development set HSCA (Hypothetical Symbolic Classification Accuracy) for different batch sizes over epochs. Smaller batch sizes, particularly bs64, lead to a more rapid and consistent improvement in HSCA. Larger batch sizes (e.g., bs512) show slower and less significant improvements, indicating that smaller batch sizes may generalize better on the development set.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_dev_HSCA_curves.png"}, {"analysis": "This bar chart compares the final test HSCA achieved by different batch sizes. The smallest batch size, bs64, achieves the highest HSCA, followed by bs128. Larger batch sizes (bs256 and bs512) perform worse, reinforcing the observation that smaller batch sizes are more effective for this task.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_final_test_HSCA.png"}, {"analysis": "The confusion matrix for the best-performing batch size (bs64) indicates good classification performance, with a high number of correctly classified samples for both Class 0 and Class 1. However, there is a slight imbalance in misclassification, with Class 1 having more false negatives than Class 0 has false positives. This suggests room for improvement in handling Class 1 predictions.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a2ccfd2983df4945be72124ab17cb9fa_proc_3069203/SPR_BENCH_confusion_best_bs64.png"}], [{"analysis": "This plot shows the performance of the model on the development set as measured by the HSCA metric. The steady increase in HSCA with more contrastive pre-training epochs indicates that the additional pre-training improves the model's ability to generalize and capture the symbolic patterns within the data. The curve suggests diminishing returns beyond 15 epochs, but the trend is still upward, implying that further exploration of pre-training epochs might yield slight additional improvements.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_dev_hsca_vs_pretrain_epochs.png"}, {"analysis": "This plot demonstrates the model's performance on the test set in terms of HSCA as a function of contrastive pre-training epochs. The linear increase in performance mirrors the trend observed in the development set, confirming the robustness of the improvements across datasets. The test HSCA surpasses 0.81 at 15 epochs, indicating that the context-aware contrastive learning framework is effective in enhancing symbolic reasoning capabilities.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_test_hsca_vs_pretrain_epochs.png"}, {"analysis": "This plot depicts the mean cross-entropy loss during fine-tuning as a function of contrastive pre-training epochs. The sharp decrease in loss at 11 epochs, followed by an increase, suggests that the model achieves optimal feature representation at this point. However, the upward trend in loss beyond 11 epochs might indicate overfitting or a reduction in the diversity of learned representations. This observation suggests a need for careful tuning of pre-training duration to balance performance and generalization.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6e73e6e0965b42e598d65ddbd0e05fdd_proc_3069204/SPR_BENCH_loss_vs_pretrain_epochs.png"}], [{"analysis": "The first plot shows the progression of Training HSCA (Hypothetical Symbolic Classification Accuracy) across epochs for different encoder hidden dimensions. As the hidden dimension increases, the model achieves higher HSCA values, indicating that larger hidden dimensions allow the model to learn more robust representations. The model with a hidden dimension of 512 achieves the best HSCA, reaching over 0.9 by epoch 5. Models with smaller hidden dimensions (e.g., 64) show slower and less significant improvements, suggesting that the capacity of the encoder is crucial for this task.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_HSCA_vs_Epoch.png"}, {"analysis": "The second plot illustrates the Training Loss versus Epoch for various encoder hidden dimensions. Loss decreases consistently across epochs for all configurations, with larger hidden dimensions leading to faster and more significant reductions in loss. The model with a hidden dimension of 512 achieves the lowest final loss, demonstrating its superior learning capacity. The slower convergence of smaller hidden dimensions indicates that they may struggle to capture the complexity of the task.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_TrainLoss_vs_Epoch.png"}, {"analysis": "The third plot summarizes the final Test HSCA achieved by models with different encoder hidden dimensions. The results confirm the trend observed during training: larger hidden dimensions result in better performance. The model with a hidden dimension of 512 achieves the highest Test HSCA, followed by 256, 128, and 64. This further supports the hypothesis that increasing the encoder's capacity enhances its ability to generalize to unseen data.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_Test_HSCA_Bar.png"}, {"analysis": "The confusion matrix for the model with the best-performing hidden dimension (512) shows high classification accuracy, with most predictions aligning with the ground truth. The model correctly classifies 211 and 252 samples for the two classes, while misclassifying only 6 and 31 samples, respectively. This indicates that the model is highly effective at distinguishing between classes, though there is still room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_155f92ba6bf5404c8781652124f14651_proc_3069201/SPR_BENCH_ConfusionMatrix_hid512.png"}], [{"analysis": "This plot illustrates the performance of the model on the development set in terms of HSCA (Harmonic Shape-Color Accuracy) over fine-tuning epochs for various contrastive temperature values (\u03c4). The model's performance consistently improves with more fine-tuning epochs, regardless of the temperature value. Notably, the gap between different temperature values narrows as the training progresses, indicating that the choice of \u03c4 has a diminishing impact on performance after sufficient epochs. However, \u03c4 = 0.2 shows slightly better performance, suggesting it might be the optimal value for this parameter.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_dev_hsca_vs_epoch.png"}, {"analysis": "This bar chart shows the final test HSCA across different contrastive temperature values. The results are nearly identical for all temperature values, with a marginal edge for \u03c4 = 0.2. This consistency indicates that the model's performance is robust to variations in \u03c4, but \u03c4 = 0.2 might still be preferred for slightly better results.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_test_hsca_vs_temperature.png"}, {"analysis": "This plot demonstrates the training loss reduction over fine-tuning epochs for the best-performing temperature value (\u03c4 = 0.2). The loss decreases steadily, indicating effective training and convergence of the model. The consistent decline also reflects the model's ability to learn meaningful representations for the SPR task as training progresses.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c94b9b7de00a4efe91b90475f216dc0d_proc_3069203/sprbench_train_loss_best_tau_0.2.png"}], [{"analysis": "The first plot shows the training loss curves for different weight decay values. As expected, all curves show a decreasing trend as the number of epochs increases, indicating that the model is learning. Among the weight decay values, 0.0005 and 0.0 exhibit the steepest decline in loss, suggesting better convergence. The curve for a weight decay of 0.001 converges more slowly, with higher loss values throughout the training process, indicating that this value might be too high and is impeding learning.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_train_loss_weight_decay.png"}, {"analysis": "The second plot displays the training HSCA (Hypothetical Symbolic Contextual Accuracy) curves for the same weight decay values. Here, weight decay values of 0.0005 and 0.0 show the steepest increase in HSCA, achieving the highest accuracy by the end of training. The weight decay of 0.001 again lags behind, showing the slowest improvement and the lowest final accuracy, reinforcing the earlier observation that this value might be suboptimal for the task.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_train_HSCA_weight_decay.png"}, {"analysis": "The third plot compares the test HSCA values for different weight decay values. Weight decay of 0.0005 achieves the highest test HSCA, followed closely by 0.0. The weight decay of 0.001 has the lowest test HSCA, further confirming that it is not suitable for this task. These results suggest that a weight decay of 0.0005 offers the best balance between regularization and model performance, leading to better generalization.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_07121a74ad7f417a9c0dfc0a9651c5c4_proc_3069204/SPR_BENCH_test_HSCA_vs_decay.png"}], [{"analysis": "This plot shows the progression of HSCA (Hypothetical Shape-Contextual Accuracy) across epochs for different embedding dimensions. It is evident that larger embedding dimensions (dim=256) result in significantly better performance, as the HSCA consistently increases and reaches higher values compared to smaller dimensions (dim=64 and dim=128). This suggests that higher-dimensional embeddings capture more meaningful contextual information, which is beneficial for the SPR task. The steady improvement across epochs also indicates that the model is effectively learning the task.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_train_HSCA_vs_epoch.png"}, {"analysis": "This plot illustrates the training loss reduction across epochs for different embedding dimensions. All configurations exhibit a downward trend, indicating effective learning. However, the model with dim=256 achieves the lowest loss, followed by dim=128 and dim=64. This aligns with the HSCA results, further supporting the idea that higher-dimensional embeddings improve the model's learning capacity and generalization ability.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_train_loss_vs_epoch.png"}, {"analysis": "This bar chart compares the final test HSCA achieved for different embedding dimensions. The results clearly demonstrate that dim=256 outperforms dim=128 and dim=64, with a noticeable margin. This reinforces the previous observations that higher embedding dimensions improve the model's ability to capture contextual information and perform better on the SPR task. The increasing trend highlights the importance of embedding dimensionality in this context-aware contrastive learning framework.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0e1ef5c1e1a249c78b01d78d929e0924_proc_3069202/SPR_BENCH_final_test_HSCA.png"}], [{"analysis": "The HSCA curves demonstrate the performance of the model across different epochs for both training and validation datasets. The solid lines correspond to training performance, while the dashed lines represent validation performance. As the number of epochs increases, the HSCA improves for both train and validation sets, with diminishing returns after around 10 epochs. This suggests that the model benefits from additional training initially but reaches a performance plateau beyond 10 epochs. The alignment between the train and validation curves indicates that the model generalizes well without significant overfitting, as the validation performance closely follows the training performance. However, slight overfitting is visible at higher epochs, as the validation curves do not improve as much as the training curves.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/SPR_BENCH_hsca_train_val_curves.png"}, {"analysis": "The bar chart presents the test HSCA scores achieved after fine-tuning the model for different maximum epochs. The performance improves significantly from 5 epochs (HSCA: 0.794) to 10 epochs (HSCA: 0.942) and reaches a peak at 15 epochs (HSCA: 0.962). However, the performance slightly declines at 20 epochs (HSCA: 0.940), suggesting that prolonged fine-tuning may lead to overfitting or diminishing returns. The results indicate that fine-tuning for around 10-15 epochs is optimal for achieving the best test performance while avoiding overfitting.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/SPR_BENCH_test_hsca_bar.png"}], [{"analysis": "The HSCA (Hierarchical Symbolic Contrastive Accuracy) curves show consistent improvement in both training and validation accuracies as the number of epochs increases. The solid lines represent training performance, while dashed lines represent validation performance. For all configurations (5, 10, 15, and 20 epochs), there is a clear upward trend in accuracy over the initial epochs, indicating effective learning. However, the validation curves exhibit some flattening or slight divergence from the training curves at higher epochs, which could indicate potential overfitting, particularly for longer training durations. The model achieves near-perfect HSCA for both training and validation with 20 epochs, suggesting that the hyperparameters chosen allow for effective fine-tuning of the model over extended training periods.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/SPR_BENCH_hsca_train_val_curves.png"}, {"analysis": "This bar chart illustrates the test HSCA achieved after fine-tuning with different maximum epochs. There is a clear improvement in test performance as the number of fine-tuning epochs increases, with HSCA values rising from 0.763 at 5 epochs to 0.980 at 20 epochs. The largest improvement occurs between 5 and 10 epochs, with diminishing returns beyond 10 epochs. This suggests that while additional epochs improve performance, the most significant gains are achieved early in the fine-tuning process. The near-saturation of HSCA at 20 epochs indicates that further fine-tuning may not yield substantial benefits.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/SPR_BENCH_test_hsca_bar.png"}], [{"analysis": "The first plot illustrates the HSCA (Hypothetical Symbolic Classification Accuracy) curves for both training and validation datasets across different numbers of epochs: 5, 10, 15, and 20. It is evident that as the number of epochs increases, the HSCA improves, with the training curves generally achieving higher accuracy than the validation curves. This suggests that the model is effectively learning from the training data, but there is a slight gap between training and validation performances, indicating potential overfitting for higher epochs. The curves for 10 and 15 epochs show a good balance between training and validation performance, suggesting these might be optimal configurations for this experiment. Beyond 15 epochs, the validation performance plateaus or slightly decreases, further suggesting diminishing returns or overfitting with prolonged training.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/SPR_BENCH_hsca_train_val_curves.png"}, {"analysis": "The second plot displays the Test HSCA as a function of the maximum number of fine-tuning epochs, with the values 5, 10, 15, and 20. The Test HSCA increases significantly from 5 epochs (0.711) to 10 epochs (0.945) and reaches a peak at 15 epochs (0.957). However, at 20 epochs, the Test HSCA drops slightly to 0.927, indicating that excessive fine-tuning may lead to overfitting or reduced generalization ability. The results suggest that 15 epochs is the most effective configuration for maximizing test performance, providing a good trade-off between training and generalization.", "plot_path": "experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/SPR_BENCH_test_hsca_bar.png"}], []], "vlm_feedback_summary": ["The plots show positive trends in both accuracy and loss metrics. The model\ndemonstrates improvement in its performance and convergence during training,\nwhich aligns with the goals of the initial implementation stage.", "The plots demonstrate the effectiveness of fine-tuning epochs on improving HSCA\nmetrics for the SPR task. While longer training improves accuracy, diminishing\nreturns and overfitting are observed after 10 epochs, particularly in the\nvalidation set.", "The plots indicate a strong dependency of model performance on hyperparameter\nchoices, with some configurations achieving near-optimal results. However, the\nvalidation loss behavior is anomalous and requires debugging. Learning rate\ntuning highlights the importance of aggressive learning rates for achieving high\nvalidation HSCA.", "The results indicate that smaller batch sizes (e.g., bs64) consistently\noutperform larger ones in terms of training convergence, development set HSCA,\nand final test HSCA. The confusion matrix highlights good overall classification\nperformance for the best batch size, though some imbalance in misclassification\nremains.", "The plots provide valuable insights into the impact of contrastive pre-training\nepochs on model performance. Both HSCA metrics (Dev and Test) show consistent\nimprovements with longer pre-training, while the fine-tune loss plot highlights\nthe importance of selecting an optimal number of pre-training epochs to avoid\npotential overfitting or loss of representation diversity.", "The analysis highlights that increasing the encoder hidden dimension\nsignificantly improves both training and test performance, as evidenced by\nhigher HSCA values and lower training loss. The confusion matrix for the best-\nperforming model (hidden dimension 512) demonstrates strong classification\naccuracy, with minimal misclassifications. The results suggest that larger\nencoder capacities are crucial for learning robust representations in the SPR\ntask.", "The plots collectively demonstrate that the model's performance improves with\nfine-tuning epochs, and the choice of contrastive temperature (\u03c4) has a limited\nbut observable impact. \u03c4 = 0.2 appears to be slightly better than other values.\nThe steady reduction in training loss confirms effective training and\nconvergence. These results suggest that the current approach is effective, with\nroom for minor parameter optimization.", "The plots indicate that weight decay significantly affects both training\nperformance and generalization. A weight decay of 0.0005 consistently provides\nthe best results across all metrics, achieving the lowest training loss, highest\ntraining HSCA, and highest test HSCA. In contrast, a weight decay of 0.001\nperforms poorly, highlighting the importance of carefully tuning this\nhyperparameter.", "The plots collectively highlight the positive impact of embedding dimensionality\non the performance of the context-aware contrastive learning model for the SPR\ntask. Larger embedding dimensions consistently lead to better HSCA and lower\ntraining loss, indicating improved learning and generalization capabilities.\nThis underscores the importance of optimizing hyperparameters like embedding\nsize to achieve state-of-the-art results.", "The analysis highlights steady improvements in HSCA with increasing epochs, with\noptimal performance achieved at around 10-15 epochs. The model generalizes well,\nbut slight overfitting is observed at higher epochs.", "The results indicate that the model's performance improves significantly with\nincreased fine-tuning epochs, achieving near-perfect accuracy at 20 epochs. The\ntraining and validation curves suggest effective learning, with potential\noverfitting at higher epochs. The diminishing returns in test accuracy beyond 10\nepochs highlight the importance of balancing training duration with performance\ngains.", "The plots indicate that the model benefits from increasing the number of epochs\nup to a certain point, with 15 epochs being the optimal configuration for both\nvalidation and test performance. Prolonged training beyond this point leads to\ndiminishing returns and possible overfitting. These insights are valuable for\nrefining the training strategy and achieving better generalization on the SPR\ntask.", "[]"], "exec_time": [6.3231236934661865, 30.530200481414795, 75.97905421257019, 23.351307153701782, 62.74824047088623, 27.275651454925537, 24.217557668685913, 28.14731788635254, 20.399004697799683, 28.408642053604126, 10.16840672492981, 9.62877893447876, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH Synthetic\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to safely fetch series\ndef get_series(ds, key_chain):\n    cur = experiment_data.get(ds, {})\n    for k in key_chain:\n        cur = cur.get(k, [])\n    return cur\n\n\ndataset_name = \"SPR_BENCH\"\ntrain_hsca = get_series(dataset_name, [\"metrics\", \"train\"])\nval_hsca = get_series(dataset_name, [\"metrics\", \"val\"])\ntrain_loss = get_series(dataset_name, [\"losses\", \"train\"])\n\n# -----------------------------------------------------------\n# Plot 1: HSCA curves\ntry:\n    if train_hsca:\n        epochs = np.arange(1, len(train_hsca) + 1)\n        plt.figure()\n        plt.plot(epochs, train_hsca, marker=\"o\", label=\"Train HSCA\")\n        if len(val_hsca) == len(train_hsca):\n            plt.plot(epochs, val_hsca, marker=\"s\", label=\"Validation/Test HSCA\")\n        plt.title(f\"{dataset_name} \u2013 Harmonic SCA over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HSCA\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dataset_name}_HSCA_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    else:\n        print(\"No HSCA data found; skipping HSCA plot.\")\nexcept Exception as e:\n    print(f\"Error creating HSCA plot: {e}\")\n    plt.close()\n\n# -----------------------------------------------------------\n# Plot 2: Training loss curve\ntry:\n    if train_loss:\n        epochs = np.arange(1, len(train_loss) + 1)\n        plt.figure()\n        plt.plot(epochs, train_loss, marker=\"x\", color=\"tab:red\", label=\"Train CE Loss\")\n        plt.title(f\"{dataset_name} \u2013 Training Loss over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dataset_name}_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    else:\n        print(\"No loss data found; skipping loss plot.\")\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# -----------------------------------------------------------\n# Print final evaluation metric if available\nif val_hsca:\n    print(f\"Final Test HSCA: {val_hsca[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------\nspr_key = (\"supervised_finetuning_epochs\", \"SPR_BENCH\")\ntry:\n    sweep = experiment_data[spr_key[0]][spr_key[1]]\n    epochs_grid = sweep[\"epochs_grid\"]\n    train_hsca = sweep[\"metrics\"][\"train\"]  # list of lists\n    val_hsca = sweep[\"metrics\"][\"val\"]  # list of lists\n    test_hsca = sweep[\"test_hsca\"]  # list\nexcept Exception as e:\n    print(f\"Error extracting data: {e}\")\n    epochs_grid, train_hsca, val_hsca, test_hsca = [], [], [], []\n\n# ------------------------------------------------------------------\n# Plot 1: train / val HSCA curves\ntry:\n    plt.figure()\n    colors = plt.cm.tab10(np.linspace(0, 1, len(epochs_grid)))\n    for i, max_ep in enumerate(epochs_grid):\n        ep_axis = np.arange(1, len(train_hsca[i]) + 1)\n        plt.plot(ep_axis, train_hsca[i], color=colors[i], label=f\"{max_ep}ep train\")\n        plt.plot(\n            ep_axis,\n            val_hsca[i],\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"{max_ep}ep val\",\n        )\n    plt.title(\"SPR_BENCH \u2013 HSCA Curves\\nSolid: Train, Dashed: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.legend(ncol=2, fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_hsca_train_val_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HSCA curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Plot 2: test HSCA vs max epochs\ntry:\n    plt.figure()\n    plt.bar([str(e) for e in epochs_grid], test_hsca, color=\"steelblue\")\n    plt.title(\"SPR_BENCH \u2013 Test HSCA vs Allowed Fine-tuning Epochs\")\n    plt.xlabel(\"Max Fine-tuning Epochs\")\n    plt.ylabel(\"Test HSCA\")\n    for x, y in zip(range(len(epochs_grid)), test_hsca):\n        plt.text(x, y + 0.01, f\"{y:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_hsca_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test HSCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print evaluation metrics\nfor max_ep, hsca in zip(epochs_grid, test_hsca):\n    print(f\"Max epochs={max_ep:2d} | Test HSCA={hsca:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"lr_tuning\"][\"SPR_BENCH\"]\n    train_hsca = np.array(ed[\"metrics\"][\"train\"])\n    val_hsca = np.array(ed[\"metrics\"][\"val\"])\n    train_loss = np.array(ed[\"losses\"][\"train\"])\n    val_loss = np.array(ed[\"losses\"][\"val\"])\n    lr_pairs = np.array(ed[\"lr_pairs\"])  # shape (N,2)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    train_hsca = val_hsca = train_loss = val_loss = lr_pairs = np.array([])\n\n# ------------------------------------------------------------------\n# 1) HSCA curve\ntry:\n    plt.figure()\n    x = np.arange(len(train_hsca))\n    plt.plot(x, train_hsca, label=\"Train HSCA\")\n    plt.plot(x, val_hsca, label=\"Val HSCA\")\n    plt.xlabel(\"Run index\")\n    plt.ylabel(\"HSCA\")\n    plt.title(\"SPR_BENCH \u2013 Hyper-parameter sweep HSCA\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_HSCA_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HSCA curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Loss curve\ntry:\n    plt.figure()\n    x = np.arange(len(train_loss))\n    plt.plot(x, train_loss, label=\"Train Loss\")\n    plt.plot(x, val_loss, label=\"Val Loss\")\n    plt.xlabel(\"Run index\")\n    plt.ylabel(\"Average CE Loss\")\n    plt.title(\"SPR_BENCH \u2013 Training/Validation Losses\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_Loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Scatter of LR pairs vs Val HSCA\ntry:\n    plt.figure()\n    sc = plt.scatter(lr_pairs[:, 0], lr_pairs[:, 1], c=val_hsca, cmap=\"viridis\", s=80)\n    plt.colorbar(sc, label=\"Validation HSCA\")\n    plt.xscale(\"log\")\n    plt.yscale(\"log\")\n    plt.xlabel(\"Pre-training LR\")\n    plt.ylabel(\"Supervised LR\")\n    plt.title(\"SPR_BENCH \u2013 LR grid search (colour = Val HSCA)\")\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_LR_scatter.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating LR scatter: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print best result\nif val_hsca.size:\n    best_idx = val_hsca.argmax()\n    best_pair = lr_pairs[best_idx]\n    print(\n        f\"Best val HSCA={val_hsca[best_idx]:.4f} at pre_lr={best_pair[0]:.0e}, \"\n        f\"sup_lr={best_pair[1]:.0e}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# Load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp_dict = experiment_data[\"batch_size\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp_dict = {}\n\n# Skip plotting if nothing is loaded\nif exp_dict:\n    tags = sorted(exp_dict.keys())  # e.g. ['SPR_BENCH_bs64', ...]\n    epochs_sup = max(len(v[\"losses\"][\"train\"]) for v in exp_dict.values())\n    # --------------------------------------------------------\n    # 1) Training-loss curves\n    try:\n        plt.figure()\n        for tag in tags:\n            losses = exp_dict[tag][\"losses\"][\"train\"]\n            plt.plot(range(1, len(losses) + 1), losses, label=tag.split(\"_\")[-1])\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training Loss vs Epoch\")\n        plt.legend(title=\"Batch Size\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_training_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating training-loss plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 2) Dev HSCA curves\n    try:\n        plt.figure()\n        for tag in tags:\n            hsca = exp_dict[tag][\"metrics\"][\"train\"]  # dev HSCA per epoch\n            plt.plot(range(1, len(hsca) + 1), hsca, label=tag.split(\"_\")[-1])\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HSCA\")\n        plt.title(\"SPR_BENCH \u2013 Dev HSCA vs Epoch\")\n        plt.legend(title=\"Batch Size\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_dev_HSCA_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HSCA curve plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 3) Final test HSCA bar chart\n    final_scores = {tag: exp_dict[tag][\"metrics\"][\"val\"][0] for tag in tags}\n    try:\n        plt.figure()\n        xs = np.arange(len(tags))\n        plt.bar(xs, [final_scores[t] for t in tags], color=\"skyblue\")\n        plt.xticks(xs, [t.split(\"_\")[-1] for t in tags])\n        plt.ylabel(\"HSCA\")\n        plt.title(\"SPR_BENCH \u2013 Final Test HSCA by Batch Size\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_final_test_HSCA.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating HSCA bar chart: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 4) Confusion matrix for best model\n    try:\n        best_tag = max(final_scores, key=final_scores.get)\n        preds = np.array(exp_dict[best_tag][\"predictions\"])\n        gts = np.array(exp_dict[best_tag][\"ground_truth\"])\n        cm = np.zeros((2, 2), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f'SPR_BENCH \u2013 Confusion Matrix (Best {best_tag.split(\"_\")[-1]})')\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.xticks([0, 1], [\"Class 0\", \"Class 1\"])\n        plt.yticks([0, 1], [\"Class 0\", \"Class 1\"])\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        fname = os.path.join(\n            working_dir, f'SPR_BENCH_confusion_best_{best_tag.split(\"_\")[-1]}.png'\n        )\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # Print final metrics\n    print(\"Final Test HSCA by batch size:\")\n    for t, v in final_scores.items():\n        print(f\"{t}: {v:.4f}\")\nelse:\n    print(\"No experiment data available \u2013 nothing to plot.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    sweep = experiment_data[\"contrastive_pretraining_epochs\"][\"SPR_BENCH\"]\n    epochs = sweep[\"epochs\"]  # list of pre-training epoch counts\n    dev_hsca = sweep[\"metrics\"][\"train\"]  # dev HSCA recorded after fine-tune\n    test_hsca = sweep[\"metrics\"][\"val\"]  # test HSCA\n    train_losses = sweep[\"losses\"][\"train\"]  # mean supervised loss\n\n    print(\"epochs      :\", epochs)\n    print(\"dev_hsca    :\", dev_hsca)\n    print(\"test_hsca   :\", test_hsca)\n    print(\"train_losses:\", train_losses)\n\n    # --------------------------------------------------------------\n    # 1) Dev HSCA vs pre-training epochs\n    try:\n        plt.figure()\n        plt.plot(epochs, dev_hsca, marker=\"o\")\n        plt.title(\"SPR_BENCH \u2013 Dev HSCA vs Contrastive Pre-training Epochs\")\n        plt.xlabel(\"Contrastive pre-training epochs\")\n        plt.ylabel(\"Dev HSCA\")\n        plt.grid(True)\n        fn = os.path.join(working_dir, \"SPR_BENCH_dev_hsca_vs_pretrain_epochs.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Dev HSCA plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 2) Test HSCA vs pre-training epochs\n    try:\n        plt.figure()\n        plt.plot(epochs, test_hsca, marker=\"s\", color=\"orange\")\n        plt.title(\"SPR_BENCH \u2013 Test HSCA vs Contrastive Pre-training Epochs\")\n        plt.xlabel(\"Contrastive pre-training epochs\")\n        plt.ylabel(\"Test HSCA\")\n        plt.grid(True)\n        fn = os.path.join(working_dir, \"SPR_BENCH_test_hsca_vs_pretrain_epochs.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Test HSCA plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 3) Mean supervised loss vs pre-training epochs\n    try:\n        plt.figure()\n        plt.plot(epochs, train_losses, marker=\"^\", color=\"green\")\n        plt.title(\"SPR_BENCH \u2013 Mean Fine-tune Loss vs Contrastive Pre-training Epochs\")\n        plt.xlabel(\"Contrastive pre-training epochs\")\n        plt.ylabel(\"Mean CE Loss (training)\")\n        plt.grid(True)\n        fn = os.path.join(working_dir, \"SPR_BENCH_loss_vs_pretrain_epochs.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating Loss plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---- Load data ---------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nruns = experiment_data.get(\"encoder_hidden_dim\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No experiment data found, aborting plot generation.\")\n    exit()\n\nhidden_dims = sorted([(int(k.split(\"_\")[-1]), k) for k in runs.keys()])\n# gather metrics\ntrain_hsca, train_loss, test_hsca = {}, {}, {}\nfor hdim, key in hidden_dims:\n    train_hsca[hdim] = runs[key][\"metrics\"][\"train\"]\n    train_loss[hdim] = runs[key][\"losses\"][\"train\"]\n    test_hsca[hdim] = (\n        runs[key][\"metrics\"][\"val\"][0] if runs[key][\"metrics\"][\"val\"] else np.nan\n    )\n\n# ---- Plot 1: HSCA vs epoch ---------------------------------------\ntry:\n    plt.figure()\n    for hdim in train_hsca:\n        plt.plot(\n            range(1, len(train_hsca[hdim]) + 1), train_hsca[hdim], label=f\"hid={hdim}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.title(\"SPR_BENCH \u2013 Training HSCA vs Epoch\\n(Each line: encoder hidden dim)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_HSCA_vs_Epoch.png\")\n    plt.savefig(fname)\nexcept Exception as e:\n    print(f\"Error creating HSCA plot: {e}\")\nfinally:\n    plt.close()\n\n# ---- Plot 2: Loss vs epoch ---------------------------------------\ntry:\n    plt.figure()\n    for hdim in train_loss:\n        plt.plot(\n            range(1, len(train_loss[hdim]) + 1), train_loss[hdim], label=f\"hid={hdim}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH \u2013 Training Loss vs Epoch\\n(Each line: encoder hidden dim)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_TrainLoss_vs_Epoch.png\")\n    plt.savefig(fname)\nexcept Exception as e:\n    print(f\"Error creating Loss plot: {e}\")\nfinally:\n    plt.close()\n\n# ---- Plot 3: Test HSCA bar chart ---------------------------------\ntry:\n    plt.figure()\n    dims = list(test_hsca.keys())\n    scores = [test_hsca[d] for d in dims]\n    plt.bar([str(d) for d in dims], scores, color=\"skyblue\")\n    plt.xlabel(\"Encoder Hidden Dim\")\n    plt.ylabel(\"Test HSCA\")\n    plt.title(\"SPR_BENCH \u2013 Final Test HSCA per Hidden Dim\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_Test_HSCA_Bar.png\")\n    plt.savefig(fname)\nexcept Exception as e:\n    print(f\"Error creating Test HSCA bar plot: {e}\")\nfinally:\n    plt.close()\n\n# ---- Plot 4: Confusion matrix of best run ------------------------\ntry:\n    best_dim = max(test_hsca, key=test_hsca.get)\n    best_key = f\"hid_{best_dim}\"\n    preds = np.array(runs[best_key][\"predictions\"])\n    gts = np.array(runs[best_key][\"ground_truth\"])\n    cm = np.zeros((2, 2), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\n        f\"SPR_BENCH \u2013 Confusion Matrix (Best hid={best_dim})\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    fname = os.path.join(working_dir, f\"SPR_BENCH_ConfusionMatrix_hid{best_dim}.png\")\n    plt.savefig(fname)\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\nfinally:\n    plt.close()\n\n# ---- Print summary metrics ---------------------------------------\nprint(\"HiddenDim -> Test HSCA\")\nfor hdim in sorted(test_hsca):\n    print(f\"{hdim:>4} -> {test_hsca[hdim]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntemp_dict = experiment_data.get(\"contrastive_temperature\", {})\ntemperatures = sorted([float(t) for t in temp_dict.keys()])\ndev_curves, test_scores, train_losses = {}, {}, {}\n\nfor t_str, d in temp_dict.items():\n    dev_curves[float(t_str)] = d[\"metrics\"][\"train\"]  # Dev HSCA per epoch\n    test_scores[float(t_str)] = d[\"metrics\"][\"val\"][0]  # Final Test HSCA\n    train_losses[float(t_str)] = d[\"losses\"][\"train\"]  # Supervised train loss\n\nbest_temp = max(test_scores, key=test_scores.get) if test_scores else None\n\n# ------------------------------------------------------------------\n# 1) Dev HSCA curves\ntry:\n    plt.figure()\n    for t in temperatures:\n        epochs = np.arange(1, len(dev_curves[t]) + 1)\n        plt.plot(epochs, dev_curves[t], label=f\"\u03c4={t}\")\n    plt.xlabel(\"Fine-tuning Epoch\")\n    plt.ylabel(\"HSCA (Dev)\")\n    plt.title(\"SPR_BENCH Synthetic \u2013 Dev HSCA vs Epoch (Contrastive Temperature Sweep)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"sprbench_dev_hsca_vs_epoch.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating Dev HSCA plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Final Test HSCA vs temperature\ntry:\n    plt.figure()\n    temps = list(test_scores.keys())\n    scores = [test_scores[t] for t in temps]\n    plt.bar([str(t) for t in temps], scores)\n    plt.xlabel(\"Contrastive Temperature \u03c4\")\n    plt.ylabel(\"HSCA (Test)\")\n    plt.title(\"SPR_BENCH Synthetic \u2013 Final Test HSCA across Temperatures\")\n    fname = os.path.join(working_dir, \"sprbench_test_hsca_vs_temperature.png\")\n    plt.savefig(fname)\n    plt.close()\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating Test HSCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Training loss curve for best temperature\ntry:\n    if best_temp is not None:\n        plt.figure()\n        epochs = np.arange(1, len(train_losses[best_temp]) + 1)\n        plt.plot(epochs, train_losses[best_temp], marker=\"o\")\n        plt.xlabel(\"Fine-tuning Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"SPR_BENCH Synthetic \u2013 Training Loss vs Epoch (Best \u03c4={best_temp})\")\n        fname = os.path.join(\n            working_dir, f\"sprbench_train_loss_best_tau_{best_temp}.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating training loss plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------\n# Load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    ed = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    decays = ed[\"decays\"]\n    train_loss_lists = ed[\"losses\"][\"train\"]  # list of lists\n    train_hsca_lists = ed[\"metrics\"][\"train\"]  # list of lists\n    test_hsca_vals = ed[\"metrics\"][\"val\"]  # list of floats\n\n    epochs = range(1, len(train_loss_lists[0]) + 1)  # assumed equal length\n\n    # ----------------------------------------------------------- Plot 1\n    try:\n        plt.figure()\n        for d, loss_curve in zip(decays, train_loss_lists):\n            plt.plot(epochs, loss_curve, marker=\"o\", label=f\"decay={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"Train Loss Curves \u2013 SPR_BENCH\\nWeight Decay Sweep\")\n        plt.legend()\n        fpath = os.path.join(working_dir, \"SPR_BENCH_train_loss_weight_decay.png\")\n        plt.savefig(fpath)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train-loss plot: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------- Plot 2\n    try:\n        plt.figure()\n        for d, metric_curve in zip(decays, train_hsca_lists):\n            plt.plot(epochs, metric_curve, marker=\"s\", label=f\"decay={d}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"HSCA\")\n        plt.title(\"Train HSCA Curves \u2013 SPR_BENCH\\nWeight Decay Sweep\")\n        plt.legend()\n        fpath = os.path.join(working_dir, \"SPR_BENCH_train_HSCA_weight_decay.png\")\n        plt.savefig(fpath)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train-HSCA plot: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------- Plot 3\n    try:\n        plt.figure()\n        plt.bar([str(d) for d in decays], test_hsca_vals, color=\"skyblue\")\n        plt.xlabel(\"Weight Decay\")\n        plt.ylabel(\"Test HSCA\")\n        plt.title(\"Test HSCA vs Weight Decay \u2013 SPR_BENCH\")\n        fpath = os.path.join(working_dir, \"SPR_BENCH_test_HSCA_vs_decay.png\")\n        plt.savefig(fpath)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test-HSCA plot: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------- Print summary\n    print(\"Weight Decay  |  Test HSCA\")\n    for d, h in zip(decays, test_hsca_vals):\n        print(f\"{d:<12} :  {h:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nbench_key = (\"encoder_embedding_dim\", \"SPR_BENCH\")\ndims = sorted(experiment_data.get(bench_key[0], {}).get(bench_key[1], {}).keys())\n\n# Gather data\nmetrics, losses, final_scores = {}, {}, {}\nfor d in dims:\n    rec = experiment_data[bench_key[0]][bench_key[1]][d]\n    metrics[d] = rec[\"metrics\"][\"train\"]  # list of HSCA per epoch\n    losses[d] = rec[\"losses\"][\"train\"]  # list of loss per epoch\n    final_scores[d] = rec[\"metrics\"][\"val\"][-1] if rec[\"metrics\"][\"val\"] else np.nan\n\n# ------------------------------------------------------------------\n# 1) Training HSCA curves\ntry:\n    plt.figure()\n    for d in dims:\n        plt.plot(range(1, len(metrics[d]) + 1), metrics[d], label=f\"dim={d}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.title(\"SPR_BENCH: Training HSCA vs Epoch\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_train_HSCA_vs_epoch.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HSCA plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) Training loss curves\ntry:\n    plt.figure()\n    for d in dims:\n        plt.plot(range(1, len(losses[d]) + 1), losses[d], label=f\"dim={d}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training Loss vs Epoch\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_train_loss_vs_epoch.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Final test HSCA bar chart\ntry:\n    plt.figure()\n    xs = np.arange(len(dims))\n    ys = [final_scores[d] for d in dims]\n    plt.bar(xs, ys, tick_label=dims)\n    plt.ylabel(\"HSCA\")\n    plt.title(\"SPR_BENCH: Final Test HSCA per Embedding Dim\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_test_HSCA.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final HSCA bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Console output of final scores\nfor d in dims:\n    print(f\"Embedding dim {d}: Final Test HSCA = {final_scores[d]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------\nspr_key = (\"supervised_finetuning_epochs\", \"SPR_BENCH\")\ntry:\n    sweep = experiment_data[spr_key[0]][spr_key[1]]\n    epochs_grid = sweep[\"epochs_grid\"]\n    train_hsca = sweep[\"metrics\"][\"train\"]  # list of lists\n    val_hsca = sweep[\"metrics\"][\"val\"]  # list of lists\n    test_hsca = sweep[\"test_hsca\"]  # list\nexcept Exception as e:\n    print(f\"Error extracting data: {e}\")\n    epochs_grid, train_hsca, val_hsca, test_hsca = [], [], [], []\n\n# ------------------------------------------------------------------\n# Plot 1: train / val HSCA curves\ntry:\n    plt.figure()\n    colors = plt.cm.tab10(np.linspace(0, 1, len(epochs_grid)))\n    for i, max_ep in enumerate(epochs_grid):\n        ep_axis = np.arange(1, len(train_hsca[i]) + 1)\n        plt.plot(ep_axis, train_hsca[i], color=colors[i], label=f\"{max_ep}ep train\")\n        plt.plot(\n            ep_axis,\n            val_hsca[i],\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"{max_ep}ep val\",\n        )\n    plt.title(\"SPR_BENCH \u2013 HSCA Curves\\nSolid: Train, Dashed: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.legend(ncol=2, fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_hsca_train_val_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HSCA curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Plot 2: test HSCA vs max epochs\ntry:\n    plt.figure()\n    plt.bar([str(e) for e in epochs_grid], test_hsca, color=\"steelblue\")\n    plt.title(\"SPR_BENCH \u2013 Test HSCA vs Allowed Fine-tuning Epochs\")\n    plt.xlabel(\"Max Fine-tuning Epochs\")\n    plt.ylabel(\"Test HSCA\")\n    for x, y in zip(range(len(epochs_grid)), test_hsca):\n        plt.text(x, y + 0.01, f\"{y:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_hsca_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test HSCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print evaluation metrics\nfor max_ep, hsca in zip(epochs_grid, test_hsca):\n    print(f\"Max epochs={max_ep:2d} | Test HSCA={hsca:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------\nspr_key = (\"supervised_finetuning_epochs\", \"SPR_BENCH\")\ntry:\n    sweep = experiment_data[spr_key[0]][spr_key[1]]\n    epochs_grid = sweep[\"epochs_grid\"]\n    train_hsca = sweep[\"metrics\"][\"train\"]  # list of lists\n    val_hsca = sweep[\"metrics\"][\"val\"]  # list of lists\n    test_hsca = sweep[\"test_hsca\"]  # list\nexcept Exception as e:\n    print(f\"Error extracting data: {e}\")\n    epochs_grid, train_hsca, val_hsca, test_hsca = [], [], [], []\n\n# ------------------------------------------------------------------\n# Plot 1: train / val HSCA curves\ntry:\n    plt.figure()\n    colors = plt.cm.tab10(np.linspace(0, 1, len(epochs_grid)))\n    for i, max_ep in enumerate(epochs_grid):\n        ep_axis = np.arange(1, len(train_hsca[i]) + 1)\n        plt.plot(ep_axis, train_hsca[i], color=colors[i], label=f\"{max_ep}ep train\")\n        plt.plot(\n            ep_axis,\n            val_hsca[i],\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"{max_ep}ep val\",\n        )\n    plt.title(\"SPR_BENCH \u2013 HSCA Curves\\nSolid: Train, Dashed: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.legend(ncol=2, fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_hsca_train_val_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HSCA curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Plot 2: test HSCA vs max epochs\ntry:\n    plt.figure()\n    plt.bar([str(e) for e in epochs_grid], test_hsca, color=\"steelblue\")\n    plt.title(\"SPR_BENCH \u2013 Test HSCA vs Allowed Fine-tuning Epochs\")\n    plt.xlabel(\"Max Fine-tuning Epochs\")\n    plt.ylabel(\"Test HSCA\")\n    for x, y in zip(range(len(epochs_grid)), test_hsca):\n        plt.text(x, y + 0.01, f\"{y:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_hsca_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test HSCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print evaluation metrics\nfor max_ep, hsca in zip(epochs_grid, test_hsca):\n    print(f\"Max epochs={max_ep:2d} | Test HSCA={hsca:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------\nspr_key = (\"supervised_finetuning_epochs\", \"SPR_BENCH\")\ntry:\n    sweep = experiment_data[spr_key[0]][spr_key[1]]\n    epochs_grid = sweep[\"epochs_grid\"]\n    train_hsca = sweep[\"metrics\"][\"train\"]  # list of lists\n    val_hsca = sweep[\"metrics\"][\"val\"]  # list of lists\n    test_hsca = sweep[\"test_hsca\"]  # list\nexcept Exception as e:\n    print(f\"Error extracting data: {e}\")\n    epochs_grid, train_hsca, val_hsca, test_hsca = [], [], [], []\n\n# ------------------------------------------------------------------\n# Plot 1: train / val HSCA curves\ntry:\n    plt.figure()\n    colors = plt.cm.tab10(np.linspace(0, 1, len(epochs_grid)))\n    for i, max_ep in enumerate(epochs_grid):\n        ep_axis = np.arange(1, len(train_hsca[i]) + 1)\n        plt.plot(ep_axis, train_hsca[i], color=colors[i], label=f\"{max_ep}ep train\")\n        plt.plot(\n            ep_axis,\n            val_hsca[i],\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"{max_ep}ep val\",\n        )\n    plt.title(\"SPR_BENCH \u2013 HSCA Curves\\nSolid: Train, Dashed: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.legend(ncol=2, fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_hsca_train_val_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating HSCA curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Plot 2: test HSCA vs max epochs\ntry:\n    plt.figure()\n    plt.bar([str(e) for e in epochs_grid], test_hsca, color=\"steelblue\")\n    plt.title(\"SPR_BENCH \u2013 Test HSCA vs Allowed Fine-tuning Epochs\")\n    plt.xlabel(\"Max Fine-tuning Epochs\")\n    plt.ylabel(\"Test HSCA\")\n    for x, y in zip(range(len(epochs_grid)), test_hsca):\n        plt.text(x, y + 0.01, f\"{y:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_hsca_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test HSCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print evaluation metrics\nfor max_ep, hsca in zip(epochs_grid, test_hsca):\n    print(f\"Max epochs={max_ep:2d} | Test HSCA={hsca:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# List of experiment data paths relative to $AI_SCIENTIST_ROOT\nexperiment_data_path_list = [\n    \"experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8b1d16c5a96546e481d660ca48e4700b_proc_3069202/experiment_data.npy\",\n    \"experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6822c11b92d8462c9e3bb65826a85a13_proc_3069201/experiment_data.npy\",\n    \"experiments/2025-08-16_00-46-17_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a54f9e8b65c741868428553a88742bf8_proc_3069204/experiment_data.npy\",\n]\n\nall_sweeps = []\nspr_key = (\"supervised_finetuning_epochs\", \"SPR_BENCH\")\n\n# ------------------------------------------------------------------\n# Load every run\ntry:\n    root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n    for p in experiment_data_path_list:\n        try:\n            ed = np.load(os.path.join(root, p), allow_pickle=True).item()\n            sweep = ed[spr_key[0]][spr_key[1]]\n            all_sweeps.append(sweep)\n        except Exception as e:\n            print(f\"Error loading/reading {p}: {e}\")\nexcept Exception as e:\n    print(f\"Error iterating over experiment paths: {e}\")\n\nif len(all_sweeps) == 0:\n    print(\"No experiment data could be loaded \u2013 exiting.\")\n    exit()\n\n# ------------------------------------------------------------------\n# Consistency check and aggregation\ntry:\n    epochs_grid = all_sweeps[0][\"epochs_grid\"]\n    n_runs = len(all_sweeps)\n    n_settings = len(epochs_grid)\n\n    # Containers: [run, setting, epoch]\n    train_curves = []\n    val_curves = []\n    test_scores = []\n\n    for sweep in all_sweeps:\n        if sweep[\"epochs_grid\"] != epochs_grid:\n            raise ValueError(\"Mismatch in epochs_grid across runs.\")\n        train_curves.append(sweep[\"metrics\"][\"train\"])\n        val_curves.append(sweep[\"metrics\"][\"val\"])\n        test_scores.append(sweep[\"test_hsca\"])\n\n    train_curves = np.array(train_curves, dtype=object)\n    val_curves = np.array(val_curves, dtype=object)\n    test_scores = np.array(test_scores)  # shape [runs, settings]\n\n    # Convert per-setting variable-length lists to fixed arrays (trim to min length)\n    min_len = min([min([len(c) for c in run_curves]) for run_curves in train_curves])\n    train_mean = []\n    train_sem = []\n    val_mean = []\n    val_sem = []\n    for s in range(n_settings):\n        tc = np.stack([run[s][:min_len] for run in train_curves])\n        vc = np.stack([run[s][:min_len] for run in val_curves])\n        train_mean.append(tc.mean(axis=0))\n        train_sem.append(tc.std(axis=0, ddof=1) / np.sqrt(n_runs))\n        val_mean.append(vc.mean(axis=0))\n        val_sem.append(vc.std(axis=0, ddof=1) / np.sqrt(n_runs))\n\n    train_mean = np.array(train_mean)\n    train_sem = np.array(train_sem)\n    val_mean = np.array(val_mean)\n    val_sem = np.array(val_sem)\n\n    test_mean = test_scores.mean(axis=0)  # shape [settings]\n    test_sem = test_scores.std(axis=0, ddof=1) / np.sqrt(n_runs)\nexcept Exception as e:\n    print(f\"Error during aggregation: {e}\")\n\n# ------------------------------------------------------------------\n# Plot 1: aggregated train / val HSCA curves with SEM bands\ntry:\n    plt.figure()\n    colors = plt.cm.tab10(np.linspace(0, 1, n_settings))\n    epoch_axis = np.arange(1, min_len + 1)\n    for i, max_ep in enumerate(epochs_grid):\n        # Train curve\n        plt.plot(epoch_axis, train_mean[i], color=colors[i], label=f\"{max_ep}ep train\")\n        plt.fill_between(\n            epoch_axis,\n            train_mean[i] - train_sem[i],\n            train_mean[i] + train_sem[i],\n            color=colors[i],\n            alpha=0.2,\n        )\n        # Val curve\n        plt.plot(\n            epoch_axis,\n            val_mean[i],\n            linestyle=\"--\",\n            color=colors[i],\n            label=f\"{max_ep}ep val\",\n        )\n        plt.fill_between(\n            epoch_axis,\n            val_mean[i] - val_sem[i],\n            val_mean[i] + val_sem[i],\n            color=colors[i],\n            alpha=0.2,\n            linestyle=\"--\",\n        )\n\n    plt.title(\n        \"SPR_BENCH \u2013 Aggregated HSCA Curves\\nSolid: Train, Dashed: Validation (Mean \u00b1 SEM)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"HSCA\")\n    plt.legend(ncol=2, fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_hsca_train_val_aggregated.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated HSCA curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Plot 2: test HSCA vs max epochs with SEM error bars\ntry:\n    plt.figure()\n    x_pos = np.arange(n_settings)\n    plt.bar(\n        x_pos,\n        test_mean,\n        yerr=test_sem,\n        color=\"steelblue\",\n        capsize=5,\n        alpha=0.9,\n        ecolor=\"black\",\n    )\n    plt.title(\"SPR_BENCH \u2013 Test HSCA vs Allowed Fine-tuning Epochs (Mean \u00b1 SEM)\")\n    plt.xlabel(\"Max Fine-tuning Epochs\")\n    plt.ylabel(\"Test HSCA\")\n    plt.xticks(x_pos, [str(e) for e in epochs_grid])\n    for x, y in zip(x_pos, test_mean):\n        plt.text(x, y + 0.01, f\"{y:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_hsca_bar_aggregated.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated test HSCA bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print aggregated evaluation metrics\nprint(\"Aggregated Test HSCA (mean \u00b1 SEM):\")\nfor max_ep, m, s in zip(epochs_grid, test_mean, test_sem):\n    print(f\"Max epochs={max_ep:2d} | Test HSCA={m:.4f} \u00b1 {s:.4f}\")\n"], "plot_plan": ["We will first load the stored experiment_data.npy, extract the lists of training losses and harmonic SCA (HSCA) scores, and check that data exist before plotting. Two separate plots\u2014(1) HSCA over epochs and (2) training loss over epochs\u2014will be created inside independent try-except blocks, each saved with descriptive filenames in the working directory and closed afterward. Titles will clearly state the dataset and what the figure shows. If a validation/test HSCA series is present it will be plotted together with the training curve; otherwise only the available series appears. After plotting we print the final HSCA on the test split (stored under metrics['val'] in the sample script). All code relies solely on the content of experiment_data.npy, uses plain matplotlib, and respects the five-figure maximum.", "We will first load experiment_data.npy and extract the supervised_finetuning_epochs entry for SPR_BENCH, retrieving the epochs_grid, per-run train/val HSCA histories, and the final test HSCA scores.  \nThe first figure will visualise, for each run (\u22644), both training and validation HSCA across epochs so users can see convergence and potential overfitting; each run\u2019s two curves share the same colour but use solid (train) vs dashed (val) styles.  \nThe second figure shows the final test HSCA obtained at the end of early-stopping for every value in epochs_grid, plotted as a bar chart so improvements vs allowed epochs are obvious.  \nAll plotting is wrapped in independent try-except blocks, figures get closed, and files are saved to working_dir with descriptive names that include the dataset and plot type.  \nNo data is fabricated; everything comes directly from experiment_data.npy.  \nWe also print the test HSCA scores to stdout, fulfilling the \u201cprints out the evaluation metric(s)\u201d requirement.  \nOnly basic matplotlib calls are used, respecting the developer instructions, and the code remains concise and self-contained.", "We will first load the saved experiment_data.npy, extract the lists of train/val HSCA scores, train/val losses and the (pre-lr, sup-lr) pairs.  \nA simple line plot will show how HSCA evolves across the 16 hyper-parameter runs, with separate curves for train and val.  \nA second line plot will show the corresponding average training losses (validation losses are only placeholders, so we still draw them for completeness).  \nTo visualise the influence of each learning-rate combination we add a scatter plot: points are located at (pre_lr, sup_lr) and coloured by the achieved validation HSCA; the colourbar gives an immediate sense of which regions perform best.  \nEach figure is wrapped in its own try-except block, saved under an informative name inside working_dir, and then closed to free resources.  \nOnly data present in experiment_data.npy are used; no synthetic values are introduced.  \nFinally, we print out the best validation HSCA and the corresponding learning-rate pair so the user can directly see the optimal setting.", "We will load the saved experiment_data.npy file, iterate over the four batch-size experiments, and extract (1) training losses, (2) dev HSCA across epochs, and (3) the final test HSCA value.  Two multi-line plots will visualise the loss curves and the HSCA curves, letting us compare optimisation behaviour for different batch sizes.  A third bar-chart will summarise the final HSCA scores.  To give a qualitative feel for model errors, the experiment with the highest HSCA is selected, its prediction/ground-truth pairs are converted into a 2\u00d72 confusion matrix, and this is rendered as a heat-map.  Each figure is wrapped in its own try-except block, given a descriptive title that mentions the SPR_BENCH dataset, and saved to the working directory before the figure is closed.  After plotting, the script prints a small table of the final HSCA values so the numeric results appear in the notebook log.  No synthetic data are created\u2014everything comes directly from experiment_data.npy\u2014and at most four figures are generated, satisfying the \u201c\u22645 plots\u201d rule.", "We will load the saved experiment_data.npy, extract the list of contrastive pre-training epochs and, for each epoch count, the corresponding development HSCA, test HSCA and mean supervised loss that were stored during the sweep.  \nThree simple plots convey the key story:  \n1) Development HSCA versus number of pre-training epochs (how representation quality on dev varies).  \n2) Test HSCA versus number of pre-training epochs (generalisation).  \n3) Mean supervised loss versus number of pre-training epochs (training stability).  \nEach figure is created inside its own try-except block, titled clearly and saved into working_dir with descriptive filenames that include \u2018SPR_BENCH\u2019.  \nWe cap ourselves at these three figures (well under the limit of five).  \nFigures are closed in all cases to avoid memory leaks, and only data present in the .npy file are used\u2014no synthetic values are invented.  \nFinally, the script prints the arrays so the user can quickly verify the numbers alongside the plots.", "We will load experiment_data.npy, iterate over the four hidden-dim runs, and collect: (a) per-epoch training HSCA, (b) per-epoch training loss, and (c) final test HSCA together with predictions/ground-truth.  \nFour figures will be produced, each wrapped in its own try-except:  \n1) SPR_BENCH_HSCA_vs_Epoch: multilines of HSCA curves for every hidden dim.  \n2) SPR_BENCH_TrainLoss_vs_Epoch: analogous multilines for loss.  \n3) SPR_BENCH_Test_HSCA_Bar: bar chart of final test HSCA per hidden dim.  \n4) SPR_BENCH_ConfusionMatrix_BestRun: confusion matrix for the run that achieved the highest test HSCA.  \nAll plots are saved in working_dir with descriptive filenames, and each figure is closed in a finally-style clause.  \nAfter plotting, the script prints a concise table of hidden-dim \u2192 test HSCA so users can quickly read the quantitative results from stdout.", "The code first loads experiment_data.npy, extracts the per-temperature metrics/losses, and identifies the best temperature by the stored test HSCA. It then creates three plots: (1) Dev HSCA versus epoch for all temperatures, (2) final Test HSCA versus temperature, and (3) supervised training loss versus epoch for the best temperature. Each figure is produced inside its own try-except block, titled clearly, saved to working_dir with descriptive filenames, and properly closed afterwards. No values are fabricated; only data present in the file are used, and no more than three figures are generated.", "We load the saved experiment_data.npy, extract the weight decay grid, per-epoch training losses, per-epoch HSCA metrics, and final test HSCA values. Three standard visualizations are generated: (1) training loss curves across epochs for every weight-decay setting, (2) training HSCA curves likewise, and (3) a bar chart of final test HSCA versus weight decay. Each plot is enclosed in its own try-except block, titled with the dataset name and plot type, saved to the working directory with descriptive filenames, and the figure is always closed. After plotting, the script prints a concise table of weight decay values and their corresponding test HSCA so users can quickly view the key evaluation metric. No synthetic data are invented; only the arrays present in experiment_data.npy are used. The code complies with the specified import order, path handling, and figure-saving conventions, while limiting the total number of generated figures to three.", "We will first load the saved experiment_data.npy, then loop over the three tested embedding dimensions (64, 128, 256). For each dimension we extract the lists stored in experiment_data[\"encoder_embedding_dim\"][\"SPR_BENCH\"][dim]. We will create at most three plots: (1) Training HSCA versus epoch for all dimensions on one figure; (2) Training loss versus epoch for all dimensions; (3) A bar chart comparing the final test HSCA for each embedding size. Each plotting section is wrapped in its own try-except block to avoid crashing if any key is missing. We save the figures inside working_dir with descriptive names that include the dataset (SPR_BENCH) and plot type, and we always close figures afterward. No values are fabricated\u2014everything comes directly from experiment_data.npy. At the end the script prints the final test HSCA values so the user can see them in the console. The code is concise, uses only basic matplotlib, and respects all provided guidelines.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "supervised_finetuning_epochs", "learning_rate", "batch_size", "contrastive_pretraining_epochs", "encoder_hidden_dim", "contrastive_temperature", "weight_decay", "encoder_embedding_dim", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The solution loads the saved NumPy dictionary, iterates over each dataset\ncontained in it, and extracts the numeric lists that store training/validation\nharmonic SCA scores and cross-entropy losses. For every list it computes the\n\u201cbest\u201d value (maximum for scores, minimum for losses), because those reflect\noptimal performance. It then prints the dataset name followed by clearly-\nlabelled lines such as \u201cBest train harmonic SCA\u201d or \u201cBest validation cross-\nentropy loss,\u201d thus satisfying the requirement for explicit metric names. No\nplotting or special entry point is used, so the script runs immediately when\nexecuted.", "The script will locate and load the NumPy file from the working directory,\nconvert the stored object back to a Python dict, and then iterate through the\nsupervised-fine-tuning results that belong to the \u201cSPR_BENCH\u201d dataset.   For\neach sweep (i.e., for every value in epochs_grid) it extracts: (1) the final\ntrain HSCA, (2) the best validation HSCA, (3) the final training loss, and (4)\nthe recorded test HSCA.   It then prints these values with explicit metric\nnames, always prefixing the output block with the dataset identifier (including\nthe sweep\u2019s max-epoch value for clarity).", "The script first locates the saved NumPy file in the \u201cworking\u201d directory, loads\nit as a Python dictionary, and then iterates through every dataset recorded\nunder the lr-tuning experiments.   For each dataset (here only \u201cSPR_BENCH\u201d), it\nextracts the lists that hold per-run development HSCA, test HSCA, and training\nloss values.   Because higher HSCA and lower loss are better, the code reports\nthe maximum HSCA and the minimum training loss, clearly labeling each metric\nbefore printing.   Execution starts immediately since everything is in the\nglobal scope and no special entry point is used.", "The script will locate the working directory, load the saved NumPy file into a\nPython dict, iterate over every stored experiment (each different batch-size\nsetting), and print the key metrics in a readable way.   For every experiment it\nreports:   \u2022 best development (validation) HSCA achieved during fine-tuning,   \u2022\nfinal test HSCA,   \u2022 final training loss after the last epoch.   Metric names\nare explicitly spelled out so that no ambiguity remains.", "Below we load the saved numpy dictionary, navigate its nested structure, and\nprint the most informative numbers: the best (highest) harmonic SCA achieved\nduring fine-tuning and the lowest cross-entropy loss observed, for both the\ntraining and validation splits of every dataset that was logged. The script\nfollows the directory and formatting requirements and runs immediately when\nexecuted.", "The script loads the saved NumPy dictionary, drills down to the\n\u201cencoder_hidden_dim \u2192 SPR_BENCH \u2192 <run_key>\u201d hierarchy, and for every hidden-\ndimension run prints (a) the best training harmonic shape\u2013color accuracy, (b)\nthe final training loss, and (c) the final test harmonic shape\u2013color accuracy.\nAll values are clearly labeled, and execution starts immediately because\neverything is at global scope.", "The script below loads the saved NumPy file from the working directory, loops\nthrough every temperature setting in the experiment, and cleanly reports the\nmost informative numbers. For each temperature it prints the best development\nharmonic shape-color accuracy, the last supervised training loss, the final test\nharmonic shape-color accuracy, and the last contrastive pre-training loss. All\nprinting is preceded by an explicit dataset name and a clear metric label, and\nthe code runs immediately when executed.", "The script will (1) locate the working directory, (2) load the stored numpy\ndictionary, (3) iterate through each dataset contained under the weight-decay\nsweep, and (4) for every weight-decay setting print the final training loss, the\nfinal validation harmonic SCA, and the test harmonic SCA. All prints include\nexplicit metric names and the dataset name appears once before its block of\nmetrics.", "We will load the saved NumPy dictionary, navigate to the single dataset\n(\u201cSPR_BENCH\u201d), loop over every tried embedding dimension, and extract:   \u2022 the\nbest (maximum) train HSCA recorded,   \u2022 the test HSCA (only value stored in\nmetrics[\"val\"]),   \u2022 the lowest train loss observed, and   \u2022 the validation loss\nif it exists.   Each value is printed with a clear, explicit metric label,\npreceded by the dataset name so the output is self-explanatory and complies with\nthe formatting rules.", "The script will locate and load the NumPy file from the working directory,\nconvert the stored object back to a Python dict, and then iterate through the\nsupervised-fine-tuning results that belong to the \u201cSPR_BENCH\u201d dataset.   For\neach sweep (i.e., for every value in epochs_grid) it extracts: (1) the final\ntrain HSCA, (2) the best validation HSCA, (3) the final training loss, and (4)\nthe recorded test HSCA.   It then prints these values with explicit metric\nnames, always prefixing the output block with the dataset identifier (including\nthe sweep\u2019s max-epoch value for clarity).", "The script will locate and load the NumPy file from the working directory,\nconvert the stored object back to a Python dict, and then iterate through the\nsupervised-fine-tuning results that belong to the \u201cSPR_BENCH\u201d dataset.   For\neach sweep (i.e., for every value in epochs_grid) it extracts: (1) the final\ntrain HSCA, (2) the best validation HSCA, (3) the final training loss, and (4)\nthe recorded test HSCA.   It then prints these values with explicit metric\nnames, always prefixing the output block with the dataset identifier (including\nthe sweep\u2019s max-epoch value for clarity).", "The script will locate and load the NumPy file from the working directory,\nconvert the stored object back to a Python dict, and then iterate through the\nsupervised-fine-tuning results that belong to the \u201cSPR_BENCH\u201d dataset.   For\neach sweep (i.e., for every value in epochs_grid) it extracts: (1) the final\ntrain HSCA, (2) the best validation HSCA, (3) the final training loss, and (4)\nthe recorded test HSCA.   It then prints these values with explicit metric\nnames, always prefixing the output block with the dataset identifier (including\nthe sweep\u2019s max-epoch value for clarity).", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper: decide whether to maximise or minimise\ndef select_best(values, higher_is_better=True):\n    \"\"\"Return the best value from a list according to the optimisation direction.\"\"\"\n    if not values:\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# ------------------------------------------------------------------\n# Pretty-print the desired numbers\nfor dataset_name, dataset_blob in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle score metrics (higher is better)\n    for split, values in dataset_blob.get(\"metrics\", {}).items():\n        best_val = select_best(values, higher_is_better=True)\n        if best_val is not None:\n            print(f\"Best {split} harmonic SCA: {best_val:.4f}\")\n\n    # Handle loss metrics (lower is better)\n    for split, values in dataset_blob.get(\"losses\", {}).items():\n        best_val = select_best(values, higher_is_better=False)\n        if best_val is not None:\n            print(f\"Best {split} cross-entropy loss: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. locate file and load\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. traverse and print metrics\nfine_tune_section = experiment_data[\"supervised_finetuning_epochs\"]\n\nfor dataset_name, ds in fine_tune_section.items():\n    epochs_grid = ds[\"epochs_grid\"]\n    train_hsca_runs = ds[\"metrics\"][\"train\"]\n    val_hsca_runs = ds[\"metrics\"][\"val\"]\n    train_loss_runs = ds[\"losses\"][\"train\"]\n    test_hsca_runs = ds[\"test_hsca\"]\n\n    for run_idx, max_ep in enumerate(epochs_grid):\n        # safely get lists (could be shorter due to early stopping)\n        train_hsca_hist = train_hsca_runs[run_idx]\n        val_hsca_hist = val_hsca_runs[run_idx]\n        train_loss_hist = train_loss_runs[run_idx]\n        test_hsca_value = test_hsca_runs[run_idx]\n\n        # derive requested statistics\n        final_train_hsca = train_hsca_hist[-1] if train_hsca_hist else None\n        best_val_hsca = max(val_hsca_hist) if val_hsca_hist else None\n        final_train_loss = train_loss_hist[-1] if train_loss_hist else None\n\n        # 3\u20134. print with clear labels\n        print(f\"\\nDataset: {dataset_name} (fine-tuning max_epochs={max_ep})\")\n        print(\n            f\"train HSCA (final): {final_train_hsca:.4f}\"\n            if final_train_hsca is not None\n            else \"train HSCA (final): N/A\"\n        )\n        print(\n            f\"validation HSCA (best): {best_val_hsca:.4f}\"\n            if best_val_hsca is not None\n            else \"validation HSCA (best): N/A\"\n        )\n        print(\n            f\"train loss (final): {final_train_loss:.4f}\"\n            if final_train_loss is not None\n            else \"train loss (final): N/A\"\n        )\n        print(\n            f\"test HSCA: {test_hsca_value:.4f}\"\n            if test_hsca_value is not None\n            else \"test HSCA: N/A\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the stored experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Parse and print metrics\nfor dataset_name, dataset_info in experiment_data[\"lr_tuning\"].items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract metric/loss lists\n    dev_hsca_list = dataset_info[\"metrics\"][\"train\"]  # development HSCA\n    test_hsca_list = dataset_info[\"metrics\"][\"val\"]  # test HSCA\n    train_loss_list = dataset_info[\"losses\"][\"train\"]  # training loss\n\n    # Best values (max for HSCA, min for loss)\n    if dev_hsca_list:\n        best_dev_hsca = max(dev_hsca_list)\n        print(f\"Best development HSCA: {best_dev_hsca:.4f}\")\n\n    if test_hsca_list:\n        best_test_hsca = max(test_hsca_list)\n        print(f\"Best test HSCA: {best_test_hsca:.4f}\")\n\n    if train_loss_list:\n        best_train_loss = min(train_loss_list)\n        print(f\"Lowest training loss: {best_train_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# Locate and read the saved experiment dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\ndef print_metrics(exp_dict):\n    \"\"\"\n    exp_dict has the structure:\n      experiment_data[\"batch_size\"][tag] = {\n          \"metrics\": {\"train\": [...], \"val\": [hsca_test]},\n          \"losses\":  {\"train\": [...], \"val\": []},\n          ...\n      }\n    \"\"\"\n    for tag, results in exp_dict.items():\n        metrics = results.get(\"metrics\", {})\n        losses = results.get(\"losses\", {})\n\n        # Derive values\n        train_hsca_vals = metrics.get(\"train\", [])\n        test_hsca_vals = metrics.get(\"val\", [])\n\n        best_dev_hsca = max(train_hsca_vals) if train_hsca_vals else None\n        final_test_hsca = test_hsca_vals[-1] if test_hsca_vals else None\n\n        train_loss_vals = losses.get(\"train\", [])\n        final_train_loss = train_loss_vals[-1] if train_loss_vals else None\n\n        # ------------------- Printing -------------------\n        print(f\"\\nDataset: {tag}\")\n\n        if best_dev_hsca is not None:\n            print(f\"best validation HSCA: {best_dev_hsca:.4f}\")\n        if final_test_hsca is not None:\n            print(f\"test HSCA: {final_test_hsca:.4f}\")\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n\n\n# ------------------------------------------------------------\n# Execute immediately\nprint_metrics(experiment_data.get(\"batch_size\", {}))\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef safe_best(values, mode=\"max\"):\n    \"\"\"Return best (max or min) value from a list, or None if the list is empty.\"\"\"\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\n# ------------------------------------------------------------------\nfor (\n    sweep_name,\n    datasets,\n) in experiment_data.items():  # e.g. 'contrastive_pretraining_epochs'\n    for dataset_name, result_dict in datasets.items():  # e.g. 'SPR_BENCH'\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---------- Metrics ----------\n        metrics = result_dict.get(\"metrics\", {})\n        train_hsca_values = metrics.get(\"train\", [])\n        val_hsca_values = metrics.get(\"val\", [])\n\n        best_train_hsca = safe_best(train_hsca_values, mode=\"max\")\n        best_val_hsca = safe_best(val_hsca_values, mode=\"max\")\n\n        if best_train_hsca is not None:\n            print(f\"Best train harmonic SCA: {best_train_hsca:.4f}\")\n        if best_val_hsca is not None:\n            print(f\"Best validation harmonic SCA: {best_val_hsca:.4f}\")\n\n        # ---------- Losses ----------\n        losses = result_dict.get(\"losses\", {})\n        train_loss_values = losses.get(\"train\", [])\n        val_loss_values = losses.get(\"val\", [])\n\n        min_train_loss = safe_best(train_loss_values, mode=\"min\")\n        min_val_loss = safe_best(val_loss_values, mode=\"min\")\n\n        if min_train_loss is not None:\n            print(f\"Minimum train cross-entropy loss: {min_train_loss:.4f}\")\n        if min_val_loss is not None:\n            print(f\"Minimum validation cross-entropy loss: {min_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Extract information\ntop_key = \"encoder_hidden_dim\"\ndataset_name = \"SPR_BENCH\"\n\nruns_dict = experiment_data.get(top_key, {}).get(dataset_name, {})\n\nprint(f\"DATASET: {dataset_name}\")\nfor run_key, run_info in runs_dict.items():\n    metrics = run_info.get(\"metrics\", {})\n    losses = run_info.get(\"losses\", {})\n\n    train_hsca_list = metrics.get(\"train\", [])\n    val_hsca_list = metrics.get(\"val\", [])\n    train_loss_list = losses.get(\"train\", [])\n\n    # Best / final values\n    best_train_hsca = max(train_hsca_list) if train_hsca_list else None\n    final_train_loss = train_loss_list[-1] if train_loss_list else None\n    final_test_hsca = val_hsca_list[-1] if val_hsca_list else None\n\n    # ------------------------------------------------------------------\n    # Printing\n    print(f\"\\nEXPERIMENT: {run_key}\")\n    if best_train_hsca is not None:\n        print(f\"  train harmonic SCA: {best_train_hsca:.4f}\")\n    if final_train_loss is not None:\n        print(f\"  training loss: {final_train_loss:.4f}\")\n    if final_test_hsca is not None:\n        print(f\"  test harmonic SCA: {final_test_hsca:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load experiment results\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate through every temperature setting and report metrics\nfor temp_str, result in experiment_data[\"contrastive_temperature\"].items():\n    metrics = result.get(\"metrics\", {})\n    losses = result.get(\"losses\", {})\n\n    print(f\"\\nTemperature \u03c4 = {temp_str}\")\n\n    # ---------------- Development set (stored under the 'train' key)\n    dev_hsca_arr = metrics.get(\"train\", np.array([]))\n    dev_loss_arr = losses.get(\"train\", np.array([]))\n\n    if dev_hsca_arr.size:\n        best_dev_hsca = dev_hsca_arr.max()\n    else:\n        best_dev_hsca = float(\"nan\")\n\n    if dev_loss_arr.size:\n        final_sup_loss = dev_loss_arr[-1]\n    else:\n        final_sup_loss = float(\"nan\")\n\n    print(\"Development Set:\")\n    print(f\"  best harmonic shape-color accuracy (HSCA): {best_dev_hsca:.4f}\")\n    print(f\"  final supervised training loss: {final_sup_loss:.4f}\")\n\n    # ---------------- Test set (stored under the 'val' key)\n    test_hsca_arr = metrics.get(\"val\", np.array([]))\n    contrast_loss_arr = losses.get(\"val\", np.array([]))\n\n    if test_hsca_arr.size:\n        test_hsca = test_hsca_arr[-1]\n    else:\n        test_hsca = float(\"nan\")\n\n    if contrast_loss_arr.size:\n        final_contrast_loss = contrast_loss_arr[-1]\n    else:\n        final_contrast_loss = float(\"nan\")\n\n    print(\"Test Set:\")\n    print(f\"  harmonic shape-color accuracy (HSCA): {test_hsca:.4f}\")\n    print(f\"  final contrastive pre-training loss: {final_contrast_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Traverse the data structure and report metrics\nweight_decay_block = experiment_data.get(\"weight_decay\", {})\n\nfor dataset_name, data_dict in weight_decay_block.items():\n    print(f\"Dataset: {dataset_name}\")  # dataset header\n\n    decays = data_dict[\"decays\"]\n    train_loss_lists = data_dict[\"losses\"][\"train\"]  # list of lists\n    val_metric_lists = data_dict[\"metrics\"][\"train\"]  # dev HSCA per epoch\n    test_hsca_values = data_dict[\"metrics\"][\"val\"]  # test HSCA (single value)\n\n    for idx, wd_value in enumerate(decays):\n        # Safeguard in case lists are empty\n        final_train_loss = train_loss_lists[idx][-1] if train_loss_lists[idx] else None\n        final_val_hsca = val_metric_lists[idx][-1] if val_metric_lists[idx] else None\n        test_hsca = test_hsca_values[idx]\n\n        print(f\"  Weight decay: {wd_value}\")\n        if final_train_loss is not None:\n            print(f\"    final training loss: {final_train_loss:.4f}\")\n        if final_val_hsca is not None:\n            print(f\"    final validation harmonic SCA: {final_val_hsca:.4f}\")\n        if test_hsca is not None:\n            print(f\"    test harmonic SCA: {test_hsca:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate through datasets (only one in this example) and print metrics\nfor dataset_name, dataset_dict in experiment_data[\"encoder_embedding_dim\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    # dataset_dict structure: {embed_dim: result_dict}\n    for embed_dim in sorted(dataset_dict.keys()):\n        result = dataset_dict[embed_dim]\n\n        # Metrics\n        train_hsca_history = result[\"metrics\"][\"train\"]  # list over epochs\n        test_hsca_list = result[\"metrics\"][\"val\"]  # single-element list\n        train_loss_history = result[\"losses\"][\"train\"]  # list over epochs\n        val_loss_history = result[\"losses\"][\"val\"]  # may be empty\n\n        best_train_hsca = max(train_hsca_history) if train_hsca_history else None\n        test_hsca = test_hsca_list[-1] if test_hsca_list else None\n        best_train_loss = min(train_loss_history) if train_loss_history else None\n        val_loss = val_loss_history[-1] if val_loss_history else None\n\n        # Print block for this hyper-parameter setting\n        print(f\"  Embedding dimension: {embed_dim}\")\n        if best_train_hsca is not None:\n            print(f\"    best train HSCA: {best_train_hsca:.4f}\")\n        if test_hsca is not None:\n            print(f\"    test HSCA: {test_hsca:.4f}\")\n        if best_train_loss is not None:\n            print(f\"    lowest train loss: {best_train_loss:.4f}\")\n        if val_loss is not None:\n            print(f\"    validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. locate file and load\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. traverse and print metrics\nfine_tune_section = experiment_data[\"supervised_finetuning_epochs\"]\n\nfor dataset_name, ds in fine_tune_section.items():\n    epochs_grid = ds[\"epochs_grid\"]\n    train_hsca_runs = ds[\"metrics\"][\"train\"]\n    val_hsca_runs = ds[\"metrics\"][\"val\"]\n    train_loss_runs = ds[\"losses\"][\"train\"]\n    test_hsca_runs = ds[\"test_hsca\"]\n\n    for run_idx, max_ep in enumerate(epochs_grid):\n        # safely get lists (could be shorter due to early stopping)\n        train_hsca_hist = train_hsca_runs[run_idx]\n        val_hsca_hist = val_hsca_runs[run_idx]\n        train_loss_hist = train_loss_runs[run_idx]\n        test_hsca_value = test_hsca_runs[run_idx]\n\n        # derive requested statistics\n        final_train_hsca = train_hsca_hist[-1] if train_hsca_hist else None\n        best_val_hsca = max(val_hsca_hist) if val_hsca_hist else None\n        final_train_loss = train_loss_hist[-1] if train_loss_hist else None\n\n        # 3\u20134. print with clear labels\n        print(f\"\\nDataset: {dataset_name} (fine-tuning max_epochs={max_ep})\")\n        print(\n            f\"train HSCA (final): {final_train_hsca:.4f}\"\n            if final_train_hsca is not None\n            else \"train HSCA (final): N/A\"\n        )\n        print(\n            f\"validation HSCA (best): {best_val_hsca:.4f}\"\n            if best_val_hsca is not None\n            else \"validation HSCA (best): N/A\"\n        )\n        print(\n            f\"train loss (final): {final_train_loss:.4f}\"\n            if final_train_loss is not None\n            else \"train loss (final): N/A\"\n        )\n        print(\n            f\"test HSCA: {test_hsca_value:.4f}\"\n            if test_hsca_value is not None\n            else \"test HSCA: N/A\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. locate file and load\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. traverse and print metrics\nfine_tune_section = experiment_data[\"supervised_finetuning_epochs\"]\n\nfor dataset_name, ds in fine_tune_section.items():\n    epochs_grid = ds[\"epochs_grid\"]\n    train_hsca_runs = ds[\"metrics\"][\"train\"]\n    val_hsca_runs = ds[\"metrics\"][\"val\"]\n    train_loss_runs = ds[\"losses\"][\"train\"]\n    test_hsca_runs = ds[\"test_hsca\"]\n\n    for run_idx, max_ep in enumerate(epochs_grid):\n        # safely get lists (could be shorter due to early stopping)\n        train_hsca_hist = train_hsca_runs[run_idx]\n        val_hsca_hist = val_hsca_runs[run_idx]\n        train_loss_hist = train_loss_runs[run_idx]\n        test_hsca_value = test_hsca_runs[run_idx]\n\n        # derive requested statistics\n        final_train_hsca = train_hsca_hist[-1] if train_hsca_hist else None\n        best_val_hsca = max(val_hsca_hist) if val_hsca_hist else None\n        final_train_loss = train_loss_hist[-1] if train_loss_hist else None\n\n        # 3\u20134. print with clear labels\n        print(f\"\\nDataset: {dataset_name} (fine-tuning max_epochs={max_ep})\")\n        print(\n            f\"train HSCA (final): {final_train_hsca:.4f}\"\n            if final_train_hsca is not None\n            else \"train HSCA (final): N/A\"\n        )\n        print(\n            f\"validation HSCA (best): {best_val_hsca:.4f}\"\n            if best_val_hsca is not None\n            else \"validation HSCA (best): N/A\"\n        )\n        print(\n            f\"train loss (final): {final_train_loss:.4f}\"\n            if final_train_loss is not None\n            else \"train loss (final): N/A\"\n        )\n        print(\n            f\"test HSCA: {test_hsca_value:.4f}\"\n            if test_hsca_value is not None\n            else \"test HSCA: N/A\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. locate file and load\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. traverse and print metrics\nfine_tune_section = experiment_data[\"supervised_finetuning_epochs\"]\n\nfor dataset_name, ds in fine_tune_section.items():\n    epochs_grid = ds[\"epochs_grid\"]\n    train_hsca_runs = ds[\"metrics\"][\"train\"]\n    val_hsca_runs = ds[\"metrics\"][\"val\"]\n    train_loss_runs = ds[\"losses\"][\"train\"]\n    test_hsca_runs = ds[\"test_hsca\"]\n\n    for run_idx, max_ep in enumerate(epochs_grid):\n        # safely get lists (could be shorter due to early stopping)\n        train_hsca_hist = train_hsca_runs[run_idx]\n        val_hsca_hist = val_hsca_runs[run_idx]\n        train_loss_hist = train_loss_runs[run_idx]\n        test_hsca_value = test_hsca_runs[run_idx]\n\n        # derive requested statistics\n        final_train_hsca = train_hsca_hist[-1] if train_hsca_hist else None\n        best_val_hsca = max(val_hsca_hist) if val_hsca_hist else None\n        final_train_loss = train_loss_hist[-1] if train_loss_hist else None\n\n        # 3\u20134. print with clear labels\n        print(f\"\\nDataset: {dataset_name} (fine-tuning max_epochs={max_ep})\")\n        print(\n            f\"train HSCA (final): {final_train_hsca:.4f}\"\n            if final_train_hsca is not None\n            else \"train HSCA (final): N/A\"\n        )\n        print(\n            f\"validation HSCA (best): {best_val_hsca:.4f}\"\n            if best_val_hsca is not None\n            else \"validation HSCA (best): N/A\"\n        )\n        print(\n            f\"train loss (final): {final_train_loss:.4f}\"\n            if final_train_loss is not None\n            else \"train loss (final): N/A\"\n        )\n        print(\n            f\"test HSCA: {test_hsca_value:.4f}\"\n            if test_hsca_value is not None\n            else \"test HSCA: N/A\"\n        )\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'Best train harmonic SCA: 0.7071', '\\n', 'Best\nval harmonic SCA: 0.7002', '\\n', 'Best train cross-entropy loss: 0.5580', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH (fine-tuning max_epochs=5)', '\\n', 'train HSCA (final):\n0.8217', '\\n', 'validation HSCA (best): 0.7567', '\\n', 'train loss (final):\n0.5278', '\\n', 'test HSCA: 0.7530', '\\n', '\\nDataset: SPR_BENCH (fine-tuning\nmax_epochs=10)', '\\n', 'train HSCA (final): 0.9588', '\\n', 'validation HSCA\n(best): 0.9226', '\\n', 'train loss (final): 0.2392', '\\n', 'test HSCA: 0.9419',\n'\\n', '\\nDataset: SPR_BENCH (fine-tuning max_epochs=15)', '\\n', 'train HSCA\n(final): 0.9784', '\\n', 'validation HSCA (best): 0.9301', '\\n', 'train loss\n(final): 0.1187', '\\n', 'test HSCA: 0.9488', '\\n', '\\nDataset: SPR_BENCH (fine-\ntuning max_epochs=20)', '\\n', 'train HSCA (final): 0.9962', '\\n', 'validation\nHSCA (best): 0.9630', '\\n', 'train loss (final): 0.0468', '\\n', 'test HSCA:\n0.9709', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best development HSCA: 0.9313', '\\n', 'Best test\nHSCA: 0.9351', '\\n', 'Lowest training loss: 0.3339', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH_bs64', '\\n', 'best validation HSCA: 0.9104', '\\n', 'test\nHSCA: 0.9386', '\\n', 'final training loss: 0.3730', '\\n', '\\nDataset:\nSPR_BENCH_bs128', '\\n', 'best validation HSCA: 0.7506', '\\n', 'test HSCA:\n0.7462', '\\n', 'final training loss: 0.5105', '\\n', '\\nDataset:\nSPR_BENCH_bs256', '\\n', 'best validation HSCA: 0.6255', '\\n', 'test HSCA:\n0.6433', '\\n', 'final training loss: 0.6189', '\\n', '\\nDataset:\nSPR_BENCH_bs512', '\\n', 'best validation HSCA: 0.6245', '\\n', 'test HSCA:\n0.6212', '\\n', 'final training loss: 0.6398', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best train harmonic SCA: 0.7821', '\\n', 'Best\nvalidation harmonic SCA: 0.8087', '\\n', 'Minimum train cross-entropy loss:\n0.6099', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['DATASET: SPR_BENCH', '\\n', '\\nEXPERIMENT: hid_64', '\\n', '  train harmonic\nSCA: 0.6493', '\\n', '  training loss: 0.6049', '\\n', '  test harmonic SCA:\n0.6349', '\\n', '\\nEXPERIMENT: hid_128', '\\n', '  train harmonic SCA: 0.7268',\n'\\n', '  training loss: 0.5240', '\\n', '  test harmonic SCA: 0.7143', '\\n',\n'\\nEXPERIMENT: hid_256', '\\n', '  train harmonic SCA: 0.8445', '\\n', '  training\nloss: 0.4023', '\\n', '  test harmonic SCA: 0.8501', '\\n', '\\nEXPERIMENT:\nhid_512', '\\n', '  train harmonic SCA: 0.9129', '\\n', '  training loss: 0.2309',\n'\\n', '  test harmonic SCA: 0.9393', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nTemperature \u03c4 = 0.05', '\\n', 'Development Set:', '\\n', '  best harmonic\nshape-color accuracy (HSCA): 0.7733', '\\n', '  final supervised training loss:\n0.5100', '\\n', 'Test Set:', '\\n', '  harmonic shape-color accuracy (HSCA):\n0.7845', '\\n', '  final contrastive pre-training loss: 1.6492', '\\n',\n'\\nTemperature \u03c4 = 0.08', '\\n', 'Development Set:', '\\n', '  best harmonic\nshape-color accuracy (HSCA): 0.7787', '\\n', '  final supervised training loss:\n0.5146', '\\n', 'Test Set:', '\\n', '  harmonic shape-color accuracy (HSCA):\n0.7767', '\\n', '  final contrastive pre-training loss: 1.7156', '\\n',\n'\\nTemperature \u03c4 = 0.1', '\\n', 'Development Set:', '\\n', '  best harmonic shape-\ncolor accuracy (HSCA): 0.7789', '\\n', '  final supervised training loss:\n0.5162', '\\n', 'Test Set:', '\\n', '  harmonic shape-color accuracy (HSCA):\n0.7751', '\\n', '  final contrastive pre-training loss: 1.8599', '\\n',\n'\\nTemperature \u03c4 = 0.15', '\\n', 'Development Set:', '\\n', '  best harmonic\nshape-color accuracy (HSCA): 0.7809', '\\n', '  final supervised training loss:\n0.5118', '\\n', 'Test Set:', '\\n', '  harmonic shape-color accuracy (HSCA):\n0.7723', '\\n', '  final contrastive pre-training loss: 2.4043', '\\n',\n'\\nTemperature \u03c4 = 0.2', '\\n', 'Development Set:', '\\n', '  best harmonic shape-\ncolor accuracy (HSCA): 0.7834', '\\n', '  final supervised training loss:\n0.5125', '\\n', 'Test Set:', '\\n', '  harmonic shape-color accuracy (HSCA):\n0.7881', '\\n', '  final contrastive pre-training loss: 2.9117', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Weight decay: 0.0', '\\n', '    final training\nloss: 0.5219', '\\n', '    final validation harmonic SCA: 0.8170', '\\n', '\ntest harmonic SCA: 0.8175', '\\n', '  Weight decay: 1e-05', '\\n', '    final\ntraining loss: 0.5512', '\\n', '    final validation harmonic SCA: 0.7513', '\\n',\n'    test harmonic SCA: 0.7764', '\\n', '  Weight decay: 0.0001', '\\n', '\nfinal training loss: 0.5498', '\\n', '    final validation harmonic SCA: 0.7839',\n'\\n', '    test harmonic SCA: 0.7727', '\\n', '  Weight decay: 0.0005', '\\n', '\nfinal training loss: 0.5084', '\\n', '    final validation harmonic SCA: 0.8304',\n'\\n', '    test harmonic SCA: 0.8499', '\\n', '  Weight decay: 0.001', '\\n', '\nfinal training loss: 0.6125', '\\n', '    final validation harmonic SCA: 0.6928',\n'\\n', '    test harmonic SCA: 0.6850', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Embedding dimension: 64', '\\n', '    best train\nHSCA: 0.7309', '\\n', '    test HSCA: 0.7263', '\\n', '    lowest train loss:\n0.5843', '\\n', '  Embedding dimension: 128', '\\n', '    best train HSCA:\n0.8032', '\\n', '    test HSCA: 0.7616', '\\n', '    lowest train loss: 0.5396',\n'\\n', '  Embedding dimension: 256', '\\n', '    best train HSCA: 0.8684', '\\n', '\ntest HSCA: 0.8816', '\\n', '    lowest train loss: 0.4402', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH (fine-tuning max_epochs=5)', '\\n', 'train HSCA (final):\n0.8482', '\\n', 'validation HSCA (best): 0.7885', '\\n', 'train loss (final):\n0.5280', '\\n', 'test HSCA: 0.7935', '\\n', '\\nDataset: SPR_BENCH (fine-tuning\nmax_epochs=10)', '\\n', 'train HSCA (final): 0.9583', '\\n', 'validation HSCA\n(best): 0.9393', '\\n', 'train loss (final): 0.2271', '\\n', 'test HSCA: 0.9423',\n'\\n', '\\nDataset: SPR_BENCH (fine-tuning max_epochs=15)', '\\n', 'train HSCA\n(final): 0.9888', '\\n', 'validation HSCA (best): 0.9468', '\\n', 'train loss\n(final): 0.0822', '\\n', 'test HSCA: 0.9622', '\\n', '\\nDataset: SPR_BENCH (fine-\ntuning max_epochs=20)', '\\n', 'train HSCA (final): 0.9620', '\\n', 'validation\nHSCA (best): 0.9370', '\\n', 'train loss (final): 0.1754', '\\n', 'test HSCA:\n0.9397', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH (fine-tuning max_epochs=5)', '\\n', 'train HSCA (final):\n0.8222', '\\n', 'validation HSCA (best): 0.7570', '\\n', 'train loss (final):\n0.5220', '\\n', 'test HSCA: 0.7633', '\\n', '\\nDataset: SPR_BENCH (fine-tuning\nmax_epochs=10)', '\\n', 'train HSCA (final): 0.9626', '\\n', 'validation HSCA\n(best): 0.9300', '\\n', 'train loss (final): 0.1882', '\\n', 'test HSCA: 0.9412',\n'\\n', '\\nDataset: SPR_BENCH (fine-tuning max_epochs=15)', '\\n', 'train HSCA\n(final): 0.9920', '\\n', 'validation HSCA (best): 0.9535', '\\n', 'train loss\n(final): 0.0756', '\\n', 'test HSCA: 0.9711', '\\n', '\\nDataset: SPR_BENCH (fine-\ntuning max_epochs=20)', '\\n', 'train HSCA (final): 0.9997', '\\n', 'validation\nHSCA (best): 0.9747', '\\n', 'train loss (final): 0.0281', '\\n', 'test HSCA:\n0.9800', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH (fine-tuning max_epochs=5)', '\\n', 'train HSCA (final):\n0.7310', '\\n', 'validation HSCA (best): 0.7232', '\\n', 'train loss (final):\n0.5497', '\\n', 'test HSCA: 0.7114', '\\n', '\\nDataset: SPR_BENCH (fine-tuning\nmax_epochs=10)', '\\n', 'train HSCA (final): 0.9584', '\\n', 'validation HSCA\n(best): 0.9352', '\\n', 'train loss (final): 0.2417', '\\n', 'test HSCA: 0.9446',\n'\\n', '\\nDataset: SPR_BENCH (fine-tuning max_epochs=15)', '\\n', 'train HSCA\n(final): 0.9885', '\\n', 'validation HSCA (best): 0.9521', '\\n', 'train loss\n(final): 0.0841', '\\n', 'test HSCA: 0.9566', '\\n', '\\nDataset: SPR_BENCH (fine-\ntuning max_epochs=20)', '\\n', 'train HSCA (final): 0.9639', '\\n', 'validation\nHSCA (best): 0.9370', '\\n', 'train loss (final): 0.1740', '\\n', 'test HSCA:\n0.9265', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}