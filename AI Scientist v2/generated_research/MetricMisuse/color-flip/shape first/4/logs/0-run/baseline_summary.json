{
  "best node": {
    "overall_plan": "Hyperparam tuning name: epochs.\nWe explore the \u201cepochs\u201d hyper-parameter by training several identical LSTM models for 5, 10, 20 and 30 epochs, logging train/validation loss and HWA after every epoch.  \nEach epoch\u2010setting gets its own entry in experiment_data['epochs']['SPR_BENCH'] so results are easy to compare.  \nThe script is completely self-contained: it builds/loads the data, trains, evaluates, stores everything in experiment_data.npy and can run on CPU or GPU.",
    "analysis": "The execution successfully completed without any errors or bugs. The training process was carried out for different epoch configurations (5, 10, 20, and 30 epochs), and the results were logged. The harmonic weighted accuracy (HWA) metric was used as the key evaluation metric, and the model's performance was tracked over epochs. The results show expected behavior with some overfitting observed in longer training durations. The experiment data was successfully saved to a file for further analysis.",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "training loss",
            "lower_is_better": true,
            "description": "Measures the error during training; lower values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH (epochs = 5)",
                "final_value": 0.633,
                "best_value": 0.633
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 10)",
                "final_value": 0.5117,
                "best_value": 0.5117
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 20)",
                "final_value": 0.2033,
                "best_value": 0.2033
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 30)",
                "final_value": 0.0171,
                "best_value": 0.0171
              }
            ]
          },
          {
            "metric_name": "validation loss",
            "lower_is_better": true,
            "description": "Measures the error during validation; lower values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH (epochs = 5)",
                "final_value": 0.6444,
                "best_value": 0.6444
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 10)",
                "final_value": 0.6824,
                "best_value": 0.6824
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 20)",
                "final_value": 1.1331,
                "best_value": 1.1331
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 30)",
                "final_value": 1.3624,
                "best_value": 1.3624
              }
            ]
          },
          {
            "metric_name": "validation harmonic weighted accuracy",
            "lower_is_better": false,
            "description": "Measures the weighted accuracy during validation; higher values indicate better performance.",
            "data": [
              {
                "dataset_name": "SPR_BENCH (epochs = 5)",
                "final_value": 0.582,
                "best_value": 0.582
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 10)",
                "final_value": 0.64,
                "best_value": 0.64
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 20)",
                "final_value": 0.636,
                "best_value": 0.636
              },
              {
                "dataset_name": "SPR_BENCH (epochs = 30)",
                "final_value": 0.668,
                "best_value": 0.668
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "import os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset, DatasetDict\n\n# ----------------- experiment data dict -----------------\nexperiment_data = {\n    \"epochs\": {\"SPR_BENCH\": {}}  # hyper-parameter we sweep  # dataset name\n}\n\n# ----------------- device & seeds -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper metrics -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(swa, cwa):\n    return 2 * swa * cwa / (swa + cwa + 1e-12)\n\n\n# ----------------- data loading -----------------\ndef load_or_create_dataset():\n    root = pathlib.Path(\"SPR_BENCH\")\n    if root.exists():\n        from SPR import load_spr_bench\n\n        return load_spr_bench(root)\n\n    # fallback tiny synthetic set\n    def gen_row(_id):\n        length = random.randint(4, 9)\n        shapes, colors = \"ABCD\", \"abcd\"\n        seq = \" \".join(\n            random.choice(shapes) + random.choice(colors) for _ in range(length)\n        )\n        return {\"id\": _id, \"sequence\": seq, \"label\": int(length % 2)}\n\n    train_rows = [gen_row(i) for i in range(600)]\n    dev_rows = [gen_row(1000 + i) for i in range(200)]\n    test_rows = [gen_row(2000 + i) for i in range(200)]\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_list(train_rows),\n            \"dev\": HFDataset.from_list(dev_rows),\n            \"test\": HFDataset.from_list(test_rows),\n        }\n    )\n\n\nspr = load_or_create_dataset()\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------------- vocabulary -----------------\nall_text = \" \".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text.split()))\ntok2idx = {tok: i + 2 for i, tok in enumerate(vocab)}  # 0 PAD, 1 UNK\ntok2idx[\"<PAD>\"], tok2idx[\"<UNK>\"] = 0, 1\nidx2tok = {i: t for t, i in tok2idx.items()}\nvocab_size = len(tok2idx)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(seq):\n    return [tok2idx.get(tok, 1) for tok in seq.strip().split()]\n\n\n# ----------------- torch dataset -----------------\nclass SPRTorchSet(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate(batch):\n    xs = [b[\"x\"] for b in batch]\n    lens = [len(x) for x in xs]\n    maxlen = max(lens)\n    xs_pad = torch.zeros(len(xs), maxlen, dtype=torch.long)\n    for i, x in enumerate(xs):\n        xs_pad[i, : len(x)] = x\n    ys = torch.stack([b[\"y\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    return {\n        \"x\": xs_pad.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"y\": ys.to(device),\n        \"raw_seq\": raw,\n    }\n\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    SPRTorchSet(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchSet(spr[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\nn_classes = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Num classes: {n_classes}\")\n\n\n# ----------------- model -----------------\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab, emb=64, hid=128, n_out=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n        self.fc = nn.Linear(hid, n_out)\n\n    def forward(self, x, lengths):\n        em = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            em, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (h, _) = self.lstm(packed)\n        return self.fc(h[-1])\n\n\n# ----------------- train / evaluate one run -----------------\ndef run_training(num_epochs):\n    model = LSTMClassifier(vocab_size, n_out=n_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n    run_rec = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        tloss = 0.0\n        for batch in train_loader:\n            optim.zero_grad()\n            out = model(batch[\"x\"], batch[\"len\"])\n            loss = criterion(out, batch[\"y\"])\n            loss.backward()\n            optim.step()\n            tloss += loss.item() * batch[\"y\"].size(0)\n        train_loss = tloss / len(train_loader.dataset)\n\n        # eval\n        model.eval()\n        vloss, all_seq, y_true, y_pred = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                out = model(batch[\"x\"], batch[\"len\"])\n                loss = criterion(out, batch[\"y\"])\n                vloss += loss.item() * batch[\"y\"].size(0)\n                preds = torch.argmax(out, 1).cpu().tolist()\n                y_pred.extend(preds)\n                y_true.extend(batch[\"y\"].cpu().tolist())\n                all_seq.extend(batch[\"raw_seq\"])\n        val_loss = vloss / len(dev_loader.dataset)\n        swa = shape_weighted_accuracy(all_seq, y_true, y_pred)\n        cwa = color_weighted_accuracy(all_seq, y_true, y_pred)\n        hwa = harmonic_weighted_accuracy(swa, cwa)\n\n        # record\n        run_rec[\"losses\"][\"train\"].append(train_loss)\n        run_rec[\"losses\"][\"val\"].append(val_loss)\n        run_rec[\"metrics\"][\"val\"].append(hwa)\n        run_rec[\"predictions\"].append(y_pred)\n        run_rec[\"ground_truth\"].append(y_true)\n        run_rec[\"timestamps\"].append(time.time())\n        print(\n            f\"Epochs={num_epochs} | Ep {ep}: tr_loss={train_loss:.4f} val_loss={val_loss:.4f} HWA={hwa:.3f}\"\n        )\n    return run_rec\n\n\n# ----------------- hyper-parameter sweep -----------------\nepoch_options = [5, 10, 20, 30]\nfor e in epoch_options:\n    print(f\"\\n=== Training for {e} epochs ===\")\n    experiment_data[\"epochs\"][\"SPR_BENCH\"][str(e)] = run_training(e)\n\n# ----------------- save -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# setup paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"epochs\"][\"SPR_BENCH\"]  # dict keyed by epoch count\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------- Plot 1: Train / Val loss curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"losses\"][\"train\"]) + 1)\n            plt.plot(\n                x, rec[\"losses\"][\"train\"], label=f\"{run_name}ep-train\", linestyle=\"--\"\n            )\n            plt.plot(x, rec[\"losses\"][\"val\"], label=f\"{run_name}ep-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for loss curves.\")\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\nfinally:\n    plt.close()\n\n# ---------- Plot 2: Validation HWA curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"metrics\"][\"val\"]) + 1)\n            plt.plot(x, rec[\"metrics\"][\"val\"], label=f\"{run_name}ep\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation HWA Across Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_HWA_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for HWA curves.\")\nexcept Exception as e:\n    print(f\"Error creating HWA plot: {e}\")\nfinally:\n    plt.close()\n",
    "plot_plan": "We load experiment_data.npy, extract the four training runs stored under experiment_data['epochs']['SPR_BENCH'], and iterate through each run\u2019s recorded history. For every run we have parallel lists of training losses, validation losses, and validation Harmonic-Weighted-Accuracy (HWA). We create at most two figures: (1) a two-line (train/val) loss curve for every epoch-length setting, all overlaid in a single figure so the reader can compare convergence behaviour; (2) a single HWA curve for each run, likewise overlaid. Each figure is wrapped in an independent try-except block, given a descriptive title that includes the dataset name, saved to working_dir with an informative filename, and closed. We respect the plotting guideline by not inventing data, plotting no more than the available 4 runs (\u22645), always closing figures, and using basic matplotlib defaults. If the data file is missing or malformed the code prints an error but still exits gracefully. All plotting logic is concise and self-contained.",
    "plot_analyses": [
      {
        "analysis": "The training and validation loss curves show a clear trend of overfitting as the number of epochs increases. For the 5-epoch and 10-epoch configurations, the validation loss remains relatively stable and close to the training loss, indicating a good balance between model complexity and data fit. However, for the 20-epoch and 30-epoch configurations, the validation loss starts to increase significantly after around 10 epochs, while the training loss continues to decrease. This divergence suggests that the model is overfitting to the training data, especially in the longer training durations. The 30-epoch configuration exhibits the most pronounced overfitting, with validation loss increasing steeply after approximately 15 epochs. This indicates that early stopping should be implemented to prevent overfitting and improve generalization.",
        "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_05c15e4cd885474784d82b668a6f6b01_proc_3023913/SPR_BENCH_loss_curves.png"
      },
      {
        "analysis": "The harmonic weighted accuracy (HWA) curves provide insights into the model's performance across epochs. The 30-epoch configuration achieves the highest peak HWA value, indicating that it can reach the best performance at some point during training. However, the performance fluctuates significantly, and the HWA decreases as training progresses, which is consistent with the overfitting observed in the loss curves. The 10-epoch configuration demonstrates relatively stable and consistent performance, with less fluctuation compared to the longer training durations. The 5-epoch configuration has the lowest overall HWA, suggesting that it does not allow sufficient time for the model to learn effectively. Based on these observations, the 10-epoch configuration appears to strike the best balance between training duration and model performance, but early stopping should be used to capture the peak performance in the 20-epoch and 30-epoch configurations before overfitting sets in.",
        "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_05c15e4cd885474784d82b668a6f6b01_proc_3023913/SPR_BENCH_HWA_curves.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_05c15e4cd885474784d82b668a6f6b01_proc_3023913/SPR_BENCH_loss_curves.png",
      "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_05c15e4cd885474784d82b668a6f6b01_proc_3023913/SPR_BENCH_HWA_curves.png"
    ],
    "vlm_feedback_summary": "The plots indicate that overfitting is a significant issue in longer training durations, as evidenced by the divergence between training and validation loss. The harmonic weighted accuracy (HWA) metric suggests that while longer training durations can achieve higher peak performance, they also introduce instability and performance degradation over time. Early stopping and refined hyperparameter tuning are recommended to mitigate overfitting and capture optimal performance.",
    "exp_results_dir": "experiment_results/experiment_05c15e4cd885474784d82b668a6f6b01_proc_3023913",
    "exp_results_npy_files": [
      "experiment_results/experiment_05c15e4cd885474784d82b668a6f6b01_proc_3023913/experiment_data.npy"
    ]
  },
  "best node with different seeds": [
    {
      "overall_plan": "The overall plan integrates a previous focus on hyperparameter tuning, specifically examining the 'epochs' parameter in training LSTM models, with a current foundational stage labeled as 'Seed node'. Initially, the plan aimed to determine the optimal number of training epochs by varying them across 5, 10, 20, and 30 epochs, logging metrics such as training/validation loss and HWA for comparison. This phase utilized a self-contained script for flexibility across CPU and GPU environments. The current 'Seed node' suggests setting the stage for more detailed experiments, likely expanding upon the foundational insights gained from the hyperparameter tuning phase.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "training loss",
              "lower_is_better": true,
              "description": "The loss value computed on the training dataset.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.633,
                  "best_value": 0.633
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.5117,
                  "best_value": 0.5117
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 0.2033,
                  "best_value": 0.2033
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 0.0171,
                  "best_value": 0.0171
                }
              ]
            },
            {
              "metric_name": "validation loss",
              "lower_is_better": true,
              "description": "The loss value computed on the validation dataset.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.6444,
                  "best_value": 0.6444
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.6824,
                  "best_value": 0.6824
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 1.1331,
                  "best_value": 1.1331
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 1.3624,
                  "best_value": 1.3624
                }
              ]
            },
            {
              "metric_name": "validation harmonic weighted accuracy",
              "lower_is_better": false,
              "description": "The harmonic weighted accuracy computed on the validation dataset.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.582,
                  "best_value": 0.582
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.64,
                  "best_value": 0.64
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 0.636,
                  "best_value": 0.636
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 0.668,
                  "best_value": 0.668
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset, DatasetDict\n\n# ----------------- experiment data dict -----------------\nexperiment_data = {\n    \"epochs\": {\"SPR_BENCH\": {}}  # hyper-parameter we sweep  # dataset name\n}\n\n# ----------------- device & seeds -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper metrics -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(swa, cwa):\n    return 2 * swa * cwa / (swa + cwa + 1e-12)\n\n\n# ----------------- data loading -----------------\ndef load_or_create_dataset():\n    root = pathlib.Path(\"SPR_BENCH\")\n    if root.exists():\n        from SPR import load_spr_bench\n\n        return load_spr_bench(root)\n\n    # fallback tiny synthetic set\n    def gen_row(_id):\n        length = random.randint(4, 9)\n        shapes, colors = \"ABCD\", \"abcd\"\n        seq = \" \".join(\n            random.choice(shapes) + random.choice(colors) for _ in range(length)\n        )\n        return {\"id\": _id, \"sequence\": seq, \"label\": int(length % 2)}\n\n    train_rows = [gen_row(i) for i in range(600)]\n    dev_rows = [gen_row(1000 + i) for i in range(200)]\n    test_rows = [gen_row(2000 + i) for i in range(200)]\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_list(train_rows),\n            \"dev\": HFDataset.from_list(dev_rows),\n            \"test\": HFDataset.from_list(test_rows),\n        }\n    )\n\n\nspr = load_or_create_dataset()\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------------- vocabulary -----------------\nall_text = \" \".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text.split()))\ntok2idx = {tok: i + 2 for i, tok in enumerate(vocab)}  # 0 PAD, 1 UNK\ntok2idx[\"<PAD>\"], tok2idx[\"<UNK>\"] = 0, 1\nidx2tok = {i: t for t, i in tok2idx.items()}\nvocab_size = len(tok2idx)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(seq):\n    return [tok2idx.get(tok, 1) for tok in seq.strip().split()]\n\n\n# ----------------- torch dataset -----------------\nclass SPRTorchSet(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate(batch):\n    xs = [b[\"x\"] for b in batch]\n    lens = [len(x) for x in xs]\n    maxlen = max(lens)\n    xs_pad = torch.zeros(len(xs), maxlen, dtype=torch.long)\n    for i, x in enumerate(xs):\n        xs_pad[i, : len(x)] = x\n    ys = torch.stack([b[\"y\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    return {\n        \"x\": xs_pad.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"y\": ys.to(device),\n        \"raw_seq\": raw,\n    }\n\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    SPRTorchSet(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchSet(spr[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\nn_classes = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Num classes: {n_classes}\")\n\n\n# ----------------- model -----------------\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab, emb=64, hid=128, n_out=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n        self.fc = nn.Linear(hid, n_out)\n\n    def forward(self, x, lengths):\n        em = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            em, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (h, _) = self.lstm(packed)\n        return self.fc(h[-1])\n\n\n# ----------------- train / evaluate one run -----------------\ndef run_training(num_epochs):\n    model = LSTMClassifier(vocab_size, n_out=n_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n    run_rec = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        tloss = 0.0\n        for batch in train_loader:\n            optim.zero_grad()\n            out = model(batch[\"x\"], batch[\"len\"])\n            loss = criterion(out, batch[\"y\"])\n            loss.backward()\n            optim.step()\n            tloss += loss.item() * batch[\"y\"].size(0)\n        train_loss = tloss / len(train_loader.dataset)\n\n        # eval\n        model.eval()\n        vloss, all_seq, y_true, y_pred = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                out = model(batch[\"x\"], batch[\"len\"])\n                loss = criterion(out, batch[\"y\"])\n                vloss += loss.item() * batch[\"y\"].size(0)\n                preds = torch.argmax(out, 1).cpu().tolist()\n                y_pred.extend(preds)\n                y_true.extend(batch[\"y\"].cpu().tolist())\n                all_seq.extend(batch[\"raw_seq\"])\n        val_loss = vloss / len(dev_loader.dataset)\n        swa = shape_weighted_accuracy(all_seq, y_true, y_pred)\n        cwa = color_weighted_accuracy(all_seq, y_true, y_pred)\n        hwa = harmonic_weighted_accuracy(swa, cwa)\n\n        # record\n        run_rec[\"losses\"][\"train\"].append(train_loss)\n        run_rec[\"losses\"][\"val\"].append(val_loss)\n        run_rec[\"metrics\"][\"val\"].append(hwa)\n        run_rec[\"predictions\"].append(y_pred)\n        run_rec[\"ground_truth\"].append(y_true)\n        run_rec[\"timestamps\"].append(time.time())\n        print(\n            f\"Epochs={num_epochs} | Ep {ep}: tr_loss={train_loss:.4f} val_loss={val_loss:.4f} HWA={hwa:.3f}\"\n        )\n    return run_rec\n\n\n# ----------------- hyper-parameter sweep -----------------\nepoch_options = [5, 10, 20, 30]\nfor e in epoch_options:\n    print(f\"\\n=== Training for {e} epochs ===\")\n    experiment_data[\"epochs\"][\"SPR_BENCH\"][str(e)] = run_training(e)\n\n# ----------------- save -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# setup paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"epochs\"][\"SPR_BENCH\"]  # dict keyed by epoch count\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------- Plot 1: Train / Val loss curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"losses\"][\"train\"]) + 1)\n            plt.plot(\n                x, rec[\"losses\"][\"train\"], label=f\"{run_name}ep-train\", linestyle=\"--\"\n            )\n            plt.plot(x, rec[\"losses\"][\"val\"], label=f\"{run_name}ep-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for loss curves.\")\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\nfinally:\n    plt.close()\n\n# ---------- Plot 2: Validation HWA curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"metrics\"][\"val\"]) + 1)\n            plt.plot(x, rec[\"metrics\"][\"val\"], label=f\"{run_name}ep\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation HWA Across Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_HWA_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for HWA curves.\")\nexcept Exception as e:\n    print(f\"Error creating HWA plot: {e}\")\nfinally:\n    plt.close()\n",
      "plot_analyses": [
        {
          "analysis": "The training versus validation loss curves indicate that increasing the number of epochs beyond 10 leads to overfitting. For 20 and 30 epochs, the validation loss begins to rise after around 10 epochs, while the training loss continues to decrease. This suggests that the model starts fitting noise in the training data instead of generalizing to unseen data. The 5-epoch and 10-epoch configurations show a better balance between training and validation loss, with 10 epochs achieving the lowest validation loss before overfitting begins.",
          "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915/SPR_BENCH_loss_curves.png"
        },
        {
          "analysis": "The harmonic weighted accuracy (HWA) plot shows that the 30-epoch configuration achieves the highest peak HWA, surpassing 0.66. However, the performance is inconsistent, as indicated by the fluctuations. The 10-epoch configuration demonstrates more stable improvements in HWA, peaking slightly above 0.64. The 20-epoch configuration shows a decline in HWA after an initial increase, aligning with the overfitting observed in the loss curves. The 5-epoch configuration has the lowest HWA, indicating insufficient training duration for optimal performance.",
          "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915/SPR_BENCH_HWA_curves.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915/SPR_BENCH_loss_curves.png",
        "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915/SPR_BENCH_HWA_curves.png"
      ],
      "vlm_feedback_summary": "The results suggest that the 10-epoch configuration strikes the best balance between preventing overfitting and achieving high performance. While 30 epochs achieve a higher peak HWA, the instability and overfitting make it less reliable. Further fine-tuning of hyperparameters, such as learning rate and batch size, might improve the stability and performance of the 10-epoch configuration.",
      "exp_results_dir": "experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915",
      "exp_results_npy_files": [
        "experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915/experiment_data.npy"
      ]
    },
    {
      "overall_plan": "The overall plan initially focused on exploring the 'epochs' hyper-parameter by training identical LSTM models with 5, 10, 20, and 30 epochs. This was done to determine the optimal number of epochs by evaluating train/validation loss and HWA after each epoch, with results stored in experiment_data['epochs']['SPR_BENCH']. The script used was self-contained and operational on both CPU and GPU. The current plan, described as a 'Seed node,' suggests establishing a foundational base for future experiments. This indicates a transition from specific hyper-parameter tuning to broader exploratory research, potentially involving new variables or models. The previous insights on epochs will likely inform future directions from this new starting point.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "training loss",
              "lower_is_better": true,
              "description": "The loss calculated on the training dataset.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.633,
                  "best_value": 0.633
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.5117,
                  "best_value": 0.5117
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 0.2033,
                  "best_value": 0.2033
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 0.0171,
                  "best_value": 0.0171
                }
              ]
            },
            {
              "metric_name": "validation loss",
              "lower_is_better": true,
              "description": "The loss calculated on the validation dataset.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.6444,
                  "best_value": 0.6444
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.6824,
                  "best_value": 0.6824
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 1.1331,
                  "best_value": 1.1331
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 1.3624,
                  "best_value": 1.3624
                }
              ]
            },
            {
              "metric_name": "validation harmonic weighted accuracy",
              "lower_is_better": false,
              "description": "The harmonic weighted accuracy calculated on the validation dataset.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.582,
                  "best_value": 0.582
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.64,
                  "best_value": 0.64
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 0.636,
                  "best_value": 0.636
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 0.668,
                  "best_value": 0.668
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset, DatasetDict\n\n# ----------------- experiment data dict -----------------\nexperiment_data = {\n    \"epochs\": {\"SPR_BENCH\": {}}  # hyper-parameter we sweep  # dataset name\n}\n\n# ----------------- device & seeds -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper metrics -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(swa, cwa):\n    return 2 * swa * cwa / (swa + cwa + 1e-12)\n\n\n# ----------------- data loading -----------------\ndef load_or_create_dataset():\n    root = pathlib.Path(\"SPR_BENCH\")\n    if root.exists():\n        from SPR import load_spr_bench\n\n        return load_spr_bench(root)\n\n    # fallback tiny synthetic set\n    def gen_row(_id):\n        length = random.randint(4, 9)\n        shapes, colors = \"ABCD\", \"abcd\"\n        seq = \" \".join(\n            random.choice(shapes) + random.choice(colors) for _ in range(length)\n        )\n        return {\"id\": _id, \"sequence\": seq, \"label\": int(length % 2)}\n\n    train_rows = [gen_row(i) for i in range(600)]\n    dev_rows = [gen_row(1000 + i) for i in range(200)]\n    test_rows = [gen_row(2000 + i) for i in range(200)]\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_list(train_rows),\n            \"dev\": HFDataset.from_list(dev_rows),\n            \"test\": HFDataset.from_list(test_rows),\n        }\n    )\n\n\nspr = load_or_create_dataset()\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------------- vocabulary -----------------\nall_text = \" \".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text.split()))\ntok2idx = {tok: i + 2 for i, tok in enumerate(vocab)}  # 0 PAD, 1 UNK\ntok2idx[\"<PAD>\"], tok2idx[\"<UNK>\"] = 0, 1\nidx2tok = {i: t for t, i in tok2idx.items()}\nvocab_size = len(tok2idx)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(seq):\n    return [tok2idx.get(tok, 1) for tok in seq.strip().split()]\n\n\n# ----------------- torch dataset -----------------\nclass SPRTorchSet(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate(batch):\n    xs = [b[\"x\"] for b in batch]\n    lens = [len(x) for x in xs]\n    maxlen = max(lens)\n    xs_pad = torch.zeros(len(xs), maxlen, dtype=torch.long)\n    for i, x in enumerate(xs):\n        xs_pad[i, : len(x)] = x\n    ys = torch.stack([b[\"y\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    return {\n        \"x\": xs_pad.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"y\": ys.to(device),\n        \"raw_seq\": raw,\n    }\n\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    SPRTorchSet(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchSet(spr[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\nn_classes = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Num classes: {n_classes}\")\n\n\n# ----------------- model -----------------\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab, emb=64, hid=128, n_out=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n        self.fc = nn.Linear(hid, n_out)\n\n    def forward(self, x, lengths):\n        em = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            em, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (h, _) = self.lstm(packed)\n        return self.fc(h[-1])\n\n\n# ----------------- train / evaluate one run -----------------\ndef run_training(num_epochs):\n    model = LSTMClassifier(vocab_size, n_out=n_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n    run_rec = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        tloss = 0.0\n        for batch in train_loader:\n            optim.zero_grad()\n            out = model(batch[\"x\"], batch[\"len\"])\n            loss = criterion(out, batch[\"y\"])\n            loss.backward()\n            optim.step()\n            tloss += loss.item() * batch[\"y\"].size(0)\n        train_loss = tloss / len(train_loader.dataset)\n\n        # eval\n        model.eval()\n        vloss, all_seq, y_true, y_pred = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                out = model(batch[\"x\"], batch[\"len\"])\n                loss = criterion(out, batch[\"y\"])\n                vloss += loss.item() * batch[\"y\"].size(0)\n                preds = torch.argmax(out, 1).cpu().tolist()\n                y_pred.extend(preds)\n                y_true.extend(batch[\"y\"].cpu().tolist())\n                all_seq.extend(batch[\"raw_seq\"])\n        val_loss = vloss / len(dev_loader.dataset)\n        swa = shape_weighted_accuracy(all_seq, y_true, y_pred)\n        cwa = color_weighted_accuracy(all_seq, y_true, y_pred)\n        hwa = harmonic_weighted_accuracy(swa, cwa)\n\n        # record\n        run_rec[\"losses\"][\"train\"].append(train_loss)\n        run_rec[\"losses\"][\"val\"].append(val_loss)\n        run_rec[\"metrics\"][\"val\"].append(hwa)\n        run_rec[\"predictions\"].append(y_pred)\n        run_rec[\"ground_truth\"].append(y_true)\n        run_rec[\"timestamps\"].append(time.time())\n        print(\n            f\"Epochs={num_epochs} | Ep {ep}: tr_loss={train_loss:.4f} val_loss={val_loss:.4f} HWA={hwa:.3f}\"\n        )\n    return run_rec\n\n\n# ----------------- hyper-parameter sweep -----------------\nepoch_options = [5, 10, 20, 30]\nfor e in epoch_options:\n    print(f\"\\n=== Training for {e} epochs ===\")\n    experiment_data[\"epochs\"][\"SPR_BENCH\"][str(e)] = run_training(e)\n\n# ----------------- save -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# setup paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"epochs\"][\"SPR_BENCH\"]  # dict keyed by epoch count\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------- Plot 1: Train / Val loss curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"losses\"][\"train\"]) + 1)\n            plt.plot(\n                x, rec[\"losses\"][\"train\"], label=f\"{run_name}ep-train\", linestyle=\"--\"\n            )\n            plt.plot(x, rec[\"losses\"][\"val\"], label=f\"{run_name}ep-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for loss curves.\")\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\nfinally:\n    plt.close()\n\n# ---------- Plot 2: Validation HWA curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"metrics\"][\"val\"]) + 1)\n            plt.plot(x, rec[\"metrics\"][\"val\"], label=f\"{run_name}ep\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation HWA Across Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_HWA_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for HWA curves.\")\nexcept Exception as e:\n    print(f\"Error creating HWA plot: {e}\")\nfinally:\n    plt.close()\n",
      "plot_analyses": [
        {
          "analysis": "The training vs. validation loss curves indicate that overfitting begins to occur as the number of epochs increases. Specifically, for 20 and 30 epochs, the validation loss starts increasing after a certain point, while the training loss continues to decrease. This suggests that the model is learning patterns specific to the training data but fails to generalize well to the validation set. Early stopping could help mitigate this issue by halting training once the validation loss stops improving. Additionally, the loss curves for 5 and 10 epochs show a more balanced behavior, with both training and validation losses decreasing without significant divergence.",
          "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916/SPR_BENCH_loss_curves.png"
        },
        {
          "analysis": "The validation HWA (Harmonic Weighted Accuracy) plot shows that the highest performance is achieved around 10 epochs, with a peak value slightly exceeding 0.66. However, for 20 and 30 epochs, the HWA metric becomes more volatile, indicating potential overfitting or instability in the model's performance. The 5-epoch configuration shows a steady increase in HWA but does not reach the same peak as the 10-epoch configuration. This suggests that a moderate number of epochs (around 10) may provide the best trade-off between training time and performance. Fine-tuning the learning rate and batch size around this configuration could further optimize results.",
          "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916/SPR_BENCH_HWA_curves.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916/SPR_BENCH_loss_curves.png",
        "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916/SPR_BENCH_HWA_curves.png"
      ],
      "vlm_feedback_summary": "The plots reveal that overfitting occurs with higher epoch configurations, and the best performance in terms of HWA is achieved around 10 epochs. Early stopping and fine-tuning hyperparameters could improve model performance further.",
      "exp_results_dir": "experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916",
      "exp_results_npy_files": [
        "experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916/experiment_data.npy"
      ]
    },
    {
      "overall_plan": "The overall plan involves a thorough investigation of the 'epochs' hyper-parameter for LSTM models by training identical models for 5, 10, 20, and 30 epochs. The purpose is to analyze how different epoch settings affect model performance in terms of train/validation loss and HWA. Each setting's results are meticulously organized for easy comparison within the experiment_data['epochs']['SPR_BENCH']. The script used is comprehensive and self-contained, supporting data preparation, model training, evaluation, and result storage, and is designed to run on both CPU and GPU. The current plan, noted as a 'Seed node,' suggests the start of a new or foundational phase, potentially laying the groundwork for future research directions, although specific new objectives are not detailed.",
      "analysis": "The execution completed successfully without any bugs. The script trained a model on the SPR_BENCH dataset using different epoch configurations (5, 10, 20, and 30 epochs) and recorded the training and validation loss along with the Harmonic Weighted Accuracy (HWA). The model's performance improved initially, but overfitting was observed as the number of epochs increased, especially for 20 and 30 epochs. The HWA metric peaked earlier and then started to decline, indicating a need for early stopping to prevent overfitting. The script also saved the experimental data successfully. Overall, the implementation aligns with the goals of hyperparameter optimization and provides valuable insights into the model's behavior across different epoch configurations.",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "training loss",
              "lower_is_better": true,
              "description": "The loss calculated on the training dataset, indicating how well the model is fitting the training data.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.633,
                  "best_value": 0.633
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.5117,
                  "best_value": 0.5117
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 0.2033,
                  "best_value": 0.2033
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 0.0171,
                  "best_value": 0.0171
                }
              ]
            },
            {
              "metric_name": "validation loss",
              "lower_is_better": true,
              "description": "The loss calculated on the validation dataset, used to evaluate the model's performance on unseen data.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.6444,
                  "best_value": 0.6444
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.6824,
                  "best_value": 0.6824
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 1.1331,
                  "best_value": 1.1331
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 1.3624,
                  "best_value": 1.3624
                }
              ]
            },
            {
              "metric_name": "best validation harmonic weighted accuracy",
              "lower_is_better": false,
              "description": "The best harmonic weighted accuracy achieved on the validation dataset during training.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH (epochs = 5)",
                  "final_value": 0.582,
                  "best_value": 0.582
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 10)",
                  "final_value": 0.64,
                  "best_value": 0.64
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 20)",
                  "final_value": 0.636,
                  "best_value": 0.636
                },
                {
                  "dataset_name": "SPR_BENCH (epochs = 30)",
                  "final_value": 0.668,
                  "best_value": 0.668
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset, DatasetDict\n\n# ----------------- experiment data dict -----------------\nexperiment_data = {\n    \"epochs\": {\"SPR_BENCH\": {}}  # hyper-parameter we sweep  # dataset name\n}\n\n# ----------------- device & seeds -----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\nprint(f\"Using device: {device}\")\n\n\n# ----------------- helper metrics -----------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef harmonic_weighted_accuracy(swa, cwa):\n    return 2 * swa * cwa / (swa + cwa + 1e-12)\n\n\n# ----------------- data loading -----------------\ndef load_or_create_dataset():\n    root = pathlib.Path(\"SPR_BENCH\")\n    if root.exists():\n        from SPR import load_spr_bench\n\n        return load_spr_bench(root)\n\n    # fallback tiny synthetic set\n    def gen_row(_id):\n        length = random.randint(4, 9)\n        shapes, colors = \"ABCD\", \"abcd\"\n        seq = \" \".join(\n            random.choice(shapes) + random.choice(colors) for _ in range(length)\n        )\n        return {\"id\": _id, \"sequence\": seq, \"label\": int(length % 2)}\n\n    train_rows = [gen_row(i) for i in range(600)]\n    dev_rows = [gen_row(1000 + i) for i in range(200)]\n    test_rows = [gen_row(2000 + i) for i in range(200)]\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_list(train_rows),\n            \"dev\": HFDataset.from_list(dev_rows),\n            \"test\": HFDataset.from_list(test_rows),\n        }\n    )\n\n\nspr = load_or_create_dataset()\nprint({k: len(v) for k, v in spr.items()})\n\n# ----------------- vocabulary -----------------\nall_text = \" \".join(spr[\"train\"][\"sequence\"])\nvocab = sorted(set(all_text.split()))\ntok2idx = {tok: i + 2 for i, tok in enumerate(vocab)}  # 0 PAD, 1 UNK\ntok2idx[\"<PAD>\"], tok2idx[\"<UNK>\"] = 0, 1\nidx2tok = {i: t for t, i in tok2idx.items()}\nvocab_size = len(tok2idx)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode_sequence(seq):\n    return [tok2idx.get(tok, 1) for tok in seq.strip().split()]\n\n\n# ----------------- torch dataset -----------------\nclass SPRTorchSet(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode_sequence(self.seqs[idx]), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate(batch):\n    xs = [b[\"x\"] for b in batch]\n    lens = [len(x) for x in xs]\n    maxlen = max(lens)\n    xs_pad = torch.zeros(len(xs), maxlen, dtype=torch.long)\n    for i, x in enumerate(xs):\n        xs_pad[i, : len(x)] = x\n    ys = torch.stack([b[\"y\"] for b in batch])\n    raw = [b[\"raw_seq\"] for b in batch]\n    return {\n        \"x\": xs_pad.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"y\": ys.to(device),\n        \"raw_seq\": raw,\n    }\n\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    SPRTorchSet(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchSet(spr[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\nn_classes = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Num classes: {n_classes}\")\n\n\n# ----------------- model -----------------\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab, emb=64, hid=128, n_out=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n        self.fc = nn.Linear(hid, n_out)\n\n    def forward(self, x, lengths):\n        em = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            em, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (h, _) = self.lstm(packed)\n        return self.fc(h[-1])\n\n\n# ----------------- train / evaluate one run -----------------\ndef run_training(num_epochs):\n    model = LSTMClassifier(vocab_size, n_out=n_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n    run_rec = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        tloss = 0.0\n        for batch in train_loader:\n            optim.zero_grad()\n            out = model(batch[\"x\"], batch[\"len\"])\n            loss = criterion(out, batch[\"y\"])\n            loss.backward()\n            optim.step()\n            tloss += loss.item() * batch[\"y\"].size(0)\n        train_loss = tloss / len(train_loader.dataset)\n\n        # eval\n        model.eval()\n        vloss, all_seq, y_true, y_pred = 0.0, [], [], []\n        with torch.no_grad():\n            for batch in dev_loader:\n                out = model(batch[\"x\"], batch[\"len\"])\n                loss = criterion(out, batch[\"y\"])\n                vloss += loss.item() * batch[\"y\"].size(0)\n                preds = torch.argmax(out, 1).cpu().tolist()\n                y_pred.extend(preds)\n                y_true.extend(batch[\"y\"].cpu().tolist())\n                all_seq.extend(batch[\"raw_seq\"])\n        val_loss = vloss / len(dev_loader.dataset)\n        swa = shape_weighted_accuracy(all_seq, y_true, y_pred)\n        cwa = color_weighted_accuracy(all_seq, y_true, y_pred)\n        hwa = harmonic_weighted_accuracy(swa, cwa)\n\n        # record\n        run_rec[\"losses\"][\"train\"].append(train_loss)\n        run_rec[\"losses\"][\"val\"].append(val_loss)\n        run_rec[\"metrics\"][\"val\"].append(hwa)\n        run_rec[\"predictions\"].append(y_pred)\n        run_rec[\"ground_truth\"].append(y_true)\n        run_rec[\"timestamps\"].append(time.time())\n        print(\n            f\"Epochs={num_epochs} | Ep {ep}: tr_loss={train_loss:.4f} val_loss={val_loss:.4f} HWA={hwa:.3f}\"\n        )\n    return run_rec\n\n\n# ----------------- hyper-parameter sweep -----------------\nepoch_options = [5, 10, 20, 30]\nfor e in epoch_options:\n    print(f\"\\n=== Training for {e} epochs ===\")\n    experiment_data[\"epochs\"][\"SPR_BENCH\"][str(e)] = run_training(e)\n\n# ----------------- save -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# setup paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    runs = experiment_data[\"epochs\"][\"SPR_BENCH\"]  # dict keyed by epoch count\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = {}\n\n# ---------- Plot 1: Train / Val loss curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"losses\"][\"train\"]) + 1)\n            plt.plot(\n                x, rec[\"losses\"][\"train\"], label=f\"{run_name}ep-train\", linestyle=\"--\"\n            )\n            plt.plot(x, rec[\"losses\"][\"val\"], label=f\"{run_name}ep-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss Curves\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for loss curves.\")\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\nfinally:\n    plt.close()\n\n# ---------- Plot 2: Validation HWA curves ----------\ntry:\n    if runs:\n        plt.figure(figsize=(7, 5))\n        for run_name, rec in runs.items():\n            x = np.arange(1, len(rec[\"metrics\"][\"val\"]) + 1)\n            plt.plot(x, rec[\"metrics\"][\"val\"], label=f\"{run_name}ep\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Harmonic Weighted Accuracy\")\n        plt.title(\"SPR_BENCH: Validation HWA Across Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_HWA_curves.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    else:\n        print(\"No runs to plot for HWA curves.\")\nexcept Exception as e:\n    print(f\"Error creating HWA plot: {e}\")\nfinally:\n    plt.close()\n",
      "plot_analyses": [
        {
          "analysis": "The training and validation loss curves provide insights into the model's learning dynamics across different numbers of epochs. For the 5 and 10-epoch configurations, the validation loss closely follows the training loss, indicating a good balance between fitting the training data and generalization. However, for the 20 and 30-epoch configurations, the validation loss starts increasing after a certain point (around 15 epochs for 20ep and 10 epochs for 30ep), signaling overfitting. The training loss for these configurations continues to decrease, reinforcing the evidence of overfitting. The results suggest that early stopping should be implemented to halt training once the validation loss stops improving, particularly for configurations with higher epoch counts.",
          "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914/SPR_BENCH_loss_curves.png"
        },
        {
          "analysis": "The Harmonic Weighted Accuracy (HWA) plot shows how the model's performance on the validation set changes across epochs. The 30-epoch configuration achieves the highest peak HWA (above 0.66), but the performance fluctuates significantly and does not stabilize. The 10-epoch configuration shows a more consistent improvement and achieves a competitive HWA (around 0.64) with less fluctuation, indicating better stability and generalization. The 20-epoch configuration has a similar peak performance to the 10-epoch configuration but exhibits more variability and a declining trend after its peak. The 5-epoch configuration shows the lowest HWA, suggesting insufficient training time to reach optimal performance. Overall, the 10-epoch configuration appears to offer the best trade-off between performance and stability.",
          "plot_path": "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914/SPR_BENCH_HWA_curves.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914/SPR_BENCH_loss_curves.png",
        "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914/SPR_BENCH_HWA_curves.png"
      ],
      "vlm_feedback_summary": "The analysis highlights the importance of early stopping to prevent overfitting, especially for configurations with higher epoch counts. It also identifies the 10-epoch configuration as the most stable and effective, achieving competitive performance with minimal fluctuation.",
      "exp_results_dir": "experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914",
      "exp_results_npy_files": [
        "experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914/experiment_data.npy"
      ]
    }
  ],
  "aggregated results of nodes with different seeds": {
    "overall_plan": "The overall plan involves a two-stage process. Initially, the focus was on exploring the effects of varying the 'epochs' hyper-parameter by training several identical LSTM models for 5, 10, 20, and 30 epochs, with detailed logging of train/validation loss and a performance metric (HWA) after each epoch. This structured approach facilitated easy comparison of results across different epoch settings. Subsequently, the current plan involves aggregating results from multiple seeds, which is critical for ensuring the robustness and reliability of the findings by averaging out the effects of random initialization variance. This combined approach allows for a thorough examination of the model's performance with respect to training duration while enhancing the credibility and generalizability of the experimental results.",
    "metric": {
      "value": null,
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "# plotting aggregation code",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\n# basic set-up\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load every experiment_data.npy that the user listed\n# ------------------------------------------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_43ebeb7700684ccdb5bbc99b916c8118_proc_3023916/experiment_data.npy\",\n    \"experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4c374067d39241aa8a61d322310096f4_proc_3023915/experiment_data.npy\",\n    \"experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_90429db0f47349c7a7039a95ac1deeba_proc_3023914/experiment_data.npy\",\n]\n\nall_runs = []  # each entry is a dict with keys: losses.train, losses.val, metrics.val\ndataset_name = \"SPR_BENCH\"\n\nfor p in experiment_data_path_list:\n    try:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p), allow_pickle=True\n        ).item()\n        runs_dict = data.get(\"epochs\", {}).get(dataset_name, {})\n        for _, rec in runs_dict.items():\n            all_runs.append(rec)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif not all_runs:\n    print(\"No experiment data found \u2014 nothing to plot.\")\n    exit()\n\n\n# ------------------------------------------------------------------\n# helper to aggregate list of 1-D arrays of possibly different length\n# ------------------------------------------------------------------\ndef aggregate(list_of_arrays):\n    max_len = max(len(arr) for arr in list_of_arrays)\n    mat = np.full((len(list_of_arrays), max_len), np.nan, dtype=float)\n    for i, arr in enumerate(list_of_arrays):\n        mat[i, : len(arr)] = arr\n    mean = np.nanmean(mat, axis=0)\n    se = np.nanstd(mat, axis=0, ddof=1) / np.sqrt(np.sum(~np.isnan(mat), axis=0))\n    return mean, se, np.arange(1, max_len + 1)\n\n\n# ------------------------------------------------------------------\n# collect per-run sequences\n# ------------------------------------------------------------------\ntrain_losses = [\n    np.asarray(r[\"losses\"][\"train\"], dtype=float) for r in all_runs if \"losses\" in r\n]\nval_losses = [\n    np.asarray(r[\"losses\"][\"val\"], dtype=float) for r in all_runs if \"losses\" in r\n]\nval_hwa = [\n    np.asarray(r[\"metrics\"][\"val\"], dtype=float) for r in all_runs if \"metrics\" in r\n]\n\n# ------------------------------------------------------------------\n# PLOT 1 : aggregated loss curves with SE bands\n# ------------------------------------------------------------------\ntry:\n    mean_train, se_train, epochs_train = aggregate(train_losses)\n    mean_val, se_val, epochs_val = aggregate(val_losses)\n\n    plt.figure(figsize=(7, 5))\n    plt.plot(epochs_train, mean_train, label=\"Train mean\", color=\"tab:blue\")\n    plt.fill_between(\n        epochs_train,\n        mean_train - se_train,\n        mean_train + se_train,\n        color=\"tab:blue\",\n        alpha=0.25,\n        label=\"Train \u00b11 SE\",\n    )\n\n    plt.plot(epochs_val, mean_val, label=\"Val mean\", color=\"tab:orange\")\n    plt.fill_between(\n        epochs_val,\n        mean_val - se_val,\n        mean_val + se_val,\n        color=\"tab:orange\",\n        alpha=0.25,\n        label=\"Val \u00b11 SE\",\n    )\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(f\"{dataset_name}: Mean \u00b1 SE Training / Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dataset_name}_aggregated_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\nfinally:\n    plt.close()\n\n# ------------------------------------------------------------------\n# PLOT 2 : aggregated validation HWA with SE bands\n# ------------------------------------------------------------------\ntry:\n    mean_hwa, se_hwa, epochs_hwa = aggregate(val_hwa)\n\n    plt.figure(figsize=(7, 5))\n    plt.plot(epochs_hwa, mean_hwa, label=\"Val HWA mean\", color=\"tab:green\")\n    plt.fill_between(\n        epochs_hwa,\n        mean_hwa - se_hwa,\n        mean_hwa + se_hwa,\n        color=\"tab:green\",\n        alpha=0.25,\n        label=\"Val HWA \u00b11 SE\",\n    )\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Harmonic Weighted Accuracy\")\n    plt.title(f\"{dataset_name}: Mean \u00b1 SE Validation HWA\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dataset_name}_aggregated_HWA_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\nexcept Exception as e:\n    print(f\"Error creating aggregated HWA plot: {e}\")\nfinally:\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print final-epoch statistics for quick inspection\n# ------------------------------------------------------------------\ntry:\n    final_hwa_values = [arr[-1] for arr in val_hwa if len(arr) > 0]\n    if final_hwa_values:\n        mean_final = np.mean(final_hwa_values)\n        std_final = np.std(final_hwa_values, ddof=1)\n        print(\n            f\"Final-epoch HWA across runs: {mean_final:.4f} \u00b1 {std_final:.4f} (mean \u00b1 std, n={len(final_hwa_values)})\"\n        )\nexcept Exception as e:\n    print(f\"Error computing final HWA statistics: {e}\")\n",
    "plot_analyses": [],
    "plot_paths": [
      "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_8d0ed7e775fb4e8ab8b2b9f4ba79a7f7/SPR_BENCH_aggregated_loss_curves.png",
      "experiments/2025-08-15_23-37-14_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_8d0ed7e775fb4e8ab8b2b9f4ba79a7f7/SPR_BENCH_aggregated_HWA_curves.png"
    ],
    "vlm_feedback_summary": [],
    "exp_results_dir": "experiment_results/seed_aggregation_8d0ed7e775fb4e8ab8b2b9f4ba79a7f7",
    "exp_results_npy_files": []
  }
}