
% This paper introduces SimCLR, a foundational framework for contrastive learning of visual representations. It systematically studies the critical components of contrastive learning, such as data augmentation and loss design, which are directly relevant to the proposed context-aware contrastive learning framework. This citation will be used to establish the baseline understanding of contrastive learning in the related work section.
@article{chen2020asf,
 author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {A Simple Framework for Contrastive Learning of Visual Representations},
 volume = {abs/2002.05709},
 year = {2020}
}

% This paper discusses a comprehensive framework for integrating symbolic reasoning with deep learning, emphasizing the strengths of each domain and their combination for tasks requiring complex reasoning and generalization. It is relevant for the related work section to highlight existing methods in symbolic reasoning and their limitations, which the proposed context-aware contrastive learning approach seeks to address.
@conference{himabindu2023neurosymbolicai,
 author = {Modi Himabindu and Revathi V and Manish Gupta and Ajay Rana and Pradeep Kumar Chandra and H. S. Abdulaali},
 booktitle = {IEEE Uttar Pradesh Section International Conference on Electrical, Computer and Electronics Engineering},
 journal = {2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)},
 pages = {1587-1592},
 title = {Neuro-Symbolic AI: Integrating Symbolic Reasoning with Deep Learning},
 volume = {10},
 year = {2023}
}

% The paper introduces rsbench, a neuro-symbolic benchmark suite designed to evaluate reasoning shortcuts in models. It highlights the importance of benchmarks in assessing reasoning and concept quality, which parallels the role of SPR_BENCH in symbolic reasoning. This citation will support the related work section by emphasizing the need for comprehensive evaluation frameworks for reasoning tasks.
@article{bortolotti2024anb,
 author = {Samuele Bortolotti and Emanuele Marconato and Tommaso Carraro and Paolo Morettin and Emile van Krieken and Antonio Vergari and Stefano Teso and Andrea Passerini},
 booktitle = {Neural Information Processing Systems},
 title = {A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts},
 year = {2024}
}

% Paper 1 discusses similarity-guided data augmentation and denoising techniques in sequential recommendation systems, which is directly relevant to the proposed context-aware contrastive learning framework for symbolic sequences. Paper 3 introduces a semi-supervised contrastive learning framework with novel data augmentation methods for time series, emphasizing trend preservation and latent space organization. Both citations will support the methodology section, particularly the development of advanced augmentation and denoising strategies for symbolic sequence tasks.
@inproceedings{choi2025similarityguideddf,
 author = {Jinkyeong Choi and Yejin Noh and Donghyeon Park},
 title = {Similarity-Guided Diffusion for Contrastive Sequential Recommendation},
 year = {2025}
}

@article{kim2024semisupervisedcl,
 author = {Dokyun Kim and Sukhyun Cho and Heewoong Chae and Jonghun Park and Jaeseok Huh},
 booktitle = {Intelligent Data Analysis},
 journal = {Intelligent Data Analysis},
 pages = {94 - 115},
 title = {Semi-supervised contrastive learning with decomposition-based data augmentation for time series classification},
 volume = {29},
 year = {2024}
}

% This paper discusses the challenges and pitfalls of neuro-symbolic learning, including issues with generalization, compute, and data efficiency in training models for complex reasoning tasks. It provides insights into limitations that are highly relevant to the risks and limitations section of the proposed context-aware contrastive learning framework, emphasizing the hurdles that could arise in symbolic reasoning tasks.
@article{stein2025thert,
 author = {Adam Stein and Aaditya Naik and Neelay Velingker and Mayur Naik and Eric Wong},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models},
 volume = {abs/2505.24874},
 year = {2025}
}

% The paper introduces LTLZinc, a benchmarking framework for neuro-symbolic and continual learning methods, particularly for temporal reasoning and sequence classification tasks. This is relevant for the related work section, as it contextualizes the importance of benchmarking frameworks like SPR_BENCH in evaluating symbolic reasoning tasks and highlights the challenges in designing benchmarks for such domains.
@inproceedings{lorello2025ltlzincab,
 author = {Luca Salvatore Lorello and Nikolaos Manginas and Marco Lippi and S. Melacci},
 title = {LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning},
 year = {2025}
}

% This paper surveys self-supervised learning methods and their applications within sequential transfer learning, providing an overview of taxonomy, pre-training tasks, and trends across domains. It is relevant for summarizing the foundational principles of self-supervised learning in the methodology section, supporting the broader context of contrastive learning in this work.
@article{mao2020aso,
 author = {H. H. Mao},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks},
 volume = {abs/2007.00800},
 year = {2020}
}
