{"edges": [[0, 5], [0, 8], [0, 3], [0, 1], [0, 4], [0, 6], [0, 2], [0, 7]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0], [1.0, 1.0]], "plan": ["We will run a single-configuration experiment that adds context-aware token-\ndropout and random swap augmentations during a short contrastive pre-training\nphase, followed by supervised fine-tuning.   A small bidirectional-GRU encoder\nmaps sequences (with a learned token embedding) to sentence features; the same\nencoder is shared by the contrastive and classification objectives.   During\npre-training each training example yields two independently augmented views, and\nan NT-Xent loss is minimized.   After two epochs of pre-training we fine-tune\nfor two epochs using cross-entropy on the labeled data.   At every fine-tuning\nepoch we compute and print validation loss together with Shape-Weighted\nAccuracy, Color-Weighted Accuracy, and the requested Structural-Complexity-\nAdjusted Accuracy (SCAA = accuracy weighted by (#unique shapes + #unique\ncolors)).   All metrics, losses, predictions and ground-truth labels are stored\ninside a numpy-serialised experiment_data dict so the run can be analysed later.\nIf the real SPR_BENCH folder is unavailable the script automatically falls back\nto a small synthetic dataset so it can always execute.   Everything is fully\nGPU-aware, self-contained, and respects the specified code-structure\nconstraints.", "Ablation name: No Contrastive Pre-training (Supervised-Only Baseline). Below I\nskip the contrastive pre-training phase and train the encoder + classifier end-\nto-end with the supervised cross-entropy objective only. All metrics, losses and\npredictions are stored in the required experiment_data structure and saved to\nexperiment_data.npy.", "Ablation name: Identity-Views Contrastive Pre-training (No Augmentations). The\nsolution replicates the original training pipeline but replaces the contrastive-\ndata loader with one that feeds the *same* un-augmented sequence twice, thereby\nremoving all augmentation effects. Everything else (model, optimisation, fine-\ntuning, metric logging, saving) is untouched, allowing a clean ablation\ncomparison.", "Ablation name: No-Projection-Head Encoder. We eliminate the projection head by\nredefining the encoder so that the 2\u00b7d_model Bi-GRU hidden state is used\ndirectly for both contrastive pre-training and downstream classification. The\nclassifier head now maps this 512-dimensional vector (for d_model = 256) to task\nlabels. Everything else\u2014data preparation, contrastive pre-training, fine-tuning,\nmetric logging, and saving\u2014is identical to the baseline, enabling an apples-to-\napples comparison that isolates the impact of the removed projection layer.", "Ablation name: Multi-Synthetic-Dataset Training (Rule Diversity Ablation). We\ncreate three independent synthetic datasets, each driven by a different latent\nrule (\u201cA1-parity\u201d, \u201cB2-parity\u201d, \u201cmajority-shape\u201d).   A single vocabulary is\nbuilt over the union of all datasets. The encoder is first contrastively pre-\ntrained and then fine-tuned on the concatenated training split of the three\nrules.   After every fine-tuning epoch we evaluate \u2013 separately \u2013 on the held-\nout dev split of each rule and log per-rule metrics/losses into the\nexperiment_data dict, which is finally stored as experiment_data.npy.", "Ablation name: Mean-Pooling Encoder (Remove GRU Sequence Modeling). We replace\nthe bidirectional\u2010GRU encoder with a light mean-pool encoder: token embeddings\nare averaged (masking out PADs) and the resulting vector is passed through the\nsame projection layer. The rest of the pipeline\u2014contrastive pre-training, fine-\ntuning, evaluation, and data saving\u2014remains unchanged, letting us directly\ncompare performance and gauge the importance of sequence modeling. Results are\nstored under the ablation label \u201cmean_pool_encoder\u201d in experiment_data.npy.", "Ablation name: Frozen-Encoder Fine-Tuning. The solution follows the original\npipeline (contrastive pre-training + supervised fine-tuning) but, at the fine-\ntuning stage, the encoder is frozen and only the linear classification head is\nupdated. This isolates the usefulness of representations learned during\ncontrastive pre-training. All results are stored in the required experiment_data\nstructure and dumped to experiment_data.npy.", "Ablation name: Unidirectional-GRU Encoder (Remove Backward Context). We keep\nevery training, augmentation and evaluation component unchanged, but replace the\nbidirectional GRU with a unidirectional GRU whose hidden size is doubled (so the\nencoder still outputs 512-dim features that are projected to 256).  This\nisolates the effect of removing backward context while keeping parameter count\ncomparable.  Results (losses, metrics, predictions) are stored under the\nablation key \"UniGRU_no_backward\" in experiment_data.npy exactly as required.", "Ablation name: Token-Dropout-Only Contrastive Augmentation. Below we ablate the\nswap operation by defining aug() that performs only token dropout with p = 0.1\n(falling back to \u201cA1\u201d if the result is empty). All other pipeline stages\n(contrastive pre-training, fine-tuning, metric logging, saving) remain\nunchanged, and results are stored under the ablation key \u201cTokenDropoutOnly\u201d."], "code": ["import os, random, pathlib, csv, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# mandatory working dir + device setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# helper functions for metrics\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\n# ---------------------------------------------------------------------\n# dataset loading (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    rows = []\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        for r in rdr:\n            rows.append({\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])})\n    return rows\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        # trivial parity rule on number of 'A1'\n        return sum(tok == \"A1\" for tok in seq) % 2\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\n\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------------------------------------------------------------------\n# vocabulary\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for row in split:\n        vocab.update(row[\"sequence\"].split())\nitos = list(vocab)\nstoi = {tok: i for i, tok in enumerate(itos)}\nvocab_size = len(itos)\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------------\n# augmentations for contrastive learning\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    if len(toks) > 1 and random.random() < 0.7:  # random swap\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    # token dropout / masking\n    toks = [t for t in toks if random.random() > 0.1]\n    if not toks:\n        toks = [\"A1\"]  # fallback\n    return \" \".join(toks)\n\n\n# ---------------------------------------------------------------------\n# PyTorch datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return (\n            torch.tensor(encode(aug(s), self.max_len)),\n            torch.tensor(encode(aug(s), self.max_len)),\n        )\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ---------------------------------------------------------------------\n# model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.bigru = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(2 * d_model, d_model)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.bigru(emb)\n        h = torch.cat([h[0], h[1]], dim=1)  # concat directions\n        return self.proj(h)  # [B,d_model]\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_classes):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_classes)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# ---------------------------------------------------------------------\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ---------------------------------------------------------------------\n# experiment config\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\n\nenc = Encoder(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ncontrast_loader = DataLoader(\n    ContrastiveSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------------------------------------------------------------------\n# contrastive pre-training\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f'Contrastive epoch {ep}: loss = {tot/len(dataset[\"train\"]):.4f}')\n\n# ---------------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0\n    for batch in train_loader:\n        ids, labels, _ = batch\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    # validation\n    model.eval()\n    val_loss = 0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for batch in dev_loader:\n            ids, labels, seq = batch\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # store\n    experiment_data[\"SPR\"][\"metrics\"][\"train\"].append(\n        {\"SWA\": None, \"CWA\": None, \"SCAA\": None}\n    )\n    experiment_data[\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA}\n    )\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR\"][\"epochs\"].append(ep)\n    experiment_data[\"SPR\"][\"predictions\"] = preds\n    experiment_data[\"SPR\"][\"ground_truth\"] = gts\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, csv, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------------------------------------\n# working directory / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -------------------------------------------------------------\n# metric helpers\ndef _count_shape(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# -------------------------------------------------------------\n# load real or synthetic SPR data\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        return [{\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])} for r in rdr]\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(tok == \"A1\" for tok in seq) % 2\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 generating synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\nprint({k: len(v) for k, v in dataset.items()})\n\n# -------------------------------------------------------------\n# vocabulary & encoder helper\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for r in split:\n        vocab.update(r[\"sequence\"].split())\nitos = list(vocab)\nstoi = {t: i for i, t in enumerate(itos)}\nvocab_size = len(itos)\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[t] for t in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\n# -------------------------------------------------------------\n# Dataset for supervised training\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# -------------------------------------------------------------\n# Model definition\nclass Encoder(nn.Module):\n    def __init__(self, vocab, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, d_model, padding_idx=stoi[PAD])\n        self.bigru = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(2 * d_model, d_model)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.bigru(emb)\n        h = torch.cat([h[0], h[1]], 1)\n        return self.proj(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_cls):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_cls)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# -------------------------------------------------------------\n# training configuration\nBATCH = 128\nEPOCH_SUP = 4  # purely supervised epochs\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\n\nenc = Encoder(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\n# -------------------------------------------------------------\n# experiment data dict\nexperiment_data = {\n    \"no_contrastive\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# -------------------------------------------------------------\n# supervised training loop\nfor ep in range(1, EPOCH_SUP + 1):\n    # ---------- train ----------\n    model.train()\n    tr_loss = 0\n    for ids, labels, _ in train_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    # ---------- validate ----------\n    model.eval()\n    val_loss = 0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for ids, labels, seq in dev_loader:\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n    print(\n        f\"Epoch {ep}: train_loss={tr_loss:.4f}  val_loss={val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # ---------- log ----------\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"metrics\"][\"train\"].append(\n        {\"SWA\": None, \"CWA\": None, \"SCAA\": None}\n    )\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA}\n    )\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"epochs\"].append(ep)\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"predictions\"] = preds\n    experiment_data[\"no_contrastive\"][\"SPR\"][\"ground_truth\"] = gts\n\n# -------------------------------------------------------------\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, csv, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# mandatory working dir + device setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# helper functions for metrics\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\n# ---------------------------------------------------------------------\n# dataset loading (real or synthetic fallback)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    rows = []\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        for r in rdr:\n            rows.append({\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])})\n    return rows\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(tok == \"A1\" for tok in seq) % 2\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------------------------------------------------------------------\n# vocabulary\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for row in split:\n        vocab.update(row[\"sequence\"].split())\nitos = list(vocab)\nstoi = {tok: i for i, tok in enumerate(itos)}\nvocab_size = len(itos)\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------------\n# PyTorch datasets\nclass ContrastiveSPRIdentity(Dataset):\n    \"\"\"Return two identical, un-augmented views.\"\"\"\n\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        vec = torch.tensor(encode(s, self.max_len))\n        return vec, vec  # identical views\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ---------------------------------------------------------------------\n# model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.bigru = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(2 * d_model, d_model)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.bigru(emb)\n        h = torch.cat([h[0], h[1]], dim=1)\n        return self.proj(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_classes):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_classes)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# ---------------------------------------------------------------------\ndef nt_xent(feats, temp=0.5):\n    f = nn.functional.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return nn.functional.cross_entropy(sim, targets)\n\n\n# ---------------------------------------------------------------------\n# experiment config\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\n\nenc = Encoder(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ncontrast_loader = DataLoader(\n    ContrastiveSPRIdentity(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\n# experiment data dict\nexperiment_data = {\n    \"identity_views\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# contrastive pre-training (identity views)\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f'Contrastive epoch {ep}: loss = {tot/len(dataset[\"train\"]):.4f}')\n\n# ---------------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0\n    for ids, labels, _ in train_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    model.eval()\n    val_loss, preds, gts, seqs = 0, [], [], []\n    with torch.no_grad():\n        for ids, labels, seq in dev_loader:\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # log\n    experiment_data[\"identity_views\"][\"SPR\"][\"metrics\"][\"train\"].append(\n        {\"SWA\": None, \"CWA\": None, \"SCAA\": None}\n    )\n    experiment_data[\"identity_views\"][\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA}\n    )\n    experiment_data[\"identity_views\"][\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"identity_views\"][\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"identity_views\"][\"SPR\"][\"epochs\"].append(ep)\n    experiment_data[\"identity_views\"][\"SPR\"][\"predictions\"] = preds\n    experiment_data[\"identity_views\"][\"SPR\"][\"ground_truth\"] = gts\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# No-Projection-Head ablation study : self-contained script\nimport os, random, pathlib, csv, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# experiment bookkeeping dict\nexperiment_data = {\n    \"NoProjectionHead\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# mandatory working dir + device setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# helper functions for metrics\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\n# ---------------------------------------------------------------------\n# dataset loading (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    rows = []\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        for r in rdr:\n            rows.append({\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])})\n    return rows\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(tok == \"A1\" for tok in seq) % 2  # parity rule\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\n\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------------------------------------------------------------------\n# vocabulary\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for row in split:\n        vocab.update(row[\"sequence\"].split())\nitos = list(vocab)\nstoi = {tok: i for i, tok in enumerate(itos)}\nvocab_size = len(itos)\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------------\n# augmentations for contrastive learning\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    if len(toks) > 1 and random.random() < 0.7:  # random swap\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    toks = [t for t in toks if random.random() > 0.1]  # dropout\n    if not toks:\n        toks = [\"A1\"]\n    return \" \".join(toks)\n\n\n# ---------------------------------------------------------------------\n# PyTorch datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return (\n            torch.tensor(encode(aug(s), self.max_len)),\n            torch.tensor(encode(aug(s), self.max_len)),\n        )\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ---------------------------------------------------------------------\n# model without projection head\nclass EncoderNoProj(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.bigru = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True)\n        self.feat_dim = 2 * d_model  # because of bidirection\n\n    def forward(self, x):\n        emb = self.emb(x)  # [B,T,d]\n        _, h = self.bigru(emb)\n        h = torch.cat([h[0], h[1]], dim=1)  # [B,2d]\n        return h  # directly return hidden state\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_classes):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.feat_dim, n_classes)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# ---------------------------------------------------------------------\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ---------------------------------------------------------------------\n# experiment config\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\n\nenc = EncoderNoProj(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ncontrast_loader = DataLoader(\n    ContrastiveSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\n# ---------------------------------------------------------------------\n# contrastive pre-training\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0.0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f'Contrastive epoch {ep}: loss = {tot/len(dataset[\"train\"]):.4f}')\n\n# ---------------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0.0\n    for ids, labels, _ in train_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    # validation\n    model.eval()\n    val_loss = 0.0\n    preds, gts, seqs = [], [], []\n    with torch.no_grad():\n        for ids, labels, seq in dev_loader:\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # record\n    spr_dict = experiment_data[\"NoProjectionHead\"][\"SPR\"]\n    spr_dict[\"metrics\"][\"train\"].append({\"SWA\": None, \"CWA\": None, \"SCAA\": None})\n    spr_dict[\"metrics\"][\"val\"].append({\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA})\n    spr_dict[\"losses\"][\"train\"].append(tr_loss)\n    spr_dict[\"losses\"][\"val\"].append(val_loss)\n    spr_dict[\"predictions\"] = preds\n    spr_dict[\"ground_truth\"] = gts\n    spr_dict[\"epochs\"].append(ep)\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, csv, time, pathlib\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------\n# runtime / path setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# ------------------------------------------------------------\n# metric helpers (unchanged)\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# ------------------------------------------------------------\n# synthetic-data generation for three distinct latent rules\ndef _gen_dataset(rule, n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\n# rule 1: parity of token 'A1'\nrule_a1 = lambda toks: sum(t == \"A1\" for t in toks) % 2\n# rule 2: parity of token 'B2'\nrule_b2 = lambda toks: sum(t == \"B2\" for t in toks) % 2\n\n\n# rule 3: majority shape == 'A'\ndef rule_majority_A(toks):\n    shapes = [t[0] for t in toks]\n    return int(shapes.count(\"A\") > len(shapes) // 2)\n\n\nDATASETS = {}\nfor name, rule in [\n    (\"A1_parity\", rule_a1),\n    (\"B2_parity\", rule_b2),\n    (\"MajorityA\", rule_majority_A),\n]:\n    DATASETS[name] = {\n        \"train\": _gen_dataset(rule, 4000),\n        \"dev\": _gen_dataset(rule, 1000),\n        \"test\": _gen_dataset(rule, 1000),\n    }\n\n# ------------------------------------------------------------\n# vocabulary over union of all datasets\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = set([PAD, CLS])\nfor d in DATASETS.values():\n    for split in d.values():\n        for r in split:\n            vocab.update(r[\"sequence\"].split())\nitos = list(vocab)\nstoi = {t: i for i, t in enumerate(itos)}\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[t] for t in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nvocab_size = len(itos)\n\n\n# ------------------------------------------------------------\n# contrastive augmentation\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    if len(toks) > 1 and random.random() < 0.7:\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    toks = [t for t in toks if random.random() > 0.1]\n    if not toks:\n        toks = [\"A1\"]\n    return \" \".join(toks)\n\n\n# ------------------------------------------------------------\n# PyTorch dataset wrappers\nclass Contrastive(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows, self.max_len = rows, max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return torch.tensor(encode(aug(s), self.max_len)), torch.tensor(\n            encode(aug(s), self.max_len)\n        )\n\n\nclass Labelled(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows, self.max_len = rows, max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ------------------------------------------------------------\n# model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab, dm=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, dm, padding_idx=stoi[PAD])\n        self.gru = nn.GRU(dm, dm, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(2 * dm, dm)\n\n    def forward(self, x):\n        e = self.emb(x)\n        _, h = self.gru(e)\n        h = torch.cat([h[0], h[1]], 1)\n        return self.proj(h)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc, nc):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, nc)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ------------------------------------------------------------\n# training / evaluation setup\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = 2\nenc = Encoder(vocab_size).to(device)\nmodel = Classifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# loaders: union training rows\nunion_train = [r for d in DATASETS.values() for r in d[\"train\"]]\ncontrast_loader = DataLoader(\n    Contrastive(union_train, MAX_LEN), batch_size=BATCH, shuffle=True\n)\nft_loader = DataLoader(Labelled(union_train, MAX_LEN), batch_size=BATCH, shuffle=True)\ndev_loaders = {\n    name: DataLoader(Labelled(ds[\"dev\"], MAX_LEN), batch_size=BATCH)\n    for name, ds in DATASETS.items()\n}\n\n# storage dict\nexperiment_data = {\n    \"multi_synth_rule_diversity\": {\n        name: {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n        for name in DATASETS.keys()\n    }\n}\n\n# ------------------------------------------------------------\n# contrastive pre-training\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0\n    for v1, v2 in contrast_loader:\n        v1 = v1.to(device)\n        v2 = v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f\"Contrastive {ep}: loss {tot/len(union_train):.4f}\")\n\n# ------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0\n    for ids, labels, _ in ft_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(union_train)\n\n    # per-dataset validation\n    model.eval()\n    with torch.no_grad():\n        for name, loader in dev_loaders.items():\n            val_loss = 0\n            preds = []\n            gts = []\n            seqs = []\n            for ids, labels, seq in loader:\n                ids, labels = ids.to(device), labels.to(device)\n                logits, _ = model(ids)\n                loss = criterion(logits, labels)\n                val_loss += loss.item() * ids.size(0)\n                preds.extend(torch.argmax(logits, 1).cpu().tolist())\n                gts.extend(labels.cpu().tolist())\n                seqs.extend(seq)\n            val_loss /= len(DATASETS[name][\"dev\"])\n            SWA = shape_weighted_accuracy(seqs, gts, preds)\n            CWA = color_weighted_accuracy(seqs, gts, preds)\n            SCAA = scaa(seqs, gts, preds)\n            print(\n                f\"Epoch {ep} | {name}: loss={val_loss:.4f} SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n            )\n            # logging\n            ex = experiment_data[\"multi_synth_rule_diversity\"][name]\n            ex[\"metrics\"][\"train\"].append({\"SWA\": None, \"CWA\": None, \"SCAA\": None})\n            ex[\"metrics\"][\"val\"].append({\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA})\n            ex[\"losses\"][\"train\"].append(tr_loss)\n            ex[\"losses\"][\"val\"].append(val_loss)\n            ex[\"predictions\"] = preds\n            ex[\"ground_truth\"] = gts\n            ex[\"epochs\"].append(ep)\n\n# ------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, csv, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# mandatory working dir + device setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# helper functions for metrics\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\n# ---------------------------------------------------------------------\n# dataset loading (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    rows = []\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        for r in rdr:\n            rows.append({\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])})\n    return rows\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(tok == \"A1\" for tok in seq) % 2\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------------------------------------------------------------------\n# vocabulary\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for row in split:\n        vocab.update(row[\"sequence\"].split())\nitos = list(vocab)\nstoi = {tok: i for i, tok in enumerate(itos)}\nvocab_size = len(itos)\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------------\n# augmentations for contrastive learning\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    if len(toks) > 1 and random.random() < 0.7:\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    toks = [t for t in toks if random.random() > 0.1]\n    if not toks:\n        toks = [\"A1\"]\n    return \" \".join(toks)\n\n\n# ---------------------------------------------------------------------\n# PyTorch datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows, self.max_len = rows, max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return (\n            torch.tensor(encode(aug(s), self.max_len)),\n            torch.tensor(encode(aug(s), self.max_len)),\n        )\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows, self.max_len = rows, max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ---------------------------------------------------------------------\n# Mean-pool encoder (ablation)\nclass MeanPoolEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.proj = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        emb = self.emb(x)  # [B,L,D]\n        mask = (x != stoi[PAD]).unsqueeze(-1).float()  # [B,L,1]\n        summed = (emb * mask).sum(1)\n        denom = mask.sum(1).clamp(min=1)\n        mean = summed / denom\n        return self.proj(mean)  # [B,D]\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_classes):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_classes)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# ---------------------------------------------------------------------\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ---------------------------------------------------------------------\n# experiment config\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\nenc = MeanPoolEncoder(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncontrast_loader = DataLoader(\n    ContrastiveSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\n# Experiment data dict\nexperiment_data = {\n    \"mean_pool_encoder\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# contrastive pre-training\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f'Contrastive epoch {ep}: loss = {tot/len(dataset[\"train\"]):.4f}')\n\n# ---------------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0\n    for ids, labels, _ in train_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n    # validation\n    model.eval()\n    val_loss = 0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for ids, labels, seq in dev_loader:\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n    # store\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"metrics\"][\"train\"].append(\n        {\"SWA\": None, \"CWA\": None, \"SCAA\": None}\n    )\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA}\n    )\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"epochs\"].append(ep)\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"predictions\"] = preds\n    experiment_data[\"mean_pool_encoder\"][\"SPR\"][\"ground_truth\"] = gts\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, csv\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------------------------------------------------ #\n# 1) housekeeping\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------------------------------------------------------ #\n# 2) metric helpers\ndef _count_shape(seq):\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq):\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef _w_acc(w, y_t, y_p):\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)) / max(sum(w), 1)\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    return _w_acc([_count_shape(s) for s in seqs], y_t, y_p)\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    return _w_acc([_count_color(s) for s in seqs], y_t, y_p)\n\n\ndef scaa(seqs, y_t, y_p):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    return _w_acc(w, y_t, y_p)\n\n\n# ------------------------------------------------------------------ #\n# 3) dataset (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        return [{\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])} for r in rdr]\n\n\ndef _gen_synth(n=3000, mx=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(t == \"A1\" for t in seq) % 2\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, mx))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for spl in [\"train\", \"dev\", \"test\"]:\n            dataset[spl] = _load_csv(SPR_PATH / f\"{spl}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _gen_synth(4000)\n    dataset[\"dev\"] = _gen_synth(1000)\n    dataset[\"test\"] = _gen_synth(1000)\nprint({k: len(v) for k, v in dataset.items()})\n\n# ------------------------------------------------------------------ #\n# 4) vocab / encoding\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for r in split:\n        vocab.update(r[\"sequence\"].split())\nitos = list(vocab)\nstoi = {t: i for i, t in enumerate(itos)}\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[t] for t in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nvocab_size = len(itos)\n\n\n# ------------------------------------------------------------------ #\n# 5) augmentations\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    if len(toks) > 1 and random.random() < 0.7:\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    toks = [t for t in toks if random.random() > 0.1]\n    if not toks:\n        toks = [\"A1\"]\n    return \" \".join(toks)\n\n\n# ------------------------------------------------------------------ #\n# 6) PyTorch datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.mx = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return torch.tensor(encode(aug(s), self.mx)), torch.tensor(\n            encode(aug(s), self.mx)\n        )\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.mx = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.mx)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ------------------------------------------------------------------ #\n# 7) model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.rnn = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x):\n        e = self.emb(x)\n        _, h = self.rnn(e)\n        h = torch.cat([h[0], h[1]], 1)\n        return self.proj(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_cls):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_cls)\n\n    def forward(self, x):\n        f = self.enc(x)\n        return self.head(f), f\n\n\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ------------------------------------------------------------------ #\n# 8) experiment set-up\nBATCH, EPOCH_PRE, EPOCH_FT, MAX_LEN = 128, 2, 2, 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\nenc = Encoder(vocab_size).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ncontrast_loader = DataLoader(\n    ContrastiveSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\nexperiment_data = {\n    \"frozen_encoder\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------------ #\n# 9) contrastive pre-training (encoder + head both trainable)\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f\"Contrastive epoch {ep} loss={tot/len(dataset['train']):.4f}\")\n\n# ------------------------------------------------------------------ #\n# 10) freeze encoder, new optimizer for head only\nfor p in model.enc.parameters():\n    p.requires_grad = False\noptimizer = torch.optim.Adam(model.head.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nfor ep in range(1, EPOCH_FT + 1):\n    # training (head only)\n    model.train()\n    tr_loss = 0\n    for ids, labels, _ in train_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    # validation\n    model.eval()\n    val_loss = 0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for ids, labels, seq in dev_loader:\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n    print(\n        f\"FT Epoch {ep}: val_loss={val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # log\n    ed = experiment_data[\"frozen_encoder\"][\"SPR\"]\n    ed[\"metrics\"][\"train\"].append({\"SWA\": None, \"CWA\": None, \"SCAA\": None})\n    ed[\"metrics\"][\"val\"].append({\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA})\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"epochs\"].append(ep)\n    ed[\"predictions\"] = preds\n    ed[\"ground_truth\"] = gts\n\n# ------------------------------------------------------------------ #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, csv, time, numpy as np\nfrom typing import List, Dict\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# metric helpers\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [_count_shape(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [_count_color(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\ndef scaa(seqs, y_t, y_p):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_t, y_p)]\n    return sum(c) / max(sum(w), 1)\n\n\n# ---------------------------------------------------------------------\n# data (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    with open(fp) as f:\n        return [\n            {\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])}\n            for r in csv.DictReader(f)\n        ]\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(tok == \"A1\" for tok in seq) % 2\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------------------------------------------------------------------\n# vocab & encoding\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = set([PAD, CLS])\nfor split in dataset.values():\n    for r in split:\n        vocab.update(r[\"sequence\"].split())\nitos = list(vocab)\nstoi = {tok: i for i, tok in enumerate(itos)}\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[t] for t in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nvocab_size = len(itos)\n\n\n# ---------------------------------------------------------------------\n# augmentations\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    if len(toks) > 1 and random.random() < 0.7:\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    toks = [t for t in toks if random.random() > 0.1]\n    if not toks:\n        toks = [\"A1\"]\n    return \" \".join(toks)\n\n\n# ---------------------------------------------------------------------\n# datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows, self.max_len = rows, max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return torch.tensor(encode(aug(s), self.max_len)), torch.tensor(\n            encode(aug(s), self.max_len)\n        )\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows, self.max_len = rows, max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ---------------------------------------------------------------------\n# Encoder (Uni-directional GRU, doubled hidden size)\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.gru = nn.GRU(d_model, d_model * 2, batch_first=True, bidirectional=False)\n        self.proj = nn.Linear(d_model * 2, d_model)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)  # h shape [1,B,hidden]\n        h = h.squeeze(0)  # [B,hidden]\n        return self.proj(h)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_classes):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_classes)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# ---------------------------------------------------------------------\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ---------------------------------------------------------------------\n# config\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\n\nenc = Encoder(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ncontrast_loader = DataLoader(\n    ContrastiveSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\n# experiment data container\nexperiment_data = {\n    \"UniGRU_no_backward\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# contrastive pre-training\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    total = 0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total += loss.item() * v1.size(0)\n    print(f\"Contrastive epoch {ep}: loss={total/len(dataset['train']):.4f}\")\n\n# ---------------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0\n    for ids, labels, _ in train_loader:\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    model.eval()\n    val_loss = 0\n    preds = []\n    gts = []\n    seqs = []\n    with torch.no_grad():\n        for ids, labels, seq in dev_loader:\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # logging\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"metrics\"][\"train\"].append(\n        {\"SWA\": None, \"CWA\": None, \"SCAA\": None}\n    )\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA}\n    )\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"predictions\"] = preds\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"ground_truth\"] = gts\n    experiment_data[\"UniGRU_no_backward\"][\"SPR\"][\"epochs\"].append(ep)\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, csv, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# mandatory working dir + device setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------------------------------------------------------\n# experiment-data dict (Token-Dropout-Only ablation)\nexperiment_data = {\n    \"TokenDropoutOnly\": {\n        \"SPR\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ---------------------------------------------------------------------\n# helper functions for metrics\ndef _count_shape(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split() if tok))\n\n\ndef _count_color(seq: str) -> int:\n    return len(set(tok[1] for tok in seq.split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_shape(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [_count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\ndef scaa(seqs, y_true, y_pred):\n    w = [_count_shape(s) + _count_color(s) for s in seqs]\n    correct = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(correct) / max(sum(w), 1)\n\n\n# ---------------------------------------------------------------------\n# dataset loading (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n\n\ndef _load_csv(fp):\n    rows = []\n    with open(fp) as f:\n        rdr = csv.DictReader(f)\n        for r in rdr:\n            rows.append({\"sequence\": r[\"sequence\"], \"label\": int(r[\"label\"])})\n    return rows\n\n\ndef _generate_synth(n=3000, max_len=8):\n    shapes, colors = list(\"ABC\"), list(\"123\")\n\n    def rule(seq):\n        return sum(tok == \"A1\" for tok in seq) % 2  # parity of A1 tokens\n\n    rows = []\n    for _ in range(n):\n        toks = [\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, max_len))\n        ]\n        rows.append({\"sequence\": \" \".join(toks), \"label\": rule(toks)})\n    return rows\n\n\ndataset: Dict[str, List[Dict]] = {}\ntry:\n    if SPR_PATH.exists():\n        for split in [\"train\", \"dev\", \"test\"]:\n            dataset[split] = _load_csv(SPR_PATH / f\"{split}.csv\")\n    else:\n        raise FileNotFoundError\nexcept Exception:\n    print(\"Real SPR_BENCH not found \u2013 using synthetic data\")\n    dataset[\"train\"] = _generate_synth(4000)\n    dataset[\"dev\"] = _generate_synth(1000)\n    dataset[\"test\"] = _generate_synth(1000)\n\nprint({k: len(v) for k, v in dataset.items()})\n\n# ---------------------------------------------------------------------\n# vocabulary\nPAD, CLS = \"<PAD>\", \"<CLS>\"\nvocab = {PAD, CLS}\nfor split in dataset.values():\n    for row in split:\n        vocab.update(row[\"sequence\"].split())\nvocab.add(\"A1\")  # ensure fallback token is in vocab\nitos = list(vocab)\nstoi = {tok: i for i, tok in enumerate(itos)}\nvocab_size = len(itos)\n\n\ndef encode(seq, max_len=20):\n    ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\n    ids = ids[:max_len] + [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\n# ---------------------------------------------------------------------\n# Token-Dropout-Only augmentation for contrastive learning\ndef aug(seq: str) -> str:\n    toks = seq.split()\n    toks = [t for t in toks if random.random() > 0.1]  # dropout p=0.1\n    if not toks:\n        toks = [\"A1\"]  # fallback if sequence emptied\n    return \" \".join(toks)\n\n\n# ---------------------------------------------------------------------\n# PyTorch datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        s = self.rows[idx][\"sequence\"]\n        return (\n            torch.tensor(encode(aug(s), self.max_len)),\n            torch.tensor(encode(aug(s), self.max_len)),\n        )\n\n\nclass LabelledSPR(Dataset):\n    def __init__(self, rows, max_len=20):\n        self.rows = rows\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        r = self.rows[idx]\n        return (\n            torch.tensor(encode(r[\"sequence\"], self.max_len)),\n            torch.tensor(r[\"label\"]),\n            r[\"sequence\"],\n        )\n\n\n# ---------------------------------------------------------------------\n# model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.bigru = nn.GRU(d_model, d_model, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(2 * d_model, d_model)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.bigru(emb)\n        h = torch.cat([h[0], h[1]], dim=1)  # concat directions\n        return self.proj(h)  # [B,d_model]\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, enc, n_classes):\n        super().__init__()\n        self.enc = enc\n        self.head = nn.Linear(enc.proj.out_features, n_classes)\n\n    def forward(self, x):\n        feat = self.enc(x)\n        return self.head(feat), feat\n\n\n# ---------------------------------------------------------------------\ndef nt_xent(feats, temp=0.5):\n    f = F.normalize(feats, dim=1)\n    N = f.size(0) // 2\n    sim = torch.mm(f, f.t()) / temp\n    sim.fill_diagonal_(-1e9)\n    targets = torch.arange(N, 2 * N, device=f.device)\n    targets = torch.cat([targets, torch.arange(0, N, device=f.device)])\n    return F.cross_entropy(sim, targets)\n\n\n# ---------------------------------------------------------------------\n# experiment config\nBATCH = 128\nEPOCH_PRE = 2\nEPOCH_FT = 2\nMAX_LEN = 20\nnum_classes = len({r[\"label\"] for r in dataset[\"train\"]})\n\nenc = Encoder(vocab_size, 256).to(device)\nmodel = SPRClassifier(enc, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ncontrast_loader = DataLoader(\n    ContrastiveSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ntrain_loader = DataLoader(\n    LabelledSPR(dataset[\"train\"], MAX_LEN), batch_size=BATCH, shuffle=True\n)\ndev_loader = DataLoader(LabelledSPR(dataset[\"dev\"], MAX_LEN), batch_size=BATCH)\n\n# ---------------------------------------------------------------------\n# contrastive pre-training\nfor ep in range(1, EPOCH_PRE + 1):\n    model.train()\n    tot = 0\n    for v1, v2 in contrast_loader:\n        v1, v2 = v1.to(device), v2.to(device)\n        _, f1 = model(v1)\n        _, f2 = model(v2)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * v1.size(0)\n    print(f'Contrastive epoch {ep}: loss = {tot/len(dataset[\"train\"]):.4f}')\n\n# ---------------------------------------------------------------------\n# fine-tuning\ncriterion = nn.CrossEntropyLoss()\nfor ep in range(1, EPOCH_FT + 1):\n    model.train()\n    tr_loss = 0\n    for batch in train_loader:\n        ids, labels, _ = batch\n        ids, labels = ids.to(device), labels.to(device)\n        logits, _ = model(ids)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * ids.size(0)\n    tr_loss /= len(dataset[\"train\"])\n\n    # validation\n    model.eval()\n    val_loss = 0\n    preds, gts, seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            ids, labels, seq = batch\n            ids, labels = ids.to(device), labels.to(device)\n            logits, _ = model(ids)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * ids.size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(labels.cpu().tolist())\n            seqs.extend(seq)\n    val_loss /= len(dataset[\"dev\"])\n    SWA = shape_weighted_accuracy(seqs, gts, preds)\n    CWA = color_weighted_accuracy(seqs, gts, preds)\n    SCAA = scaa(seqs, gts, preds)\n\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | SWA={SWA:.3f} CWA={CWA:.3f} SCAA={SCAA:.3f}\"\n    )\n\n    # store metrics\n    exp = experiment_data[\"TokenDropoutOnly\"][\"SPR\"]\n    exp[\"metrics\"][\"train\"].append({\"SWA\": None, \"CWA\": None, \"SCAA\": None})\n    exp[\"metrics\"][\"val\"].append({\"SWA\": SWA, \"CWA\": CWA, \"SCAA\": SCAA})\n    exp[\"losses\"][\"train\"].append(tr_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"epochs\"].append(ep)\n    exp[\"predictions\"] = preds\n    exp[\"ground_truth\"] = gts\n\n# ---------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Contrastive epoch 1: loss = 4.0017', '\\n', 'Contrastive epoch 2: loss =\n3.9433', '\\n', 'Epoch 1: validation_loss = 0.0337 | SWA=0.990 CWA=0.991\nSCAA=0.990', '\\n', 'Epoch 2: validation_loss = 0.0085 | SWA=0.997 CWA=0.997\nSCAA=0.997', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n16/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test':\n10000}\", '\\n', 'Epoch 1: train_loss=0.1188  val_loss=0.0167 | SWA=0.996\nCWA=0.996 SCAA=0.996', '\\n', 'Epoch 2: train_loss=0.0168  val_loss=0.0147 |\nSWA=0.996 CWA=0.996 SCAA=0.996', '\\n', 'Epoch 3: train_loss=0.0031\nval_loss=0.0004 | SWA=1.000 CWA=1.000 SCAA=1.000', '\\n', 'Epoch 4:\ntrain_loss=0.0001  val_loss=0.0001 | SWA=1.000 CWA=1.000 SCAA=1.000', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n20/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Contrastive epoch 1: loss = 3.7142', '\\n', 'Contrastive epoch 2: loss =\n3.6313', '\\n', 'Epoch 1: validation_loss = 0.0183 | SWA=0.993 CWA=0.994\nSCAA=0.993', '\\n', 'Epoch 2: validation_loss = 0.0069 | SWA=0.998 CWA=0.997\nSCAA=0.998', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n21/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Contrastive epoch 1: loss = 4.0050', '\\n', 'Contrastive epoch 2: loss =\n3.9433', '\\n', 'Epoch 1: validation_loss = 0.0364 | SWA=0.988 CWA=0.989\nSCAA=0.989', '\\n', 'Epoch 2: validation_loss = 0.0118 | SWA=0.997 CWA=0.997\nSCAA=0.997', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n22/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Device:', ' ', 'cuda', '\\n', 'Contrastive 1: loss 4.1021', '\\n', 'Contrastive\n2: loss 4.0432', '\\n', 'Epoch 1 | A1_parity: loss=0.4556 SWA=0.719 CWA=0.728\nSCAA=0.724', '\\n', 'Epoch 1 | B2_parity: loss=0.5431 SWA=0.725 CWA=0.719\nSCAA=0.722', '\\n', 'Epoch 1 | MajorityA: loss=0.5072 SWA=0.743 CWA=0.740\nSCAA=0.741', '\\n', 'Epoch 2 | A1_parity: loss=0.5245 SWA=0.719 CWA=0.728\nSCAA=0.723', '\\n', 'Epoch 2 | B2_parity: loss=0.4819 SWA=0.735 CWA=0.728\nSCAA=0.731', '\\n', 'Epoch 2 | MajorityA: loss=0.4434 SWA=0.761 CWA=0.755\nSCAA=0.758', '\\n', 'Saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n23/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Contrastive epoch 1: loss = 3.9522', '\\n', 'Traceback (most recent call\nlast):\\n  File \"runfile.py\", line 226, in <module>\\n    for v1, v2 in\ncontrast_loader:\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 701, in __next__\\n    data =\nself._next_data()\\n           ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 757, in _next_data\\n    data =\nself._dataset_fetcher.fetch(index)  # may raise StopIteration\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n~~~~~~~~~~~~^^^^^\\n  File \"runfile.py\", line 130, in __getitem__\\n\ntorch.tensor(encode(aug(s), self.max_len)),\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 100, in encode\\n    ids\n= [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 100, in\n<listcomp>\\n    ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]\\n\n~~~~^^^^^\\nKeyError: \\'A1\\'\\n', 'Execution time: 4 seconds seconds (time limit\nis 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test':\n10000}\", '\\n', 'Contrastive epoch 1 loss=4.0017', '\\n', 'Contrastive epoch 2\nloss=3.9438', '\\n', 'FT Epoch 1: val_loss=0.5331 | SWA=0.729 CWA=0.729\nSCAA=0.729', '\\n', 'FT Epoch 2: val_loss=0.5167 | SWA=0.748 CWA=0.746\nSCAA=0.747', '\\n', 'Saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n21/working/experiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Contrastive epoch 1: loss=4.4456', '\\n', 'Contrastive epoch 2:\nloss=4.2046', '\\n', 'Epoch 1: val_loss=0.0780 | SWA=0.975 CWA=0.974 SCAA=0.975',\n'\\n', 'Epoch 2: val_loss=0.0510 | SWA=0.985 CWA=0.984 SCAA=0.984', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n16_01-26-03_context_aware_contrastive_learning_attempt_0/0-\nrun/process_ForkProcess-22/working/experiment_data.npy', '\\n', 'Execution time:\n4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 20000, 'dev': 5000, 'test': 10000}\",\n'\\n', 'Contrastive epoch 1: loss = 3.9744', '\\n', 'Contrastive epoch 2: loss =\n3.9265', '\\n', 'Epoch 1: validation_loss = 0.0377 | SWA=0.988 CWA=0.989\nSCAA=0.989', '\\n', 'Epoch 2: validation_loss = 0.0034 | SWA=0.999 CWA=0.999\nSCAA=0.999', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-16_01-26-\n03_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n23/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']"], "analysis": ["", "The execution of the training script completed successfully without any errors\nor bugs. The model achieved perfect validation metrics (SWA, CWA, and SCAA all\nreached 1.000) by the end of the fourth epoch. The experiment data was\nsuccessfully saved, and the execution time was well within the time limit. No\nissues were observed.", "", "The execution of the training script was successful, and there are no apparent\nbugs in the output. The script performed both contrastive pre-training and fine-\ntuning without errors. The metrics (SWA, CWA, and SCAA) improved significantly\nduring fine-tuning, indicating the effectiveness of the model. The experiment\ndata was also saved successfully. No issues were observed.", "", "The execution failed due to a KeyError: 'A1' when attempting to encode augmented\nsequences. This is likely because the augmented sequence contains tokens not\npresent in the vocabulary (stoi dictionary), such as 'A1', which was removed\nduring augmentation. To fix this, ensure that augmented sequences only contain\ntokens present in the vocabulary. One approach is to validate and adjust the\naugmented sequence before encoding, ensuring all tokens exist in the stoi\ndictionary.", "", "The execution output indicates that the training script ran successfully without\nany bugs. The contrastive pre-training and fine-tuning phases completed as\nexpected, showing decreasing loss values and improved metrics (SWA, CWA, SCAA)\nover epochs. The experiment data was saved successfully, and the execution time\nwas well within the limit. No issues were detected.", "The execution of the training script was successful. The model was trained and\nvalidated without any errors or bugs. The validation metrics (SWA, CWA, and\nSCAA) showed excellent performance, reaching nearly perfect scores by the second\nepoch. The experiment data was saved correctly, and the execution was completed\nwell within the time limit. No issues were identified in the script or its\noutput."], "exc_type": [null, null, null, null, null, "KeyError", null, null, null], "exc_info": [null, null, null, null, null, {"args": ["A1"]}, null, null, null], "exc_stack": [null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 226, "<module>", "for v1, v2 in contrast_loader:"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 701, "__next__", "data = self._next_data()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 757, "_next_data", "data = self._dataset_fetcher.fetch(index)  # may raise StopIteration"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "fetch", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "<listcomp>", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["runfile.py", 130, "__getitem__", "torch.tensor(encode(aug(s), self.max_len)),"], ["runfile.py", 100, "encode", "ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]"], ["runfile.py", 100, "<listcomp>", "ids = [stoi[CLS]] + [stoi[tok] for tok in seq.split()]"]], null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error on the training dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.015, "best_value": 0.015}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.0085, "best_value": 0.0085}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Classification Weighted Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9974, "best_value": 0.9974}]}, {"metric_name": "validation SCAA", "lower_is_better": false, "description": "Segment Classification Average Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9973, "best_value": 0.9973}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Segment Weighted Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9972, "best_value": 0.9972}]}]}, {"metric_names": [{"metric_name": "Shape-Weighted Accuracy", "lower_is_better": false, "description": "Accuracy metric weighted by shape categories.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "Color-Weighted Accuracy", "lower_is_better": false, "description": "Accuracy metric weighted by color categories.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "SCAA", "lower_is_better": false, "description": "Shape-Color Accuracy Average metric.", "data": [{"dataset_name": "SPR", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.0001, "best_value": 0.0001}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.0001, "best_value": 0.0001}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final loss calculated during training.", "data": [{"dataset_name": "SPR", "final_value": 0.0117, "best_value": 0.0117}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss calculated during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.0069, "best_value": 0.0069}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Final weighted accuracy for shapes during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.9977, "best_value": 0.9977}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Final weighted accuracy for colors during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.9974, "best_value": 0.9974}]}, {"metric_name": "validation shape-color additive accuracy", "lower_is_better": false, "description": "Final additive accuracy combining shape and color during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.9976, "best_value": 0.9976}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.0196, "best_value": 0.0196}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.0118, "best_value": 0.0118}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy for shape classification during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.997, "best_value": 0.997}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "The weighted accuracy for color classification during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.997, "best_value": 0.997}]}, {"metric_name": "validation shape-color additive accuracy", "lower_is_better": false, "description": "The additive accuracy for shape and color classification during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.997, "best_value": 0.997}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, with lower values indicating better performance.", "data": [{"dataset_name": "A1_parity", "final_value": 0.4728, "best_value": 0.4728}, {"dataset_name": "B2_parity", "final_value": 0.4728, "best_value": 0.4728}, {"dataset_name": "MajorityA", "final_value": 0.4728, "best_value": 0.4728}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, with lower values indicating better performance.", "data": [{"dataset_name": "A1_parity", "final_value": 0.5245, "best_value": 0.5245}, {"dataset_name": "B2_parity", "final_value": 0.4819, "best_value": 0.4819}, {"dataset_name": "MajorityA", "final_value": 0.4434, "best_value": 0.4434}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape-related features during validation.", "data": [{"dataset_name": "A1_parity", "final_value": 0.719, "best_value": 0.719}, {"dataset_name": "B2_parity", "final_value": 0.735, "best_value": 0.735}, {"dataset_name": "MajorityA", "final_value": 0.761, "best_value": 0.761}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by color-related features during validation.", "data": [{"dataset_name": "A1_parity", "final_value": 0.728, "best_value": 0.728}, {"dataset_name": "B2_parity", "final_value": 0.728, "best_value": 0.728}, {"dataset_name": "MajorityA", "final_value": 0.755, "best_value": 0.755}]}, {"metric_name": "shape+color aggregated accuracy", "lower_is_better": false, "description": "The aggregated accuracy combining shape and color features during validation.", "data": [{"dataset_name": "A1_parity", "final_value": 0.723, "best_value": 0.723}, {"dataset_name": "B2_parity", "final_value": 0.731, "best_value": 0.731}, {"dataset_name": "MajorityA", "final_value": 0.758, "best_value": 0.758}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR", "final_value": 0.5271, "best_value": 0.5271}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.5167, "best_value": 0.5167}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.748, "best_value": 0.748}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by color during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.746, "best_value": 0.746}]}, {"metric_name": "validation shape-color additive accuracy", "lower_is_better": false, "description": "The combined shape and color accuracy during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.747, "best_value": 0.747}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.0345, "best_value": 0.0345}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.051, "best_value": 0.051}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy for shape classification on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9847, "best_value": 0.9847}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy for color classification on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9842, "best_value": 0.9842}]}, {"metric_name": "validation SCAA", "lower_is_better": false, "description": "Shape and color average accuracy on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.9845, "best_value": 0.9845}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR", "final_value": 0.0148, "best_value": 0.0148}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.0034, "best_value": 0.0034}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "The Softmax Weighted Accuracy during validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.999, "best_value": 0.999}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "The Class Weighted Accuracy during validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.999, "best_value": 0.999}]}, {"metric_name": "validation SCAA", "lower_is_better": false, "description": "The Subclass Average Accuracy during validation phase.", "data": [{"dataset_name": "SPR", "final_value": 0.999, "best_value": 0.999}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, true], "plots": [["../../logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_val_metrics.png", "../../logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_label_distribution.png"], [], ["../../logs/0-run/experiment_results/experiment_bbf2de4bb55f48618c888a956f7e5a66_proc_3109992/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bbf2de4bb55f48618c888a956f7e5a66_proc_3109992/SPR_val_metrics.png"], ["../../logs/0-run/experiment_results/experiment_97d3186b3fde4bffa2a8a6924a58feda_proc_3109993/SPR_loss_curve.png", "../../logs/0-run/experiment_results/experiment_97d3186b3fde4bffa2a8a6924a58feda_proc_3109993/SPR_validation_metrics.png"], ["../../logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_scaa_curve.png", "../../logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_swa_curve.png", "../../logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_cwa_curve.png"], [], ["../../logs/0-run/experiment_results/experiment_d6973d31b8394b47a032d135d6a5c617_proc_3109992/SPR_loss_curve_frozen_encoder.png", "../../logs/0-run/experiment_results/experiment_d6973d31b8394b47a032d135d6a5c617_proc_3109992/SPR_val_metrics_frozen_encoder.png"], ["../../logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_confusion_matrix.png"]], "plot_paths": [["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_loss_curves.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_val_metrics.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_confusion_matrix.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_label_distribution.png"], [], ["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bbf2de4bb55f48618c888a956f7e5a66_proc_3109992/SPR_loss_curves.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bbf2de4bb55f48618c888a956f7e5a66_proc_3109992/SPR_val_metrics.png"], ["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_97d3186b3fde4bffa2a8a6924a58feda_proc_3109993/SPR_loss_curve.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_97d3186b3fde4bffa2a8a6924a58feda_proc_3109993/SPR_validation_metrics.png"], ["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_loss_curves.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_scaa_curve.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_swa_curve.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_cwa_curve.png"], [], ["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d6973d31b8394b47a032d135d6a5c617_proc_3109992/SPR_loss_curve_frozen_encoder.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d6973d31b8394b47a032d135d6a5c617_proc_3109992/SPR_val_metrics_frozen_encoder.png"], ["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_loss_curves.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_validation_metrics.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_confusion_matrix.png"], ["experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_loss_curves.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_validation_metrics.png", "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_confusion_matrix.png"]], "plot_analyses": [[{"analysis": "The cross-entropy loss curves show a consistent decrease in both training and validation loss over the epochs. This indicates that the model is learning effectively without overfitting, as the validation loss follows a similar downward trend as the training loss. The loss values are relatively low, suggesting good convergence behavior for the task.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_loss_curves.png"}, {"analysis": "The metrics for Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and Shape-Color Accuracy (SCAA) all increase steadily over the epochs. The scores are already very high, nearing 0.997, which suggests that the model performs exceptionally well on the SPR task. The slight difference between the metrics might indicate varying sensitivities to shape and color complexities, but all metrics point to strong performance.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_val_metrics.png"}, {"analysis": "The confusion matrix indicates excellent classification performance, with the majority of predictions aligning with the true labels. The high density along the diagonal shows that both classes are being accurately classified, with minimal misclassifications for either class.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_confusion_matrix.png"}, {"analysis": "The bar chart comparing the ground truth and predicted label distributions shows a near-perfect match between the two. This indicates that the model performs well in maintaining the class balance and accurately predicting the labels without bias towards any specific class.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8a33f23bcfd7419f92fa799829e32c66_proc_3108302/SPR_label_distribution.png"}], [], [{"analysis": "This plot shows the training and validation loss over two epochs. Both losses decrease steadily, indicating that the model is learning effectively. The training loss decreases more sharply compared to the validation loss, which suggests that the model is fitting the training data well. However, the relatively small gap between the training and validation losses implies that the model is generalizing reasonably well to unseen data. This trend is promising for the robustness of the learned embeddings.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bbf2de4bb55f48618c888a956f7e5a66_proc_3109992/SPR_loss_curves.png"}, {"analysis": "This plot displays the validation metrics (SWA, CWA, and SCAA) over epochs. All three metrics show a consistent upward trend, indicating that the model's performance is improving across these metrics. The slight differences in the slopes suggest that the model may be learning certain aspects of the task (e.g., shape or color recognition) at slightly different rates. The high accuracy values (above 0.993) suggest that the model is performing exceptionally well, but further evaluation on a more challenging test set would be necessary to confirm generalization.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bbf2de4bb55f48618c888a956f7e5a66_proc_3109992/SPR_val_metrics.png"}], [{"analysis": "The plot shows a clear reduction in both training and validation loss over two epochs. The training loss decreases at a faster rate compared to the validation loss, which indicates effective learning without overfitting at this stage. The consistent downward trend in validation loss implies that the model generalizes well to unseen data.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_97d3186b3fde4bffa2a8a6924a58feda_proc_3109993/SPR_loss_curve.png"}, {"analysis": "The plot demonstrates steady improvement in all three validation metrics (SWA, CWA, and SCAA) over two epochs, with CWA achieving slightly higher accuracy than SWA and SCAA. The close alignment of the metrics suggests that the model is performing consistently across different evaluation criteria. The upward trend indicates that the context-aware contrastive learning framework is effective in enhancing symbolic pattern recognition.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_97d3186b3fde4bffa2a8a6924a58feda_proc_3109993/SPR_validation_metrics.png"}], [{"analysis": "This plot shows the cross-entropy loss curves for training and validation sets across different datasets (A1_parity, B2_parity, and MajorityA). The training loss for A1_parity and B2_parity decreases steadily, indicating the model is learning effectively. However, the validation loss for A1_parity remains flat, suggesting potential overfitting or a lack of generalization. The MajorityA dataset exhibits consistent performance across both training and validation losses, which indicates a more stable learning process.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_loss_curves.png"}, {"analysis": "This plot illustrates the SCAA (Shape-Color Accuracy Average) metric on the validation set for different datasets over epochs. The MajorityA dataset shows a clear upward trend, indicating substantial improvement in performance with training. B2_parity also improves over time but at a slower rate. A1_parity remains flat, suggesting that the model struggles to improve on this dataset, possibly due to insufficient contextual learning or dataset complexity.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_scaa_curve.png"}, {"analysis": "This plot depicts the SWA (Shape-Weighted Accuracy) metric for validation over epochs. The MajorityA dataset again shows the most significant improvement, demonstrating the model's ability to learn shape diversity effectively. B2_parity exhibits a moderate upward trend, while A1_parity remains stagnant, indicating challenges in capturing shape-related features in this dataset.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_swa_curve.png"}, {"analysis": "This plot shows the CWA (Color-Weighted Accuracy) metric for validation over epochs. Similar to SWA, the MajorityA dataset demonstrates a strong upward trend, reflecting the model's ability to capture color diversity effectively. B2_parity improves gradually, while A1_parity stays flat, suggesting difficulties in learning color-related features for this dataset.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ff16f9083d9047baabc8a81aa2701738_proc_3109994/multi_synth_rule_diversity_cwa_curve.png"}], [], [{"analysis": "This plot shows the training and validation loss curves over two epochs for the SPR task with a frozen encoder. Both the training and validation loss decrease steadily, indicating that the model is learning effectively. The validation loss is consistently lower than the training loss, which may suggest that the model is not overfitting at this stage. However, the small gap between the two loss curves could also indicate that the model has limited capacity for further improvement with the current configuration.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d6973d31b8394b47a032d135d6a5c617_proc_3109992/SPR_loss_curve_frozen_encoder.png"}, {"analysis": "This plot displays the validation metrics (SWA, CWA, and SCAA) for the SPR task over two epochs. All three metrics show a consistent upward trend, indicating improved performance as training progresses. SWA is slightly higher than CWA and SCAA, suggesting that the model may be better at capturing shape-related features compared to color-related features or a combined assessment. The steady increase across all metrics reflects effective learning, though the gains are relatively small, which might imply the need for additional tuning or training epochs for substantial improvement.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d6973d31b8394b47a032d135d6a5c617_proc_3109992/SPR_val_metrics_frozen_encoder.png"}], [{"analysis": "The plot shows a clear and consistent decrease in both training and validation loss over epochs, indicating that the model is learning effectively. The validation loss is consistently lower than the training loss, which could suggest that the model is generalizing well to unseen data. However, this behavior could also indicate potential underfitting, as the training loss does not approach zero.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_loss_curves.png"}, {"analysis": "The validation metrics (SWA, CWA, and SCAA) remain constant at 1.0 throughout the epochs. This suggests that the model achieves perfect scores on the validation set, which might indicate overfitting or a lack of diversity in the validation set. It is essential to verify whether these metrics are calculated correctly and if the validation set is representative of the test distribution.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_validation_metrics.png"}, {"analysis": "The confusion matrix demonstrates that the model achieves high accuracy, with only a small number of misclassifications (37 false positives and 40 false negatives). The balance of errors across classes suggests that the model does not exhibit significant bias towards any particular class. This result aligns with the high validation metrics observed.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_a8fdb28192d64cc1bbe6b8dc7ee5b906_proc_3109993/SPR_confusion_matrix.png"}], [{"analysis": "This plot illustrates the training and validation loss over two epochs. Both losses decrease consistently, with the training loss showing a steeper decline compared to the validation loss. This indicates that the model is learning effectively while maintaining generalizability, as there is no sign of overfitting (e.g., validation loss increasing). The rapid decrease in loss suggests that the model is converging quickly, potentially due to the effectiveness of the contrastive learning framework and the chosen hyperparameters.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_loss_curves.png"}, {"analysis": "This plot shows the accuracy metrics (SWA, CWA, and SCAA) on the validation set over two epochs. All three metrics increase steadily and are closely aligned, indicating that the model is performing consistently across different evaluation criteria. The high and improving accuracy values suggest that the model is effectively capturing the symbolic rules in the dataset, and the context-aware contrastive learning approach is likely contributing to this performance.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_validation_metrics.png"}, {"analysis": "This confusion matrix represents the model's predictions at the final epoch. The model correctly classifies almost all instances, with only a few misclassifications (4 false positives and 0 false negatives). This indicates that the model has achieved near-perfect performance on the test set, with excellent precision and recall. The results further validate the hypothesis that the proposed method enhances symbolic pattern recognition.", "plot_path": "experiments/2025-08-16_01-26-03_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c1d6ae11dc19454293ea9d535acaa90a_proc_3109994/SPR_confusion_matrix.png"}]], "vlm_feedback_summary": ["The plots indicate strong performance of the context-aware contrastive learning\nframework. The loss curves show effective learning and convergence, while the\nvalidation metrics demonstrate exceptional accuracy and robustness. The\nconfusion matrix and label distribution further confirm that the model achieves\nhigh classification accuracy and maintains class balance.", "[]", "The provided plots indicate that the model is learning effectively and\ngeneralizing well to the validation set. The steady decrease in losses and the\nupward trend in validation metrics are promising signs of the model's capability\nto handle the SPR task. However, the high accuracy values warrant further\ntesting to ensure robustness and to avoid overfitting.", "The results indicate effective learning and generalization, with consistent\nimprovements in both loss reduction and validation metrics. The proposed\nframework appears to be performing well in enhancing symbolic pattern\nrecognition.", "The plots reveal that the MajorityA dataset consistently outperforms A1_parity\nand B2_parity across all metrics (loss, SCAA, SWA, and CWA). This suggests that\nthe model is more effective at learning from MajorityA, likely due to better\ncontextual information or simpler patterns. A1_parity's flat performance across\nmetrics indicates potential issues with the dataset or the model's ability to\ngeneralize to it. B2_parity shows moderate improvement, hinting at partial\nsuccess in learning symbolic patterns.", "[]", "The plots indicate effective learning with steady improvements in both loss and\nvalidation metrics. However, the limited scale of improvements suggests that\nfurther optimization may be necessary to achieve significant performance gains.", "The plots indicate strong performance with low loss values, perfect validation\nmetrics, and a balanced confusion matrix. However, there is a need to verify the\nvalidity of the validation metrics and ensure the dataset's representativeness\nto avoid overfitting concerns.", "The plots demonstrate that the model is learning effectively and generalizing\nwell. Training and validation losses decrease consistently, accuracy metrics\nimprove steadily, and the confusion matrix shows near-perfect classification.\nThese results strongly support the effectiveness of the proposed context-aware\ncontrastive learning approach for the SPR task."], "exec_time": [5.060753345489502, 10.350319385528564, 7.067150592803955, 6.180718898773193, 6.741247892379761, 4.018969535827637, 6.045442342758179, 4.752669811248779, 9.398087978363037], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['experiment_data']"], [], ["['SPR']"], ["['SPR']"], ["['MajorityA']"], [], ["['SPR']"], ["['SPR']"], ["['SPR']"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------- iterate over datasets ----------\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = np.array(ds_dict.get(\"epochs\", []))\n    train_loss = np.array(ds_dict.get(\"losses\", {}).get(\"train\", []))\n    val_loss = np.array(ds_dict.get(\"losses\", {}).get(\"val\", []))\n    val_metrics = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    preds = np.array(ds_dict.get(\"predictions\", []))\n    gts = np.array(ds_dict.get(\"ground_truth\", []))\n\n    # ---- Plot 1: loss curves ----\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"--\", label=\"Train\")\n        plt.plot(epochs, val_loss, \"-\", label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{ds_name} Loss Curves\\nLeft: Train (--), Right: Validation (\u2014)\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ---- Plot 2: validation metrics ----\n    try:\n        if val_metrics:\n            swa = np.array([m[\"SWA\"] for m in val_metrics])\n            cwa = np.array([m[\"CWA\"] for m in val_metrics])\n            scaa = np.array([m[\"SCAA\"] for m in val_metrics])\n            plt.figure()\n            plt.plot(epochs, swa, label=\"SWA\")\n            plt.plot(epochs, cwa, label=\"CWA\")\n            plt.plot(epochs, scaa, label=\"SCAA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{ds_name} Validation Metrics Across Epochs\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n            plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            raise ValueError(\"No validation metrics found\")\n    except Exception as e:\n        print(f\"Error creating metrics plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ---- Plot 3: confusion matrix ----\n    try:\n        if preds.size and gts.size:\n            labels = sorted(set(gts.tolist() + preds.tolist()))\n            n = len(labels)\n            cm = np.zeros((n, n), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[labels.index(t), labels.index(p)] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046)\n            plt.xticks(range(n), labels)\n            plt.yticks(range(n), labels)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{ds_name} Confusion Matrix\\nDataset: {ds_name}\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            raise ValueError(\"Predictions/ground truth missing\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---- Plot 4: label distribution ----\n    try:\n        if preds.size and gts.size:\n            uniq = sorted(set(gts.tolist() + preds.tolist()))\n            gt_counts = [np.sum(gts == u) for u in uniq]\n            pr_counts = [np.sum(preds == u) for u in uniq]\n            x = np.arange(len(uniq))\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, pr_counts, width, label=\"Predictions\")\n            plt.xlabel(\"Class Label\")\n            plt.ylabel(\"Count\")\n            plt.title(\n                f\"{ds_name} Label Distribution\\nLeft: Ground Truth, Right: Predictions\"\n            )\n            plt.xticks(x, uniq)\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_label_distribution.png\")\n            plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            raise ValueError(\"Predictions/ground truth missing\")\n    except Exception as e:\n        print(f\"Error creating label distribution for {ds_name}: {e}\")\n        plt.close()\n\n    # ---- console summary ----\n    if val_metrics:\n        best_scaa = max(m[\"SCAA\"] for m in val_metrics)\n        best_cwa = max(m[\"CWA\"] for m in val_metrics)\n        best_swa = max(m[\"SWA\"] for m in val_metrics)\n        print(\n            f\"{ds_name} \u2013 best validation SCAA={best_scaa:.3f}, \"\n            f\"CWA={best_cwa:.3f}, SWA={best_swa:.3f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\n\nWe load experiment_data.npy, iterate through each experiment and dataset to retrieve epochs, loss arrays and validation metrics. For every dataset we first create a line plot showing training- and validation-loss versus epoch, helping to diagnose over/under-fitting. Next we collect the per-epoch validation SWA, CWA and SCAA values and place them on one figure so the user can track the metric evolution. Because predictions and ground-truth vectors are stored only for the final epoch, we compute and print the overall accuracy and a 2\u00d72 confusion matrix; additionally we visualise this matrix with imshow to give a quick performance snapshot. Each of the three figures is wrapped in its own try-except block, saved to working_dir with descriptive filenames that include the dataset name and plot type, then closed. We respect the plotting guidelines: basic matplotlib, clear titles/subtitles, no synthetic data generation, and no more than five figures. Finally the script prints the last-epoch metrics for quick reference.", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"identity_views\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data:\n    epochs = spr_data[\"epochs\"]\n    tr_loss = spr_data[\"losses\"][\"train\"]\n    val_loss = spr_data[\"losses\"][\"val\"]\n    val_metrics = spr_data[\"metrics\"][\"val\"]  # list of dicts\n    swa = [m[\"SWA\"] for m in val_metrics]\n    cwa = [m[\"CWA\"] for m in val_metrics]\n    scaa = [m[\"SCAA\"] for m in val_metrics]\n\n    # --------------------------------------------------------------\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.title(\"SPR Dataset \u2013 Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved loss curves to {fname}\")\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # 2) Metric curves\n    try:\n        plt.figure()\n        plt.plot(epochs, swa, label=\"SWA\")\n        plt.plot(epochs, cwa, label=\"CWA\")\n        plt.plot(epochs, scaa, label=\"SCAA\")\n        plt.title(\"SPR Dataset \u2013 Validation Metrics Over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_val_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved validation metrics to {fname}\")\n    except Exception as e:\n        print(f\"Error creating metric plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------\n    # Print final epoch metrics\n    print(\"Final Validation Metrics:\")\n    print(f\"  SWA  = {swa[-1]:.3f}\")\n    print(f\"  CWA  = {cwa[-1]:.3f}\")\n    print(f\"  SCAA = {scaa[-1]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nif experiment_data:\n    spr = experiment_data[\"NoProjectionHead\"][\"SPR\"]\n    epochs = spr[\"epochs\"]\n    tr_loss = spr[\"losses\"][\"train\"]\n    val_loss = spr[\"losses\"][\"val\"]\n    swa = [m[\"SWA\"] for m in spr[\"metrics\"][\"val\"]]\n    cwa = [m[\"CWA\"] for m in spr[\"metrics\"][\"val\"]]\n    scaa = [m[\"SCAA\"] for m in spr[\"metrics\"][\"val\"]]\n\n    # -----------------------------------------------------------------\n    # Plot 1: Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR Dataset: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------------------\n    # Plot 2: Validation metrics\n    try:\n        plt.figure()\n        plt.plot(epochs, swa, label=\"SWA\")\n        plt.plot(epochs, cwa, label=\"CWA\")\n        plt.plot(epochs, scaa, label=\"SCAA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR Dataset: Validation Metrics\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_validation_metrics.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------------------\n    print(\n        \"Final Validation Metrics:\",\n        spr[\"metrics\"][\"val\"][-1] if spr[\"metrics\"][\"val\"] else {},\n    )\nelse:\n    print(\"No experiment data found \u2013 nothing to plot.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------------------------------------- load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndatasets = experiment_data.get(\"multi_synth_rule_diversity\", {})  # dict of datasets\ncolors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n\n\n# ----------------------------------------------------- helper\ndef get_curve(ex, key):  # key in ['losses','metrics']\n    if key == \"losses\":\n        tr = ex[\"losses\"][\"train\"]\n        val = ex[\"losses\"][\"val\"]\n        return list(range(1, len(tr) + 1)), tr, val\n    else:  # metrics\n        val = [d[key] for d in ex[\"metrics\"][\"val\"]]\n        return list(range(1, len(val) + 1)), val\n\n\n# ----------------------------------------------------- 1) loss curves\ntry:\n    plt.figure()\n    for i, (name, ex) in enumerate(datasets.items()):\n        epochs, tr, val = get_curve(ex, \"losses\")\n        c = colors[i % len(colors)]\n        plt.plot(epochs, tr, linestyle=\"--\", color=c, label=f\"{name}-train\")\n        plt.plot(epochs, val, linestyle=\"-\", color=c, label=f\"{name}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Loss Curves across Datasets (multi_synth_rule_diversity)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"multi_synth_rule_diversity_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ----------------------------------------------------- metric plots\nfor metric in [\"SCAA\", \"SWA\", \"CWA\"]:\n    try:\n        plt.figure()\n        for i, (name, ex) in enumerate(datasets.items()):\n            epochs, val = get_curve(ex, metric)\n            plt.plot(epochs, val, color=colors[i % len(colors)], label=name)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(metric)\n        plt.title(f\"Validation {metric} over Epochs (multi_synth_rule_diversity)\")\n        plt.legend()\n        fname = os.path.join(\n            working_dir, f\"multi_synth_rule_diversity_{metric.lower()}_curve.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {metric} plot: {e}\")\n        plt.close()\n\n# ----------------------------------------------------- print final metrics\nfor name, ex in datasets.items():\n    last = ex[\"metrics\"][\"val\"][-1] if ex[\"metrics\"][\"val\"] else {}\n    print(f\"{name} final metrics:\", last)\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Iterate through stored results\nfor model_name, datasets in experiment_data.items():\n    for dset_name, ed in datasets.items():\n        epochs = ed.get(\"epochs\", [])\n        train_loss = ed.get(\"losses\", {}).get(\"train\", [])\n        val_loss = ed.get(\"losses\", {}).get(\"val\", [])\n        val_metrics = ed.get(\"metrics\", {}).get(\"val\", [])\n\n        # 1) Loss curves -----------------------------------------------------\n        try:\n            plt.figure()\n            if train_loss:\n                plt.plot(epochs, train_loss, label=\"Train Loss\")\n            if val_loss:\n                plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset_name} Loss Curves ({model_name})\\nTraining vs Validation\")\n            plt.legend()\n            fname = f\"{dset_name}_loss_curve_{model_name}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dset_name}: {e}\")\n            plt.close()\n\n        # 2) Validation metric curves ----------------------------------------\n        try:\n            if val_metrics:\n                swa = [m[\"SWA\"] for m in val_metrics]\n                cwa = [m[\"CWA\"] for m in val_metrics]\n                scaa = [m[\"SCAA\"] for m in val_metrics]\n\n                plt.figure()\n                plt.plot(epochs, swa, label=\"SWA\")\n                plt.plot(epochs, cwa, label=\"CWA\")\n                plt.plot(epochs, scaa, label=\"SCAA\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Accuracy\")\n                plt.title(\n                    f\"{dset_name} Validation Metrics ({model_name})\\nSWA, CWA, SCAA\"\n                )\n                plt.legend()\n                fname = f\"{dset_name}_val_metrics_{model_name}.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n\n                print(f\"Final {dset_name} metrics ({model_name}): {val_metrics[-1]}\")\n        except Exception as e:\n            print(f\"Error creating metric plot for {dset_name}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"UniGRU_no_backward\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp:\n    epochs = exp[\"epochs\"]\n    tr_losses = exp[\"losses\"][\"train\"]\n    val_losses = exp[\"losses\"][\"val\"]\n    val_metrics = exp[\"metrics\"][\"val\"]  # list of dicts\n    preds = np.array(exp[\"predictions\"])\n    gts = np.array(exp[\"ground_truth\"])\n\n    # -----------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_losses, label=\"Train Loss\")\n        plt.plot(epochs, val_losses, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR Dataset \u2013 Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------\n    try:\n        swa = [d[\"SWA\"] for d in val_metrics]\n        cwa = [d[\"CWA\"] for d in val_metrics]\n        scaa = [d[\"SCAA\"] for d in val_metrics]\n\n        plt.figure()\n        plt.plot(epochs, swa, label=\"SWA\")\n        plt.plot(epochs, cwa, label=\"CWA\")\n        plt.plot(epochs, scaa, label=\"SCAA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR Dataset \u2013 Validation Metrics Over Epochs\")\n        plt.legend()\n        plt.ylim(0, 1)\n        plt.savefig(os.path.join(working_dir, \"SPR_validation_metrics.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metrics plot: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------\n    try:\n        n_classes = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((n_classes, n_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\n            \"SPR Dataset \u2013 Confusion Matrix (Final Epoch)\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.xticks(range(n_classes))\n        plt.yticks(range(n_classes))\n        for i in range(n_classes):\n            for j in range(n_classes):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.savefig(os.path.join(working_dir, \"SPR_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # -----------------------------------------------------\n    last_metrics = val_metrics[-1]\n    print(\n        f\"Final Validation Metrics \u2013 SWA: {last_metrics['SWA']:.3f}, \"\n        f\"CWA: {last_metrics['CWA']:.3f}, SCAA: {last_metrics['SCAA']:.3f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ------------------------------------------------------------------\n# helper to safely fetch the SPR sub-dict\ndef get_spr(exp_dict):\n    for ablation in exp_dict.values():  # e.g. 'TokenDropoutOnly'\n        if \"SPR\" in ablation:\n            return ablation[\"SPR\"]\n    return None\n\n\nspr = get_spr(experiment_data)\nif spr is None:\n    print(\"SPR dataset entry not found in experiment_data.\")\n    exit()\n\nepochs = spr[\"epochs\"]\nloss_tr = spr[\"losses\"][\"train\"]\nloss_val = spr[\"losses\"][\"val\"]\nmetrics_val = spr[\"metrics\"][\"val\"]  # list of dicts\npreds = spr[\"predictions\"]\ngts = spr[\"ground_truth\"]\n\n# ------------------------------------------------------------------\n# 1. Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_tr, label=\"Train Loss\")\n    plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR: Training vs. Validation Loss\")\n    plt.legend()\n    fpath = os.path.join(working_dir, \"SPR_loss_curves.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2. Validation metrics curves\ntry:\n    swa = [m[\"SWA\"] for m in metrics_val]\n    cwa = [m[\"CWA\"] for m in metrics_val]\n    scaa = [m[\"SCAA\"] for m in metrics_val]\n\n    plt.figure()\n    plt.plot(epochs, swa, label=\"SWA\")\n    plt.plot(epochs, cwa, label=\"CWA\")\n    plt.plot(epochs, scaa, label=\"SCAA\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR: Validation Metrics Over Epochs\")\n    plt.legend()\n    fpath = os.path.join(working_dir, \"SPR_validation_metrics.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metrics plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3. Confusion matrix (final epoch)\ntry:\n    classes = sorted(set(gts) | set(preds))\n    n_cls = len(classes)\n    cm = np.zeros((n_cls, n_cls), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar()\n    plt.xticks(range(n_cls), classes)\n    plt.yticks(range(n_cls), classes)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(\n        \"SPR: Confusion Matrix (Final Epoch)\\nLeft: Ground Truth, Right: Predictions\"\n    )\n    for i in range(n_cls):\n        for j in range(n_cls):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n    fpath = os.path.join(working_dir, \"SPR_confusion_matrix.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Final numeric summary\nif metrics_val:\n    final_metrics = metrics_val[-1]\n    print(\n        f\"Final Validation Metrics - SCAA: {final_metrics['SCAA']:.3f}, \"\n        f\"SWA: {final_metrics['SWA']:.3f}, CWA: {final_metrics['CWA']:.3f}\"\n    )\n"], "plot_plan": ["We load experiment_data.npy, iterate over every dataset stored, and extract per-epoch arrays for training loss, validation loss, and validation metrics (SWA, CWA, SCAA).  For each dataset we generate four plots inside independent try-except blocks: 1) train vs validation loss across epochs, 2) the three validation metrics across epochs, 3) a confusion matrix of final validation predictions vs. ground-truth labels, and 4) a bar chart comparing class\u2010label frequency in predictions and ground truth.  Every figure is given an informative title that clearly states the dataset and includes a \u201cLeft/Right\u201d subtitle when two quantities are contrasted.  Each plot is saved in the working directory with descriptive filenames (<dataset>_loss_curves.png, etc.) and the figure is immediately closed afterward to free resources.  We limit ourselves to these four figures (\u22645 as required) and respect the rule of plotting only data that already exists in the .npy file.  After plotting, we print simple summary statistics\u2014the best validation SCAA, CWA and SWA achieved during training\u2014for quick inspection on the console.  The code begins with the mandated import stanza, uses only basic matplotlib, and follows the required directory handling exactly.  All logic is kept concise and self-contained.", "", "We will load the saved experiment_data.npy from the working directory and first verify that the expected SPR entry exists. From this dictionary we extract the epoch list, the train/val loss histories, and the validation metrics (SWA, CWA, SCAA). We then produce two figures: (1) a line chart with train-vs-validation loss across epochs; (2) a line chart with the three validation metrics across epochs. Each figure is wrapped in its own try-except block, saved with descriptive names that include the dataset and curve type, and the figure is always closed afterward. We also print the final epoch\u2019s validation metrics so that they appear in the notebook/console. The plotting uses plain matplotlib without extra styling, titles are informative, and the code respects all directory and resource constraints outlined in the guidelines. Only data present in experiment_data.npy are accessed, and we limit the number of figures to two\u2014well under the five-figure maximum.", "We first load experiment_data.npy, then isolate the SPR entry produced by the \u201cNo-Projection-Head\u201d run.  From it we pull the epoch list together with per-epoch training/validation losses and the validation metrics SWA, CWA, SCAA.  We create two standard plots: (1) a line chart comparing training vs validation cross-entropy loss over epochs, (2) a line chart showing the three validation accuracies over epochs.  Each plot is wrapped in its own try-except block, is titled clearly, saved inside working_dir with descriptive filenames, and the figure is closed afterwards.  No synthetic values are generated; only the arrays stored in experiment_data are used.  After plotting, the script prints the last-epoch validation metrics so users can quickly see the final model quality.  The code follows the required imports, directory handling, plotting guidelines, and keeps everything concise.  Only two figures are emitted, staying well under the 5-figure limit.", "We load experiment_data.npy, gather per-epoch losses and the three weighted-accuracy metrics for every dataset. We then create at most four figures: (1) an aggregated training/validation loss curve where every line is labelled by dataset-split, and (2)\u2013(4) separate validation curves for SCAA, SWA and CWA (again coloured by dataset). Each plot is wrapped in its own try-except, given a descriptive title, saved under working_dir with meaningful names, and the figure is immediately closed. Finally we print the last-epoch metric values so the notebook logs a quick quantitative summary.", null, "We first load the saved experiment_data.npy and iterate through each recorded dataset (here only \u201cSPR\u201d).  Per-epoch training and validation losses are extracted, along with the validation metrics SWA, CWA, and SCAA.  Two figures are generated: a loss curve comparing training vs. validation loss, and a metric curve showing SWA, CWA, and SCAA across epochs.  Each figure is wrapped in its own try-except block, saved with descriptive filenames inside working_dir, and explicitly closed afterwards to avoid memory leaks.  Plot titles contain the dataset name and a subtitle clarifying the contents of each subplot.  No data outside experiment_data.npy are created or assumed, and no more than the required two figures are produced (well under the five-figure limit).  Finally, the script prints out the last epoch\u2019s validation metrics to give a quick textual summary of model performance.  All requirements for imports, directory handling, and basic matplotlib usage are respected.", "We will load the saved NumPy dictionary, extract the single experiment (\u201cUniGRU_no_backward / SPR\u201d), and pull out per-epoch arrays for training/validation loss and validation metrics (SWA, CWA, SCAA). Three plots are useful: (1) loss curves, (2) metric curves, and (3) a confusion-matrix heat-map of final predictions vs. ground truth. Each figure will be wrapped in its own try-except, saved to working_dir with clear filenames, titled appropriately, and closed afterward. Only data that actually exist in experiment_data.npy will be used\u2014nothing is synthesized. If the file cannot be loaded we simply print the error. After plotting we also print the final epoch\u2019s validation metrics so the evaluation values are visible in stdout. At most one figure per kind is generated, complying with the five-figure cap. The script starts with the mandated imports and creates working_dir if it does not exist.", "We load experiment_data.npy, pull out the inner dict for the SPR dataset, and extract the epoch list, loss arrays (train/val) and validation metric dictionaries (SWA, CWA, SCAA).  Using these we draw three standard figures: (1) training vs. validation loss over epochs, (2) validation metrics over epochs with three separate lines in one panel, and (3) a confusion-matrix heat-map for the final epoch based on the stored predictions and ground-truth labels.  All plots are enclosed in their own try-except blocks, use basic matplotlib, carry informative titles/sub-titles, and are saved to working_dir with descriptive names.  We cap the number of figures at three\u2014well below the five-plot limit\u2014and close every figure explicitly.  After plotting we also recompute and print the final SCAA, SWA and CWA to give a quick numerical summary.  The code begins with the mandated imports and working_dir definition, opens the saved .npy file safely, and creates working_dir if it is missing.  Confusion matrix is built with numpy alone (no external libs).  All operations rely solely on data present in experiment_data.npy; nothing is synthesized.  The entire script is concise and can be dropped into a single cell or file."], "ablation_name": [null, "No Contrastive Pre-training (Supervised-Only Baseline)", "Identity-Views Contrastive Pre-training (No Augmentations)", "No-Projection-Head Encoder", "Multi-Synthetic-Dataset Training (Rule Diversity Ablation)", "Mean-Pooling Encoder (Remove GRU Sequence Modeling)", "Frozen-Encoder Fine-Tuning", "Unidirectional-GRU Encoder (Remove Backward Context)", "Token-Dropout-Only Contrastive Augmentation"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["Below is a small utility that immediately loads the experiment data, fetches the\nlast-epoch (i.e., final) values for every stored metric and loss, and prints\nthem clearly for each dataset contained in the file.  It assumes that \u201cbest\u201d\nmeans the final logged value (the list\u2019s last element).  The script follows the\nexact directory/location conventions used in the training code, contains no\nplots, and places no code behind an `if __name__ == \"__main__\":` guard.  Simply\nrun the file and it will print the requested information.", "The script will locate the saved NumPy file in the working directory, load it as\na Python dictionary, and then iterate through the stored experiments and\ndatasets. For every dataset (e.g., \u201cSPR\u201d), it will compute the best (maximum)\nvalidation accuracies and the best (minimum) losses across all epochs, then\nprint them with explicit, descriptive names. Execution starts immediately\u2014there\nis no `if __name__ == \"__main__\":` guard.", "The script will load the saved numpy dictionary, walk through every experiment\nblock and dataset, pick the last-epoch entry for every stored list, and print\nthose values with explicit names. Because the training accuracy metrics were\nlogged as None we only report the meaningful validation metrics together with\nboth training and validation losses. The code runs immediately on import and\nfollows all structural constraints.", "We simply reload the stored numpy dictionary, navigate down to the No-\nProjection-Head experiment\u2019s SPR entry, grab the final (last-epoch) values for\nevery list that was populated during training/validation, and print them with\nexplicit, descriptive names.  Only the last element of each list is reported,\nwhich acts as the \u201cfinal\u201d value.  The code lives at top level so that it runs\nimmediately when executed, and it respects the required working directory\nlocation.", "The script first loads the saved numpy dictionary from the working directory,\nthen iterates over each dataset contained in the \u201cmulti_synth_rule_diversity\u201d\nexperiment. For every dataset it fetches the last (i.e., final-epoch) entry of\neach stored list: validation metrics (SWA, CWA, SCAA) as well as training and\nvalidation losses. It prints the dataset name followed by clearly-labelled\nmetric names and their corresponding final values. All logic executes directly\nat the top level so the file runs immediately when invoked.", "", "We load the stored numpy dictionary from the working directory, navigate through\nits nested structure to reach each dataset (e.g., \u201cSPR\u201d), and take the last\nentry in every logged list to obtain the final values. For losses we directly\nread the last element of the \u201ctrain\u201d and \u201cval\u201d loss lists, and for the\nvalidation metrics we read the last dictionary (corresponding to the final\nepoch) and print each metric separately with explicit names. Everything is\nexecuted in the global scope so the script runs immediately upon execution.", "The script will directly load the saved NumPy dictionary, drill down through its\nnested structure (model \u2192 dataset/task), and retrieve the lists that store\nlosses and metric dictionaries. It then selects the last element of each\nlist\u2014which corresponds to the final epoch\u2014to report as the \u201cfinal\u201d value (this\nsatisfies the \u201cbest or final\u201d requirement). Each dataset name is printed first,\nfollowed by clearly-labeled metric lines such as \u201ctraining loss,\u201d \u201cvalidation\nloss,\u201d \u201cvalidation shape weighted accuracy,\u201d etc. No plots are generated, and\nall executable code is placed at the global scope so it runs immediately when\nthe file is executed.", "The script will directly load the numpy file from the working directory, walk\nthrough every stored experiment and its datasets, and then compute the \u201cbest\u201d\nscore for each metric list: minimum for losses and maximum for all other\nmetrics. For clarity it prints the dataset name first, followed by clearly-\nlabeled final/best values such as \u201ctraining loss,\u201d \u201cvalidation loss,\u201d\n\u201cvalidation SWA,\u201d etc. Because every metric list already keeps its historical\nvalues, simply selecting the min or max (as appropriate) satisfies the \u201cbest or\nfinal\u201d requirement without extra checks."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfp = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(fp, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef _last(lst):\n    \"\"\"Return the last non-None element of a list (or None).\"\"\"\n    for item in reversed(lst):\n        if item is not None:\n            return item\n    return None\n\n\n# ------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ----- losses -----\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"final validation loss: {val_losses[-1]:.4f}\")\n\n    # ----- metrics -----\n    metrics = data.get(\"metrics\", {})\n    train_metrics = metrics.get(\"train\", [])\n    val_metrics = metrics.get(\"val\", [])\n\n    # Helper: collect all metric keys that exist\n    metric_keys = set()\n    if train_metrics:\n        metric_keys.update(train_metrics[-1].keys())\n    if val_metrics:\n        metric_keys.update(val_metrics[-1].keys())\n\n    for key in sorted(metric_keys):\n        # training metrics\n        if train_metrics:\n            val = _last([m.get(key) for m in train_metrics])\n            if val is not None:\n                print(f\"final training {key}: {val:.4f}\")\n        # validation metrics\n        if val_metrics:\n            val = _last([m.get(key) for m in val_metrics])\n            if val is not None:\n                print(f\"final validation {key}: {val:.4f}\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------------------\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------------------\n# iterate through experiments and datasets\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, details in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # --- collect metric trajectories ---\n        val_metrics = details[\"metrics\"][\"val\"]  # list of dicts\n        train_losses = details[\"losses\"][\"train\"]  # list of floats\n        val_losses = details[\"losses\"][\"val\"]  # list of floats\n\n        # best validation metrics (higher is better)\n        swa_values = [m[\"SWA\"] for m in val_metrics if m[\"SWA\"] is not None]\n        cwa_values = [m[\"CWA\"] for m in val_metrics if m[\"CWA\"] is not None]\n        scaa_values = [m[\"SCAA\"] for m in val_metrics if m[\"SCAA\"] is not None]\n\n        best_swa = max(swa_values) if swa_values else None\n        best_cwa = max(cwa_values) if cwa_values else None\n        best_scaa = max(scaa_values) if scaa_values else None\n\n        # best (minimum) losses\n        best_train_loss = min(train_losses) if train_losses else None\n        best_val_loss = min(val_losses) if val_losses else None\n\n        # --- print results with explicit names ---\n        if best_swa is not None:\n            print(f\"best validation Shape-Weighted Accuracy: {best_swa:.3f}\")\n        if best_cwa is not None:\n            print(f\"best validation Color-Weighted Accuracy: {best_cwa:.3f}\")\n        if best_scaa is not None:\n            print(f\"best validation SCAA: {best_scaa:.3f}\")\n        if best_train_loss is not None:\n            print(f\"best training loss: {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"best validation loss: {best_val_loss:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the saved experiment results\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\ndef _format_float(x):\n    \"\"\"Helper to format floats uniformly.\"\"\"\n    return f\"{x:.4f}\" if isinstance(x, float) else str(x)\n\n\ndef report_metrics():\n    for exp_name, exp_content in experiment_data.items():  # e.g. 'identity_views'\n        for dataset_name, ds_data in exp_content.items():  # e.g. 'SPR'\n            print(f\"\\nDataset: {dataset_name}\")\n\n            # -------- losses (take final value) --------\n            train_losses = ds_data[\"losses\"].get(\"train\", [])\n            val_losses = ds_data[\"losses\"].get(\"val\", [])\n            if train_losses:\n                print(\"final training loss:\", _format_float(train_losses[-1]))\n            if val_losses:\n                print(\"final validation loss:\", _format_float(val_losses[-1]))\n\n            # -------- metrics (take final validation metrics) --------\n            val_metrics = ds_data[\"metrics\"].get(\"val\", [])\n            if val_metrics:\n                last_val = val_metrics[-1] or {}\n                if \"SWA\" in last_val and last_val[\"SWA\"] is not None:\n                    print(\n                        \"validation shape weighted accuracy:\",\n                        _format_float(last_val[\"SWA\"]),\n                    )\n                if \"CWA\" in last_val and last_val[\"CWA\"] is not None:\n                    print(\n                        \"validation color weighted accuracy:\",\n                        _format_float(last_val[\"CWA\"]),\n                    )\n                if \"SCAA\" in last_val and last_val[\"SCAA\"] is not None:\n                    print(\n                        \"validation shape-color additive accuracy:\",\n                        _format_float(last_val[\"SCAA\"]),\n                    )\n\n\n# ---------------------------------------------------------------------\nreport_metrics()\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the saved experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# navigate to the SPR results inside the NoProjectionHead experiment\nspr_results = experiment_data[\"NoProjectionHead\"][\"SPR\"]\n\ntrain_losses = spr_results[\"losses\"][\"train\"]  # list of floats\nval_losses = spr_results[\"losses\"][\"val\"]  # list of floats\nval_metrics = spr_results[\"metrics\"][\"val\"]  # list of dicts with SWA/CWA/SCAA\n\n# ---------------------------------------------------------------------\n# fetch the FINAL values from each list\nfinal_train_loss = train_losses[-1] if train_losses else None\nfinal_val_loss = val_losses[-1] if val_losses else None\nfinal_val_metric = val_metrics[-1] if val_metrics else {}\n\n# ---------------------------------------------------------------------\n# printing results with explicit names\nprint(\"SPR dataset:\")\nif final_train_loss is not None:\n    print(f\"training loss: {final_train_loss:.4f}\")\nif final_val_loss is not None:\n    print(f\"validation loss: {final_val_loss:.4f}\")\n\nif final_val_metric:\n    if \"SWA\" in final_val_metric and final_val_metric[\"SWA\"] is not None:\n        print(f\"validation shape weighted accuracy: {final_val_metric['SWA']:.3f}\")\n    if \"CWA\" in final_val_metric and final_val_metric[\"CWA\"] is not None:\n        print(f\"validation color weighted accuracy: {final_val_metric['CWA']:.3f}\")\n    if \"SCAA\" in final_val_metric and final_val_metric[\"SCAA\"] is not None:\n        print(\n            f\"validation shape-color additive accuracy: {final_val_metric['SCAA']:.3f}\"\n        )\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# locate and load the experiment data file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------\n# extract and print final metrics for each dataset\nexp = experiment_data.get(\"multi_synth_rule_diversity\", {})\nfor dataset_name, data in exp.items():\n    # indices of the final epoch (lists grow epoch-by-epoch)\n    final_idx = -1\n\n    # metric dictionaries\n    val_metrics = data[\"metrics\"][\"val\"][final_idx] if data[\"metrics\"][\"val\"] else {}\n    train_losses = (\n        data[\"losses\"][\"train\"][final_idx] if data[\"losses\"][\"train\"] else None\n    )\n    val_losses = data[\"losses\"][\"val\"][final_idx] if data[\"losses\"][\"val\"] else None\n\n    # print results\n    print(f\"Dataset: {dataset_name}\")\n    if train_losses is not None:\n        print(f\"Final training loss: {train_losses:.4f}\")\n    if val_losses is not None:\n        print(f\"Final validation loss: {val_losses:.4f}\")\n\n    # iterate over known metric keys in a stable order\n    for key in [\"SWA\", \"CWA\", \"SCAA\"]:\n        if key in val_metrics and val_metrics[key] is not None:\n            metric_name = {\n                \"SWA\": \"Shape-weighted accuracy (validation)\",\n                \"CWA\": \"Color-weighted accuracy (validation)\",\n                \"SCAA\": \"Shape+Color aggregated accuracy (validation)\",\n            }[key]\n            print(f\"{metric_name}: {val_metrics[key]:.3f}\")\n    print()  # blank line between datasets\n", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ #\n# 1) locate and load experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------ #\n# 2) iterate over stored runs / datasets\nfor run_name, run_content in experiment_data.items():  # e.g. \"frozen_encoder\"\n    for dataset_name, ds_data in run_content.items():  # e.g. \"SPR\"\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # losses\n        train_losses = ds_data[\"losses\"][\"train\"]\n        val_losses = ds_data[\"losses\"][\"val\"]\n        final_train_loss = train_losses[-1] if train_losses else None\n        final_val_loss = val_losses[-1] if val_losses else None\n\n        # validation metrics (take final epoch entry)\n        final_metrics = (\n            ds_data[\"metrics\"][\"val\"][-1] if ds_data[\"metrics\"][\"val\"] else {}\n        )\n\n        # print results\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"final validation loss: {final_val_loss:.4f}\")\n\n        for m_name, m_val in final_metrics.items():\n            if m_val is not None:\n                full_name = {\n                    \"SWA\": \"final validation shape-weighted accuracy\",\n                    \"CWA\": \"final validation color-weighted accuracy\",\n                    \"SCAA\": \"final validation shape-color additive accuracy\",\n                }.get(m_name, m_name)\n                print(f\"{full_name}: {m_val:.3f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to print metrics cleanly\ndef print_final_metrics(data_dict):\n    \"\"\"\n    data_dict corresponds to one dataset entry, e.g.\n    experiment_data[model_name][dataset_name]\n    \"\"\"\n    # Final (last) epoch values\n    final_train_loss = (\n        data_dict[\"losses\"][\"train\"][-1] if data_dict[\"losses\"][\"train\"] else None\n    )\n    final_val_loss = (\n        data_dict[\"losses\"][\"val\"][-1] if data_dict[\"losses\"][\"val\"] else None\n    )\n    final_val_metrics = (\n        data_dict[\"metrics\"][\"val\"][-1] if data_dict[\"metrics\"][\"val\"] else {}\n    )\n\n    if final_train_loss is not None:\n        print(f\"  training loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"  validation loss: {final_val_loss:.4f}\")\n\n    # Detailed validation metrics\n    for key, value in final_val_metrics.items():\n        if value is not None:\n            metric_label = {\n                \"SWA\": \"validation shape weighted accuracy\",\n                \"CWA\": \"validation color weighted accuracy\",\n                \"SCAA\": \"validation SCAA\",\n            }.get(key, f\"validation {key}\")\n            print(f\"  {metric_label}: {value:.4f}\")\n\n\n# ---------------------------------------------------------------------\n# Iterate through models and their datasets\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, dataset_info in datasets.items():\n        print(f\"{dataset_name}:\")\n        print_final_metrics(dataset_info)\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the saved experiment dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef _best(values, larger_is_better=True):\n    \"\"\"Return best value from a list according to direction.\"\"\"\n    if not values:\n        return None\n    return max(values) if larger_is_better else min(values)\n\n\n# ------------------------------------------------------------------\nfor exp_name, exp_dict in experiment_data.items():\n    for dataset_name, data in exp_dict.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---- losses ------------------------------------------------\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n        best_train_loss = _best(train_losses, larger_is_better=False)\n        best_val_loss = _best(val_losses, larger_is_better=False)\n\n        if best_train_loss is not None:\n            print(f\"training loss: {best_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"validation loss: {best_val_loss:.4f}\")\n\n        # ---- metrics ----------------------------------------------\n        for split, metric_list in data[\"metrics\"].items():\n            # We are only asked for best/final values; pick best (max) for each metric\n            if not metric_list:\n                continue\n            # transpose list-of-dicts to dict-of-lists\n            metric_names = metric_list[0].keys()\n            metrics_by_name = {\n                m: [d[m] for d in metric_list if d[m] is not None] for m in metric_names\n            }\n\n            for m_name, values in metrics_by_name.items():\n                if not values:\n                    continue\n                best_val = _best(values, larger_is_better=True)\n                split_label = \"training\" if split == \"train\" else \"validation\"\n                print(f\"{split_label} {m_name}: {best_val:.3f}\")\n"], "parse_term_out": ["['\\nDataset: SPR', '\\n', 'final training loss: 0.0150', '\\n', 'final validation\nloss: 0.0085', '\\n', 'final validation CWA: 0.9974', '\\n', 'final validation\nSCAA: 0.9973', '\\n', 'final validation SWA: 0.9972', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR', '\\n', 'best validation Shape-Weighted Accuracy: 1.000', '\\n',\n'best validation Color-Weighted Accuracy: 1.000', '\\n', 'best validation SCAA:\n1.000', '\\n', 'best training loss: 0.0001', '\\n', 'best validation loss:\n0.0001', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR', '\\n', 'final training loss:', ' ', '0.0117', '\\n', 'final\nvalidation loss:', ' ', '0.0069', '\\n', 'validation shape weighted accuracy:', '\n', '0.9977', '\\n', 'validation color weighted accuracy:', ' ', '0.9974', '\\n',\n'validation shape-color additive accuracy:', ' ', '0.9976', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR dataset:', '\\n', 'training loss: 0.0196', '\\n', 'validation loss: 0.0118',\n'\\n', 'validation shape weighted accuracy: 0.997', '\\n', 'validation color\nweighted accuracy: 0.997', '\\n', 'validation shape-color additive accuracy:\n0.997', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: A1_parity', '\\n', 'Final training loss: 0.4728', '\\n', 'Final\nvalidation loss: 0.5245', '\\n', 'Shape-weighted accuracy (validation): 0.719',\n'\\n', 'Color-weighted accuracy (validation): 0.728', '\\n', 'Shape+Color\naggregated accuracy (validation): 0.723', '\\n', '\\n', 'Dataset: B2_parity',\n'\\n', 'Final training loss: 0.4728', '\\n', 'Final validation loss: 0.4819',\n'\\n', 'Shape-weighted accuracy (validation): 0.735', '\\n', 'Color-weighted\naccuracy (validation): 0.728', '\\n', 'Shape+Color aggregated accuracy\n(validation): 0.731', '\\n', '\\n', 'Dataset: MajorityA', '\\n', 'Final training\nloss: 0.4728', '\\n', 'Final validation loss: 0.4434', '\\n', 'Shape-weighted\naccuracy (validation): 0.761', '\\n', 'Color-weighted accuracy (validation):\n0.755', '\\n', 'Shape+Color aggregated accuracy (validation): 0.758', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['\\nDataset: SPR', '\\n', 'final training loss: 0.5271', '\\n', 'final validation\nloss: 0.5167', '\\n', 'final validation shape-weighted accuracy: 0.748', '\\n',\n'final validation color-weighted accuracy: 0.746', '\\n', 'final validation\nshape-color additive accuracy: 0.747', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR:', '\\n', '  training loss: 0.0345', '\\n', '  validation loss: 0.0510',\n'\\n', '  validation shape weighted accuracy: 0.9847', '\\n', '  validation color\nweighted accuracy: 0.9842', '\\n', '  validation SCAA: 0.9845', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR', '\\n', 'training loss: 0.0148', '\\n', 'validation loss:\n0.0034', '\\n', 'validation SWA: 0.999', '\\n', 'validation CWA: 0.999', '\\n',\n'validation SCAA: 0.999', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}