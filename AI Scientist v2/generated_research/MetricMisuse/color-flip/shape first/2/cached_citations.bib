
% This paper introduces SimCLR, a foundational framework for contrastive learning of visual representations. It systematically studies key components like data augmentations, nonlinear transformations, and batch size requirements, which are directly relevant to the methodology of context-aware contrastive learning described in the proposed study. It should be cited in the related work section when discussing the principles and benefits of contrastive learning.
@article{chen2020asf,
 author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {A Simple Framework for Contrastive Learning of Visual Representations},
 volume = {abs/2002.05709},
 year = {2020}
}

% This paper discusses the limitations of symbolic reasoning methods, including challenges with generalization and scalability, and explores their integration with neural networks in Neurosymbolic AI. It is relevant for highlighting the gaps in traditional symbolic reasoning approaches and motivating the use of contrastive learning for symbolic pattern recognition. This should be cited in the related work section when discussing symbolic reasoning and its limitations.
@article{pulicharla2025neurosymbolicab,
 author = {Mohan Raja Pulicharla},
 booktitle = {World Journal of Advanced Research and Reviews},
 journal = {World Journal of Advanced Research and Reviews},
 title = {Neurosymbolic AI: Bridging neural networks and symbolic reasoning},
 year = {2025}
}

% Paper 1 discusses similarity-guided data augmentation and denoising techniques to enhance contrastive learning, relevant for supporting the hypothesis on the role of augmentation in robust feature representation learning. Paper 3 introduces a novel data augmentation method tailored for semi-supervised contrastive learning of time series, highlighting the importance of task-specific augmentation strategies. These papers should be cited in the related work section to strengthen the discussion on data augmentation and denoising techniques in contrastive learning.
@inproceedings{choi2025similarityguideddf,
 author = {Jinkyeong Choi and Yejin Noh and Donghyeon Park},
 title = {Similarity-Guided Diffusion for Contrastive Sequential Recommendation},
 year = {2025}
}

@article{kim2024semisupervisedcl,
 author = {Dokyun Kim and Sukhyun Cho and Heewoong Chae and Jonghun Park and Jaeseok Huh},
 booktitle = {Intelligent Data Analysis},
 journal = {Intelligent Data Analysis},
 pages = {94 - 115},
 title = {Semi-supervised contrastive learning with decomposition-based data augmentation for time series classification},
 volume = {29},
 year = {2024}
}

% This paper introduces DSR-LM, a Differentiable Symbolic Reasoning framework for enhancing logical reasoning in pre-trained language models using symbolic programming. It discusses methods for improving reasoning accuracy and adapting systematic changes, which are relevant to symbolic reasoning and the improvement of benchmarks like SPR_BENCH. This citation should be included in the related work section to discuss advancements in symbolic reasoning and logical deduction frameworks.
@article{zhang2023improvedlr,
 author = {Hanlin Zhang and Jiani Huang and Ziyang Li and M. Naik and Eric P. Xing},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 pages = {3062-3077},
 title = {Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming},
 year = {2023}
}
