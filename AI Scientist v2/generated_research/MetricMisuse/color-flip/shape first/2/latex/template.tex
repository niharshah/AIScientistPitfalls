\documentclass{article} % For LaTeX2e
\usepackage{iclr2025,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Custom
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\graphicspath{{../figures/}} 

\begin{filecontents}{references.bib}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{chen2020asf,
 author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {A Simple Framework for Contrastive Learning of Visual Representations},
 volume = {abs/2002.05709},
 year = {2020}
}

@article{pulicharla2025neurosymbolicab,
 author = {Mohan Raja Pulicharla},
 booktitle = {World Journal of Advanced Research and Reviews},
 journal = {World Journal of Advanced Research and Reviews},
 title = {Neurosymbolic AI: Bridging neural networks and symbolic reasoning},
 year = {2025}
}

@inproceedings{choi2025similarityguideddf,
 author = {Jinkyeong Choi and Yejin Noh and Donghyeon Park},
 title = {Similarity-Guided Diffusion for Contrastive Sequential Recommendation},
 year = {2025}
}

@article{kim2024semisupervisedcl,
 author = {Dokyun Kim and Sukhyun Cho and Heewoong Chae and Jonghun Park and Jaeseok Huh},
 booktitle = {Intelligent Data Analysis},
 journal = {Intelligent Data Analysis},
 pages = {94 - 115},
 title = {Semi-supervised contrastive learning with decomposition-based data augmentation for time series classification},
 volume = {29},
 year = {2024}
}

@article{zhang2023improvedlr,
 author = {Hanlin Zhang and Jiani Huang and Ziyang Li and M. Naik and Eric P. Xing},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 pages = {3062-3077},
 title = {Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming},
 year = {2023}
}
\end{filecontents}

\title{Context-Aware Contrastive Learning for Symbolic Sequence Reasoning}

\author{Anonymous}

\begin{document}
\maketitle

\begin{abstract}
We present a context-aware contrastive learning approach for symbolic sequence recognition on the Synthetic PolyRule Reasoning (SPR) task. Our self-supervised method uses carefully designed data augmentations and denoising to produce robust embeddings for discrete sequences. In downstream evaluation on the SPR\_BENCH dataset, these representations, once fine-tuned, achieve near-perfect Shape-Weighted Accuracy (SWA) and Color-Weighted Accuracy (CWA). Although the synthetic domain is less noisy than real-world scenarios, we discuss pitfalls related to ambiguous symbolic elements or limited augmentable dimensions. These insights can guide future attempts to extend contrastive strategies to more complex, naturally-occurring symbolic datasets.
\end{abstract}

\section{Introduction}
Symbolic sequence recognition remains critical for tasks that involve combinatorial rules or structured representations, including certain robotics and formal language contexts. However, learning robust representations for sequences of discrete tokens can be challenging, particularly when faced with real-world imperfections. Recent progress in contrastive learning \citep{chen2020asf} shows promise, motivating us to explore a self-supervised approach on synthetic but rule-intensive data. By identifying and analyzing pitfalls in discrete symbolic domains, our results highlight lessons relevant to broader applications of contrastive methods in logic-centric settings.

Our contributions are: (1) a set of context-aware augmentations for symbolic tokens that respect logical constraints, (2) an empirical demonstration that combining contrastive pre-training with subsequent fine-tuning yields strong performance on SWA/CWA in SPR\_BENCH, and (3) a detailed discussion of pitfalls such as domain shift brittleness and token ambiguity.

\section{Related Work}
Contrastive learning has been applied successfully to image and text \citep{chen2020asf, choi2025similarityguideddf, kim2024semisupervisedcl}, leveraging large unlabeled datasets. In symbolic reasoning, methods that fuse neural networks with symbolic logic \citep{pulicharla2025neurosymbolicab, zhang2023improvedlr} demonstrate the potential for structured, rule-based tasks. However, applying contrastive techniques to purely symbolic data remains relatively unexplored, posing unique challenges in designing augmentations that preserve semantic validity.

\section{Method}
We generate positive examples by permuting or masking symbolic attributes in ways that maintain structural consistency. Negative samples are constructed by contradictions or alterations that break logical rules. A bidirectional LSTM encoder maps sequences to embeddings. We train with the temperature-scaled InfoNCE objective \citep{chen2020asf}, pushing similar pairs together and dissimilar pairs apart in the embedding space. After pre-training, we introduce a classification head and fine-tune on labeled data. We emphasize potential pitfalls: if certain tokens are highly similar, negative sampling can fail to provide sufficient contrast, limiting generalization.

\section{Experiments}
We evaluate on SPR\_BENCH (20k/5k/10k splits). The baseline is a standard BiLSTM trained with supervised cross-entropy. \cref{fig:baseline_figs} (left) shows training and validation loss curves over epochs for various epoch caps, while \cref{fig:baseline_figs} (right) reports the validation HWA. Overall performance is strong, but validation results occasionally fluctuate, suggesting potential instability when shape-/color-specific features dominate.

\begin{figure}[t!]
\centering
\subfigure[Training/Validation Loss]{%
  \includegraphics[width=0.48\textwidth]{baseline_loss_curves.png}
}
\hspace{0.02\textwidth}
\subfigure[Validation HWA]{%
  \includegraphics[width=0.48\textwidth]{baseline_HWA_curves.png}
}
\caption{\textbf{BiLSTM baseline on SPR\_BENCH.} (Left) The loss curves generally decrease as epochs increase, but validation loss exhibits some fluctuations. (Right) HWA remains high (above 0.97) across different epoch settings, though small performance dips occur.}
\label{fig:baseline_figs}
\end{figure}

We then incorporate a contrastive objective alongside supervised fine-tuning. \cref{fig:joint_figs} (left) depicts the training/validation loss, revealing stable convergence, while \cref{fig:joint_figs} (right) shows SWA, CWA, and a combined metric trending near unity. Although the synthetic domain is relatively straightforward, a small pilot with slightly modified token vocabularies resulted in significant performance drops, highlighting brittleness to domain shift.

\begin{figure}[t!]
\centering
\subfigure[Joint-Training Loss]{%
  \includegraphics[width=0.48\textwidth]{research_joint_loss_curves.png}
}
\hspace{0.02\textwidth}
\subfigure[Validation Metrics]{%
  \includegraphics[width=0.48\textwidth]{research_metric_curves.png}
}
\caption{\textbf{Context-aware contrastive approach.} (Left) Training and validation loss curves converge steadily. (Right) SWA, CWA, and the combined metric approach near-perfect levels, though minor validation loss increases occur at later epochs.}
\label{fig:joint_figs}
\end{figure}

\section{Conclusion}
We presented a self-supervised framework leveraging context-aware augmentations for symbolic sequence recognition, leading to excellent performance on the SPR\_BENCH benchmark. Despite these gains, our results underscore notable pitfalls, including sensitivity to domain shifts and token similarity. Future work includes designing robust augmentations for ambiguous tokens and evaluating how domain mismatch can be mitigated via more diverse data or adaptive sampling strategies.

\bibliography{references}
\bibliographystyle{iclr2025}

\appendix

\section*{\LARGE Supplementary Material}
\label{sec:appendix}

\section{Implementation Details and Hyperparameters}
We used a bidirectional LSTM encoder (128-dimensional hidden size), followed by a 64-dimensional projection for contrastive embedding. The contrastive temperature was set to 0.07. All models were trained with Adam at a 0.001 learning rate, halved every 10 epochs. For classification, a single-hidden-layer MLP (64-dim) fed into a softmax.

\section{Ablation Studies}
We tested two variants: \emph{no-contrastive} (purely supervised) and a \emph{dual-encoder} approach separating color and shape embeddings. We combined all ablation results into \cref{fig:ablation_summary} to illustrate loss curves and confusion matrices.

\begin{figure}[h!]
\centering
\subfigure[No-contrastive loss curves]{%
  \includegraphics[width=0.45\textwidth]{abl_no_contrastive_loss_curves.png}
}
\subfigure[No-contrastive confusion matrix]{%
  \includegraphics[width=0.45\textwidth]{abl_no_contrastive_confusion_matrix.png}
}
\\
\subfigure[Dual-encoder metrics]{%
  \includegraphics[width=0.45\textwidth]{abl_dual_encoder_metrics.png}
}
\subfigure[Dual-encoder confusion matrix]{%
  \includegraphics[width=0.45\textwidth]{abl_dual_encoder_confusion_matrix.png}
}
\caption{\textbf{Ablation results.} (a,b) Removing the contrastive component increases misclassifications in shape-heavy sequences, as seen in higher validation loss and off-diagonal confusion. (c,d) The dual-encoder approach provides similar final accuracy but slightly faster training convergence.}
\label{fig:ablation_summary}
\end{figure}

In \textbf{no-contrastive} mode, shape-heavy sequences create confusion due to the lack of an explicit embedding constraint. The \textbf{dual-encoder} approach accelerates training by splitting symbolic attributes, though final metrics remain comparable to the single-encoder variant.

\end{document}