\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chen2020asf}
\citation{chen2020asf,choi2025similarityguideddf,kim2024semisupervisedcl}
\citation{pulicharla2025neurosymbolicab,zhang2023improvedlr}
\citation{chen2020asf}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{1}{section.3}\protected@file@percent }
\bibdata{references}
\bibcite{chen2020asf}{{1}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{choi2025similarityguideddf}{{2}{2025}{{Choi et~al.}}{{Choi, Noh, and Park}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {BiLSTM baseline on SPR\_BENCH.} (Left) The loss curves generally decrease as epochs increase, but validation loss exhibits some fluctuations. (Right) HWA remains high (above 0.97) across different epoch settings, though small performance dips occur.}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training/Validation Loss}}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Validation HWA}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:baseline_figs}{{1}{2}{\textbf {BiLSTM baseline on SPR\_BENCH.} (Left) The loss curves generally decrease as epochs increase, but validation loss exhibits some fluctuations. (Right) HWA remains high (above 0.97) across different epoch settings, though small performance dips occur}{figure.1}{}}
\newlabel{fig:baseline_figs@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{2}{section.5}\protected@file@percent }
\bibcite{kim2024semisupervisedcl}{{3}{2024}{{Kim et~al.}}{{Kim, Cho, Chae, Park, and Huh}}}
\bibcite{pulicharla2025neurosymbolicab}{{4}{2025}{{Pulicharla}}{{}}}
\bibcite{zhang2023improvedlr}{{5}{2023}{{Zhang et~al.}}{{Zhang, Huang, Li, Naik, and Xing}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Context-aware contrastive approach.} (Left) Training and validation loss curves converge steadily. (Right) SWA, CWA, and the combined metric approach near-perfect levels, though minor validation loss increases occur at later epochs.}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Joint-Training Loss}}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Validation Metrics}}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:joint_figs}{{2}{3}{\textbf {Context-aware contrastive approach.} (Left) Training and validation loss curves converge steadily. (Right) SWA, CWA, and the combined metric approach near-perfect levels, though minor validation loss increases occur at later epochs}{figure.2}{}}
\newlabel{fig:joint_figs@cref}{{[figure][2][]2}{[1][2][]3}}
\newlabel{sec:appendix}{{5}{3}{\LARGE Supplementary Material}{section*.2}{}}
\newlabel{sec:appendix@cref}{{[section][5][]5}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementation Details and Hyperparameters}{3}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Ablation Studies}{3}{appendix.B}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Ablation results.} (a,b) Removing the contrastive component increases misclassifications in shape-heavy sequences, as seen in higher validation loss and off-diagonal confusion. (c,d) The dual-encoder approach provides similar final accuracy but slightly faster training convergence.}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {No-contrastive loss curves}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {No-contrastive confusion matrix}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Dual-encoder metrics}}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Dual-encoder confusion matrix}}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:ablation_summary}{{3}{4}{\textbf {Ablation results.} (a,b) Removing the contrastive component increases misclassifications in shape-heavy sequences, as seen in higher validation loss and off-diagonal confusion. (c,d) The dual-encoder approach provides similar final accuracy but slightly faster training convergence}{figure.3}{}}
\newlabel{fig:ablation_summary@cref}{{[figure][3][2147483647]3}{[1][3][]4}}
\gdef \@abspage@last{4}
