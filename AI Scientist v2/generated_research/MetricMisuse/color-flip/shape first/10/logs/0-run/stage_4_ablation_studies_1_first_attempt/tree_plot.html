<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 6], [0, 5], [0, 3], [0, 8], [0, 7], [0, 2], [0, 4], [0, 1]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.14285714285714285, 1.0], [0.2857142857142857, 1.0], [0.42857142857142855, 1.0], [0.5714285714285714, 1.0], [0.7142857142857143, 1.0], [0.8571428571428571, 1.0], [1.0, 1.0]], "plan": ["The failure came from passing raw Python dictionaries to\n`datasets.load_dataset`, which expects file paths or URLs.   The fix is to build\nthe synthetic splits with `datasets.Dataset.from_dict`, wrap them in a\n`DatasetDict`, and directly return that.   No other logic needs to change; we\nsimply replace the erroneous loading code.   Below is a self-contained script\nwith the corrected `load_spr` routine plus the required device handling, metric\ntracking and data saving.", "Ablation name: Remove Contrastive Pre-training (Train Encoder From Scratch). The\nablation simply deletes the contrastive pre-training stage: the encoder is\ninitialized randomly and trained only with the supervised loss. All other\ncomponents (data preparation, augmentations, ACS calculation, logging and\nsaving) remain identical to the baseline so that differences in final metrics\ndirectly reflect the contribution of the removed stage. Results are stored under\nthe key \"remove_contrastive_pretrain\" for easy side-by-side comparison with\nprevious runs.", "Ablation name: Remove Data Augmentation in Contrastive Stage. We reuse the\noriginal pipeline but create a new contrastive dataset that outputs two\nidentical (non-augmented) views, letting the rest of the code remain unchanged.\nThis isolates the effect of augmentation during pre-training. All metrics,\nlosses and predictions are stored under the ablation key \"no_aug_contrast\" for\nthe SPR dataset and saved to working/experiment_data.npy.", "Ablation name: Multi-Synthetic-Dataset Pre-training. We create three independent\nsynthetic pre-training corpora, each built with a different \u201chidden\u201d labeling\nrule (mod-4 count of \u201cA\u201d, parity of \u201cC0\u201d, majority color).  The labels\nthemselves are never used; we only need the sequences, but drawing them from\nthree distinct generative regimes injects structural variety.  The three sets\nare concatenated and fed to the usual contrastive encoder training, after which\nthe encoder is fine-tuned and evaluated on the original SPR task exactly as\nbefore.  Results, losses and augmentation-consistency statistics are stored\nunder the ablation key multi_syn_pretrain and saved to\nworking/experiment_data.npy.", "Ablation name: No Encoder Normalization (Unnormalized Contrastive\nRepresentations). The ablation simply removes the L2-normalisation step inside\nthe encoder so that the raw GRU hidden state is used by the contrastive NT-Xent\nloss and the downstream classifier. Everything else (data preparation, loss\ncomputation, training and evaluation loops, logging, saving) is left unchanged\nso we can directly compare with the normalised baseline. All collected curves,\nmetrics and predictions are stored under the ablation key \u201cno_encoder_norm\u201d and\nwritten to working/experiment_data.npy at the end of the run.", "Ablation name: Freeze Pre-trained Encoder (Linear-Probe Fine-Tuning). We keep\nthe full contrastive-pretraining phase unchanged, then build a new SPRModel\nwhose encoder weights are loaded from the pretrained encoder but frozen\n(requires_grad = False). We train only the final linear layer and log the same\nlosses, accuracy and augmentation-consistency into an ablation-aware\nexperiment_data dictionary. All plottable information is saved to\nworking/experiment_data.npy.", "Ablation name: Remove Recurrent Encoder (Mean-Pooling Encoder). Below is an\nabridged outline followed by the full single-file script.   We keep every\ncomponent of the baseline unchanged except the sequence encoder: the GRU is\nremoved and the representation is obtained by mean-pooling over (non-pad) token\nembeddings and then L2-normalising.  This encoder is used during both\ncontrastive pre-training and supervised fine-tuning.  All bookkeeping is stored\nin the required `experiment_data` dictionary and saved to\n`working/experiment_data.npy`.", "Ablation name: One-Sided Augmentation in Contrastive Pre-training. The solution\nadds a second contrastive-pre-training run where only the second view is\naugmented (raw \u2192 aug).  A flag one_sided controls whether the first view is kept\nclean.  We wrap the whole pipeline (data, pre-train, fine-tune, evaluation,\nlogging) into a function that is called twice: first for the original aug \u2192 aug\nbaseline, then for the one-sided ablation.  Results of both runs are collected\nin a unified experiment_data dictionary and saved to\nworking/experiment_data.npy.", "Ablation name: Non-Sharing Twin Encoders in Contrastive Pre-training. The\nsolution instantiates two separate encoders (encoder_A and encoder_B) for\ncontrastive pre-training. During NT-Xent optimisation only their respective\nparameters are updated, with no weight sharing enforced. After pre-training,\nencoder_A is plugged into the downstream SPR classifier and fine-tuned, while\nencoder_B is discarded\u2014this isolates the contribution of tied parameters. All\nlosses, accuracies and augmentation-consistency scores are logged under the\nablation tag \u201cNonSharingTwin\u201d and saved to working/experiment_data.npy."], "code": ["import os, random, pathlib, itertools, time, math\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------\n# mandatory working dir & device ------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------\n# experiment tracking container -------------------------------\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train_ACS\": [], \"val_ACS\": []},\n        \"losses\": {\"contrastive\": [], \"train_sup\": [], \"val_sup\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ------------------------------------------------------------\n#  Utility: load real or synthetic SPR ------------------------\nSHAPES = list(\"ABCDEFGH\")\nCOLORS = list(\"01234567\")\n\n\ndef generate_seq(min_len: int = 5, max_len: int = 15) -> str:\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str) -> int:\n    # Hidden (toy) rule: modulo-4 count of 'A' shapes\n    cnt = sum(tok[0] == \"A\" for tok in seq.split())\n    return cnt % 4  # 0..3\n\n\ndef synthetic_split(n: int) -> Dict[str, List]:\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    \"\"\"\n    Try to load the official SPR_BENCH; fall back to on-the-fly synthetic data.\n    \"\"\"\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n\n    # ---- Fallback synthetic ----\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr_dict, dv_dict, te_dict = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr_dict),\n            \"dev\": HFDataset.from_dict(dv_dict),\n            \"test\": HFDataset.from_dict(te_dict),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------------------------------\n#  Vocabulary & tokenization ---------------------------------\ndef build_vocab(dataset_split):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\ndef decode(ids: List[int]) -> List[str]:\n    inv = {i: t for t, i in vocab.items()}\n    return [inv[i] for i in ids if i != PAD]\n\n\n# ------------------------------------------------------------\n#   Augmentations --------------------------------------------\ndef token_mask(ids: List[int], prob: float = 0.15) -> List[int]:\n    return [MASK if (i != PAD and random.random() < prob) else i for i in ids]\n\n\ndef local_shuffle(ids: List[int], window: int = 3) -> List[int]:\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids: List[int]) -> List[int]:\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ------------------------------------------------------------\n#  PyTorch datasets ------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences):\n        self.samples = [encode(s) for s in sequences]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ------------------------------------------------------------\n#  Model definitions -----------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        h = torch.nn.functional.normalize(h.squeeze(0), dim=-1)\n        return h  # [B, hid]\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)\n        return self.classifier(h)\n\n\ndef nt_xent(z1, z2, temp: float = 0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2B x d\n    sim = torch.matmul(z, z.T) / temp  # cosine sim since vectors normalized\n    mask = torch.eye(2 * B, device=z.device).bool()\n    sim.masked_fill_(mask, -9e15)\n    pos_idx = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    loss = nn.CrossEntropyLoss()(sim, pos_idx)\n    return loss\n\n\n# ------------------------------------------------------------\n#  DataLoaders -----------------------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDataset(spr[\"train\"][\"sequence\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------\n#  Contrastive pre-training ----------------------------------\nencoder = Encoder(vocab_size).to(device)\nenc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n\npre_epochs = 2\nfor epoch in range(1, pre_epochs + 1):\n    encoder.train()\n    total_loss = 0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = encoder(batch[\"v1\"]), encoder(batch[\"v2\"])\n        loss = nt_xent(z1, z2)\n        enc_opt.zero_grad()\n        loss.backward()\n        enc_opt.step()\n        total_loss += loss.item() * batch[\"v1\"].size(0)\n    epoch_loss = total_loss / len(contrast_loader.dataset)\n    experiment_data[\"SPR\"][\"losses\"][\"contrastive\"].append(epoch_loss)\n    print(f\"Contrastive epoch {epoch}: loss = {epoch_loss:.4f}\")\n\n# ------------------------------------------------------------\n#  Supervised fine-tuning ------------------------------------\nmodel = SPRModel(vocab_size, num_classes).to(device)\nmodel.encoder.load_state_dict(encoder.state_dict())\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct = 0, 0\n    preds_all, gts_all = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(dim=-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs = batch[\"seq\"]\n            labels = batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base_pred = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base_pred == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = torch.nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    aug_pred = model(aug_ids.to(device)).argmax().item()\n                    if aug_pred != base_pred:\n                        ok = False\n                consistent += ok\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    # training\n    model.train()\n    running_loss = 0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        running_loss += loss.item() * batch[\"seq\"].size(0)\n\n    train_loss = running_loss / len(sup_train_loader.dataset)\n\n    # validation\n    val_loss, val_preds, val_gts, val_acc = evaluate(sup_val_loader)\n    ACS = augmentation_consistency(sup_val_loader)\n\n    # track\n    experiment_data[\"SPR\"][\"losses\"][\"train_sup\"].append(train_loss)\n    experiment_data[\"SPR\"][\"losses\"][\"val_sup\"].append(val_loss)\n    experiment_data[\"SPR\"][\"metrics\"][\"val_ACS\"].append(ACS)\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR\"][\"predictions\"] = val_preds\n    experiment_data[\"SPR\"][\"ground_truth\"] = val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f} \"\n        f\"val_acc={val_acc:.4f}  ACS={ACS:.4f}\"\n    )\n\n# ------------------------------------------------------------\n#  Save experiment data --------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Training complete, data saved to 'working/experiment_data.npy'\")\n", "# ------------------------------------------------------------\n#  Remove Contrastive Pre-training (Train Encoder From Scratch)\n# ------------------------------------------------------------\nimport os, random, pathlib, math, itertools, time\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict\n\n# ------------------------------------ misc/paths/device -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------ experiment container ---\nexperiment_data = {\n    \"remove_contrastive_pretrain\": {\n        \"SPR\": {\n            \"metrics\": {\"val_acc\": [], \"val_ACS\": []},\n            \"losses\": {\"train_sup\": [], \"val_sup\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ------------------------------------ SPR loading ------------\nSHAPES, COLORS = list(\"ABCDEFGH\"), list(\"01234567\")\n\n\ndef generate_seq(min_len=5, max_len=15):\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str) -> int:\n    return sum(tok[0] == \"A\" for tok in seq.split()) % 4\n\n\ndef synthetic_split(n: int):\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n    if root.exists() and load_spr_bench:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n    print(\"SPR_BENCH not found, creating synthetic data\")\n    tr, dv, te = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr),\n            \"dev\": HFDataset.from_dict(dv),\n            \"test\": HFDataset.from_dict(te),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------ vocabulary -------------\ndef build_vocab(ds_split):\n    vocab, idx = {\"<pad>\": 0, \"<mask>\": 1}, 2\n    for seq in ds_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\ndef augment(ids: List[int]) -> List[int]:\n    # 50% token mask, else local shuffle inside windows of 3\n    if random.random() < 0.5:\n        return [MASK if (i != PAD and random.random() < 0.15) else i for i in ids]\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + 3)\n        random.shuffle(ids[i:j])\n        i += 3\n    return ids\n\n\n# ------------------------------------ datasets ---------------\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels, training=False):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n        self.training = training\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        if self.training:  # optional augmentation during training\n            ids = augment(ids) if random.random() < 0.5 else ids\n        return torch.LongTensor(ids), torch.tensor(self.labels[idx]).long()\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    L = max(len(s) for s in seq_list)\n    return torch.stack(\n        [nn.functional.pad(s, (0, L - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_sup(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch])\n    return {\"seq\": seqs, \"label\": labels}\n\n\nbatch_size = 128\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"], training=True),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_sup,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"], training=False),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_sup,\n)\n\n\n# ------------------------------------ model ------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        e = self.emb(x)\n        _, h = self.gru(e)\n        return h.squeeze(0)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)\n        return self.classifier(h)\n\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\nmodel = SPRModel(vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------------------------ evaluation utils -------\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct, preds_all, gts_all = 0.0, 0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            seq, lbl = batch[\"seq\"].to(device), batch[\"label\"].to(device)\n            out = model(seq)\n            loss = criterion(out, lbl)\n            tot_loss += loss.item() * seq.size(0)\n            preds = out.argmax(-1)\n            correct += (preds == lbl).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(lbl.cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base == l.item()\n                for _ in range(variants):\n                    aug = torch.LongTensor(augment(ids))\n                    aug = nn.functional.pad(\n                        aug, (0, s.size(0) - len(aug)), value=PAD\n                    ).unsqueeze(0)\n                    if model(aug.to(device)).argmax().item() != base:\n                        ok = False\n                total += 1\n                consistent += ok\n    return consistent / total if total else 0.0\n\n\n# ------------------------------------ supervised training ----\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    model.train()\n    running = 0.0\n    for batch in sup_train_loader:\n        seq, lbl = batch[\"seq\"].to(device), batch[\"label\"].to(device)\n        out = model(seq)\n        loss = criterion(out, lbl)\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        running += loss.item() * seq.size(0)\n\n    train_loss = running / len(sup_train_loader.dataset)\n    val_loss, preds, gts, val_acc = evaluate(sup_val_loader)\n    val_ACS = augmentation_consistency(sup_val_loader)\n\n    # log\n    exp = experiment_data[\"remove_contrastive_pretrain\"][\"SPR\"]\n    exp[\"losses\"][\"train_sup\"].append(train_loss)\n    exp[\"losses\"][\"val_sup\"].append(val_loss)\n    exp[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp[\"metrics\"][\"val_ACS\"].append(val_ACS)\n    exp[\"predictions\"] = preds\n    exp[\"ground_truth\"] = gts\n    exp[\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}  ACS={val_ACS:.4f}\"\n    )\n\n# ------------------------------------ save -------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Finished. Data saved to working/experiment_data.npy\")\n", "import os, random, pathlib, itertools, time, math\nfrom typing import List, Dict\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------- working dir & device ----------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- experiment container ----------------------\nexperiment_data = {\n    \"no_aug_contrast\": {\n        \"SPR\": {\n            \"metrics\": {\"train_ACS\": [], \"val_ACS\": []},\n            \"losses\": {\"contrastive\": [], \"train_sup\": [], \"val_sup\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nED = experiment_data[\"no_aug_contrast\"][\"SPR\"]  # shortcut\n\n# ---------------- synthetic / real SPR ----------------------\nSHAPES, COLORS = list(\"ABCDEFGH\"), list(\"01234567\")\n\n\ndef generate_seq(min_len=5, max_len=15):\n    return \" \".join(\n        random.choice(SHAPES) + random.choice(COLORS)\n        for _ in range(random.randint(min_len, max_len))\n    )\n\n\ndef rule_label(seq):\n    return sum(tok[0] == \"A\" for tok in seq.split()) % 4\n\n\ndef synthetic_split(n):\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)):\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench\n    except ImportError:\n        load_spr_bench = None\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr, dv, te = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr),\n            \"dev\": HFDataset.from_dict(dv),\n            \"test\": HFDataset.from_dict(te),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ---------------- vocabulary & tokenization -----------------\ndef build_vocab(ds):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in ds[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\n\n\ndef encode(seq):\n    return [vocab[t] for t in seq.split()]\n\n\ndef decode(ids):\n    inv = {i: t for t, i in vocab.items()}\n    return [inv[i] for i in ids if i != PAD]\n\n\n# ----------------- augmentations (kept for ACS only) --------\ndef token_mask(ids, prob=0.15):\n    return [MASK if (i != PAD and random.random() < prob) else i for i in ids]\n\n\ndef local_shuffle(ids, window=3):\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids):\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ---------------- PyTorch datasets --------------------------\nclass ContrastiveDatasetNoAug(Dataset):  # identical views\n    def __init__(self, seqs):\n        self.samples = [encode(s) for s in seqs]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = torch.LongTensor(self.samples[idx])\n        return ids, ids  # no augmentation, identical\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, seqs, labels):\n        self.samples = [encode(s) for s in seqs]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list):\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ---------------- model definitions -------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        h = h.squeeze(0)\n        return nn.functional.normalize(h, dim=-1)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        return self.classifier(self.encoder(x))\n\n\ndef nt_xent(z1, z2, temp=0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], 0)\n    sim = torch.matmul(z, z.T) / temp\n    sim.fill_diagonal_(-9e15)\n    pos_idx = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    return nn.CrossEntropyLoss()(sim, pos_idx)\n\n\n# ---------------- DataLoaders --------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDatasetNoAug(spr[\"train\"][\"sequence\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\n\n# ---------------- contrastive pre-training -------------------\nencoder = Encoder(vocab_size).to(device)\nenc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npre_epochs = 2\nfor epoch in range(1, pre_epochs + 1):\n    encoder.train()\n    total_loss = 0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        loss = nt_xent(encoder(batch[\"v1\"]), encoder(batch[\"v2\"]))\n        enc_opt.zero_grad()\n        loss.backward()\n        enc_opt.step()\n        total_loss += loss.item() * batch[\"v1\"].size(0)\n    epoch_loss = total_loss / len(contrast_loader.dataset)\n    ED[\"losses\"][\"contrastive\"].append(epoch_loss)\n    print(f\"Contrastive epoch {epoch}: loss={epoch_loss:.4f}\")\n\n# ---------------- supervised fine-tuning ---------------------\nnum_classes = len(set(spr[\"train\"][\"label\"]))\nmodel = SPRModel(vocab_size, num_classes).to(device)\nmodel.encoder.load_state_dict(encoder.state_dict())\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss = correct = 0\n    preds_all = []\n    gts_all = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total = consistent = 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base_pred = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base_pred == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    if model(aug_ids.to(device)).argmax().item() != base_pred:\n                        ok = False\n                consistent += ok\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    model.train()\n    run_loss = 0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        loss = criterion(model(batch[\"seq\"]), batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        run_loss += loss.item() * batch[\"seq\"].size(0)\n    train_loss = run_loss / len(sup_train_loader.dataset)\n    val_loss, val_preds, val_gts, val_acc = evaluate(sup_val_loader)\n    ACS = augmentation_consistency(sup_val_loader)\n    ED[\"losses\"][\"train_sup\"].append(train_loss)\n    ED[\"losses\"][\"val_sup\"].append(val_loss)\n    ED[\"metrics\"][\"val_ACS\"].append(ACS)\n    ED[\"epochs\"].append(epoch)\n    ED[\"predictions\"] = val_preds\n    ED[\"ground_truth\"] = val_gts\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} ACS={ACS:.4f}\"\n    )\n\n# ---------------- save experiment data -----------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Training complete, data saved to 'working/experiment_data.npy'\")\n", "import os, random, pathlib, itertools, time, math\nfrom typing import List, Dict\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------\n# mandatory working dir & device ------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------\n# experiment tracking container -------------------------------\nexperiment_data = {\n    \"multi_syn_pretrain\": {\n        \"SPR\": {\n            \"metrics\": {\"val_ACS\": []},\n            \"losses\": {\"contrastive\": [], \"train_sup\": [], \"val_sup\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ------------------------------------------------------------\n#  Utility: real or synthetic SPR -----------------------------\nSHAPES, COLORS = list(\"ABCDEFGH\"), list(\"01234567\")\n\n\ndef generate_seq(min_len: int = 5, max_len: int = 15) -> str:\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\n#   three different hidden rules (labels unused in contrastive pre-training)\ndef rule1(seq: str) -> int:  # baseline : mod-4 count of 'A'\n    return sum(tok[0] == \"A\" for tok in seq.split()) % 4\n\n\ndef rule2(seq: str) -> int:  # parity of token 'C0'\n    return sum(tok == \"C0\" for tok in seq.split()) % 2\n\n\ndef rule3(seq: str) -> int:  # majority colour 0-7\n    col_cnt = [0] * 8\n    for tok in seq.split():\n        col_cnt[int(tok[1])] += 1\n    return int(np.argmax(col_cnt))\n\n\ndef synthetic_split(n: int, rule_fn) -> Dict[str, List]:\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_fn(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr, dv, te = map(lambda c: synthetic_split(c, rule1), split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr),\n            \"dev\": HFDataset.from_dict(dv),\n            \"test\": HFDataset.from_dict(te),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------------------------------\n# build vocab ------------------------------------------------\ndef build_vocab(dataset_split):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\ndef decode(ids: List[int]) -> List[str]:\n    inv = {i: t for t, i in vocab.items()}\n    return [inv[i] for i in ids if i != PAD]\n\n\n# ------------------------------------------------------------\n# augmentations ----------------------------------------------\ndef token_mask(ids: List[int], p: float = 0.15) -> List[int]:\n    return [MASK if (i != PAD and random.random() < p) else i for i in ids]\n\n\ndef local_shuffle(ids: List[int], window: int = 3) -> List[int]:\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids: List[int]) -> List[int]:\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ------------------------------------------------------------\n#  PyTorch datasets ------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences):\n        self.samples = [encode(s) for s in sequences]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, seqs, labels):\n        self.samples = [encode(s) for s in seqs]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ------------------------------------------------------------\n#  Model defs -------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return torch.nn.functional.normalize(h.squeeze(0), dim=-1)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        return self.classifier(self.encoder(x))\n\n\ndef nt_xent(z1, z2, temp=0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)\n    sim = torch.matmul(z, z.T) / temp\n    mask = torch.eye(2 * B, device=z.device).bool()\n    sim.masked_fill_(mask, -9e15)\n    pos_idx = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    return nn.CrossEntropyLoss()(sim, pos_idx)\n\n\n# ------------------------------------------------------------\n#  Build multi-synthetic corpora for pre-training ------------\nN_PER = 4000\nsyn1 = synthetic_split(N_PER, rule1)[\"sequence\"]\nsyn2 = synthetic_split(N_PER, rule2)[\"sequence\"]\nsyn3 = synthetic_split(N_PER, rule3)[\"sequence\"]\nmulti_sequences = syn1 + syn2 + syn3\nprint(f\"Combined synthetic corpus size: {len(multi_sequences)}\")\n\n# ------------------------------------------------------------\n#  DataLoaders -----------------------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDataset(multi_sequences),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------\n#  Contrastive pre-training ----------------------------------\nencoder = Encoder(vocab_size).to(device)\nenc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npre_epochs = 2\nfor epoch in range(1, pre_epochs + 1):\n    encoder.train()\n    tot = 0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = encoder(batch[\"v1\"]), encoder(batch[\"v2\"])\n        loss = nt_xent(z1, z2)\n        enc_opt.zero_grad()\n        loss.backward()\n        enc_opt.step()\n        tot += loss.item() * batch[\"v1\"].size(0)\n    ep_loss = tot / len(contrast_loader.dataset)\n    experiment_data[\"multi_syn_pretrain\"][\"SPR\"][\"losses\"][\"contrastive\"].append(\n        ep_loss\n    )\n    print(f\"Contrastive epoch {epoch}: loss={ep_loss:.4f}\")\n\n# ------------------------------------------------------------\n#  Supervised fine-tuning ------------------------------------\nmodel = SPRModel(vocab_size, num_classes).to(device)\nmodel.encoder.load_state_dict(encoder.state_dict())\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct = 0, 0\n    preds_all, gts_all = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(dim=-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base_pred = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base_pred == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = torch.nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    if model(aug_ids.to(device)).argmax().item() != base_pred:\n                        ok = False\n                consistent += ok\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    model.train()\n    run_loss = 0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        run_loss += loss.item() * batch[\"seq\"].size(0)\n    train_loss = run_loss / len(sup_train_loader.dataset)\n\n    val_loss, val_preds, val_gts, val_acc = evaluate(sup_val_loader)\n    ACS = augmentation_consistency(sup_val_loader)\n\n    exp = experiment_data[\"multi_syn_pretrain\"][\"SPR\"]\n    exp[\"losses\"][\"train_sup\"].append(train_loss)\n    exp[\"losses\"][\"val_sup\"].append(val_loss)\n    exp[\"metrics\"][\"val_ACS\"].append(ACS)\n    exp[\"epochs\"].append(epoch)\n    exp[\"predictions\"], exp[\"ground_truth\"] = val_preds, val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} ACS={ACS:.4f}\"\n    )\n\n# ------------------------------------------------------------\n#  Save experiment data --------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Training complete, data saved to 'working/experiment_data.npy'\")\n", "# ------------------------------------------------------------\n#  No-Encoder-Normalisation Ablation for SPR\n# ------------------------------------------------------------\nimport os, random, pathlib, itertools, time, math\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------\n# mandatory working dir & device ------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------\n# experiment tracking container -------------------------------\nexperiment_data = {\n    \"no_encoder_norm\": {\n        \"SPR\": {\n            \"metrics\": {\"train_ACS\": [], \"val_ACS\": []},\n            \"losses\": {\"contrastive\": [], \"train_sup\": [], \"val_sup\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_store = experiment_data[\"no_encoder_norm\"][\"SPR\"]\n\n# ------------------------------------------------------------\n#  Utility: load real or synthetic SPR ------------------------\nSHAPES = list(\"ABCDEFGH\")\nCOLORS = list(\"01234567\")\n\n\ndef generate_seq(min_len: int = 5, max_len: int = 15) -> str:\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str) -> int:\n    # Hidden (toy) rule: modulo-4 count of 'A' shapes\n    cnt = sum(tok[0] == \"A\" for tok in seq.split())\n    return cnt % 4  # 0..3\n\n\ndef synthetic_split(n: int) -> Dict[str, List]:\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    \"\"\"\n    Try to load the official SPR_BENCH; fall back to on-the-fly synthetic data.\n    \"\"\"\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n\n    # ---- Fallback synthetic ----\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr_dict, dv_dict, te_dict = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr_dict),\n            \"dev\": HFDataset.from_dict(dv_dict),\n            \"test\": HFDataset.from_dict(te_dict),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------------------------------\n#  Vocabulary & tokenisation ---------------------------------\ndef build_vocab(dataset_split):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\ndef decode(ids: List[int]) -> List[str]:\n    inv = {i: t for t, i in vocab.items()}\n    return [inv[i] for i in ids if i != PAD]\n\n\n# ------------------------------------------------------------\n#   Augmentations --------------------------------------------\ndef token_mask(ids: List[int], prob: float = 0.15) -> List[int]:\n    return [MASK if (i != PAD and random.random() < prob) else i for i in ids]\n\n\ndef local_shuffle(ids: List[int], window: int = 3) -> List[int]:\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids: List[int]) -> List[int]:\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ------------------------------------------------------------\n#  PyTorch datasets ------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences):\n        self.samples = [encode(s) for s in sequences]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ------------------------------------------------------------\n#  Model definitions -----------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)  # h: [1,B,hid]\n        h = h.squeeze(0)  # NO normalisation here (ablation)\n        return h  # [B, hid]\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)\n        return self.classifier(h)\n\n\ndef nt_xent(z1, z2, temp: float = 0.5):\n    \"\"\"\n    NT-Xent with raw (unnormalised) representations.\n    Uses scaled dot-product without cosine normalisation.\n    \"\"\"\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2B x d\n    sim = torch.matmul(z, z.T) / temp  # raw dot product similarity\n    mask = torch.eye(2 * B, device=z.device).bool()\n    sim.masked_fill_(mask, -9e15)\n    pos_idx = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    loss = nn.CrossEntropyLoss()(sim, pos_idx)\n    return loss\n\n\n# ------------------------------------------------------------\n#  DataLoaders -----------------------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDataset(spr[\"train\"][\"sequence\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------\n#  Contrastive pre-training ----------------------------------\nencoder = Encoder(vocab_size).to(device)\nenc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n\npre_epochs = 2\nfor epoch in range(1, pre_epochs + 1):\n    encoder.train()\n    total_loss = 0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = encoder(batch[\"v1\"]), encoder(batch[\"v2\"])\n        loss = nt_xent(z1, z2)\n        enc_opt.zero_grad()\n        loss.backward()\n        enc_opt.step()\n        total_loss += loss.item() * batch[\"v1\"].size(0)\n    epoch_loss = total_loss / len(contrast_loader.dataset)\n    exp_store[\"losses\"][\"contrastive\"].append(epoch_loss)\n    print(f\"[Contrastive] epoch {epoch}: loss = {epoch_loss:.4f}\")\n\n# ------------------------------------------------------------\n#  Supervised fine-tuning ------------------------------------\nmodel = SPRModel(vocab_size, num_classes).to(device)\nmodel.encoder.load_state_dict(encoder.state_dict())\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct = 0, 0\n    preds_all, gts_all = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(dim=-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs = batch[\"seq\"]\n            labels = batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base_pred = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base_pred == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = torch.nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    aug_pred = model(aug_ids.to(device)).argmax().item()\n                    if aug_pred != base_pred:\n                        ok = False\n                consistent += ok\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    # training\n    model.train()\n    running_loss = 0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        running_loss += loss.item() * batch[\"seq\"].size(0)\n\n    train_loss = running_loss / len(sup_train_loader.dataset)\n\n    # validation\n    val_loss, val_preds, val_gts, val_acc = evaluate(sup_val_loader)\n    ACS = augmentation_consistency(sup_val_loader)\n\n    # log\n    exp_store[\"losses\"][\"train_sup\"].append(train_loss)\n    exp_store[\"losses\"][\"val_sup\"].append(val_loss)\n    exp_store[\"metrics\"][\"train_ACS\"].append(None)  # placeholder (not computed)\n    exp_store[\"metrics\"][\"val_ACS\"].append(ACS)\n    exp_store[\"epochs\"].append(epoch)\n    exp_store[\"predictions\"] = val_preds\n    exp_store[\"ground_truth\"] = val_gts\n\n    print(\n        f\"[Fine-tune] Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}  ACS={ACS:.4f}\"\n    )\n\n# ------------------------------------------------------------\n#  Save experiment data --------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Training complete; results saved to 'working/experiment_data.npy'\")\n", "import os, random, pathlib, itertools, time, math\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------\n# mandatory working dir & device ------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------\n# experiment tracking container -------------------------------\nexperiment_data = {\n    \"freeze_encoder\": {\n        \"SPR\": {\n            \"metrics\": {\"train_ACS\": [], \"val_ACS\": [], \"val_acc\": []},\n            \"losses\": {\"contrastive\": [], \"train_sup\": [], \"val_sup\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nEXP_KEY = \"freeze_encoder\"  # shorthand\n\n# ------------------------------------------------------------\n#  Utility: load real or synthetic SPR ------------------------\nSHAPES = list(\"ABCDEFGH\")\nCOLORS = list(\"01234567\")\n\n\ndef generate_seq(min_len: int = 5, max_len: int = 15) -> str:\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str) -> int:\n    cnt = sum(tok[0] == \"A\" for tok in seq.split())\n    return cnt % 4  # 0..3\n\n\ndef synthetic_split(n: int) -> Dict[str, List]:\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr_dict, dv_dict, te_dict = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr_dict),\n            \"dev\": HFDataset.from_dict(dv_dict),\n            \"test\": HFDataset.from_dict(te_dict),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------------------------------\n#  Vocabulary & tokenization ---------------------------------\ndef build_vocab(dataset_split):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------------------------------------------------\n#   Augmentations --------------------------------------------\ndef token_mask(ids: List[int], prob: float = 0.15) -> List[int]:\n    return [MASK if (i != PAD and random.random() < prob) else i for i in ids]\n\n\ndef local_shuffle(ids: List[int], window: int = 3) -> List[int]:\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids: List[int]) -> List[int]:\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ------------------------------------------------------------\n#  PyTorch datasets ------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences):\n        self.samples = [encode(s) for s in sequences]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ------------------------------------------------------------\n#  Model definitions -----------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        h = torch.nn.functional.normalize(h.squeeze(0), dim=-1)\n        return h  # [B, hid]\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)\n        return self.classifier(h)\n\n\ndef nt_xent(z1, z2, temp: float = 0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2B x d\n    sim = torch.matmul(z, z.T) / temp\n    mask = torch.eye(2 * B, device=z.device, dtype=torch.bool)\n    sim.masked_fill_(mask, -9e15)\n    pos_idx = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    loss = nn.CrossEntropyLoss()(sim, pos_idx)\n    return loss\n\n\n# ------------------------------------------------------------\n#  DataLoaders -----------------------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDataset(spr[\"train\"][\"sequence\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------\n#  Contrastive pre-training ----------------------------------\nencoder = Encoder(vocab_size).to(device)\nenc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n\npre_epochs = 2\nfor epoch in range(1, pre_epochs + 1):\n    encoder.train()\n    total_loss = 0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = encoder(batch[\"v1\"]), encoder(batch[\"v2\"])\n        loss = nt_xent(z1, z2)\n        enc_opt.zero_grad()\n        loss.backward()\n        enc_opt.step()\n        total_loss += loss.item() * batch[\"v1\"].size(0)\n    epoch_loss = total_loss / len(contrast_loader.dataset)\n    experiment_data[EXP_KEY][\"SPR\"][\"losses\"][\"contrastive\"].append(epoch_loss)\n    print(f\"[Contrastive] epoch {epoch}: loss = {epoch_loss:.4f}\")\n\n# ------------------------------------------------------------\n#  Linear-probe supervised stage (encoder frozen) ------------\nmodel = SPRModel(vocab_size, num_classes).to(device)\nmodel.encoder.load_state_dict(encoder.state_dict())\n\n# Freeze encoder\nfor p in model.encoder.parameters():\n    p.requires_grad = False\n\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3\n)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct = 0, 0\n    preds_all, gts_all = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(dim=-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base_pred = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base_pred == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = torch.nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    aug_pred = model(aug_ids.to(device)).argmax().item()\n                    if aug_pred != base_pred:\n                        ok = False\n                consistent += ok\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    # training\n    model.train()\n    running_loss = 0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        running_loss += loss.item() * batch[\"seq\"].size(0)\n\n    train_loss = running_loss / len(sup_train_loader.dataset)\n    val_loss, val_preds, val_gts, val_acc = evaluate(sup_val_loader)\n    train_ACS = augmentation_consistency(sup_train_loader)\n    val_ACS = augmentation_consistency(sup_val_loader)\n\n    # track\n    exp = experiment_data[EXP_KEY][\"SPR\"]\n    exp[\"losses\"][\"train_sup\"].append(train_loss)\n    exp[\"losses\"][\"val_sup\"].append(val_loss)\n    exp[\"metrics\"][\"train_ACS\"].append(train_ACS)\n    exp[\"metrics\"][\"val_ACS\"].append(val_ACS)\n    exp[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp[\"epochs\"].append(epoch)\n    exp[\"predictions\"] = val_preds\n    exp[\"ground_truth\"] = val_gts\n\n    print(\n        f\"[Linear-Probe] Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} \"\n        f\"train_ACS={train_ACS:.4f} val_ACS={val_ACS:.4f}\"\n    )\n\n# ------------------------------------------------------------\n#  Save experiment data --------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Ablation run complete, data saved to 'working/experiment_data.npy'\")\n", "import os, random, pathlib, math, itertools, time\nfrom typing import List, Dict\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict\n\n# ------------------------------------------------------------\n# mandatory working dir & device ------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------\n# experiment tracking container -------------------------------\nexperiment_data = {\n    \"RemoveRecurrent\": {\n        \"SPR\": {\n            \"metrics\": {\"train_ACS\": [], \"val_ACS\": []},\n            \"losses\": {\n                \"contrastive\": [],\n                \"train_sup\": [],\n                \"val_sup\": [],\n            },\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n# handy alias\nexp = experiment_data[\"RemoveRecurrent\"][\"SPR\"]\n\n# ------------------------------------------------------------\n#  Utility: load real or synthetic SPR ------------------------\nSHAPES = list(\"ABCDEFGH\")\nCOLORS = list(\"01234567\")\n\n\ndef generate_seq(min_len: int = 5, max_len: int = 15) -> str:\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str) -> int:\n    cnt = sum(tok[0] == \"A\" for tok in seq.split())\n    return cnt % 4  # 0..3\n\n\ndef synthetic_split(n: int) -> Dict[str, List]:\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr_dict, dv_dict, te_dict = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr_dict),\n            \"dev\": HFDataset.from_dict(dv_dict),\n            \"test\": HFDataset.from_dict(te_dict),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------------------------------\n#  Vocabulary & tokenization ---------------------------------\ndef build_vocab(dataset_split):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\ndef decode(ids: List[int]) -> List[str]:\n    inv = {i: t for t, i in vocab.items()}\n    return [inv[i] for i in ids if i != PAD]\n\n\n# ------------------------------------------------------------\n#   Augmentations --------------------------------------------\ndef token_mask(ids: List[int], prob: float = 0.15) -> List[int]:\n    return [MASK if (i != PAD and random.random() < prob) else i for i in ids]\n\n\ndef local_shuffle(ids: List[int], window: int = 3) -> List[int]:\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids: List[int]) -> List[int]:\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ------------------------------------------------------------\n#  PyTorch datasets ------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences):\n        self.samples = [encode(s) for s in sequences]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ------------------------------------------------------------\n#  Mean-Pooling Encoder (GRU removed) -------------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n\n    def forward(self, x):\n        # x: [B,L]\n        emb = self.emb(x)  # [B,L,E]\n        mask = (x != PAD).unsqueeze(-1)  # [B,L,1]\n        length = mask.sum(1).clamp(min=1)  # [B,1]\n        summed = (emb * mask).sum(1)  # [B,E]\n        mean = summed / length  # broadcast\n        return torch.nn.functional.normalize(mean, dim=-1)  # [B,E]\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid_dim=128):\n        super().__init__()\n        self.encoder = MeanEncoder(vocab_size, emb_dim=hid_dim)\n        self.classifier = nn.Linear(hid_dim, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)\n        return self.classifier(h)\n\n\ndef nt_xent(z1, z2, temp=0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # [2B,d] already L2-normed\n    sim = torch.mm(z, z.t()) / temp\n    sim.masked_fill_(torch.eye(2 * B, device=z.device).bool(), -9e15)\n    pos_idx = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    return nn.CrossEntropyLoss()(sim, pos_idx)\n\n\n# ------------------------------------------------------------\n#  DataLoaders -----------------------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDataset(spr[\"train\"][\"sequence\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n# ------------------------------------------------------------\n#  Contrastive pre-training ----------------------------------\nencoder = MeanEncoder(vocab_size).to(device)\nenc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npre_epochs = 2\nfor epoch in range(1, pre_epochs + 1):\n    encoder.train()\n    tot = 0.0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = encoder(batch[\"v1\"]), encoder(batch[\"v2\"])\n        loss = nt_xent(z1, z2)\n        enc_opt.zero_grad()\n        loss.backward()\n        enc_opt.step()\n        tot += loss.item() * batch[\"v1\"].size(0)\n    epoch_loss = tot / len(contrast_loader.dataset)\n    exp[\"losses\"][\"contrastive\"].append(epoch_loss)\n    print(f\"Contrastive epoch {epoch}: loss = {epoch_loss:.4f}\")\n\n# ------------------------------------------------------------\n#  Supervised fine-tuning ------------------------------------\nmodel = SPRModel(vocab_size, num_classes).to(device)\nmodel.encoder.load_state_dict(encoder.state_dict())\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct = 0, 0\n    preds_all, gts_all = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_pad = torch.nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    aug_pred = model(aug_pad.to(device)).argmax().item()\n                    if aug_pred != base:\n                        ok = False\n                        break\n                consistent += int(ok)\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor epoch in range(1, sup_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        run_loss += loss.item() * batch[\"seq\"].size(0)\n    train_loss = run_loss / len(sup_train_loader.dataset)\n\n    val_loss, val_preds, val_gts, val_acc = evaluate(sup_val_loader)\n    ACS = augmentation_consistency(sup_val_loader)\n\n    exp[\"losses\"][\"train_sup\"].append(train_loss)\n    exp[\"losses\"][\"val_sup\"].append(val_loss)\n    exp[\"metrics\"][\"val_ACS\"].append(ACS)\n    exp[\"epochs\"].append(epoch)\n    exp[\"predictions\"] = val_preds\n    exp[\"ground_truth\"] = val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}  \"\n        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}  ACS={ACS:.4f}\"\n    )\n\n# ------------------------------------------------------------\n#  Save experiment data --------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Training complete, data saved to 'working/experiment_data.npy'\")\n", "import os, random, pathlib, math, itertools, time\nfrom typing import List, Dict\nimport numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------------------------------------------------\n#  dirs / device ---------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ------------------------------------------------------------\n#  experiment container --------------------------------------\nexperiment_data = {\n    \"baseline_aug_aug\": {},\n    \"one_sided_raw_aug\": {},\n}\n\n# ------------------------------------------------------------\n#  SPR loading (real or synthetic) ---------------------------\nSHAPES, COLORS = list(\"ABCDEFGH\"), list(\"01234567\")\n\n\ndef generate_seq(min_len=5, max_len=15):\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str):\n    cnt = sum(tok[0] == \"A\" for tok in seq.split())\n    return cnt % 4  # 4-class toy rule\n\n\ndef synthetic_split(n):\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)):\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH dataset\")\n        return load_spr_bench(root)\n    print(\"SPR_BENCH not found, building synthetic dataset\")\n    tr, dv, te = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr),\n            \"dev\": HFDataset.from_dict(dv),\n            \"test\": HFDataset.from_dict(te),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ------------------------------------------------------------\n#  Vocab/tokenisation ----------------------------------------\ndef build_vocab(dataset_split):\n    vocab = {\"<pad>\": 0, \"<mask>\": 1}\n    idx = 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\ndef augment(ids: List[int]) -> List[int]:\n    # 50-50 choose mask or local shuffle\n    if random.random() < 0.5:\n        return [MASK if (i != PAD and random.random() < 0.15) else i for i in ids]\n    # local shuffle\n    ids = ids.copy()\n    i, window = 0, 3\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\n# ------------------------------------------------------------\n#  Datasets --------------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences, one_sided: bool = False):\n        self.samples = [encode(s) for s in sequences]\n        self.one_sided = one_sided\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        if self.one_sided:\n            return torch.LongTensor(ids), torch.LongTensor(augment(ids))\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    return torch.stack(\n        [torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list]\n    )\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# ------------------------------------------------------------\n#  Models ----------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return torch.nn.functional.normalize(h.squeeze(0), dim=-1)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_size, num_classes, hid=128):\n        super().__init__()\n        self.encoder = Encoder(vocab_size, hid=hid)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        h = self.encoder(x)\n        return self.classifier(h)\n\n\ndef nt_xent(z1, z2, temp=0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], 0)\n    sim = torch.matmul(z, z.t()) / temp\n    sim.masked_fill_(torch.eye(2 * B, device=z.device).bool(), -9e15)\n    pos = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    return nn.CrossEntropyLoss()(sim, pos)\n\n\n# ------------------------------------------------------------\n#  common helpers --------------------------------------------\nnum_classes = len(set(spr[\"train\"][\"label\"]))\nbatch_size = 128\npre_epochs, sup_epochs = 2, 3\n\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    tot_loss, correct, preds_all, gts_all = 0, 0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(batch[\"seq\"])\n            loss = criterion(out, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = out.argmax(-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        preds_all,\n        gts_all,\n    )\n\n\ndef augmentation_consistency(model, loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [i.item() for i in s if i != PAD]\n                base = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = torch.nn.functional.pad(\n                        aug_ids, (0, s.size(0) - aug_ids.size(0)), value=PAD\n                    ).unsqueeze(0)\n                    aug_pred = model(aug_ids.to(device)).argmax().item()\n                    if aug_pred != base:\n                        ok = False\n                total += 1\n                consistent += ok\n    return consistent / total\n\n\n# ------------------------------------------------------------\n#  core experiment runner ------------------------------------\ndef run_experiment(tag: str, one_sided: bool):\n    print(f\"\\n=== Running experiment: {tag} | one_sided={one_sided} ===\")\n    # containers\n    experiment_data[tag] = {\n        \"SPR\": {\n            \"metrics\": {\"val_ACS\": []},\n            \"losses\": {\"contrastive\": [], \"train_sup\": [], \"val_sup\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n    # loaders\n    contrast_loader = DataLoader(\n        ContrastiveDataset(spr[\"train\"][\"sequence\"], one_sided=one_sided),\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_contrastive,\n    )\n    sup_train_loader = DataLoader(\n        SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_supervised,\n    )\n    sup_val_loader = DataLoader(\n        SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_supervised,\n    )\n    # contrastive pre-training\n    encoder = Encoder(vocab_size).to(device)\n    enc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n    for ep in range(1, pre_epochs + 1):\n        encoder.train()\n        tot = 0\n        for batch in contrast_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            loss = nt_xent(encoder(batch[\"v1\"]), encoder(batch[\"v2\"]))\n            enc_opt.zero_grad()\n            loss.backward()\n            enc_opt.step()\n            tot += loss.item() * batch[\"v1\"].size(0)\n        epoch_loss = tot / len(contrast_loader.dataset)\n        experiment_data[tag][\"SPR\"][\"losses\"][\"contrastive\"].append(epoch_loss)\n        print(f\" contrastive ep {ep}: loss={epoch_loss:.4f}\")\n    # supervised fine-tune\n    model = SPRModel(vocab_size, num_classes).to(device)\n    model.encoder.load_state_dict(encoder.state_dict())\n    criterion = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    for ep in range(1, sup_epochs + 1):\n        model.train()\n        run_loss = 0\n        for batch in sup_train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            loss = criterion(model(batch[\"seq\"]), batch[\"label\"])\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            run_loss += loss.item() * batch[\"seq\"].size(0)\n        train_loss = run_loss / len(sup_train_loader.dataset)\n        val_loss, val_acc, preds, gts = evaluate(model, sup_val_loader, criterion)\n        ACS = augmentation_consistency(model, sup_val_loader)\n        # log\n        ed = experiment_data[tag][\"SPR\"]\n        ed[\"losses\"][\"train_sup\"].append(train_loss)\n        ed[\"losses\"][\"val_sup\"].append(val_loss)\n        ed[\"metrics\"][\"val_ACS\"].append(ACS)\n        ed[\"predictions\"] = preds\n        ed[\"ground_truth\"] = gts\n        ed[\"epochs\"].append(ep)\n        print(\n            f\"  finetune ep {ep}: train={train_loss:.4f} val={val_loss:.4f} \"\n            f\"acc={val_acc:.4f} ACS={ACS:.4f}\"\n        )\n\n\n# ------------------------------------------------------------\n#  execute both variants -------------------------------------\nrun_experiment(\"baseline_aug_aug\", one_sided=False)\nrun_experiment(\"one_sided_raw_aug\", one_sided=True)\n\n# ------------------------------------------------------------\n#  save -------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nAll experiments finished. Data saved to 'working/experiment_data.npy'\")\n", "import os, random, pathlib, math, time\nfrom typing import List, Dict\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import Dataset as HFDataset\nfrom datasets import DatasetDict, load_dataset  # noqa\n\n# ------------------  misc & dirs -----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# -------------------- experiment container -------------------\nexperiment_data = {\n    \"NonSharingTwin\": {\n        \"SPR\": {\n            \"metrics\": {\"train_ACS\": [], \"val_ACS\": []},\n            \"losses\": {\n                \"contrastive\": [],\n                \"train_sup\": [],\n                \"val_sup\": [],\n            },\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ----------------- dataset utils -----------------------------\nSHAPES = list(\"ABCDEFGH\")\nCOLORS = list(\"01234567\")\n\n\ndef generate_seq(min_len=5, max_len=15):\n    L = random.randint(min_len, max_len)\n    return \" \".join(random.choice(SHAPES) + random.choice(COLORS) for _ in range(L))\n\n\ndef rule_label(seq: str) -> int:\n    # toy rule: modulo-4 count of 'A' shapes\n    cnt = sum(tok[0] == \"A\" for tok in seq.split())\n    return cnt % 4\n\n\ndef synthetic_split(n: int):\n    seqs, labels = [], []\n    for i in range(n):\n        s = generate_seq()\n        seqs.append(s)\n        labels.append(rule_label(s))\n    return {\"id\": [str(i) for i in range(n)], \"sequence\": seqs, \"label\": labels}\n\n\ndef load_spr(split_counts=(4000, 1000, 1000)) -> DatasetDict:\n    root = pathlib.Path(\"SPR_BENCH\")\n    try:\n        from SPR import load_spr_bench  # type: ignore\n    except ImportError:\n        load_spr_bench = None\n    if root.exists() and load_spr_bench is not None:\n        print(\"Loading official SPR_BENCH\")\n        return load_spr_bench(root)\n    print(\"Building synthetic SPR\")\n    tr, dv, te = map(synthetic_split, split_counts)\n    return DatasetDict(\n        {\n            \"train\": HFDataset.from_dict(tr),\n            \"dev\": HFDataset.from_dict(dv),\n            \"test\": HFDataset.from_dict(te),\n        }\n    )\n\n\nspr = load_spr()\n\n\n# ----------------- vocab / tokenisation ----------------------\ndef build_vocab(dataset_split):\n    vocab, idx = {\"<pad>\": 0, \"<mask>\": 1}, 2\n    for seq in dataset_split[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nPAD, MASK = vocab[\"<pad>\"], vocab[\"<mask>\"]\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n\ndef encode(seq: str) -> List[int]:\n    return [vocab[t] for t in seq.split()]\n\n\n# ------------------ augmentations ----------------------------\ndef token_mask(ids: List[int], p=0.15):\n    return [MASK if (tok != PAD and random.random() < p) else tok for tok in ids]\n\n\ndef local_shuffle(ids: List[int], window=3):\n    ids = ids.copy()\n    i = 0\n    while i < len(ids):\n        j = min(len(ids), i + window)\n        random.shuffle(ids[i:j])\n        i += window\n    return ids\n\n\ndef augment(ids: List[int]):\n    return token_mask(ids) if random.random() < 0.5 else local_shuffle(ids)\n\n\n# ------------------- torch datasets --------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, sequences):\n        self.samples = [encode(s) for s in sequences]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        return torch.LongTensor(augment(ids)), torch.LongTensor(augment(ids))\n\n\nclass SupervisedDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.samples = [encode(s) for s in sequences]\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return torch.LongTensor(self.samples[idx]), torch.tensor(self.labels[idx])\n\n\ndef pad_sequences(seq_list: List[torch.Tensor]) -> torch.Tensor:\n    maxlen = max(len(s) for s in seq_list)\n    padded = [\n        torch.nn.functional.pad(s, (0, maxlen - len(s)), value=PAD) for s in seq_list\n    ]\n    return torch.stack(padded)\n\n\ndef collate_contrastive(batch):\n    v1 = pad_sequences([b[0] for b in batch])\n    v2 = pad_sequences([b[1] for b in batch])\n    return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_supervised(batch):\n    seqs = pad_sequences([b[0] for b in batch])\n    labels = torch.stack([b[1] for b in batch]).long()\n    return {\"seq\": seqs, \"label\": labels}\n\n\n# -------------------- model defs -----------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True)\n\n    def forward(self, x):\n        e = self.emb(x)\n        _, h = self.gru(e)\n        return torch.nn.functional.normalize(h.squeeze(0), dim=-1)\n\n\nclass SPRModel(nn.Module):\n    def __init__(self, encoder: Encoder, num_classes: int):\n        super().__init__()\n        self.encoder = encoder\n        hid = next(encoder.parameters()).size(-1)\n        self.classifier = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        return self.classifier(self.encoder(x))\n\n\ndef nt_xent(z1, z2, temp=0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)\n    sim = torch.matmul(z, z.T) / temp\n    mask = torch.eye(2 * B, device=z.device).bool()\n    sim.masked_fill_(mask, -9e15)\n    pos = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    return nn.CrossEntropyLoss()(sim, pos)\n\n\n# ------------------ dataloaders ------------------------------\nbatch_size = 128\ncontrast_loader = DataLoader(\n    ContrastiveDataset(spr[\"train\"][\"sequence\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\nsup_train_loader = DataLoader(\n    SupervisedDataset(spr[\"train\"][\"sequence\"], spr[\"train\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_supervised,\n)\nsup_val_loader = DataLoader(\n    SupervisedDataset(spr[\"dev\"][\"sequence\"], spr[\"dev\"][\"label\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_supervised,\n)\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n# --------------- Non-sharing twin encoders -------------------\nencoder_A, encoder_B = Encoder(vocab_size).to(device), Encoder(vocab_size).to(device)\nopt = torch.optim.Adam(\n    itertools.chain(encoder_A.parameters(), encoder_B.parameters()), lr=1e-3\n)\n\npre_epochs = 2\nfor ep in range(1, pre_epochs + 1):\n    encoder_A.train(), encoder_B.train()\n    total = 0\n    for batch in contrast_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = encoder_A(batch[\"v1\"]), encoder_B(batch[\"v2\"])\n        loss = nt_xent(z1, z2)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item() * batch[\"v1\"].size(0)\n    epoch_loss = total / len(contrast_loader.dataset)\n    experiment_data[\"NonSharingTwin\"][\"SPR\"][\"losses\"][\"contrastive\"].append(epoch_loss)\n    print(f\"[Contrastive] epoch {ep}: loss={epoch_loss:.4f}\")\n\n# ------------------- fine-tuning -----------------------------\nmodel = SPRModel(encoder_A, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef evaluate(loader):\n    model.eval()\n    tot_loss, correct, preds_all, gts_all = 0, 0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"seq\"])\n            loss = criterion(logits, batch[\"label\"])\n            tot_loss += loss.item() * batch[\"seq\"].size(0)\n            preds = logits.argmax(-1)\n            correct += (preds == batch[\"label\"]).sum().item()\n            preds_all.extend(preds.cpu().tolist())\n            gts_all.extend(batch[\"label\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        preds_all,\n        gts_all,\n        correct / len(loader.dataset),\n    )\n\n\ndef augmentation_consistency(loader, variants=3):\n    model.eval()\n    total, consistent = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            seqs, labels = batch[\"seq\"], batch[\"label\"]\n            for s, l in zip(seqs, labels):\n                ids = [tok.item() for tok in s if tok != PAD]\n                base_pred = model(s.unsqueeze(0).to(device)).argmax().item()\n                ok = base_pred == l.item()\n                for _ in range(variants):\n                    aug_ids = torch.LongTensor(augment(ids))\n                    aug_ids = nn.functional.pad(\n                        aug_ids, (0, s.size(0) - len(aug_ids)), value=PAD\n                    ).unsqueeze(0)\n                    if model(aug_ids.to(device)).argmax().item() != base_pred:\n                        ok = False\n                consistent += ok\n                total += 1\n    return consistent / total if total else 0.0\n\n\nsup_epochs = 3\nfor ep in range(1, sup_epochs + 1):\n    # train\n    model.train()\n    running = 0\n    for batch in sup_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"seq\"])\n        loss = criterion(out, batch[\"label\"])\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        running += loss.item() * batch[\"seq\"].size(0)\n    train_loss = running / len(sup_train_loader.dataset)\n\n    # val\n    val_loss, preds, gts, val_acc = evaluate(sup_val_loader)\n    ACS = augmentation_consistency(sup_val_loader)\n\n    # log\n    ed = experiment_data[\"NonSharingTwin\"][\"SPR\"]\n    ed[\"losses\"][\"train_sup\"].append(train_loss)\n    ed[\"losses\"][\"val_sup\"].append(val_loss)\n    ed[\"metrics\"][\"val_ACS\"].append(ACS)\n    ed[\"epochs\"].append(ep)\n    ed[\"predictions\"] = preds\n    ed[\"ground_truth\"] = gts\n\n    print(\n        f\"[FineTune] epoch {ep}: train_loss={train_loss:.4f} \"\n        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} ACS={ACS:.4f}\"\n    )\n\n# -------------------- save -----------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Done. Data saved to working/experiment_data.npy\")\n"], "term_out": ["['Using device: cuda', '\\n', 'SPR_BENCH not found, building synthetic dataset',\n'\\n', 'Vocab size: 66', '\\n', 'Contrastive epoch 1: loss = 4.4463', '\\n',\n'Contrastive epoch 2: loss = 3.9439', '\\n', 'Epoch 1: train_loss=1.2951\nval_loss=1.1763 val_acc=0.5200  ACS=0.4510', '\\n', 'Epoch 2: train_loss=1.0265\nval_loss=0.8432 val_acc=0.8480  ACS=0.7010', '\\n', 'Epoch 3: train_loss=0.7411\nval_loss=0.6301 val_acc=0.8630  ACS=0.7170', '\\n', \"Training complete, data\nsaved to 'working/experiment_data.npy'\", '\\n', 'Execution time: a minute seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, creating synthetic data',\n'\\n', 'Vocab size: 66', '\\n', 'Epoch 1: train_loss=1.2921  val_loss=1.1418\nval_acc=0.3730  ACS=0.3420', '\\n', 'Epoch 2: train_loss=0.9160  val_loss=0.5849\nval_acc=0.8540  ACS=0.6490', '\\n', 'Epoch 3: train_loss=0.5294  val_loss=0.4090\nval_acc=0.8440  ACS=0.6170', '\\n', 'Finished. Data saved to\nworking/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, building synthetic dataset',\n'\\n', 'Contrastive epoch 1: loss=4.3097', '\\n', 'Contrastive epoch 2:\nloss=3.7895', '\\n', 'Epoch 1: train_loss=1.2909 val_loss=1.1781 val_acc=0.5310\nACS=0.4300', '\\n', 'Epoch 2: train_loss=1.0209 val_loss=0.8627 val_acc=0.8150\nACS=0.6490', '\\n', 'Epoch 3: train_loss=0.7484 val_loss=0.6541 val_acc=0.8410\nACS=0.5790', '\\n', \"Training complete, data saved to\n'working/experiment_data.npy'\", '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, building synthetic dataset',\n'\\n', 'Vocab size: 66', '\\n', 'Combined synthetic corpus size: 12000', '\\n',\n'Contrastive epoch 1: loss=4.1045', '\\n', 'Contrastive epoch 2: loss=3.8489',\n'\\n', 'Epoch 1: train_loss=1.2958 val_loss=1.1938 val_acc=0.5220 ACS=0.4410',\n'\\n', 'Epoch 2: train_loss=1.0296 val_loss=0.8617 val_acc=0.8440 ACS=0.7040',\n'\\n', 'Epoch 3: train_loss=0.7516 val_loss=0.6582 val_acc=0.8330 ACS=0.6880',\n'\\n', \"Training complete, data saved to 'working/experiment_data.npy'\", '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, building synthetic dataset',\n'\\n', 'Vocab size: 66', '\\n', '[Contrastive] epoch 1: loss = 3.9955', '\\n',\n'[Contrastive] epoch 2: loss = 1.3766', '\\n', '[Fine-tune] Epoch 1:\ntrain_loss=1.1614  val_loss=0.8986  val_acc=0.6590  ACS=0.5350', '\\n', '[Fine-\ntune] Epoch 2: train_loss=0.6389  val_loss=0.4358  val_acc=0.8930  ACS=0.7270',\n'\\n', '[Fine-tune] Epoch 3: train_loss=0.3415  val_loss=0.2889  val_acc=0.9180\nACS=0.7220', '\\n', \"Training complete; results saved to\n'working/experiment_data.npy'\", '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, building synthetic dataset',\n'\\n', 'Vocab size: 66', '\\n', '[Contrastive] epoch 1: loss = 4.4437', '\\n',\n'[Contrastive] epoch 2: loss = 3.9425', '\\n', '[Linear-Probe] Epoch 1:\ntrain_loss=1.3674  val_loss=1.3521 val_acc=0.3420 train_ACS=0.3073\nval_ACS=0.2790', '\\n', '[Linear-Probe] Epoch 2: train_loss=1.3404\nval_loss=1.3300 val_acc=0.3640 train_ACS=0.3055 val_ACS=0.2970', '\\n', '[Linear-\nProbe] Epoch 3: train_loss=1.3205  val_loss=1.3127 val_acc=0.3700\ntrain_ACS=0.3157 val_ACS=0.3160', '\\n', \"Ablation run complete, data saved to\n'working/experiment_data.npy'\", '\\n', 'Execution time: 5 minutes seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found, building synthetic dataset',\n'\\n', 'Vocab size: 66', '\\n', 'Contrastive epoch 1: loss = 4.0998', '\\n',\n'Contrastive epoch 2: loss = 4.0650', '\\n', 'Epoch 1: train_loss=1.3681\nval_loss=1.3499  val_acc=0.3660  ACS=0.3540', '\\n', 'Epoch 2: train_loss=1.3331\nval_loss=1.3167  val_acc=0.3660  ACS=0.3540', '\\n', 'Epoch 3: train_loss=1.3022\nval_loss=1.2872  val_acc=0.3880  ACS=0.3590', '\\n', \"Training complete, data\nsaved to 'working/experiment_data.npy'\", '\\n', 'Execution time: 5 seconds\nseconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'SPR_BENCH not found, building synthetic\ndataset', '\\n', 'Vocab size:', ' ', '66', '\\n', '\\n=== Running experiment:\nbaseline_aug_aug | one_sided=False ===', '\\n', ' contrastive ep 1: loss=4.4693',\n'\\n', ' contrastive ep 2: loss=3.9228', '\\n', '  finetune ep 1: train=1.2951\nval=1.1774 acc=0.5450 ACS=0.4560', '\\n', '  finetune ep 2: train=1.0344\nval=0.8740 acc=0.8370 ACS=0.6380', '\\n', '  finetune ep 3: train=0.7579\nval=0.6440 acc=0.8490 ACS=0.6600', '\\n', '\\n=== Running experiment:\none_sided_raw_aug | one_sided=True ===', '\\n', ' contrastive ep 1: loss=4.3686',\n'\\n', ' contrastive ep 2: loss=3.8682', '\\n', '  finetune ep 1: train=1.2662\nval=1.1349 acc=0.5700 ACS=0.4210', '\\n', '  finetune ep 2: train=0.9856\nval=0.8328 acc=0.8160 ACS=0.6210', '\\n', '  finetune ep 3: train=0.7278\nval=0.6268 acc=0.8470 ACS=0.6340', '\\n', \"\\nAll experiments finished. Data saved\nto 'working/experiment_data.npy'\", '\\n', 'Execution time: 11 seconds seconds\n(time limit is 30 minutes).']", "['Device:', ' ', 'cuda', '\\n', 'Building synthetic SPR', '\\n', 'Vocab size:', '\n', '66', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n224, in <module>\\n    itertools.chain(encoder_A.parameters(),\nencoder_B.parameters()), lr=1e-3\\n    ^^^^^^^^^\\nNameError: name \\'itertools\\'\nis not defined\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']"], "analysis": ["", "", "", "", "", "", "", "", "The execution failed due to a missing import for the 'itertools' module. The\nerror occurred when attempting to use 'itertools.chain' for combining parameters\nof two encoders. To fix this issue, add 'import itertools' at the top of the\nscript."], "exc_type": [null, null, null, null, null, null, null, null, "NameError"], "exc_info": [null, null, null, null, null, null, null, null, {"args": ["name 'itertools' is not defined"], "name": "itertools"}], "exc_stack": [null, null, null, null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 224, "<module>", "itertools.chain(encoder_A.parameters(), encoder_B.parameters()), lr=1e-3"]]], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "contrastive loss", "lower_is_better": true, "description": "Measures the performance of the contrastive model during training.", "data": [{"dataset_name": "SPR", "final_value": 3.9439, "best_value": 3.9439}]}, {"metric_name": "supervised training loss", "lower_is_better": true, "description": "Measures the performance of the supervised model during training.", "data": [{"dataset_name": "SPR", "final_value": 0.7411, "best_value": 0.7411}]}, {"metric_name": "supervised validation loss", "lower_is_better": true, "description": "Measures the performance of the supervised model on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.6301, "best_value": 0.6301}]}, {"metric_name": "validation augmentation consistency score", "lower_is_better": false, "description": "Measures the consistency of augmented data during validation.", "data": [{"dataset_name": "SPR", "final_value": 0.717, "best_value": 0.717}]}]}, {"metric_names": [{"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "validation", "final_value": 0.854, "best_value": 0.854}]}, {"metric_name": "validation augmentation consistency score", "lower_is_better": false, "description": "The consistency score of the model under data augmentation on the validation dataset.", "data": [{"dataset_name": "validation", "final_value": 0.649, "best_value": 0.649}]}, {"metric_name": "training supervised loss", "lower_is_better": true, "description": "The supervised loss of the model on the training dataset.", "data": [{"dataset_name": "training", "final_value": 0.5294, "best_value": 0.5294}]}, {"metric_name": "validation supervised loss", "lower_is_better": true, "description": "The supervised loss of the model on the validation dataset.", "data": [{"dataset_name": "validation", "final_value": 0.409, "best_value": 0.409}]}]}, {"metric_names": [{"metric_name": "Validation augmentation consistency score", "lower_is_better": false, "description": "Measures the consistency of the validation data under augmentation.", "data": [{"dataset_name": "SPR", "final_value": 0.579, "best_value": 0.579}]}, {"metric_name": "Contrastive pre-training loss", "lower_is_better": true, "description": "Loss during the contrastive pre-training phase.", "data": [{"dataset_name": "SPR", "final_value": 3.7895, "best_value": 3.7895}]}, {"metric_name": "Supervised training loss", "lower_is_better": true, "description": "Loss during the supervised training phase.", "data": [{"dataset_name": "SPR", "final_value": 0.7484, "best_value": 0.7484}]}, {"metric_name": "Supervised validation loss", "lower_is_better": true, "description": "Loss during validation in the supervised training phase.", "data": [{"dataset_name": "SPR", "final_value": 0.6541, "best_value": 0.6541}]}]}, {"metric_names": [{"metric_name": "contrastive loss", "lower_is_better": true, "description": "Measures the contrastive loss during training.", "data": [{"dataset_name": "multi_syn_pretrain / SPR", "final_value": 3.8489, "best_value": 3.8489}]}, {"metric_name": "supervised training loss", "lower_is_better": true, "description": "Measures the supervised training loss.", "data": [{"dataset_name": "multi_syn_pretrain / SPR", "final_value": 0.7516, "best_value": 0.7516}]}, {"metric_name": "supervised validation loss", "lower_is_better": true, "description": "Measures the supervised validation loss.", "data": [{"dataset_name": "multi_syn_pretrain / SPR", "final_value": 0.6582, "best_value": 0.6582}]}, {"metric_name": "validation augmentation consistency (ACS)", "lower_is_better": false, "description": "Measures the consistency of augmentations during validation.", "data": [{"dataset_name": "multi_syn_pretrain / SPR", "final_value": 0.704, "best_value": 0.704}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation set.", "data": [{"dataset_name": "multi_syn_pretrain / SPR", "final_value": 0.833, "best_value": 0.833}]}]}, {"metric_names": [{"metric_name": "validation augmentation consistency score", "lower_is_better": false, "description": "Measures the consistency of the model's performance under validation augmentation.", "data": [{"dataset_name": "validation", "final_value": 0.727, "best_value": 0.727}]}, {"metric_name": "contrastive pre-training loss", "lower_is_better": true, "description": "Loss value during contrastive pre-training.", "data": [{"dataset_name": "training", "final_value": 1.3766, "best_value": 1.3766}]}, {"metric_name": "training supervised loss", "lower_is_better": true, "description": "Loss value during supervised training.", "data": [{"dataset_name": "training", "final_value": 1.1614, "best_value": 1.1614}]}, {"metric_name": "validation supervised loss", "lower_is_better": true, "description": "Loss value during validation in supervised training.", "data": [{"dataset_name": "validation", "final_value": 0.8986, "best_value": 0.8986}]}]}, {"metric_names": [{"metric_name": "training augmentation consistency", "lower_is_better": false, "description": "Measures the consistency of training data augmentation.", "data": [{"dataset_name": "SPR", "final_value": 0.3157, "best_value": 0.3157}]}, {"metric_name": "validation augmentation consistency", "lower_is_better": false, "description": "Measures the consistency of validation data augmentation.", "data": [{"dataset_name": "SPR", "final_value": 0.316, "best_value": 0.316}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation dataset.", "data": [{"dataset_name": "SPR", "final_value": 0.37, "best_value": 0.37}]}, {"metric_name": "contrastive loss", "lower_is_better": true, "description": "Measures the contrastive loss during training.", "data": [{"dataset_name": "SPR", "final_value": 3.9425, "best_value": 3.9425}]}, {"metric_name": "supervised training loss", "lower_is_better": true, "description": "Measures the supervised training loss.", "data": [{"dataset_name": "SPR", "final_value": 1.3205, "best_value": 1.3205}]}, {"metric_name": "supervised validation loss", "lower_is_better": true, "description": "Measures the supervised validation loss.", "data": [{"dataset_name": "SPR", "final_value": 1.3127, "best_value": 1.3127}]}]}, {"metric_names": [{"metric_name": "validation ACS", "lower_is_better": false, "description": "The final validation ACS score.", "data": [{"dataset_name": "RemoveRecurrent", "final_value": 0.359, "best_value": 0.359}]}, {"metric_name": "contrastive pre-training loss", "lower_is_better": true, "description": "The final contrastive pre-training loss value.", "data": [{"dataset_name": "RemoveRecurrent", "final_value": 4.065, "best_value": 4.065}]}, {"metric_name": "supervised training loss", "lower_is_better": true, "description": "The final supervised training loss value.", "data": [{"dataset_name": "RemoveRecurrent", "final_value": 1.3022, "best_value": 1.3022}]}, {"metric_name": "supervised validation loss", "lower_is_better": true, "description": "The final supervised validation loss value.", "data": [{"dataset_name": "RemoveRecurrent", "final_value": 1.2872, "best_value": 1.2872}]}]}, {"metric_names": [{"metric_name": "contrastive pretraining loss", "lower_is_better": true, "description": "Loss during the contrastive pretraining phase.", "data": [{"dataset_name": "baseline_aug_aug", "final_value": 3.9228, "best_value": 3.9228}, {"dataset_name": "one_sided_raw_aug", "final_value": 3.8682, "best_value": 3.8682}]}, {"metric_name": "supervised training loss", "lower_is_better": true, "description": "Loss during the supervised training phase.", "data": [{"dataset_name": "baseline_aug_aug", "final_value": 0.7579, "best_value": 0.7579}, {"dataset_name": "one_sided_raw_aug", "final_value": 0.7278, "best_value": 0.7278}]}, {"metric_name": "supervised validation loss", "lower_is_better": true, "description": "Loss during the supervised validation phase.", "data": [{"dataset_name": "baseline_aug_aug", "final_value": 0.644, "best_value": 0.644}, {"dataset_name": "one_sided_raw_aug", "final_value": 0.6268, "best_value": 0.6268}]}, {"metric_name": "validation augmentation consistency score", "lower_is_better": false, "description": "Consistency score for validation augmentation.", "data": [{"dataset_name": "baseline_aug_aug", "final_value": 0.66, "best_value": 0.66}, {"dataset_name": "one_sided_raw_aug", "final_value": 0.634, "best_value": 0.634}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_supervised_loss_curves.png", "../../logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_val_ACS.png", "../../logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_metrics_curves.png", "../../logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_supervised_loss.png", "../../logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_ACS_curve.png", "../../logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_supervised_losses.png", "../../logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_val_ACS.png"], ["../../logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_supervised_losses.png", "../../logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_val_ACS.png", "../../logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_supervised_loss.png", "../../logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_ACS.png"], ["../../logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_supervised_losses.png", "../../logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_val_ACS.png", "../../logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_contrastive_loss.png", "../../logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_sup_loss.png", "../../logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_ACS_curve.png", "../../logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_ACS_final.png"], []], "plot_paths": [["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_supervised_loss_curves.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_val_ACS.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_confusion_matrix.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_loss_curves.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_metrics_curves.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_confusion_matrix.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_supervised_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_ACS_curve.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_confusion_matrix.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_supervised_losses.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_val_ACS.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_supervised_losses.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_val_ACS.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_confusion_matrix.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_supervised_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_val_accuracy.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_ACS.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_supervised_losses.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_val_ACS.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_class_distribution.png"], ["experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_contrastive_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_sup_loss.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_ACS_curve.png", "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_ACS_final.png"], []], "plot_analyses": [[{"analysis": "The contrastive pre-training loss shows a steady decline over the epochs, starting from 4.4 and reaching 4.0 by the second epoch. This indicates that the contrastive learning framework is effectively reducing the NT-Xent loss, suggesting an improvement in the quality of the learned embeddings. However, the loss is still above the target of 0.4, indicating room for further optimization, possibly through enhanced data augmentation or alternative loss functions.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_contrastive_loss.png"}, {"analysis": "The supervised training loss and validation loss both decrease consistently over three epochs, with the training loss dropping from 1.3 to below 0.7 and the validation loss showing a similar trend. This indicates effective learning and generalization. The gap between the training and validation losses is minimal, suggesting no significant overfitting at this stage. However, the target supervised training loss of 0.35 has not been reached, which may require adjustments in learning rate schedules or optimizers.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_supervised_loss_curves.png"}, {"analysis": "The validation augmentation-consistency score (ACS) improves steadily from 0.45 to approximately 0.8 over three epochs. This suggests that the model is becoming more robust to augmented data and is learning consistent representations across augmentations. The plateauing trend after the second epoch might indicate that further improvements could require more diverse or targeted augmentation strategies.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_val_ACS.png"}, {"analysis": "The confusion matrix for the final validation epoch reveals that the model performs well on most classes, with the highest correct classification counts in classes 0 and 1. However, there are some misclassifications, particularly in class 2, where a significant number of samples are predicted as class 0. This suggests potential issues with class imbalance or representation quality for class 2, which could be addressed by rebalancing the dataset or incorporating class-specific data augmentation techniques.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cc4be2d211904af3a9e19d8ba6f9338e_proc_3101765/SPR_confusion_matrix.png"}], [{"analysis": "The supervised loss curves indicate consistent improvement in both training and validation losses over the epochs, showing convergence. The training loss decreases steadily, while the validation loss also drops, suggesting that the model generalizes well to the validation data and is not overfitting within the observed epochs.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_loss_curves.png"}, {"analysis": "The validation accuracy and ACS (Average Class Score) curves show that validation accuracy improves significantly in the first two epochs and then plateaus, indicating that the model reaches its peak performance early. The ACS follows a similar trend but shows a slight decline in the third epoch, which may indicate some degradation in balanced performance across classes.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_metrics_curves.png"}, {"analysis": "The confusion matrix for the final epoch reveals strong performance for the majority of classes, especially for class 1 (330 correct predictions) and class 0 (284 correct predictions). However, there is noticeable misclassification in class 3, where 79 instances are classified as class 2. This suggests that the model struggles with distinguishing between these classes, possibly due to similar features or insufficient training data for class 3.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa5f8f749b394275b8ff599e7f720f85_proc_3107293/SPR_confusion_matrix.png"}], [{"analysis": "The plot shows the cross-entropy loss for both training and validation datasets over three epochs. Both losses decrease steadily, indicating effective learning and no immediate signs of overfitting. The validation loss being consistently lower than the training loss might suggest that the model generalizes well, although further analysis is needed to confirm this.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_supervised_loss.png"}, {"analysis": "This plot depicts the NT-Xent loss during the contrastive pre-training stage. The loss decreases steadily, which is a positive sign that the model is learning meaningful representations through contrastive learning.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_contrastive_loss.png"}, {"analysis": "The augmentation consistency score (ACS) increases initially but then slightly declines after reaching its peak. This suggests that the augmentations initially enhanced the model's consistency but might have introduced noise or diminishing returns in later epochs. Fine-tuning the augmentation strategy could help optimize this behavior.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_ACS_curve.png"}, {"analysis": "The confusion matrix indicates the model's classification performance across four classes. Class 1 shows the highest accuracy, while Class 3 has the lowest performance with significant misclassifications. This suggests that the model struggles with certain patterns or features in Class 3, requiring further investigation or targeted improvements.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_4e0f95adff324cbd8ca6bc7b26ad51c5_proc_3107294/SPR_confusion_matrix.png"}], [{"analysis": "The plot indicates a steady decrease in contrastive loss over two epochs, suggesting that the pre-training process is effective in learning meaningful representations. The consistent decline in loss demonstrates that the model is optimizing its contrastive learning objective, which is a positive sign for the robustness of the learned embeddings.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_contrastive_loss.png"}, {"analysis": "The plot shows a consistent decrease in both training and validation loss over three epochs. This indicates that the model is learning effectively during fine-tuning and is not overfitting, as the validation loss decreases alongside the training loss. The gap between the two losses is also narrowing, which is another positive indicator of generalization.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_supervised_losses.png"}, {"analysis": "The augmentation consistency score improves significantly during the first two epochs and then stabilizes in the third epoch. This suggests that the model is becoming more robust to data augmentations as training progresses, which aligns with the goal of enhancing the model's generalization capabilities.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_40a00200d5e44f00a5782c3d89156782_proc_3107295/SPR_val_ACS.png"}], [{"analysis": "This plot shows a significant decrease in NT-Xent loss over two epochs, indicating that the contrastive learning objective is being effectively optimized. The rapid reduction in loss suggests that the model is learning meaningful representations from the symbolic sequences.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_contrastive_loss.png"}, {"analysis": "The plot illustrates the supervised cross-entropy loss for both training and validation sets over three epochs. Both losses decrease steadily, with the validation loss closely following the training loss, indicating good generalization and a lack of overfitting. This suggests that the learned representations are effectively aiding the supervised task.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_supervised_losses.png"}, {"analysis": "The Augmentation Consistency Score (ACS) for the validation set increases significantly from epoch 1 to epoch 2 and then plateaus. This indicates that the model's representations are becoming more invariant to data augmentations, which is a desirable property for robust feature learning.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_val_ACS.png"}, {"analysis": "The confusion matrix for the development set reveals that the model performs well for most classes, with high counts along the diagonal. However, there are some misclassifications, particularly for class 3, where a notable number of samples are misclassified as class 0. This suggests that further refinement of the model or data preprocessing may be needed to improve class-specific performance.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9ef66dae64644763bb39e373340414fa_proc_3107296/SPR_no_encoder_norm_confusion_matrix.png"}], [{"analysis": "This plot shows the NT-Xent loss during the contrastive learning stage. The loss decreases steadily from 4.4 to 4.0 across two epochs, indicating that the model is effectively learning to distinguish between positive and negative pairs. This suggests that the contrastive learning framework is functioning as intended, and the embeddings are becoming more discriminative.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_contrastive_loss.png"}, {"analysis": "This plot compares the cross-entropy loss for training and validation datasets across three epochs. Both losses decrease consistently, with the training loss slightly lower than the validation loss throughout. This indicates that the model is learning effectively without significant overfitting at this stage. The gap between the two losses is minimal, suggesting good generalization.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_supervised_loss.png"}, {"analysis": "This plot illustrates the validation accuracy over three epochs. The accuracy shows a slight upward trend, improving from approximately 0.4 to just above 0.4. While the improvement is modest, it indicates that the model is making progress in learning the task.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_val_accuracy.png"}, {"analysis": "This plot displays the Augmentation Consistency Score (ACS) for both training and validation datasets over three epochs. Both scores show a slight increase, suggesting improved consistency in the model's predictions when augmented data is used. However, the scores remain relatively low, indicating room for further optimization in the augmentation strategies.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d7c93885602d4455af5558d392bdc224_proc_3107294/SPR_ACS.png"}], [{"analysis": "This plot shows the pre-training loss for the contrastive learning model over two epochs. The steady decline in loss from 4.1 to 4.065 indicates that the model is effectively optimizing its objective during pre-training. However, the small number of epochs suggests that the pre-training process might be incomplete or that the model converged quickly. Further training could reveal additional trends or improvements.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_contrastive_loss.png"}, {"analysis": "This plot compares the supervised loss for both the training and validation datasets over three epochs. The consistent decline in both curves suggests that the model is learning effectively without overfitting. The close alignment of the training and validation losses indicates good generalization, which is a positive outcome for the supervised fine-tuning stage.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_supervised_losses.png"}, {"analysis": "This plot tracks the augmentation consistency score (ACS) on the validation set. The flat trend during the first two epochs followed by a sharp increase in the third epoch suggests that the model initially struggled to maintain consistency across augmented samples but improved significantly later. This improvement could result from better embeddings or refined augmentations.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_val_ACS.png"}, {"analysis": "This plot compares the ground truth class distribution with the predicted distribution at the last epoch. The mismatch in class distributions, particularly the over-prediction of class 1 and under-prediction of other classes, indicates a bias in the model's predictions. This bias could stem from imbalanced training data or insufficient representation of certain classes in the learned embeddings.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_bc967a2feb4d4d3fb8bdcf9014e86af9_proc_3107295/SPR_class_distribution.png"}], [{"analysis": "The plot compares the pre-training loss trajectories for two augmentation strategies: 'baseline_aug_aug' and 'one_sided_raw_aug.' Both strategies show a consistent reduction in loss over the two epochs, indicating successful convergence. However, 'one_sided_raw_aug' achieves a slightly lower loss than 'baseline_aug_aug,' suggesting that it may be more effective at encoding symbolic representations during pre-training.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_contrastive_loss.png"}, {"analysis": "This plot illustrates the supervised training and validation loss for the two augmentation strategies over three fine-tuning epochs. Both strategies exhibit a steady decrease in loss, with 'one_sided_raw_aug' consistently outperforming 'baseline_aug_aug' in both training and validation losses. This indicates that 'one_sided_raw_aug' leads to better generalization and fine-tuning performance for the SPR task.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_sup_loss.png"}, {"analysis": "The augmentation-consistency score (ACS) is plotted for the validation set over three fine-tuning epochs. Both strategies show an improvement in ACS, with 'baseline_aug_aug' slightly outperforming 'one_sided_raw_aug' in all epochs. This suggests that 'baseline_aug_aug' maintains better consistency under data augmentations during validation.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_ACS_curve.png"}, {"analysis": "The bar chart presents the final augmentation-consistency scores (ACS) for the two strategies after fine-tuning. 'baseline_aug_aug' achieves a slightly higher ACS (0.66) compared to 'one_sided_raw_aug' (0.63), reinforcing the observation that 'baseline_aug_aug' provides better augmentation-consistency.", "plot_path": "experiments/2025-08-16_02-32-02_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b477d7c812664fb7b26ea61ecd108991_proc_3107293/SPR_ACS_final.png"}], []], "vlm_feedback_summary": ["The plots indicate steady improvements in pre-training and supervised learning\nstages, with decreasing losses and increasing augmentation-consistency scores.\nHowever, the target loss values have not yet been met, and some class-specific\nmisclassification issues remain. Further optimization of loss functions,\naugmentation strategies, and dataset balancing is recommended.", "The plots collectively demonstrate that the model converges well and achieves\ngood validation performance. However, there are signs of class imbalance and\nspecific misclassification issues, particularly between certain classes, which\ncould be addressed in future iterations.", "The plots collectively indicate that the model is learning effectively during\nboth the supervised training and contrastive pre-training stages. However, there\nare areas for improvement, such as optimizing augmentation strategies and\naddressing misclassifications in specific classes.", "The provided plots demonstrate effective pre-training and fine-tuning processes,\nwith steady improvements in contrastive loss, supervised training/validation\nlosses, and augmentation consistency scores. These results suggest that the\ncontext-aware contrastive learning approach is progressing well and contributing\nto robust feature representation learning.", "The plots collectively demonstrate effective optimization of the contrastive\nloss, good generalization in supervised learning, improved augmentation\nconsistency, and reasonable classification performance. However, there is room\nfor improvement in class-specific accuracy, particularly for class 3.", "The plots demonstrate steady improvements in loss metrics and slight gains in\naccuracy and augmentation consistency. The results indicate that the model is\nlearning effectively, but there is potential for further optimization,\nparticularly in augmentation strategies and accuracy improvements.", "The experimental plots provide valuable insights into the training process and\nmodel behavior. The pre-training loss indicates effective optimization, though\nadditional epochs could be explored. The supervised loss curves show promising\ngeneralization without overfitting. The augmentation consistency score\nhighlights late-stage improvements in handling augmented samples. However, the\nclass distribution plot reveals prediction biases, suggesting areas for\nimprovement in class representation.", "The experimental results indicate that the 'one_sided_raw_aug' strategy performs\nbetter in terms of pre-training loss and supervised training/validation loss,\nsuggesting enhanced generalization and fine-tuning capabilities. However, the\n'baseline_aug_aug' strategy demonstrates superior augmentation-consistency, as\nevidenced by higher ACS values throughout the experiments. This trade-off\nhighlights the need to balance generalization and augmentation-consistency in\nthe proposed context-aware contrastive learning framework.", "[]"], "exec_time": [76.45946168899536, 78.02150845527649, 7.346853494644165, 7.554171562194824, 63.22986030578613, 359.5128755569458, 5.903555393218994, 11.297147274017334, 1.1203677654266357], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["['SPR']"], ["[\"SPR\"]"], ["[\"multi_syn_pretrain\"]"], ["['SPR \u2013 no_encoder_norm']"], ["['SPR']"], ["[]"], ["['SPR']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\nspr = experiment_data.get(\"SPR\", {})\nlosses = spr.get(\"losses\", {})\nmetrics = spr.get(\"metrics\", {})\nepochs = np.array(spr.get(\"epochs\", []))\n\n# 1) Contrastive pre-training loss --------------------------------------------\ntry:\n    cont_loss = np.array(losses.get(\"contrastive\", []))\n    if cont_loss.size:\n        plt.figure()\n        plt.plot(np.arange(1, len(cont_loss) + 1), cont_loss, marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"NT-Xent Loss\")\n        plt.title(\"SPR: Contrastive Pre-training Loss\")\n        fname = os.path.join(working_dir, \"SPR_contrastive_loss.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating contrastive loss plot: {e}\")\n    plt.close()\n\n# 2) Supervised train / val loss ----------------------------------------------\ntry:\n    tr_sup = np.array(losses.get(\"train_sup\", []))\n    val_sup = np.array(losses.get(\"val_sup\", []))\n    if tr_sup.size and val_sup.size:\n        plt.figure()\n        plt.plot(epochs, tr_sup, label=\"Train\")\n        plt.plot(epochs, val_sup, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR: Supervised Training vs. Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_supervised_loss_curves.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating supervised loss plot: {e}\")\n    plt.close()\n\n# 3) Augmentation Consistency Score (ACS) -------------------------------------\ntry:\n    acs = np.array(metrics.get(\"val_ACS\", []))\n    if acs.size:\n        plt.figure()\n        plt.plot(epochs, acs, marker=\"s\", color=\"green\")\n        plt.ylim(0, 1)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"ACS\")\n        plt.title(\"SPR: Validation Augmentation-Consistency Score\")\n        fname = os.path.join(working_dir, \"SPR_val_ACS.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating ACS plot: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for final epoch -----------------------------------------\ntry:\n    preds = np.array(spr.get(\"predictions\", []))\n    gts = np.array(spr.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        num_classes = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"SPR: Confusion Matrix (Validation, Final Epoch)\")\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        fname = os.path.join(working_dir, \"SPR_confusion_matrix.png\")\n        plt.savefig(fname)\n        print(\"Saved\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# Optional: print raw arrays for quick inspection\nprint(\"Contrastive loss:\", losses.get(\"contrastive\"))\nprint(\"Train_sup loss:\", losses.get(\"train_sup\"))\nprint(\"Val_sup loss:\", losses.get(\"val_sup\"))\nprint(\"Val ACS:\", metrics.get(\"val_ACS\"))\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr_data = experiment_data[\"remove_contrastive_pretrain\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data:\n    epochs = spr_data.get(\"epochs\", [])\n    train_loss = spr_data[\"losses\"].get(\"train_sup\", [])\n    val_loss = spr_data[\"losses\"].get(\"val_sup\", [])\n    val_acc = spr_data[\"metrics\"].get(\"val_acc\", [])\n    val_acS = spr_data[\"metrics\"].get(\"val_ACS\", [])\n    preds = np.array(spr_data.get(\"predictions\", []))\n    gts = np.array(spr_data.get(\"ground_truth\", []))\n\n    # Plot 1: Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR: Supervised Loss Curves\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # Plot 2: Accuracy & ACS\n    try:\n        plt.figure()\n        plt.plot(epochs, val_acc, label=\"Validation Accuracy\")\n        plt.plot(epochs, val_acS, label=\"Validation ACS\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR: Validation Accuracy & ACS\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_metrics_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metrics plot: {e}\")\n        plt.close()\n\n    # Plot 3: Confusion matrix (only if predictions & gts align)\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            num_classes = len(np.unique(gts))\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for p, g in zip(preds, gts):\n                cm[g, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\"SPR: Confusion Matrix (Final Epoch)\")\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            plt.savefig(os.path.join(working_dir, \"SPR_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ED = experiment_data[\"no_aug_contrast\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ED = None\n\n\n# ---------- helper ----------\ndef save_close(fig_path):\n    plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n    plt.close()\n\n\n# ---------- 1. supervised losses ----------\ntry:\n    if ED and ED[\"losses\"][\"train_sup\"]:\n        epochs = ED[\"epochs\"]\n        plt.figure()\n        plt.plot(epochs, ED[\"losses\"][\"train_sup\"], label=\"Train Loss\")\n        plt.plot(epochs, ED[\"losses\"][\"val_sup\"], label=\"Val Loss\")\n        plt.title(\"SPR Supervised Training\\nLeft: Train Loss, Right: Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        save_close(os.path.join(working_dir, \"SPR_supervised_loss.png\"))\nexcept Exception as e:\n    print(f\"Error creating supervised loss plot: {e}\")\n    plt.close()\n\n# ---------- 2. contrastive pre-training loss ----------\ntry:\n    if ED and ED[\"losses\"][\"contrastive\"]:\n        c_epochs = np.arange(1, len(ED[\"losses\"][\"contrastive\"]) + 1)\n        plt.figure()\n        plt.plot(c_epochs, ED[\"losses\"][\"contrastive\"], marker=\"o\")\n        plt.title(\"SPR Contrastive Pre-training Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"NT-Xent Loss\")\n        save_close(os.path.join(working_dir, \"SPR_contrastive_loss.png\"))\nexcept Exception as e:\n    print(f\"Error creating contrastive loss plot: {e}\")\n    plt.close()\n\n# ---------- 3. ACS over epochs ----------\ntry:\n    if ED and ED[\"metrics\"][\"val_ACS\"]:\n        plt.figure()\n        plt.plot(ED[\"epochs\"], ED[\"metrics\"][\"val_ACS\"], marker=\"s\", color=\"green\")\n        plt.title(\"SPR Augmentation Consistency Score (ACS)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"ACS\")\n        plt.ylim(0, 1)\n        save_close(os.path.join(working_dir, \"SPR_ACS_curve.png\"))\nexcept Exception as e:\n    print(f\"Error creating ACS plot: {e}\")\n    plt.close()\n\n# ---------- 4. confusion matrix ----------\ntry:\n    preds = ED[\"predictions\"]\n    gts = ED[\"ground_truth\"]\n    if preds and gts:\n        num_classes = len(set(gts))\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\n            \"SPR Confusion Matrix\\nLeft: Ground Truth (rows), Right: Predictions (cols)\"\n        )\n        plt.xlabel(\"Predicted label\")\n        plt.ylabel(\"True label\")\n        plt.xticks(range(num_classes))\n        plt.yticks(range(num_classes))\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(\n                    j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n                )\n        save_close(os.path.join(working_dir, \"SPR_confusion_matrix.png\"))\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\nprint(\"Plotting complete.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data --------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nexp = experiment_data.get(\"multi_syn_pretrain\", {}).get(\"SPR\", {})\n\n\n# helper to maybe subsample long curves ---------------------------------------\ndef limit_points(xs, ys, max_pts=5):\n    if len(xs) <= max_pts:\n        return xs, ys\n    idx = np.linspace(0, len(xs) - 1, max_pts, dtype=int)\n    return [xs[i] for i in idx], [ys[i] for i in idx]\n\n\n# 1) contrastive pre-training loss --------------------------------------------\ntry:\n    losses = exp.get(\"losses\", {}).get(\"contrastive\", [])\n    if losses:\n        epochs = list(range(1, len(losses) + 1))\n        epochs, losses = limit_points(epochs, losses)\n        plt.figure()\n        plt.plot(epochs, losses, marker=\"o\", label=\"Contrastive Loss\")\n        plt.xlabel(\"Pre-training Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR: Contrastive Pre-training Loss Curve\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_contrastive_loss.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating contrastive loss plot: {e}\")\n    plt.close()\n\n# 2) supervised train/val loss -------------------------------------------------\ntry:\n    tr = exp.get(\"losses\", {}).get(\"train_sup\", [])\n    vl = exp.get(\"losses\", {}).get(\"val_sup\", [])\n    if tr and vl:\n        epochs = exp.get(\"epochs\", list(range(1, len(tr) + 1)))\n        epochs, tr = limit_points(epochs, tr)\n        _, vl = limit_points(epochs, vl)  # same indices as epochs\n        plt.figure()\n        plt.plot(epochs, tr, marker=\"o\", label=\"Train Loss\")\n        plt.plot(epochs, vl, marker=\"s\", label=\"Val Loss\")\n        plt.xlabel(\"Fine-tuning Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR: Supervised Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_supervised_losses.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating supervised loss plot: {e}\")\n    plt.close()\n\n# 3) validation ACS ------------------------------------------------------------\ntry:\n    acs = exp.get(\"metrics\", {}).get(\"val_ACS\", [])\n    if acs:\n        epochs = exp.get(\"epochs\", list(range(1, len(acs) + 1)))\n        epochs, acs = limit_points(epochs, acs)\n        plt.figure()\n        plt.plot(epochs, acs, marker=\"^\", color=\"green\", label=\"Val ACS\")\n        plt.xlabel(\"Fine-tuning Epoch\")\n        plt.ylabel(\"Augmentation Consistency Score\")\n        plt.title(\"SPR: Validation Augmentation Consistency Score\")\n        plt.ylim(0, 1)\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_val_ACS.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating ACS plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"no_encoder_norm\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp:\n    epochs = exp[\"epochs\"]\n    # 1) Contrastive loss plot\n    try:\n        plt.figure()\n        plt.plot(\n            range(1, len(exp[\"losses\"][\"contrastive\"]) + 1),\n            exp[\"losses\"][\"contrastive\"],\n            marker=\"o\",\n        )\n        plt.title(\"SPR \u2013 no_encoder_norm\\nContrastive Loss vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"NT-Xent Loss\")\n        fname = os.path.join(working_dir, \"SPR_no_encoder_norm_contrastive_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating contrastive plot: {e}\")\n        plt.close()\n\n    # 2) Supervised losses\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"losses\"][\"train_sup\"], label=\"Train\")\n        plt.plot(epochs, exp[\"losses\"][\"val_sup\"], label=\"Validation\")\n        plt.title(\"SPR \u2013 no_encoder_norm\\nSupervised Loss vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_no_encoder_norm_supervised_losses.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating supervised loss plot: {e}\")\n        plt.close()\n\n    # 3) Augmentation Consistency Score\n    try:\n        plt.figure()\n        plt.plot(epochs, exp[\"metrics\"][\"val_ACS\"], marker=\"s\", color=\"green\")\n        plt.title(\"SPR \u2013 no_encoder_norm\\nValidation ACS vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Augmentation Consistency Score\")\n        fname = os.path.join(working_dir, \"SPR_no_encoder_norm_val_ACS.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating ACS plot: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix\n    try:\n        from itertools import product\n\n        preds = np.array(exp[\"predictions\"])\n        gts = np.array(exp[\"ground_truth\"])\n        num_classes = len(np.unique(gts))\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for g, p in zip(gts, preds):\n            cm[g, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.title(\"SPR \u2013 no_encoder_norm\\nConfusion Matrix (Dev Set)\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        for i, j in product(range(num_classes), repeat=2):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                fontsize=8,\n            )\n        fname = os.path.join(working_dir, \"SPR_no_encoder_norm_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(f\"Saved {fname}\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_exp = experiment_data.get(\"freeze_encoder\", {}).get(\"SPR\", {})\n\n\n# helper ------------------------------------------------\ndef save_fig(fig, fname):\n    fig.savefig(os.path.join(working_dir, fname), dpi=150, bbox_inches=\"tight\")\n    plt.close(fig)\n\n\n# 1) contrastive loss -----------------------------------\ntry:\n    losses = spr_exp.get(\"losses\", {})\n    contr = losses.get(\"contrastive\", [])\n    if contr:\n        fig = plt.figure()\n        plt.plot(range(1, len(contr) + 1), contr, marker=\"o\")\n        plt.title(\"SPR dataset \u2013 Contrastive Stage Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"NT-Xent Loss\")\n        save_fig(fig, \"SPR_contrastive_loss.png\")\nexcept Exception as e:\n    print(f\"Error creating contrastive plot: {e}\")\n    plt.close()\n\n# 2) supervised loss ------------------------------------\ntry:\n    train_sup = losses.get(\"train_sup\", [])\n    val_sup = losses.get(\"val_sup\", [])\n    if train_sup and val_sup:\n        fig = plt.figure()\n        epochs = spr_exp.get(\"epochs\", list(range(1, len(train_sup) + 1)))\n        plt.plot(epochs, train_sup, label=\"Train\")\n        plt.plot(epochs, val_sup, label=\"Validation\")\n        plt.title(\"SPR dataset \u2013 Supervised Loss (Train vs Validation)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        save_fig(fig, \"SPR_supervised_loss.png\")\nexcept Exception as e:\n    print(f\"Error creating supervised loss plot: {e}\")\n    plt.close()\n\n# 3) validation accuracy --------------------------------\ntry:\n    val_acc = spr_exp.get(\"metrics\", {}).get(\"val_acc\", [])\n    if val_acc:\n        fig = plt.figure()\n        epochs = spr_exp.get(\"epochs\", list(range(1, len(val_acc) + 1)))\n        plt.plot(epochs, val_acc, marker=\"s\", color=\"green\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR dataset \u2013 Validation Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        save_fig(fig, \"SPR_val_accuracy.png\")\nexcept Exception as e:\n    print(f\"Error creating val accuracy plot: {e}\")\n    plt.close()\n\n# 4) augmentation-consistency ---------------------------\ntry:\n    m = spr_exp.get(\"metrics\", {})\n    tr_acs = m.get(\"train_ACS\", [])\n    val_acs = m.get(\"val_ACS\", [])\n    if tr_acs and val_acs:\n        fig = plt.figure()\n        epochs = spr_exp.get(\"epochs\", list(range(1, len(tr_acs) + 1)))\n        plt.plot(epochs, tr_acs, label=\"Train ACS\")\n        plt.plot(epochs, val_acs, label=\"Val ACS\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR dataset \u2013 Augmentation Consistency Score\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"ACS\")\n        plt.legend()\n        save_fig(fig, \"SPR_ACS.png\")\nexcept Exception as e:\n    print(f\"Error creating ACS plot: {e}\")\n    plt.close()\n\n# ---------------- print evaluation metric ---------------\ntry:\n    if val_acc:\n        print(f\"Final validation accuracy: {val_acc[-1]:.4f}\")\nexcept Exception as e:\n    print(f\"Error printing evaluation metric: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------\n# set up paths ------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# load data ---------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"RemoveRecurrent\"][\"SPR\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None:\n    epochs = exp.get(\"epochs\", [])\n    c_loss = exp.get(\"losses\", {}).get(\"contrastive\", [])\n    tr_loss = exp.get(\"losses\", {}).get(\"train_sup\", [])\n    val_loss = exp.get(\"losses\", {}).get(\"val_sup\", [])\n    acs = exp.get(\"metrics\", {}).get(\"val_ACS\", [])\n    preds = np.array(exp.get(\"predictions\", []))\n    gts = np.array(exp.get(\"ground_truth\", []))\n\n    # --------------------------------------------------------\n    # 1. contrastive loss ------------------------------------\n    try:\n        if c_loss:\n            plt.figure()\n            plt.plot(range(1, len(c_loss) + 1), c_loss, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"SPR: Contrastive Pre-training Loss\")\n            plt.tight_layout()\n            fname = os.path.join(working_dir, \"SPR_contrastive_loss.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating contrastive plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 2. supervised losses -----------------------------------\n    try:\n        if tr_loss and val_loss:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\", marker=\"o\")\n            plt.plot(epochs, val_loss, label=\"Validation\", marker=\"s\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"SPR: Supervised Loss Curves\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, \"SPR_supervised_losses.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating supervised loss plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 3. augmentation consistency ----------------------------\n    try:\n        if acs:\n            plt.figure()\n            plt.plot(epochs, acs, marker=\"^\", color=\"purple\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"ACS\")\n            plt.title(\"SPR: Augmentation Consistency Score (Validation)\")\n            plt.tight_layout()\n            fname = os.path.join(working_dir, \"SPR_val_ACS.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating ACS plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 4. prediction vs ground truth distribution -------------\n    try:\n        if preds.size and gts.size:\n            n_classes = max(preds.max(), gts.max()) + 1\n            ind = np.arange(n_classes)\n            plt.figure()\n            plt.bar(\n                ind - 0.15,\n                np.bincount(gts, minlength=n_classes),\n                width=0.3,\n                label=\"Ground Truth\",\n            )\n            plt.bar(\n                ind + 0.15,\n                np.bincount(preds, minlength=n_classes),\n                width=0.3,\n                label=\"Predictions\",\n            )\n            plt.xlabel(\"Class\")\n            plt.ylabel(\"Count\")\n            plt.title(\n                \"SPR: Class Distribution (Last Epoch)\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, \"SPR_class_distribution.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating distribution plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # print final metrics ------------------------------------\n    if preds.size and gts.size:\n        accuracy = (preds == gts).mean()\n        print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n    if acs:\n        print(f\"Final Augmentation Consistency Score: {acs[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nexp_tags = list(experiment_data.keys())\ndataset_name = \"SPR\"  # only dataset in dict\n\n# ------------------------------------------------------------\n# 1. Contrastive loss curves ---------------------------------\ntry:\n    plt.figure()\n    for tag in exp_tags:\n        losses = experiment_data[tag][dataset_name][\"losses\"].get(\"contrastive\", [])\n        if losses:\n            plt.plot(range(1, len(losses) + 1), losses, label=tag)\n    plt.xlabel(\"Pre-training epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{dataset_name}: Contrastive Pre-training Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dataset_name}_contrastive_loss.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating contrastive loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# 2. Supervised train / val loss curves ----------------------\ntry:\n    plt.figure()\n    for tag in exp_tags:\n        train_l = experiment_data[tag][dataset_name][\"losses\"].get(\"train_sup\", [])\n        val_l = experiment_data[tag][dataset_name][\"losses\"].get(\"val_sup\", [])\n        epochs = experiment_data[tag][dataset_name].get(\n            \"epochs\", list(range(1, len(train_l) + 1))\n        )\n        if train_l:\n            plt.plot(epochs, train_l, linestyle=\"--\", label=f\"{tag} train\")\n        if val_l:\n            plt.plot(epochs, val_l, linestyle=\"-\", label=f\"{tag} val\")\n    plt.xlabel(\"Fine-tuning epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{dataset_name}: Supervised Train/Val Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dataset_name}_sup_loss.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating supervised loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# 3. Augmentation Consistency Score vs epoch -----------------\ntry:\n    plt.figure()\n    for tag in exp_tags:\n        acs = experiment_data[tag][dataset_name][\"metrics\"].get(\"val_ACS\", [])\n        if acs:\n            plt.plot(range(1, len(acs) + 1), acs, marker=\"o\", label=tag)\n    plt.xlabel(\"Fine-tuning epoch\")\n    plt.ylabel(\"ACS\")\n    plt.title(f\"{dataset_name}: Validation Augmentation-Consistency Score\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dataset_name}_ACS_curve.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ACS curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# 4. Final ACS bar comparison --------------------------------\ntry:\n    plt.figure()\n    tags, finals = [], []\n    for tag in exp_tags:\n        acs = experiment_data[tag][dataset_name][\"metrics\"].get(\"val_ACS\", [])\n        if acs:\n            tags.append(tag)\n            finals.append(acs[-1])\n    if finals:\n        plt.bar(tags, finals, color=[\"steelblue\", \"orange\"])\n        plt.ylabel(\"ACS\")\n        plt.title(f\"{dataset_name}: Final Augmentation-Consistency Score\")\n        for i, v in enumerate(finals):\n            plt.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n        fname = os.path.join(working_dir, f\"{dataset_name}_ACS_final.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final ACS bar plot: {e}\")\n    plt.close()\n", null], "plot_plan": ["We load the saved experiment_data.npy and focus on the \u201cSPR\u201d entry that contains everything produced during training. Four simple visuals are generated: (1) contrastive-pretraining loss across epochs; (2) supervised fine-tuning curves showing train vs. validation loss; (3) augmentation-consistency score during fine-tuning; and (4) a confusion-matrix heat-map for the final validation predictions. Each plot is wrapped in its own try-except block, titled clearly, saved to the working directory with descriptive names, and the figure is closed afterwards. We never invent data\u2014if any list is empty we silently skip that figure. No more than four figures are produced, satisfying the \u22645 requirement. Finally, for completeness we print the arrays used so a downstream notebook can inspect exact values.", "We load the saved dictionary, focus on the \"remove_contrastive_pretrain \u2192 SPR\" entry, and pull out epochs, train/val losses, validation accuracy, and augmentation-consistency scores.  Three visualisations are generated: (1) train vs-validation loss curves over epochs, (2) accuracy and ACS curves over epochs, and (3) a confusion-matrix heat-map for the final predictions versus ground truth.  Each figure is wrapped in its own try-except so a failure in one plot does not halt the rest, and all figures are explicitly closed.  Descriptive titles include the dataset name (\u201cSPR\u201d) and the plot type, and file names mirror this convention inside working_dir.  Only data present in experiment_data.npy are used\u2014no synthetic values are invented.  Confusion matrix creation is skipped gracefully if predictions or labels are missing.  No more than the required three plots are produced, remaining well under the five-figure limit.  The code starts with the mandated imports and creates working_dir if necessary, ensuring everything is self-contained.", "We will load the saved experiment_data.npy, drill down to the \u201cno_aug_contrast / SPR\u201d branch, and pull out the lists recorded during training: contrastive losses, supervised train/val losses, ACS values, epoch indices, and the final predictions/ground-truth.  Four standard plots will be produced, each in its own try\u2013except block: (1) supervised train vs. val loss over epochs; (2) contrastive pre-training loss over epochs; (3) augmentation-consistency score (ACS) over epochs; and (4) a confusion-matrix heat-map for the final validation predictions.  All figures are saved into working_dir with descriptive names that mention the dataset (SPR) and plot type, each figure is explicitly closed whether or not an exception occurs, and no fabricated data are introduced.  Where an expected array is missing/empty, the corresponding plot is skipped gracefully.  Only basic matplotlib is used, no custom styling.  At most one confusion-matrix is plotted, so the five-figure limit is respected.  Finally, the code prints a short \u201cdone\u201d message and nothing else.", "We will first load the saved experiment_data dictionary, then extract the series stored under experiment_data['multi_syn_pretrain']['SPR']. Three simple visualizations are produced if the corresponding arrays exist: (1) contrastive-pretraining loss across epochs; (2) supervised fine-tuning losses (train vs. val) across epochs; (3) validation Augmentation Consistency Score (ACS) across epochs. Each plot is wrapped in an independent try-except so that one failing plot does not stop the rest. Every figure is given a clear title and saved inside working_dir with descriptive names that include the dataset and curve type. All figures are closed after saving to free resources. At most the first 5 epochs are shown for any very long run by subsampling with np.linspace. Finally, the script prints the absolute path of every saved figure for quick inspection.", "We will load the serialized experiment_data.npy, locate the run keyed by ['no_encoder_norm']['SPR'], and extract the stored arrays for contrastive pre-training loss, supervised train/val loss, validation ACS, and the dev-set predictions + ground-truth. Four plots will be produced, each in its own try-except block: (1) contrastive loss versus epoch, (2) supervised train/val loss versus epoch, (3) validation ACS versus epoch, and (4) a confusion-matrix heat-map (dev set).  The code first ensures the working directory exists, then iterates through the plotting tasks, always closing figures even on failure.  All filenames include the dataset name (SPR), the experiment tag (no_encoder_norm) and the plot type for clarity.  No synthetic data are generated; everything comes directly from the saved numpy dictionary.  Only basic matplotlib is used, and at most four figures are saved (well below the 5-figure limit).  The script ends by printing the paths of the saved plots so downstream steps can pick them up.", "We will first load the saved experiment_data.npy, pull out the SPR subsection, and then generate at most four standard figures: (1) contrastive-stage loss over epochs; (2) supervised train/validation loss over epochs; (3) validation accuracy over epochs; and (4) augmentation-consistency scores for train/val over epochs.  Every figure is wrapped in its own try-except so a failure in one plot does not stop the rest.  Each file name embeds the dataset and plot type for clarity, and figures are always closed.  If some key is missing, that plot is silently skipped.  After plotting we compute and print the final validation accuracy stored in the data so the user sees an evaluation number in the console.  The code obeys the provided style constraints (basic matplotlib, max five figures, etc.) and saves everything to the working directory.", "We load the saved experiment_data.npy, extract the \u201cRemoveRecurrent / SPR\u201d branch, and verify what arrays are present.  We then plot up to four standard diagnostics: (1) the contrastive-pretraining loss over epochs, (2) supervised training/validation loss curves, (3) augmentation-consistency score across epochs, and (4) a bar chart comparing the ground-truth versus predicted class counts for the last epoch.  Each visualization is wrapped in its own try-except block, given a descriptive title/subtitle that states the dataset (SPR) and the content of each subplot, then saved in the working directory with an informative filename.  All figures are closed no matter what, preventing matplotlib from exhausting resources.  After plotting, the script computes and prints the final validation accuracy and augmentation-consistency score so the user sees numeric metrics alongside the plots.  We rely exclusively on the data stored in experiment_data.npy\u2014no fabricated values\u2014ensuring reproducibility and compliance with the guidelines.  At most five figures are generated, satisfying the plotting limit.", "We load the saved experiment_data dict, loop over both experiment tags, and for each available metric we build simple matplotlib line plots. First, for every experiment we draw the contrastive-pretraining loss curve; next we overlay supervised train/val loss on the same axes for clarity. We then plot Augmentation-Consistency-Score (ACS) across epochs, again one line per experiment to ease comparison. To highlight the final behaviour we add a small bar chart comparing the last-epoch ACS of both settings. All plotting code is wrapped in individual try-except blocks, figures are titled with the experiment name plus a clear subtitle (e.g. \u201cSPR \u2013 Contrastive Loss\u201d), and each is saved into working_dir with descriptive filenames. We open and close a fresh figure inside each try-block to guarantee resources are freed. No fake data are generated; every curve is drawn only if the corresponding key exists in the numpy file. Finally, the script prints the paths of the saved figures so the user can locate them.", null], "ablation_name": [null, "Remove Contrastive Pre-training (Train Encoder From Scratch)", "Remove Data Augmentation in Contrastive Stage", "Multi-Synthetic-Dataset Pre-training", "No Encoder Normalization (Unnormalized Contrastive Representations)", "Freeze Pre-trained Encoder (Linear-Probe Fine-Tuning)", "Remove Recurrent Encoder (Mean-Pooling Encoder)", "One-Sided Augmentation in Contrastive Pre-training", "Non-Sharing Twin Encoders in Contrastive Pre-training"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["The script loads the saved NumPy dictionary, walks through each dataset, and\nprints human-readable summaries of the recorded statistics. For every list-\nvalued field it either selects the best value (minimum for any loss, maximum for\nany score/accuracy) or, when more sensible, the final value (e.g., the last\nsupervised training loss). All output is clearly labeled with both the dataset\nname and an explicit metric description, and the code runs immediately upon\nexecution.", "The script loads the stored numpy dictionary from the working directory,\niterates through every experiment and its contained datasets, and then inspects\nthe \u201cmetrics\u201d and \u201closses\u201d sub-dictionaries.   For each dataset it prints the\ndataset name first, followed by the best (max for scores, min for losses) value\nof every recorded metric with clear, explicit labels such as \u201cvalidation\naccuracy\u201d or \u201ctraining supervised loss.\u201d   No extra entry-point guard or\nplotting code is used, so the script runs immediately when executed.", "The script will locate the saved experiment_data.npy file inside the \u201cworking\u201d\ndirectory, load it into a Python dictionary, and iterate through every\nexperiment and dataset it contains. For each dataset it will pull the lists\nstored under the \u201cmetrics\u201d and \u201closses\u201d keys, take the last (i.e., final)\nelement from every list, and print those values using explicit, unambiguous\nnames such as \u201cContrastive pre-training loss\u201d or \u201cValidation augmentation\nconsistency score.\u201d If a particular list is missing or empty the script simply\nskips that line, so it works for any future experiments written in the same\nformat. Everything is executed immediately at import time\u2014no special entry point\nis required.", "The script loads the saved NumPy dictionary, iterates through each experiment\nand its contained datasets, and prints clearly-named final metrics (and\nvalidation accuracy calculated from stored predictions). It respects the\noriginal data hierarchy, chooses the last recorded value for time-series\nmetrics, and prints everything immediately without extra boilerplate.", "The script will load the saved numpy dictionary from the working directory,\ntraverse every experiment and dataset it contains, and retrieve each list of\nlogged numbers.   For score-type metrics (e.g., anything whose key contains\n\u201cACS\u201d or \u201cacc\u201d), the script prints the maximum non-None value, while for loss\nentries it reports the minimum value (best).   Train ACS may be missing (all\nNone), so such metrics are skipped.   Each dataset name is printed first,\nfollowed by clearly labelled metric lines that state both the metric\u2019s full name\nand its best value.", "The script will locate the saved NumPy file under the working directory, load it\ninto memory, and convert the object array back to a Python dict.   It then\niterates through every experiment branch and, within each branch, through every\ndataset (e.g., \u201cSPR\u201d).   For every dataset it searches the \u201cmetrics\u201d section,\ntaking the maximum value for score-type metrics (accuracy, ACS, etc.) and the\n\u201closses\u201d section, taking the minimum value for each loss curve.   All\ninformation is printed in a clear, human-readable form, always starting with the\ndataset name followed by explicit metric labels such as \u201cbest validation\naccuracy\u201d or \u201clowest contrastive loss.\u201d   Nothing is plotted and no `if __name__\n== \"__main__\":` guard is used, so the script runs immediately when executed.", "We simply load the saved numpy dictionary, walk through each recorded experiment\nhierarchy, grab the last element in every metric / loss list, and print it with\nan explicit, human-readable label. This respects the original nested structure\n(e.g. dataset \u2192 task \u2192 metrics / losses) while ensuring the output is self-\nexplanatory. No entry-point guard is used, so the script runs immediately. The\ncode below fulfils all formatting requirements and produces a concise textual\nsummary\u2014no figures or additional files are generated.", "We will load the saved experiment dictionary from working/experiment_data.npy,\niterate over each experimental run (e.g., \u2018baseline_aug_aug\u2019,\n\u2018one_sided_raw_aug\u2019) and then over each dataset inside it (only \u201cSPR\u201d in this\nexperiment).   For every dataset we extract the final entry of every tracked\nlist: contrastive loss, supervised\u2010train loss, supervised\u2010validation loss, and\nvalidation augmentation-consistency score.   We then print the dataset name\nfollowed by each metric with an explicit, self-explanatory label.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper functions --------------------------------------------------\ndef is_loss(metric_name: str) -> bool:\n    \"\"\"Heuristic to decide if a metric is a loss.\"\"\"\n    return \"loss\" in metric_name or metric_name in {\n        \"contrastive\",\n        \"train_sup\",\n        \"val_sup\",\n    }\n\n\ndef summarize_list(name: str, values):\n    \"\"\"\n    Return either the best (min for losses / max otherwise) or the\n    final value, depending on the metric type.\n    \"\"\"\n    if not values:  # empty list\n        return None\n\n    if name == \"train_sup\":  # want the last epoch's value\n        return values[-1]\n\n    # For the rest: choose best\n    if is_loss(name):\n        return min(values)\n    else:\n        return max(values)\n\n\n# ------------------------------------------------------------------\n# Print metrics -----------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---- Losses ---------------------------------------------------\n    for loss_name, loss_values in data.get(\"losses\", {}).items():\n        best_loss = summarize_list(loss_name, loss_values)\n        if best_loss is not None:\n            if loss_name == \"contrastive\":\n                label = \"best contrastive loss\"\n            elif loss_name == \"train_sup\":\n                label = \"final supervised training loss\"\n            elif loss_name == \"val_sup\":\n                label = \"best supervised validation loss\"\n            else:\n                label = f\"{loss_name} (best)\"\n            print(f\"{label}: {best_loss:.4f}\")\n\n    # ---- Metrics --------------------------------------------------\n    for metric_name, metric_values in data.get(\"metrics\", {}).items():\n        best_metric = summarize_list(metric_name, metric_values)\n        if best_metric is not None:\n            if metric_name == \"train_ACS\":\n                label = \"best training augmentation consistency score\"\n            elif metric_name == \"val_ACS\":\n                label = \"best validation augmentation consistency score\"\n            else:\n                label = f\"{metric_name} (best)\"\n            print(f\"{label}: {best_metric:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# locate and load experiment data\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# helper to print the best (min or max) value for a sequence\n# ------------------------------------------------------------\ndef best(values, mode=\"max\"):\n    if not values:  # empty list protection\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\n# ------------------------------------------------------------\n# iterate and report\n# ------------------------------------------------------------\nfor _, datasets in experiment_data.items():  # first level = experiment name\n    for dataset_name, content in datasets.items():  # second level = dataset name\n        print(dataset_name)  # dataset heading\n\n        # retrieve stored metrics and losses (may be absent)\n        metrics = content.get(\"metrics\", {})\n        losses = content.get(\"losses\", {})\n\n        # accuracy\n        if \"val_acc\" in metrics:\n            val_acc = best(metrics[\"val_acc\"], mode=\"max\")\n            if val_acc is not None:\n                print(f\"validation accuracy: {val_acc:.4f}\")\n\n        # augmentation consistency score\n        if \"val_ACS\" in metrics:\n            val_acs = best(metrics[\"val_ACS\"], mode=\"max\")\n            if val_acs is not None:\n                print(f\"validation augmentation consistency score: {val_acs:.4f}\")\n\n        # supervised losses\n        if \"train_sup\" in losses:\n            tr_loss = best(losses[\"train_sup\"], mode=\"min\")\n            if tr_loss is not None:\n                print(f\"training supervised loss: {tr_loss:.4f}\")\n\n        if \"val_sup\" in losses:\n            val_loss = best(losses[\"val_sup\"], mode=\"min\")\n            if val_loss is not None:\n                print(f\"validation supervised loss: {val_loss:.4f}\")\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the stored experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper: pretty-print the last element of a list, if it exists\n# ------------------------------------------------------------------\ndef print_last(label: str, values):\n    if isinstance(values, (list, tuple)) and values:\n        print(f\"{label}: {values[-1]:.4f}\")\n\n\n# ------------------------------------------------------------------\n# Traverse experiment \u2192 dataset \u2192 metrics/losses and report results\n# ------------------------------------------------------------------\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, record in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ----- Metrics -----\n        metrics = record.get(\"metrics\", {})\n        print_last(\"Training augmentation consistency score\", metrics.get(\"train_ACS\"))\n        print_last(\"Validation augmentation consistency score\", metrics.get(\"val_ACS\"))\n\n        # ----- Losses -----\n        losses = record.get(\"losses\", {})\n        print_last(\"Contrastive pre-training loss\", losses.get(\"contrastive\"))\n        print_last(\"Supervised training loss\", losses.get(\"train_sup\"))\n        print_last(\"Supervised validation loss\", losses.get(\"val_sup\"))\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# locate and load experiment data ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# helper to compute accuracy ---------------------------------\ndef accuracy(preds, gts):\n    if not preds or not gts:\n        return None\n    correct = sum(int(p == g) for p, g in zip(preds, gts))\n    return correct / len(gts)\n\n\n# ------------------------------------------------------------\n# iterate and report -----------------------------------------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(f\"\\nDataset: {exp_name} / {dataset_name}\")\n\n        # 1. Contrastive pre-training loss --------------------\n        contrastive_losses = content.get(\"losses\", {}).get(\"contrastive\", [])\n        if contrastive_losses:\n            print(f\"final contrastive loss: {contrastive_losses[-1]:.4f}\")\n\n        # 2. Supervised training & validation losses ----------\n        train_sup = content.get(\"losses\", {}).get(\"train_sup\", [])\n        if train_sup:\n            print(f\"final supervised training loss: {train_sup[-1]:.4f}\")\n\n        val_sup = content.get(\"losses\", {}).get(\"val_sup\", [])\n        if val_sup:\n            print(f\"final supervised validation loss: {val_sup[-1]:.4f}\")\n\n        # 3. Validation augmentation consistency (ACS) --------\n        acs_vals = content.get(\"metrics\", {}).get(\"val_ACS\", [])\n        if acs_vals:\n            best_acs = max(acs_vals)  # higher is better\n            print(f\"best validation augmentation consistency (ACS): {best_acs:.4f}\")\n\n        # 4. Validation accuracy ------------------------------\n        preds = content.get(\"predictions\", [])\n        gts = content.get(\"ground_truth\", [])\n        val_acc = accuracy(preds, gts)\n        if val_acc is not None:\n            print(f\"validation accuracy: {val_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# Locate and load the stored experiment dictionary\n# ------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------\ndef is_loss(name: str) -> bool:\n    \"\"\"Heuristic to decide whether a metric list represents a loss.\"\"\"\n    return \"loss\" in name.lower() or \"contrastive\" in name.lower()\n\n\ndef pretty_name(raw_key: str) -> str:\n    \"\"\"Convert raw dict keys to a clearer metric description.\"\"\"\n    mapping = {\n        \"train_ACS\": \"training augmentation consistency score\",\n        \"val_ACS\": \"validation augmentation consistency score\",\n        \"contrastive\": \"contrastive pre-training loss\",\n        \"train_sup\": \"training supervised loss\",\n        \"val_sup\": \"validation supervised loss\",\n    }\n    return mapping.get(raw_key, raw_key)\n\n\ndef best_value(values, loss_like: bool):\n    \"\"\"Return the best value according to the metric type.\"\"\"\n    # Filter out None placeholders\n    cleaned = [v for v in values if v is not None]\n    if not cleaned:\n        return None\n    return min(cleaned) if loss_like else max(cleaned)\n\n\n# ------------------------------------------------------------\n# Main parsing / printing loop\n# ------------------------------------------------------------\nfor exp_name, exp_content in experiment_data.items():\n    for dataset_name, ds_content in exp_content.items():\n        # Print dataset heading\n        print(dataset_name)\n        # Metrics section\n        for section in (\"metrics\", \"losses\"):\n            for key, val_list in ds_content[section].items():\n                value = best_value(val_list, loss_like=is_loss(key))\n                if value is None:\n                    continue  # nothing logged for this metric\n                metric_label = pretty_name(key)\n                print(f\"{metric_label}: {value:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper functions to decide whether a metric should be maximised or minimised\n# ------------------------------------------------------------------\ndef is_loss(name: str) -> bool:\n    \"\"\"Return True if the metric name looks like a loss.\"\"\"\n    return \"loss\" in name or name.endswith(\"_sup\") or name == \"contrastive\"\n\n\ndef best_value(values, maximise=True):\n    \"\"\"Return best (max or min) value from a list; if list is empty, return None.\"\"\"\n    if not values:\n        return None\n    return max(values) if maximise else min(values)\n\n\n# Pretty names for printing\nPRETTY = {\n    \"train_ACS\": \"best training augmentation consistency\",\n    \"val_ACS\": \"best validation augmentation consistency\",\n    \"val_acc\": \"best validation accuracy\",\n    \"contrastive\": \"lowest contrastive loss\",\n    \"train_sup\": \"lowest supervised training loss\",\n    \"val_sup\": \"lowest supervised validation loss\",\n}\n\n# ------------------------------------------------------------------\n# Iterate through experiments and datasets, printing requested stats\n# ------------------------------------------------------------------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # ---- Metrics (scores) ------------------------------------------------\n        for key, values in data.get(\"metrics\", {}).items():\n            if not values:  # skip empty lists\n                continue\n            maximise = not is_loss(key)\n            best = best_value(values, maximise=maximise)\n            pretty_name = PRETTY.get(key, f\"best {key}\")\n            print(f\"{pretty_name}: {best:.4f}\")\n\n        # ---- Losses ----------------------------------------------------------\n        for key, values in data.get(\"losses\", {}).items():\n            if not values:\n                continue\n            # All losses are minimised\n            best = best_value(values, maximise=False)\n            pretty_key = PRETTY.get(key, f\"lowest {key} loss\")\n            print(f\"{pretty_key}: {best:.4f}\")\n\n        # Extra blank line between datasets\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# Locate and load experiment file ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# Helper: safely fetch the last value in a list --------------\ndef last_or_none(lst):\n    return lst[-1] if isinstance(lst, (list, tuple)) and lst else None\n\n\n# Mapping from raw keys to clearer descriptions\nmetric_name_map = {\n    \"train_ACS\": \"final training ACS\",\n    \"val_ACS\": \"final validation ACS\",\n    \"contrastive\": \"final contrastive pre-training loss\",\n    \"train_sup\": \"final supervised training loss\",\n    \"val_sup\": \"final supervised validation loss\",\n}\n\n# ------------------------------------------------------------\n# Iterate and print metrics ----------------------------------\nfor dataset_name, task_dict in experiment_data.items():\n    for task_name, records in task_dict.items():\n        print(f\"\\nDataset: {dataset_name} | Task: {task_name}\")\n\n        # Metrics dictionary\n        for raw_key, values in records.get(\"metrics\", {}).items():\n            value = last_or_none(values)\n            if value is not None:\n                label = metric_name_map.get(raw_key, raw_key)\n                print(f\"{label}: {value:.4f}\")\n\n        # Losses dictionary\n        for raw_key, values in records.get(\"losses\", {}).items():\n            value = last_or_none(values)\n            if value is not None:\n                label = metric_name_map.get(raw_key, raw_key)\n                print(f\"{label}: {value:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the saved experiment data ------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper to fetch last (i.e., final) element safely ----------------\ndef last(lst):\n    return lst[-1] if lst else None\n\n\n# ------------------------------------------------------------------\n# traverse and display the requested statistics --------------------\nfor run_name, datasets in experiment_data.items():\n    print(f\"\\n=== Experiment: {run_name} ===\")\n    for dataset_name, content in datasets.items():\n        print(f\"{dataset_name}:\")\n        # final metrics\n        acs_final = last(content[\"metrics\"].get(\"val_ACS\", []))\n        contrastive_final = last(content[\"losses\"].get(\"contrastive\", []))\n        train_sup_final = last(content[\"losses\"].get(\"train_sup\", []))\n        val_sup_final = last(content[\"losses\"].get(\"val_sup\", []))\n\n        if contrastive_final is not None:\n            print(f\"  contrastive pretraining loss (final): {contrastive_final:.4f}\")\n        if train_sup_final is not None:\n            print(f\"  supervised training loss (final): {train_sup_final:.4f}\")\n        if val_sup_final is not None:\n            print(f\"  supervised validation loss (final): {val_sup_final:.4f}\")\n        if acs_final is not None:\n            print(\n                f\"  validation augmentation consistency score (final): {acs_final:.4f}\"\n            )\n", ""], "parse_term_out": ["['\\nDataset: SPR', '\\n', 'best contrastive loss: 3.9439', '\\n', 'final\nsupervised training loss: 0.7411', '\\n', 'best supervised validation loss:\n0.6301', '\\n', 'best validation augmentation consistency score: 0.7170', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR', '\\n', 'validation accuracy: 0.8540', '\\n', 'validation augmentation\nconsistency score: 0.6490', '\\n', 'training supervised loss: 0.5294', '\\n',\n'validation supervised loss: 0.4090', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR', '\\n', 'Validation augmentation consistency score: 0.5790',\n'\\n', 'Contrastive pre-training loss: 3.7895', '\\n', 'Supervised training loss:\n0.7484', '\\n', 'Supervised validation loss: 0.6541', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: multi_syn_pretrain / SPR', '\\n', 'final contrastive loss: 3.8489',\n'\\n', 'final supervised training loss: 0.7516', '\\n', 'final supervised\nvalidation loss: 0.6582', '\\n', 'best validation augmentation consistency (ACS):\n0.7040', '\\n', 'validation accuracy: 0.8330', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR', '\\n', 'validation augmentation consistency score: 0.7270', '\\n',\n'contrastive pre-training loss: 1.3766', '\\n', 'training supervised loss:\n1.1614', '\\n', 'validation supervised loss: 0.8986', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR', '\\n', 'best training augmentation consistency: 0.3157', '\\n',\n'best validation augmentation consistency: 0.3160', '\\n', 'best validation\naccuracy: 0.3700', '\\n', 'lowest contrastive loss: 3.9425', '\\n', 'lowest\nsupervised training loss: 1.3205', '\\n', 'lowest supervised validation loss:\n1.3127', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: RemoveRecurrent | Task: SPR', '\\n', 'final validation ACS: 0.3590',\n'\\n', 'final contrastive pre-training loss: 4.0650', '\\n', 'final supervised\ntraining loss: 1.3022', '\\n', 'final supervised validation loss: 1.2872', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\n=== Experiment: baseline_aug_aug ===', '\\n', 'SPR:', '\\n', '  contrastive\npretraining loss (final): 3.9228', '\\n', '  supervised training loss (final):\n0.7579', '\\n', '  supervised validation loss (final): 0.6440', '\\n', '\nvalidation augmentation consistency score (final): 0.6600', '\\n', '\\n===\nExperiment: one_sided_raw_aug ===', '\\n', 'SPR:', '\\n', '  contrastive\npretraining loss (final): 3.8682', '\\n', '  supervised training loss (final):\n0.7278', '\\n', '  supervised validation loss (final): 0.6268', '\\n', '\nvalidation augmentation consistency score (final): 0.6340', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
