{"edges": [[0, 4], [0, 3], [0, 7], [0, 5], [0, 6], [0, 8], [0, 1], [0, 2], [1, 9], [1, 10], [1, 11], [1, 12]], "layout": [[0.5882352941176471, 0.0], [0.17647058823529413, 0.5], [0.29411764705882354, 0.5], [0.4117647058823529, 0.5], [0.5294117647058824, 0.5], [0.6470588235294118, 0.5], [0.7647058823529411, 0.5], [0.8823529411764706, 0.5], [1.0, 0.5], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["The main bug is in the contrastive objective: the current positive\u2013pair index\nmapping is wrong (`targets = (idx + N//2) % N` pairs each view with an unrelated\nsample).   Fix: when we push two augmented views of every sequence in the order\n`[v1, v2, v1, v2, \u2026]`, the positive of index `i` is simply `i^1` (flip the\nleast\u2013significant bit: 0\u21941, 2\u21943 \u2026).  Re-computing the InfoNCE targets this way\nlets the encoder learn meaningful invariances, improving downstream performance.\nWe also scale the experiment a little (more epochs, larger hidden size) while\nstaying within the time budget.", "Ablation name: Multi-Synthetic Dataset Generalization Ablation. The script\nbuilds three independent synthetic datasets, each governed by a different\nlabeling rule (shape-vs-color variety, sequence-length parity, and majority-\nshape class). It constructs a shared vocabulary, contrastively pre-trains a\nsingle encoder on the union of the unlabeled sequences, then fine-tunes a fresh\nclassification head for each dataset individually while re-loading the same pre-\ntrained encoder weights. After every fine-tuning run the model is evaluated on\nits own test split and\u2014crucially\u2014on the other two datasets\u2019 test splits to\nmeasure cross-dataset transfer. All losses, accuracies, ACA scores, predictions\nand ground-truth labels are stored in the required experiment_data structure and\nfinally saved to experiment_data.npy.", "Ablation name: Frozen-Encoder Fine-Tuning Ablation. We keep the original\npipeline (contrastive pre-training + full fine-tuning) as the \u201cfull_tune\u201d\nbaseline, duplicate the pre-trained encoder weights, and launch a second\ntraining run where every encoder parameter has requires_grad=False.   Only the\nfinal linear layer is optimised in this \u201cfrozen_encoder\u201d ablation, letting us\nquantify the pure transferability of the contrastive features.   For both runs\nwe log losses, ACC, SWA, CWA and ACA on validation and test splits, store\npredictions/ground-truth, and save everything into the required\nexperiment_data.npy file.", "Ablation name: No-Pretraining Ablation. We remove the contrastive pre-training\nstage and initialize Encoder + Classifier from scratch. The model is then\ntrained end-to-end with the supervised split only, and we log standard accuracy\ntogether with robustness metrics (ACA, SWA, CWA). All losses, metrics,\npredictions and ground-truth labels are stored under the new ablation key\n\u201cno_pretraining\u201d and saved to experiment_data.npy for later analysis.", "Ablation name: No-Augmentation Pre-training Ablation. The ablation simply adds a\nglobal flag USE_AUG.   The original augmentation routine is kept as _augment,\nand the public augment() symbol is redirected to either _augment (USE_AUG=True)\nor a no-op lambda that returns the raw sequence (USE_AUG=False).   Because every\nplace in the pipeline (contrastive pre-training and ACA evaluation) calls\naugment(), flipping this flag cleanly disables all data augmentation while\nleaving the remainder of the training / evaluation code untouched.   All\nbookkeeping is stored in the required experiment_data structure and saved as\nexperiment_data.npy.", "Ablation name: Masking-Only Augmentation Ablation. We keep every component\nidentical to the baseline but replace the augmentation used during contrastive\npre-training and downstream evaluation with a pure \u201cmasking\u201d variant. The new\naugment_mask_only function randomly replaces 30 % of tokens by <mask> and never\nperforms local shuffling. We train the encoder contrastively with this single\ncorruption, fine-tune the classifier, collect the same losses/metrics, and store\neverything under the ablation key \"mask_only\" inside experiment_data, which is\nfinally saved to experiment_data.npy.", "Ablation name: No-Recurrent Encoder (Bag-of-Tokens) Ablation. The recurrent GRU\nin the original encoder is replaced with a Bag-of-Tokens (BoT) encoder that\nsimply embeds every token, masks out pads, averages the embeddings and feeds the\nmean vector straight to the classifier.  This variant keeps every other\ncomponent (data loading, augmentation, contrastive pre-training, fine-tuning,\nmetrics, persistence) identical, letting us attribute performance differences\npurely to the loss of sequential modeling.  All results are stored in the\nrequired experiment_data structure under the key \"NoRecurrentEncoder\" and\nwritten to experiment_data.npy.", "Ablation name: Shuffle-Only Augmentation Ablation. The implementation adds a\ndedicated shuffle-only augmentation function and uses it exclusively when\nforming both contrastive views during pre-training, while leaving the mixed\naugmentation for robustness evaluation (ACA).  All bookkeeping is stored under\nthe \u201cshuffle_only\u201d ablation key and saved to `experiment_data.npy`.  The rest of\nthe training / evaluation pipeline is unchanged from the baseline.", "Ablation name: Masked-Language-Modeling (MLM) Pre-training Ablation. The\nsolution replaces the contrastive pre-training objective with a BERT-style\nmasked-language-modeling (MLM) objective while leaving every other component\nuntouched.   During pre-training, each sequence is encoded once, 15 % of its\ntokens are masked (80 % <mask>, 10 % random, 10 % unchanged) and the model is\ntrained to predict those masked tokens via a simple linear MLM head on top of\nthe same GRU encoder.   After MLM pre-training the encoder is fine-tuned on the\ndownstream classification task exactly as in the baseline; evaluation,\nrobustness (ACA), and logging are unchanged. All plottable data are stored in\n`experiment_data.npy`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers from baseline\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Corrected contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n        \"test\": {},\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, copy, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Helper metrics\ndef count_shape_variety(sequence):  # first char of token\n    return len(set(tok[0] for tok in sequence.strip().split()))\n\n\ndef count_color_variety(sequence):  # second char of token\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# -----------------------------------------------------------------------------#\n# Synthetic dataset generators for three different rules\nshapes, colors = list(\"ABCDE\"), list(\"12345\")\n\n\ndef gen_sequence():\n    L = random.randint(3, 10)\n    return \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n\n\ndef label_rule1(seq):  # shape-vs-color variety\n    return int(count_shape_variety(seq) >= count_color_variety(seq))\n\n\ndef label_rule2(seq):  # sequence length parity (even=1, odd=0)\n    return int(len(seq.split()) % 2 == 0)\n\n\ndef label_rule3(seq):  # majority shape class exists\n    toks = [t[0] for t in seq.split()]\n    most = max(set(toks), key=toks.count)\n    return int(toks.count(most) > len(toks) / 2)\n\n\nRULES = {\n    \"variety\": label_rule1,\n    \"length_parity\": label_rule2,\n    \"majority_shape\": label_rule3,\n}\n\n\ndef make_dataset(n, rule_func):\n    seqs, labels = [], []\n    for _ in range(n):\n        s = gen_sequence()\n        seqs.append(s)\n        labels.append(rule_func(s))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nsplits = {}\nfor name, rule in RULES.items():\n    splits[name] = {\n        \"train\": make_dataset(6000, rule),\n        \"dev\": make_dataset(1200, rule),\n        \"test\": make_dataset(1200, rule),\n    }\n\n# -----------------------------------------------------------------------------#\n# Shared vocabulary (union of all training sequences)\nall_tokens = set()\nfor d in splits.values():\n    for s in d[\"train\"][\"sequence\"]:\n        all_tokens.update(s.split())\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = [\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n]\nmax_len = (\n    max(len(s.split()) for d in splits.values() for s in d[\"train\"][\"sequence\"]) + 1\n)\n\n\ndef encode(seq):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Dataset wrappers\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:\n        win = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - win)\n        seg = toks[i : i + win]\n        random.shuffle(seg)\n        toks[i : i + win] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=layers, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return h[-1]  # (B,hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = z @ z.T / temp\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos_idx)\n\n\n# -----------------------------------------------------------------------------#\n# Experiment data structure\nexperiment_data = {\n    \"multi_synth_generalization\": {\n        name: {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for name in RULES\n    }\n}\nexperiment_data[\"multi_synth_generalization\"][\"transfer\"] = {}\n\n# -----------------------------------------------------------------------------#\n# Pre-training on union of unlabeled sequences\npretrain_seqs = []\nfor d in splits.values():\n    pretrain_seqs.extend(d[\"train\"][\"sequence\"])\npre_loader = DataLoader(\n    SeqOnlyDS(pretrain_seqs), batch_size=256, shuffle=True, drop_last=True\n)\n\nencoder = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(encoder.parameters(), lr=1e-3)\nprint(\"\\n--- Contrastive pre-training ---\")\nfor ep in range(1, 9):\n    encoder.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch in pre_loader:\n        views = []\n        for s in batch:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(encoder(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"Epoch {ep}: loss={tot/cnt:.4f} ({time.time()-t0:.1f}s)\")\npretrain_state = copy.deepcopy(encoder.state_dict())\n\n# -----------------------------------------------------------------------------#\n# Helper functions\ncrit = nn.CrossEntropyLoss()\n\n\ndef eval_model(model, loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = float(np.mean(np.array(preds) == np.array(gts)))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(model, seqs, labels, M=3):\n    correct = total = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            xs = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(xs).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            total += len(variants)\n    return correct / total\n\n\n# -----------------------------------------------------------------------------#\n# Fine-tune on each dataset and evaluate cross-transfer\nf_epochs = 10\nfor name in RULES:\n    print(f\"\\n=== Fine-tuning on dataset: {name} ===\")\n    enc_i = Encoder(len(vocab)).to(device)\n    enc_i.load_state_dict(pretrain_state)\n    model = Classifier(enc_i, hid=256, n_cls=2).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    train_loader = DataLoader(\n        LabeledDS(splits[name][\"train\"][\"sequence\"], splits[name][\"train\"][\"label\"]),\n        batch_size=128,\n        shuffle=True,\n    )\n    dev_loader = DataLoader(\n        LabeledDS(splits[name][\"dev\"][\"sequence\"], splits[name][\"dev\"][\"label\"]),\n        batch_size=256,\n    )\n    for ep in range(1, f_epochs + 1):\n        model.train()\n        tot = n = 0\n        for seqs, labels in train_loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            opt.zero_grad()\n            loss = crit(model(x), y)\n            loss.backward()\n            opt.step()\n            tot += loss.item() * y.size(0)\n            n += y.size(0)\n        train_loss = tot / n\n        val_loss, val_acc, _, _, _ = eval_model(model, dev_loader)\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"train\"].append(\n            train_loss\n        )  # storing loss as proxy\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"val\"].append(\n            val_acc\n        )\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # Evaluation on own test set\n    test_loader = DataLoader(\n        LabeledDS(splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]),\n        batch_size=256,\n    )\n    tl, ta, tp, tg, tr = eval_model(model, test_loader)\n    aca = aca_metric(\n        model, splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]\n    )\n    experiment_data[\"multi_synth_generalization\"][name][\"predictions\"] = tp\n    experiment_data[\"multi_synth_generalization\"][name][\"ground_truth\"] = tg\n    print(f\"Test on {name}: loss={tl:.4f} | acc={ta:.4f} | ACA={aca:.4f}\")\n\n    # Cross-dataset transfer evaluation\n    for other in RULES:\n        if other == name:\n            continue\n        other_loader = DataLoader(\n            LabeledDS(\n                splits[other][\"test\"][\"sequence\"], splits[other][\"test\"][\"label\"]\n            ),\n            batch_size=256,\n        )\n        _, acc, pred, gt, _ = eval_model(model, other_loader)\n        key = f\"{name}_to_{other}\"\n        experiment_data[\"multi_synth_generalization\"][\"transfer\"][key] = {\n            \"acc\": acc,\n            \"predictions\": pred,\n            \"ground_truth\": gt,\n        }\n        print(f\"  Transfer {name}->{other}: acc={acc:.4f}\")\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to working/experiment_data.npy\")\n", "import os, random, pathlib, math, time, copy, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data, dev_data, test_data = (\n        make_synth(6000),\n        make_synth(1200),\n        make_synth(1200),\n    )\n\n# -----------------------------------------------------------------------------#\n# Vocabulary\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = [\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n]\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = z @ z.T / temp\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos_idx)\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"full_tune\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    },\n    \"frozen_encoder\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    },\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"full_tune\"][\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss={avg:.4f} ({time.time()-t0:.1f}s)\")\n\n# Save a snapshot of the encoder just after pre-training\nenc_pretrained_state = copy.deepcopy(enc.state_dict())\n\n# -----------------------------------------------------------------------------#\n# Helper functions\ncrit = nn.CrossEntropyLoss()\n\n\ndef eval_model(model, loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(model, seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\n# -----------------------------------------------------------------------------#\n# ---------------- Full fine-tuning (baseline) --------------------------------#\nnum_cls = len(set(train_data[\"label\"]))\nenc_full = Encoder(len(vocab)).to(device)\nenc_full.load_state_dict(enc_pretrained_state)\nmodel_full = Classifier(enc_full, hid=256, n_cls=num_cls).to(device)\nopt_full = torch.optim.Adam(model_full.parameters(), lr=1e-3)\nf_epochs = 10\nprint(\"\\n--- Fine-tuning (full_tune) ---\")\nfor ep in range(1, f_epochs + 1):\n    model_full.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_full.zero_grad()\n        loss = crit(model_full(x), y)\n        loss.backward()\n        opt_full.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(model_full, dev_loader)\n    val_aca = aca_metric(model_full, dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"full_tune\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"full_tune\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"full_tune\"][\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"full_tune\"][\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# Test baseline\ntl, ta, tp, tg, tr = eval_model(model_full, test_loader)\nswa = shape_weighted_accuracy(tr, tg, tp)\ncwa = color_weighted_accuracy(tr, tg, tp)\naca = aca_metric(model_full, test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"full_tune\"][\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": tl,\n    \"acc\": ta,\n    \"swa\": swa,\n    \"cwa\": cwa,\n    \"aca\": aca,\n    \"predictions\": tp,\n    \"ground_truth\": tg,\n}\nprint(\"\\n--- Test results (full_tune) ---\")\nprint(f\"Loss={tl:.4f} | Acc={ta:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | ACA={aca:.4f}\")\n\n# -----------------------------------------------------------------------------#\n# --------------- Frozen-encoder fine-tuning (ablation) -----------------------#\nenc_frozen = Encoder(len(vocab)).to(device)\nenc_frozen.load_state_dict(enc_pretrained_state)\nfor p in enc_frozen.parameters():\n    p.requires_grad = False  # freeze\nmodel_frozen = Classifier(enc_frozen, hid=256, n_cls=num_cls).to(device)\nopt_frozen = torch.optim.Adam(\n    model_frozen.fc.parameters(), lr=1e-3\n)  # only classifier params\nprint(\"\\n--- Fine-tuning (frozen_encoder) ---\")\nfor ep in range(1, f_epochs + 1):\n    model_frozen.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_frozen.zero_grad()\n        loss = crit(model_frozen(x), y)\n        loss.backward()\n        opt_frozen.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(model_frozen, dev_loader)\n    val_aca = aca_metric(model_frozen, dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"frozen_encoder\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"frozen_encoder\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"frozen_encoder\"][\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"frozen_encoder\"][\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# Test ablation\ntl, ta, tp, tg, tr = eval_model(model_frozen, test_loader)\nswa = shape_weighted_accuracy(tr, tg, tp)\ncwa = color_weighted_accuracy(tr, tg, tp)\naca = aca_metric(model_frozen, test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"frozen_encoder\"][\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": tl,\n    \"acc\": ta,\n    \"swa\": swa,\n    \"cwa\": cwa,\n    \"aca\": aca,\n    \"predictions\": tp,\n    \"ground_truth\": tg,\n}\nprint(\"\\n--- Test results (frozen_encoder) ---\")\nprint(f\"Loss={tl:.4f} | Acc={ta:.4f} | SWA={swa:.4f} | CWA={cwa:.4f} | ACA={aca:.4f}\")\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    \"\\nAll experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\")\n)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # +<cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation (for robustness evaluation only)\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return h[-1]  # (B,hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, enc, hid, n_cls):\n        super().__init__()\n        self.enc = enc\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\nbatch_train = 128\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]),\n    batch_size=batch_train,\n    shuffle=True,\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"no_pretraining\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"val_aca\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"test\": {},\n        }\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Training from scratch (No-Pretraining Ablation)\nenc = Encoder(len(vocab)).to(device)\nmodel = Classifier(enc, hid=256, n_cls=len(set(train_data[\"label\"]))).to(device)\ncrit = nn.CrossEntropyLoss()\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\nepochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = float(np.mean(np.array(preds) == np.array(gts)))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    model.eval()\n    correct = tot = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            xs = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(xs).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            tot += len(variants)\n    return correct / tot\n\n\nprint(\"\\n--- Supervised training (No-Pretraining) ---\")\nfor ep in range(1, epochs + 1):\n    # train\n    model.train()\n    tot_loss = n_items = 0\n    correct = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt.zero_grad()\n        logits = model(x)\n        loss = crit(logits, y)\n        loss.backward()\n        opt.step()\n        tot_loss += loss.item() * y.size(0)\n        n_items += y.size(0)\n        correct += (logits.argmax(1) == y).sum().item()\n    train_loss = tot_loss / n_items\n    train_acc = correct / n_items\n\n    # validation\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n\n    # log\n    ed = experiment_data[\"no_pretraining\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"val_aca\"].append(val_aca)\n\n    print(\n        f\"Epoch {ep}: train_acc={train_acc:.4f} | val_acc={val_acc:.4f} | \"\n        f\"ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\n\nexperiment_data[\"no_pretraining\"][\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"no_pretraining\"][\"SPR_BENCH\"][\"ground_truth\"] = test_gt\nexperiment_data[\"no_pretraining\"][\"SPR_BENCH\"][\"test\"] = dict(\n    loss=test_loss, acc=test_acc, swa=test_swa, cwa=test_cwa, aca=test_aca\n)\n\nprint(\"\\n--- Test results (No-Pretraining) ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------------- Ablation flag ---------------------------------------#\nUSE_AUG = False  # set to True to run the original augmented pipeline\n\n# -------------------- Boiler-plate & reproducibility -------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# ----------------------------- Metric helpers --------------------------------#\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# ----------------------------- Data loading ----------------------------------#\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# ------------------------------ Vocabulary -----------------------------------#\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# ------------------------------ Datasets -------------------------------------#\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# ----------------------------- Augmentation ----------------------------------#\ndef _augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\naugment = _augment if USE_AUG else (lambda s: s)\n\n\n# ------------------------------- Model ---------------------------------------#\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# ------------------------ Corrected contrastive loss -------------------------#\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# ------------------------------ DataLoaders ----------------------------------#\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# ------------------------- Experiment bookkeeping ----------------------------#\nexperiment_data = {\n    \"no_aug_pretraining\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    }\n}\n\n# ------------------------------ Pre-training ---------------------------------#\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"no_aug_pretraining\"][\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# ------------------------------ Fine-tuning ----------------------------------#\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    exp = experiment_data[\"no_aug_pretraining\"][\"SPR_BENCH\"]\n    exp[\"losses\"][\"train\"].append(train_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp[\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# ---------------------------- Test evaluation --------------------------------#\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"no_aug_pretraining\"][\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"ground_truth\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# ----------------------------- Save artifacts --------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers\ndef count_shape_variety(seq):\n    return len(set(t[0] for t in seq.split()))\n\n\ndef count_color_variety(seq):\n    return len(set(t[1] for t in seq.split()))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH (real or synthetic)\ndef load_real_spr(path):\n    try:\n        from datasets import load_dataset\n\n        def _ld(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _ld(\"train.csv\"),\n            \"dev\": _ld(\"dev.csv\"),\n            \"test\": _ld(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data, dev_data, test_data = (\n        make_synth(6000),\n        make_synth(1200),\n        make_synth(1200),\n    )\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = [\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n]\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1\n\n\ndef encode(seq):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(torch.utils.data.Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(torch.utils.data.Dataset):\n    def __init__(self, seqs, lbl):\n        self.seqs, self.lbl = seqs, lbl\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.lbl[i]\n\n\n# -----------------------------------------------------------------------------#\n# Masking-only augmentation\ndef augment_mask_only(seq: str):\n    toks = [t if random.random() > 0.3 else \"<mask>\" for t in seq.split()]\n    return \" \".join(toks)\n\n\naugment = augment_mask_only  # enforce masking-only policy\n\n\n# -----------------------------------------------------------------------------#\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, emb=128, hid=256, layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=layers, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return h[-1]\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = z @ z.T / temp\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos)\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"mask_only\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    }\n}\nrec = experiment_data[\"mask_only\"][\"SPR_BENCH\"]\n\n# -----------------------------------------------------------------------------#\n# Contrastive pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training (masking only) ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch in pretrain_loader:\n        views = []\n        for s in batch:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    rec[\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: loss={avg:.4f} ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nmodel = Classifier(enc, hid=256, n_cls=len(set(train_data[\"label\"]))).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_loader(loader):\n    model.eval()\n    loss_tot = n = 0\n    preds = gts = raws = []\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, lbl in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(lbl, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(lbl)\n            raws.extend(seqs)\n    return loss_tot / n, np.mean(np.array(preds) == np.array(gts)), preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    model.eval()\n    correct = total = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            x = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(x).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, lbl in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(lbl, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_loader(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    rec[\"losses\"][\"train\"].append(train_loss)\n    rec[\"losses\"][\"val\"].append(val_loss)\n    rec[\"metrics\"][\"val_acc\"].append(val_acc)\n    rec[\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Ep {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_loader(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nrec[\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"ground_truth\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers (unchanged)\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Bag-of-Tokens Encoder (No recurrent module)\nclass BoTEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n\n    def forward(self, x):  # x: (B, T)\n        m = (x != pad_id).unsqueeze(-1)  # mask pads\n        e = self.emb(x) * m  # (B, T, D)\n        sum_e = e.sum(1)\n        cnt = m.sum(1).clamp(min=1)\n        return sum_e / cnt  # (B, D)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid_dim, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Contrastive loss (unchanged)\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos_idx)\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment data container\nexperiment_data = {\n    \"NoRecurrentEncoder\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = BoTEncoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training (BoT) ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"NoRecurrentEncoder\"][\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nhid_dim = 128  # same as embedding size\nmodel = Classifier(enc, hid_dim, num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss_tot += crit(logits, y).item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    model.eval()\n    correct = total = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            xs = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(xs).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    exp = experiment_data[\"NoRecurrentEncoder\"][\"SPR_BENCH\"]\n    exp[\"losses\"][\"train\"].append(train_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp[\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\n\nexperiment_data[\"NoRecurrentEncoder\"][\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results (BoT) ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping container\nexperiment_data = {\n    \"shuffle_only\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    }\n}\nexp_ref = experiment_data[\"shuffle_only\"][\"SPR_BENCH\"]\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Augmentations\ndef local_shuffle(seq: str) -> str:\n    toks = seq.strip().split()\n    if len(toks) <= 1:\n        return seq\n    window = max(1, len(toks) // 4)\n    i = random.randint(0, len(toks) - window)\n    seg = toks[i : i + window]\n    random.shuffle(seg)\n    toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# mixed augment (for ACA evaluation, unchanged from baseline)\ndef augment(seq: str) -> str:\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Pre-training with SHUFFLE-ONLY augmentation\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training (Shuffle-Only) ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            # both views use only local shuffle\n            views.append(encode(local_shuffle(s)))\n            views.append(encode(local_shuffle(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    exp_ref[\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    exp_ref[\"losses\"][\"train\"].append(train_loss)\n    exp_ref[\"losses\"][\"val\"].append(val_loss)\n    exp_ref[\"metrics\"][\"val_acc\"].append(val_acc)\n    exp_ref[\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexp_ref[\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # +<cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation (same as baseline)\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, n_layers, batch_first=True)\n\n    def forward(self, x, seq_output=False):\n        z, h = self.gru(self.emb(x))\n        return z if seq_output else h[-1]  # (B,L,H) or (B,H)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x, seq_output=False))\n\n\n# -----------------------------------------------------------------------------#\n# Helper: create MLM masks\ndef mask_ids(batch_ids, prob=0.15):\n    \"\"\"\n    batch_ids: (B,L) tensor, returns input_ids, labels\n    labels == -100 where no prediction\n    \"\"\"\n    input_ids = batch_ids.clone()\n    labels = batch_ids.clone()\n    # mask selection\n    mask = (\n        (torch.rand(batch_ids.shape, device=batch_ids.device) < prob)\n        & (batch_ids != pad_id)\n        & (batch_ids != cls_id)\n    )\n    labels[~mask] = -100\n    # 80% -> <mask>\n    mask_token_mask = mask & (\n        torch.rand(batch_ids.shape, device=batch_ids.device) < 0.8\n    )\n    input_ids[mask_token_mask] = mask_id\n    # 10% -> random\n    rand_token_mask = (\n        mask\n        & (~mask_token_mask)\n        & (torch.rand(batch_ids.shape, device=batch_ids.device) < 0.5)\n    )\n    random_tokens = torch.randint(\n        4, len(vocab), size=(rand_token_mask.sum(),), device=device\n    )\n    input_ids[rand_token_mask] = random_tokens\n    # 10% keep unchanged (already in input_ids)\n    return input_ids, labels\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"MLM_pretrain\": {\n        \"SPR_BENCH\": {\n            \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n            \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n            \"test\": {},\n        }\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# MLM Pre-training\nenc = Encoder(len(vocab)).to(device)\nmlm_head = nn.Linear(256, len(vocab)).to(device)\nopt_enc = torch.optim.Adam(\n    list(enc.parameters()) + list(mlm_head.parameters()), lr=1e-3\n)\nmlm_crit = nn.CrossEntropyLoss(ignore_index=-100)\npre_epochs = 8\nprint(\"\\n--- MLM Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    mlm_head.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        batch_ids = torch.stack([encode(s) for s in batch_seqs]).to(device)\n        inp, tgt = mask_ids(batch_ids)\n        opt_enc.zero_grad()\n        logits = mlm_head(enc(inp, seq_output=True))  # (B,L,V)\n        loss = mlm_crit(logits.view(-1, len(vocab)), tgt.view(-1))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"MLM_pretrain\"][\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: mlm_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit_cls = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit_cls(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit_cls(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    ed = experiment_data[\"MLM_pretrain\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"MLM_pretrain\"][\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nExperiment data saved.\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, math, time, copy, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Helper metrics\ndef count_shape_variety(sequence):  # first char of token\n    return len(set(tok[0] for tok in sequence.strip().split()))\n\n\ndef count_color_variety(sequence):  # second char of token\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# -----------------------------------------------------------------------------#\n# Synthetic dataset generators for three different rules\nshapes, colors = list(\"ABCDE\"), list(\"12345\")\n\n\ndef gen_sequence():\n    L = random.randint(3, 10)\n    return \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n\n\ndef label_rule1(seq):  # shape-vs-color variety\n    return int(count_shape_variety(seq) >= count_color_variety(seq))\n\n\ndef label_rule2(seq):  # sequence length parity (even=1, odd=0)\n    return int(len(seq.split()) % 2 == 0)\n\n\ndef label_rule3(seq):  # majority shape class exists\n    toks = [t[0] for t in seq.split()]\n    most = max(set(toks), key=toks.count)\n    return int(toks.count(most) > len(toks) / 2)\n\n\nRULES = {\n    \"variety\": label_rule1,\n    \"length_parity\": label_rule2,\n    \"majority_shape\": label_rule3,\n}\n\n\ndef make_dataset(n, rule_func):\n    seqs, labels = [], []\n    for _ in range(n):\n        s = gen_sequence()\n        seqs.append(s)\n        labels.append(rule_func(s))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nsplits = {}\nfor name, rule in RULES.items():\n    splits[name] = {\n        \"train\": make_dataset(6000, rule),\n        \"dev\": make_dataset(1200, rule),\n        \"test\": make_dataset(1200, rule),\n    }\n\n# -----------------------------------------------------------------------------#\n# Shared vocabulary (union of all training sequences)\nall_tokens = set()\nfor d in splits.values():\n    for s in d[\"train\"][\"sequence\"]:\n        all_tokens.update(s.split())\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = [\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n]\nmax_len = (\n    max(len(s.split()) for d in splits.values() for s in d[\"train\"][\"sequence\"]) + 1\n)\n\n\ndef encode(seq):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Dataset wrappers\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:\n        win = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - win)\n        seg = toks[i : i + win]\n        random.shuffle(seg)\n        toks[i : i + win] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=layers, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return h[-1]  # (B,hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = z @ z.T / temp\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos_idx)\n\n\n# -----------------------------------------------------------------------------#\n# Experiment data structure\nexperiment_data = {\n    \"multi_synth_generalization\": {\n        name: {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for name in RULES\n    }\n}\nexperiment_data[\"multi_synth_generalization\"][\"transfer\"] = {}\n\n# -----------------------------------------------------------------------------#\n# Pre-training on union of unlabeled sequences\npretrain_seqs = []\nfor d in splits.values():\n    pretrain_seqs.extend(d[\"train\"][\"sequence\"])\npre_loader = DataLoader(\n    SeqOnlyDS(pretrain_seqs), batch_size=256, shuffle=True, drop_last=True\n)\n\nencoder = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(encoder.parameters(), lr=1e-3)\nprint(\"\\n--- Contrastive pre-training ---\")\nfor ep in range(1, 9):\n    encoder.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch in pre_loader:\n        views = []\n        for s in batch:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(encoder(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"Epoch {ep}: loss={tot/cnt:.4f} ({time.time()-t0:.1f}s)\")\npretrain_state = copy.deepcopy(encoder.state_dict())\n\n# -----------------------------------------------------------------------------#\n# Helper functions\ncrit = nn.CrossEntropyLoss()\n\n\ndef eval_model(model, loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = float(np.mean(np.array(preds) == np.array(gts)))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(model, seqs, labels, M=3):\n    correct = total = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            xs = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(xs).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            total += len(variants)\n    return correct / total\n\n\n# -----------------------------------------------------------------------------#\n# Fine-tune on each dataset and evaluate cross-transfer\nf_epochs = 10\nfor name in RULES:\n    print(f\"\\n=== Fine-tuning on dataset: {name} ===\")\n    enc_i = Encoder(len(vocab)).to(device)\n    enc_i.load_state_dict(pretrain_state)\n    model = Classifier(enc_i, hid=256, n_cls=2).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    train_loader = DataLoader(\n        LabeledDS(splits[name][\"train\"][\"sequence\"], splits[name][\"train\"][\"label\"]),\n        batch_size=128,\n        shuffle=True,\n    )\n    dev_loader = DataLoader(\n        LabeledDS(splits[name][\"dev\"][\"sequence\"], splits[name][\"dev\"][\"label\"]),\n        batch_size=256,\n    )\n    for ep in range(1, f_epochs + 1):\n        model.train()\n        tot = n = 0\n        for seqs, labels in train_loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            opt.zero_grad()\n            loss = crit(model(x), y)\n            loss.backward()\n            opt.step()\n            tot += loss.item() * y.size(0)\n            n += y.size(0)\n        train_loss = tot / n\n        val_loss, val_acc, _, _, _ = eval_model(model, dev_loader)\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"train\"].append(\n            train_loss\n        )  # storing loss as proxy\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"val\"].append(\n            val_acc\n        )\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # Evaluation on own test set\n    test_loader = DataLoader(\n        LabeledDS(splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]),\n        batch_size=256,\n    )\n    tl, ta, tp, tg, tr = eval_model(model, test_loader)\n    aca = aca_metric(\n        model, splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]\n    )\n    experiment_data[\"multi_synth_generalization\"][name][\"predictions\"] = tp\n    experiment_data[\"multi_synth_generalization\"][name][\"ground_truth\"] = tg\n    print(f\"Test on {name}: loss={tl:.4f} | acc={ta:.4f} | ACA={aca:.4f}\")\n\n    # Cross-dataset transfer evaluation\n    for other in RULES:\n        if other == name:\n            continue\n        other_loader = DataLoader(\n            LabeledDS(\n                splits[other][\"test\"][\"sequence\"], splits[other][\"test\"][\"label\"]\n            ),\n            batch_size=256,\n        )\n        _, acc, pred, gt, _ = eval_model(model, other_loader)\n        key = f\"{name}_to_{other}\"\n        experiment_data[\"multi_synth_generalization\"][\"transfer\"][key] = {\n            \"acc\": acc,\n            \"predictions\": pred,\n            \"ground_truth\": gt,\n        }\n        print(f\"  Transfer {name}->{other}: acc={acc:.4f}\")\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to working/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, math, time, copy, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Helper metrics\ndef count_shape_variety(sequence):  # first char of token\n    return len(set(tok[0] for tok in sequence.strip().split()))\n\n\ndef count_color_variety(sequence):  # second char of token\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# -----------------------------------------------------------------------------#\n# Synthetic dataset generators for three different rules\nshapes, colors = list(\"ABCDE\"), list(\"12345\")\n\n\ndef gen_sequence():\n    L = random.randint(3, 10)\n    return \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n\n\ndef label_rule1(seq):  # shape-vs-color variety\n    return int(count_shape_variety(seq) >= count_color_variety(seq))\n\n\ndef label_rule2(seq):  # sequence length parity (even=1, odd=0)\n    return int(len(seq.split()) % 2 == 0)\n\n\ndef label_rule3(seq):  # majority shape class exists\n    toks = [t[0] for t in seq.split()]\n    most = max(set(toks), key=toks.count)\n    return int(toks.count(most) > len(toks) / 2)\n\n\nRULES = {\n    \"variety\": label_rule1,\n    \"length_parity\": label_rule2,\n    \"majority_shape\": label_rule3,\n}\n\n\ndef make_dataset(n, rule_func):\n    seqs, labels = [], []\n    for _ in range(n):\n        s = gen_sequence()\n        seqs.append(s)\n        labels.append(rule_func(s))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nsplits = {}\nfor name, rule in RULES.items():\n    splits[name] = {\n        \"train\": make_dataset(6000, rule),\n        \"dev\": make_dataset(1200, rule),\n        \"test\": make_dataset(1200, rule),\n    }\n\n# -----------------------------------------------------------------------------#\n# Shared vocabulary (union of all training sequences)\nall_tokens = set()\nfor d in splits.values():\n    for s in d[\"train\"][\"sequence\"]:\n        all_tokens.update(s.split())\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = [\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n]\nmax_len = (\n    max(len(s.split()) for d in splits.values() for s in d[\"train\"][\"sequence\"]) + 1\n)\n\n\ndef encode(seq):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Dataset wrappers\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:\n        win = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - win)\n        seg = toks[i : i + win]\n        random.shuffle(seg)\n        toks[i : i + win] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=layers, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return h[-1]  # (B,hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = z @ z.T / temp\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos_idx)\n\n\n# -----------------------------------------------------------------------------#\n# Experiment data structure\nexperiment_data = {\n    \"multi_synth_generalization\": {\n        name: {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for name in RULES\n    }\n}\nexperiment_data[\"multi_synth_generalization\"][\"transfer\"] = {}\n\n# -----------------------------------------------------------------------------#\n# Pre-training on union of unlabeled sequences\npretrain_seqs = []\nfor d in splits.values():\n    pretrain_seqs.extend(d[\"train\"][\"sequence\"])\npre_loader = DataLoader(\n    SeqOnlyDS(pretrain_seqs), batch_size=256, shuffle=True, drop_last=True\n)\n\nencoder = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(encoder.parameters(), lr=1e-3)\nprint(\"\\n--- Contrastive pre-training ---\")\nfor ep in range(1, 9):\n    encoder.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch in pre_loader:\n        views = []\n        for s in batch:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(encoder(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"Epoch {ep}: loss={tot/cnt:.4f} ({time.time()-t0:.1f}s)\")\npretrain_state = copy.deepcopy(encoder.state_dict())\n\n# -----------------------------------------------------------------------------#\n# Helper functions\ncrit = nn.CrossEntropyLoss()\n\n\ndef eval_model(model, loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = float(np.mean(np.array(preds) == np.array(gts)))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(model, seqs, labels, M=3):\n    correct = total = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            xs = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(xs).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            total += len(variants)\n    return correct / total\n\n\n# -----------------------------------------------------------------------------#\n# Fine-tune on each dataset and evaluate cross-transfer\nf_epochs = 10\nfor name in RULES:\n    print(f\"\\n=== Fine-tuning on dataset: {name} ===\")\n    enc_i = Encoder(len(vocab)).to(device)\n    enc_i.load_state_dict(pretrain_state)\n    model = Classifier(enc_i, hid=256, n_cls=2).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    train_loader = DataLoader(\n        LabeledDS(splits[name][\"train\"][\"sequence\"], splits[name][\"train\"][\"label\"]),\n        batch_size=128,\n        shuffle=True,\n    )\n    dev_loader = DataLoader(\n        LabeledDS(splits[name][\"dev\"][\"sequence\"], splits[name][\"dev\"][\"label\"]),\n        batch_size=256,\n    )\n    for ep in range(1, f_epochs + 1):\n        model.train()\n        tot = n = 0\n        for seqs, labels in train_loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            opt.zero_grad()\n            loss = crit(model(x), y)\n            loss.backward()\n            opt.step()\n            tot += loss.item() * y.size(0)\n            n += y.size(0)\n        train_loss = tot / n\n        val_loss, val_acc, _, _, _ = eval_model(model, dev_loader)\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"train\"].append(\n            train_loss\n        )  # storing loss as proxy\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"val\"].append(\n            val_acc\n        )\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # Evaluation on own test set\n    test_loader = DataLoader(\n        LabeledDS(splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]),\n        batch_size=256,\n    )\n    tl, ta, tp, tg, tr = eval_model(model, test_loader)\n    aca = aca_metric(\n        model, splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]\n    )\n    experiment_data[\"multi_synth_generalization\"][name][\"predictions\"] = tp\n    experiment_data[\"multi_synth_generalization\"][name][\"ground_truth\"] = tg\n    print(f\"Test on {name}: loss={tl:.4f} | acc={ta:.4f} | ACA={aca:.4f}\")\n\n    # Cross-dataset transfer evaluation\n    for other in RULES:\n        if other == name:\n            continue\n        other_loader = DataLoader(\n            LabeledDS(\n                splits[other][\"test\"][\"sequence\"], splits[other][\"test\"][\"label\"]\n            ),\n            batch_size=256,\n        )\n        _, acc, pred, gt, _ = eval_model(model, other_loader)\n        key = f\"{name}_to_{other}\"\n        experiment_data[\"multi_synth_generalization\"][\"transfer\"][key] = {\n            \"acc\": acc,\n            \"predictions\": pred,\n            \"ground_truth\": gt,\n        }\n        print(f\"  Transfer {name}->{other}: acc={acc:.4f}\")\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to working/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, math, time, copy, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Helper metrics\ndef count_shape_variety(sequence):  # first char of token\n    return len(set(tok[0] for tok in sequence.strip().split()))\n\n\ndef count_color_variety(sequence):  # second char of token\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# -----------------------------------------------------------------------------#\n# Synthetic dataset generators for three different rules\nshapes, colors = list(\"ABCDE\"), list(\"12345\")\n\n\ndef gen_sequence():\n    L = random.randint(3, 10)\n    return \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n\n\ndef label_rule1(seq):  # shape-vs-color variety\n    return int(count_shape_variety(seq) >= count_color_variety(seq))\n\n\ndef label_rule2(seq):  # sequence length parity (even=1, odd=0)\n    return int(len(seq.split()) % 2 == 0)\n\n\ndef label_rule3(seq):  # majority shape class exists\n    toks = [t[0] for t in seq.split()]\n    most = max(set(toks), key=toks.count)\n    return int(toks.count(most) > len(toks) / 2)\n\n\nRULES = {\n    \"variety\": label_rule1,\n    \"length_parity\": label_rule2,\n    \"majority_shape\": label_rule3,\n}\n\n\ndef make_dataset(n, rule_func):\n    seqs, labels = [], []\n    for _ in range(n):\n        s = gen_sequence()\n        seqs.append(s)\n        labels.append(rule_func(s))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nsplits = {}\nfor name, rule in RULES.items():\n    splits[name] = {\n        \"train\": make_dataset(6000, rule),\n        \"dev\": make_dataset(1200, rule),\n        \"test\": make_dataset(1200, rule),\n    }\n\n# -----------------------------------------------------------------------------#\n# Shared vocabulary (union of all training sequences)\nall_tokens = set()\nfor d in splits.values():\n    for s in d[\"train\"][\"sequence\"]:\n        all_tokens.update(s.split())\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = [\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n]\nmax_len = (\n    max(len(s.split()) for d in splits.values() for s in d[\"train\"][\"sequence\"]) + 1\n)\n\n\ndef encode(seq):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Dataset wrappers\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:\n        win = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - win)\n        seg = toks[i : i + win]\n        random.shuffle(seg)\n        toks[i : i + win] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model definitions\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=layers, batch_first=True)\n\n    def forward(self, x):\n        _, h = self.gru(self.emb(x))\n        return h[-1]  # (B,hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = z @ z.T / temp\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1\n    return nn.functional.cross_entropy(sim, pos_idx)\n\n\n# -----------------------------------------------------------------------------#\n# Experiment data structure\nexperiment_data = {\n    \"multi_synth_generalization\": {\n        name: {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for name in RULES\n    }\n}\nexperiment_data[\"multi_synth_generalization\"][\"transfer\"] = {}\n\n# -----------------------------------------------------------------------------#\n# Pre-training on union of unlabeled sequences\npretrain_seqs = []\nfor d in splits.values():\n    pretrain_seqs.extend(d[\"train\"][\"sequence\"])\npre_loader = DataLoader(\n    SeqOnlyDS(pretrain_seqs), batch_size=256, shuffle=True, drop_last=True\n)\n\nencoder = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(encoder.parameters(), lr=1e-3)\nprint(\"\\n--- Contrastive pre-training ---\")\nfor ep in range(1, 9):\n    encoder.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch in pre_loader:\n        views = []\n        for s in batch:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(encoder(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    print(f\"Epoch {ep}: loss={tot/cnt:.4f} ({time.time()-t0:.1f}s)\")\npretrain_state = copy.deepcopy(encoder.state_dict())\n\n# -----------------------------------------------------------------------------#\n# Helper functions\ncrit = nn.CrossEntropyLoss()\n\n\ndef eval_model(model, loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = float(np.mean(np.array(preds) == np.array(gts)))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(model, seqs, labels, M=3):\n    correct = total = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            variants = [s] + [augment(s) for _ in range(M)]\n            xs = torch.stack([encode(v) for v in variants]).to(device)\n            p = model(xs).argmax(1).cpu().tolist()\n            correct += sum(int(pi == l) for pi in p)\n            total += len(variants)\n    return correct / total\n\n\n# -----------------------------------------------------------------------------#\n# Fine-tune on each dataset and evaluate cross-transfer\nf_epochs = 10\nfor name in RULES:\n    print(f\"\\n=== Fine-tuning on dataset: {name} ===\")\n    enc_i = Encoder(len(vocab)).to(device)\n    enc_i.load_state_dict(pretrain_state)\n    model = Classifier(enc_i, hid=256, n_cls=2).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    train_loader = DataLoader(\n        LabeledDS(splits[name][\"train\"][\"sequence\"], splits[name][\"train\"][\"label\"]),\n        batch_size=128,\n        shuffle=True,\n    )\n    dev_loader = DataLoader(\n        LabeledDS(splits[name][\"dev\"][\"sequence\"], splits[name][\"dev\"][\"label\"]),\n        batch_size=256,\n    )\n    for ep in range(1, f_epochs + 1):\n        model.train()\n        tot = n = 0\n        for seqs, labels in train_loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            opt.zero_grad()\n            loss = crit(model(x), y)\n            loss.backward()\n            opt.step()\n            tot += loss.item() * y.size(0)\n            n += y.size(0)\n        train_loss = tot / n\n        val_loss, val_acc, _, _, _ = eval_model(model, dev_loader)\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"train\"].append(\n            train_loss\n        )  # storing loss as proxy\n        experiment_data[\"multi_synth_generalization\"][name][\"metrics\"][\"val\"].append(\n            val_acc\n        )\n        print(f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # Evaluation on own test set\n    test_loader = DataLoader(\n        LabeledDS(splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]),\n        batch_size=256,\n    )\n    tl, ta, tp, tg, tr = eval_model(model, test_loader)\n    aca = aca_metric(\n        model, splits[name][\"test\"][\"sequence\"], splits[name][\"test\"][\"label\"]\n    )\n    experiment_data[\"multi_synth_generalization\"][name][\"predictions\"] = tp\n    experiment_data[\"multi_synth_generalization\"][name][\"ground_truth\"] = tg\n    print(f\"Test on {name}: loss={tl:.4f} | acc={ta:.4f} | ACA={aca:.4f}\")\n\n    # Cross-dataset transfer evaluation\n    for other in RULES:\n        if other == name:\n            continue\n        other_loader = DataLoader(\n            LabeledDS(\n                splits[other][\"test\"][\"sequence\"], splits[other][\"test\"][\"label\"]\n            ),\n            batch_size=256,\n        )\n        _, acc, pred, gt, _ = eval_model(model, other_loader)\n        key = f\"{name}_to_{other}\"\n        experiment_data[\"multi_synth_generalization\"][\"transfer\"][key] = {\n            \"acc\": acc,\n            \"predictions\": pred,\n            \"ground_truth\": gt,\n        }\n        print(f\"  Transfer {name}->{other}: acc={acc:.4f}\")\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved results to working/experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch\n1: contrastive_loss = 1.3732  (0.8s)', '\\n', 'Epoch 2: contrastive_loss = 0.9218\n(0.5s)', '\\n', 'Epoch 3: contrastive_loss = 0.7865  (0.4s)', '\\n', 'Epoch 4:\ncontrastive_loss = 0.7359  (0.4s)', '\\n', 'Epoch 5: contrastive_loss = 0.7111\n(0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.6922  (0.4s)', '\\n', 'Epoch 7:\ncontrastive_loss = 0.6796  (0.4s)', '\\n', 'Epoch 8: contrastive_loss = 0.6809\n(0.4s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:284: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:254: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0644 | val_acc=0.9812 | ACA=0.9003', '\\n', 'Epoch 2:\nval_loss=0.0065 | val_acc=0.9984 | ACA=0.8890', '\\n', 'Epoch 3: val_loss=0.0017\n| val_acc=1.0000 | ACA=0.8811', '\\n', 'Epoch 4: val_loss=0.0006 | val_acc=0.9998\n| ACA=0.8895', '\\n', 'Epoch 5: val_loss=0.0006 | val_acc=0.9998 | ACA=0.8834',\n'\\n', 'Epoch 6: val_loss=0.0005 | val_acc=0.9998 | ACA=0.8848', '\\n', 'Epoch 7:\nval_loss=0.0004 | val_acc=0.9998 | ACA=0.8818', '\\n', 'Epoch 8: val_loss=0.0004\n| val_acc=0.9998 | ACA=0.8815', '\\n', 'Epoch 9: val_loss=0.0003 | val_acc=1.0000\n| ACA=0.8840', '\\n', 'Epoch 10: val_loss=0.0003 | val_acc=1.0000 | ACA=0.8830',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=4.1363 | Acc=0.7001 | SWA=0.7001 |\nCWA=0.6355 | ACA=0.6522', '\\n', 'Execution time: 57 seconds seconds (time limit\nis 30 minutes).']", "['\\n--- Contrastive pre-training ---', '\\n', 'Epoch 1: loss=1.3318 (0.7s)',\n'\\n', 'Epoch 2: loss=0.8521 (0.4s)', '\\n', 'Epoch 3: loss=0.6992 (0.4s)', '\\n',\n'Epoch 4: loss=0.6293 (0.4s)', '\\n', 'Epoch 5: loss=0.5547 (0.4s)', '\\n', 'Epoch\n6: loss=0.5173 (0.4s)', '\\n', 'Epoch 7: loss=0.5027 (0.4s)', '\\n', 'Epoch 8:\nloss=0.4745 (0.4s)', '\\n', '\\n=== Fine-tuning on dataset: variety ===', '\\n',\n'runfile.py:296: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:248: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.6162 | val_acc=0.6925', '\\n', 'Epoch 2: val_loss=0.6138 |\nval_acc=0.6908', '\\n', 'Epoch 3: val_loss=0.5439 | val_acc=0.7300', '\\n', 'Epoch\n4: val_loss=0.4407 | val_acc=0.7975', '\\n', 'Epoch 5: val_loss=0.3917 |\nval_acc=0.8242', '\\n', 'Epoch 6: val_loss=0.3881 | val_acc=0.8358', '\\n', 'Epoch\n7: val_loss=0.3988 | val_acc=0.8567', '\\n', 'Epoch 8: val_loss=0.4284 |\nval_acc=0.8442', '\\n', 'Epoch 9: val_loss=0.3843 | val_acc=0.8742', '\\n', 'Epoch\n10: val_loss=0.4336 | val_acc=0.8775', '\\n', 'Test on variety: loss=0.4883 |\nacc=0.8625 | ACA=0.8258', '\\n', '  Transfer variety->length_parity: acc=0.5458',\n'\\n', '  Transfer variety->majority_shape: acc=0.2517', '\\n', '\\n=== Fine-tuning\non dataset: length_parity ===', '\\n', 'Epoch 1: val_loss=0.1125 |\nval_acc=1.0000', '\\n', 'Epoch 2: val_loss=0.0025 | val_acc=1.0000', '\\n', 'Epoch\n3: val_loss=0.0006 | val_acc=1.0000', '\\n', 'Epoch 4: val_loss=0.0004 |\nval_acc=1.0000', '\\n', 'Epoch 5: val_loss=0.0003 | val_acc=1.0000', '\\n', 'Epoch\n6: val_loss=0.0002 | val_acc=1.0000', '\\n', 'Epoch 7: val_loss=0.0002 |\nval_acc=1.0000', '\\n', 'Epoch 8: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch\n9: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch 10: val_loss=0.0001 |\nval_acc=1.0000', '\\n', 'Test on length_parity: loss=0.0001 | acc=1.0000 |\nACA=1.0000', '\\n', '  Transfer length_parity->variety: acc=0.4942', '\\n', '\nTransfer length_parity->majority_shape: acc=0.4075', '\\n', '\\n=== Fine-tuning on\ndataset: majority_shape ===', '\\n', 'Epoch 1: val_loss=0.3788 | val_acc=0.8425',\n'\\n', 'Epoch 2: val_loss=0.3101 | val_acc=0.8675', '\\n', 'Epoch 3:\nval_loss=0.1830 | val_acc=0.9308', '\\n', 'Epoch 4: val_loss=0.1198 |\nval_acc=0.9508', '\\n', 'Epoch 5: val_loss=0.1340 | val_acc=0.9542', '\\n', 'Epoch\n6: val_loss=0.1282 | val_acc=0.9542', '\\n', 'Epoch 7: val_loss=0.1075 |\nval_acc=0.9675', '\\n', 'Epoch 8: val_loss=0.1534 | val_acc=0.9408', '\\n', 'Epoch\n9: val_loss=0.1261 | val_acc=0.9592', '\\n', 'Epoch 10: val_loss=0.1147 |\nval_acc=0.9658', '\\n', 'Test on majority_shape: loss=0.0971 | acc=0.9692 |\nACA=0.9265', '\\n', '  Transfer majority_shape->variety: acc=0.2617', '\\n', '\nTransfer majority_shape->length_parity: acc=0.4200', '\\n', '\\nSaved results to\nworking/experiment_data.npy', '\\n', 'Execution time: 12 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 432166.34\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 480315.15\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 528782.65\nexamples/s]', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch 1:\ncontrastive_loss=1.3732 (0.8s)', '\\n', 'Epoch 2: contrastive_loss=0.9218\n(0.5s)', '\\n', 'Epoch 3: contrastive_loss=0.7865 (0.5s)', '\\n', 'Epoch 4:\ncontrastive_loss=0.7359 (0.5s)', '\\n', 'Epoch 5: contrastive_loss=0.7111\n(0.5s)', '\\n', 'Epoch 6: contrastive_loss=0.6922 (0.5s)', '\\n', 'Epoch 7:\ncontrastive_loss=0.6796 (0.5s)', '\\n', 'Epoch 8: contrastive_loss=0.6809\n(0.4s)', '\\n', '\\n--- Fine-tuning (full_tune) ---', '\\n', 'runfile.py:302:\nUserWarning: To copy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:264: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0496 | val_acc=0.9876 | ACA=0.8951', '\\n', 'Epoch 2:\nval_loss=0.0095 | val_acc=0.9972 | ACA=0.8837', '\\n', 'Epoch 3: val_loss=0.0021\n| val_acc=0.9996 | ACA=0.8862', '\\n', 'Epoch 4: val_loss=0.0027 | val_acc=0.9992\n| ACA=0.8861', '\\n', 'Epoch 5: val_loss=0.0002 | val_acc=1.0000 | ACA=0.8781',\n'\\n', 'Epoch 6: val_loss=0.0002 | val_acc=1.0000 | ACA=0.8823', '\\n', 'Epoch 7:\nval_loss=0.0001 | val_acc=1.0000 | ACA=0.8784', '\\n', 'Epoch 8: val_loss=0.0001\n| val_acc=1.0000 | ACA=0.8780', '\\n', 'Epoch 9: val_loss=0.0001 | val_acc=1.0000\n| ACA=0.8810', '\\n', 'Epoch 10: val_loss=0.0001 | val_acc=1.0000 | ACA=0.8783',\n'\\n', '\\n--- Test results (full_tune) ---', '\\n', 'Loss=4.0304 | Acc=0.7000 |\nSWA=0.7000 | CWA=0.6354 | ACA=0.6515', '\\n', '\\n--- Fine-tuning (frozen_encoder)\n---', '\\n', 'runfile.py:353: UserWarning: To copy construct from a tensor, it is\nrecommended to use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.2880 | val_acc=0.9224 | ACA=0.8881', '\\n', 'Epoch 2:\nval_loss=0.2351 | val_acc=0.9298 | ACA=0.8933', '\\n', 'Epoch 3: val_loss=0.2183\n| val_acc=0.9300 | ACA=0.8952', '\\n', 'Epoch 4: val_loss=0.2104 | val_acc=0.9312\n| ACA=0.8946', '\\n', 'Epoch 5: val_loss=0.2056 | val_acc=0.9334 | ACA=0.8968',\n'\\n', 'Epoch 6: val_loss=0.2026 | val_acc=0.9332 | ACA=0.8952', '\\n', 'Epoch 7:\nval_loss=0.2008 | val_acc=0.9344 | ACA=0.8934', '\\n', 'Epoch 8: val_loss=0.1992\n| val_acc=0.9346 | ACA=0.8954', '\\n', 'Epoch 9: val_loss=0.1974 | val_acc=0.9356\n| ACA=0.8933', '\\n', 'Epoch 10: val_loss=0.1965 | val_acc=0.9360 | ACA=0.8952',\n'\\n', '\\n--- Test results (frozen_encoder) ---', '\\n', 'Loss=1.2079 | Acc=0.6753\n| SWA=0.6759 | CWA=0.6183 | ACA=0.6602', '\\n', '\\nAll experiment data saved to',\n' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_23-37-\n11_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n17/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 517489.48\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 626782.63\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 792994.03\nexamples/s]', '\\n', '\\n--- Supervised training (No-Pretraining) ---', '\\n',\n'runfile.py:250: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:217: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: train_acc=0.9347 | val_acc=0.9748 | ACA=0.9082', '\\n', 'Epoch 2:\ntrain_acc=0.9891 | val_acc=0.9970 | ACA=0.8758', '\\n', 'Epoch 3:\ntrain_acc=0.9964 | val_acc=0.9970 | ACA=0.8810', '\\n', 'Epoch 4:\ntrain_acc=0.9997 | val_acc=0.9998 | ACA=0.8777', '\\n', 'Epoch 5:\ntrain_acc=1.0000 | val_acc=1.0000 | ACA=0.8744', '\\n', 'Epoch 6:\ntrain_acc=1.0000 | val_acc=1.0000 | ACA=0.8765', '\\n', 'Epoch 7:\ntrain_acc=1.0000 | val_acc=1.0000 | ACA=0.8734', '\\n', 'Epoch 8:\ntrain_acc=1.0000 | val_acc=1.0000 | ACA=0.8711', '\\n', 'Epoch 9:\ntrain_acc=1.0000 | val_acc=1.0000 | ACA=0.8753', '\\n', 'Epoch 10:\ntrain_acc=1.0000 | val_acc=1.0000 | ACA=0.8706', '\\n', '\\n--- Test results (No-\nPretraining) ---', '\\n', 'Loss=3.8734 | Acc=0.7000 | SWA=0.7000 | CWA=0.6354 |\nACA=0.6466', '\\n', 'Execution time: 40 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 440689.25\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 614298.02\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 767147.82\nexamples/s]', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch 1:\ncontrastive_loss = 0.2834  (0.7s)', '\\n', 'Epoch 2: contrastive_loss = 0.1152\n(0.4s)', '\\n', 'Epoch 3: contrastive_loss = 0.0651  (0.4s)', '\\n', 'Epoch 4:\ncontrastive_loss = 0.0467  (0.4s)', '\\n', 'Epoch 5: contrastive_loss = 0.0405\n(0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.0374  (0.4s)', '\\n', 'Epoch 7:\ncontrastive_loss = 0.0353  (0.5s)', '\\n', 'Epoch 8: contrastive_loss = 0.0342\n(0.5s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:278: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:248: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0709 | val_acc=0.9788 | ACA=0.9788', '\\n', 'Epoch 2:\nval_loss=0.0146 | val_acc=0.9954 | ACA=0.9954', '\\n', 'Epoch 3: val_loss=0.0062\n| val_acc=0.9984 | ACA=0.9984', '\\n', 'Epoch 4: val_loss=0.0019 | val_acc=0.9994\n| ACA=0.9994', '\\n', 'Epoch 5: val_loss=0.0014 | val_acc=0.9996 | ACA=0.9996',\n'\\n', 'Epoch 6: val_loss=0.0014 | val_acc=0.9996 | ACA=0.9996', '\\n', 'Epoch 7:\nval_loss=0.0013 | val_acc=0.9996 | ACA=0.9996', '\\n', 'Epoch 8: val_loss=0.0013\n| val_acc=0.9996 | ACA=0.9996', '\\n', 'Epoch 9: val_loss=0.0012 | val_acc=0.9996\n| ACA=0.9996', '\\n', 'Epoch 10: val_loss=0.0012 | val_acc=0.9996 | ACA=0.9996',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=3.9352 | Acc=0.6995 | SWA=0.6995 |\nCWA=0.6350 | ACA=0.6995', '\\n', 'Execution time: 48 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training (masking only)\n---', '\\n', 'Epoch 1: loss=2.5330 (0.8s)', '\\n', 'Epoch 2: loss=1.8419 (0.5s)',\n'\\n', 'Epoch 3: loss=1.6517 (0.5s)', '\\n', 'Epoch 4: loss=1.5789 (0.4s)', '\\n',\n'Epoch 5: loss=1.5712 (0.4s)', '\\n', 'Epoch 6: loss=1.5453 (0.4s)', '\\n', 'Epoch\n7: loss=1.5077 (0.4s)', '\\n', 'Epoch 8: loss=1.5064 (0.4s)', '\\n', '\\n--- Fine-\ntuning ---', '\\n', 'runfile.py:278: UserWarning: To copy construct from a\ntensor, it is recommended to use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(lbl, device=device)\\n',\n'runfile.py:248: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(lbl, device=device)\\n', 'Ep 1:\nval_loss=0.0468 | val_acc=0.9880 | ACA=0.8093', '\\n', 'Ep 2: val_loss=0.0059 |\nval_acc=0.9988 | ACA=0.7765', '\\n', 'Ep 3: val_loss=0.0010 | val_acc=0.9998 |\nACA=0.7659', '\\n', 'Ep 4: val_loss=0.0004 | val_acc=1.0000 | ACA=0.7678', '\\n',\n'Ep 5: val_loss=0.0003 | val_acc=1.0000 | ACA=0.7649', '\\n', 'Ep 6:\nval_loss=0.0003 | val_acc=1.0000 | ACA=0.7673', '\\n', 'Ep 7: val_loss=0.0002 |\nval_acc=1.0000 | ACA=0.7655', '\\n', 'Ep 8: val_loss=0.0002 | val_acc=1.0000 |\nACA=0.7627', '\\n', 'Ep 9: val_loss=0.0002 | val_acc=1.0000 | ACA=0.7644', '\\n',\n'Ep 10: val_loss=0.0002 | val_acc=1.0000 | ACA=0.7642', '\\n', '\\n--- Test\nresults ---', '\\n', 'Loss=4.2892 | Acc=0.6999 | SWA=0.6999 | CWA=0.6353 |\nACA=0.6033', '\\n', 'Execution time: 52 seconds seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n491211.07 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 644999.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 616383.38\nexamples/s]', '\\n', '\\n--- Contrastive Pre-training (BoT) ---', '\\n', 'Epoch 1:\ncontrastive_loss = 3.7022  (0.6s)', '\\n', 'Epoch 2: contrastive_loss = 3.3271\n(0.3s)', '\\n', 'Epoch 3: contrastive_loss = 3.0898  (0.3s)', '\\n', 'Epoch 4:\ncontrastive_loss = 2.8446  (0.3s)', '\\n', 'Epoch 5: contrastive_loss = 2.7039\n(0.3s)', '\\n', 'Epoch 6: contrastive_loss = 2.5452  (0.3s)', '\\n', 'Epoch 7:\ncontrastive_loss = 2.4285  (0.3s)', '\\n', 'Epoch 8: contrastive_loss = 2.3382\n(0.3s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:285: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:255: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.5229 | val_acc=0.7416 | ACA=0.7265', '\\n', 'Epoch 2:\nval_loss=0.5219 | val_acc=0.7540 | ACA=0.7328', '\\n', 'Epoch 3: val_loss=0.5215\n| val_acc=0.7544 | ACA=0.7342', '\\n', 'Epoch 4: val_loss=0.5215 | val_acc=0.7418\n| ACA=0.7219', '\\n', 'Epoch 5: val_loss=0.5211 | val_acc=0.7390 | ACA=0.7247',\n'\\n', 'Epoch 6: val_loss=0.5221 | val_acc=0.7534 | ACA=0.7336', '\\n', 'Epoch 7:\nval_loss=0.5233 | val_acc=0.7620 | ACA=0.7378', '\\n', 'Epoch 8: val_loss=0.5215\n| val_acc=0.7490 | ACA=0.7275', '\\n', 'Epoch 9: val_loss=0.5216 | val_acc=0.7632\n| ACA=0.7418', '\\n', 'Epoch 10: val_loss=0.5203 | val_acc=0.7458 | ACA=0.7277',\n'\\n', '\\n--- Test results (BoT) ---', '\\n', 'Loss=0.7182 | Acc=0.6300 |\nSWA=0.6310 | CWA=0.5974 | ACA=0.6198', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_23-37-\n11_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n16/working/experiment_data.npy', '\\n', 'Execution time: 27 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training (Shuffle-Only)\n---', '\\n', 'Epoch 1: contrastive_loss = 0.2834  (0.8s)', '\\n', 'Epoch 2:\ncontrastive_loss = 0.1152  (0.5s)', '\\n', 'Epoch 3: contrastive_loss = 0.0651\n(0.5s)', '\\n', 'Epoch 4: contrastive_loss = 0.0467  (0.5s)', '\\n', 'Epoch 5:\ncontrastive_loss = 0.0405  (0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.0374\n(0.4s)', '\\n', 'Epoch 7: contrastive_loss = 0.0353  (0.4s)', '\\n', 'Epoch 8:\ncontrastive_loss = 0.0342  (0.4s)', '\\n', '\\n--- Fine-tuning ---', '\\n',\n'runfile.py:301: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:271: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0709 | val_acc=0.9788 | ACA=0.9063', '\\n', 'Epoch 2:\nval_loss=0.0146 | val_acc=0.9954 | ACA=0.8805', '\\n', 'Epoch 3: val_loss=0.0062\n| val_acc=0.9984 | ACA=0.8833', '\\n', 'Epoch 4: val_loss=0.0019 | val_acc=0.9994\n| ACA=0.8846', '\\n', 'Epoch 5: val_loss=0.0014 | val_acc=0.9996 | ACA=0.8856',\n'\\n', 'Epoch 6: val_loss=0.0014 | val_acc=0.9996 | ACA=0.8833', '\\n', 'Epoch 7:\nval_loss=0.0013 | val_acc=0.9996 | ACA=0.8843', '\\n', 'Epoch 8: val_loss=0.0013\n| val_acc=0.9996 | ACA=0.8778', '\\n', 'Epoch 9: val_loss=0.0012 | val_acc=0.9996\n| ACA=0.8852', '\\n', 'Epoch 10: val_loss=0.0012 | val_acc=0.9996 | ACA=0.8843',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=3.9352 | Acc=0.6995 | SWA=0.6995 |\nCWA=0.6350 | ACA=0.6531', '\\n', 'Execution time: 40 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- MLM Pre-training ---', '\\n', 'Epoch 1:\nmlm_loss = 2.6530  (0.6s)', '\\n', 'Epoch 2: mlm_loss = 2.5447  (0.3s)', '\\n',\n'Epoch 3: mlm_loss = 2.5062  (0.3s)', '\\n', 'Epoch 4: mlm_loss = 2.5015\n(0.3s)', '\\n', 'Epoch 5: mlm_loss = 2.4834  (0.2s)', '\\n', 'Epoch 6: mlm_loss =\n2.4661  (0.2s)', '\\n', 'Epoch 7: mlm_loss = 2.4781  (0.2s)', '\\n', 'Epoch 8:\nmlm_loss = 2.4735  (0.2s)', '\\n', '\\n--- Fine-tuning ---', '\\n',\n'runfile.py:309: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:279: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0226 | val_acc=0.9934 | ACA=0.8599', '\\n', 'Epoch 2:\nval_loss=0.0139 | val_acc=0.9956 | ACA=0.8625', '\\n', 'Epoch 3: val_loss=0.0094\n| val_acc=0.9976 | ACA=0.8646', '\\n', 'Epoch 4: val_loss=0.0037 | val_acc=0.9988\n| ACA=0.8598', '\\n', 'Epoch 5: val_loss=0.0041 | val_acc=0.9988 | ACA=0.8623',\n'\\n', 'Epoch 6: val_loss=0.0037 | val_acc=0.9990 | ACA=0.8604', '\\n', 'Epoch 7:\nval_loss=0.0037 | val_acc=0.9990 | ACA=0.8603', '\\n', 'Epoch 8: val_loss=0.0038\n| val_acc=0.9990 | ACA=0.8582', '\\n', 'Epoch 9: val_loss=0.0037 | val_acc=0.9990\n| ACA=0.8615', '\\n', 'Epoch 10: val_loss=0.0037 | val_acc=0.9990 | ACA=0.8598',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=4.1752 | Acc=0.6992 | SWA=0.6991 |\nCWA=0.6347 | ACA=0.6423', '\\n', '\\nExperiment data saved.', '\\n', 'Execution\ntime: 38 seconds seconds (time limit is 30 minutes).']", "['\\n--- Contrastive pre-training ---', '\\n', 'Epoch 1: loss=1.3318 (0.7s)',\n'\\n', 'Epoch 2: loss=0.8521 (0.4s)', '\\n', 'Epoch 3: loss=0.6992 (0.4s)', '\\n',\n'Epoch 4: loss=0.6293 (0.4s)', '\\n', 'Epoch 5: loss=0.5547 (0.4s)', '\\n', 'Epoch\n6: loss=0.5173 (0.4s)', '\\n', 'Epoch 7: loss=0.5027 (0.4s)', '\\n', 'Epoch 8:\nloss=0.4745 (0.4s)', '\\n', '\\n=== Fine-tuning on dataset: variety ===', '\\n',\n'runfile.py:308: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:260: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.6162 | val_acc=0.6925', '\\n', 'Epoch 2: val_loss=0.6138 |\nval_acc=0.6908', '\\n', 'Epoch 3: val_loss=0.5439 | val_acc=0.7300', '\\n', 'Epoch\n4: val_loss=0.4407 | val_acc=0.7975', '\\n', 'Epoch 5: val_loss=0.3917 |\nval_acc=0.8242', '\\n', 'Epoch 6: val_loss=0.3881 | val_acc=0.8358', '\\n', 'Epoch\n7: val_loss=0.3988 | val_acc=0.8567', '\\n', 'Epoch 8: val_loss=0.4284 |\nval_acc=0.8442', '\\n', 'Epoch 9: val_loss=0.3843 | val_acc=0.8742', '\\n', 'Epoch\n10: val_loss=0.4336 | val_acc=0.8775', '\\n', 'Test on variety: loss=0.4883 |\nacc=0.8625 | ACA=0.8258', '\\n', '  Transfer variety->length_parity: acc=0.5458',\n'\\n', '  Transfer variety->majority_shape: acc=0.2517', '\\n', '\\n=== Fine-tuning\non dataset: length_parity ===', '\\n', 'Epoch 1: val_loss=0.1125 |\nval_acc=1.0000', '\\n', 'Epoch 2: val_loss=0.0025 | val_acc=1.0000', '\\n', 'Epoch\n3: val_loss=0.0006 | val_acc=1.0000', '\\n', 'Epoch 4: val_loss=0.0004 |\nval_acc=1.0000', '\\n', 'Epoch 5: val_loss=0.0003 | val_acc=1.0000', '\\n', 'Epoch\n6: val_loss=0.0002 | val_acc=1.0000', '\\n', 'Epoch 7: val_loss=0.0002 |\nval_acc=1.0000', '\\n', 'Epoch 8: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch\n9: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch 10: val_loss=0.0001 |\nval_acc=1.0000', '\\n', 'Test on length_parity: loss=0.0001 | acc=1.0000 |\nACA=1.0000', '\\n', '  Transfer length_parity->variety: acc=0.4942', '\\n', '\nTransfer length_parity->majority_shape: acc=0.4075', '\\n', '\\n=== Fine-tuning on\ndataset: majority_shape ===', '\\n', 'Epoch 1: val_loss=0.3788 | val_acc=0.8425',\n'\\n', 'Epoch 2: val_loss=0.3101 | val_acc=0.8675', '\\n', 'Epoch 3:\nval_loss=0.1830 | val_acc=0.9308', '\\n', 'Epoch 4: val_loss=0.1198 |\nval_acc=0.9508', '\\n', 'Epoch 5: val_loss=0.1340 | val_acc=0.9542', '\\n', 'Epoch\n6: val_loss=0.1282 | val_acc=0.9542', '\\n', 'Epoch 7: val_loss=0.1075 |\nval_acc=0.9675', '\\n', 'Epoch 8: val_loss=0.1534 | val_acc=0.9408', '\\n', 'Epoch\n9: val_loss=0.1261 | val_acc=0.9592', '\\n', 'Epoch 10: val_loss=0.1147 |\nval_acc=0.9658', '\\n', 'Test on majority_shape: loss=0.0971 | acc=0.9692 |\nACA=0.9265', '\\n', '  Transfer majority_shape->variety: acc=0.2617', '\\n', '\nTransfer majority_shape->length_parity: acc=0.4200', '\\n', '\\nSaved results to\nworking/experiment_data.npy', '\\n', 'Execution time: 12 seconds seconds (time\nlimit is 30 minutes).']", "['\\n--- Contrastive pre-training ---', '\\n', 'Epoch 1: loss=1.3318 (0.8s)',\n'\\n', 'Epoch 2: loss=0.8521 (0.5s)', '\\n', 'Epoch 3: loss=0.6992 (0.5s)', '\\n',\n'Epoch 4: loss=0.6293 (0.4s)', '\\n', 'Epoch 5: loss=0.5547 (0.4s)', '\\n', 'Epoch\n6: loss=0.5173 (0.4s)', '\\n', 'Epoch 7: loss=0.5027 (0.4s)', '\\n', 'Epoch 8:\nloss=0.4745 (0.4s)', '\\n', '\\n=== Fine-tuning on dataset: variety ===', '\\n',\n'runfile.py:308: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:260: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.6162 | val_acc=0.6925', '\\n', 'Epoch 2: val_loss=0.6138 |\nval_acc=0.6908', '\\n', 'Epoch 3: val_loss=0.5439 | val_acc=0.7300', '\\n', 'Epoch\n4: val_loss=0.4407 | val_acc=0.7975', '\\n', 'Epoch 5: val_loss=0.3917 |\nval_acc=0.8242', '\\n', 'Epoch 6: val_loss=0.3881 | val_acc=0.8358', '\\n', 'Epoch\n7: val_loss=0.3988 | val_acc=0.8567', '\\n', 'Epoch 8: val_loss=0.4284 |\nval_acc=0.8442', '\\n', 'Epoch 9: val_loss=0.3843 | val_acc=0.8742', '\\n', 'Epoch\n10: val_loss=0.4336 | val_acc=0.8775', '\\n', 'Test on variety: loss=0.4883 |\nacc=0.8625 | ACA=0.8258', '\\n', '  Transfer variety->length_parity: acc=0.5458',\n'\\n', '  Transfer variety->majority_shape: acc=0.2517', '\\n', '\\n=== Fine-tuning\non dataset: length_parity ===', '\\n', 'Epoch 1: val_loss=0.1125 |\nval_acc=1.0000', '\\n', 'Epoch 2: val_loss=0.0025 | val_acc=1.0000', '\\n', 'Epoch\n3: val_loss=0.0006 | val_acc=1.0000', '\\n', 'Epoch 4: val_loss=0.0004 |\nval_acc=1.0000', '\\n', 'Epoch 5: val_loss=0.0003 | val_acc=1.0000', '\\n', 'Epoch\n6: val_loss=0.0002 | val_acc=1.0000', '\\n', 'Epoch 7: val_loss=0.0002 |\nval_acc=1.0000', '\\n', 'Epoch 8: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch\n9: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch 10: val_loss=0.0001 |\nval_acc=1.0000', '\\n', 'Test on length_parity: loss=0.0001 | acc=1.0000 |\nACA=1.0000', '\\n', '  Transfer length_parity->variety: acc=0.4942', '\\n', '\nTransfer length_parity->majority_shape: acc=0.4075', '\\n', '\\n=== Fine-tuning on\ndataset: majority_shape ===', '\\n', 'Epoch 1: val_loss=0.3788 | val_acc=0.8425',\n'\\n', 'Epoch 2: val_loss=0.3101 | val_acc=0.8675', '\\n', 'Epoch 3:\nval_loss=0.1830 | val_acc=0.9308', '\\n', 'Epoch 4: val_loss=0.1198 |\nval_acc=0.9508', '\\n', 'Epoch 5: val_loss=0.1340 | val_acc=0.9542', '\\n', 'Epoch\n6: val_loss=0.1282 | val_acc=0.9542', '\\n', 'Epoch 7: val_loss=0.1075 |\nval_acc=0.9675', '\\n', 'Epoch 8: val_loss=0.1534 | val_acc=0.9408', '\\n', 'Epoch\n9: val_loss=0.1261 | val_acc=0.9592', '\\n', 'Epoch 10: val_loss=0.1147 |\nval_acc=0.9658', '\\n', 'Test on majority_shape: loss=0.0971 | acc=0.9692 |\nACA=0.9265', '\\n', '  Transfer majority_shape->variety: acc=0.2617', '\\n', '\nTransfer majority_shape->length_parity: acc=0.4200', '\\n', '\\nSaved results to\nworking/experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['\\n--- Contrastive pre-training ---', '\\n', 'Epoch 1: loss=1.3318 (0.7s)',\n'\\n', 'Epoch 2: loss=0.8521 (0.4s)', '\\n', 'Epoch 3: loss=0.6992 (0.4s)', '\\n',\n'Epoch 4: loss=0.6293 (0.4s)', '\\n', 'Epoch 5: loss=0.5547 (0.4s)', '\\n', 'Epoch\n6: loss=0.5173 (0.5s)', '\\n', 'Epoch 7: loss=0.5027 (0.5s)', '\\n', 'Epoch 8:\nloss=0.4745 (0.5s)', '\\n', '\\n=== Fine-tuning on dataset: variety ===', '\\n',\n'runfile.py:308: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:260: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.6162 | val_acc=0.6925', '\\n', 'Epoch 2: val_loss=0.6138 |\nval_acc=0.6908', '\\n', 'Epoch 3: val_loss=0.5439 | val_acc=0.7300', '\\n', 'Epoch\n4: val_loss=0.4407 | val_acc=0.7975', '\\n', 'Epoch 5: val_loss=0.3917 |\nval_acc=0.8242', '\\n', 'Epoch 6: val_loss=0.3881 | val_acc=0.8358', '\\n', 'Epoch\n7: val_loss=0.3988 | val_acc=0.8567', '\\n', 'Epoch 8: val_loss=0.4284 |\nval_acc=0.8442', '\\n', 'Epoch 9: val_loss=0.3843 | val_acc=0.8742', '\\n', 'Epoch\n10: val_loss=0.4336 | val_acc=0.8775', '\\n', 'Test on variety: loss=0.4883 |\nacc=0.8625 | ACA=0.8258', '\\n', '  Transfer variety->length_parity: acc=0.5458',\n'\\n', '  Transfer variety->majority_shape: acc=0.2517', '\\n', '\\n=== Fine-tuning\non dataset: length_parity ===', '\\n', 'Epoch 1: val_loss=0.1125 |\nval_acc=1.0000', '\\n', 'Epoch 2: val_loss=0.0025 | val_acc=1.0000', '\\n', 'Epoch\n3: val_loss=0.0006 | val_acc=1.0000', '\\n', 'Epoch 4: val_loss=0.0004 |\nval_acc=1.0000', '\\n', 'Epoch 5: val_loss=0.0003 | val_acc=1.0000', '\\n', 'Epoch\n6: val_loss=0.0002 | val_acc=1.0000', '\\n', 'Epoch 7: val_loss=0.0002 |\nval_acc=1.0000', '\\n', 'Epoch 8: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch\n9: val_loss=0.0001 | val_acc=1.0000', '\\n', 'Epoch 10: val_loss=0.0001 |\nval_acc=1.0000', '\\n', 'Test on length_parity: loss=0.0001 | acc=1.0000 |\nACA=1.0000', '\\n', '  Transfer length_parity->variety: acc=0.4942', '\\n', '\nTransfer length_parity->majority_shape: acc=0.4075', '\\n', '\\n=== Fine-tuning on\ndataset: majority_shape ===', '\\n', 'Epoch 1: val_loss=0.3788 | val_acc=0.8425',\n'\\n', 'Epoch 2: val_loss=0.3101 | val_acc=0.8675', '\\n', 'Epoch 3:\nval_loss=0.1830 | val_acc=0.9308', '\\n', 'Epoch 4: val_loss=0.1198 |\nval_acc=0.9508', '\\n', 'Epoch 5: val_loss=0.1340 | val_acc=0.9542', '\\n', 'Epoch\n6: val_loss=0.1282 | val_acc=0.9542', '\\n', 'Epoch 7: val_loss=0.1075 |\nval_acc=0.9675', '\\n', 'Epoch 8: val_loss=0.1534 | val_acc=0.9408', '\\n', 'Epoch\n9: val_loss=0.1261 | val_acc=0.9592', '\\n', 'Epoch 10: val_loss=0.1147 |\nval_acc=0.9658', '\\n', 'Test on majority_shape: loss=0.0971 | acc=0.9692 |\nACA=0.9265', '\\n', '  Transfer majority_shape->variety: acc=0.2617', '\\n', '\nTransfer majority_shape->length_parity: acc=0.4200', '\\n', '\\nSaved results to\nworking/experiment_data.npy', '\\n', 'Execution time: 12 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["The execution of the training script completed successfully without any errors\nor bugs. The model underwent contrastive pre-training and fine-tuning, achieving\na test accuracy of 70.01%, SWA of 70.01%, and CWA of 63.55%. While this\nperformance does not surpass the SOTA benchmarks of 65.0% SWA and 70.0% CWA, the\nimplementation is functioning as intended. Further optimization and\nexperimentation are needed to improve the model's performance.", "", "", "", "", "", "", "", "", "The execution of the training script was successful without any bugs. The\ncontrastive pre-training and fine-tuning stages were executed as expected, and\nthe results were saved to the specified file. The performance metrics, such as\naccuracy and ACA, were reported for each dataset, and cross-dataset transfer\nevaluations were conducted. There are no apparent issues or errors in the\noutput.", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "The loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6796, "best_value": 0.6796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0003, "best_value": 0.0003}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "The average class accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9003, "best_value": 0.9003}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.1363, "best_value": 4.1363}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6355, "best_value": 0.6355}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "The average class accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6522, "best_value": 0.6522}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, where lower values indicate better performance.", "data": [{"dataset_name": "variety", "final_value": 0.0202, "best_value": 0.0202}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.0141, "best_value": 0.0141}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation dataset, where lower values indicate better performance.", "data": [{"dataset_name": "variety", "final_value": 0.4336, "best_value": 0.4336}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.1147, "best_value": 0.1147}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset, where higher values indicate better performance.", "data": [{"dataset_name": "variety", "final_value": 0.8775, "best_value": 0.8775}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9658, "best_value": 0.9658}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy on the test dataset, where higher values indicate better performance.", "data": [{"dataset_name": "variety", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9692, "best_value": 0.9692}]}]}, {"metric_names": [{"metric_name": "Pretraining loss", "lower_is_better": true, "description": "The loss value during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.6809, "best_value": 0.6809}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "The loss value during the training phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.1911, "best_value": 0.1911}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "The loss value during the validation phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.1965, "best_value": 0.1965}]}, {"metric_name": "Validation accuracy", "lower_is_better": false, "description": "The accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.936, "best_value": 0.936}]}, {"metric_name": "Validation ACA", "lower_is_better": false, "description": "The ACA metric during the validation phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.8783, "best_value": 0.8783}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.8952, "best_value": 0.8952}]}, {"metric_name": "Test loss", "lower_is_better": true, "description": "The loss value during the test phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 4.0304, "best_value": 4.0304}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 1.2079, "best_value": 1.2079}]}, {"metric_name": "Test accuracy", "lower_is_better": false, "description": "The accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.7, "best_value": 0.7}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.6753, "best_value": 0.6753}]}, {"metric_name": "Test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.7, "best_value": 0.7}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.6759, "best_value": 0.6759}]}, {"metric_name": "Test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.6354, "best_value": 0.6354}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.6183, "best_value": 0.6183}]}, {"metric_name": "Test ACA", "lower_is_better": false, "description": "The ACA metric during the test phase.", "data": [{"dataset_name": "SPR_BENCH (experiment: full_tune)", "final_value": 0.6515, "best_value": 0.6515}, {"dataset_name": "SPR_BENCH (experiment: frozen_encoder)", "final_value": 0.6602, "best_value": 0.6602}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the model's error on the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating the model's error on the validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0001, "best_value": 0.0001}]}, {"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "The Average Class Accuracy (ACA) on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8706, "best_value": 0.8706}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value on the test dataset, indicating the model's error.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 3.8734, "best_value": 3.8734}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6354, "best_value": 0.6354}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "The Average Class Accuracy (ACA) on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6466, "best_value": 0.6466}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0342, "best_value": 0.0342}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0012, "best_value": 0.0012}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Average Class Accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 3.9352, "best_value": 3.9352}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6995, "best_value": 0.6995}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6995, "best_value": 0.6995}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.635, "best_value": 0.635}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Average Class Accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6995, "best_value": 0.6995}]}]}, {"metric_names": [{"metric_name": "pre-training loss", "lower_is_better": true, "description": "The loss during the pre-training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.5064, "best_value": 1.5064}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0002, "best_value": 0.0002}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "The ACA (Average Class Accuracy) during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8093, "best_value": 0.8093}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.2892, "best_value": 4.2892}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6353, "best_value": 0.6353}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "The ACA (Average Class Accuracy) on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6033, "best_value": 0.6033}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.3382, "best_value": 2.3382}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5202, "best_value": 0.5202}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5203, "best_value": 0.5203}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7632, "best_value": 0.7632}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Average Class Accuracy (ACA) during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7418, "best_value": 0.7418}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7182, "best_value": 0.7182}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.63, "best_value": 0.63}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.631, "best_value": 0.631}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5974, "best_value": 0.5974}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Average Class Accuracy (ACA) during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6198, "best_value": 0.6198}]}]}, {"metric_names": [{"metric_name": "pre-training loss", "lower_is_better": true, "description": "Loss during the pre-training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.034218, "best_value": 0.034218}]}, {"metric_name": "fine-tuning training loss", "lower_is_better": true, "description": "Loss during the fine-tuning phase of training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4e-05, "best_value": 4e-05}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.001167, "best_value": 0.001167}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Average Class Accuracy (ACA) on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9063, "best_value": 0.9063}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 3.935185, "best_value": 3.935185}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6995, "best_value": 0.6995}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.699464, "best_value": 0.699464}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.635038, "best_value": 0.635038}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Average Class Accuracy (ACA) on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6531, "best_value": 0.6531}]}]}, {"metric_names": [{"metric_name": "MLM pretraining loss", "lower_is_better": true, "description": "Loss for Masked Language Model pretraining.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.4735, "best_value": 2.4735}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The overall training loss during the experiment.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0037, "best_value": 0.0037}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.999, "best_value": 0.999}]}, {"metric_name": "validation augmented consistency accuracy", "lower_is_better": false, "description": "Augmented consistency accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8646, "best_value": 0.8646}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.1752, "best_value": 4.1752}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6992, "best_value": 0.6992}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6991, "best_value": 0.6991}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6347, "best_value": 0.6347}]}, {"metric_name": "test augmented consistency accuracy", "lower_is_better": false, "description": "Augmented consistency accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6423, "best_value": 0.6423}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss of the model", "data": [{"dataset_name": "variety", "final_value": 0.0202, "best_value": 0.0202}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.0141, "best_value": 0.0141}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss of the model", "data": [{"dataset_name": "variety", "final_value": 0.4336, "best_value": 0.4336}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.1147, "best_value": 0.1147}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy of the model", "data": [{"dataset_name": "variety", "final_value": 0.8775, "best_value": 0.8775}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9658, "best_value": 0.9658}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Final test accuracy of the model", "data": [{"dataset_name": "variety", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9692, "best_value": 0.9692}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final loss value achieved during training.", "data": [{"dataset_name": "variety", "final_value": 0.0202, "best_value": 0.0202}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.0141, "best_value": 0.0141}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final loss value achieved during validation.", "data": [{"dataset_name": "variety", "final_value": 0.4336, "best_value": 0.4336}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.1147, "best_value": 0.1147}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The final accuracy achieved during validation.", "data": [{"dataset_name": "variety", "final_value": 0.8775, "best_value": 0.8775}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9658, "best_value": 0.9658}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "variety", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9692, "best_value": 0.9692}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset at the end of the training process.", "data": [{"dataset_name": "variety", "final_value": 0.0202, "best_value": 0.0202}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.0141, "best_value": 0.0141}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset at the end of the training process.", "data": [{"dataset_name": "variety", "final_value": 0.4336, "best_value": 0.4336}, {"dataset_name": "length_parity", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "majority_shape", "final_value": 0.1147, "best_value": 0.1147}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy calculated on the validation dataset at the end of the training process.", "data": [{"dataset_name": "variety", "final_value": 0.8775, "best_value": 0.8775}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9658, "best_value": 0.9658}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy calculated on the test dataset after the training process.", "data": [{"dataset_name": "variety", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "length_parity", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "majority_shape", "final_value": 0.9692, "best_value": 0.9692}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/variety_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/length_parity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/majority_shape_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/val_accuracy_all_datasets.png", "../../logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/transfer_accuracy_heatmap.png"], ["../../logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_full_tune_loss_curves.png", "../../logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_full_tune_val_acc.png", "../../logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_val_aca_comparison.png", "../../logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_test_metrics_comparison.png"], ["../../logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_val_acc_aca.png", "../../logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_validation_metrics.png", "../../logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_val_acc_aca.png", "../../logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/variety_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/length_parity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/majority_shape_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/val_accuracy_all_datasets.png", "../../logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/transfer_accuracy_heatmap.png"], ["../../logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/variety_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/length_parity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/majority_shape_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/val_accuracy_all_datasets.png", "../../logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/transfer_accuracy_heatmap.png"], ["../../logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/variety_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/length_parity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/majority_shape_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/val_accuracy_all_datasets.png", "../../logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/transfer_accuracy_heatmap.png"], ["../../logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/variety_loss_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/length_parity_loss_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/majority_shape_loss_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/val_accuracy_all_datasets_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/transfer_accuracy_heatmap_mean.png"]], "plot_paths": [["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/variety_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/length_parity_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/majority_shape_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/val_accuracy_all_datasets.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/transfer_accuracy_heatmap.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_full_tune_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_full_tune_val_acc.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_val_aca_comparison.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_test_metrics_comparison.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_val_acc_aca.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_validation_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_val_acc_aca.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/variety_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/length_parity_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/majority_shape_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/val_accuracy_all_datasets.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/transfer_accuracy_heatmap.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/variety_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/length_parity_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/majority_shape_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/val_accuracy_all_datasets.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/transfer_accuracy_heatmap.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/variety_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/length_parity_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/majority_shape_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/val_accuracy_all_datasets.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/transfer_accuracy_heatmap.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/variety_loss_curves_mean_se.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/length_parity_loss_curves_mean_se.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/majority_shape_loss_curves_mean_se.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/val_accuracy_all_datasets_mean_se.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_d86ea4e283e84590aeb7e638ff697583/transfer_accuracy_heatmap_mean.png"]], "plot_analyses": [[{"analysis": "The contrastive pre-training loss decreases steadily over 8 epochs, indicating that the model is effectively learning meaningful representations through contrastive learning. The rapid drop in the initial epochs suggests that the model quickly captures fundamental patterns, while the gradual decline afterward implies fine-tuning of the learned embeddings.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_pretrain_loss.png"}, {"analysis": "Both train and validation losses decrease significantly in the first few epochs and stabilize near zero. This behavior suggests that the model generalizes well to the validation set without overfitting, as there is no divergence between the train and validation loss curves.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_train_val_loss.png"}, {"analysis": "Validation accuracy remains consistently high, approaching 1.0, while the auxiliary classification accuracy (ACA) slightly decreases over epochs. This indicates that while the primary task performance is excellent, the auxiliary task might need further refinement, possibly due to insufficient alignment between the auxiliary task objective and the main task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_val_metrics.png"}, {"analysis": "The test metric summary shows that the model achieves competitive scores across all metrics, with accuracy and SWA being slightly higher than CWA and ACA. This suggests that the model performs well on the primary SPR task, but there might be room for improvement in capturing color-related features and auxiliary classification tasks.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_test_metrics.png"}], [{"analysis": "The plot shows the training and validation losses for the 'variety' dataset over 10 epochs. The training loss decreases steadily, reaching close to zero by the 10th epoch. The validation loss decreases initially but starts fluctuating after the 6th epoch, indicating potential overfitting or noise in the validation set. While the model learns effectively on the training data, the fluctuations in validation loss suggest that regularization techniques might be necessary to improve generalization.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/variety_loss_curves.png"}, {"analysis": "This plot represents the training and validation losses for the 'length_parity' dataset. Both losses drop rapidly within the first two epochs and stabilize at nearly zero for the remaining epochs. This indicates that the model has effectively learned the task with minimal overfitting. The clear convergence suggests that this dataset is relatively simple or well-suited to the model.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/length_parity_loss_curves.png"}, {"analysis": "The training and validation losses for the 'majority_shape' dataset show a steady decline. The training loss approaches zero, while the validation loss decreases and then fluctuates slightly after the 5th epoch. The fluctuations in validation loss might indicate minor overfitting or variability in the validation set. Overall, the model performs well on this dataset, although further tuning might reduce the fluctuations.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/majority_shape_loss_curves.png"}, {"analysis": "The plot compares the validation accuracy across three datasets over 10 epochs. The 'majority_shape' dataset achieves the highest accuracy, stabilizing around 95%. The 'length_parity' dataset also performs well, with accuracy reaching close to 90%. The 'variety' dataset shows slower and less consistent improvement, with accuracy peaking below 80%. This suggests that the model generalizes better on 'majority_shape' and 'length_parity' datasets but struggles with the complexity or variability in the 'variety' dataset.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/val_accuracy_all_datasets.png"}, {"analysis": "The heatmap illustrates transfer accuracy between datasets. The diagonal values (self-transfer) are high, indicating strong performance when the model is tested on the same dataset it was trained on. However, off-diagonal values are significantly lower, especially for transferring from 'variety' to 'majority_shape'. This suggests that the model struggles to generalize across datasets, possibly due to differences in data distribution or task complexity. Improving the robustness of embeddings or incorporating domain adaptation techniques might enhance transferability.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0fa35057d875425896e1309d0d0d35a6_proc_3039480/transfer_accuracy_heatmap.png"}], [{"analysis": "The contrastive pre-training loss decreases steadily across epochs, starting from 1.4 and stabilizing around 0.7. This indicates that the contrastive learning framework is effectively optimizing the loss function, improving the quality of the learned embeddings for the SPR_BENCH dataset.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_pretrain_loss.png"}, {"analysis": "The full-tune loss curves for both training and validation datasets show a rapid decrease in loss within the first three epochs, followed by stabilization near zero. This suggests that the model is learning effectively without overfitting, as the validation loss aligns closely with the training loss.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_full_tune_loss_curves.png"}, {"analysis": "Validation accuracy improves significantly during the initial epochs, reaching near-perfect accuracy (close to 1.0) by the fourth epoch and maintaining stability thereafter. This demonstrates the model's ability to generalize well on the validation set after fine-tuning.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_full_tune_val_acc.png"}, {"analysis": "The validation ACA metric shows a slight advantage for the frozen encoder over the full-tune approach, with the frozen encoder maintaining higher ACA values throughout. This suggests that freezing the encoder during training helps retain context-aware features that are beneficial for accurate classification.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_val_aca_comparison.png"}, {"analysis": "The test metrics comparison reveals that the full-tune approach slightly outperforms the frozen encoder in accuracy (acc), shape-weighted accuracy (SWA), and color-weighted accuracy (CWA). However, the frozen encoder performs comparably in ACA, indicating that both methods are competitive, with full-tuning providing a slight edge in most metrics.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_83927c188e504dbe930b476c6498caf9_proc_3039481/spr_bench_test_metrics_comparison.png"}], [{"analysis": "The plot shows the training and validation loss over epochs. Both losses decrease rapidly in the initial epochs and stabilize close to zero after epoch 5. This indicates that the model is learning effectively without overfitting, as the validation loss closely follows the training loss.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot displays the training accuracy, validation accuracy, and validation ACA (Average Class Accuracy) over epochs. The training and validation accuracies converge to nearly 100%, demonstrating excellent performance on both datasets. However, the validation ACA stabilizes around 88%, suggesting that while the model performs well overall, there may be some class imbalance or difficulty with certain classes.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The bar chart presents the test set performance metrics. The accuracy and SWA (Shape-Weighted Accuracy) metrics are both at 70%, which matches the validation accuracy. However, the CWA (Color-Weighted Accuracy) is slightly lower at 64%, and the ACA (Average Class Accuracy) is 65%. These results indicate that while the model performs well overall, it struggles slightly with color-related features and maintaining balanced performance across all classes.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6557f600264840f4818341e20c0af110_proc_3039482/SPR_BENCH_test_metrics.png"}], [{"analysis": "The pre-training loss decreases sharply in the initial epochs and then plateaus around epoch 5. This indicates that the contrastive learning model quickly learns useful representations in the beginning and stabilizes as it converges to an optimal solution. The low final loss suggests effective pre-training.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_pretrain_loss.png"}, {"analysis": "Both training and validation loss decrease rapidly in the first few epochs and then stabilize, with validation loss closely tracking training loss. This implies that the fine-tuning process is effective and the model generalizes well to unseen data without overfitting.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_train_val_loss.png"}, {"analysis": "Validation accuracy and ACA (Average Class Accuracy) increase sharply in the initial epochs and reach a near-perfect score around epoch 4, maintaining stability thereafter. This demonstrates the model's strong capacity to learn and generalize effectively to the validation set.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_val_acc_aca.png"}, {"analysis": "The test metrics summary shows that the model achieves 70% accuracy and SWA, 64% CWA, and 70% ACA. While the SWA and ACA meet the state-of-the-art benchmark, the CWA lags slightly, suggesting that the model could be improved in handling color-based variations in the symbolic sequences.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6c1be8b615414139a313c27536ad49b0_proc_3039483/SPR_BENCH_test_metrics.png"}], [{"analysis": "The plot shows the contrastive pre-training loss decreasing consistently over 8 epochs. The sharp decline in the first few epochs indicates rapid learning during the initial stages of training. The loss plateauing towards the end suggests that the model is converging, which is a positive sign for the stability of the training process. The use of mask-only augmentation appears to be effective in reducing the loss.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_pretrain_loss.png"}, {"analysis": "This plot illustrates the training and validation loss during the fine-tuning phase. Both losses decrease rapidly and converge to near-zero values by the fourth epoch, indicating effective fine-tuning. The absence of significant divergence between the two curves suggests that the model is not overfitting and generalizes well to the validation data.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_train_val_loss.png"}, {"analysis": "The validation metrics plot shows that the validation accuracy (Val Acc) quickly reaches near-perfect values, while the average class accuracy (Val ACA) stabilizes around 0.8 after an initial drop. This suggests that the model performs well overall but may struggle with certain classes, leading to a lower ACA compared to Val Acc.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_val_metrics.png"}, {"analysis": "The test metrics summary shows that the model achieves 0.70 for both accuracy and shape-weighted accuracy (SWA), which matches the SOTA benchmark for SWA. The color-weighted accuracy (CWA) is slightly lower at 0.64, indicating room for improvement in capturing color-related patterns. The average class accuracy (ACA) is the lowest at 0.60, highlighting potential class imbalance or difficulty in generalizing across all classes.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cf0efedcdafb4e39afd1fa8244b1c609_proc_3039482/SPR_BENCH_test_metrics.png"}], [{"analysis": "The pretraining loss curve demonstrates a steady decrease over the epochs, indicating successful optimization of the contrastive learning objective during the pretraining phase. The train and validation loss curves during fine-tuning remain flat and low, suggesting that the model has converged and is not overfitting to the training data. This behavior indicates a well-regularized model and effective pretraining that facilitated fine-tuning.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation accuracy and ACA (Adjusted Classification Accuracy) fluctuate over the epochs during fine-tuning, showing some instability. While the validation accuracy peaks at certain epochs, the ACA metric appears to lag slightly behind. The fluctuations suggest that the model may be sensitive to hyperparameter settings or batch variations, and further tuning or regularization might stabilize the performance.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_validation_metrics.png"}, {"analysis": "The test metrics summary indicates that the model achieves 63% accuracy and SWA (Shape-Weighted Accuracy), 60% CWA (Color-Weighted Accuracy), and 62% ACA. These results demonstrate that the model performs consistently across different metrics but falls short of surpassing the SOTA benchmarks of 65% SWA and 70% CWA. This suggests that while the proposed approach is effective, further refinements in the contrastive learning framework or augmentation strategies may be required to achieve SOTA performance.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ca7dc21ef046db8a0bde42d92a91fc_proc_3039480/SPR_BENCH_test_metrics.png"}], [{"analysis": "The contrastive pre-training loss decreases sharply within the first few epochs and plateaus around epoch 4. This indicates that the contrastive learning framework effectively minimizes the loss and achieves a stable representation early in the training process. The rapid convergence suggests that the model quickly learns meaningful embeddings for the symbolic patterns.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_pretrain_loss.png"}, {"analysis": "The fine-tuning loss curves for both training and validation data decrease significantly within the first few epochs and stabilize near zero. The alignment between training and validation loss indicates good generalization and suggests that the model is not overfitting. The rapid convergence highlights the effectiveness of the pre-trained embeddings for the SPR task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_train_val_loss.png"}, {"analysis": "The validation accuracy (Val Accuracy) improves quickly and reaches near-perfect scores early in the training process, stabilizing close to 1.0. However, the validation ACA (average class accuracy) starts lower and fluctuates slightly, suggesting some class imbalance or difficulty in achieving uniform performance across all classes. This could be an aspect to address in future experiments.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_val_acc_aca.png"}, {"analysis": "The test metrics overview shows that the model achieves 70% accuracy and SWA, surpassing the current SOTA benchmarks mentioned in the hypothesis. The CWA and ACA metrics are slightly lower, at 64% and 65%, respectively. While the SWA and accuracy improvements are promising, the relatively lower CWA indicates that the model might struggle with color-specific variations in the symbolic sequences.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_12e16392042d45ad9a185c438c345017_proc_3039483/SPR_BENCH_test_metrics.png"}], [{"analysis": "The plot shows the loss curves for the pre-training phase using masked language modeling (MLM) and the fine-tuning phase for both training and validation sets. The MLM loss decreases gradually, suggesting that the model is learning meaningful representations during pre-training. Fine-tuning loss for both training and validation sets converges to near-zero values after a few epochs, indicating that the model fits the downstream SPR task well. However, the relatively flat behavior of the MLM loss after a few epochs may suggest diminishing returns in pre-training, which could benefit from additional regularization or a more challenging pre-training objective.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot presents two validation accuracy metrics, Val ACC and Val ACA, over fine-tuning epochs. While Val ACC shows a steady increase and plateaus near 1.0, Val ACA remains relatively stable and does not improve significantly. This discrepancy suggests that while the model performs well on overall validation accuracy, it may not generalize equally well across all categories. This behavior could indicate a need for further refinement of the context-aware contrastive learning framework or augmentation strategies to ensure balanced generalization.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_val_metrics.png"}, {"analysis": "This plot displays the test metrics for the SPR_BENCH dataset, including ACC, SWA, CWA, and ACA. ACC and SWA achieve the same score of 0.699, indicating that shape variety does not significantly impact the overall accuracy. CWA and ACA scores are slightly lower at 0.635 and 0.642, respectively, suggesting that the model struggles more with color-based variations and overall contextual accuracy. These results highlight the potential need for improved data augmentation or fine-tuning techniques to enhance performance on these specific metrics.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_cfd2f21f107e4c62a05b9617ff257d86_proc_3039481/SPR_BENCH_test_metrics.png"}], [{"analysis": "This plot shows the training and validation loss for the 'variety' dataset over 10 epochs. The training loss decreases steadily, indicating effective learning. The validation loss decreases initially but starts to fluctuate after epoch 6, suggesting some overfitting. The gap between training and validation loss widens slightly, which may indicate a need for regularization or better data augmentation techniques.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/variety_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation loss for the 'length_parity' dataset. Both losses drop rapidly to near-zero by epoch 3, indicating that the model has learned the task effectively and with minimal overfitting. The validation loss remaining constant at zero after epoch 3 suggests the model generalizes well on this dataset.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/length_parity_loss_curves.png"}, {"analysis": "This plot represents the training and validation loss for the 'majority_shape' dataset. Both losses decrease steadily, with the validation loss showing minor fluctuations after epoch 5. The close alignment between training and validation loss suggests good generalization, though the fluctuations in validation loss warrant further investigation.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/majority_shape_loss_curves.png"}, {"analysis": "This plot compares validation accuracy across the three datasets over 10 epochs. The 'majority_shape' dataset achieves the highest accuracy, stabilizing at around 0.95. The 'length_parity' dataset also performs well, reaching near-perfect accuracy. The 'variety' dataset shows slower improvement, plateauing below the other two datasets, indicating it is the most challenging task for the model.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/val_accuracy_all_datasets.png"}, {"analysis": "This heatmap depicts transfer accuracy between datasets. The diagonal entries indicate high self-transfer accuracy, with the 'length_parity' and 'majority_shape' datasets achieving near-perfect performance. Cross-dataset transfer accuracy is lower, particularly from 'variety' to 'majority_shape' and vice versa, suggesting limited generalization across datasets with different characteristics. This highlights the need for further enhancements in the model's ability to transfer learned representations.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/transfer_accuracy_heatmap.png"}], [{"analysis": "The training and validation loss curves for the variety dataset show a consistent decrease in loss over the epochs. While the training loss decreases steadily, the validation loss plateaus and slightly increases after epoch 8, indicating potential overfitting beyond this point. This suggests that early stopping might be beneficial to prevent overfitting.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/variety_loss_curves.png"}, {"analysis": "The loss curves for the length_parity dataset indicate rapid convergence, with both training and validation loss reaching near-zero values by epoch 2. This suggests that the model is performing exceedingly well on this dataset, possibly due to its simpler structure or lower complexity compared to other datasets.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/length_parity_loss_curves.png"}, {"analysis": "The majority_shape dataset exhibits a steady decrease in training loss, while the validation loss decreases initially but begins to plateau and show minor fluctuations after epoch 4. This could indicate some overfitting or sensitivity to validation data noise. The overall trend suggests that the model is learning effectively but may require regularization to improve generalization.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/majority_shape_loss_curves.png"}, {"analysis": "The validation accuracy plot across datasets demonstrates that the model performs best on the majority_shape dataset, achieving over 90% accuracy by epoch 6. The variety dataset shows a slower increase in accuracy, stabilizing at around 80%. The length_parity dataset achieves rapid convergence with near-perfect accuracy, consistent with its loss curves. This highlights the varying difficulty levels of the datasets and the model's ability to adapt.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/val_accuracy_all_datasets.png"}, {"analysis": "The transfer accuracy heatmap highlights strong transfer performance within the same dataset (diagonal values near 1.0). However, transfer performance across datasets is weaker, particularly from variety to majority_shape and vice versa. This indicates that the learned embeddings are not fully generalizable across datasets, likely due to differences in their underlying patterns or complexities. Enhancing cross-dataset transferability may require additional domain-specific augmentations or regularization techniques.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/transfer_accuracy_heatmap.png"}], [{"analysis": "The plot shows the training and validation loss curves for the 'variety' dataset. Both losses decrease over the epochs, indicating that the model is learning effectively. However, the validation loss starts to flatten and slightly increase after epoch 7, suggesting potential overfitting. This could be mitigated with regularization techniques or early stopping.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/variety_loss_curves.png"}, {"analysis": "This plot demonstrates the training and validation loss for the 'length_parity' dataset. The losses drop sharply within the first two epochs and stabilize at near-zero values. This indicates that the dataset may be relatively simple for the model to learn, potentially due to less complexity or high predictability in the data.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/length_parity_loss_curves.png"}, {"analysis": "The plot illustrates the training and validation loss for the 'majority_shape' dataset. Both losses decrease steadily, but the validation loss shows fluctuations after epoch 4, indicating some instability in the model's generalization. This could be due to noise in the data or suboptimal hyperparameters.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/majority_shape_loss_curves.png"}, {"analysis": "The plot compares validation accuracy across the 'variety,' 'length_parity,' and 'majority_shape' datasets. The 'majority_shape' dataset achieves the highest accuracy early and maintains it, while the 'length_parity' dataset also performs well but stabilizes at a slightly lower accuracy. The 'variety' dataset shows slower improvement and lower overall accuracy, indicating it may be more challenging for the model.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/val_accuracy_all_datasets.png"}, {"analysis": "The heatmap displays transfer accuracy between datasets. The diagonal values are high, confirming that the model performs well when trained and tested on the same dataset. However, off-diagonal values are lower, indicating limited transferability between datasets. For example, the model trained on 'variety' performs poorly on 'majority_shape,' suggesting significant differences in data distributions or task complexity.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/transfer_accuracy_heatmap.png"}], []], "vlm_feedback_summary": ["The plots indicate strong performance in terms of contrastive loss reduction,\ngeneralization (train vs. validation loss), and test metrics. However, auxiliary\nclassification accuracy (ACA) and color-weighted accuracy (CWA) lag slightly\nbehind, suggesting areas for further refinement.", "The plots collectively indicate that the model performs effectively on\nindividual datasets, with minor overfitting in some cases. However, the transfer\naccuracy heatmap reveals challenges in cross-dataset generalization,\nhighlighting an area for improvement in the proposed approach.", "The plots demonstrate the effectiveness of the proposed context-aware\ncontrastive learning framework. Pre-training loss decreases steadily, and fine-\ntuning achieves near-perfect validation accuracy. The full-tune approach\nmarginally outperforms the frozen encoder in most metrics, while the frozen\nencoder excels in ACA, highlighting trade-offs between the two approaches.", "The provided plots effectively convey the performance of the proposed model. The\nloss plot demonstrates efficient learning without overfitting. The accuracy plot\nhighlights high performance on both training and validation datasets but raises\nquestions about class-specific performance. The test metrics chart confirms\nstrong generalization but identifies areas for improvement in color-related\nfeatures and class balance.", "The experimental results indicate effective pre-training and fine-tuning\nprocesses, with strong validation performance and near-SOTA test metrics.\nHowever, there is room for improvement in the CWA metric, which measures color-\nbased accuracy.", "The plots indicate that the proposed context-aware contrastive learning\nframework is effective in reducing pre-training and fine-tuning losses,\nachieving SOTA performance for SWA. However, there is room for improvement in\nCWA and ACA, suggesting that the model may need further refinement to handle\ncolor-related patterns and class imbalances better.", "The plots indicate that the proposed context-aware contrastive learning\nframework is effective in generating meaningful embeddings for the SPR task.\nHowever, the results highlight areas for improvement, particularly in achieving\ngreater stability during fine-tuning and surpassing SOTA metrics. The steady\npretraining loss reduction and consistent test metrics demonstrate the potential\nof the approach, but further optimization is needed to fully realize its\nadvantages.", "The experimental results demonstrate effective optimization and generalization,\nwith the proposed framework surpassing the SOTA benchmarks in SWA and overall\naccuracy. However, further improvements in handling color-specific variations\n(CWA) and class balance (as indicated by ACA) could enhance the model's\nrobustness.", "The plots provide insights into the learning dynamics and performance metrics of\nthe proposed model. Pre-training effectively reduces loss, but its plateau\nsuggests room for optimization. Fine-tuning achieves near-zero loss, and\nvalidation accuracy indicates strong performance, albeit with potential\nimbalances in generalization. Test metrics reveal areas for improvement,\nparticularly in handling color-based variations and contextual accuracy.", "The experimental results demonstrate effective learning and generalization\nwithin individual datasets, particularly for 'length_parity' and\n'majority_shape'. However, there is evidence of overfitting in the 'variety'\ndataset and limited cross-dataset transferability, suggesting areas for\nimprovement in the contrastive learning framework and augmentation strategies.", "The provided plots indicate effective learning on individual datasets, with\nvarying degrees of generalization and transferability. Overfitting is observed\nin some cases, and transfer learning performance across datasets is limited,\nsuggesting areas for further improvement in model design and training\nstrategies.", "The plots reveal that the model performs well on simpler datasets like\n'length_parity' but struggles with more complex or noisy datasets like\n'variety.' Overfitting and fluctuations in validation metrics suggest areas for\nimprovement, such as regularization or better hyperparameter tuning. The\ntransferability heatmap highlights challenges in generalizing across datasets,\nemphasizing the need for more robust feature representations.", "[]"], "exec_time": [57.40973997116089, 12.845139265060425, 83.21900391578674, 40.95579791069031, 48.05019283294678, 52.50675988197327, 27.992645502090454, 40.69790172576904, 38.64865970611572, 12.913165807723999, 11.96517300605774, 12.551639556884766, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["['length_parity'", "'majority_shape']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['length_parity'", "'majority_shape']"], ["['length_parity'", "'majority_shape']"], ["['length_parity']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n\n    # ------------------------------------------------- Plot 1: contrastive pretrain loss\n    try:\n        plt.figure()\n        epochs = range(1, len(rec[\"losses\"][\"pretrain\"]) + 1)\n        plt.plot(epochs, rec[\"losses\"][\"pretrain\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Contrastive Loss\")\n        plt.title(f\"{ds_name}: Contrastive Pre-training Loss\")\n        fn = os.path.join(working_dir, f\"{ds_name}_pretrain_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: fine-tuning losses\n    try:\n        plt.figure()\n        e2 = range(1, len(rec[\"losses\"][\"train\"]) + 1)\n        plt.plot(e2, rec[\"losses\"][\"train\"], \"--\", label=\"train\")\n        plt.plot(e2, rec[\"losses\"][\"val\"], \"-\", label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy Loss\")\n        plt.title(f\"{ds_name}: Train vs Val Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_train_val_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train/val loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation accuracy & ACA\n    try:\n        plt.figure()\n        plt.plot(e2, rec[\"metrics\"][\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(e2, rec[\"metrics\"][\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Validation Metrics\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: test metric summary\n    try:\n        plt.figure()\n        test_metrics = rec[\"test\"]\n        names = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n        scores = [test_metrics[n] for n in names]\n        plt.bar(names, scores, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Test Metrics Summary\\nLeft to Right: Acc, SWA, CWA, ACA\")\n        fn = os.path.join(working_dir, f\"{ds_name}_test_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\nelse:\n    print(\"experiment_data not found or SPR_BENCH key missing.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------------- #\n# Setup & load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()[\"multi_synth_generalization\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    datasets = [\"variety\", \"length_parity\", \"majority_shape\"]\n\n    # 1-3) per-dataset loss curves ---------------------------------------- #\n    for ds in datasets:\n        try:\n            tr_loss = experiment_data[ds][\"losses\"][\"train\"]\n            val_loss = experiment_data[ds][\"losses\"][\"val\"]\n            epochs = range(1, len(tr_loss) + 1)\n\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Validation Loss\")\n            plt.title(f\"{ds} dataset \u2013 Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error plotting loss for {ds}: {e}\")\n            plt.close()\n\n    # 4) aggregate validation accuracy curves ----------------------------- #\n    try:\n        plt.figure()\n        for ds in datasets:\n            val_acc = experiment_data[ds][\"metrics\"][\"val\"]  # accuracy stored here\n            plt.plot(range(1, len(val_acc) + 1), val_acc, label=ds)\n        plt.title(\"Validation Accuracy Across Datasets\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.ylim(0, 1)\n        fname = os.path.join(working_dir, \"val_accuracy_all_datasets.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated accuracy: {e}\")\n        plt.close()\n\n    # 5) transfer accuracy heat-map --------------------------------------- #\n    try:\n        mat = np.zeros((3, 3))\n        for i, src in enumerate(datasets):\n            # diagonal: final validation accuracy\n            mat[i, i] = experiment_data[src][\"metrics\"][\"val\"][-1]\n            for j, tgt in enumerate(datasets):\n                if i == j:\n                    continue\n                key = f\"{src}_to_{tgt}\"\n                mat[i, j] = experiment_data[\"transfer\"][key][\"acc\"]\n        plt.figure()\n        im = plt.imshow(mat, vmin=0, vmax=1, cmap=\"viridis\")\n        plt.colorbar(im)\n        plt.xticks(range(3), datasets, rotation=45)\n        plt.yticks(range(3), datasets)\n        plt.title(\n            \"Left/Top: Source Dataset, Right/Bottom: Target Dataset\\nTransfer Accuracy Heat-map\"\n        )\n        for i in range(3):\n            for j in range(3):\n                plt.text(\n                    j,\n                    i,\n                    f\"{mat[i,j]:.2f}\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\" if mat[i, j] < 0.5 else \"k\",\n                )\n        fname = os.path.join(working_dir, \"transfer_accuracy_heatmap.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting transfer heatmap: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------------- #\n    # Print final metrics\n    final_val_acc = {ds: experiment_data[ds][\"metrics\"][\"val\"][-1] for ds in datasets}\n    print(\"Final Validation Accuracies:\", final_val_acc)\n    print(\"Transfer Accuracy Matrix (rows=source, cols=target):\\n\", mat)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper for safe dict access\ndef safe_get(d, *keys, default=None):\n    for k in keys:\n        d = d.get(k, {})\n    return d if d else default\n\n\nds_key = \"SPR_BENCH\"\nexp_keys = [\"full_tune\", \"frozen_encoder\"]\n\n# 1. Contrastive pre-training loss (only in full_tune)\ntry:\n    losses = safe_get(experiment_data, \"full_tune\", ds_key, \"losses\", \"pretrain\")\n    if losses:\n        plt.figure()\n        plt.plot(range(1, len(losses) + 1), losses, marker=\"o\")\n        plt.title(\"Contrastive Pre-training Loss (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        fname = os.path.join(working_dir, \"spr_bench_pretrain_loss.png\")\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretrain plot: {e}\")\n    plt.close()\n\n# 2. Train vs Val loss (full_tune)\ntry:\n    tr = safe_get(experiment_data, \"full_tune\", ds_key, \"losses\", \"train\")\n    vl = safe_get(experiment_data, \"full_tune\", ds_key, \"losses\", \"val\")\n    if tr and vl:\n        plt.figure()\n        plt.plot(range(1, len(tr) + 1), tr, label=\"Train\")\n        plt.plot(range(1, len(vl) + 1), vl, label=\"Val\")\n        plt.legend()\n        plt.title(\"Full-tune Loss Curves (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        fname = os.path.join(working_dir, \"spr_bench_full_tune_loss_curves.png\")\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating full_tune loss plot: {e}\")\n    plt.close()\n\n# 3. Validation accuracy (full_tune)\ntry:\n    acc = safe_get(experiment_data, \"full_tune\", ds_key, \"metrics\", \"val_acc\")\n    if acc:\n        plt.figure()\n        plt.plot(range(1, len(acc) + 1), acc, marker=\"s\", color=\"green\")\n        plt.title(\"Validation Accuracy (Full-tune, SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        fname = os.path.join(working_dir, \"spr_bench_full_tune_val_acc.png\")\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating val_acc plot: {e}\")\n    plt.close()\n\n# 4. Validation ACA for both settings\ntry:\n    plt.figure()\n    for k in exp_keys:\n        aca = safe_get(experiment_data, k, ds_key, \"metrics\", \"val_aca\")\n        if aca:\n            plt.plot(range(1, len(aca) + 1), aca, label=k)\n    if plt.gca().has_data():\n        plt.legend()\n        plt.title(\"Validation ACA (SPR_BENCH)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"ACA\")\n        fname = os.path.join(working_dir, \"spr_bench_val_aca_comparison.png\")\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ACA plot: {e}\")\n    plt.close()\n\n# 5. Final test metrics bar chart\ntry:\n    metrics = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n    width = 0.35\n    x = np.arange(len(metrics))\n    fig, ax = plt.subplots()\n    plotted = False\n    for i, k in enumerate(exp_keys):\n        vals = [\n            safe_get(experiment_data, k, ds_key, \"test\", m, default=np.nan)\n            for m in metrics\n        ]\n        if not all(np.isnan(vals)):\n            ax.bar(x + i * width, vals, width, label=k)\n            plotted = True\n    if plotted:\n        ax.set_xticks(x + width / 2)\n        ax.set_xticklabels(metrics)\n        ax.set_ylim(0, 1)\n        ax.set_title(\"Test Metrics Comparison (SPR_BENCH)\")\n        ax.legend()\n        fname = os.path.join(working_dir, \"spr_bench_test_metrics_comparison.png\")\n        plt.savefig(fname)\n        print(\"Saved:\", fname)\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating test metrics bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------------------------#\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"no_pretraining\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    ed = None\n\nif ed:\n    epochs = range(1, len(ed[\"losses\"][\"train\"]) + 1)\n\n    # -----------------------------#\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, ed[\"losses\"][\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -----------------------------#\n    # 2) Accuracy curves\n    try:\n        plt.figure()\n        plt.plot(epochs, ed[\"metrics\"][\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_acc\"], label=\"Val Acc\")\n        plt.plot(epochs, ed[\"metrics\"][\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Accuracy & ACA over Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # -----------------------------#\n    # 3) Test metrics bar chart\n    try:\n        test_metrics = ed[\"test\"]\n        names = [\"Acc\", \"SWA\", \"CWA\", \"ACA\"]\n        values = [\n            test_metrics.get(\"acc\", 0),\n            test_metrics.get(\"swa\", 0),\n            test_metrics.get(\"cwa\", 0),\n            test_metrics.get(\"aca\", 0),\n        ]\n        plt.figure()\n        plt.bar(names, values, color=[\"steelblue\", \"orange\", \"green\", \"red\"])\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH: Test Metrics\")\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- Load experiment data ---------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"no_aug_pretraining\"][\"SPR_BENCH\"]\n    losses, metrics, test = exp[\"losses\"], exp[\"metrics\"], exp.get(\"test\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    losses, metrics, test = {}, {}, {}\n\n# ---------------- Figure 1: pre-training loss ---------------- #\ntry:\n    if losses.get(\"pretrain\"):\n        plt.figure()\n        plt.plot(range(1, len(losses[\"pretrain\"]) + 1), losses[\"pretrain\"], marker=\"o\")\n        plt.title(\"SPR_BENCH: Contrastive Pre-training Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_pretrain_loss.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretraining loss plot: {e}\")\n    plt.close()\n\n# ---------------- Figure 2: train & val loss ---------------- #\ntry:\n    if losses.get(\"train\") and losses.get(\"val\"):\n        plt.figure()\n        ep = range(1, len(losses[\"train\"]) + 1)\n        plt.plot(ep, losses[\"train\"], label=\"Train\")\n        plt.plot(ep, losses[\"val\"], label=\"Validation\")\n        plt.title(\"SPR_BENCH: Fine-tune Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_val_loss.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val loss plot: {e}\")\n    plt.close()\n\n# ---------------- Figure 3: val ACC & ACA ---------------- #\ntry:\n    if metrics.get(\"val_acc\") and metrics.get(\"val_aca\"):\n        plt.figure()\n        ep = range(1, len(metrics[\"val_acc\"]) + 1)\n        plt.plot(ep, metrics[\"val_acc\"], label=\"Val Acc\")\n        plt.plot(ep, metrics[\"val_aca\"], label=\"Val ACA\")\n        plt.title(\"SPR_BENCH: Validation Accuracy & ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_acc_aca.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val metrics plot: {e}\")\n    plt.close()\n\n# ---------------- Figure 4: test metric bars ---------------- #\ntry:\n    if test:\n        labels = [\"ACC\", \"SWA\", \"CWA\", \"ACA\"]\n        values = [test.get(\"acc\"), test.get(\"swa\"), test.get(\"cwa\"), test.get(\"aca\")]\n        if all(v is not None for v in values):\n            plt.figure()\n            plt.bar(labels, values, color=\"skyblue\")\n            plt.ylim(0, 1)\n            plt.title(\"SPR_BENCH: Test Metrics Summary\")\n            for idx, v in enumerate(values):\n                plt.text(idx, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metrics plot: {e}\")\n    plt.close()\n\n# ---------------- Print test metrics ---------------- #\nif test:\n    print(\"--- Test metrics ---\")\n    for k, v in test.items():\n        if isinstance(v, (float, int)):\n            print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nvariant = \"mask_only\"\ndataset = \"SPR_BENCH\"\nrec = experiment_data.get(variant, {}).get(dataset, {})\n\n\n# Helper to safely grab arrays\ndef get(path, default=None):\n    d = rec\n    for k in path.split(\".\"):\n        d = d.get(k, {})\n    return d if isinstance(d, (list, tuple)) else default\n\n\n# 1) Contrastive pretraining loss\ntry:\n    pre_loss = get(\"losses.pretrain\", [])\n    if pre_loss:\n        plt.figure()\n        plt.plot(range(1, len(pre_loss) + 1), pre_loss, marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dataset} \u2013 Contrastive Pre-training Loss\", fontsize=12)\n        plt.suptitle(\"Mask-only augmentation (Left: Epochs, Right: Loss)\", fontsize=9)\n        fname = os.path.join(working_dir, f\"{dataset}_pretrain_loss.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating pretrain plot: {e}\")\n    plt.close()\n\n# 2) Fine-tuning train/val loss\ntry:\n    tr_loss = get(\"losses.train\", [])\n    val_loss = get(\"losses.val\", [])\n    if tr_loss and val_loss:\n        plt.figure()\n        epochs = range(1, len(tr_loss) + 1)\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.title(f\"{dataset} \u2013 Fine-tuning Loss Curves\", fontsize=12)\n        plt.suptitle(\"Left: Train Loss, Right: Validation Loss\", fontsize=9)\n        fname = os.path.join(working_dir, f\"{dataset}_train_val_loss.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating train/val loss plot: {e}\")\n    plt.close()\n\n# 3) Validation accuracy & ACA\ntry:\n    val_acc = get(\"metrics.val_acc\", [])\n    val_aca = get(\"metrics.val_aca\", [])\n    if val_acc and val_aca:\n        plt.figure()\n        epochs = range(1, len(val_acc) + 1)\n        plt.plot(epochs, val_acc, label=\"Val Acc\")\n        plt.plot(epochs, val_aca, label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric\")\n        plt.ylim(0, 1)\n        plt.legend()\n        plt.title(f\"{dataset} \u2013 Validation Metrics\", fontsize=12)\n        plt.suptitle(\"Left: Accuracy, Right: ACA\", fontsize=9)\n        fname = os.path.join(working_dir, f\"{dataset}_val_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metrics plot: {e}\")\n    plt.close()\n\n# 4) Test metrics bar chart\ntry:\n    test = rec.get(\"test\", {})\n    labels = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n    vals = [test.get(k, np.nan) for k in labels]\n    if any(np.isfinite(vals)):\n        plt.figure()\n        plt.bar(labels, vals)\n        plt.ylim(0, 1)\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n        plt.title(f\"{dataset} \u2013 Test Metrics Summary\", fontsize=12)\n        plt.suptitle(\"Acc, Shape-weighted Acc, Color-weighted Acc, ACA\", fontsize=9)\n        fname = os.path.join(working_dir, f\"{dataset}_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n        print(\"Test metrics:\", {k: round(test.get(k, np.nan), 4) for k in labels})\nexcept Exception as e:\n    print(f\"Error creating test metrics plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------------------------#\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    # Convenient handles\n    model_key = \"NoRecurrentEncoder\"\n    ds_key = \"SPR_BENCH\"\n    try:\n        data = experiment_data[model_key][ds_key]\n        losses = data[\"losses\"]\n        metrics = data[\"metrics\"]\n        test = data[\"test\"]\n    except Exception as e:\n        print(f\"Error extracting keys: {e}\")\n        data = None\n\n    # -------------------------------------------------------------------------#\n    # 1) Loss curves\n    try:\n        plt.figure()\n        x_pre = range(1, len(losses[\"pretrain\"]) + 1)\n        x_ft = range(1, len(losses[\"train\"]) + 1)\n        plt.plot(x_pre, losses[\"pretrain\"], label=\"Pretrain loss\")\n        plt.plot(x_ft, losses[\"train\"], label=\"Train loss\")\n        plt.plot(x_ft, losses[\"val\"], label=\"Val loss\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH: Loss Curves\\nLeft: Pretraining, Right: Fine-tuning\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # -------------------------------------------------------------------------#\n    # 2) Validation metrics curves\n    try:\n        plt.figure()\n        x_ft = range(1, len(metrics[\"val_acc\"]) + 1)\n        plt.plot(x_ft, metrics[\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(x_ft, metrics[\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Score\")\n        plt.title(\"SPR_BENCH: Validation Metrics Over Fine-tuning\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_validation_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # -------------------------------------------------------------------------#\n    # 3) Test metrics bar chart\n    try:\n        plt.figure()\n        names = [\"Acc\", \"SWA\", \"CWA\", \"ACA\"]\n        values = [test.get(\"acc\"), test.get(\"swa\"), test.get(\"cwa\"), test.get(\"aca\")]\n        plt.bar(names, values, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH: Test Metrics Summary\")\n        for i, v in enumerate(values):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics bar chart: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"shuffle_only\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp:\n    losses = exp[\"losses\"]\n    metrics = exp[\"metrics\"]\n    test = exp[\"test\"]\n\n    # --------------------------------------------------------------#\n    # 1) Pre-training loss\n    try:\n        plt.figure()\n        plt.plot(losses[\"pretrain\"], marker=\"o\", label=\"Pretrain CL Loss\")\n        plt.title(\"SPR_BENCH \u2013 Contrastive Pre-training Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_pretrain_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------#\n    # 2) Training vs Validation loss\n    try:\n        plt.figure()\n        plt.plot(losses[\"train\"], label=\"Train Loss\")\n        plt.plot(losses[\"val\"], label=\"Validation Loss\")\n        plt.title(\"SPR_BENCH \u2013 Fine-tuning Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_val_loss.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train/val loss plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------#\n    # 3) Validation Accuracy & ACA\n    try:\n        plt.figure()\n        plt.plot(metrics[\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(metrics[\"val_aca\"], label=\"Val ACA\")\n        plt.title(\"SPR_BENCH \u2013 Validation Accuracy (Left: Acc, Right: ACA)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_acc_aca.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating val acc/aca plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------#\n    # 4) Final Test Metrics\n    try:\n        plt.figure()\n        names = [\"Acc\", \"SWA\", \"CWA\", \"ACA\"]\n        vals = [test.get(\"acc\"), test.get(\"swa\"), test.get(\"cwa\"), test.get(\"aca\")]\n        plt.bar(names, vals)\n        plt.ylim(0, 1)\n        plt.title(\"SPR_BENCH \u2013 Test Metrics Overview\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\n\nprint(\"Plotting complete.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nrun = experiment_data.get(\"MLM_pretrain\", {}).get(\"SPR_BENCH\", {})\n\nlosses = run.get(\"losses\", {})\nmetrics = run.get(\"metrics\", {})\ntest_res = run.get(\"test\", {})\n\n# ----------------- plot 1: loss curves ------------------\ntry:\n    plt.figure()\n    if \"pretrain\" in losses:\n        plt.plot(\n            range(1, len(losses[\"pretrain\"]) + 1),\n            losses[\"pretrain\"],\n            label=\"Pre-train MLM\",\n        )\n    if \"train\" in losses:\n        # offset fine-tuning epochs so the x-axis is continuous\n        offset = len(losses.get(\"pretrain\", []))\n        xs = list(range(1 + offset, len(losses[\"train\"]) + 1 + offset))\n        plt.plot(xs, losses[\"train\"], label=\"Fine-tune Train\")\n    if \"val\" in losses:\n        offset = len(losses.get(\"pretrain\", []))\n        xs = list(range(1 + offset, len(losses[\"val\"]) + 1 + offset))\n        plt.plot(xs, losses[\"val\"], label=\"Fine-tune Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Loss Curves (MLM & Fine-tune)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ----------------- plot 2: validation metrics -----------\ntry:\n    plt.figure()\n    if \"val_acc\" in metrics:\n        plt.plot(\n            range(1, len(metrics[\"val_acc\"]) + 1), metrics[\"val_acc\"], label=\"Val ACC\"\n        )\n    if \"val_aca\" in metrics:\n        plt.plot(\n            range(1, len(metrics[\"val_aca\"]) + 1), metrics[\"val_aca\"], label=\"Val ACA\"\n        )\n    plt.xlabel(\"Fine-tune Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Validation Accuracy Metrics\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_metrics.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val metric plot: {e}\")\n    plt.close()\n\n# ----------------- plot 3: test metrics -----------------\ntry:\n    plt.figure()\n    keys = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n    vals = [test_res[k] for k in keys if k in test_res]\n    shown_keys = [k.upper() for k in keys if k in test_res]\n    plt.bar(shown_keys, vals)\n    plt.ylim(0, 1)\n    plt.title(\"SPR_BENCH: Test Metrics\")\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metric plot: {e}\")\n    plt.close()\n\n# ----------------- print exact test numbers -------------\nif test_res:\n    for k, v in test_res.items():\n        if isinstance(v, (int, float)):\n            print(f\"{k.upper()}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------------- #\n# Setup & load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()[\"multi_synth_generalization\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    datasets = [\"variety\", \"length_parity\", \"majority_shape\"]\n\n    # 1-3) per-dataset loss curves ---------------------------------------- #\n    for ds in datasets:\n        try:\n            tr_loss = experiment_data[ds][\"losses\"][\"train\"]\n            val_loss = experiment_data[ds][\"losses\"][\"val\"]\n            epochs = range(1, len(tr_loss) + 1)\n\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Validation Loss\")\n            plt.title(f\"{ds} dataset \u2013 Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error plotting loss for {ds}: {e}\")\n            plt.close()\n\n    # 4) aggregate validation accuracy curves ----------------------------- #\n    try:\n        plt.figure()\n        for ds in datasets:\n            val_acc = experiment_data[ds][\"metrics\"][\"val\"]  # accuracy stored here\n            plt.plot(range(1, len(val_acc) + 1), val_acc, label=ds)\n        plt.title(\"Validation Accuracy Across Datasets\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.ylim(0, 1)\n        fname = os.path.join(working_dir, \"val_accuracy_all_datasets.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated accuracy: {e}\")\n        plt.close()\n\n    # 5) transfer accuracy heat-map --------------------------------------- #\n    try:\n        mat = np.zeros((3, 3))\n        for i, src in enumerate(datasets):\n            # diagonal: final validation accuracy\n            mat[i, i] = experiment_data[src][\"metrics\"][\"val\"][-1]\n            for j, tgt in enumerate(datasets):\n                if i == j:\n                    continue\n                key = f\"{src}_to_{tgt}\"\n                mat[i, j] = experiment_data[\"transfer\"][key][\"acc\"]\n        plt.figure()\n        im = plt.imshow(mat, vmin=0, vmax=1, cmap=\"viridis\")\n        plt.colorbar(im)\n        plt.xticks(range(3), datasets, rotation=45)\n        plt.yticks(range(3), datasets)\n        plt.title(\n            \"Left/Top: Source Dataset, Right/Bottom: Target Dataset\\nTransfer Accuracy Heat-map\"\n        )\n        for i in range(3):\n            for j in range(3):\n                plt.text(\n                    j,\n                    i,\n                    f\"{mat[i,j]:.2f}\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\" if mat[i, j] < 0.5 else \"k\",\n                )\n        fname = os.path.join(working_dir, \"transfer_accuracy_heatmap.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting transfer heatmap: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------------- #\n    # Print final metrics\n    final_val_acc = {ds: experiment_data[ds][\"metrics\"][\"val\"][-1] for ds in datasets}\n    print(\"Final Validation Accuracies:\", final_val_acc)\n    print(\"Transfer Accuracy Matrix (rows=source, cols=target):\\n\", mat)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------------- #\n# Setup & load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()[\"multi_synth_generalization\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    datasets = [\"variety\", \"length_parity\", \"majority_shape\"]\n\n    # 1-3) per-dataset loss curves ---------------------------------------- #\n    for ds in datasets:\n        try:\n            tr_loss = experiment_data[ds][\"losses\"][\"train\"]\n            val_loss = experiment_data[ds][\"losses\"][\"val\"]\n            epochs = range(1, len(tr_loss) + 1)\n\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Validation Loss\")\n            plt.title(f\"{ds} dataset \u2013 Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error plotting loss for {ds}: {e}\")\n            plt.close()\n\n    # 4) aggregate validation accuracy curves ----------------------------- #\n    try:\n        plt.figure()\n        for ds in datasets:\n            val_acc = experiment_data[ds][\"metrics\"][\"val\"]  # accuracy stored here\n            plt.plot(range(1, len(val_acc) + 1), val_acc, label=ds)\n        plt.title(\"Validation Accuracy Across Datasets\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.ylim(0, 1)\n        fname = os.path.join(working_dir, \"val_accuracy_all_datasets.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated accuracy: {e}\")\n        plt.close()\n\n    # 5) transfer accuracy heat-map --------------------------------------- #\n    try:\n        mat = np.zeros((3, 3))\n        for i, src in enumerate(datasets):\n            # diagonal: final validation accuracy\n            mat[i, i] = experiment_data[src][\"metrics\"][\"val\"][-1]\n            for j, tgt in enumerate(datasets):\n                if i == j:\n                    continue\n                key = f\"{src}_to_{tgt}\"\n                mat[i, j] = experiment_data[\"transfer\"][key][\"acc\"]\n        plt.figure()\n        im = plt.imshow(mat, vmin=0, vmax=1, cmap=\"viridis\")\n        plt.colorbar(im)\n        plt.xticks(range(3), datasets, rotation=45)\n        plt.yticks(range(3), datasets)\n        plt.title(\n            \"Left/Top: Source Dataset, Right/Bottom: Target Dataset\\nTransfer Accuracy Heat-map\"\n        )\n        for i in range(3):\n            for j in range(3):\n                plt.text(\n                    j,\n                    i,\n                    f\"{mat[i,j]:.2f}\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\" if mat[i, j] < 0.5 else \"k\",\n                )\n        fname = os.path.join(working_dir, \"transfer_accuracy_heatmap.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting transfer heatmap: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------------- #\n    # Print final metrics\n    final_val_acc = {ds: experiment_data[ds][\"metrics\"][\"val\"][-1] for ds in datasets}\n    print(\"Final Validation Accuracies:\", final_val_acc)\n    print(\"Transfer Accuracy Matrix (rows=source, cols=target):\\n\", mat)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------------- #\n# Setup & load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()[\"multi_synth_generalization\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    datasets = [\"variety\", \"length_parity\", \"majority_shape\"]\n\n    # 1-3) per-dataset loss curves ---------------------------------------- #\n    for ds in datasets:\n        try:\n            tr_loss = experiment_data[ds][\"losses\"][\"train\"]\n            val_loss = experiment_data[ds][\"losses\"][\"val\"]\n            epochs = range(1, len(tr_loss) + 1)\n\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Validation Loss\")\n            plt.title(f\"{ds} dataset \u2013 Training vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error plotting loss for {ds}: {e}\")\n            plt.close()\n\n    # 4) aggregate validation accuracy curves ----------------------------- #\n    try:\n        plt.figure()\n        for ds in datasets:\n            val_acc = experiment_data[ds][\"metrics\"][\"val\"]  # accuracy stored here\n            plt.plot(range(1, len(val_acc) + 1), val_acc, label=ds)\n        plt.title(\"Validation Accuracy Across Datasets\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.ylim(0, 1)\n        fname = os.path.join(working_dir, \"val_accuracy_all_datasets.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated accuracy: {e}\")\n        plt.close()\n\n    # 5) transfer accuracy heat-map --------------------------------------- #\n    try:\n        mat = np.zeros((3, 3))\n        for i, src in enumerate(datasets):\n            # diagonal: final validation accuracy\n            mat[i, i] = experiment_data[src][\"metrics\"][\"val\"][-1]\n            for j, tgt in enumerate(datasets):\n                if i == j:\n                    continue\n                key = f\"{src}_to_{tgt}\"\n                mat[i, j] = experiment_data[\"transfer\"][key][\"acc\"]\n        plt.figure()\n        im = plt.imshow(mat, vmin=0, vmax=1, cmap=\"viridis\")\n        plt.colorbar(im)\n        plt.xticks(range(3), datasets, rotation=45)\n        plt.yticks(range(3), datasets)\n        plt.title(\n            \"Left/Top: Source Dataset, Right/Bottom: Target Dataset\\nTransfer Accuracy Heat-map\"\n        )\n        for i in range(3):\n            for j in range(3):\n                plt.text(\n                    j,\n                    i,\n                    f\"{mat[i,j]:.2f}\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\" if mat[i, j] < 0.5 else \"k\",\n                )\n        fname = os.path.join(working_dir, \"transfer_accuracy_heatmap.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting transfer heatmap: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------------- #\n    # Print final metrics\n    final_val_acc = {ds: experiment_data[ds][\"metrics\"][\"val\"][-1] for ds in datasets}\n    print(\"Final Validation Accuracies:\", final_val_acc)\n    print(\"Transfer Accuracy Matrix (rows=source, cols=target):\\n\", mat)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------------------------------------------------------- #\n# Setup & load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# List of experiment result files (relative to $AI_SCIENTIST_ROOT)\nexperiment_data_path_list = [\n    \"experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d88cc6ba07294bf19dfd3113e3a84a69_proc_3039480/experiment_data.npy\",\n    \"experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7fd20bec3327401a812995812cb62863_proc_3039482/experiment_data.npy\",\n    \"experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_3c5d711f32d94e318a8bdd81f7382098_proc_3039483/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_p = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_p, allow_pickle=True).item()\n        # Keep only the relevant sub-dict\n        if \"multi_synth_generalization\" in exp:\n            all_experiment_data.append(exp[\"multi_synth_generalization\"])\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nif len(all_experiment_data) == 0:\n    print(\"No experiment data could be loaded \u2013 aborting.\")\nelse:\n    # Determine datasets that exist in every run\n    datasets = [\"variety\", \"length_parity\", \"majority_shape\"]\n    n_runs = len(all_experiment_data)\n\n    # --------------------------------------------------------------- #\n    # 1-3) Dataset-specific loss curves (mean \u00b1 stderr)\n    for ds in datasets:\n        try:\n            # Collect curves\n            tr_mat, val_mat = [], []\n            for exp in all_experiment_data:\n                if ds not in exp:\n                    continue\n                tr_mat.append(exp[ds][\"losses\"][\"train\"])\n                val_mat.append(exp[ds][\"losses\"][\"val\"])\n            tr_mat = np.array(tr_mat)\n            val_mat = np.array(val_mat)\n            if tr_mat.size == 0 or val_mat.size == 0:\n                raise ValueError(f\"No data for {ds}\")\n\n            mean_tr, se_tr = tr_mat.mean(0), tr_mat.std(0) / np.sqrt(tr_mat.shape[0])\n            mean_val, se_val = val_mat.mean(0), val_mat.std(0) / np.sqrt(\n                val_mat.shape[0]\n            )\n            epochs = np.arange(1, len(mean_tr) + 1)\n\n            plt.figure()\n            plt.plot(epochs, mean_tr, label=\"Train Loss (mean)\", color=\"tab:blue\")\n            plt.fill_between(\n                epochs, mean_tr - se_tr, mean_tr + se_tr, alpha=0.3, color=\"tab:blue\"\n            )\n            plt.plot(\n                epochs, mean_val, label=\"Validation Loss (mean)\", color=\"tab:orange\"\n            )\n            plt.fill_between(\n                epochs,\n                mean_val - se_val,\n                mean_val + se_val,\n                alpha=0.3,\n                color=\"tab:orange\",\n            )\n            plt.title(f\"{ds} dataset \u2013 Training vs Validation Loss (mean \u00b1 SE)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds}_loss_curves_mean_se.png\")\n            plt.savefig(fname, bbox_inches=\"tight\")\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------- #\n    # 4) Aggregate validation-accuracy curves across datasets\n    try:\n        plt.figure()\n        for ds in datasets:\n            val_mat = []\n            for exp in all_experiment_data:\n                if ds in exp:\n                    val_mat.append(exp[ds][\"metrics\"][\"val\"])\n            val_mat = np.array(val_mat)\n            if val_mat.size == 0:\n                continue\n            mean_val = val_mat.mean(0)\n            se_val = val_mat.std(0) / np.sqrt(val_mat.shape[0])\n            epochs = np.arange(1, len(mean_val) + 1)\n            plt.plot(epochs, mean_val, label=f\"{ds} (mean)\")\n            plt.fill_between(epochs, mean_val - se_val, mean_val + se_val, alpha=0.25)\n        plt.title(\"Validation Accuracy Across Datasets (mean \u00b1 SE)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.ylim(0, 1)\n        plt.legend()\n        fname = os.path.join(working_dir, \"val_accuracy_all_datasets_mean_se.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated accuracy plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------- #\n    # 5) Transfer accuracy heat-map (mean across runs)\n    try:\n        n_ds = len(datasets)\n        mat_runs = np.zeros((n_runs, n_ds, n_ds))\n        for run_idx, exp in enumerate(all_experiment_data):\n            for i, src in enumerate(datasets):\n                # diagonal: final val acc on the same dataset\n                if src in exp:\n                    mat_runs[run_idx, i, i] = exp[src][\"metrics\"][\"val\"][-1]\n                for j, tgt in enumerate(datasets):\n                    if i == j:\n                        continue\n                    key = f\"{src}_to_{tgt}\"\n                    if \"transfer\" in exp and key in exp[\"transfer\"]:\n                        mat_runs[run_idx, i, j] = exp[\"transfer\"][key][\"acc\"]\n        mat_mean = mat_runs.mean(0)\n        mat_se = mat_runs.std(0) / np.sqrt(n_runs)\n\n        plt.figure()\n        im = plt.imshow(mat_mean, vmin=0, vmax=1, cmap=\"viridis\")\n        plt.colorbar(im)\n        plt.xticks(range(n_ds), datasets, rotation=45)\n        plt.yticks(range(n_ds), datasets)\n        plt.title(\n            \"Transfer Accuracy Heat-map (mean values)\\nLeft/Top: Source, Right/Bottom: Target\"\n        )\n        # annotate with mean\u00b1SE\n        for i in range(n_ds):\n            for j in range(n_ds):\n                txt = f\"{mat_mean[i, j]:.2f}\"\n                plt.text(\n                    j,\n                    i,\n                    txt,\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\" if mat_mean[i, j] < 0.5 else \"k\",\n                )\n        fname = os.path.join(working_dir, \"transfer_accuracy_heatmap_mean.png\")\n        plt.savefig(fname, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating transfer heat-map: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------------- #\n    # Console summary of final validation accuracies\n    try:\n        final_stats = {}\n        for ds in datasets:\n            finals = []\n            for exp in all_experiment_data:\n                if ds in exp:\n                    finals.append(exp[ds][\"metrics\"][\"val\"][-1])\n            finals = np.array(finals)\n            if finals.size > 0:\n                final_stats[ds] = (finals.mean(), finals.std())\n        print(\"Final Validation Accuracies (mean \u00b1 std):\")\n        for ds, (m, s) in final_stats.items():\n            print(f\"  {ds}: {m:.3f} \u00b1 {s:.3f}\")\n    except Exception as e:\n        print(f\"Error printing final stats: {e}\")\n"], "plot_plan": ["We load the saved experiment_data.npy, focus on the single dataset key \u201cSPR_BENCH\u201d, and safely exit if it is missing or malformed. For visualization we create up to four standard plots: (1) contrastive-pretraining loss over epochs, (2) fine-tuning train vs validation loss, (3) validation accuracy and ACA curves, and (4) a bar chart summarizing the four test metrics (acc, swa, cwa, aca). Each figure is wrapped in its own try-except so that a failure in one plot does not affect the rest, and every figure is closed after saving. File names include the dataset name and plot description for clarity. No synthetic data are generated; only arrays already present in experiment_data are used, and we subsample nothing because the lists are short (<10). Basic matplotlib is used without extra styles, with clear axis labels and informative titles/subtitles. All plots are written to the previously created \u201cworking\u201d directory. Finally, if everything succeeds the script prints the absolute paths of the saved figures.", "We will load the saved experiment_data.npy, loop through the three synthetic datasets, and for each generate separate line plots of training/validation loss. Then we aggregate their validation accuracies into one shared curve plot, and finally construct a 3\u00d73 heat-map of self and cross-dataset transfer accuracies (using last-epoch validation accuracy on the diagonal and stored transfer accuracies off-diagonal). All figures are produced inside individual try-except blocks, saved to working_dir with descriptive names, and closed afterwards. After plotting we print the final validation accuracies and the transfer matrix to give immediate numerical feedback.", "We first load experiment_data.npy, extracted as a nested dict. For each experiment setting (full_tune, frozen_encoder) under the SPR_BENCH dataset, we read the recorded lists: pretrain loss, train loss, val loss, val_acc and val_aca. We prepare up to five plots: (1) contrastive-pretraining loss vs. epoch, (2) fine-tuning train/val loss curves, (3) validation accuracy vs. epoch, (4) validation ACA vs. epoch, and (5) a bar chart comparing final test metrics (Acc, SWA, CWA, ACA) between full_tune and frozen_encoder. Each figure is wrapped in a try-except block, saved with descriptive filenames in working_dir, titled clearly, and closed afterwards. We limit epochs plotted to those stored, use basic matplotlib, and never fabricate data. Finally, the script prints the path of every saved figure.", "We load the saved experiment_data, grab the \u201cno_pretraining/SPR_BENCH\u201d subsection, and pull out the arrays for train/val loss, train accuracy, val accuracy, and val ACA. Epoch indices are simply range(len(losses)). We then generate three separate figures: (1) train vs validation loss across epochs, (2) train accuracy vs validation accuracy vs validation ACA across epochs, and (3) a bar chart of the four reported test metrics (ACC, SWA, CWA, ACA). Each figure is wrapped in its own try-except block, given a clear title that names the dataset and plot type, and saved to working_dir with descriptive filenames like \u201cSPR_BENCH_loss_curves.png\u201d. All figures are explicitly closed in both success and failure cases. No other plots are produced so we stay under the five-figure cap. All plotting relies solely on the arrays present in experiment_data.npy without inventing new data.", "We load the saved experiment_data, isolate the \u201cno_aug_pretraining / SPR_BENCH\u201d branch, and grab the lists for pre-training losses, training/validation losses, validation accuracy, and ACA.  Four figures are generated (each in its own try-except): (1) contrastive pre-training loss over epochs, (2) fine-tune training vs. validation loss, (3) validation accuracy & ACA, and (4) a bar chart summarising final test metrics (ACC, SWA, CWA, ACA).  Each figure is titled clearly, saved to working_dir with descriptive filenames, and then closed.  We plot at most one point per epoch, so no more than 5 figures are needed.  After plotting we print the test metrics to stdout for quick inspection.  All plotting uses plain matplotlib, no fabricated data, and will silently skip plots if the required data array is empty or missing.  This satisfies the visualization and robustness requirements while keeping the code concise and self-contained.", "We will load the saved experiment_data.npy, extract the only variant (\u201cmask_only\u201d) and dataset (\u201cSPR_BENCH\u201d), then retrieve its stored arrays.  \nFour independent figures are produced inside separate try-except blocks: (1) contrastive pre-training loss over epochs; (2) fine-tuning train/validation loss over epochs; (3) validation accuracy and ACA curves; (4) a bar plot summarising test-set metrics (acc, swa, cwa, aca).  \nEach figure uses clear titles that mention the dataset and an explanatory subtitle; filenames mirror this description for easy retrieval.  \nThe code obeys the plotting guidelines: imports are as requested, plots are closed after saving, and only recorded values are visualised\u2014no fabricated data.  \nA maximum of four plots (below the five-figure limit) keeps the output concise.  \nAll plots are deposited in working_dir, which is created if missing.  \nAfter plotting, the script also prints the test metrics to stdout for quick inspection.  \nEverything is wrapped in a single, concise script that can be run as-is.", "We will first load the stored experiment_data.npy file and extract the results for model \u201cNoRecurrentEncoder\u201d on the \u201cSPR_BENCH\u201d dataset. Three visualizations will be produced: (1) a line plot showing contrastive-pretraining loss and fine-tuning train/val losses over their respective epochs; (2) a line plot displaying validation accuracy and validation ACA across fine-tuning epochs; and (3) a bar chart summarising final test metrics (Acc, SWA, CWA, ACA). Each figure is generated inside its own try-except block, saved to working_dir with descriptive names, given clear titles and legends, plotted only with real logged values, and the figure is always closed afterward. We strictly plot at most these three figures, satisfying the \u22645-figure rule. All matplotlib calls use the default style, and every plot is self-contained and reproducible. The code begins with the required imports and ensures working_dir exists before saving. Any loading or plotting error will be caught, printed, and the figure closed gracefully.", "We load experiment_data.npy from the working directory, pull out the single experiment located at shuffle_only \u2192 SPR_BENCH, and extract the stored loss and metric arrays. Four plots are produced: (1) contrastive-pretraining loss across epochs, (2) fine-tuning training vs. validation loss, (3) validation accuracy vs. ACA, and (4) a bar chart of final test metrics (Acc, SWA, CWA, ACA). Each figure is built inside its own try-except block, given a clear title/sub-title that includes the dataset name (\u201cSPR_BENCH\u201d), saved with a descriptive filename, and then closed to free memory. The code respects the five-figure limit and never fabricates data\u2014everything comes directly from experiment_data.npy. All plots are saved under working_dir, and any loading or plotting failure is reported without stopping subsequent plots. Finally, the script prints a short confirmation once all figures have been attempted.", "We will load experiment_data.npy, extract the dictionary under the single run MLM_pretrain \u2192 SPR_BENCH, and pull out three groups of values: (1) loss trajectories for pre-training, fine-tuning-train, and fine-tuning-validation; (2) validation metrics per epoch (accuracy and ACA); and (3) the final test metrics. We create at most three figures: one line plot comparing the three loss curves, one line plot comparing the two validation metrics, and one bar chart summarising the four test metrics (ACC, SWA, CWA, ACA). Each figure is wrapped in its own try-except block, saved into working_dir with descriptive filenames, given clear titles and closed afterwards. We respect the requirement not to fabricate data by only plotting keys that exist in the loaded dict. After plotting, the script prints the exact numeric test metrics for quick inspection. The entire routine is enclosed in a concise script starting with the prescribed imports and path setup.", null, null, null, null], "ablation_name": [null, "Multi-Synthetic Dataset Generalization Ablation", "Frozen-Encoder Fine-Tuning Ablation", "No-Pretraining Ablation", "No-Augmentation Pre-training Ablation", "Masking-Only Augmentation Ablation", "No-Recurrent Encoder (Bag-of-Tokens) Ablation", "Shuffle-Only Augmentation Ablation", "Masked-Language-Modeling (MLM) Pre-training Ablation", null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script loads working/experiment_data.npy, iterates over every top-level\ndataset (e.g., \u201cSPR_BENCH\u201d), and for each metric series it reports the best\nvalue (minimum for losses, maximum for accuracies) while single-valued test\nstatistics are printed directly. Every printout begins with the dataset name\nfollowed by explicit, self-describing metric labels such as \u201cbest validation\naccuracy\u201d or \u201ctest color-weighted accuracy.\u201d The code runs immediately when\nexecuted without any special entry point.", "The script will load the saved numpy dictionary, iterate over each of the three\nrule-based datasets, and pull out the final values recorded during fine-tuning.\nSpecifically, it prints the last training loss, the last validation loss, the\nlast validation accuracy, and recomputes the test accuracy from the stored\npredictions and ground-truth labels. Each metric is clearly labeled so the\noutput is self-explanatory, and the code runs immediately when executed.", "The script will load the saved numpy dictionary, walk through every experiment\nvariant (e.g., \u201cfull_tune\u201d, \u201cfrozen_encoder\u201d) and its contained dataset\n(\u201cSPR_BENCH\u201d), pull out the lists of losses/metrics, select the final entry in\neach training-time list, and directly read the scalar values stored for test\nperformance. It then prints the dataset name once, followed by clear, explicit\nmetric names and their chosen values, formatted to four decimal places. The code\nruns immediately when executed, with no special entry point or plotting.", "The script will load the numpy file from the \u201cworking\u201d directory, convert it\nback into a Python dictionary, then iterate through every top-level experiment\nand nested dataset.   For each dataset it will pick the last value from every\ntraining/validation list (these represent the final epoch) and directly read the\nscalar values stored under the \u201ctest\u201d key.   It will print the dataset name\nonce, followed by clearly labelled lines for each metric: final training loss,\nfinal validation loss, final training accuracy, final validation accuracy, final\nvalidation ACA, and the five test metrics.   No figures are produced and all\ncode is at module scope so it runs immediately when executed.", "The script below immediately loads the saved NumPy file, traverses its nested\ndictionary, and prints the relevant metrics.  For losses it reports the minimum\n(best) value observed, while for accuracy-like measures it reports the maximum.\nTest-set metrics are printed as stored because they are already final.  The code\nfollows the required structure\u2014nothing is placed under an `if __name__ ==\n\"__main__\":` guard, it executes on import, and it produces only textual output.", "The script will load experiment_data.npy from the working directory, traverse\nits nested dictionary structure, and for every dataset (e.g., \u201cSPR_BENCH\u201d)\ncompute the best value for each stored metric (minimum for any \u201closs\u201d entry and\nmaximum for all others). It then prints the dataset name followed by clearly-\nlabeled metric names such as \u201cpre-training loss,\u201d \u201cvalidation accuracy,\u201d or\n\u201ctest shape-weighted accuracy,\u201d together with their best/final values. The code\nis placed at global scope so it executes immediately when run.", "The script below loads the saved NumPy dictionary, drills down to the nested\nsub-dictionaries that store losses and evaluation metrics, selects the final (or\nbest, where appropriate) value for each metric, and prints them with clear,\ndescriptive labels. All execution happens at the top level so the file runs\nimmediately when executed.", "The script will load the serialized numpy dictionary, walk through every\nexperiment and dataset it contains, and then inspect each category of metrics.\nFor lists (e.g., training/validation losses or accuracies) it will report the\n\u201cbest\u201d value \u2013 minimum for losses and maximum for accuracy\u2013type scores; scalar\ntest-set results are already final and will be shown unchanged.   Each block of\noutput begins with the dataset\u2019s name, followed by clearly labelled lines such\nas \u201cbest pre-training loss\u201d or \u201ctest color-weighted accuracy,\u201d ensuring no\nambiguous labels like \u201ctrain\u201d or \u201cval\u201d appear on their own.   The code is\nentirely in the global scope so it executes immediately when run.", "The script loads the saved experiment_data.npy from the \u201cworking\u201d directory,\ndrills down to every dataset inside every experiment entry, and prints a concise\nsummary.  For each dataset it reports   \u2022 the final MLM pre-training loss,   \u2022\nthe final supervised training loss,   \u2022 the best (minimum) validation loss,   \u2022\nthe best validation accuracy and ACA,   \u2022 and every test-time metric stored.\nAll labels are printed with explicit names, and the code runs immediately\nwithout needing an entry-point guard.", "The script will load the saved numpy dictionary, iterate over each of the three\nrule-based datasets, and pull out the final values recorded during fine-tuning.\nSpecifically, it prints the last training loss, the last validation loss, the\nlast validation accuracy, and recomputes the test accuracy from the stored\npredictions and ground-truth labels. Each metric is clearly labeled so the\noutput is self-explanatory, and the code runs immediately when executed.", "The script will load the saved numpy dictionary, iterate over each of the three\nrule-based datasets, and pull out the final values recorded during fine-tuning.\nSpecifically, it prints the last training loss, the last validation loss, the\nlast validation accuracy, and recomputes the test accuracy from the stored\npredictions and ground-truth labels. Each metric is clearly labeled so the\noutput is self-explanatory, and the code runs immediately when executed.", "The script will load the saved numpy dictionary, iterate over each of the three\nrule-based datasets, and pull out the final values recorded during fine-tuning.\nSpecifically, it prints the last training loss, the last validation loss, the\nlast validation accuracy, and recomputes the test accuracy from the stored\npredictions and ground-truth labels. Each metric is clearly labeled so the\noutput is self-explanatory, and the code runs immediately when executed.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions to select best values\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_accuracy(values):\n    return max(values) if values else None\n\n\n# Mapping from metric key to (pretty name, selector)\nseries_handlers = {\n    (\"losses\", \"pretrain\"): (\"best pretraining loss\", best_loss),\n    (\"losses\", \"train\"): (\"best training loss\", best_loss),\n    (\"losses\", \"val\"): (\"best validation loss\", best_loss),\n    (\"metrics\", \"val_acc\"): (\"best validation accuracy\", best_accuracy),\n    (\"metrics\", \"val_aca\"): (\"best validation ACA\", best_accuracy),\n}\n\n# -----------------------------------------------------------------------------#\n# Iterate and print\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle series metrics (stored as lists)\n    for (section, key), (pretty_name, selector) in series_handlers.items():\n        values = data.get(section, {}).get(key, [])\n        if values:\n            print(f\"{pretty_name}: {selector(values):.4f}\")\n\n    # Handle single-valued test metrics\n    test_metrics = data.get(\"test\", {})\n    single_metric_names = {\n        \"loss\": \"test loss\",\n        \"acc\": \"test accuracy\",\n        \"swa\": \"test shape-weighted accuracy\",\n        \"cwa\": \"test color-weighted accuracy\",\n        \"aca\": \"test ACA\",\n    }\n    for k, pretty_name in single_metric_names.items():\n        if k in test_metrics:\n            print(f\"{pretty_name}: {test_metrics[k]:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nroot = experiment_data[\"multi_synth_generalization\"]\n\n\n# -----------------------------------------------------------------------------#\n# Helper for accuracy\ndef accuracy(pred, gt):\n    pred = np.asarray(pred)\n    gt = np.asarray(gt)\n    return float((pred == gt).mean()) if pred.size else float(\"nan\")\n\n\n# -----------------------------------------------------------------------------#\n# Iterate over each dataset and print the required metrics\nfor dataset_name in [\"variety\", \"length_parity\", \"majority_shape\"]:\n    data_block = root[dataset_name]\n\n    train_loss_final = data_block[\"losses\"][\"train\"][-1]\n    validation_loss_final = data_block[\"losses\"][\"val\"][-1]\n    validation_acc_final = data_block[\"metrics\"][\"val\"][-1]  # stored as accuracy\n    test_acc = accuracy(data_block[\"predictions\"], data_block[\"ground_truth\"])\n\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"  final training loss: {train_loss_final:.4f}\")\n    print(f\"  final validation loss: {validation_loss_final:.4f}\")\n    print(f\"  final validation accuracy: {validation_acc_final:.4f}\")\n    print(f\"  test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# 0. Locate and load the results dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# 1. Helper to fetch the \u201cfinal\u201d value in a list (if list is non-empty)\ndef final_value(lst):\n    return lst[-1] if isinstance(lst, (list, tuple)) and lst else None\n\n\n# -----------------------------------------------------------------------------#\n# 2. Iterate and print metrics\nfor (\n    exp_name,\n    datasets,\n) in experiment_data.items():  # exp_name: full_tune / frozen_encoder\n    for dset_name, dset_dict in datasets.items():  # dset_name: SPR_BENCH\n        print(f\"\\nDataset: {dset_name}  (experiment: {exp_name})\")\n\n        # --- Pre-training loss (only present for 'full_tune')\n        pre_losses = dset_dict[\"losses\"].get(\"pretrain\")\n        if pre_losses:\n            print(f\"Final pretraining loss: {final_value(pre_losses):.4f}\")\n\n        # --- Training / Validation losses\n        train_loss = final_value(dset_dict[\"losses\"].get(\"train\", []))\n        if train_loss is not None:\n            print(f\"Final training loss: {train_loss:.4f}\")\n\n        val_loss = final_value(dset_dict[\"losses\"].get(\"val\", []))\n        if val_loss is not None:\n            print(f\"Final validation loss: {val_loss:.4f}\")\n\n        # --- Validation metrics\n        val_acc = final_value(dset_dict[\"metrics\"].get(\"val_acc\", []))\n        if val_acc is not None:\n            print(f\"Final validation accuracy: {val_acc:.4f}\")\n\n        val_aca = final_value(dset_dict[\"metrics\"].get(\"val_aca\", []))\n        if val_aca is not None:\n            print(f\"Final validation ACA: {val_aca:.4f}\")\n\n        # --- Test metrics (single scalars)\n        test_dict = dset_dict.get(\"test\", {})\n        if test_dict:\n            print(f\"Test loss: {test_dict.get('loss', float('nan')):.4f}\")\n            print(f\"Test accuracy: {test_dict.get('acc', float('nan')):.4f}\")\n            print(\n                f\"Test shape-weighted accuracy: {test_dict.get('swa', float('nan')):.4f}\"\n            )\n            print(\n                f\"Test color-weighted accuracy: {test_dict.get('cwa', float('nan')):.4f}\"\n            )\n            print(f\"Test ACA: {test_dict.get('aca', float('nan')):.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper to fetch the last element from a list safely\ndef last(lst, default=None):\n    if isinstance(lst, (list, tuple)) and len(lst):\n        return lst[-1]\n    return default\n\n\n# -----------------------------------------------------------------------------#\n# Iterate and print metrics\nfor exp_name, exp_content in experiment_data.items():\n    # exp_content is a dict whose keys are dataset names (e.g., \"SPR_BENCH\")\n    for dataset_name, data in exp_content.items():\n        print(dataset_name)  # Dataset header\n\n        # Training and validation losses\n        train_loss_final = last(data.get(\"losses\", {}).get(\"train\", []))\n        val_loss_final = last(data.get(\"losses\", {}).get(\"val\", []))\n\n        # Training & validation metrics\n        train_acc_final = last(data.get(\"metrics\", {}).get(\"train_acc\", []))\n        val_acc_final = last(data.get(\"metrics\", {}).get(\"val_acc\", []))\n        val_aca_final = last(data.get(\"metrics\", {}).get(\"val_aca\", []))\n\n        # Test set metrics (scalars)\n        test_metrics = data.get(\"test\", {})\n        test_loss = test_metrics.get(\"loss\")\n        test_acc = test_metrics.get(\"acc\")\n        test_swa = test_metrics.get(\"swa\")\n        test_cwa = test_metrics.get(\"cwa\")\n        test_aca = test_metrics.get(\"aca\")\n\n        # Print final / best values with clear metric names\n        if train_loss_final is not None:\n            print(f\"final training loss: {train_loss_final:.4f}\")\n        if val_loss_final is not None:\n            print(f\"final validation loss: {val_loss_final:.4f}\")\n        if train_acc_final is not None:\n            print(f\"final training accuracy: {train_acc_final:.4f}\")\n        if val_acc_final is not None:\n            print(f\"final validation accuracy: {val_acc_final:.4f}\")\n        if val_aca_final is not None:\n            print(f\"final validation ACA: {val_aca_final:.4f}\")\n\n        if test_loss is not None:\n            print(f\"test loss: {test_loss:.4f}\")\n        if test_acc is not None:\n            print(f\"test accuracy: {test_acc:.4f}\")\n        if test_swa is not None:\n            print(f\"test shape-weighted accuracy: {test_swa:.4f}\")\n        if test_cwa is not None:\n            print(f\"test color-weighted accuracy: {test_cwa:.4f}\")\n        if test_aca is not None:\n            print(f\"test ACA: {test_aca:.4f}\")\n\n        # Blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n# Load experiment data\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------\n# Helper: decide whether higher or lower is better\n# -----------------------------------------------------------------------------\ndef best_value(values, metric_name):\n    \"\"\"Return the best value according to the metric type.\"\"\"\n    if not isinstance(values, (list, tuple)) or len(values) == 0:\n        return None\n    # Losses: lower is better, Accuracies & others: higher is better\n    loss_keywords = {\"loss\"}\n    if any(k in metric_name.lower() for k in loss_keywords):\n        return min(values)\n    return max(values)\n\n\n# -----------------------------------------------------------------------------\n# Iterate and print metrics\n# -----------------------------------------------------------------------------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, content in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---------- Losses ----------\n        for loss_key, loss_values in content.get(\"losses\", {}).items():\n            metric_label = {\n                \"pretrain\": \"pretraining loss\",\n                \"train\": \"training loss\",\n                \"val\": \"validation loss\",\n            }.get(loss_key, f\"{loss_key} loss\")\n            best_loss = best_value(loss_values, metric_label)\n            if best_loss is not None:\n                print(f\"{metric_label}: {best_loss:.4f}\")\n\n        # ---------- Validation metrics ----------\n        for m_key, m_values in content.get(\"metrics\", {}).items():\n            metric_label = {\n                \"val_acc\": \"validation accuracy\",\n                \"val_aca\": \"validation ACA\",\n            }.get(m_key, m_key)\n            best_metric = best_value(m_values, metric_label)\n            if best_metric is not None:\n                print(f\"{metric_label}: {best_metric:.4f}\")\n\n        # ---------- Test metrics ----------\n        test_dict = content.get(\"test\", {})\n        for t_key, t_val in test_dict.items():\n            metric_label = {\n                \"loss\": \"test loss\",\n                \"acc\": \"test accuracy\",\n                \"swa\": \"test shape weighted accuracy\",\n                \"cwa\": \"test color weighted accuracy\",\n                \"aca\": \"test ACA\",\n            }.get(t_key, f\"test {t_key}\")\n            # Skip arrays like predictions / ground_truth\n            if isinstance(t_val, (list, np.ndarray)):\n                continue\n            print(f\"{metric_label}: {t_val:.4f}\")\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions\ndef _best(values, minimize=True):\n    \"\"\"Return the best value from a list according to minimize/maximize rule.\"\"\"\n    if not values:  # empty list guard\n        return None\n    return min(values) if minimize else max(values)\n\n\ndef _print_metric(name, value):\n    \"\"\"Consistent pretty printing of a metric name/value pair.\"\"\"\n    if value is None:\n        return\n    print(\n        f\"  {name}: {value:.4f}\"\n        if isinstance(value, (int, float))\n        else f\"  {name}: {value}\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# Traverse and report\nfor setup_name, datasets in experiment_data.items():  # e.g. \"mask_only\"\n    for dataset_name, rec in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ---------------------------  Losses ---------------------------------\n        losses = rec.get(\"losses\", {})\n        pretrain_loss_best = _best(losses.get(\"pretrain\", []), minimize=True)\n        train_loss_best = _best(losses.get(\"train\", []), minimize=True)\n        val_loss_best = _best(losses.get(\"val\", []), minimize=True)\n\n        _print_metric(\"best pre-training loss\", pretrain_loss_best)\n        _print_metric(\"best training loss\", train_loss_best)\n        _print_metric(\"best validation loss\", val_loss_best)\n\n        # ---------------------------  Validation metrics ----------------------\n        metrics = rec.get(\"metrics\", {})\n        val_acc_best = _best(metrics.get(\"val_acc\", []), minimize=False)\n        val_aca_best = _best(metrics.get(\"val_aca\", []), minimize=False)\n\n        _print_metric(\"best validation accuracy\", val_acc_best)\n        _print_metric(\"best validation ACA\", val_aca_best)\n\n        # ---------------------------  Test metrics ----------------------------\n        test = rec.get(\"test\", {})\n        _print_metric(\"test loss\", test.get(\"loss\"))\n        _print_metric(\"test accuracy\", test.get(\"acc\"))\n        _print_metric(\"test shape-weighted accuracy\", test.get(\"swa\"))\n        _print_metric(\"test color-weighted accuracy\", test.get(\"cwa\"))\n        _print_metric(\"test ACA\", test.get(\"aca\"))\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper for consistent numeric formatting\ndef fmt(val):\n    return f\"{val:.4f}\" if isinstance(val, (float, int, np.floating)) else str(val)\n\n\n# -----------------------------------------------------------------------------#\n# Traverse the stored results and print final / best metrics\nfor model_name, model_dict in experiment_data.items():\n    for dataset_name, data in model_dict.items():\n        print(f\"Dataset: {dataset_name}\")  # dataset header\n\n        # -------- Pre-training -------------------------------------------------\n        pre_losses = data.get(\"losses\", {}).get(\"pretrain\", [])\n        if pre_losses:\n            print(\"pretraining loss:\", fmt(pre_losses[-1]))\n\n        # -------- Fine-tuning (train / validation) ----------------------------\n        train_losses = data.get(\"losses\", {}).get(\"train\", [])\n        if train_losses:\n            print(\"training loss:\", fmt(train_losses[-1]))\n\n        val_losses = data.get(\"losses\", {}).get(\"val\", [])\n        if val_losses:\n            print(\"validation loss:\", fmt(val_losses[-1]))\n\n        val_accs = data.get(\"metrics\", {}).get(\"val_acc\", [])\n        if val_accs:\n            print(\"best validation accuracy:\", fmt(max(val_accs)))\n\n        val_acas = data.get(\"metrics\", {}).get(\"val_aca\", [])\n        if val_acas:\n            print(\"best validation ACA:\", fmt(max(val_acas)))\n\n        # -------- Test split ---------------------------------------------------\n        test_metrics = data.get(\"test\", {})\n        if test_metrics:\n            for key in [\"loss\", \"acc\", \"swa\", \"cwa\", \"aca\"]:\n                if key in test_metrics:\n                    metric_name = {\n                        \"loss\": \"test loss\",\n                        \"acc\": \"test accuracy\",\n                        \"swa\": \"test shape-weighted accuracy\",\n                        \"cwa\": \"test color-weighted accuracy\",\n                        \"aca\": \"test ACA\",\n                    }[key]\n                    print(f\"{metric_name}:\", fmt(test_metrics[key]))\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# 0. Locate working directory and load file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions to pick the \u201cbest\u201d value from a list\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_accuracy(values):\n    return max(values) if values else None\n\n\n# -----------------------------------------------------------------------------#\n# 2\u20135. Traverse structure, extract, and print metrics\nfor exp_name, datasets in experiment_data.items():\n    for ds_name, ds_content in datasets.items():\n        print(f\"\\nDataset: {exp_name} - {ds_name}\")\n\n        # ----- Losses -----\n        losses = ds_content.get(\"losses\", {})\n        pre_loss_best = best_loss(losses.get(\"pretrain\", []))\n        train_loss_best = best_loss(losses.get(\"train\", []))\n        val_loss_best = best_loss(losses.get(\"val\", []))\n\n        if pre_loss_best is not None:\n            print(f\"best pre-training loss: {pre_loss_best:.6f}\")\n        if train_loss_best is not None:\n            print(f\"best fine-tuning training loss: {train_loss_best:.6f}\")\n        if val_loss_best is not None:\n            print(f\"best validation loss: {val_loss_best:.6f}\")\n\n        # ----- Validation metrics -----\n        metrics = ds_content.get(\"metrics\", {})\n        val_acc_best = best_accuracy(metrics.get(\"val_acc\", []))\n        val_aca_best = best_accuracy(metrics.get(\"val_aca\", []))\n\n        if val_acc_best is not None:\n            print(f\"best validation accuracy: {val_acc_best:.6f}\")\n        if val_aca_best is not None:\n            print(f\"best validation ACA: {val_aca_best:.6f}\")\n\n        # ----- Test metrics -----\n        test = ds_content.get(\"test\", {})\n        if test:\n            if \"loss\" in test:\n                print(f\"test loss: {test['loss']:.6f}\")\n            if \"acc\" in test:\n                print(f\"test accuracy: {test['acc']:.6f}\")\n            if \"swa\" in test:\n                print(f\"test shape-weighted accuracy: {test['swa']:.6f}\")\n            if \"cwa\" in test:\n                print(f\"test color-weighted accuracy: {test['cwa']:.6f}\")\n            if \"aca\" in test:\n                print(f\"test ACA: {test['aca']:.6f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper for safe extraction\ndef _last(lst):\n    return lst[-1] if lst else None\n\n\ndef _best(lst, mode=\"min\"):\n    if not lst:\n        return None\n    return min(lst) if mode == \"min\" else max(lst)\n\n\n# -----------------------------------------------------------------------------#\n# Traverse and print\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, ds in datasets.items():\n        print(f\"\\nDataset: {dataset_name}  (Experiment: {experiment_name})\")\n\n        # ----- losses -----\n        losses = ds.get(\"losses\", {})\n        final_mlm_loss = _last(losses.get(\"pretrain\", []))\n        final_train_loss = _last(losses.get(\"train\", []))\n        best_val_loss = _best(losses.get(\"val\", []), mode=\"min\")\n\n        if final_mlm_loss is not None:\n            print(f\"final MLM pretraining loss: {final_mlm_loss:.4f}\")\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n        if best_val_loss is not None:\n            print(f\"best validation loss: {best_val_loss:.4f}\")\n\n        # ----- validation metrics -----\n        metrics = ds.get(\"metrics\", {})\n        best_val_acc = _best(metrics.get(\"val_acc\", []), mode=\"max\")\n        best_val_aca = _best(metrics.get(\"val_aca\", []), mode=\"max\")\n\n        if best_val_acc is not None:\n            print(f\"best validation accuracy: {best_val_acc:.4f}\")\n        if best_val_aca is not None:\n            print(f\"best validation augmented consistency accuracy: {best_val_aca:.4f}\")\n\n        # ----- test metrics -----\n        test = ds.get(\"test\", {})\n        if test:\n            if \"loss\" in test:\n                print(f\"test loss: {test['loss']:.4f}\")\n            if \"acc\" in test:\n                print(f\"test accuracy: {test['acc']:.4f}\")\n            if \"swa\" in test:\n                print(f\"test shape weighted accuracy: {test['swa']:.4f}\")\n            if \"cwa\" in test:\n                print(f\"test color weighted accuracy: {test['cwa']:.4f}\")\n            if \"aca\" in test:\n                print(f\"test augmented consistency accuracy: {test['aca']:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nroot = experiment_data[\"multi_synth_generalization\"]\n\n\n# -----------------------------------------------------------------------------#\n# Helper for accuracy\ndef accuracy(pred, gt):\n    pred = np.asarray(pred)\n    gt = np.asarray(gt)\n    return float((pred == gt).mean()) if pred.size else float(\"nan\")\n\n\n# -----------------------------------------------------------------------------#\n# Iterate over each dataset and print the required metrics\nfor dataset_name in [\"variety\", \"length_parity\", \"majority_shape\"]:\n    data_block = root[dataset_name]\n\n    train_loss_final = data_block[\"losses\"][\"train\"][-1]\n    validation_loss_final = data_block[\"losses\"][\"val\"][-1]\n    validation_acc_final = data_block[\"metrics\"][\"val\"][-1]  # stored as accuracy\n    test_acc = accuracy(data_block[\"predictions\"], data_block[\"ground_truth\"])\n\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"  final training loss: {train_loss_final:.4f}\")\n    print(f\"  final validation loss: {validation_loss_final:.4f}\")\n    print(f\"  final validation accuracy: {validation_acc_final:.4f}\")\n    print(f\"  test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nroot = experiment_data[\"multi_synth_generalization\"]\n\n\n# -----------------------------------------------------------------------------#\n# Helper for accuracy\ndef accuracy(pred, gt):\n    pred = np.asarray(pred)\n    gt = np.asarray(gt)\n    return float((pred == gt).mean()) if pred.size else float(\"nan\")\n\n\n# -----------------------------------------------------------------------------#\n# Iterate over each dataset and print the required metrics\nfor dataset_name in [\"variety\", \"length_parity\", \"majority_shape\"]:\n    data_block = root[dataset_name]\n\n    train_loss_final = data_block[\"losses\"][\"train\"][-1]\n    validation_loss_final = data_block[\"losses\"][\"val\"][-1]\n    validation_acc_final = data_block[\"metrics\"][\"val\"][-1]  # stored as accuracy\n    test_acc = accuracy(data_block[\"predictions\"], data_block[\"ground_truth\"])\n\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"  final training loss: {train_loss_final:.4f}\")\n    print(f\"  final validation loss: {validation_loss_final:.4f}\")\n    print(f\"  final validation accuracy: {validation_acc_final:.4f}\")\n    print(f\"  test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nroot = experiment_data[\"multi_synth_generalization\"]\n\n\n# -----------------------------------------------------------------------------#\n# Helper for accuracy\ndef accuracy(pred, gt):\n    pred = np.asarray(pred)\n    gt = np.asarray(gt)\n    return float((pred == gt).mean()) if pred.size else float(\"nan\")\n\n\n# -----------------------------------------------------------------------------#\n# Iterate over each dataset and print the required metrics\nfor dataset_name in [\"variety\", \"length_parity\", \"majority_shape\"]:\n    data_block = root[dataset_name]\n\n    train_loss_final = data_block[\"losses\"][\"train\"][-1]\n    validation_loss_final = data_block[\"losses\"][\"val\"][-1]\n    validation_acc_final = data_block[\"metrics\"][\"val\"][-1]  # stored as accuracy\n    test_acc = accuracy(data_block[\"predictions\"], data_block[\"ground_truth\"])\n\n    print(f\"\\nDataset: {dataset_name}\")\n    print(f\"  final training loss: {train_loss_final:.4f}\")\n    print(f\"  final validation loss: {validation_loss_final:.4f}\")\n    print(f\"  final validation accuracy: {validation_acc_final:.4f}\")\n    print(f\"  test accuracy: {test_acc:.4f}\")\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'best pretraining loss: 0.6796', '\\n', 'best\ntraining loss: 0.0000', '\\n', 'best validation loss: 0.0003', '\\n', 'best\nvalidation accuracy: 1.0000', '\\n', 'best validation ACA: 0.9003', '\\n', 'test\nloss: 4.1363', '\\n', 'test accuracy: 0.7001', '\\n', 'test shape-weighted\naccuracy: 0.7001', '\\n', 'test color-weighted accuracy: 0.6355', '\\n', 'test\nACA: 0.6522', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: variety', '\\n', '  final training loss: 0.0202', '\\n', '  final\nvalidation loss: 0.4336', '\\n', '  final validation accuracy: 0.8775', '\\n', '\ntest accuracy: 0.8625', '\\n', '\\nDataset: length_parity', '\\n', '  final\ntraining loss: 0.0001', '\\n', '  final validation loss: 0.0001', '\\n', '  final\nvalidation accuracy: 1.0000', '\\n', '  test accuracy: 1.0000', '\\n', '\\nDataset:\nmajority_shape', '\\n', '  final training loss: 0.0141', '\\n', '  final\nvalidation loss: 0.1147', '\\n', '  final validation accuracy: 0.9658', '\\n', '\ntest accuracy: 0.9692', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: SPR_BENCH  (experiment: full_tune)', '\\n', 'Final pretraining loss:\n0.6809', '\\n', 'Final training loss: 0.0000', '\\n', 'Final validation loss:\n0.0001', '\\n', 'Final validation accuracy: 1.0000', '\\n', 'Final validation ACA:\n0.8783', '\\n', 'Test loss: 4.0304', '\\n', 'Test accuracy: 0.7000', '\\n', 'Test\nshape-weighted accuracy: 0.7000', '\\n', 'Test color-weighted accuracy: 0.6354',\n'\\n', 'Test ACA: 0.6515', '\\n', '\\nDataset: SPR_BENCH  (experiment:\nfrozen_encoder)', '\\n', 'Final training loss: 0.1911', '\\n', 'Final validation\nloss: 0.1965', '\\n', 'Final validation accuracy: 0.9360', '\\n', 'Final\nvalidation ACA: 0.8952', '\\n', 'Test loss: 1.2079', '\\n', 'Test accuracy:\n0.6753', '\\n', 'Test shape-weighted accuracy: 0.6759', '\\n', 'Test color-\nweighted accuracy: 0.6183', '\\n', 'Test ACA: 0.6602', '\\n', 'Execution time: a\nsecond seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.0000', '\\n', 'final validation loss:\n0.0001', '\\n', 'final training accuracy: 1.0000', '\\n', 'final validation\naccuracy: 1.0000', '\\n', 'final validation ACA: 0.8706', '\\n', 'test loss:\n3.8734', '\\n', 'test accuracy: 0.7000', '\\n', 'test shape-weighted accuracy:\n0.7000', '\\n', 'test color-weighted accuracy: 0.6354', '\\n', 'test ACA: 0.6466',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'pretraining loss: 0.0342', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0012', '\\n', 'validation accuracy: 0.9996',\n'\\n', 'validation ACA: 0.9996', '\\n', 'test loss: 3.9352', '\\n', 'test accuracy:\n0.6995', '\\n', 'test shape weighted accuracy: 0.6995', '\\n', 'test color\nweighted accuracy: 0.6350', '\\n', 'test ACA: 0.6995', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best pre-training loss: 1.5064', '\\n', '  best\ntraining loss: 0.0000', '\\n', '  best validation loss: 0.0002', '\\n', '  best\nvalidation accuracy: 1.0000', '\\n', '  best validation ACA: 0.8093', '\\n', '\ntest loss: 4.2892', '\\n', '  test accuracy: 0.6999', '\\n', '  test shape-\nweighted accuracy: 0.6999', '\\n', '  test color-weighted accuracy: 0.6353',\n'\\n', '  test ACA: 0.6033', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'pretraining loss:', ' ', '2.3382', '\\n', 'training\nloss:', ' ', '0.5202', '\\n', 'validation loss:', ' ', '0.5203', '\\n', 'best\nvalidation accuracy:', ' ', '0.7632', '\\n', 'best validation ACA:', ' ',\n'0.7418', '\\n', 'test loss:', ' ', '0.7182', '\\n', 'test accuracy:', ' ',\n'0.6300', '\\n', 'test shape-weighted accuracy:', ' ', '0.6310', '\\n', 'test\ncolor-weighted accuracy:', ' ', '0.5974', '\\n', 'test ACA:', ' ', '0.6198',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: shuffle_only - SPR_BENCH', '\\n', 'best pre-training loss:\n0.034218', '\\n', 'best fine-tuning training loss: 0.000040', '\\n', 'best\nvalidation loss: 0.001167', '\\n', 'best validation accuracy: 0.999600', '\\n',\n'best validation ACA: 0.906300', '\\n', 'test loss: 3.935185', '\\n', 'test\naccuracy: 0.699500', '\\n', 'test shape-weighted accuracy: 0.699464', '\\n', 'test\ncolor-weighted accuracy: 0.635038', '\\n', 'test ACA: 0.653100', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  (Experiment: MLM_pretrain)', '\\n', 'final MLM\npretraining loss: 2.4735', '\\n', 'final training loss: 0.0000', '\\n', 'best\nvalidation loss: 0.0037', '\\n', 'best validation accuracy: 0.9990', '\\n', 'best\nvalidation augmented consistency accuracy: 0.8646', '\\n', 'test loss: 4.1752',\n'\\n', 'test accuracy: 0.6992', '\\n', 'test shape weighted accuracy: 0.6991',\n'\\n', 'test color weighted accuracy: 0.6347', '\\n', 'test augmented consistency\naccuracy: 0.6423', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: variety', '\\n', '  final training loss: 0.0202', '\\n', '  final\nvalidation loss: 0.4336', '\\n', '  final validation accuracy: 0.8775', '\\n', '\ntest accuracy: 0.8625', '\\n', '\\nDataset: length_parity', '\\n', '  final\ntraining loss: 0.0001', '\\n', '  final validation loss: 0.0001', '\\n', '  final\nvalidation accuracy: 1.0000', '\\n', '  test accuracy: 1.0000', '\\n', '\\nDataset:\nmajority_shape', '\\n', '  final training loss: 0.0141', '\\n', '  final\nvalidation loss: 0.1147', '\\n', '  final validation accuracy: 0.9658', '\\n', '\ntest accuracy: 0.9692', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: variety', '\\n', '  final training loss: 0.0202', '\\n', '  final\nvalidation loss: 0.4336', '\\n', '  final validation accuracy: 0.8775', '\\n', '\ntest accuracy: 0.8625', '\\n', '\\nDataset: length_parity', '\\n', '  final\ntraining loss: 0.0001', '\\n', '  final validation loss: 0.0001', '\\n', '  final\nvalidation accuracy: 1.0000', '\\n', '  test accuracy: 1.0000', '\\n', '\\nDataset:\nmajority_shape', '\\n', '  final training loss: 0.0141', '\\n', '  final\nvalidation loss: 0.1147', '\\n', '  final validation accuracy: 0.9658', '\\n', '\ntest accuracy: 0.9692', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: variety', '\\n', '  final training loss: 0.0202', '\\n', '  final\nvalidation loss: 0.4336', '\\n', '  final validation accuracy: 0.8775', '\\n', '\ntest accuracy: 0.8625', '\\n', '\\nDataset: length_parity', '\\n', '  final\ntraining loss: 0.0001', '\\n', '  final validation loss: 0.0001', '\\n', '  final\nvalidation accuracy: 1.0000', '\\n', '  test accuracy: 1.0000', '\\n', '\\nDataset:\nmajority_shape', '\\n', '  final training loss: 0.0141', '\\n', '  final\nvalidation loss: 0.1147', '\\n', '  final validation accuracy: 0.9658', '\\n', '\ntest accuracy: 0.9692', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}