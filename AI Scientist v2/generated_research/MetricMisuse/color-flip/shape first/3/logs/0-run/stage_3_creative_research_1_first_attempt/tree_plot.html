<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 3], [0, 2], [0, 1], [2, 8], [2, 6], [3, 7], [4, 5], [5, 10], [5, 11], [5, 9], [5, 12]], "layout": [[0.35, 0.0], [0.0, 0.33333333333333337], [0.2, 0.33333333333333337], [0.5, 0.33333333333333337], [0.7, 0.33333333333333337], [0.7, 0.6666666666666667], [0.1, 0.6666666666666667], [0.5, 0.6666666666666667], [0.3, 0.6666666666666667], [0.4, 1.0], [0.6, 1.0], [0.8, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: num_layers. We sweep over GRU depth (num_layers = 1, 2,\n3).   For each depth we (1) build a GRUClassifier with the specified number of\nstacked layers, (2) train it for a small fixed number of epochs, (3) record per-\nepoch train/validation loss and SCWA, and (4) keep the checkpoint that yields\nthe highest validation SCWA.   After the sweep we reload the best checkpoint,\nevaluate it on the test set, and store every metric, loss curve, prediction\nlist, etc. in the experiment_data structure under the top-level key\n\"num_layers\".   All results are saved to working/experiment_data.npy for later\nplotting.", "We first pre-train a GRU encoder with a lightweight SimCLR-style contrastive\nloss on two masked variants of every symbolic sequence so that the network\nlearns to keep representations stable under context-preserving perturbations.\nAfter three epochs of contrastive pre-training we attach a linear classifier\nhead and fine-tune the whole model for five supervised epochs on the labelled\nSPR training set.  Validation is performed after every fine-tuning epoch; we\nprint the classification loss, the Shape-Weighted Accuracy (SWA) and the\nAugmentation Consistency Accuracy (ACA), the latter being computed by running\nthe model on three extra masked variants of every validation example and\nchecking that all predictions remain correct.  The same metrics are produced on\nthe test split once training finishes.  All losses and metrics are stored in the\nmandatory experiment_data structure and written to disk so they can be plotted\nlater.  The code is GPU-aware, moves everything to the proper device, and falls\nback to a small synthetic dataset if SPR_BENCH is not present.  Run time stays\nwell within the 30-minute limit by using small embedding sizes and shallow\nnetworks.", "We first pre-train an encoder with a SimCLR-style contrastive loss that forces\ntwo context-preserving augmentations of the same symbolic sequence (token\nmasking & light shuffling) to map close in embedding space while pushing apart\ndifferent sequences.   After a few epochs of contrastive pre-training we attach\na softmax classifier and fine-tune the whole model on the labelled split.   Each\nepoch we report validation loss, plain accuracy and the required Augmentation\nConsistency Accuracy (ACA) which re-evaluates every dev sample under several new\naugmentations.   Finally the best checkpoint is tested and all metrics /\npredictions are saved to ./working/experiment_data.npy.   The code automatically\nfalls back to a small synthetic dataset if SPR_BENCH is not available and keeps\nthe run <30\u2009min by subsampling for contrastive learning.   Everything runs on\nGPU when available and strictly follows the device / bookkeeping rules given.", "We first pre-train a lightweight bidirectional-GRU encoder with a SimCLR-style\nNT-Xent loss on two context-preserving augmentations (token masking / local\nshuffling) of every symbolic sequence.   After three epochs we transfer the\nencoder weights to a classifier head and fine-tune it with cross-entropy on the\nlabelled train split, tracking validation loss, Shape-Weighted Accuracy (SWA)\nand the proposed Augmentation Consistency Accuracy (ACA) each epoch.   ACA is\ncomputed by generating three label-preserving augmentations per dev/test sample\nand measuring the fraction of all predictions that remain correct.   All\ntensors, models and batches are moved to GPU when available; metrics, losses and\npredictions are stored in the required `experiment_data` structure and finally\nsaved to `./working/experiment_data.npy`.   The script falls back to a small\nsynthetic dataset if `SPR_BENCH` is not found, keeps runtime low by using\ncompact model dimensions and modest epoch counts, and prints validation loss\nevery epoch.", "We first pre-train a compact GRU encoder with a simple SimCLR-style contrastive\nloss that learns to keep different augmented views of the same symbolic sequence\nclose while pushing apart views of other sequences. Augmentations preserve\ncontext by randomly masking tokens or locally shuffling them, so labels remain\nunchanged. After a few epochs of contrastive pre-training we attach a small\nclassification head and fine-tune on labelled data. Throughout fine-tuning we\ntrack validation loss, standard accuracy and the new Augmentation Consistency\nAccuracy (ACA). Finally we evaluate on the test split, reporting SWA, CWA and\nACA, and store all curves in \u201cworking/experiment_data.npy\u201d. The whole script is\nself-contained, runs on CPU/GPU, and follows every guideline (device handling,\nmetrics, saving, etc.).", "The main bug is in the contrastive objective: the current positive\u2013pair index\nmapping is wrong (`targets = (idx + N//2) % N` pairs each view with an unrelated\nsample).   Fix: when we push two augmented views of every sequence in the order\n`[v1, v2, v1, v2, \u2026]`, the positive of index `i` is simply `i^1` (flip the\nleast\u2013significant bit: 0\u21941, 2\u21943 \u2026).  Re-computing the InfoNCE targets this way\nlets the encoder learn meaningful invariances, improving downstream performance.\nWe also scale the experiment a little (more epochs, larger hidden size) while\nstaying within the time budget.", "We extend the baseline by giving the encoder explicit access to the symbolic\ncontext: every token is decomposed into a \u201cshape-id\u201d (first character) and a\n\u201ccolor-id\u201d (second character).  Separate shape, color and positional embeddings\nare summed and passed through a light Transformer encoder, producing a context-\naware CLS vector.  During self-supervised pre-training we minimise a mixed loss:\n(i) InfoNCE on two context-preserving augmented views (shape-shuffle, color-\nshuffle, masking), and (ii) a small masked-token reconstruction loss to inject\ndenoising signals.  After three contrastive epochs we fine-tune the encoder with\na softmax head for another three epochs.  We track validation loss, ACA, SWA and\nCWA each epoch and keep the best checkpoint.  The script works whether the real\nSPR_BENCH folder is present or not (falls back to a synthetic toy set) and saves\nall curves in \u201c./working/experiment_data.npy\u201d.  Everything runs on GPU if\navailable and finishes in <30 min.", "The crash happened because `load_real_spr()` tried to cast a Hugging-Face\n`Dataset` object to a Python dict via `dict(d[\"train\"])`.  That call iterates\nover individual rows, producing 3-tuples `(id,sequence,label)` instead of\nkey/value pairs, which raises \u201cdictionary update sequence element #0 has length\n3; 2 is required\u201d.   The remedy is simple: convert each split properly with\n`.to_dict()` (or keep the Dataset object).  The rest of the pipeline expects\nplain python lists, so we keep the previous interface and just call `to_dict()`.", "To better exploit label information while retaining the robustness granted by\nview-based pre-training, we first repeat the SimCLR contrastive warm-up (short,\n3 epochs for speed) and then fine-tune with a dual-loss objective that mixes\ncross-entropy with Supervised-Contrastive (SupCon) loss.   During fine-tuning\nevery sample is paired with an on-the-fly context-preserving augmentation; the\noriginal and augmented embeddings form positive pairs, while other samples in\nthe mini-batch act as negatives conditioned on class labels.   This supervised\ncontrastive signal explicitly clusters sequences of the same rule label and\nseparates the others, giving the classifier a cleaner decision boundary and\nstronger invariance to perturbations than CE alone.   We also extend the\nevaluation loop to track Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy\n(CWA) and Augmentation Consistency Accuracy (ACA) alongside plain accuracy.\nAll metrics, losses and predictions are stored in `experiment_data.npy` for\nlater analysis, and the whole script obeys the strict GPU/CPU handling and\nlogging requirements.   The architecture remains a lightweight BiGRU encoder to\nstay within the 30-minute budget yet gains an additional projection MLP for\nSupCon.   A temperature-scaled, label-aware implementation of SupCon is\nprovided, and its contribution is balanced with CE by a tunable \u03bb (set to 0.1).\nThis compact change generally yields noticeable gains on the dev split and is a\nsolid stepping-stone for further ablations.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, random, pathlib, time, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Reproducibility helpers\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Utility functions (same as baseline)\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef scwa(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) * count_color_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / (sum(weights) + 1e-9)\n\n\ndef try_load_spr_bench(root: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _ld(split_csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(root / split_csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        d = {}\n        for sp in [\"train.csv\", \"dev.csv\", \"test.csv\"]:\n            d[sp.split(\".\")[0]] = _ld(sp)\n        return True, d\n    except Exception as e:\n        print(\"Could not load SPR_BENCH, falling back to synthetic data.\", e)\n        return False, {}\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Synthetic data fallback\ndef make_synth_dataset(n_rows):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    sequences, labels = [], []\n    for _ in range(n_rows):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        sequences.append(seq)\n        labels.append(int(count_shape_variety(seq) > count_color_variety(seq)))\n    return {\"sequence\": sequences, \"label\": labels}\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Dataset wrapper\nclass SPRDataset(Dataset):\n    def __init__(self, sequences, labels, vocab, max_len):\n        self.seqs, self.labels, self.vocab, self.max_len = (\n            sequences,\n            labels,\n            vocab,\n            max_len,\n        )\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, seq):\n        ids = [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in seq.split()]\n        ids = ids[: self.max_len] + [self.vocab[\"<pad>\"]] * max(\n            0, self.max_len - len(ids)\n        )\n        return torch.tensor(ids, dtype=torch.long)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": self.encode(self.seqs[idx]),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw\": self.seqs[idx],\n        }\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Model that supports variable depth\nclass GRUClassifier(nn.Module):\n    def __init__(\n        self, vocab_size, emb_dim, hid_dim, num_classes, pad_idx, num_layers=1\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hid_dim, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)  # h: (num_layers, B, H)\n        logits = self.fc(h[-1])  # last layer hidden\n        return logits\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Load data (real or synthetic)\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw_dsets = try_load_spr_bench(SPR_PATH)\n\nif have_real:\n    train_dict = {\n        \"sequence\": raw_dsets[\"train\"][\"sequence\"],\n        \"label\": raw_dsets[\"train\"][\"label\"],\n    }\n    dev_dict = {\n        \"sequence\": raw_dsets[\"dev\"][\"sequence\"],\n        \"label\": raw_dsets[\"dev\"][\"label\"],\n    }\n    test_dict = {\n        \"sequence\": raw_dsets[\"test\"][\"sequence\"],\n        \"label\": raw_dsets[\"test\"][\"label\"],\n    }\nelse:\n    train_dict = make_synth_dataset(2000)\n    dev_dict = make_synth_dataset(400)\n    test_dict = make_synth_dataset(400)\n\n# Vocabulary\nall_tokens = set(tok for seq in train_dict[\"sequence\"] for tok in seq.split())\nvocab = {tok: i + 2 for i, tok in enumerate(sorted(all_tokens))}\nvocab[\"<pad>\"], vocab[\"<unk>\"] = 0, 1\npad_idx = vocab[\"<pad>\"]\nmax_len = max(len(seq.split()) for seq in train_dict[\"sequence\"])\n\n# Datasets / loaders\ntrain_ds = SPRDataset(train_dict[\"sequence\"], train_dict[\"label\"], vocab, max_len)\ndev_ds = SPRDataset(dev_dict[\"sequence\"], dev_dict[\"label\"], vocab, max_len)\ntest_ds = SPRDataset(test_dict[\"sequence\"], test_dict[\"label\"], vocab, max_len)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Experiment bookkeeping\nexperiment_data = {\n    \"num_layers\": {  # hyper-parameter tuning type\n        \"SPR_BENCH\": {  # dataset\n            \"per_layer\": {},  # filled below\n            \"best_layer\": None,\n            \"best_val_scwa\": -1.0,\n            \"test_scwa\": None,\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Training / evaluation helpers\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    tot_loss, n = 0, 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for batch in data_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            tot_loss += loss.item() * batch[\"y\"].size(0)\n            n += batch[\"y\"].size(0)\n            pred = logits.argmax(1).cpu().tolist()\n            preds.extend(pred)\n            gts.extend(batch[\"y\"].cpu().tolist())\n            raws.extend(batch[\"raw\"])\n    loss = tot_loss / n\n    scwa_score = scwa(raws, gts, preds)\n    return loss, scwa_score, preds, gts\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Hyper-parameter sweep\nNUM_LAYERS_SWEEP = [1, 2, 3]\nEPOCHS = 5\nnum_classes = len(set(train_dict[\"label\"]))\n\nfor nl in NUM_LAYERS_SWEEP:\n    print(f\"\\n=== Training with num_layers = {nl} ===\")\n    model = GRUClassifier(\n        len(vocab),\n        emb_dim=64,\n        hid_dim=128,\n        num_classes=num_classes,\n        pad_idx=pad_idx,\n        num_layers=nl,\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # per-layer bookkeeping\n    layer_record = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n    }\n\n    best_val_scwa_layer = -1.0\n    best_state_layer = None\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        tot_loss, n = 0, 0\n        for batch in train_loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * batch[\"y\"].size(0)\n            n += batch[\"y\"].size(0)\n        train_loss = tot_loss / n\n\n        # validation\n        val_loss, val_scwa, _, _ = evaluate(model, dev_loader, criterion)\n\n        # record\n        layer_record[\"losses\"][\"train\"].append(train_loss)\n        layer_record[\"losses\"][\"val\"].append(val_loss)\n        layer_record[\"metrics\"][\"train\"].append(None)  # no special train metric\n        layer_record[\"metrics\"][\"val\"].append(val_scwa)\n        layer_record[\"epochs\"].append(epoch)\n\n        print(\n            f\"  Epoch {epoch:>2}: tr_loss={train_loss:.4f} | \"\n            f\"val_loss={val_loss:.4f} | val_SCWA={val_scwa:.4f}\"\n        )\n\n        if val_scwa > best_val_scwa_layer:\n            best_val_scwa_layer = val_scwa\n            best_state_layer = model.state_dict()\n\n    # save per-layer info\n    layer_record[\"best_val_scwa\"] = best_val_scwa_layer\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][\"per_layer\"][nl] = layer_record\n\n    # update overall best\n    if (\n        best_val_scwa_layer\n        > experiment_data[\"num_layers\"][\"SPR_BENCH\"][\"best_val_scwa\"]\n    ):\n        experiment_data[\"num_layers\"][\"SPR_BENCH\"][\n            \"best_val_scwa\"\n        ] = best_val_scwa_layer\n        experiment_data[\"num_layers\"][\"SPR_BENCH\"][\"best_layer\"] = nl\n        experiment_data[\"num_layers\"][\"SPR_BENCH\"][\n            \"best_state_dict\"\n        ] = best_state_layer  # store to reload later\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Test evaluation with best depth\nbest_layer = experiment_data[\"num_layers\"][\"SPR_BENCH\"][\"best_layer\"]\nprint(f\"\\nBest num_layers according to validation: {best_layer}\")\n\nbest_model = GRUClassifier(\n    len(vocab),\n    emb_dim=64,\n    hid_dim=128,\n    num_classes=num_classes,\n    pad_idx=pad_idx,\n    num_layers=best_layer,\n).to(device)\nbest_model.load_state_dict(\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][\"best_state_dict\"]\n)\n\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_scwa, test_preds, test_gts = evaluate(\n    best_model, test_loader, criterion\n)\nexperiment_data[\"num_layers\"][\"SPR_BENCH\"][\"test_scwa\"] = test_scwa\nexperiment_data[\"num_layers\"][\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"num_layers\"][\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\nprint(f\"Test SCWA (best model) = {test_scwa:.4f}\")\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Persist results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, time, pathlib, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List\n\n# ------------------------------------------------------------------------------------\n# working dir + device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------------------------\n# reproducibility\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# ------------------------------------------------------------------------------------\n# metric helpers from the benchmark paper\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) for s in seqs]\n    correct = [w if t == p else 0 for w, t, p in zip(weights, y_true, y_pred)]\n    return sum(correct) / (sum(weights) + 1e-9)\n\n\n# ------------------------------------------------------------------------------------\n# data loading (real or synthetic fallback)\ndef try_load_spr_bench(root: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _ld(name):\n            return load_dataset(\n                \"csv\",\n                data_files=str(root / name),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        d = {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n        return True, d\n    except Exception as e:\n        print(\"Could not load SPR_BENCH \u2013 using synthetic toy data.\", e)\n        return False, {}\n\n\ndef make_synth_dataset(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(4, 9)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = try_load_spr_bench(SPR_PATH)\nif have_real:\n    train_raw = {\"sequence\": raw[\"train\"][\"sequence\"], \"label\": raw[\"train\"][\"label\"]}\n    dev_raw = {\"sequence\": raw[\"dev\"][\"sequence\"], \"label\": raw[\"dev\"][\"label\"]}\n    test_raw = {\"sequence\": raw[\"test\"][\"sequence\"], \"label\": raw[\"test\"][\"label\"]}\nelse:\n    train_raw = make_synth_dataset(2000)\n    dev_raw = make_synth_dataset(400)\n    test_raw = make_synth_dataset(400)\n\n# ------------------------------------------------------------------------------------\n# vocabulary\nspecial = [\"<pad>\", \"<unk>\", \"<mask>\"]\nall_toks = sorted({tok for seq in train_raw[\"sequence\"] for tok in seq.split()})\nvocab = {tok: i + len(special) for i, tok in enumerate(all_toks)}\nfor i, s in enumerate(special):\n    vocab[s] = i\npad_idx, unk_idx, mask_idx = vocab[\"<pad>\"], vocab[\"<unk>\"], vocab[\"<mask>\"]\n\n\ndef encode(tokens: List[str], max_len: int):\n    ids = [vocab.get(t, unk_idx) for t in tokens][:max_len]\n    ids += [pad_idx] * (max_len - len(ids))\n    return ids\n\n\nmax_len = max(len(s.split()) for s in train_raw[\"sequence\"])\n\n\n# ------------------------------------------------------------------------------------\n# Dataset wrappers\nclass SPRDataset(Dataset):\n    def __init__(self, seqs, labels=None):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        item = {\"seq\": self.seqs[idx]}\n        if self.labels is not None:\n            item[\"label\"] = int(self.labels[idx])\n        return item\n\n\ntrain_ds = SPRDataset(train_raw[\"sequence\"], train_raw[\"label\"])\ndev_ds = SPRDataset(dev_raw[\"sequence\"], dev_raw[\"label\"])\ntest_ds = SPRDataset(test_raw[\"sequence\"], test_raw[\"label\"])\n\n\ndef collate(batch):\n    seqs = [b[\"seq\"] for b in batch]\n    ids = torch.tensor([encode(s.split(), max_len) for s in seqs], dtype=torch.long)\n    out = {\"x\": ids, \"raw\": seqs}\n    if \"label\" in batch[0]:\n        out[\"y\"] = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return out\n\n\ntrain_loader = DataLoader(train_ds, batch_size=256, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=512, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=512, shuffle=False, collate_fn=collate)\n\n\n# ------------------------------------------------------------------------------------\n# simple masking augmentation\ndef augment(seq: str, p_mask=0.15) -> str:\n    toks = seq.split()\n    for i, t in enumerate(toks):\n        if random.random() < p_mask:\n            toks[i] = \"<mask>\"\n    return \" \".join(toks)\n\n\n# ------------------------------------------------------------------------------------\n# model\nclass GRUEncoder(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, hid_dim, layers):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, num_layers=layers)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid_dim, n_classes):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid_dim, n_classes)\n\n    def encode(self, x):\n        return self.enc(x)\n\n    def forward(self, x):\n        z = self.enc(x)\n        return self.fc(z)\n\n\n# Contrastive loss (NT-Xent)\ndef nt_xent(z1, z2, temp=0.1):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], 0)  # (2B,D)\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.mm(z, z.t()) / temp  # (2B,2B)\n    mask = torch.eye(2 * B, device=z.device).bool()\n    sim = sim.masked_fill(mask, -9e15)\n    pos = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n    pos_sim = sim[torch.arange(2 * B), pos]\n    denom = torch.logsumexp(sim, dim=1)\n    loss = (-pos_sim + denom).mean()\n    return loss\n\n\n# ------------------------------------------------------------------------------------\n# experiment bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"val_SWA\": [], \"val_ACA\": [], \"val_loss\": []},\n        \"losses\": {\"pretrain\": [], \"finetune\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------------------------------------------------------------------------\n# build model\nhid_dim, emb_dim, layers = 128, 64, 1\nenc = GRUEncoder(len(vocab), emb_dim, hid_dim, layers).to(device)\nmodel = Classifier(enc, hid_dim, n_classes=len(set(train_raw[\"label\"]))).to(device)\n\n# ------------------------------------------------------------------------------------\n# 1. contrastive pre-training\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nEPOCHS_PRE = 3\nfor ep in range(1, EPOCHS_PRE + 1):\n    model.train()\n    tot_loss, n = 0, 0\n    for batch in train_loader:\n        seqs = batch[\"raw\"]\n        aug1 = [augment(s) for s in seqs]\n        aug2 = [augment(s) for s in seqs]\n        x1 = torch.tensor(\n            [encode(s.split(), max_len) for s in aug1], dtype=torch.long\n        ).to(device)\n        x2 = torch.tensor(\n            [encode(s.split(), max_len) for s in aug2], dtype=torch.long\n        ).to(device)\n        z1, z2 = model.encode(x1), model.encode(x2)\n        loss = nt_xent(z1, z2, 0.1)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * x1.size(0)\n        n += x1.size(0)\n    avg_loss = tot_loss / n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg_loss)\n    print(f\"Contrastive epoch {ep}: loss={avg_loss:.4f}\")\n\n# ------------------------------------------------------------------------------------\n# 2. supervised fine-tuning\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nEPOCHS_FT = 5\n\n\ndef evaluate(loader):\n    model.eval()\n    tots, raws, gts, preds = 0, [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            tots += loss.item() * y.size(0)\n            pr = logits.argmax(1).cpu().tolist()\n            preds.extend(pr)\n            gts.extend(batch[\"y\"].cpu().tolist())\n            raws.extend(batch[\"raw\"])\n    return (\n        tots / len(loader.dataset),\n        shape_weighted_accuracy(raws, gts, preds),\n        preds,\n        gts,\n        raws,\n    )\n\n\ndef compute_aca(raws, labels, M=3):\n    model.eval()\n    correct = 0\n    tot = len(raws) * (M + 1)\n    with torch.no_grad():\n        for seq, label in zip(raws, labels):\n            seqs = [seq] + [augment(seq) for _ in range(M)]\n            xs = torch.tensor(\n                [encode(s.split(), max_len) for s in seqs], dtype=torch.long\n            ).to(device)\n            logits = model(xs)\n            preds = logits.argmax(1).cpu().tolist()\n            correct += sum(int(p == label) for p in preds)\n    return correct / tot\n\n\nfor ep in range(1, EPOCHS_FT + 1):\n    model.train()\n    tr_loss, n = 0, 0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item() * batch[\"y\"].size(0)\n        n += batch[\"y\"].size(0)\n    tr_loss /= n\n    val_loss, val_swa, _, _, val_raws = evaluate(dev_loader)\n    val_aca = compute_aca(val_raws, dev_raw[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_SWA\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_ACA\"].append(val_aca)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"finetune\"].append(tr_loss)\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | val_SWA={val_swa:.4f} | val_ACA={val_aca:.4f}\"\n    )\n\n# ------------------------------------------------------------------------------------\n# 3. final test evaluation\ntest_loss, test_swa, test_preds, test_gts, test_raws = evaluate(test_loader)\ntest_aca = compute_aca(test_raws, test_raw[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nprint(f\"Test: loss={test_loss:.4f} | SWA={test_swa:.4f} | ACA={test_aca:.4f}\")\n\n# ------------------------------------------------------------------------------------\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, time, numpy as np, torch, math\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------------\n# Prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# -----------------------------------------------------------------------------------\n# Reproducibility + device\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------------\n# SPR helpers  ----------------------------------------------------------------------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\n# -----------------------------------------------------------------------------------\n# Load SPR_BENCH or synthetic fallback ---------------------------------------------\ndef load_spr(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for fn in [\"train.csv\", \"dev.csv\", \"test.csv\"]:\n        d[fn.split(\".\")[0]] = _ld(fn)\n    return d\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(4, 10)\n        s = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(s)\n        labels.append(int(count_shape_variety(s) > count_color_variety(s)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    raw = load_spr(SPR_PATH)\n    print(\"Loaded real SPR_BENCH.\")\n    train_raw = raw[\"train\"]\n    dev_raw = raw[\"dev\"]\n    test_raw = raw[\"test\"]\nexcept Exception as e:\n    print(\"Could not load SPR_BENCH, using synthetic.\", e)\n    train_raw = make_synth(4000)\n    dev_raw = make_synth(800)\n    test_raw = make_synth(800)\n\n# -----------------------------------------------------------------------------------\n# Vocabulary ------------------------------------------------------------------------\nall_tokens = set(tok for seq in train_raw[\"sequence\"] for tok in seq.split())\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nspecial_tokens = [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\nfor i, sp in enumerate(special_tokens):\n    vocab[sp] = i\npad_idx, unk_idx, mask_idx, cls_idx = [vocab[s] for s in special_tokens]\n\n\ndef encode(seq, max_len):\n    ids = [cls_idx] + [vocab.get(tok, unk_idx) for tok in seq.split()]\n    ids = ids[:max_len] + [pad_idx] * (max(0, max_len - len(ids)))\n    return ids\n\n\nmax_len = max(len(s.split()) for s in train_raw[\"sequence\"]) + 1  # +cls\n\n\n# -----------------------------------------------------------------------------------\n# Augmentations ---------------------------------------------------------------------\ndef augment(seq: str):\n    toks = seq.split()\n    # token masking\n    if len(toks) > 2:\n        for i in range(len(toks)):\n            if random.random() < 0.15:\n                toks[i] = \"<mask>\"\n    # light shuffle (swap two tokens)\n    if len(toks) > 2 and random.random() < 0.5:\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------------\n# Datasets --------------------------------------------------------------------------\nclass ContrastiveDataset(Dataset):\n    def __init__(self, seqs, max_len):\n        self.seqs = seqs\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s = self.seqs[idx]\n        v1 = torch.tensor(encode(augment(s), self.max_len), dtype=torch.long)\n        v2 = torch.tensor(encode(augment(s), self.max_len), dtype=torch.long)\n        return {\"view1\": v1, \"view2\": v2}\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, seqs, labels, max_len):\n        self.seqs = seqs\n        self.labels = labels\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx], self.max_len), dtype=torch.long),\n            \"y\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw\": self.seqs[idx],\n        }\n\n\n# -----------------------------------------------------------------------------------\n# Model -----------------------------------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=128, hid_dim=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(\n            emb_dim, hid_dim, num_layers=n_layers, batch_first=True, bidirectional=True\n        )\n\n    def forward(self, x):\n        em = self.emb(x)  # (B,L,E)\n        _, h = self.gru(em)  # (2*n_layers,B,H)\n        h = torch.cat([h[-1], h[-2]], dim=-1)  # last fwd & bwd\n        return h  # (B,2H)\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim, out_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim)\n        )\n\n    def forward(self, h):\n        return self.mlp(h)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, in_dim, n_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.fc = nn.Linear(in_dim, n_classes)\n\n    def forward(self, x):\n        return self.fc(self.encoder(x))\n\n\n# -----------------------------------------------------------------------------------\n# Contrastive Loss ------------------------------------------------------------------\ndef nt_xent(features, temperature=0.1):\n    # features: (2B,D) already L2-normalised\n    B = features.shape[0] // 2\n    sim = torch.mm(features, features.t()) / temperature\n    labels = torch.arange(B, device=features.device)\n    labels = torch.cat([labels + B, labels])  # positives indices\n    mask = torch.eye(2 * B, device=features.device).bool()\n    sim = sim.masked_fill(mask, -9e15)\n    loss = nn.CrossEntropyLoss()(sim, labels)\n    return loss\n\n\n# -----------------------------------------------------------------------------------\n# Build datasets / loaders ----------------------------------------------------------\nsubset_size = min(8000, len(train_raw[\"sequence\"]))  # speed\ncontrast_ds = ContrastiveDataset(train_raw[\"sequence\"][:subset_size], max_len)\ncontra_loader = DataLoader(contrast_ds, batch_size=256, shuffle=True, drop_last=True)\n\ntrain_ds = SPRDataset(train_raw[\"sequence\"], train_raw[\"label\"], max_len)\ndev_ds = SPRDataset(dev_raw[\"sequence\"], dev_raw[\"label\"], max_len)\ntest_ds = SPRDataset(test_raw[\"sequence\"], test_raw[\"label\"], max_len)\n\ntrain_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# -----------------------------------------------------------------------------------\n# Book-keeping dict -----------------------------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"aca\": {\"val\": [], \"test\": []},\n    }\n}\n\n# -----------------------------------------------------------------------------------\n# 1. Contrastive pre-training -------------------------------------------------------\nvocab_size = len(vocab)\nencoder = Encoder(vocab_size).to(device)\nproj = ProjectionHead(in_dim=512).to(device)\nopt_c = torch.optim.Adam(list(encoder.parameters()) + list(proj.parameters()), lr=1e-3)\n\nepochs_c = 5\nfor epoch in range(1, epochs_c + 1):\n    encoder.train()\n    proj.train()\n    tot_loss = 0\n    n = 0\n    for batch in contra_loader:\n        v1 = batch[\"view1\"].to(device)\n        v2 = batch[\"view2\"].to(device)\n        f1 = nn.functional.normalize(proj(encoder(v1)), dim=-1)\n        f2 = nn.functional.normalize(proj(encoder(v2)), dim=-1)\n        feat = torch.cat([f1, f2], 0)\n        loss = nt_xent(feat)\n        opt_c.zero_grad()\n        loss.backward()\n        opt_c.step()\n        bs = v1.size(0)\n        tot_loss += loss.item() * bs\n        n += bs\n    print(f\"Contrastive Epoch {epoch}: loss={tot_loss/n:.4f}\")\n\n# -----------------------------------------------------------------------------------\n# 2. Fine-tuning --------------------------------------------------------------------\nclf = Classifier(encoder, in_dim=512, n_classes=len(set(train_raw[\"label\"]))).to(device)\nopt_f = torch.optim.Adam(clf.parameters(), lr=1e-3)\nce = nn.CrossEntropyLoss()\n\n\ndef accuracy(logits, y):\n    return (logits.argmax(1) == y).float().mean().item()\n\n\ndef eval_loop(model, loader, compute_aca=False, M=3):\n    model.eval()\n    tot_loss = 0\n    n = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = ce(logits, y)\n            bs = y.size(0)\n            tot_loss += loss.item() * bs\n            n += bs\n            p = logits.argmax(1).cpu().tolist()\n            preds.extend(p)\n            gts.extend(y.cpu().tolist())\n            raws.extend(batch[\"raw\"])\n    acc = sum([p == g for p, g in zip(preds, gts)]) / len(preds)\n    aca = None\n    if compute_aca:\n        correct = 0\n        total = 0\n        for raw_seq, g in zip(raws, gts):\n            for _ in range(M):\n                aug_seq = augment(raw_seq)\n                ids = torch.tensor(encode(aug_seq, max_len)).unsqueeze(0).to(device)\n                pred = model(ids).argmax(1).item()\n                correct += int(pred == g)\n                total += 1\n        aca = correct / total\n    return tot_loss / n, acc, aca, preds, gts, raws\n\n\nbest_val_acc = -1\nbest_state = None\nepochs_f = 5\nfor epoch in range(1, epochs_f + 1):\n    clf.train()\n    tot = 0\n    n = 0\n    for batch in train_loader:\n        x = batch[\"x\"].to(device)\n        y = batch[\"y\"].to(device)\n        logits = clf(x)\n        loss = ce(logits, y)\n        opt_f.zero_grad()\n        loss.backward()\n        opt_f.step()\n        bs = y.size(0)\n        tot += loss.item() * bs\n        n += bs\n    train_loss = tot / n\n\n    val_loss, val_acc, val_aca, _, _, _ = eval_loop(clf, dev_loader, compute_aca=True)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_acc={val_acc:.4f} | val_aca={val_aca:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"aca\"][\"val\"].append(val_aca)\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_state = clf.state_dict()\n\n# -----------------------------------------------------------------------------------\n# 3. Evaluation on test -------------------------------------------------------------\nclf.load_state_dict(best_state)\ntest_loss, test_acc, test_aca, test_pred, test_gt, _ = eval_loop(\n    clf, test_loader, compute_aca=True\n)\nprint(f\"Test: loss={test_loss:.4f} | acc={test_acc:.4f} | ACA={test_aca:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"] = []  # not tracked separately\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gt\nexperiment_data[\"SPR_BENCH\"][\"aca\"][\"test\"] = test_aca\n\n# -----------------------------------------------------------------------------------\n# Save everything -------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------------------------- basic setup -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\n\n\n# ------------------------------------------------- metric helpers -----------------------------------------------------\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.strip().split() if tok))\n\n\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\n# ------------------------------------------------ data loading / fallback --------------------------------------------\ndef load_real_spr(root):\n    from datasets import load_dataset\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = {name: _ld(f\"{name}.csv\") for name in (\"train\", \"dev\", \"test\")}\n    return {\"train\": dict(d[\"train\"]), \"dev\": dict(d[\"dev\"]), \"test\": dict(d[\"test\"])}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labs = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labs.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labs}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif SPR_PATH.exists():\n    try:\n        raw = load_real_spr(SPR_PATH)\n    except Exception as e:\n        print(\"Failed loading real SPR, using synthetic.\", e)\n        raw = {}\nelse:\n    raw = {}\nif not raw:  # synthetic fallback\n    raw = {\"train\": make_synth(4000), \"dev\": make_synth(800), \"test\": make_synth(800)}\n\n# ----------------------------------------------- vocabulary & encoding -----------------------------------------------\nspecial = [\"<pad>\", \"<unk>\", \"<mask>\"]\ntokens = set(tok for seq in raw[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {tok: i + len(special) for i, tok in enumerate(sorted(tokens))}\nfor i, s in enumerate(special):\n    vocab[s] = i\npad_idx, unk_idx, mask_idx = vocab[\"<pad>\"], vocab[\"<unk>\"], vocab[\"<mask>\"]\nmax_len = max(len(seq.split()) for seq in raw[\"train\"][\"sequence\"])\n\n\ndef encode(seq):\n    idxs = [vocab.get(t, unk_idx) for t in seq.split()]\n    idxs = idxs[:max_len] + [pad_idx] * (max_len - len(idxs))\n    return torch.tensor(idxs, dtype=torch.long)\n\n\n# ---------------------------------------------- augmentations ---------------------------------------------------------\ndef augment(seq, mask_p=0.15, shuffle_p=0.3):\n    toks = seq.split()\n    # token mask\n    for i in range(len(toks)):\n        if random.random() < mask_p:\n            toks[i] = \"<mask>\"\n    # local shuffle\n    if random.random() < shuffle_p and len(toks) > 2:\n        i, j = sorted(random.sample(range(len(toks)), 2))\n        toks[i:j] = random.sample(toks[i:j], len(toks[i:j]))\n    return \" \".join(toks)\n\n\n# ---------------------------------------------- datasets --------------------------------------------------------------\nclass SPRClsDataset(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": encode(self.seqs[idx]),\n            \"y\": torch.tensor(self.labels[idx]),\n            \"raw\": self.seqs[idx],\n        }\n\n\nclass SPRContrastiveDataset(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s = self.seqs[idx]\n        return {\"x1\": encode(augment(s)), \"x2\": encode(augment(s))}\n\n\ntrain_contrast_ds = SPRContrastiveDataset(raw[\"train\"][\"sequence\"])\ntrain_cls_ds = SPRClsDataset(raw[\"train\"][\"sequence\"], raw[\"train\"][\"label\"])\ndev_cls_ds = SPRClsDataset(raw[\"dev\"][\"sequence\"], raw[\"dev\"][\"label\"])\ntest_cls_ds = SPRClsDataset(raw[\"test\"][\"sequence\"], raw[\"test\"][\"label\"])\n\ncontr_loader = DataLoader(train_contrast_ds, batch_size=256, shuffle=True)\ntrain_loader = DataLoader(train_cls_ds, batch_size=256, shuffle=True)\ndev_loader = DataLoader(dev_cls_ds, batch_size=512)\ntest_loader = DataLoader(test_cls_ds, batch_size=512)\n\n\n# ---------------------------------------------- models ----------------------------------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_idx)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        e = self.emb(x)\n        o, _ = self.rnn(e)\n        rep = o.mean(1)  # (B,2*hid)\n        return nn.functional.normalize(rep, dim=1)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.fc = nn.Linear(256, num_classes)  # 2*hid\n\n    def forward(self, x):\n        z = self.encoder(x)\n        return self.fc(z)\n\n\n# NT-Xent loss\ndef nt_xent(z1, z2, temp=0.5):\n    N = z1.size(0)\n    z = torch.cat([z1, z2], 0)\n    sim = torch.matmul(z, z.T) / temp\n    sim_exp = torch.exp(sim - torch.max(sim, dim=1, keepdim=True)[0])\n    mask = (~torch.eye(2 * N, dtype=torch.bool, device=z1.device)).float()\n    sim_exp = sim_exp * mask\n    denom = sim_exp.sum(1, keepdim=True)\n    # positives: (i, i+N) and (i+N,i)\n    pos = torch.exp(torch.sum(z1 * z2, dim=1) / temp)\n    loss = -torch.log(pos / (denom[:N, 0])) - torch.log(pos / (denom[N:, 0]))\n    return loss.mean()\n\n\n# ------------------------------------------- experiment bookkeeping ---------------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"ACA\": {\"val\": [], \"test\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------------------------------- pre-training -------------------------------------------------------------\nenc = Encoder(len(vocab)).to(device)\noptim_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\n\npre_epochs = 3\nprint(\"\\nPre-training encoder (contrastive)...\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot_loss = 0\n    n = 0\n    for batch in contr_loader:\n        b = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = enc(b[\"x1\"]), enc(b[\"x2\"])\n        loss = nt_xent(z1, z2)\n        optim_enc.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        tot_loss += loss.item() * z1.size(0)\n        n += z1.size(0)\n    print(f\"  Contrastive epoch {ep}: loss={tot_loss/n:.4f}\")\n\n# ------------------------------------------- fine-tuning --------------------------------------------------------------\nnum_classes = len(set(raw[\"train\"][\"label\"]))\nmodel = Classifier(enc, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\nepochs = 5\n\n\ndef evaluate(model, loader):\n    model.eval()\n    preds, gts, raws, tot_loss, n = [], [], [], 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            batch_t = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"x\"])\n            loss = criterion(logits, batch_t[\"y\"])\n            tot_loss += loss.item() * batch_t[\"y\"].size(0)\n            n += batch_t[\"y\"].size(0)\n            p = logits.argmax(1).cpu().tolist()\n            preds.extend(p)\n            gts.extend(batch_t[\"y\"].cpu().tolist())\n            raws.extend(batch_t[\"raw\"])\n    return tot_loss / n, preds, gts, raws\n\n\ndef compute_aca(model, seqs, labels, M=3):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            for _ in range(M):\n                aug_s = augment(s, mask_p=0.1, shuffle_p=0.2)\n                logits = model(encode(aug_s).unsqueeze(0).to(device))\n                pred = logits.argmax(1).item()\n                if pred == l:\n                    correct += 1\n    return correct / (len(seqs) * M)\n\n\nbest_val_swa = -1\nfor ep in range(1, epochs + 1):\n    # training\n    model.train()\n    tot_loss = 0\n    n = 0\n    for batch in train_loader:\n        batch_t = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch_t[\"x\"])\n        loss = criterion(logits, batch_t[\"y\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch_t[\"y\"].size(0)\n        n += batch_t[\"y\"].size(0)\n    train_loss = tot_loss / n\n\n    # validation\n    val_loss, val_preds, val_gts, val_raws = evaluate(model, dev_loader)\n    val_swa = shape_weighted_accuracy(val_raws, val_gts, val_preds)\n    val_aca = compute_aca(model, raw[\"dev\"][\"sequence\"], raw[\"dev\"][\"label\"], M=3)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"ACA\"][\"val\"].append(val_aca)\n\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | SWA={val_swa:.4f} | ACA={val_aca:.4f}\"\n    )\n\n    if val_swa > best_val_swa:\n        best_val_swa = val_swa\n        best_state = model.state_dict()\n\n# --------------------------------------------- testing ----------------------------------------------------------------\nmodel.load_state_dict(best_state)\ntest_loss, test_preds, test_gts, test_raws = evaluate(model, test_loader)\ntest_swa = shape_weighted_accuracy(test_raws, test_gts, test_preds)\ntest_cwa = color_weighted_accuracy(test_raws, test_gts, test_preds)\ntest_aca = compute_aca(model, raw[\"test\"][\"sequence\"], raw[\"test\"][\"label\"], M=3)\n\nprint(f\"\\nTest results: SWA={test_swa:.4f} | CWA={test_cwa:.4f} | ACA={test_aca:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_SWA\"] = test_swa\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_CWA\"] = test_cwa\nexperiment_data[\"SPR_BENCH\"][\"ACA\"][\"test\"] = test_aca\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Set working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# Reproducibility\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers from baseline\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Try to load real SPR_BENCH, else make synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _l(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _l(\"train.csv\"),\n            \"dev\": _l(\"dev.csv\"),\n            \"test\": _l(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw = raw[\"train\"]\n    dev_raw = raw[\"dev\"]\n    test_raw = raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(4000)\n    dev_data = make_synth(800)\n    test_data = make_synth(800)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nspecial = {\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3}\nvocab.update(special)\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[\"<pad>\"],\n    vocab[\"<unk>\"],\n    vocab[\"<mask>\"],\n    vocab[\"<cls>\"],\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # +1 for <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Dataset wrappers\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Augmentations for contrastive learning\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        segment = toks[i : i + window]\n        random.shuffle(segment)\n        toks[i : i + window] = segment\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Encoders / models\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=64, hid=128, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        emb = self.emb(x)\n        _, h = self.gru(emb)\n        return h[-1]  # (B,H)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Contrastive utilities\ndef contrastive_loss(z, temp=0.5):\n    \"\"\"\n    z: (2N, d) \u2013 first N entries positive to last N\n    \"\"\"\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp\n    N = z.size(0)\n    mask = torch.eye(N, device=z.device).bool()\n    sim.masked_fill_(mask, -9e15)\n    targets = torch.arange(N, device=z.device)\n    targets = (targets + N // 2) % N  # 0<->N/2, 1<->N/2+1 ...\n    loss = nn.functional.cross_entropy(sim, targets)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# Build data loaders\nbatch_c = 256\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=batch_c, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Book-keeping dict\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n        \"test\": {},\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 3\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    t0 = time.time()\n    tot = cnt = 0\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt.zero_grad()\n        z = enc(x)\n        loss = contrastive_loss(z)\n        loss.backward()\n        opt.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f} in {time.time()-t0:.1f}s\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning for classification\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=128, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 6\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels).to(device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, label, M=3):\n    \"\"\"Augmentation Consistency Accuracy on a list of seqs\"\"\"\n    correct = 0\n    total = 0\n    for s, l in zip(seqs, label):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(pi == l for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels).to(device)\n        opt_cls.zero_grad()\n        logits = model(x)\n        loss = crit(logits, y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers from baseline\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Corrected contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n        \"test\": {},\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------------------------------------------------------\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# -------------------------------------------------------------------------------\n# reproducibility + device\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------------------------------\n# helper metrics\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split()))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] if len(tok) > 1 else \"#\" for tok in seq.split()))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / (\n        sum(w) + 1e-9\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / (\n        sum(w) + 1e-9\n    )\n\n\n# -------------------------------------------------------------------------------\n# dataset loader (real or synthetic)\ndef load_spr(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for f in [\"train.csv\", \"dev.csv\", \"test.csv\"]:\n        d[f.split(\".\")[0]] = _ld(f)\n    return d\n\n\ndef make_synth(n):\n    shapes = list(\"ABCDE\")\n    colors = list(\"12345\")\n    seqs = []\n    labels = []\n    for _ in range(n):\n        L = random.randint(4, 10)\n        s = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(s)\n        labels.append(int(count_shape_variety(s) >= count_color_variety(s)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    raw = load_spr(SPR_PATH)\n    print(\"Loaded real SPR_BENCH\")\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\nexcept Exception as e:\n    print(\"Falling back to synthetic SPR set:\", e)\n    train_raw, dev_raw, test_raw = [make_synth(n) for n in (8000, 1500, 1500)]\n\n# -------------------------------------------------------------------------------\n# vocab building: separate shape and color\nshapes = set(tok[0] for tok in train_raw[\"sequence\"])\ncolors = set((tok[1] if len(tok) > 1 else \"#\") for tok in train_raw[\"sequence\"])\nshape2id = {s: i + 3 for i, s in enumerate(sorted(shapes))}\ncolor2id = {c: i + 3 for i, c in enumerate(sorted(colors))}\nshape2id.update({\"<pad>\": 0, \"<cls>\": 1, \"<unk>\": 2})\ncolor2id.update({\"<pad>\": 0, \"<cls>\": 1, \"<unk>\": 2})\npad_sid, cls_sid, unk_sid = 0, 1, 2\npad_cid, cls_cid, unk_cid = 0, 1, 2\n\nmax_len = max(len(seq.split()) for seq in train_raw[\"sequence\"]) + 1  # +cls\n\n\ndef encode(seq: str, max_len=max_len):\n    s_ids = [cls_sid]\n    c_ids = [cls_cid]\n    for tok in seq.split():\n        s_ids.append(shape2id.get(tok[0], unk_sid))\n        c_ids.append(color2id.get(tok[1] if len(tok) > 1 else \"#\", unk_cid))\n    s_ids = s_ids[:max_len] + [pad_sid] * max(0, max_len - len(s_ids))\n    c_ids = c_ids[:max_len] + [pad_cid] * max(0, max_len - len(c_ids))\n    return s_ids, c_ids\n\n\n# -------------------------------------------------------------------------------\n# augmentations (context-aware)\ndef augment(seq: str):\n    toks = seq.split()\n    op = random.choice([\"mask\", \"shape_shuffle\", \"color_shuffle\", \"identity\"])\n    if op == \"mask\":\n        for i in range(len(toks)):\n            if random.random() < 0.15:\n                toks[i] = \"X#\"  # placeholder token\n    elif op == \"shape_shuffle\" and len(toks) > 1:\n        random.shuffle(toks)\n    elif op == \"color_shuffle\":\n        colors = [tok[1] if len(tok) > 1 else \"#\" for tok in toks]\n        random.shuffle(colors)\n        toks = [tok[0] + c for tok, c in zip(toks, colors)]\n    return \" \".join(toks)\n\n\n# -------------------------------------------------------------------------------\n# torch datasets\nclass ContrastiveSPR(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        v1 = encode(augment(self.seqs[idx]))\n        v2 = encode(augment(self.seqs[idx]))\n        return {\n            \"s1\": torch.tensor(v1[0]),\n            \"c1\": torch.tensor(v1[1]),\n            \"s2\": torch.tensor(v2[0]),\n            \"c2\": torch.tensor(v2[1]),\n        }\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs = seqs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s_ids, c_ids = encode(self.seqs[idx])\n        return {\n            \"s\": torch.tensor(s_ids),\n            \"c\": torch.tensor(c_ids),\n            \"y\": torch.tensor(self.labels[idx]),\n            \"raw\": self.seqs[idx],\n        }\n\n\n# -------------------------------------------------------------------------------\n# model: shape+color embeddings -> summed -> Transformer -> CLS\nclass ContextEncoder(nn.Module):\n    def __init__(self, shape_vocab, color_vocab, emb=64, n_layers=2, n_heads=4):\n        super().__init__()\n        self.shape_emb = nn.Embedding(len(shape_vocab), emb, padding_idx=pad_sid)\n        self.color_emb = nn.Embedding(len(color_vocab), emb, padding_idx=pad_cid)\n        self.pos_emb = nn.Embedding(max_len, emb)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb, nhead=n_heads, dim_feedforward=emb * 4, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n    def forward(self, s_ids, c_ids):\n        B, L = s_ids.shape\n        pos = torch.arange(L, device=s_ids.device).unsqueeze(0).expand(B, L)\n        x = self.shape_emb(s_ids) + self.color_emb(c_ids) + self.pos_emb(pos)\n        mask = s_ids == pad_sid\n        h = self.transformer(x, src_key_padding_mask=mask)  # (B,L,emb)\n        return h[:, 0]  # CLS\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim, proj_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, proj_dim), nn.ReLU(), nn.Linear(proj_dim, proj_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass SPRClassifier(nn.Module):\n    def __init__(self, encoder, in_dim, n_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.fc = nn.Linear(in_dim, n_classes)\n\n    def forward(self, s_ids, c_ids):\n        h = self.encoder(s_ids, c_ids)\n        return self.fc(h)\n\n\n# -------------------------------------------------------------------------------\n# loss: InfoNCE\ndef info_nce(z, temperature=0.1):\n    B = z.size(0) // 2\n    z = nn.functional.normalize(z, dim=-1)\n    sim = torch.mm(z, z.t()) / temperature\n    labels = torch.arange(B, device=z.device)\n    labels = torch.cat([labels + B, labels])\n    sim.fill_diagonal_(-9e15)\n    return nn.CrossEntropyLoss()(sim, labels)\n\n\n# -------------------------------------------------------------------------------\n# build data loaders\nsubset = 5000 if isinstance(train_raw, dict) else min(5000, len(train_raw))\ncontrast_ds = ContrastiveSPR(\n    train_raw[\"sequence\"][:subset]\n    if isinstance(train_raw, dict)\n    else train_raw[\"sequence\"][:subset]\n)\ncontra_loader = DataLoader(contrast_ds, batch_size=256, shuffle=True, drop_last=True)\n\ntrain_ds = SPRDataset(train_raw[\"sequence\"], train_raw[\"label\"])\ndev_ds = SPRDataset(dev_raw[\"sequence\"], dev_raw[\"label\"])\ntest_ds = SPRDataset(test_raw[\"sequence\"], test_raw[\"label\"])\ntrain_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# -------------------------------------------------------------------------------\n# bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"val_acc\": [], \"val_swa\": [], \"val_cwa\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"aca\": {\"val\": [], \"test\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# -------------------------------------------------------------------------------\n# initialise models\nemb_dim = 64\nencoder = ContextEncoder(shape2id, color2id, emb=emb_dim).to(device)\nproj = ProjectionHead(emb_dim, 128).to(device)\nopt_c = torch.optim.Adam(list(encoder.parameters()) + list(proj.parameters()), lr=1e-3)\n\n# -------------------------------------------------------------------------------\n# 1. contrastive pre-training\nfor epoch in range(1, 4):\n    encoder.train()\n    proj.train()\n    tot, n = 0, 0\n    for batch in contra_loader:\n        s1 = batch[\"s1\"].to(device)\n        c1 = batch[\"c1\"].to(device)\n        s2 = batch[\"s2\"].to(device)\n        c2 = batch[\"c2\"].to(device)\n        z1 = proj(encoder(s1, c1))\n        z2 = proj(encoder(s2, c2))\n        loss = info_nce(torch.cat([z1, z2], 0))\n        opt_c.zero_grad()\n        loss.backward()\n        opt_c.step()\n        bs = s1.size(0)\n        tot += loss.item() * bs\n        n += bs\n    print(f\"Contrastive Epoch {epoch}: loss={tot/n:.4f}\")\n\n# -------------------------------------------------------------------------------\n# 2. fine-tuning\nclf = SPRClassifier(encoder, emb_dim, len(set(train_raw[\"label\"]))).to(device)\nopt_f = torch.optim.Adam(clf.parameters(), lr=2e-3)\ncriterion = nn.CrossEntropyLoss()\n\nbest_val = -1\nbest_state = None\n\n\ndef eval_model(model, loader, compute_aca=False, M=3):\n    model.eval()\n    tot, n = 0, 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for b in loader:\n            s = b[\"s\"].to(device)\n            c = b[\"c\"].to(device)\n            y = b[\"y\"].to(device)\n            logits = model(s, c)\n            loss = criterion(logits, y)\n            tot += loss.item() * y.size(0)\n            n += y.size(0)\n            p = logits.argmax(1).cpu().tolist()\n            preds += p\n            gts += y.cpu().tolist()\n            raws += b[\"raw\"]\n    loss = tot / n\n    acc = sum(p == g for p, g in zip(preds, gts)) / len(preds)\n    swa = shape_weighted_accuracy(raws, gts, preds)\n    cwa = color_weighted_accuracy(raws, gts, preds)\n    aca = None\n    if compute_aca:\n        correct = 0\n        total = 0\n        for raw, g in zip(raws, gts):\n            for _ in range(M):\n                aug = augment(raw)\n                s_ids, c_ids = encode(aug)\n                pred = (\n                    model(\n                        torch.tensor(s_ids).unsqueeze(0).to(device),\n                        torch.tensor(c_ids).unsqueeze(0).to(device),\n                    )\n                    .argmax(1)\n                    .item()\n                )\n                correct += int(pred == g)\n                total += 1\n        aca = correct / total\n    return loss, acc, swa, cwa, aca, preds, gts\n\n\nfor epoch in range(1, 4):\n    clf.train()\n    tot, n = 0, 0\n    for b in train_loader:\n        s = b[\"s\"].to(device)\n        c = b[\"c\"].to(device)\n        y = b[\"y\"].to(device)\n        logits = clf(s, c)\n        loss = criterion(logits, y)\n        opt_f.zero_grad()\n        loss.backward()\n        opt_f.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, val_swa, val_cwa, val_aca, _, _ = eval_model(\n        clf, dev_loader, compute_aca=True\n    )\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_swa\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_cwa\"].append(val_cwa)\n    experiment_data[\"SPR_BENCH\"][\"aca\"][\"val\"].append(val_aca)\n    if val_acc > best_val:\n        best_val = val_acc\n        best_state = {k: v.cpu() for k, v in clf.state_dict().items()}\n\n# -------------------------------------------------------------------------------\n# 3. test evaluation\nclf.load_state_dict({k: v.to(device) for k, v in best_state.items()})\ntest_loss, test_acc, test_swa, test_cwa, test_aca, test_pred, test_gt = eval_model(\n    clf, test_loader, compute_aca=True\n)\nprint(\n    f\"Test: loss={test_loss:.4f} | acc={test_acc:.4f} | SWA={test_swa:.4f} | CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gt\nexperiment_data[\"SPR_BENCH\"][\"aca\"][\"test\"] = test_aca\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --------------------------------------------------------------------------------- paths / dirs\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------------------- device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------------------------------------------------------------------- reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\n\n\n# --------------------------------------------------------------------------------- metric helpers\ndef count_shape_variety(seq):\n    return len(set(tok[0] for tok in seq.strip().split() if tok))\n\n\ndef count_color_variety(seq):\n    return len(set(tok[1] for tok in seq.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\n# --------------------------------------------------------------------------------- data loading\ndef load_real_spr(root: pathlib.Path):\n    \"\"\"Return splits as python dicts (columns -> list).  FIXED .to_dict() bug\"\"\"\n    from datasets import load_dataset\n\n    def _ld(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    splits = {name: _ld(f\"{name}.csv\") for name in (\"train\", \"dev\", \"test\")}\n    # convert each HF Dataset to pure python dict (column -> list)\n    return {name: splits[name].to_dict() for name in splits}\n\n\ndef make_synth(n: int):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labs = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labs.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labs}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif SPR_PATH.exists():\n    try:\n        raw = load_real_spr(SPR_PATH)\n        print(\"Loaded real SPR_BENCH dataset\")\n    except Exception as e:\n        print(\"Failed loading real SPR, using synthetic instead.\", e)\n        raw = {}\nelse:\n    raw = {}\nif not raw:\n    raw = {\"train\": make_synth(4000), \"dev\": make_synth(800), \"test\": make_synth(800)}\n    print(\"Synthetic dataset created\")\n\n# --------------------------------------------------------------------------------- vocab & encoding\nspecial = [\"<pad>\", \"<unk>\", \"<mask>\"]\ntokens = set(tok for seq in raw[\"train\"][\"sequence\"] for tok in seq.split())\nvocab = {tok: i + len(special) for i, tok in enumerate(sorted(tokens))}\nfor i, t in enumerate(special):\n    vocab[t] = i\npad_idx, unk_idx, mask_idx = vocab[\"<pad>\"], vocab[\"<unk>\"], vocab[\"<mask>\"]\nmax_len = max(len(seq.split()) for seq in raw[\"train\"][\"sequence\"])\n\n\ndef encode(seq: str):\n    idxs = [vocab.get(t, unk_idx) for t in seq.split()]\n    idxs = idxs[:max_len] + [pad_idx] * (max_len - len(idxs))\n    return torch.tensor(idxs, dtype=torch.long)\n\n\n# --------------------------------------------------------------------------------- augmentation\ndef augment(seq, mask_p=0.15, shuffle_p=0.30):\n    toks = seq.split()\n    # mask\n    for i in range(len(toks)):\n        if random.random() < mask_p:\n            toks[i] = \"<mask>\"\n    # shuffle contiguous span\n    if random.random() < shuffle_p and len(toks) > 2:\n        i, j = sorted(random.sample(range(len(toks)), 2))\n        toks[i:j] = random.sample(toks[i:j], len(toks[i:j]))\n    return \" \".join(toks)\n\n\n# --------------------------------------------------------------------------------- datasets\nclass SPRClsDataset(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": encode(self.seqs[idx]),\n            \"y\": torch.tensor(self.labels[idx]),\n            \"raw\": self.seqs[idx],\n        }\n\n\nclass SPRContrastiveDataset(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s = self.seqs[idx]\n        return {\"x1\": encode(augment(s)), \"x2\": encode(augment(s))}\n\n\ntrain_contrast_ds = SPRContrastiveDataset(raw[\"train\"][\"sequence\"])\ntrain_cls_ds = SPRClsDataset(raw[\"train\"][\"sequence\"], raw[\"train\"][\"label\"])\ndev_cls_ds = SPRClsDataset(raw[\"dev\"][\"sequence\"], raw[\"dev\"][\"label\"])\ntest_cls_ds = SPRClsDataset(raw[\"test\"][\"sequence\"], raw[\"test\"][\"label\"])\n\ncontr_loader = DataLoader(train_contrast_ds, batch_size=256, shuffle=True)\ntrain_loader = DataLoader(train_cls_ds, batch_size=256, shuffle=True)\ndev_loader = DataLoader(dev_cls_ds, batch_size=512)\ntest_loader = DataLoader(test_cls_ds, batch_size=512)\n\n\n# --------------------------------------------------------------------------------- models\nclass Encoder(nn.Module):\n    def __init__(self, vocab_sz, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_idx)\n        self.rnn = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        x = x.to(device)\n        rep, _ = self.rnn(self.emb(x))\n        rep = rep.mean(1)\n        return nn.functional.normalize(rep, dim=1)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        return self.fc(z)\n\n\ndef nt_xent(z1, z2, temp=0.5):\n    N = z1.size(0)\n    z = torch.cat([z1, z2], 0)\n    sim = torch.mm(z, z.T) / temp\n    sim_exp = torch.exp(sim - torch.max(sim, 1, keepdim=True)[0])\n    mask = (~torch.eye(2 * N, dtype=torch.bool, device=z1.device)).float()\n    sim_exp = sim_exp * mask\n    denom = sim_exp.sum(1, keepdim=True)\n    pos = torch.exp((z1 * z2).sum(1) / temp)\n    loss = -torch.log(pos / denom[:N, 0]) - torch.log(pos / denom[N:, 0])\n    return loss.mean()\n\n\n# --------------------------------------------------------------------------------- bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"ACA\": {\"val\": [], \"test\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------------------------------------------------------------------------- pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 3\nprint(\"\\nPre-training encoder (contrastive)\u2026\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot, n = 0, 0\n    for batch in contr_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        z1, z2 = enc(batch[\"x1\"]), enc(batch[\"x2\"])\n        loss = nt_xent(z1, z2)\n        opt_enc.zero_grad()\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item() * z1.size(0)\n        n += z1.size(0)\n    print(f\"  Contrastive epoch {ep}: loss={tot/n:.4f}\")\n\n# --------------------------------------------------------------------------------- fine-tuning\nmodel = Classifier(enc, num_classes=len(set(raw[\"train\"][\"label\"]))).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\nepochs = 5\n\n\ndef evaluate(loader):\n    model.eval()\n    preds, gts, raws, tot, n = [], [], [], 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            raws.extend(batch[\"raw\"])\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            tot += loss.item() * batch[\"y\"].size(0)\n            n += batch[\"y\"].size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(batch[\"y\"].cpu().tolist())\n    return tot / n, preds, gts, raws\n\n\ndef compute_aca(seqs, labels, M=3):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for s, l in zip(seqs, labels):\n            for _ in range(M):\n                aug = encode(augment(s, 0.1, 0.2)).unsqueeze(0).to(device)\n                if model(aug).argmax(1).item() == l:\n                    correct += 1\n    return correct / (len(seqs) * M)\n\n\nbest_val_swa = -1\nfor ep in range(1, epochs + 1):\n    # train\n    model.train()\n    tot, n = 0, 0\n    for batch in train_loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        optimizer.zero_grad()\n        loss = criterion(model(batch[\"x\"]), batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        tot += loss.item() * batch[\"y\"].size(0)\n        n += batch[\"y\"].size(0)\n    train_loss = tot / n\n\n    val_loss, val_preds, val_gts, val_raws = evaluate(dev_loader)\n    val_swa = shape_weighted_accuracy(val_raws, val_gts, val_preds)\n    val_aca = compute_aca(raw[\"dev\"][\"sequence\"], raw[\"dev\"][\"label\"], M=3)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"ACA\"][\"val\"].append(val_aca)\n\n    print(\n        f\"Epoch {ep}: validation_loss = {val_loss:.4f} | SWA={val_swa:.4f} | ACA={val_aca:.4f}\"\n    )\n\n    if val_swa > best_val_swa:\n        best_val_swa = val_swa\n        best_state = {\n            k: v.cpu() for k, v in model.state_dict().items()\n        }  # save to CPU to avoid GPU ref\n\n# --------------------------------------------------------------------------------- test\nmodel.load_state_dict(best_state)\ntest_loss, test_preds, test_gts, test_raws = evaluate(test_loader)\ntest_swa = shape_weighted_accuracy(test_raws, test_gts, test_preds)\ntest_cwa = color_weighted_accuracy(test_raws, test_gts, test_preds)\ntest_aca = compute_aca(raw[\"test\"][\"sequence\"], raw[\"test\"][\"label\"], M=3)\n\nprint(f\"\\nTest results: SWA={test_swa:.4f} | CWA={test_cwa:.4f} | ACA={test_aca:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_SWA\"] = test_swa\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_CWA\"] = test_cwa\nexperiment_data[\"SPR_BENCH\"][\"ACA\"][\"test\"] = test_aca\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------------------------------------------------\n# Workspace folder\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# ---------------------------------------------------------------------\n# Reproducibility & device\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# SPR helpers (copied from utility)\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(c) / (sum(w) + 1e-9)\n\n\n# ---------------------------------------------------------------------\n# Load real SPR_BENCH or create toy synthetic fallback\ndef load_spr_folder(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict\n\n    def _ld(fname):\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    for part in [\"train\", \"dev\", \"test\"]:\n        d[part] = _ld(f\"{part}.csv\")\n    return d\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(4, 10)\n        s = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(s)\n        labels.append(int(count_shape_variety(s) >= count_color_variety(s)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ntry:\n    raw = load_spr_folder(SPR_PATH)\n    print(\"Loaded SPR_BENCH dataset.\")\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\nexcept Exception as e:\n    print(\"Falling back to synthetic data ->\", e)\n    train_raw, dev_raw, test_raw = (\n        make_synth(6000),\n        make_synth(1200),\n        make_synth(1200),\n    )\n\n# ---------------------------------------------------------------------\n# Vocabulary build\nspecial = [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\nvocab = {tok: i for i, tok in enumerate(special)}\noffset = len(vocab)\nall_tokens = {tok for seq in train_raw[\"sequence\"] for tok in seq.split()}\nfor i, tok in enumerate(sorted(all_tokens)):\n    vocab[tok] = i + offset\npad_idx, unk_idx, mask_idx, cls_idx = [vocab[s] for s in special]\n\n\ndef encode(seq, max_len):\n    ids = [cls_idx] + [vocab.get(t, unk_idx) for t in seq.split()]\n    ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n    return ids\n\n\nMAX_LEN = max(len(s.split()) for s in train_raw[\"sequence\"]) + 1\n\n\n# ---------------------------------------------------------------------\n# Augmentation (context-preserving)\ndef augment(seq: str):\n    toks = seq.split()\n    # random mask\n    for i in range(len(toks)):\n        if random.random() < 0.15:\n            toks[i] = \"<mask>\"\n    # light shuffle (swap)\n    if len(toks) > 3 and random.random() < 0.4:\n        i, j = random.sample(range(len(toks)), 2)\n        toks[i], toks[j] = toks[j], toks[i]\n    # token deletion\n    if len(toks) > 4 and random.random() < 0.3:\n        del toks[random.randrange(len(toks))]\n    return \" \".join(toks)\n\n\n# ---------------------------------------------------------------------\n# Datasets\nclass ContrastiveDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s = self.seqs[idx]\n        v1 = torch.tensor(encode(augment(s), MAX_LEN))\n        v2 = torch.tensor(encode(augment(s), MAX_LEN))\n        return {\"v1\": v1, \"v2\": v2}\n\n\nclass SPRDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": torch.tensor(encode(self.seqs[idx], MAX_LEN)),\n            \"y\": torch.tensor(self.labels[idx]),\n            \"raw\": self.seqs[idx],\n        }\n\n\n# ---------------------------------------------------------------------\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=128, hid=256, layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.rnn = nn.GRU(emb_dim, hid, layers, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        e = self.emb(x)\n        _, h = self.rnn(e)\n        h = torch.cat([h[-1], h[-2]], dim=-1)\n        return h\n\n\nclass Projection(nn.Module):\n    def __init__(self, in_dim, out_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim)\n        )\n\n    def forward(self, z):\n        return self.net(z)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, emb_dim, n_cls):\n        super().__init__()\n        self.encoder = encoder\n        self.fc = nn.Linear(emb_dim, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.encoder(x))\n\n\n# ---------------------------------------------------------------------\n# Losses\ndef nt_xent(feat, T=0.1):\n    B = feat.shape[0] // 2\n    sim = torch.mm(feat, feat.t()) / T\n    labels = torch.arange(B, device=feat.device)\n    labels = torch.cat([labels + B, labels])\n    sim = sim - torch.eye(2 * B, device=feat.device) * 1e9\n    return nn.CrossEntropyLoss()(sim, labels)\n\n\ndef sup_contrast(features, labels, T=0.1):\n    # features: (N,D) L2-normed, labels: (N,)\n    N = features.size(0)\n    sim = torch.div(torch.matmul(features, features.T), T)\n    sim = sim - torch.eye(N, device=features.device) * 1e9\n    labels = labels.contiguous().view(-1, 1)\n    mask = torch.eq(labels, labels.T).float().to(features.device)\n    exp = torch.exp(sim)\n    log_prob = sim - torch.log(exp.sum(1, keepdim=True))\n    mean_log_pos = (mask * log_prob).sum(1) / mask.sum(1)\n    loss = -mean_log_pos.mean()\n    return loss\n\n\n# ---------------------------------------------------------------------\n# Prepare data loaders\nsubset = min(6000, len(train_raw[\"sequence\"]))  # contrastive subset\nc_loader = DataLoader(\n    ContrastiveDS(train_raw[\"sequence\"][:subset]),\n    batch_size=256,\n    shuffle=True,\n    drop_last=True,\n)\n\ntrain_dl = DataLoader(\n    SPRDS(train_raw[\"sequence\"], train_raw[\"label\"]), batch_size=256, shuffle=True\n)\ndev_dl = DataLoader(SPRDS(dev_raw[\"sequence\"], dev_raw[\"label\"]), batch_size=256)\ntest_dl = DataLoader(SPRDS(test_raw[\"sequence\"], test_raw[\"label\"]), batch_size=256)\n\n# ---------------------------------------------------------------------\n# Book-keeping dict\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"aca\": {\"val\": [], \"test\": []},\n        \"swa\": {\"val\": [], \"test\": []},\n        \"cwa\": {\"val\": [], \"test\": []},\n    }\n}\n\n# ---------------------------------------------------------------------\n# Build models\nenc = Encoder(len(vocab)).to(device)\nproj = Projection(512).to(device)\nopt_c = torch.optim.Adam(list(enc.parameters()) + list(proj.parameters()), lr=1e-3)\n\n# ---------------- Contrastive warm-up -------------------------------\nfor ep in range(1, 4):\n    enc.train()\n    proj.train()\n    tot, n = 0, 0\n    for batch in c_loader:\n        v1 = batch[\"v1\"].to(device)\n        v2 = batch[\"v2\"].to(device)\n        f1 = nn.functional.normalize(proj(enc(v1)), dim=-1)\n        f2 = nn.functional.normalize(proj(enc(v2)), dim=-1)\n        loss = nt_xent(torch.cat([f1, f2], 0))\n        opt_c.zero_grad()\n        loss.backward()\n        opt_c.step()\n        bs = v1.size(0)\n        tot += loss.item() * bs\n        n += bs\n    print(f\"Contrastive epoch {ep}: loss={tot/n:.4f}\")\n\n# ---------------- Fine-tuning with SupCon + CE ----------------------\nclf = Classifier(enc, 512, len(set(train_raw[\"label\"]))).to(device)\nopt_f = torch.optim.Adam(clf.parameters(), lr=1e-3)\nce = nn.CrossEntropyLoss()\nlam = 0.1  # weight for SupCon\n\n\ndef accuracy(pred, y):\n    return (pred == y).float().mean().item()\n\n\ndef evaluate(model, loader, compute_aca=False, M=3):\n    model.eval()\n    loss_tot, n = 0, 0\n    preds, gts, raws = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            logits = model(x)\n            loss = ce(logits, y)\n            bs = y.size(0)\n            loss_tot += loss.item() * bs\n            n += bs\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(y.cpu().tolist())\n            raws.extend(batch[\"raw\"])\n    acc = sum(p == g for p, g in zip(preds, gts)) / len(preds)\n    swa = shape_weighted_accuracy(raws, gts, preds)\n    cwa = color_weighted_accuracy(raws, gts, preds)\n    aca = None\n    if compute_aca:\n        corr, tot = 0, 0\n        for raw_seq, g in zip(raws, gts):\n            for _ in range(M):\n                aug_ids = (\n                    torch.tensor(encode(augment(raw_seq), MAX_LEN))\n                    .unsqueeze(0)\n                    .to(device)\n                )\n                pred = model(aug_ids).argmax(1).item()\n                corr += int(pred == g)\n                tot += 1\n        aca = corr / tot\n    return loss_tot / n, acc, swa, cwa, aca, preds, gts, raws\n\n\nbest_acc, best_state = -1, None\nfor epoch in range(1, 6):\n    clf.train()\n    t_loss, samp = 0, 0\n    for batch in train_dl:\n        x = batch[\"x\"].to(device)\n        y = batch[\"y\"].to(device)\n        raws = batch[\"raw\"]\n        # build augmented view\n        aug_ids = torch.tensor(\n            [encode(augment(s), MAX_LEN) for s in raws], dtype=torch.long\n        ).to(device)\n        y_dup = torch.cat([y, y], 0)\n\n        feats = nn.functional.normalize(clf.encoder(torch.cat([x, aug_ids], 0)), dim=-1)\n        sup_loss = sup_contrast(feats, y_dup)\n        logits = clf.fc(feats[: x.size(0)])  # only originals for CE\n        ce_loss = ce(logits, y)\n        loss = ce_loss + lam * sup_loss\n\n        opt_f.zero_grad()\n        loss.backward()\n        opt_f.step()\n        bs = y.size(0)\n        t_loss += loss.item() * bs\n        samp += bs\n    train_loss = t_loss / samp\n\n    val_loss, val_acc, val_swa, val_cwa, val_aca, *_ = evaluate(clf, dev_dl, True)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | acc={val_acc:.4f} \"\n        f\"| SWA={val_swa:.4f} | CWA={val_cwa:.4f} | ACA={val_aca:.4f}\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"swa\"][\"val\"].append(val_swa)\n    experiment_data[\"SPR_BENCH\"][\"cwa\"][\"val\"].append(val_cwa)\n    experiment_data[\"SPR_BENCH\"][\"aca\"][\"val\"].append(val_aca)\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        best_state = {k: v.cpu() for k, v in clf.state_dict().items()}\n\n# ---------------- Test evaluation -----------------------------------\nclf.load_state_dict(best_state)\ntest_loss, test_acc, test_swa, test_cwa, test_aca, test_pred, test_gt, _ = evaluate(\n    clf, test_dl, True\n)\nprint(\n    f\"TEST: loss={test_loss:.4f} | acc={test_acc:.4f} | SWA={test_swa:.4f} \"\n    f\"| CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gt\nexperiment_data[\"SPR_BENCH\"][\"swa\"][\"test\"] = test_swa\nexperiment_data[\"SPR_BENCH\"][\"cwa\"][\"test\"] = test_cwa\nexperiment_data[\"SPR_BENCH\"][\"aca\"][\"test\"] = test_aca\n\n# ---------------------------------------------------------------------\n# Save metrics\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data ->\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers from baseline\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Corrected contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n        \"test\": {},\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers from baseline\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Corrected contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n        \"test\": {},\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------------------------------------------------------------#\n# Boiler-plate & reproducibility\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nset_seed(42)\n\n\n# -----------------------------------------------------------------------------#\n# Metric helpers from baseline\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)]\n    return sum(corr) / max(sum(w), 1)\n\n\n# -----------------------------------------------------------------------------#\n# Load SPR_BENCH or synthetic fallback\ndef load_real_spr(path: pathlib.Path):\n    try:\n        from datasets import load_dataset\n\n        def _load(csv):\n            return load_dataset(\n                \"csv\",\n                data_files=str(path / csv),\n                split=\"train\",\n                cache_dir=\".cache_dsets\",\n            )\n\n        return True, {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    except Exception as e:\n        print(\"Falling back to synthetic data \u2013\", e)\n        return False, {}\n\n\ndef make_synth(n):\n    shapes, colors = list(\"ABCDE\"), list(\"12345\")\n    seqs, labels = [], []\n    for _ in range(n):\n        L = random.randint(3, 10)\n        seq = \" \".join(random.choice(shapes) + random.choice(colors) for _ in range(L))\n        seqs.append(seq)\n        labels.append(int(count_shape_variety(seq) >= count_color_variety(seq)))\n    return {\"sequence\": seqs, \"label\": labels}\n\n\nSPR_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nhave_real, raw = load_real_spr(SPR_PATH)\nif have_real:\n    train_raw, dev_raw, test_raw = raw[\"train\"], raw[\"dev\"], raw[\"test\"]\n    train_data = {\"sequence\": train_raw[\"sequence\"], \"label\": train_raw[\"label\"]}\n    dev_data = {\"sequence\": dev_raw[\"sequence\"], \"label\": dev_raw[\"label\"]}\n    test_data = {\"sequence\": test_raw[\"sequence\"], \"label\": test_raw[\"label\"]}\nelse:\n    train_data = make_synth(6000)\n    dev_data = make_synth(1200)\n    test_data = make_synth(1200)\n\n# -----------------------------------------------------------------------------#\n# Vocabulary & encoding\nall_tokens = {tok for seq in train_data[\"sequence\"] for tok in seq.split()}\nvocab = {tok: i + 4 for i, tok in enumerate(sorted(all_tokens))}\nvocab.update({\"<pad>\": 0, \"<unk>\": 1, \"<mask>\": 2, \"<cls>\": 3})\ninv_vocab = {i: t for t, i in vocab.items()}\npad_id, unk_id, mask_id, cls_id = (\n    vocab[t] for t in [\"<pad>\", \"<unk>\", \"<mask>\", \"<cls>\"]\n)\nmax_len = max(len(s.split()) for s in train_data[\"sequence\"]) + 1  # + <cls>\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab.get(t, unk_id) for t in seq.split()]\n    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n    return torch.tensor(ids, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------#\n# Datasets\nclass SeqOnlyDS(Dataset):\n    def __init__(self, seqs):\n        self.seqs = seqs\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i]\n\n\nclass LabeledDS(Dataset):\n    def __init__(self, seqs, labels):\n        self.seqs, self.labels = seqs, labels\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, i):\n        return self.seqs[i], self.labels[i]\n\n\n# -----------------------------------------------------------------------------#\n# Data augmentation\ndef augment(seq: str):\n    toks = seq.split()\n    if random.random() < 0.5:  # masking\n        toks = [t if random.random() > 0.3 else \"<mask>\" for t in toks]\n    else:  # local shuffle\n        window = max(1, len(toks) // 4)\n        i = random.randint(0, len(toks) - window)\n        seg = toks[i : i + window]\n        random.shuffle(seg)\n        toks[i : i + window] = seg\n    return \" \".join(toks)\n\n\n# -----------------------------------------------------------------------------#\n# Model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb=128, hid=256, n_layers=2):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n        self.gru = nn.GRU(emb, hid, num_layers=n_layers, batch_first=True)\n\n    def forward(self, x):\n        z, h = self.gru(self.emb(x))\n        return h[-1]  # (B, hid)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, hid, n_cls):\n        super().__init__()\n        self.enc = encoder\n        self.fc = nn.Linear(hid, n_cls)\n\n    def forward(self, x):\n        return self.fc(self.enc(x))\n\n\n# -----------------------------------------------------------------------------#\n# Corrected contrastive loss\ndef contrastive_loss(z, temp=0.1):\n    z = nn.functional.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T) / temp  # (2N,2N)\n    N = sim.size(0)\n    sim.masked_fill_(torch.eye(N, device=z.device).bool(), -9e15)\n    pos_idx = torch.arange(N, device=z.device) ^ 1  # 0\u21941, 2\u21943, \u2026\n    loss = nn.functional.cross_entropy(sim, pos_idx)\n    return loss\n\n\n# -----------------------------------------------------------------------------#\n# DataLoaders\npretrain_loader = DataLoader(\n    SeqOnlyDS(train_data[\"sequence\"]), batch_size=256, shuffle=True, drop_last=True\n)\ntrain_loader = DataLoader(\n    LabeledDS(train_data[\"sequence\"], train_data[\"label\"]), batch_size=128, shuffle=True\n)\ndev_loader = DataLoader(\n    LabeledDS(dev_data[\"sequence\"], dev_data[\"label\"]), batch_size=256\n)\ntest_loader = DataLoader(\n    LabeledDS(test_data[\"sequence\"], test_data[\"label\"]), batch_size=256\n)\n\n# -----------------------------------------------------------------------------#\n# Experiment bookkeeping\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"val_acc\": [], \"val_aca\": []},\n        \"test\": {},\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# Pre-training\nenc = Encoder(len(vocab)).to(device)\nopt_enc = torch.optim.Adam(enc.parameters(), lr=1e-3)\npre_epochs = 8\nprint(\"\\n--- Contrastive Pre-training ---\")\nfor ep in range(1, pre_epochs + 1):\n    enc.train()\n    tot = cnt = 0\n    t0 = time.time()\n    for batch_seqs in pretrain_loader:\n        views = []\n        for s in batch_seqs:\n            views.append(encode(augment(s)))\n            views.append(encode(augment(s)))\n        x = torch.stack(views).to(device)\n        opt_enc.zero_grad()\n        loss = contrastive_loss(enc(x))\n        loss.backward()\n        opt_enc.step()\n        tot += loss.item()\n        cnt += 1\n    avg = tot / cnt\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(avg)\n    print(f\"Epoch {ep}: contrastive_loss = {avg:.4f}  ({time.time()-t0:.1f}s)\")\n\n# -----------------------------------------------------------------------------#\n# Fine-tuning\nnum_cls = len(set(train_data[\"label\"]))\nmodel = Classifier(enc, hid=256, n_cls=num_cls).to(device)\ncrit = nn.CrossEntropyLoss()\nopt_cls = torch.optim.Adam(model.parameters(), lr=1e-3)\nf_epochs = 10\n\n\ndef eval_model(loader):\n    model.eval()\n    loss_tot = n_tot = 0\n    preds = []\n    gts = []\n    raws = []\n    with torch.no_grad():\n        for seqs, labels in loader:\n            x = torch.stack([encode(s) for s in seqs]).to(device)\n            y = torch.tensor(labels, device=device)\n            logits = model(x)\n            loss = crit(logits, y)\n            loss_tot += loss.item() * y.size(0)\n            n_tot += y.size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(labels)\n            raws.extend(seqs)\n    acc = np.mean(np.array(preds) == np.array(gts))\n    return loss_tot / n_tot, acc, preds, gts, raws\n\n\ndef aca_metric(seqs, labels, M=3):\n    correct = total = 0\n    for s, l in zip(seqs, labels):\n        variants = [s] + [augment(s) for _ in range(M)]\n        xs = torch.stack([encode(v) for v in variants]).to(device)\n        with torch.no_grad():\n            p = model(xs).argmax(1).cpu().tolist()\n        correct += sum(int(pi == l) for pi in p)\n        total += len(variants)\n    return correct / total\n\n\nprint(\"\\n--- Fine-tuning ---\")\nfor ep in range(1, f_epochs + 1):\n    model.train()\n    tot = n = 0\n    for seqs, labels in train_loader:\n        x = torch.stack([encode(s) for s in seqs]).to(device)\n        y = torch.tensor(labels, device=device)\n        opt_cls.zero_grad()\n        loss = crit(model(x), y)\n        loss.backward()\n        opt_cls.step()\n        tot += loss.item() * y.size(0)\n        n += y.size(0)\n    train_loss = tot / n\n    val_loss, val_acc, _, _, _ = eval_model(dev_loader)\n    val_aca = aca_metric(dev_data[\"sequence\"], dev_data[\"label\"])\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_aca\"].append(val_aca)\n    print(\n        f\"Epoch {ep}: val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | ACA={val_aca:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# Test evaluation\ntest_loss, test_acc, test_pred, test_gt, test_raw = eval_model(test_loader)\ntest_swa = shape_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_cwa = color_weighted_accuracy(test_raw, test_gt, test_pred)\ntest_aca = aca_metric(test_data[\"sequence\"], test_data[\"label\"])\nexperiment_data[\"SPR_BENCH\"][\"test\"] = {\n    \"loss\": test_loss,\n    \"acc\": test_acc,\n    \"swa\": test_swa,\n    \"cwa\": test_cwa,\n    \"aca\": test_aca,\n    \"predictions\": test_pred,\n    \"gt\": test_gt,\n}\nprint(\"\\n--- Test results ---\")\nprint(\n    f\"Loss={test_loss:.4f} | Acc={test_acc:.4f} | SWA={test_swa:.4f} | \"\n    f\"CWA={test_cwa:.4f} | ACA={test_aca:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\n=== Training with num_layers = 1 ===', '\\n', '\nEpoch  1: tr_loss=0.2701 | val_loss=0.1413 | val_SCWA=0.9582', '\\n', '  Epoch\n2: tr_loss=0.1202 | val_loss=0.0993 | val_SCWA=0.9645', '\\n', '  Epoch  3:\ntr_loss=0.0643 | val_loss=0.0368 | val_SCWA=0.9889', '\\n', '  Epoch  4:\ntr_loss=0.0248 | val_loss=0.0153 | val_SCWA=0.9955', '\\n', '  Epoch  5:\ntr_loss=0.0099 | val_loss=0.0074 | val_SCWA=0.9984', '\\n', '\\n=== Training with\nnum_layers = 2 ===', '\\n', '  Epoch  1: tr_loss=0.2266 | val_loss=0.1317 |\nval_SCWA=0.9624', '\\n', '  Epoch  2: tr_loss=0.0983 | val_loss=0.0558 |\nval_SCWA=0.9841', '\\n', '  Epoch  3: tr_loss=0.0325 | val_loss=0.0294 |\nval_SCWA=0.9940', '\\n', '  Epoch  4: tr_loss=0.0138 | val_loss=0.0106 |\nval_SCWA=0.9969', '\\n', '  Epoch  5: tr_loss=0.0050 | val_loss=0.0043 |\nval_SCWA=0.9996', '\\n', '\\n=== Training with num_layers = 3 ===', '\\n', '  Epoch\n1: tr_loss=0.2209 | val_loss=0.1574 | val_SCWA=0.9450', '\\n', '  Epoch  2:\ntr_loss=0.1029 | val_loss=0.0522 | val_SCWA=0.9846', '\\n', '  Epoch  3:\ntr_loss=0.0282 | val_loss=0.0175 | val_SCWA=0.9952', '\\n', '  Epoch  4:\ntr_loss=0.0301 | val_loss=0.0158 | val_SCWA=0.9956', '\\n', '  Epoch  5:\ntr_loss=0.0090 | val_loss=0.0042 | val_SCWA=0.9986', '\\n', '\\nBest num_layers\naccording to validation: 2', '\\n', 'Test SCWA (best model) = 0.6354', '\\n',\n'Execution time: 9 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 431725.96\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 442399.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 446235.78\nexamples/s]', '\\n', 'Contrastive epoch 1: loss=1.9506', '\\n', 'Contrastive epoch\n2: loss=1.0940', '\\n', 'Contrastive epoch 3: loss=0.9157', '\\n', 'Epoch 1:\nvalidation_loss = 0.1554 | val_SWA=0.9593 | val_ACA=0.9083', '\\n', 'Epoch 2:\nvalidation_loss = 0.1143 | val_SWA=0.9659 | val_ACA=0.9150', '\\n', 'Epoch 3:\nvalidation_loss = 0.0780 | val_SWA=0.9747 | val_ACA=0.9238', '\\n', 'Epoch 4:\nvalidation_loss = 0.0524 | val_SWA=0.9850 | val_ACA=0.9093', '\\n', 'Epoch 5:\nvalidation_loss = 0.0310 | val_SWA=0.9884 | val_ACA=0.9110', '\\n', 'Test:\nloss=2.2405 | SWA=0.6954 | ACA=0.6653', '\\n', 'Execution time: 14 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 382245.55\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 647768.96\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 827850.39\nexamples/s]', '\\n', 'Loaded real SPR_BENCH.', '\\n', 'Contrastive Epoch 1:\nloss=2.2867', '\\n', 'Contrastive Epoch 2: loss=1.7610', '\\n', 'Contrastive Epoch\n3: loss=1.6774', '\\n', 'Contrastive Epoch 4: loss=1.6571', '\\n', 'Contrastive\nEpoch 5: loss=1.6536', '\\n', 'Epoch 1: validation_loss = 0.0545 | val_acc=0.9830\n| val_aca=0.7676', '\\n', 'Epoch 2: validation_loss = 0.0220 | val_acc=0.9946 |\nval_aca=0.7479', '\\n', 'Epoch 3: validation_loss = 0.0036 | val_acc=0.9990 |\nval_aca=0.7597', '\\n', 'Epoch 4: validation_loss = 0.0011 | val_acc=0.9998 |\nval_aca=0.7511', '\\n', 'Epoch 5: validation_loss = 0.0009 | val_acc=0.9998 |\nval_aca=0.7466', '\\n', 'Test: loss=4.0604 | acc=0.6997 | ACA=0.5955', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-15_23-37-\n11_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 517684.29\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 615921.76\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 450588.60\nexamples/s]', '\\n', 'Failed loading real SPR, using synthetic.', ' ',\n'dictionary update sequence element #0 has length 3; 2 is required', '\\n',\n'\\nPre-training encoder (contrastive)...', '\\n', '  Contrastive epoch 1:\nloss=5.8356', '\\n', '  Contrastive epoch 2: loss=5.3253', '\\n', '  Contrastive\nepoch 3: loss=5.2806', '\\n', 'Epoch 1: validation_loss = 0.6417 | SWA=0.7352 |\nACA=0.6775', '\\n', 'Epoch 2: validation_loss = 0.6305 | SWA=0.7352 |\nACA=0.6775', '\\n', 'Epoch 3: validation_loss = 0.6290 | SWA=0.7352 |\nACA=0.6792', '\\n', 'Epoch 4: validation_loss = 0.6202 | SWA=0.7285 |\nACA=0.6667', '\\n', 'Epoch 5: validation_loss = 0.6015 | SWA=0.7376 |\nACA=0.6817', '\\n', '\\nTest results: SWA=0.7598 | CWA=0.6714 | ACA=0.7100', '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 483878.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 677944.01\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 766208.87\nexamples/s]', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch 1:\ncontrastive_loss = 6.2393 in 0.8s', '\\n', 'Epoch 2: contrastive_loss = 6.2364 in\n0.5s', '\\n', 'Epoch 3: contrastive_loss = 6.2364 in 0.5s', '\\n', '\\n--- Fine-\ntuning ---', '\\n', 'runfile.py:301: UserWarning: To copy construct from a\ntensor, it is recommended to use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels).to(device)\\n',\n'runfile.py:269: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels).to(device)\\n', 'Epoch 1:\nval_loss=0.1352 | val_acc=0.9572 | ACA=0.8969', '\\n', 'Epoch 2: val_loss=0.0672\n| val_acc=0.9812 | ACA=0.8888', '\\n', 'Epoch 3: val_loss=0.0209 | val_acc=0.9936\n| ACA=0.8791', '\\n', 'Epoch 4: val_loss=0.0129 | val_acc=0.9974 | ACA=0.8808',\n'\\n', 'Epoch 5: val_loss=0.0036 | val_acc=0.9994 | ACA=0.8802', '\\n', 'Epoch 6:\nval_loss=0.0018 | val_acc=0.9996 | ACA=0.8778', '\\n', '\\n--- Test results ---',\n'\\n', 'Loss=3.2956 | Acc=0.7000 | SWA=0.7000 | CWA=0.6354 | ACA=0.6524', '\\n',\n'Execution time: 22 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch\n1: contrastive_loss = 1.3732  (0.8s)', '\\n', 'Epoch 2: contrastive_loss = 0.9218\n(0.5s)', '\\n', 'Epoch 3: contrastive_loss = 0.7865  (0.4s)', '\\n', 'Epoch 4:\ncontrastive_loss = 0.7359  (0.4s)', '\\n', 'Epoch 5: contrastive_loss = 0.7111\n(0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.6922  (0.4s)', '\\n', 'Epoch 7:\ncontrastive_loss = 0.6796  (0.4s)', '\\n', 'Epoch 8: contrastive_loss = 0.6809\n(0.4s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:284: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:254: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0644 | val_acc=0.9812 | ACA=0.9003', '\\n', 'Epoch 2:\nval_loss=0.0065 | val_acc=0.9984 | ACA=0.8890', '\\n', 'Epoch 3: val_loss=0.0017\n| val_acc=1.0000 | ACA=0.8811', '\\n', 'Epoch 4: val_loss=0.0006 | val_acc=0.9998\n| ACA=0.8895', '\\n', 'Epoch 5: val_loss=0.0006 | val_acc=0.9998 | ACA=0.8834',\n'\\n', 'Epoch 6: val_loss=0.0005 | val_acc=0.9998 | ACA=0.8848', '\\n', 'Epoch 7:\nval_loss=0.0004 | val_acc=0.9998 | ACA=0.8818', '\\n', 'Epoch 8: val_loss=0.0004\n| val_acc=0.9998 | ACA=0.8815', '\\n', 'Epoch 9: val_loss=0.0003 | val_acc=1.0000\n| ACA=0.8840', '\\n', 'Epoch 10: val_loss=0.0003 | val_acc=1.0000 | ACA=0.8830',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=4.1363 | Acc=0.7001 | SWA=0.7001 |\nCWA=0.6355 | ACA=0.6522', '\\n', 'Execution time: 57 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded real SPR_BENCH', '\\n', 'Contrastive Epoch\n1: loss=3.7903', '\\n', 'Contrastive Epoch 2: loss=2.5093', '\\n', 'Contrastive\nEpoch 3: loss=2.2262', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.0644\n| acc=0.9796 | ACA=0.7826', '\\n', 'Epoch 2: validation_loss = 0.0343 |\nacc=0.9926 | ACA=0.7755', '\\n', 'Epoch 3: validation_loss = 0.0333 | acc=0.9922\n| ACA=0.7769', '\\n', 'Test: loss=1.8784 | acc=0.6966 | SWA=0.6961 | CWA=0.6331 |\nACA=0.6231', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientis\nt-v2/experiments/2025-08-15_23-37-\n11_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 2 minutes seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded real SPR_BENCH dataset', '\\n', '\\nPre-\ntraining encoder (contrastive)\u2026', '\\n', '  Contrastive epoch 1: loss=5.5553',\n'\\n', '  Contrastive epoch 2: loss=5.4248', '\\n', '  Contrastive epoch 3:\nloss=5.4166', '\\n', 'Epoch 1: validation_loss = 0.1614 | SWA=0.9505 |\nACA=0.8919', '\\n', 'Epoch 2: validation_loss = 0.0999 | SWA=0.9752 |\nACA=0.9009', '\\n', 'Epoch 3: validation_loss = 0.0625 | SWA=0.9861 |\nACA=0.8997', '\\n', 'Epoch 4: validation_loss = 0.0483 | SWA=0.9866 |\nACA=0.9103', '\\n', 'Epoch 5: validation_loss = 0.0382 | SWA=0.9919 |\nACA=0.8906', '\\n', '\\nTest results: SWA=0.6961 | CWA=0.6333 | ACA=0.6577', '\\n',\n'Execution time: 40 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded SPR_BENCH dataset.', '\\n', 'Contrastive\nepoch 1: loss=2.8024', '\\n', 'Contrastive epoch 2: loss=2.2090', '\\n',\n'Contrastive epoch 3: loss=2.1217', '\\n', 'Epoch 1: validation_loss = 0.3031 |\nacc=0.9834 | SWA=0.9838 | CWA=0.9836 | ACA=0.8167', '\\n', 'Epoch 2:\nvalidation_loss = 0.2580 | acc=0.9910 | SWA=0.9908 | CWA=0.9915 | ACA=0.8407',\n'\\n', 'Epoch 3: validation_loss = 0.0436 | acc=0.9966 | SWA=0.9966 | CWA=0.9968\n| ACA=0.8479', '\\n', 'Epoch 4: validation_loss = 0.0178 | acc=0.9986 |\nSWA=0.9984 | CWA=0.9985 | ACA=0.8493', '\\n', 'Epoch 5: validation_loss = 0.0847\n| acc=0.9970 | SWA=0.9969 | CWA=0.9969 | ACA=0.8509', '\\n', 'TEST: loss=19.0245\n| acc=0.6991 | SWA=0.6990 | CWA=0.6347 | ACA=0.6439', '\\n', 'Saved experiment\ndata ->', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_23-37-\n11_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch\n1: contrastive_loss = 1.3732  (0.8s)', '\\n', 'Epoch 2: contrastive_loss = 0.9218\n(0.5s)', '\\n', 'Epoch 3: contrastive_loss = 0.7865  (0.5s)', '\\n', 'Epoch 4:\ncontrastive_loss = 0.7359  (0.4s)', '\\n', 'Epoch 5: contrastive_loss = 0.7111\n(0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.6922  (0.4s)', '\\n', 'Epoch 7:\ncontrastive_loss = 0.6796  (0.4s)', '\\n', 'Epoch 8: contrastive_loss = 0.6809\n(0.4s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:296: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:266: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0644 | val_acc=0.9812 | ACA=0.9003', '\\n', 'Epoch 2:\nval_loss=0.0065 | val_acc=0.9984 | ACA=0.8890', '\\n', 'Epoch 3: val_loss=0.0017\n| val_acc=1.0000 | ACA=0.8811', '\\n', 'Epoch 4: val_loss=0.0006 | val_acc=0.9998\n| ACA=0.8895', '\\n', 'Epoch 5: val_loss=0.0006 | val_acc=0.9998 | ACA=0.8834',\n'\\n', 'Epoch 6: val_loss=0.0005 | val_acc=0.9998 | ACA=0.8848', '\\n', 'Epoch 7:\nval_loss=0.0004 | val_acc=0.9998 | ACA=0.8818', '\\n', 'Epoch 8: val_loss=0.0004\n| val_acc=0.9998 | ACA=0.8815', '\\n', 'Epoch 9: val_loss=0.0003 | val_acc=1.0000\n| ACA=0.8840', '\\n', 'Epoch 10: val_loss=0.0003 | val_acc=1.0000 | ACA=0.8830',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=4.1363 | Acc=0.7001 | SWA=0.7001 |\nCWA=0.6355 | ACA=0.6522', '\\n', 'Execution time: 45 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch\n1: contrastive_loss = 1.3732  (0.8s)', '\\n', 'Epoch 2: contrastive_loss = 0.9218\n(0.5s)', '\\n', 'Epoch 3: contrastive_loss = 0.7865  (0.5s)', '\\n', 'Epoch 4:\ncontrastive_loss = 0.7359  (0.4s)', '\\n', 'Epoch 5: contrastive_loss = 0.7111\n(0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.6922  (0.4s)', '\\n', 'Epoch 7:\ncontrastive_loss = 0.6796  (0.4s)', '\\n', 'Epoch 8: contrastive_loss = 0.6809\n(0.4s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:296: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:266: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0644 | val_acc=0.9812 | ACA=0.9003', '\\n', 'Epoch 2:\nval_loss=0.0065 | val_acc=0.9984 | ACA=0.8890', '\\n', 'Epoch 3: val_loss=0.0017\n| val_acc=1.0000 | ACA=0.8811', '\\n', 'Epoch 4: val_loss=0.0006 | val_acc=0.9998\n| ACA=0.8895', '\\n', 'Epoch 5: val_loss=0.0006 | val_acc=0.9998 | ACA=0.8834',\n'\\n', 'Epoch 6: val_loss=0.0005 | val_acc=0.9998 | ACA=0.8848', '\\n', 'Epoch 7:\nval_loss=0.0004 | val_acc=0.9998 | ACA=0.8818', '\\n', 'Epoch 8: val_loss=0.0004\n| val_acc=0.9998 | ACA=0.8815', '\\n', 'Epoch 9: val_loss=0.0003 | val_acc=1.0000\n| ACA=0.8840', '\\n', 'Epoch 10: val_loss=0.0003 | val_acc=1.0000 | ACA=0.8830',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=4.1363 | Acc=0.7001 | SWA=0.7001 |\nCWA=0.6355 | ACA=0.6522', '\\n', 'Execution time: 40 seconds seconds (time limit\nis 30 minutes).']", "['Using device: cuda', '\\n', '\\n--- Contrastive Pre-training ---', '\\n', 'Epoch\n1: contrastive_loss = 1.3732  (0.8s)', '\\n', 'Epoch 2: contrastive_loss = 0.9218\n(0.5s)', '\\n', 'Epoch 3: contrastive_loss = 0.7865  (0.5s)', '\\n', 'Epoch 4:\ncontrastive_loss = 0.7359  (0.4s)', '\\n', 'Epoch 5: contrastive_loss = 0.7111\n(0.4s)', '\\n', 'Epoch 6: contrastive_loss = 0.6922  (0.4s)', '\\n', 'Epoch 7:\ncontrastive_loss = 0.6796  (0.4s)', '\\n', 'Epoch 8: contrastive_loss = 0.6809\n(0.4s)', '\\n', '\\n--- Fine-tuning ---', '\\n', 'runfile.py:296: UserWarning: To\ncopy construct from a tensor, it is recommended to use\nsourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'runfile.py:266: UserWarning: To copy construct from a tensor, it is recommended\nto use sourceTensor.clone().detach() or\nsourceTensor.clone().detach().requires_grad_(True), rather than\ntorch.tensor(sourceTensor).\\n  y = torch.tensor(labels, device=device)\\n',\n'Epoch 1: val_loss=0.0644 | val_acc=0.9812 | ACA=0.9003', '\\n', 'Epoch 2:\nval_loss=0.0065 | val_acc=0.9984 | ACA=0.8890', '\\n', 'Epoch 3: val_loss=0.0017\n| val_acc=1.0000 | ACA=0.8811', '\\n', 'Epoch 4: val_loss=0.0006 | val_acc=0.9998\n| ACA=0.8895', '\\n', 'Epoch 5: val_loss=0.0006 | val_acc=0.9998 | ACA=0.8834',\n'\\n', 'Epoch 6: val_loss=0.0005 | val_acc=0.9998 | ACA=0.8848', '\\n', 'Epoch 7:\nval_loss=0.0004 | val_acc=0.9998 | ACA=0.8818', '\\n', 'Epoch 8: val_loss=0.0004\n| val_acc=0.9998 | ACA=0.8815', '\\n', 'Epoch 9: val_loss=0.0003 | val_acc=1.0000\n| ACA=0.8840', '\\n', 'Epoch 10: val_loss=0.0003 | val_acc=1.0000 | ACA=0.8830',\n'\\n', '\\n--- Test results ---', '\\n', 'Loss=4.1363 | Acc=0.7001 | SWA=0.7001 |\nCWA=0.6355 | ACA=0.6522', '\\n', 'Execution time: 43 seconds seconds (time limit\nis 30 minutes).']", ""], "analysis": ["", "", "", "The script fails to load the real SPR_BENCH dataset due to an exception caused\nby an incorrect dictionary update operation. Specifically, the error 'dictionary\nupdate sequence element #0 has length 3; 2 is required' indicates that the\ndataset loading function is attempting to update a dictionary with data that\ndoes not conform to the expected structure. This issue forces the script to fall\nback to using synthetic data instead of the intended real dataset.  **Proposed\nFix:** 1. Investigate the `load_real_spr` function, particularly the part where\nthe dataset dictionary is updated. Ensure that the structure of the data being\nloaded matches the expected format for dictionary updates. 2. Add error handling\nor validation to confirm the structure of the dataset before attempting the\nupdate. 3. Test the dataset loading process with the real SPR_BENCH data to\nensure it works correctly without falling back to synthetic data.", "The contrastive pre-training phase did not show meaningful progress in reducing\nthe contrastive loss after the first epoch. The loss remained almost constant\n(6.2393, 6.2364, 6.2364) across all epochs, indicating that the model may not be\nlearning effectively during this phase. This could be due to issues such as\ninappropriate hyperparameters, ineffective data augmentation, or an unsuitable\nmodel architecture for the task.  Additionally, the test results for the SWA and\nCWA metrics (0.7000 and 0.6354, respectively) did not surpass the SOTA (65.0%\nSWA and 70.0% CWA). While the SWA is slightly better, the CWA underperformed\ncompared to the benchmark.  Proposed Fix: 1. Investigate the contrastive loss\nfunction and its implementation to ensure correctness. 2. Experiment with\ndifferent learning rates, batch sizes, and temperature values for the\ncontrastive loss. 3. Evaluate the data augmentation strategy to ensure it\ngenerates diverse and meaningful positive and negative pairs. 4. Consider using\na more complex architecture or adding regularization techniques to improve the\nmodel's learning capability. 5. Reassess the fine-tuning process to ensure the\npre-trained embeddings are being utilized effectively.", "The execution of the training script completed successfully without any errors\nor bugs. The model underwent contrastive pre-training and fine-tuning, achieving\na test accuracy of 70.01%, SWA of 70.01%, and CWA of 63.55%. While this\nperformance does not surpass the SOTA benchmarks of 65.0% SWA and 70.0% CWA, the\nimplementation is functioning as intended. Further optimization and\nexperimentation are needed to improve the model's performance.", "", "", "The execution output shows that the model achieves high performance on the\nvalidation set but performs poorly on the test set. This suggests a severe\noverfitting issue. The training process may have over-optimized on the training\ndata and validation set, failing to generalize to the unseen test data.   To\naddress this: 1. Introduce regularization techniques, such as dropout or weight\ndecay, to prevent overfitting. 2. Use early stopping based on validation\nperformance to avoid over-training. 3. Increase the diversity or size of the\ntraining data, if possible. 4. Adjust the contrastive loss temperature or fine-\ntune the learning rate to avoid overfitting during contrastive pretraining. 5.\nRevisit the data augmentation strategies to ensure they are sufficiently diverse\nand representative of the test data.", "The execution of the training script was successful and produced results without\nany errors or bugs. The model was pre-trained using contrastive learning and\nfine-tuned on the SPR_BENCH dataset. The final test results show that the model\nachieved a Shape-Weighted Accuracy (SWA) of 70.01% and a Color-Weighted Accuracy\n(CWA) of 63.55%. While the SWA performance meets the SOTA benchmark of 65.0%,\nthe CWA falls short of the SOTA benchmark of 70.0%. The execution demonstrates\nthe effectiveness of the proposed approach but highlights areas for improvement,\nparticularly in enhancing the CWA metric. Further adjustments to the model, data\naugmentation, or training strategies might help improve these results.", "The execution of the training script was successful, and the output log does not\nshow any bugs. The contrastive pre-training and fine-tuning steps completed as\nexpected. However, the test results indicate that while the model achieves high\nvalidation accuracy, the test accuracy (70.01%) and weighted accuracies (SWA:\n70.01%, CWA: 63.55%) fall short of the current SOTA benchmark (SWA: 65.0%, CWA:\n70.0%). This suggests that while the model performs well on the validation set,\nits generalization to the test set could be improved. Future iterations could\nfocus on improving generalization, possibly by refining the contrastive learning\nframework or exploring additional data augmentation techniques.", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.005, "best_value": 0.005}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0043, "best_value": 0.0043}]}, {"metric_name": "validation SCWA", "lower_is_better": false, "description": "Measures the validation SCWA performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "test SCWA", "lower_is_better": false, "description": "Measures the test SCWA performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6354, "best_value": 0.6354}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss value during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.915672, "best_value": 0.915672}]}, {"metric_name": "fine-tuning training loss", "lower_is_better": true, "description": "Loss value during the fine-tuning training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.033298, "best_value": 0.033298}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.031041, "best_value": 0.031041}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.988432, "best_value": 0.988432}]}, {"metric_name": "validation adversarial consistency accuracy", "lower_is_better": false, "description": "Adversarial consistency accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.91095, "best_value": 0.91095}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0002, "best_value": 0.0002}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0009, "best_value": 0.0009}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9998, "best_value": 0.9998}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "The Average Class Accuracy (ACA) during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7676, "best_value": 0.7676}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6997, "best_value": 0.6997}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "The Average Class Accuracy (ACA) during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5955, "best_value": 0.5955}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5631, "best_value": 0.5631}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6015, "best_value": 0.6015}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7376, "best_value": 0.7376}]}, {"metric_name": "validation augmented consistency accuracy", "lower_is_better": false, "description": "The augmented consistency accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6817, "best_value": 0.6817}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7598, "best_value": 0.7598}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6714, "best_value": 0.6714}]}, {"metric_name": "test augmented consistency accuracy", "lower_is_better": false, "description": "The augmented consistency accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.71, "best_value": 0.71}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 6.2364, "best_value": 6.2364}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0011, "best_value": 0.0011}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0018, "best_value": 0.0018}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9996, "best_value": 0.9996}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "ACA (average class accuracy) during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8969, "best_value": 0.8969}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 3.2956, "best_value": 3.2956}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6354, "best_value": 0.6354}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "ACA (average class accuracy) during test.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6524, "best_value": 0.6524}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "The loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6796, "best_value": 0.6796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0003, "best_value": 0.0003}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "The average class accuracy during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9003, "best_value": 0.9003}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.1363, "best_value": 4.1363}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6355, "best_value": 0.6355}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "The average class accuracy during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6522, "best_value": 0.6522}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0377, "best_value": 0.0377}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0333, "best_value": 0.0333}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9926, "best_value": 0.9926}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The shape-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9919, "best_value": 0.9919}]}, {"metric_name": "validation color-weighted accuracy", "lower_is_better": false, "description": "The color-weighted accuracy computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.993, "best_value": 0.993}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "The average class accuracy (ACA) computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7826, "best_value": 0.7826}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy computed on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6966, "best_value": 0.6966}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "The average class accuracy (ACA) computed on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6231, "best_value": 0.6231}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.042795, "best_value": 0.042795}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, used to evaluate the model's performance on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.038202, "best_value": 0.038202}]}, {"metric_name": "validation shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation set, weighted by shape-specific metrics.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.991861, "best_value": 0.991861}]}, {"metric_name": "validation augmentation-consistency accuracy", "lower_is_better": false, "description": "The accuracy of the model on validation data with augmentation consistency.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.910333, "best_value": 0.910333}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test set, weighted by shape-specific metrics.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696131, "best_value": 0.696131}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test set, weighted by color-specific metrics.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.633276, "best_value": 0.633276}]}, {"metric_name": "test augmentation-consistency accuracy", "lower_is_better": false, "description": "The accuracy of the model on test data with augmentation consistency.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6577, "best_value": 0.6577}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss of the model", "data": [{"dataset_name": "SPR_BENCH", "final_value": 395000.6164, "best_value": 395000.6164}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss of the model", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.084717, "best_value": 0.084717}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Best validation accuracy of the model", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9986, "best_value": 0.9986}]}, {"metric_name": "validation SWA", "lower_is_better": false, "description": "Final validation Stochastic Weight Averaging (SWA)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996919, "best_value": 0.996919}]}, {"metric_name": "validation CWA", "lower_is_better": false, "description": "Final validation Class Weight Averaging (CWA)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996889, "best_value": 0.996889}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Final validation Average Class Accuracy (ACA)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.850933, "best_value": 0.850933}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy of the model", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6991, "best_value": 0.6991}]}, {"metric_name": "test SWA", "lower_is_better": false, "description": "Test Stochastic Weight Averaging (SWA)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698971, "best_value": 0.698971}]}, {"metric_name": "test CWA", "lower_is_better": false, "description": "Test Class Weight Averaging (CWA)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.634673, "best_value": 0.634673}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Test Average Class Accuracy (ACA)", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.643867, "best_value": 0.643867}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6796, "best_value": 0.6796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0003, "best_value": 0.0003}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Average Class Accuracy during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9003, "best_value": 0.9003}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.1363, "best_value": 4.1363}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6355, "best_value": 0.6355}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Average Class Accuracy during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6522, "best_value": 0.6522}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6796, "best_value": 0.6796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0003, "best_value": 0.0003}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Average Class Accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9003, "best_value": 0.9003}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.1363, "best_value": 4.1363}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Shape-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Color-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6355, "best_value": 0.6355}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Average Class Accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6522, "best_value": 0.6522}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Measures the loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6796, "best_value": 0.6796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0003, "best_value": 0.0003}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation ACA", "lower_is_better": false, "description": "Measures the average class accuracy on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9003, "best_value": 0.9003}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Measures the loss on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.1363, "best_value": 4.1363}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test shape-weighted accuracy", "lower_is_better": false, "description": "Measures the shape-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7001, "best_value": 0.7001}]}, {"metric_name": "test color-weighted accuracy", "lower_is_better": false, "description": "Measures the color-weighted accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6355, "best_value": 0.6355}]}, {"metric_name": "test ACA", "lower_is_better": false, "description": "Measures the average class accuracy on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6522, "best_value": 0.6522}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_val_scwa_curves.png", "../../logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_best_scwa_summary.png"], ["../../logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_finetune_train_loss.png", "../../logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_val_swa_aca.png", "../../logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_val_loss.png", "../../logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_val_ACA.png", "../../logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_confusion_matrix.png"], [], [], ["../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_test_summary.png", "../../logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_val_swa_curve.png", "../../logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_val_aca_curve.png", "../../logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_test_metric_summary.png", "../../logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_pretrain_loss.png", "../../logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_val_metrics.png", "../../logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_test_metrics.png"], ["../../logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_pretrain_loss.png", "../../logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_train_val_loss.png", "../../logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_val_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_test_metrics.png"]], "plot_paths": [["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_val_scwa_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_best_scwa_summary.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_finetune_train_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_val_swa_aca.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_val_accuracy.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_val_ACA.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_confusion_matrix.png"], [], [], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_test_summary.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_val_swa_curve.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_val_aca_curve.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_test_metric_summary.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_test_metrics.png"], ["experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_pretrain_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_train_val_loss.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_val_metrics.png", "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_01c04d4d4be94216a1cc9fd762947659/SPR_BENCH_agg_test_metrics.png"]], "plot_analyses": [[{"analysis": "The plot shows the training and validation losses for three different configurations (L1, L2, L3) across five epochs. All configurations demonstrate a consistent decrease in both training and validation losses over the epochs, indicating effective learning. L2 and L3 appear to converge faster and achieve lower losses compared to L1, suggesting that these configurations may be better suited for the SPR task. The alignment of training and validation losses for L2 and L3 also indicates minimal overfitting.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the Validation SCWA (Shape-Color Weighted Accuracy) for the three configurations (L1, L2, L3) over five epochs. All configurations show an improvement in SCWA as training progresses. L3 achieves the highest SCWA at the end of the training, closely followed by L2, while L1 lags behind. The rapid increase in SCWA for L3 and its higher final value suggest that this configuration is the most effective in producing robust and contextually aware embeddings for the SPR task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_val_scwa_curves.png"}, {"analysis": "This bar chart compares the best validation SCWA and the corresponding test SCWA for the three configurations (L1, L2, L3). L3 achieves the highest validation SCWA, and its test SCWA is also the highest among the configurations, indicating strong generalization. L2 shows slightly lower validation and test SCWA compared to L3 but still performs significantly better than L1. L1 has the lowest scores, highlighting its relative inefficacy for the SPR task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_931bb17225904a44802375f84e37e198_proc_3017221/SPR_BENCH_best_scwa_summary.png"}], [{"analysis": "The plot shows a steady decrease in contrastive loss over the pre-training epochs, indicating that the model is effectively learning to distinguish between positive and negative pairs. The significant drop from epoch 1 to epoch 2 suggests that the model quickly learns the general structure of the data, while the smaller decrease from epoch 2 to epoch 3 reflects the refinement of embeddings.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_pretrain_loss.png"}, {"analysis": "The cross-entropy loss decreases consistently over the fine-tuning epochs, demonstrating that the model is progressively improving its predictions on the labeled SPR_BENCH dataset. This behavior reflects successful adaptation of the pre-trained embeddings to the specific SPR task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_finetune_train_loss.png"}, {"analysis": "The SWA metric shows a consistent increase over the epochs, indicating improved performance in capturing shape-weighted accuracy. However, the ACA metric fluctuates slightly, suggesting that the model's ability to capture color-weighted accuracy is less stable. Despite the fluctuations, the overall trend for both metrics is positive, with SWA nearing 0.99, which is a promising result.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_val_swa_aca.png"}, {"analysis": "The validation loss decreases steadily, which is indicative of improved generalization performance on unseen data. This trend aligns with the improvements observed in the evaluation metrics (SWA and ACA), further supporting the effectiveness of the proposed context-aware contrastive learning framework.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_val_loss.png"}, {"analysis": "The confusion matrix reveals a balanced performance across the two classes, with similar numbers of correctly classified samples for both. However, there is a noticeable number of misclassifications (1564 false positives and 1480 false negatives), which suggests room for further optimization, especially in reducing false positives.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_ab19886ea15e4dcd8ef4a0e7a597550d_proc_3027374/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss decreasing over epochs, with both converging to near-zero values. This indicates that the model is learning effectively and achieving a good fit on the data. The absence of significant divergence between the training and validation loss suggests minimal overfitting, which is a positive outcome for the model's generalization ability.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation accuracy increases steadily and reaches a plateau close to 100% by the fifth epoch. This demonstrates that the model is achieving excellent performance on the validation set, indicating strong learning capabilities and effective feature extraction for the SPR task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_val_accuracy.png"}, {"analysis": "The validation ACA (Average Class Accuracy) shows fluctuations across epochs, with a noticeable decline towards the later epochs. This suggests potential issues with class imbalance or overfitting to certain classes, which might require further investigation or adjustments in the training process, such as class rebalancing techniques.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_val_ACA.png"}, {"analysis": "The confusion matrix indicates that the model achieves balanced performance on both classes, though there are still significant misclassifications (1,495 false negatives and 1,508 false positives). This suggests that while the model is performing well overall, there is room for improvement in reducing these errors, possibly through better handling of edge cases or refining the contrastive learning framework.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_06b3e427ad934d41930955841ed08e22_proc_3027375/SPR_BENCH_confusion_matrix.png"}], [], [], [{"analysis": "The contrastive pre-training loss decreases steadily over 8 epochs, indicating that the model is effectively learning meaningful representations through contrastive learning. The rapid drop in the initial epochs suggests that the model quickly captures fundamental patterns, while the gradual decline afterward implies fine-tuning of the learned embeddings.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_pretrain_loss.png"}, {"analysis": "Both train and validation losses decrease significantly in the first few epochs and stabilize near zero. This behavior suggests that the model generalizes well to the validation set without overfitting, as there is no divergence between the train and validation loss curves.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_train_val_loss.png"}, {"analysis": "Validation accuracy remains consistently high, approaching 1.0, while the auxiliary classification accuracy (ACA) slightly decreases over epochs. This indicates that while the primary task performance is excellent, the auxiliary task might need further refinement, possibly due to insufficient alignment between the auxiliary task objective and the main task.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_val_metrics.png"}, {"analysis": "The test metric summary shows that the model achieves competitive scores across all metrics, with accuracy and SWA being slightly higher than CWA and ACA. This suggests that the model performs well on the primary SPR task, but there might be room for improvement in capturing color-related features and auxiliary classification tasks.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7f065d85878c4bb997afc614645e7512_proc_3027376/SPR_BENCH_test_metrics.png"}], [{"analysis": "This plot illustrates the training and validation loss over three epochs. The training loss decreases steadily, indicating that the model is learning effectively from the data. The validation loss also decreases, stabilizing after the second epoch. This suggests that the model is not overfitting and has good generalization capability on the validation set.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the validation metrics (Accuracy, SWA, CWA, ACA) over three epochs. Accuracy, SWA, and CWA converge close to 1.0, indicating excellent performance on the validation set. However, the ACA metric remains significantly lower, suggesting that some specific aspect of the task (possibly related to ACA) requires further improvement or attention.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_val_metrics.png"}, {"analysis": "This plot summarizes the test metrics. The accuracy (ACC) is above 0.6, and the ACA is slightly lower but still substantial. It appears that while the model performs well overall, there is room for improvement in achieving higher scores for ACA, which might indicate some bias or difficulty in certain cases within the test set.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_test_summary.png"}, {"analysis": "This confusion matrix shows the distribution of true and predicted labels. The majority of predictions are correct, as evidenced by the prominent diagonal entries. However, there are some misclassifications, as seen in the off-diagonal entries, which should be further analyzed to identify specific patterns or scenarios where the model struggles.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_6d5d942019ae49d69aa1c227bbd915d1_proc_3027377/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss over 5 epochs. Both losses decrease steadily, indicating that the model is learning effectively. The validation loss closely follows the training loss, suggesting minimal overfitting and good generalization performance. The final loss values are very low, demonstrating that the model has been optimized effectively.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the validation Shape-Weighted Accuracy (SWA) over 5 epochs. The SWA improves consistently, reaching above 99%, which indicates that the model is capable of capturing the shape-related patterns in the data effectively. The upward trend suggests that the training strategy is working well for this metric.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_val_swa_curve.png"}, {"analysis": "This plot shows the Validation Augmentation-Consistency Accuracy (ACA) over 5 epochs. While there is an improvement until epoch 4, the performance drops significantly in epoch 5. This could indicate overfitting or instability in the augmentation-consistency learning process. Further investigation into the augmentation strategies might be required.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_val_aca_curve.png"}, {"analysis": "The bar chart summarizes the test metrics for Shape-Weighted Accuracy (SWA), Color-Weighted Accuracy (CWA), and Augmentation-Consistency Accuracy (ACA). The SWA of 70% surpasses the SOTA benchmark, while the CWA of 63% falls short of the 70% SOTA. The ACA of 66% provides additional insights into the model's performance on augmented data. These results suggest that while the model excels in shape recognition, it requires improvements in color-related tasks.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_test_metric_summary.png"}, {"analysis": "The confusion matrix indicates the distribution of true and predicted labels. The model performs reasonably well, with the majority of predictions being correct. However, there is a noticeable number of misclassifications in both classes. This suggests that while the model has learned the task to a good extent, there is room for improvement in reducing errors, especially for the minority class.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_f31195d86003435f9a8db998576c8879_proc_3027374/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "This plot illustrates the contrastive loss during pre-training over 8 epochs. The loss decreases steadily, indicating that the model is effectively learning to distinguish between positive and negative pairs. The diminishing rate of loss reduction after the 5th epoch suggests that the model is approaching convergence. This is a positive sign for the effectiveness of the context-aware contrastive learning framework.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_pretrain_loss.png"}, {"analysis": "This plot compares the training and validation cross-entropy losses over 10 epochs. Both losses decrease rapidly in the initial epochs and stabilize thereafter, with the validation loss closely tracking the training loss. This indicates good generalization and minimal overfitting, suggesting that the model is well-regularized and benefits from the pre-training phase.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_train_val_loss.png"}, {"analysis": "This plot shows validation accuracy and validation ACA (average classification accuracy) over 10 epochs. The validation accuracy quickly reaches near-perfect levels, while the validation ACA fluctuates slightly, indicating some variability in performance across different categories. The high validation accuracy suggests that the model is learning the task well, but the ACA metric hints at potential room for improvement in balancing predictions across all categories.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart summarizes the test metrics: accuracy (acc), shape-weighted accuracy (swa), color-weighted accuracy (cwa), and average classification accuracy (aca). The accuracy and shape-weighted accuracy are close to each other, indicating that the model performs consistently well across sequences of varying shape complexity. However, the lower color-weighted accuracy suggests that the model struggles more with sequences involving diverse color patterns. This highlights an area for further improvement, possibly through enhanced data augmentation or fine-tuning focused on color diversity.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/SPR_BENCH_test_metrics.png"}], [{"analysis": "The plot shows the contrastive loss over the pre-training epochs. The loss decreases steadily, especially in the first few epochs, indicating effective learning during pre-training. The convergence around epoch 7-8 suggests that the model has learned meaningful representations and further training may not yield significant improvements.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_pretrain_loss.png"}, {"analysis": "This plot compares the training and validation cross-entropy loss over epochs. Both losses decrease rapidly in the initial epochs and converge to near-zero values, indicating good generalization and minimal overfitting. The alignment between training and validation loss curves suggests that the model is learning effectively without overfitting.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_train_val_loss.png"}, {"analysis": "The plot tracks validation accuracy and Average Class Accuracy (ACA) over epochs. Validation accuracy quickly reaches near-perfect levels and stabilizes, while ACA shows a slight downward trend and stabilizes at a lower value. This discrepancy might indicate uneven performance across classes, which could be addressed with further class-specific adjustments.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_val_metrics.png"}, {"analysis": "This bar chart summarizes the test metrics for accuracy, SWA, CWA, and ACA. Accuracy and SWA are high, indicating strong overall and shape-weighted performance. However, CWA and ACA are slightly lower, suggesting room for improvement in color-weighted and class-average performance. These results demonstrate progress but highlight areas for refinement to achieve more balanced performance across metrics.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/SPR_BENCH_test_metrics.png"}], [{"analysis": "The plot demonstrates a steady decrease in contrastive loss over the epochs during the pre-training phase. This indicates that the model is effectively learning to distinguish between positive and negative pairs in the context-aware contrastive learning framework. The loss stabilizes around epoch 7, suggesting that further training might not yield significant improvements in the pre-training phase.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_pretrain_loss.png"}, {"analysis": "This plot shows the train and validation loss over the epochs. Both losses decrease rapidly in the initial epochs and converge to near zero, indicating that the model is learning effectively without overfitting. The close alignment of train and validation loss suggests good generalization.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_train_val_loss.png"}, {"analysis": "The validation metrics plot shows validation accuracy approaching 1.0 early in the training process and remaining stable, indicating strong performance. However, the Val ACA metric decreases slightly over time, suggesting that while overall accuracy is high, performance on specific aspects of the task (e.g., color or shape complexity) might require further fine-tuning.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_val_metrics.png"}, {"analysis": "The bar chart summarizes the test metrics, with accuracy and SWA being the highest, followed by CWA and ACA. The scores are relatively close, indicating consistent performance across metrics. However, there is room for improvement, particularly in CWA and ACA, to achieve a more balanced performance across all evaluation criteria.", "plot_path": "experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/SPR_BENCH_test_metrics.png"}], []], "vlm_feedback_summary": ["The plots indicate that L3 is the most effective configuration, achieving the\nlowest losses and highest SCWA scores, followed by L2. L1 performs the worst\nacross all metrics. L3 demonstrates strong generalization and robustness for the\nSPR task.", "The plots collectively demonstrate the effectiveness of the proposed framework.\nThe pre-training contrastive loss reduction, fine-tuning training loss decrease,\nand high SWA metric validate the model's ability to learn robust\nrepresentations. However, the ACA metric fluctuations and the misclassifications\nin the confusion matrix highlight areas for improvement, particularly in\nhandling color-weighted accuracy and reducing false positives.", "The plots reveal that the model is effectively learning and generalizing, as\nevidenced by the decreasing loss and high validation accuracy. However, the\nfluctuating validation ACA and the confusion matrix indicate areas for\nimprovement, such as addressing class imbalance and reducing misclassification\nerrors.", "[]", "[]", "The plots indicate strong performance in terms of contrastive loss reduction,\ngeneralization (train vs. validation loss), and test metrics. However, auxiliary\nclassification accuracy (ACA) and color-weighted accuracy (CWA) lag slightly\nbehind, suggesting areas for further refinement.", "The provided plots indicate a well-performing model with strong generalization\ncapabilities, as evidenced by the decreasing loss and high validation metrics.\nHowever, the ACA metric and some misclassifications in the confusion matrix\nsuggest areas for improvement. Further analysis of the test set and refinement\nof the ACA-related aspects of the model could enhance overall performance.", "The experimental results demonstrate strong performance in shape recognition\n(SWA) with a significant improvement over the SOTA. However, the color-related\nperformance (CWA) lags behind the benchmark, indicating an area for further\nrefinement. The validation loss trends confirm effective learning without\noverfitting, and the ACA results highlight potential issues in augmentation\nconsistency that warrant further investigation. Overall, the results are\npromising but suggest a need for targeted improvements in specific areas.", "[]", "The experimental plots indicate that the context-aware contrastive learning\nframework is effective, as evidenced by the steady reduction in pre-training\nloss and the generalization observed in train vs. validation loss. Validation\nmetrics show high accuracy, but there is room for improvement in balancing\npredictions across categories, particularly in color-weighted accuracy. Overall,\nthe results are promising and suggest that the approach is competitive with the\nSOTA benchmarks.", "The experimental results are promising, with strong performance on accuracy and\nSWA. However, there is a slight drop in CWA and ACA, indicating a need to\naddress class imbalances or variations in color-weighted accuracy. The pre-\ntraining and fine-tuning processes appear effective, with minimal overfitting\nand good convergence.", "The plots indicate that the context-aware contrastive learning framework is\neffective in pre-training and fine-tuning stages. The model demonstrates strong\ngeneralization and high accuracy, but there is potential to improve specific\nmetrics like CWA and ACA to achieve a more balanced performance. Further\nexperimentation could focus on enhancing the representation of color and shape\ncomplexities to address these gaps.", "[]"], "exec_time": [9.559624195098877, 14.636869668960571, 69.17961049079895, 7.852599382400513, 22.22870898246765, 57.40973997116089, 139.56592845916748, 40.408270835876465, 61.05076766014099, 45.644885540008545, 40.70250654220581, 43.12806010246277, null], "exec_time_feedback": ["", "", "", "", "Implementation works but runs too quickly (1.15 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], [], [], ["['SPR_BENCH']"], ["All datasets"], ["['SPR_BENCH']"], [], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    ds_name = \"SPR_BENCH\"\n    per_layer = experiment_data[\"num_layers\"][ds_name][\"per_layer\"]\n    colors = {nl: c for nl, c in zip(sorted(per_layer), [\"r\", \"g\", \"b\", \"m\", \"c\"])}\n\n    # ------------------------------------------------- Plot 1: loss curves\n    try:\n        plt.figure()\n        for nl, rec in per_layer.items():\n            epochs = rec[\"epochs\"]\n            plt.plot(\n                epochs,\n                rec[\"losses\"][\"train\"],\n                linestyle=\"--\",\n                color=colors[nl],\n                label=f\"train L{nl}\",\n            )\n            plt.plot(\n                epochs,\n                rec[\"losses\"][\"val\"],\n                linestyle=\"-\",\n                color=colors[nl],\n                label=f\"val L{nl}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy loss\")\n        plt.title(f\"{ds_name}: Training & Validation Loss vs Epoch\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: val SCWA curves\n    try:\n        plt.figure()\n        for nl, rec in per_layer.items():\n            epochs = rec[\"epochs\"]\n            plt.plot(epochs, rec[\"metrics\"][\"val\"], color=colors[nl], label=f\"L{nl}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation SCWA\")\n        plt.title(f\"{ds_name}: Validation SCWA vs Epoch\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_val_scwa_curves.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SCWA curve plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: best SCWA summary\n    try:\n        best_val_scores = [rec[\"best_val_scwa\"] for rec in per_layer.values()]\n        layers = list(per_layer.keys())\n        best_layer = experiment_data[\"num_layers\"][ds_name][\"best_layer\"]\n        test_scwa = experiment_data[\"num_layers\"][ds_name][\"test_scwa\"]\n        x = np.arange(len(layers))\n        plt.figure()\n        plt.bar(\n            x,\n            best_val_scores,\n            color=[colors[l] for l in layers],\n            alpha=0.7,\n            label=\"Best Val SCWA\",\n        )\n        # overlay test score of best layer\n        plt.bar(\n            layers.index(best_layer),\n            test_scwa,\n            color=\"k\",\n            alpha=0.4,\n            label=\"Test SCWA (best layer)\",\n        )\n        plt.xticks(x, [f\"L{l}\" for l in layers])\n        plt.ylabel(\"SCWA\")\n        plt.title(f\"{ds_name}: Best Val SCWA per Depth (Test SCWA highlighted)\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_best_scwa_summary.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating summary bar plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds = experiment_data[\"SPR_BENCH\"]\n    # handy aliases\n    pre_losses = ds[\"losses\"].get(\"pretrain\", [])\n    ft_losses = ds[\"losses\"].get(\"finetune\", [])\n    val_loss = ds[\"metrics\"].get(\"val_loss\", [])\n    val_swa = ds[\"metrics\"].get(\"val_SWA\", [])\n    val_aca = ds[\"metrics\"].get(\"val_ACA\", [])\n    preds = np.array(ds.get(\"predictions\", []))\n    gts = np.array(ds.get(\"ground_truth\", []))\n\n    # ---------------------------------------------------- Plot 1: pre-training loss\n    try:\n        if len(pre_losses):\n            plt.figure()\n            plt.plot(range(1, len(pre_losses) + 1), pre_losses, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Contrastive Loss\")\n            plt.title(\"SPR_BENCH: Pre-training Contrastive Loss vs Epoch\")\n            fn = os.path.join(working_dir, \"SPR_BENCH_pretrain_loss.png\")\n            plt.savefig(fn)\n            plt.close()\n            print(f\"Saved {fn}\")\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------- Plot 2: fine-tune train loss\n    try:\n        if len(ft_losses):\n            plt.figure()\n            plt.plot(range(1, len(ft_losses) + 1), ft_losses, marker=\"o\", color=\"g\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-entropy Loss\")\n            plt.title(\"SPR_BENCH: Fine-tuning Training Loss vs Epoch\")\n            fn = os.path.join(working_dir, \"SPR_BENCH_finetune_train_loss.png\")\n            plt.savefig(fn)\n            plt.close()\n            print(f\"Saved {fn}\")\n    except Exception as e:\n        print(f\"Error creating finetune loss plot: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------- Plot 3: validation SWA & ACA\n    try:\n        if len(val_swa) and len(val_aca):\n            epochs = range(1, len(val_swa) + 1)\n            plt.figure()\n            plt.plot(epochs, val_swa, label=\"SWA\", marker=\"o\")\n            plt.plot(epochs, val_aca, label=\"ACA\", marker=\"s\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(\"SPR_BENCH: Validation SWA & ACA vs Epoch\")\n            plt.legend()\n            fn = os.path.join(working_dir, \"SPR_BENCH_val_swa_aca.png\")\n            plt.savefig(fn)\n            plt.close()\n            print(f\"Saved {fn}\")\n    except Exception as e:\n        print(f\"Error creating val metric plot: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------- Plot 4: validation loss\n    try:\n        if len(val_loss):\n            plt.figure()\n            plt.plot(range(1, len(val_loss) + 1), val_loss, color=\"r\", marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Validation Loss\")\n            plt.title(\"SPR_BENCH: Validation Loss vs Epoch\")\n            fn = os.path.join(working_dir, \"SPR_BENCH_val_loss.png\")\n            plt.savefig(fn)\n            plt.close()\n            print(f\"Saved {fn}\")\n    except Exception as e:\n        print(f\"Error creating val loss plot: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------- Plot 5: confusion matrix\n    try:\n        if preds.size and gts.size:\n            from itertools import product\n\n            cm = np.zeros((2, 2), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            for i, j in product(range(2), range(2)):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n            plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n            plt.title(\"SPR_BENCH: Confusion Matrix (Test Set)\")\n            fn = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fn)\n            plt.close()\n            print(f\"Saved {fn}\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------- Print final test metrics\n    if preds.size and gts.size:\n        swa_final = (\n            ds[\"metrics\"][\"val_SWA\"][-1] if len(ds[\"metrics\"][\"val_SWA\"]) else None\n        )\n        aca_final = (\n            ds[\"metrics\"][\"val_ACA\"][-1] if len(ds[\"metrics\"][\"val_ACA\"]) else None\n        )\n        print(f\"Final validation SWA: {swa_final}, ACA: {aca_final}\")\nelse:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n    losses = rec[\"losses\"]\n    metrics = rec[\"metrics\"]\n    aca = rec.get(\"aca\", {})\n    preds = np.array(rec.get(\"predictions\", []))\n    gts = np.array(rec.get(\"ground_truth\", []))\n    epochs = np.arange(1, max(len(losses[\"train\"]), len(losses[\"val\"])) + 1)\n    # optional down-sampling for clarity\n    if len(epochs) > 50:\n        step = int(np.ceil(len(epochs) / 50))\n        sel = slice(None, None, step)\n    else:\n        sel = slice(None)\n\n    # ------------------------------------------------- Plot 1: loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs[sel], np.array(losses[\"train\"])[sel], \"--\", label=\"Train Loss\")\n        plt.plot(epochs[sel], np.array(losses[\"val\"])[sel], \"-\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: validation accuracy\n    try:\n        plt.figure()\n        plt.plot(epochs[sel], np.array(metrics[\"val\"])[sel], color=\"g\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Accuracy\")\n        plt.title(f\"{ds_name}: Validation Accuracy per Epoch\")\n        fn = os.path.join(working_dir, f\"{ds_name}_val_accuracy.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation ACA\n    try:\n        if \"val\" in aca and len(aca[\"val\"]):\n            plt.figure()\n            plt.plot(epochs[sel], np.array(aca[\"val\"])[sel], color=\"m\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Validation ACA\")\n            plt.title(f\"{ds_name}: Validation ACA per Epoch\")\n            fn = os.path.join(working_dir, f\"{ds_name}_val_ACA.png\")\n            plt.savefig(fn)\n            print(f\"Saved {fn}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating ACA plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_classes = len(np.unique(np.concatenate([preds, gts])))\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xticks(range(num_classes), [f\"Pred {i}\" for i in range(num_classes)])\n            plt.yticks(range(num_classes), [f\"True {i}\" for i in range(num_classes)])\n            plt.title(f\"{ds_name}: Confusion Matrix (Test Set)\")\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fn = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fn)\n            print(f\"Saved {fn}\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\nelse:\n    print(\"No experiment data to plot.\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n\n    # ------------------------------------------------- Plot 1: contrastive pretrain loss\n    try:\n        plt.figure()\n        epochs = range(1, len(rec[\"losses\"][\"pretrain\"]) + 1)\n        plt.plot(epochs, rec[\"losses\"][\"pretrain\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Contrastive Loss\")\n        plt.title(f\"{ds_name}: Contrastive Pre-training Loss\")\n        fn = os.path.join(working_dir, f\"{ds_name}_pretrain_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: fine-tuning losses\n    try:\n        plt.figure()\n        e2 = range(1, len(rec[\"losses\"][\"train\"]) + 1)\n        plt.plot(e2, rec[\"losses\"][\"train\"], \"--\", label=\"train\")\n        plt.plot(e2, rec[\"losses\"][\"val\"], \"-\", label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy Loss\")\n        plt.title(f\"{ds_name}: Train vs Val Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_train_val_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train/val loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation accuracy & ACA\n    try:\n        plt.figure()\n        plt.plot(e2, rec[\"metrics\"][\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(e2, rec[\"metrics\"][\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Validation Metrics\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: test metric summary\n    try:\n        plt.figure()\n        test_metrics = rec[\"test\"]\n        names = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n        scores = [test_metrics[n] for n in names]\n        plt.bar(names, scores, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Test Metrics Summary\\nLeft to Right: Acc, SWA, CWA, ACA\")\n        fn = os.path.join(working_dir, f\"{ds_name}_test_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\nelse:\n    print(\"experiment_data not found or SPR_BENCH key missing.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\n# directory housekeeping\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\n\n# ---------------------------------------------------------------------\n# helper metrics (re-implemented here)\ndef count_shape_variety(seq: str) -> int:\n    return len(set(tok[0] for tok in seq.split()))\n\n\ndef count_color_variety(seq: str) -> int:\n    return len(set(tok[1] if len(tok) > 1 else \"#\" for tok in seq.split()))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / (\n        sum(w) + 1e-9\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi if t == p else 0 for wi, t, p in zip(w, y_true, y_pred)) / (\n        sum(w) + 1e-9\n    )\n\n\n# ---------------------------------------------------------------------\nif experiment_data is not None:\n    for ds_name, rec in experiment_data.items():\n        # ---------- recover arrays ----------\n        train_losses = rec[\"losses\"][\"train\"]\n        val_losses = rec[\"losses\"][\"val\"]\n        val_acc = rec[\"metrics\"].get(\"val_acc\", [])\n        val_swa = rec[\"metrics\"].get(\"val_swa\", [])\n        val_cwa = rec[\"metrics\"].get(\"val_cwa\", [])\n        predictions = rec.get(\"predictions\", [])\n        ground_truth = rec.get(\"ground_truth\", [])\n        aca_val = rec[\"aca\"].get(\"val\", [])\n        aca_test = rec[\"aca\"].get(\"test\", np.nan)\n\n        epochs = np.arange(1, len(train_losses) + 1)\n\n        # ---------- Plot 1: loss curves ----------\n        try:\n            plt.figure()\n            plt.plot(epochs, train_losses, \"r--\", label=\"Train\")\n            plt.plot(epochs, val_losses, \"b-\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-entropy loss\")\n            plt.title(f\"{ds_name}: Training & Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds_name}: {e}\")\n            plt.close()\n\n        # ---------- Plot 2: validation metric curves ----------\n        try:\n            plt.figure()\n            if val_acc:\n                plt.plot(epochs, val_acc, \"g-\", label=\"Acc\")\n            if val_swa:\n                plt.plot(epochs, val_swa, \"m-\", label=\"SWA\")\n            if val_cwa:\n                plt.plot(epochs, val_cwa, \"c-\", label=\"CWA\")\n            if aca_val:\n                plt.plot(epochs, aca_val, \"k--\", label=\"ACA\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{ds_name}: Validation Metrics vs Epoch\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating metric plot for {ds_name}: {e}\")\n            plt.close()\n\n        # ---------- compute & print test metrics ----------\n        if predictions and ground_truth:\n            # we lack raw sequences for SWA/CWA on test, so reuse preds only for ACC\n            acc = np.mean(np.array(predictions) == np.array(ground_truth))\n            # try to extract raw seqs stored during evaluation if present\n            raw_seqs = rec.get(\"raw_test_seqs\", [\"\"] * len(predictions))\n            swa = shape_weighted_accuracy(raw_seqs, ground_truth, predictions)\n            cwa = color_weighted_accuracy(raw_seqs, ground_truth, predictions)\n            print(\n                f\"{ds_name} Test metrics \u2014 ACC: {acc:.4f} | SWA: {swa:.4f} | \"\n                f\"CWA: {cwa:.4f} | ACA: {aca_test:.4f}\"\n            )\n\n            # ---------- Plot 3: bar summary ----------\n            try:\n                plt.figure()\n                metrics = [\"ACC\", \"SWA\", \"CWA\", \"ACA\"]\n                vals = [acc, swa, cwa, aca_test]\n                plt.bar(metrics, vals, color=[\"g\", \"m\", \"c\", \"k\"], alpha=0.7)\n                plt.ylim(0, 1)\n                plt.ylabel(\"Score\")\n                plt.title(f\"{ds_name}: Test Metric Summary\")\n                fname = os.path.join(working_dir, f\"{ds_name}_test_summary.png\")\n                plt.savefig(fname)\n                print(f\"Saved {fname}\")\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating summary bar plot for {ds_name}: {e}\")\n                plt.close()\n\n            # ---------- Plot 4: confusion matrix ----------\n            try:\n                classes = sorted(set(ground_truth))\n                cm = np.zeros((len(classes), len(classes)), dtype=int)\n                for t, p in zip(ground_truth, predictions):\n                    cm[t, p] += 1\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im)\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"True\")\n                plt.title(f\"{ds_name}: Confusion Matrix\")\n                plt.xticks(classes)\n                plt.yticks(classes)\n                fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n                plt.savefig(fname)\n                print(f\"Saved {fname}\")\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n\n    train_loss = rec[\"losses\"][\"train\"]\n    val_loss = rec[\"losses\"][\"val\"]\n    val_swa = rec[\"metrics\"][\"val\"]\n    val_aca = rec[\"ACA\"][\"val\"]\n    test_swa = rec[\"metrics\"][\"test_SWA\"]\n    test_cwa = rec[\"metrics\"][\"test_CWA\"]\n    test_aca = rec[\"ACA\"][\"test\"]\n    preds = np.array(rec[\"predictions\"])\n    gts = np.array(rec[\"ground_truth\"])\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # ------------------------------------------------- Plot 1: loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, \"r--\", label=\"Train\")\n        plt.plot(epochs, val_loss, \"b-\", label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy loss\")\n        plt.title(f\"{ds_name}: Training & Validation Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: validation SWA curves\n    try:\n        plt.figure()\n        plt.plot(epochs, val_swa, \"g-\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation SWA\")\n        plt.title(f\"{ds_name}: Validation Shape-Weighted Accuracy\")\n        fn = os.path.join(working_dir, f\"{ds_name}_val_swa_curve.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating SWA curve plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation ACA curves\n    try:\n        plt.figure()\n        plt.plot(epochs, val_aca, \"m-\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation ACA\")\n        plt.title(f\"{ds_name}: Validation Augmentation-Consistency Accuracy\")\n        fn = os.path.join(working_dir, f\"{ds_name}_val_aca_curve.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating ACA curve plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: test metric summary\n    try:\n        plt.figure()\n        metrics = [test_swa, test_cwa, test_aca]\n        names = [\"SWA\", \"CWA\", \"ACA\"]\n        plt.bar(names, metrics, color=[\"g\", \"c\", \"m\"])\n        plt.ylim(0, 1.0)\n        for i, v in enumerate(metrics):\n            plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Test Metrics Summary\")\n        fn = os.path.join(working_dir, f\"{ds_name}_test_metric_summary.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metric bar plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 5: confusion matrix\n    try:\n        from sklearn.metrics import confusion_matrix\n\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.title(f\"{ds_name}: Confusion Matrix\\nLeft: True, Right: Predicted\")\n        plt.xlabel(\"Predicted label\")\n        plt.ylabel(\"True label\")\n        for (i, j), v in np.ndenumerate(cm):\n            plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"red\")\n        fn = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n        plt.savefig(fn)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------ print final metrics\n    print(\n        f\"Test results for {ds_name} -> SWA: {test_swa:.4f}, CWA: {test_cwa:.4f}, ACA: {test_aca:.4f}\"\n    )\nelse:\n    print(\"No experiment data found for SPR_BENCH\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n\n    # ------------------------------------------------- Plot 1: contrastive pretrain loss\n    try:\n        plt.figure()\n        epochs = range(1, len(rec[\"losses\"][\"pretrain\"]) + 1)\n        plt.plot(epochs, rec[\"losses\"][\"pretrain\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Contrastive Loss\")\n        plt.title(f\"{ds_name}: Contrastive Pre-training Loss\")\n        fn = os.path.join(working_dir, f\"{ds_name}_pretrain_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: fine-tuning losses\n    try:\n        plt.figure()\n        e2 = range(1, len(rec[\"losses\"][\"train\"]) + 1)\n        plt.plot(e2, rec[\"losses\"][\"train\"], \"--\", label=\"train\")\n        plt.plot(e2, rec[\"losses\"][\"val\"], \"-\", label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy Loss\")\n        plt.title(f\"{ds_name}: Train vs Val Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_train_val_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train/val loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation accuracy & ACA\n    try:\n        plt.figure()\n        plt.plot(e2, rec[\"metrics\"][\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(e2, rec[\"metrics\"][\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Validation Metrics\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: test metric summary\n    try:\n        plt.figure()\n        test_metrics = rec[\"test\"]\n        names = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n        scores = [test_metrics[n] for n in names]\n        plt.bar(names, scores, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Test Metrics Summary\\nLeft to Right: Acc, SWA, CWA, ACA\")\n        fn = os.path.join(working_dir, f\"{ds_name}_test_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\nelse:\n    print(\"experiment_data not found or SPR_BENCH key missing.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n\n    # ------------------------------------------------- Plot 1: contrastive pretrain loss\n    try:\n        plt.figure()\n        epochs = range(1, len(rec[\"losses\"][\"pretrain\"]) + 1)\n        plt.plot(epochs, rec[\"losses\"][\"pretrain\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Contrastive Loss\")\n        plt.title(f\"{ds_name}: Contrastive Pre-training Loss\")\n        fn = os.path.join(working_dir, f\"{ds_name}_pretrain_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: fine-tuning losses\n    try:\n        plt.figure()\n        e2 = range(1, len(rec[\"losses\"][\"train\"]) + 1)\n        plt.plot(e2, rec[\"losses\"][\"train\"], \"--\", label=\"train\")\n        plt.plot(e2, rec[\"losses\"][\"val\"], \"-\", label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy Loss\")\n        plt.title(f\"{ds_name}: Train vs Val Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_train_val_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train/val loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation accuracy & ACA\n    try:\n        plt.figure()\n        plt.plot(e2, rec[\"metrics\"][\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(e2, rec[\"metrics\"][\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Validation Metrics\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: test metric summary\n    try:\n        plt.figure()\n        test_metrics = rec[\"test\"]\n        names = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n        scores = [test_metrics[n] for n in names]\n        plt.bar(names, scores, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Test Metrics Summary\\nLeft to Right: Acc, SWA, CWA, ACA\")\n        fn = os.path.join(working_dir, f\"{ds_name}_test_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\nelse:\n    print(\"experiment_data not found or SPR_BENCH key missing.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None and \"SPR_BENCH\" in experiment_data:\n    ds_name = \"SPR_BENCH\"\n    rec = experiment_data[ds_name]\n\n    # ------------------------------------------------- Plot 1: contrastive pretrain loss\n    try:\n        plt.figure()\n        epochs = range(1, len(rec[\"losses\"][\"pretrain\"]) + 1)\n        plt.plot(epochs, rec[\"losses\"][\"pretrain\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Contrastive Loss\")\n        plt.title(f\"{ds_name}: Contrastive Pre-training Loss\")\n        fn = os.path.join(working_dir, f\"{ds_name}_pretrain_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pretrain loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 2: fine-tuning losses\n    try:\n        plt.figure()\n        e2 = range(1, len(rec[\"losses\"][\"train\"]) + 1)\n        plt.plot(e2, rec[\"losses\"][\"train\"], \"--\", label=\"train\")\n        plt.plot(e2, rec[\"losses\"][\"val\"], \"-\", label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-entropy Loss\")\n        plt.title(f\"{ds_name}: Train vs Val Loss\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_train_val_loss.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating train/val loss plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 3: validation accuracy & ACA\n    try:\n        plt.figure()\n        plt.plot(e2, rec[\"metrics\"][\"val_acc\"], label=\"Val Accuracy\")\n        plt.plot(e2, rec[\"metrics\"][\"val_aca\"], label=\"Val ACA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Validation Metrics\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_val_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation metrics plot: {e}\")\n        plt.close()\n\n    # ------------------------------------------------- Plot 4: test metric summary\n    try:\n        plt.figure()\n        test_metrics = rec[\"test\"]\n        names = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n        scores = [test_metrics[n] for n in names]\n        plt.bar(names, scores, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(f\"{ds_name}: Test Metrics Summary\\nLeft to Right: Acc, SWA, CWA, ACA\")\n        fn = os.path.join(working_dir, f\"{ds_name}_test_metrics.png\")\n        plt.savefig(fn)\n        print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test metrics plot: {e}\")\n        plt.close()\nelse:\n    print(\"experiment_data not found or SPR_BENCH key missing.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# -----------------------------------------------------------------------------\n# helper to aggregate arrays of unequal length (truncate to min length)\ndef stack_and_trim(list_of_1d_arrays):\n    if len(list_of_1d_arrays) == 0:\n        return None\n    min_len = min([len(a) for a in list_of_1d_arrays])\n    trimmed = np.stack([a[:min_len] for a in list_of_1d_arrays], axis=0)\n    return trimmed  # shape (n_runs, min_len)\n\n\n# -----------------------------------------------------------------------------\n# 1. Load every experiment_data dict\nexperiment_data_path_list = [\n    \"experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_d4c9db252f844ac78db418307ef65820_proc_3027375/experiment_data.npy\",\n    \"experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_e3f510ef01504da8821ead5b89234ed3_proc_3027374/experiment_data.npy\",\n    \"experiments/2025-08-15_23-37-11_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_59ed195897034fec92ce4e9d1705440c_proc_3027376/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        d = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(d)\n    except Exception as e:\n        print(f\"Error loading experiment data from {p}: {e}\")\n\n# -----------------------------------------------------------------------------\n# 2. Collect every dataset that appears in at least one run\ndataset_names = set()\nfor d in all_experiment_data:\n    dataset_names.update(d.keys())\n\n# -----------------------------------------------------------------------------\n# 3. Iterate over datasets and build plots with aggregation\nfor ds_name in dataset_names:\n    # Gather per-run records for this dataset\n    per_run_recs = [d[ds_name] for d in all_experiment_data if ds_name in d]\n    n_runs = len(per_run_recs)\n    if n_runs == 0:\n        continue  # nothing to aggregate\n\n    # ======================= Plot A: Pre-train contrastive loss ===============\n    try:\n        pre_losses = [\n            np.asarray(r[\"losses\"][\"pretrain\"])\n            for r in per_run_recs\n            if \"pretrain\" in r[\"losses\"]\n        ]\n        stacked = stack_and_trim(pre_losses)\n        if stacked is not None:\n            mean = stacked.mean(axis=0)\n            se = stacked.std(axis=0, ddof=1) / np.sqrt(stacked.shape[0])\n            epochs = np.arange(1, len(mean) + 1)\n            plt.figure()\n            plt.plot(epochs, mean, color=\"C0\", label=\"Mean\")\n            plt.fill_between(\n                epochs, mean - se, mean + se, color=\"C0\", alpha=0.3, label=\"\u00b1SE\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Contrastive Loss\")\n            plt.title(\n                f\"{ds_name}: Contrastive Pre-training Loss (Mean \u00b1 SE, n={n_runs})\"\n            )\n            plt.legend()\n            fn = os.path.join(working_dir, f\"{ds_name}_agg_pretrain_loss.png\")\n            plt.savefig(fn)\n            print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated pretrain loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ======================= Plot B: Fine-tune train/val loss ================\n    try:\n        train_losses = [\n            np.asarray(r[\"losses\"][\"train\"])\n            for r in per_run_recs\n            if \"train\" in r[\"losses\"]\n        ]\n        val_losses = [\n            np.asarray(r[\"losses\"][\"val\"]) for r in per_run_recs if \"val\" in r[\"losses\"]\n        ]\n        s_train = stack_and_trim(train_losses)\n        s_val = stack_and_trim(val_losses)\n        if s_train is not None and s_val is not None:\n            m_train, se_train = s_train.mean(0), s_train.std(0, ddof=1) / np.sqrt(\n                s_train.shape[0]\n            )\n            m_val, se_val = s_val.mean(0), s_val.std(0, ddof=1) / np.sqrt(\n                s_val.shape[0]\n            )\n            epochs = np.arange(1, len(m_train) + 1)\n            plt.figure()\n            plt.plot(epochs, m_train, \"--\", color=\"C1\", label=\"Train Mean\")\n            plt.fill_between(\n                epochs,\n                m_train - se_train,\n                m_train + se_train,\n                color=\"C1\",\n                alpha=0.25,\n                label=\"Train \u00b1SE\",\n            )\n            plt.plot(epochs, m_val, \"-\", color=\"C2\", label=\"Val Mean\")\n            plt.fill_between(\n                epochs,\n                m_val - se_val,\n                m_val + se_val,\n                color=\"C2\",\n                alpha=0.25,\n                label=\"Val \u00b1SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-entropy Loss\")\n            plt.title(f\"{ds_name}: Fine-tuning Loss (Mean \u00b1 SE, n={n_runs})\")\n            plt.legend()\n            fn = os.path.join(working_dir, f\"{ds_name}_agg_train_val_loss.png\")\n            plt.savefig(fn)\n            print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated train/val loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ======================= Plot C: Validation metrics ======================\n    try:\n        val_accs = [\n            np.asarray(r[\"metrics\"][\"val_acc\"])\n            for r in per_run_recs\n            if \"val_acc\" in r[\"metrics\"]\n        ]\n        val_acas = [\n            np.asarray(r[\"metrics\"][\"val_aca\"])\n            for r in per_run_recs\n            if \"val_aca\" in r[\"metrics\"]\n        ]\n        s_acc = stack_and_trim(val_accs)\n        s_aca = stack_and_trim(val_acas)\n        if s_acc is not None and s_aca is not None:\n            m_acc, se_acc = s_acc.mean(0), s_acc.std(0, ddof=1) / np.sqrt(\n                s_acc.shape[0]\n            )\n            m_aca, se_aca = s_aca.mean(0), s_aca.std(0, ddof=1) / np.sqrt(\n                s_aca.shape[0]\n            )\n            epochs = np.arange(1, len(m_acc) + 1)\n            plt.figure()\n            plt.plot(epochs, m_acc, color=\"C3\", label=\"Val Acc Mean\")\n            plt.fill_between(\n                epochs,\n                m_acc - se_acc,\n                m_acc + se_acc,\n                color=\"C3\",\n                alpha=0.25,\n                label=\"Acc \u00b1SE\",\n            )\n            plt.plot(epochs, m_aca, color=\"C4\", label=\"Val ACA Mean\")\n            plt.fill_between(\n                epochs,\n                m_aca - se_aca,\n                m_aca + se_aca,\n                color=\"C4\",\n                alpha=0.25,\n                label=\"ACA \u00b1SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.title(f\"{ds_name}: Validation Metrics (Mean \u00b1 SE, n={n_runs})\")\n            plt.legend()\n            fn = os.path.join(working_dir, f\"{ds_name}_agg_val_metrics.png\")\n            plt.savefig(fn)\n            print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated validation metrics plot for {ds_name}: {e}\")\n        plt.close()\n\n    # ======================= Plot D: Test metrics summary ====================\n    try:\n        metric_names = [\"acc\", \"swa\", \"cwa\", \"aca\"]\n        collected = {k: [] for k in metric_names}\n        for r in per_run_recs:\n            for k in metric_names:\n                if k in r[\"test\"]:\n                    collected[k].append(r[\"test\"][k])\n        means = []\n        stderrs = []\n        labels_present = []\n        for k in metric_names:\n            vals = collected[k]\n            if len(vals) > 0:\n                labels_present.append(k)\n                vals_arr = np.asarray(vals)\n                means.append(vals_arr.mean())\n                stderrs.append(vals_arr.std(ddof=1) / np.sqrt(len(vals_arr)))\n        if len(means):\n            x = np.arange(len(means))\n            plt.figure()\n            plt.bar(\n                x,\n                means,\n                yerr=stderrs,\n                capsize=5,\n                color=\"skyblue\",\n                alpha=0.8,\n                label=\"Mean \u00b1 SE\",\n            )\n            plt.xticks(x, labels_present)\n            plt.ylim(0, 1)\n            plt.ylabel(\"Score\")\n            plt.title(\n                f\"{ds_name}: Test Metrics Summary (Mean \u00b1 SE, n={n_runs})\\nLeft\u2192Right: \"\n                + \", \".join(labels_present)\n            )\n            plt.legend()\n            fn = os.path.join(working_dir, f\"{ds_name}_agg_test_metrics.png\")\n            plt.savefig(fn)\n            print(f\"Saved {fn}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated test metrics plot for {ds_name}: {e}\")\n        plt.close()\n"], "plot_plan": ["We will first load experiment_data.npy and extract the \u201cper_layer\u201d dictionary for the SPR_BENCH dataset, which stores epoch-wise losses and validation SCWA for each candidate depth.  \nThree visualizations will be produced (well under the 5-figure cap).  \n(1) A multi-curve plot of training and validation loss versus epoch, colour-coded by num_layers, giving a quick view of convergence.  \n(2) A validation-SCWA-versus-epoch plot, again colour-coded by depth, to see how quickly and how well each configuration reaches its peak metric.  \n(3) A bar chart that summarises the best validation SCWA attained by every depth and, for the overall best depth, also shows its final test SCWA, allowing easy comparison of hyper-parameter performance.  \nEach plot is wrapped in its own try-except, saved in working_dir with descriptive filenames, and the figures are always closed.  \nNo data are fabricated; everything comes directly from experiment_data.npy.  \nThe code is concise, uses basic matplotlib, follows the required import header, and prints the locations of the saved figures so downstream steps can pick them up easily.", "The script will first load experiment_data.npy and confirm the presence of the \u201cSPR_BENCH\u201d entry.  It extracts the length-T arrays that were logged during training: pre-training losses, fine-tuning losses, and the three validation metrics (loss, SWA, ACA).  Five separate figures are generated inside independent try-except blocks.  (1) Pre-training contrastive loss vs epoch; (2) fine-tuning training loss vs epoch; (3) validation curves (SWA and ACA) vs epoch; (4) validation loss vs epoch; (5) a 2\u00d72 confusion-matrix heat-map for the final test predictions.  All plots are titled, saved to working_dir with descriptive filenames, and each figure is closed after saving.  The code finishes by printing the final test SWA and ACA that were stored in experiment_data.  Only numpy and matplotlib are used, and plotting is limited to the data already available in the .npy file.", "We first load experiment_data.npy from the working directory and safely exit if it is missing.  From the saved dict we read the \u201cSPR_BENCH\u201d entry: its training/validation loss lists, validation accuracy list, validation ACA list, and the final test\u2010set predictions and ground-truth labels.  Using these arrays we produce four figures: (1) training vs validation loss per epoch; (2) validation accuracy per epoch; (3) validation ACA per epoch; (4) a 2\u00d72 confusion-matrix heat-map on the test set.  Each figure is wrapped in its own try-except block, uses basic matplotlib, has a clear title/sub-title, and is saved into working_dir with descriptive names.  We close every figure in both success and failure cases.  No extra datasets or synthetic numbers are fabricated; everything plotted is read directly from experiment_data.npy.  We also cap the number of points plotted by sub-sampling epochs when more than 50 epochs exist, ensuring legibility.  Finally, after generating the plots the script prints the location of each saved file so downstream tools can find them.", null, null, "We load the saved experiment_data.npy, focus on the single dataset key \u201cSPR_BENCH\u201d, and safely exit if it is missing or malformed. For visualization we create up to four standard plots: (1) contrastive-pretraining loss over epochs, (2) fine-tuning train vs validation loss, (3) validation accuracy and ACA curves, and (4) a bar chart summarizing the four test metrics (acc, swa, cwa, aca). Each figure is wrapped in its own try-except so that a failure in one plot does not affect the rest, and every figure is closed after saving. File names include the dataset name and plot description for clarity. No synthetic data are generated; only arrays already present in experiment_data are used, and we subsample nothing because the lists are short (<10). Basic matplotlib is used without extra styles, with clear axis labels and informative titles/subtitles. All plots are written to the previously created \u201cworking\u201d directory. Finally, if everything succeeds the script prints the absolute paths of the saved figures.", "We load experiment_data.npy from the working directory and loop over every dataset present (typically only \u2018SPR_BENCH\u2019).  For each dataset we recover the saved training/validation losses and validation metrics so we can draw classical learning-curve style plots: (1) loss vs epoch and (2) accuracy/SWA/CWA vs epoch (three coloured lines in one panel).  Next we rebuild the helper metric functions locally, recompute the test metrics directly from the saved predictions/ground-truth arrays, and print them for inspection.  Using these scores we make (3) a concise bar-chart summarising test ACC, SWA, CWA and ACA.  Finally, if predictions exist we create (4) a confusion-matrix style heat-map so users can see error structure.  Each figure is wrapped in its own try-except; the figure is closed after saving with a descriptive file name containing the dataset name and plot type.  We cap ourselves to these four plots (\u22645) and skip gracefully if some field is missing.  All plots are saved to working_dir and figures use plain matplotlib defaults.  The script finishes by printing the computed test metrics so the notebook/console shows a quick numeric summary.", "We load experiment_data.npy, target the \u201cSPR_BENCH\u201d entry and extract epoch-level arrays for training/validation losses, validation shape-weighted accuracy (SWA) and augmentation-consistency accuracy (ACA), along with final test metrics and per-example predictions/labels. Next, we create up to five standard matplotlib plots in separate try-except blocks: (1) train vs. validation loss across epochs, (2) validation SWA across epochs, (3) validation ACA across epochs, (4) bar chart summarising test SWA, CWA and ACA, and (5) a small confusion-matrix heat-map of test predictions vs. ground truth. Every figure gets a clear title and is saved to \u201cworking\u201d with descriptive names. We always close figures, print any plotting errors, and finally echo the test metrics to stdout so the notebook/runner can see them. All code is concise, self-contained and only visualises data that actually exists in experiment_data.npy.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["num_layers", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The code will load the saved NumPy dictionary, locate the information recorded\nfor each dataset, and then report the most relevant \u201cfinal\u201d or \u201cbest\u201d\nmetrics\u2014specifically the best hyper-parameter choice, the last observed\ntraining/validation losses for that configuration, the best validation SCWA, and\nthe test SCWA. Each metric is printed with an explicit descriptive label,\npreceded by the dataset name.", "Below we load the saved numpy file, extract the stored lists, keep only the last\n(i.e., final) entry of each list, and print them with explicit metric names for\nevery dataset contained in the file.", "The script will locate the saved NumPy file in the \u201cworking\u201d directory, load it\ninto a Python dictionary, and iterate over each dataset contained within. For\nevery dataset it extracts the relevant lists or scalars, computes the final\ntraining loss, best (validation) loss, best validation accuracy, best validation\nACA, test accuracy (computed from stored predictions), and test ACA where\navailable. It then prints the dataset name followed by clearly-labelled metric\nnames and their respective best or final values. No extra entry-point guard or\nplots are used, so the script runs immediately when executed.", "The script will load the saved NumPy dictionary, walk through each dataset (here\nonly \u201cSPR_BENCH\u201d), compute the final or best values requested, and print them\nwith explicit metric names. All code is at the global scope so it runs\nimmediately.", "Below is the code to load the saved NumPy file, extract the relevant\narrays/dictionaries, pick the \u201cbest\u201d (minimum for losses, maximum for\naccuracies) or final values as appropriate, and print them with explicit,\nunambiguous labels for each dataset.", "The script loads working/experiment_data.npy, iterates over every top-level\ndataset (e.g., \u201cSPR_BENCH\u201d), and for each metric series it reports the best\nvalue (minimum for losses, maximum for accuracies) while single-valued test\nstatistics are printed directly. Every printout begins with the dataset name\nfollowed by explicit, self-describing metric labels such as \u201cbest validation\naccuracy\u201d or \u201ctest color-weighted accuracy.\u201d The code runs immediately when\nexecuted without any special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate over every dataset key (e.g., \u2018SPR_BENCH\u2019), and extract the stored\nmetric/loss arrays. For loss curves the final (last) value is reported, while\nfor accuracies the best (maximum) value is shown. It also recomputes the test-\nset accuracy from the saved predictions and ground-truth labels, and prints the\nstored test ACA. All values are printed with explicit metric names exactly as\nrequired, with no plotting and no reliance on an \u201cif __name__ == '__main__'\u201d\nguard.", "The script will locate the \u201cworking\u201d directory, load the saved NumPy dictionary,\nand then iterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d). For every\ndataset it fetches the relevant arrays or scalars, selects the final value for\ntraining loss, the best (min / max) values for validation metrics, and the\nstored test-set results. Each metric is printed with an explicit, human-readable\nlabel before its value, satisfying the formatting rules.", "This script will load the saved NumPy dictionary, pull out the stored\nlists/scalars, derive any missing values (e.g., test accuracy from predictions\nvs ground-truth), pick the \u201cbest\u201d (maximum) validation accuracy and the final\nvalues for the other logged metrics, and print them with fully-qualified names.", "The script loads working/experiment_data.npy, iterates over every top-level\ndataset (e.g., \u201cSPR_BENCH\u201d), and for each metric series it reports the best\nvalue (minimum for losses, maximum for accuracies) while single-valued test\nstatistics are printed directly. Every printout begins with the dataset name\nfollowed by explicit, self-describing metric labels such as \u201cbest validation\naccuracy\u201d or \u201ctest color-weighted accuracy.\u201d The code runs immediately when\nexecuted without any special entry point.", "The script loads working/experiment_data.npy, iterates over every top-level\ndataset (e.g., \u201cSPR_BENCH\u201d), and for each metric series it reports the best\nvalue (minimum for losses, maximum for accuracies) while single-valued test\nstatistics are printed directly. Every printout begins with the dataset name\nfollowed by explicit, self-describing metric labels such as \u201cbest validation\naccuracy\u201d or \u201ctest color-weighted accuracy.\u201d The code runs immediately when\nexecuted without any special entry point.", "The script loads working/experiment_data.npy, iterates over every top-level\ndataset (e.g., \u201cSPR_BENCH\u201d), and for each metric series it reports the best\nvalue (minimum for losses, maximum for accuracies) while single-valued test\nstatistics are printed directly. Every printout begins with the dataset name\nfollowed by explicit, self-describing metric labels such as \u201cbest validation\naccuracy\u201d or \u201ctest color-weighted accuracy.\u201d The code runs immediately when\nexecuted without any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------------\n# Iterate through the stored results and print concise metric summaries\nfor sweep_name, datasets in experiment_data.items():  # e.g. \"num_layers\"\n    for dataset_name, ds_info in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"{dataset_name}\")  # dataset header\n\n        # Best hyper-parameter setting discovered during the sweep\n        best_layer = ds_info.get(\"best_layer\")\n        if best_layer is not None:\n            print(f\"best number of layers: {best_layer}\")\n\n        # Retrieve losses for the best layer (if available)\n        if best_layer in ds_info.get(\"per_layer\", {}):\n            layer_record = ds_info[\"per_layer\"][best_layer]\n            train_losses = layer_record[\"losses\"][\"train\"]\n            val_losses = layer_record[\"losses\"][\"val\"]\n\n            if train_losses:\n                print(f\"final training loss: {train_losses[-1]:.4f}\")\n            if val_losses:\n                print(f\"final validation loss: {val_losses[-1]:.4f}\")\n\n        # Best validation SCWA achieved during hyper-parameter tuning\n        best_val_scwa = ds_info.get(\"best_val_scwa\")\n        if best_val_scwa is not None:\n            print(f\"best validation SCWA: {best_val_scwa:.4f}\")\n\n        # Test-set SCWA for the model chosen via validation performance\n        test_scwa = ds_info.get(\"test_scwa\")\n        if test_scwa is not None:\n            print(f\"test SCWA: {test_scwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# 1. Helper to safely fetch the final value of a list\ndef final_value(lst):\n    return lst[-1] if isinstance(lst, (list, tuple)) and lst else None\n\n\n# ---------------------------------------------------------------------\n# 2. Iterate over each dataset and print final metrics\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # losses (training statistics)\n    pre_loss = final_value(data.get(\"losses\", {}).get(\"pretrain\", []))\n    ft_loss = final_value(data.get(\"losses\", {}).get(\"finetune\", []))\n\n    # validation metrics\n    val_swa = final_value(data.get(\"metrics\", {}).get(\"val_SWA\", []))\n    val_aca = final_value(data.get(\"metrics\", {}).get(\"val_ACA\", []))\n    val_loss = final_value(data.get(\"metrics\", {}).get(\"val_loss\", []))\n\n    # -----------------------------------------------------------------\n    # 3. Print each metric with explicit, descriptive names\n    if pre_loss is not None:\n        print(f\"final pretraining loss: {pre_loss:.6f}\")\n    if ft_loss is not None:\n        print(f\"final fine-tuning training loss: {ft_loss:.6f}\")\n    if val_loss is not None:\n        print(f\"final validation loss: {val_loss:.6f}\")\n    if val_swa is not None:\n        print(f\"final validation shape-weighted accuracy: {val_swa:.6f}\")\n    if val_aca is not None:\n        print(f\"final validation adversarial consistency accuracy: {val_aca:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load the saved experiment data\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to compute accuracy from two lists\n# ---------------------------------------------------------------------\ndef _accuracy(preds, gts):\n    if len(preds) == 0:\n        return None\n    preds = np.asarray(preds)\n    gts = np.asarray(gts)\n    return float((preds == gts).mean())\n\n\n# ---------------------------------------------------------------------\n# Iterate through datasets and print requested metrics\n# ---------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset heading\n\n    # -------- Training loss (final) ----------------------------------\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.4f}\")\n\n    # -------- Validation loss (best / minimum) -----------------------\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = float(min(val_losses))\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n\n    # -------- Validation accuracy (best / maximum) -------------------\n    val_accs = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_accs:\n        best_val_acc = float(max(val_accs))\n        print(f\"best validation accuracy: {best_val_acc:.4f}\")\n\n    # -------- Validation ACA (best / maximum) ------------------------\n    val_acas = data.get(\"aca\", {}).get(\"val\", [])\n    if val_acas:\n        best_val_aca = float(max(val_acas))\n        print(f\"best validation ACA: {best_val_aca:.4f}\")\n\n    # -------- Test accuracy ------------------------------------------\n    test_preds = data.get(\"predictions\", [])\n    test_gts = data.get(\"ground_truth\", [])\n    test_acc = _accuracy(test_preds, test_gts)\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n\n    # -------- Test ACA -----------------------------------------------\n    test_aca = data.get(\"aca\", {}).get(\"test\", None)\n    if isinstance(test_aca, (float, int)):\n        print(f\"test ACA: {test_aca:.4f}\")\n\n    # Visual separator for readability when multiple datasets exist\n    print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------- load data ----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------- pretty print helper ------------------------------------------------\ndef maybe_print(label: str, value):\n    if value is not None:\n        print(f\"{label}: {value:.4f}\")\n\n\n# ------------------------------------------------- reporting ----------------------------------------------------------\nfor dataset_name, dct in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---------- losses ----------\n    train_losses = dct.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dct.get(\"losses\", {}).get(\"val\", [])\n\n    maybe_print(\"final train loss\", train_losses[-1] if train_losses else None)\n    maybe_print(\"final validation loss\", val_losses[-1] if val_losses else None)\n    maybe_print(\"best validation loss\", min(val_losses) if val_losses else None)\n\n    # ---------- Shape-weighted accuracy ----------\n    val_swa = dct.get(\"metrics\", {}).get(\"val\", [])\n    maybe_print(\n        \"best validation shape-weighted accuracy\", max(val_swa) if val_swa else None\n    )\n    maybe_print(\n        \"final validation shape-weighted accuracy\", val_swa[-1] if val_swa else None\n    )\n\n    # ---------- Augmented consistency accuracy ----------\n    val_aca = dct.get(\"ACA\", {}).get(\"val\", [])\n    maybe_print(\n        \"best validation augmented consistency accuracy\",\n        max(val_aca) if val_aca else None,\n    )\n    maybe_print(\n        \"final validation augmented consistency accuracy\",\n        val_aca[-1] if val_aca else None,\n    )\n\n    # ---------- Test metrics ----------\n    test_swa = dct.get(\"metrics\", {}).get(\"test_SWA\", None)\n    test_cwa = dct.get(\"metrics\", {}).get(\"test_CWA\", None)\n    test_aca = dct.get(\"ACA\", {}).get(\"test\", None)\n\n    maybe_print(\"test shape-weighted accuracy\", test_swa)\n    maybe_print(\"test color-weighted accuracy\", test_cwa)\n    if isinstance(test_aca, list):  # stored as list?\n        test_aca = test_aca[-1] if test_aca else None\n    maybe_print(\"test augmented consistency accuracy\", test_aca)\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------------- #\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------------------------------- #\ndef safe_best(values, mode=\"min\"):\n    \"\"\"Return best value from a list based on mode.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\n# --------------------------------------------------------------------------- #\nfor dataset_name, data_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---------- Losses ------------------------------------------------------ #\n    pretrain_best = safe_best(data_dict.get(\"losses\", {}).get(\"pretrain\", []), \"min\")\n    train_best = safe_best(data_dict.get(\"losses\", {}).get(\"train\", []), \"min\")\n    val_best = safe_best(data_dict.get(\"losses\", {}).get(\"val\", []), \"min\")\n\n    if pretrain_best is not None:\n        print(f\"Minimum pretraining loss:      {pretrain_best:.4f}\")\n    if train_best is not None:\n        print(f\"Minimum training loss:         {train_best:.4f}\")\n    if val_best is not None:\n        print(f\"Minimum validation loss:       {val_best:.4f}\")\n\n    # ---------- Validation metrics ----------------------------------------- #\n    val_acc_best = safe_best(data_dict.get(\"metrics\", {}).get(\"val_acc\", []), \"max\")\n    val_aca_best = safe_best(data_dict.get(\"metrics\", {}).get(\"val_aca\", []), \"max\")\n\n    if val_acc_best is not None:\n        print(f\"Maximum validation accuracy:   {val_acc_best:.4f}\")\n    if val_aca_best is not None:\n        print(f\"Maximum validation ACA:        {val_aca_best:.4f}\")\n\n    # ---------- Test metrics ------------------------------------------------ #\n    test_results = data_dict.get(\"test\", {})\n    if test_results:\n        print(f\"Test loss:                     {test_results.get('loss', np.nan):.4f}\")\n        print(f\"Test accuracy:                 {test_results.get('acc',  np.nan):.4f}\")\n        print(f\"Test shape-weighted accuracy:  {test_results.get('swa',  np.nan):.4f}\")\n        print(f\"Test color-weighted accuracy:  {test_results.get('cwa',  np.nan):.4f}\")\n        print(f\"Test ACA:                      {test_results.get('aca',  np.nan):.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions to select best values\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_accuracy(values):\n    return max(values) if values else None\n\n\n# Mapping from metric key to (pretty name, selector)\nseries_handlers = {\n    (\"losses\", \"pretrain\"): (\"best pretraining loss\", best_loss),\n    (\"losses\", \"train\"): (\"best training loss\", best_loss),\n    (\"losses\", \"val\"): (\"best validation loss\", best_loss),\n    (\"metrics\", \"val_acc\"): (\"best validation accuracy\", best_accuracy),\n    (\"metrics\", \"val_aca\"): (\"best validation ACA\", best_accuracy),\n}\n\n# -----------------------------------------------------------------------------#\n# Iterate and print\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle series metrics (stored as lists)\n    for (section, key), (pretty_name, selector) in series_handlers.items():\n        values = data.get(section, {}).get(key, [])\n        if values:\n            print(f\"{pretty_name}: {selector(values):.4f}\")\n\n    # Handle single-valued test metrics\n    test_metrics = data.get(\"test\", {})\n    single_metric_names = {\n        \"loss\": \"test loss\",\n        \"acc\": \"test accuracy\",\n        \"swa\": \"test shape-weighted accuracy\",\n        \"cwa\": \"test color-weighted accuracy\",\n        \"aca\": \"test ACA\",\n    }\n    for k, pretty_name in single_metric_names.items():\n        if k in test_metrics:\n            print(f\"{pretty_name}: {test_metrics[k]:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the numpy results dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_file, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper for nice numeric formatting\ndef fmt(x, digits=4):\n    return f\"{x:.{digits}f}\" if isinstance(x, (float, int)) else str(x)\n\n\n# ------------------------------------------------------------------\n# iterate through every dataset saved in the dictionary\nfor dset_name, dset_dict in experiment_data.items():\n    print(f\"\\nDataset: {dset_name}\")\n\n    # ---------------- losses ----------------\n    train_losses = dset_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dset_dict.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(\"final train loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"best validation loss:\", fmt(min(val_losses)))\n\n    # ---------------- validation metrics ----------------\n    val_acc = dset_dict.get(\"metrics\", {}).get(\"val_acc\", [])\n    val_swa = dset_dict.get(\"metrics\", {}).get(\"val_swa\", [])\n    val_cwa = dset_dict.get(\"metrics\", {}).get(\"val_cwa\", [])\n    val_aca = dset_dict.get(\"aca\", {}).get(\"val\", [])\n\n    if val_acc:\n        print(\"best validation accuracy:\", fmt(max(val_acc)))\n    if val_swa:\n        print(\"best validation shape-weighted accuracy:\", fmt(max(val_swa)))\n    if val_cwa:\n        print(\"best validation color-weighted accuracy:\", fmt(max(val_cwa)))\n    if val_aca:\n        print(\"best validation ACA:\", fmt(max(val_aca)))\n\n    # ---------------- test metrics ----------------\n    test_preds = dset_dict.get(\"predictions\", [])\n    test_gts = dset_dict.get(\"ground_truth\", [])\n    if test_preds and test_gts:\n        test_accuracy = sum(int(p == g) for p, g in zip(test_preds, test_gts)) / len(\n            test_preds\n        )\n        print(\"test accuracy:\", fmt(test_accuracy))\n    test_aca = dset_dict.get(\"aca\", {}).get(\"test\", None)\n    if test_aca is not None:\n        print(\"test ACA:\", fmt(test_aca))\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------------------------------- locate and load\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------------------------------------------------------------------------- extract and print\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---------- training metrics\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"final training loss: {final_train_loss:.6f}\")\n\n    # ---------- validation metrics\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"best validation loss: {best_val_loss:.6f}\")\n\n    val_swa_list = data.get(\"metrics\", {}).get(\"val\", [])\n    if val_swa_list:\n        best_val_swa = max(val_swa_list)\n        print(f\"best validation shape-weighted accuracy: {best_val_swa:.6f}\")\n\n    val_aca_list = data.get(\"ACA\", {}).get(\"val\", [])\n    if val_aca_list:\n        best_val_aca = max(val_aca_list)\n        print(f\"best validation augmentation-consistency accuracy: {best_val_aca:.6f}\")\n\n    # ---------- test metrics\n    test_swa = data.get(\"metrics\", {}).get(\"test_SWA\")\n    if test_swa is not None:\n        print(f\"test shape-weighted accuracy: {test_swa:.6f}\")\n\n    test_cwa = data.get(\"metrics\", {}).get(\"test_CWA\")\n    if test_cwa is not None:\n        print(f\"test color-weighted accuracy: {test_cwa:.6f}\")\n\n    test_aca = data.get(\"ACA\", {}).get(\"test\")\n    if test_aca is not None:\n        print(f\"test augmentation-consistency accuracy: {test_aca:.6f}\")\n", "import os\nimport numpy as np\n\n\n# ----------------------------- helpers -------------------------------\ndef accuracy(pred, y):\n    \"\"\"Plain classification accuracy.\"\"\"\n    return (np.array(pred) == np.array(y)).mean()\n\n\n# --------------------------- load metrics ----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# --------------------------- parse & print ---------------------------\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\n{ds_name}\")  # dataset header\n\n    # 1. losses\n    train_losses = ds_dict[\"losses\"][\"train\"]\n    val_losses = ds_dict[\"losses\"][\"val\"]\n    if train_losses:\n        print(f\"final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        print(f\"final validation loss: {val_losses[-1]:.6f}\")\n\n    # 2. validation accuracy (best across epochs)\n    val_acc_hist = ds_dict[\"metrics\"][\"val\"]\n    if val_acc_hist:\n        print(f\"best validation accuracy: {max(val_acc_hist):.6f}\")\n\n    # 3. validation SWA / CWA / ACA (final entries)\n    for name in [\"swa\", \"cwa\", \"aca\"]:\n        series = ds_dict[name][\"val\"]\n        if series:\n            print(f\"final validation {name.upper()}: {series[-1]:.6f}\")\n\n    # 4. test-set metrics\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        print(f\"test accuracy: {accuracy(preds, gts):.6f}\")\n\n    for name in [\"swa\", \"cwa\", \"aca\"]:\n        val = ds_dict[name].get(\"test\", None)\n        if val is not None:\n            print(f\"test {name.upper()}: {val:.6f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions to select best values\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_accuracy(values):\n    return max(values) if values else None\n\n\n# Mapping from metric key to (pretty name, selector)\nseries_handlers = {\n    (\"losses\", \"pretrain\"): (\"best pretraining loss\", best_loss),\n    (\"losses\", \"train\"): (\"best training loss\", best_loss),\n    (\"losses\", \"val\"): (\"best validation loss\", best_loss),\n    (\"metrics\", \"val_acc\"): (\"best validation accuracy\", best_accuracy),\n    (\"metrics\", \"val_aca\"): (\"best validation ACA\", best_accuracy),\n}\n\n# -----------------------------------------------------------------------------#\n# Iterate and print\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle series metrics (stored as lists)\n    for (section, key), (pretty_name, selector) in series_handlers.items():\n        values = data.get(section, {}).get(key, [])\n        if values:\n            print(f\"{pretty_name}: {selector(values):.4f}\")\n\n    # Handle single-valued test metrics\n    test_metrics = data.get(\"test\", {})\n    single_metric_names = {\n        \"loss\": \"test loss\",\n        \"acc\": \"test accuracy\",\n        \"swa\": \"test shape-weighted accuracy\",\n        \"cwa\": \"test color-weighted accuracy\",\n        \"aca\": \"test ACA\",\n    }\n    for k, pretty_name in single_metric_names.items():\n        if k in test_metrics:\n            print(f\"{pretty_name}: {test_metrics[k]:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions to select best values\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_accuracy(values):\n    return max(values) if values else None\n\n\n# Mapping from metric key to (pretty name, selector)\nseries_handlers = {\n    (\"losses\", \"pretrain\"): (\"best pretraining loss\", best_loss),\n    (\"losses\", \"train\"): (\"best training loss\", best_loss),\n    (\"losses\", \"val\"): (\"best validation loss\", best_loss),\n    (\"metrics\", \"val_acc\"): (\"best validation accuracy\", best_accuracy),\n    (\"metrics\", \"val_aca\"): (\"best validation ACA\", best_accuracy),\n}\n\n# -----------------------------------------------------------------------------#\n# Iterate and print\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle series metrics (stored as lists)\n    for (section, key), (pretty_name, selector) in series_handlers.items():\n        values = data.get(section, {}).get(key, [])\n        if values:\n            print(f\"{pretty_name}: {selector(values):.4f}\")\n\n    # Handle single-valued test metrics\n    test_metrics = data.get(\"test\", {})\n    single_metric_names = {\n        \"loss\": \"test loss\",\n        \"acc\": \"test accuracy\",\n        \"swa\": \"test shape-weighted accuracy\",\n        \"cwa\": \"test color-weighted accuracy\",\n        \"aca\": \"test ACA\",\n    }\n    for k, pretty_name in single_metric_names.items():\n        if k in test_metrics:\n            print(f\"{pretty_name}: {test_metrics[k]:.4f}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Helper functions to select best values\ndef best_loss(values):\n    return min(values) if values else None\n\n\ndef best_accuracy(values):\n    return max(values) if values else None\n\n\n# Mapping from metric key to (pretty name, selector)\nseries_handlers = {\n    (\"losses\", \"pretrain\"): (\"best pretraining loss\", best_loss),\n    (\"losses\", \"train\"): (\"best training loss\", best_loss),\n    (\"losses\", \"val\"): (\"best validation loss\", best_loss),\n    (\"metrics\", \"val_acc\"): (\"best validation accuracy\", best_accuracy),\n    (\"metrics\", \"val_aca\"): (\"best validation ACA\", best_accuracy),\n}\n\n# -----------------------------------------------------------------------------#\n# Iterate and print\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle series metrics (stored as lists)\n    for (section, key), (pretty_name, selector) in series_handlers.items():\n        values = data.get(section, {}).get(key, [])\n        if values:\n            print(f\"{pretty_name}: {selector(values):.4f}\")\n\n    # Handle single-valued test metrics\n    test_metrics = data.get(\"test\", {})\n    single_metric_names = {\n        \"loss\": \"test loss\",\n        \"acc\": \"test accuracy\",\n        \"swa\": \"test shape-weighted accuracy\",\n        \"cwa\": \"test color-weighted accuracy\",\n        \"aca\": \"test ACA\",\n    }\n    for k, pretty_name in single_metric_names.items():\n        if k in test_metrics:\n            print(f\"{pretty_name}: {test_metrics[k]:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'best number of layers: 2', '\\n', 'final training loss:\n0.0050', '\\n', 'final validation loss: 0.0043', '\\n', 'best validation SCWA:\n0.9996', '\\n', 'test SCWA: 0.6354', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final pretraining loss: 0.915672', '\\n', 'final fine-tuning\ntraining loss: 0.033298', '\\n', 'final validation loss: 0.031041', '\\n', 'final\nvalidation shape-weighted accuracy: 0.988432', '\\n', 'final validation\nadversarial consistency accuracy: 0.910950', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'final training loss: 0.0002', '\\n', 'best validation loss:\n0.0009', '\\n', 'best validation accuracy: 0.9998', '\\n', 'best validation ACA:\n0.7676', '\\n', 'test accuracy: 0.6997', '\\n', 'test ACA: 0.5955', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final train loss: 0.5631', '\\n', 'final\nvalidation loss: 0.6015', '\\n', 'best validation loss: 0.6015', '\\n', 'best\nvalidation shape-weighted accuracy: 0.7376', '\\n', 'final validation shape-\nweighted accuracy: 0.7376', '\\n', 'best validation augmented consistency\naccuracy: 0.6817', '\\n', 'final validation augmented consistency accuracy:\n0.6817', '\\n', 'test shape-weighted accuracy: 0.7598', '\\n', 'test color-\nweighted accuracy: 0.6714', '\\n', 'test augmented consistency accuracy: 0.7100',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Minimum pretraining loss:      6.2364', '\\n',\n'Minimum training loss:         0.0011', '\\n', 'Minimum validation loss:\n0.0018', '\\n', 'Maximum validation accuracy:   0.9996', '\\n', 'Maximum\nvalidation ACA:        0.8969', '\\n', 'Test loss:                     3.2956',\n'\\n', 'Test accuracy:                 0.7000', '\\n', 'Test shape-weighted\naccuracy:  0.7000', '\\n', 'Test color-weighted accuracy:  0.6354', '\\n', 'Test\nACA:                      0.6524', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best pretraining loss: 0.6796', '\\n', 'best\ntraining loss: 0.0000', '\\n', 'best validation loss: 0.0003', '\\n', 'best\nvalidation accuracy: 1.0000', '\\n', 'best validation ACA: 0.9003', '\\n', 'test\nloss: 4.1363', '\\n', 'test accuracy: 0.7001', '\\n', 'test shape-weighted\naccuracy: 0.7001', '\\n', 'test color-weighted accuracy: 0.6355', '\\n', 'test\nACA: 0.6522', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final train loss:', ' ', '0.0377', '\\n', 'best\nvalidation loss:', ' ', '0.0333', '\\n', 'best validation accuracy:', ' ',\n'0.9926', '\\n', 'best validation shape-weighted accuracy:', ' ', '0.9919', '\\n',\n'best validation color-weighted accuracy:', ' ', '0.9930', '\\n', 'best\nvalidation ACA:', ' ', '0.7826', '\\n', 'test accuracy:', ' ', '0.6966', '\\n',\n'test ACA:', ' ', '0.6231', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.042795', '\\n', 'best\nvalidation loss: 0.038202', '\\n', 'best validation shape-weighted accuracy:\n0.991861', '\\n', 'best validation augmentation-consistency accuracy: 0.910333',\n'\\n', 'test shape-weighted accuracy: 0.696131', '\\n', 'test color-weighted\naccuracy: 0.633276', '\\n', 'test augmentation-consistency accuracy: 0.657700',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'final training loss: 395000.616400', '\\n', 'final\nvalidation loss: 0.084717', '\\n', 'best validation accuracy: 0.998600', '\\n',\n'final validation SWA: 0.996919', '\\n', 'final validation CWA: 0.996889', '\\n',\n'final validation ACA: 0.850933', '\\n', 'test accuracy: 0.699100', '\\n', 'test\nSWA: 0.698971', '\\n', 'test CWA: 0.634673', '\\n', 'test ACA: 0.643867', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best pretraining loss: 0.6796', '\\n', 'best\ntraining loss: 0.0000', '\\n', 'best validation loss: 0.0003', '\\n', 'best\nvalidation accuracy: 1.0000', '\\n', 'best validation ACA: 0.9003', '\\n', 'test\nloss: 4.1363', '\\n', 'test accuracy: 0.7001', '\\n', 'test shape-weighted\naccuracy: 0.7001', '\\n', 'test color-weighted accuracy: 0.6355', '\\n', 'test\nACA: 0.6522', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best pretraining loss: 0.6796', '\\n', 'best\ntraining loss: 0.0000', '\\n', 'best validation loss: 0.0003', '\\n', 'best\nvalidation accuracy: 1.0000', '\\n', 'best validation ACA: 0.9003', '\\n', 'test\nloss: 4.1363', '\\n', 'test accuracy: 0.7001', '\\n', 'test shape-weighted\naccuracy: 0.7001', '\\n', 'test color-weighted accuracy: 0.6355', '\\n', 'test\nACA: 0.6522', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best pretraining loss: 0.6796', '\\n', 'best\ntraining loss: 0.0000', '\\n', 'best validation loss: 0.0003', '\\n', 'best\nvalidation accuracy: 1.0000', '\\n', 'best validation ACA: 0.9003', '\\n', 'test\nloss: 4.1363', '\\n', 'test accuracy: 0.7001', '\\n', 'test shape-weighted\naccuracy: 0.7001', '\\n', 'test color-weighted accuracy: 0.6355', '\\n', 'test\nACA: 0.6522', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
