<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 9], [0, 8], [0, 10], [0, 11]], "layout": [[0.17647058823529413, 0.0], [0.29411764705882354, 0.0], [0.4117647058823529, 0.0], [0.5294117647058824, 0.0], [0.6470588235294118, 0.0], [0.7647058823529411, 0.0], [0.8823529411764706, 0.0], [1.0, 0.0], [0.0, 1.0], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0]], "plan": ["Hyperparam tuning name: num_epochs. We keep the original data-loading, model,\nloss and metric code, but wrap training in a function that accepts a\nmaximum\u2010epoch budget.   A sweep over max_epochs \u2208 {10, 20, 30, 40, 50} is\nexecuted; within each run we apply early-stopping (patience = 3) on validation\nloss.   After every epoch we log train / validation loss and the complexity-\nweighted accuracy; the best-validation snapshot is retained, its predictions\ncollected, and all run-level information is appended to experiment_data.\nFinally the whole dictionary is stored in working/experiment_data.npy and a\nloss-curve figure for the best run is saved.", "Hyperparam tuning name: learning_rate. The solution trains a fresh MeanEncoder\nmodel for each candidate learning-rate (1e-4, 5e-4, 1e-3, 2e-3), recording\nlosses and complexity-weighted accuracy per epoch. Results are stored in a\nhierarchically organized experiment_data dictionary keyed by the hyper-parameter\nvalue, then saved to experiment_data.npy. This lets you later identify the best\nlearning-rate without rerunning experiments.", "Hyperparam tuning name: embedding_dim. We loop over embedding dimensions 32, 64,\n128 and 256, train the same Mean-Encoder model for 5 epochs each, record per-\nepoch losses and the validation CWA-2D metric, plus final predictions/ground-\ntruth, and store everything in a single experiment_data dictionary under the\nhyper-parameter key \"embedding_dim\".  All arrays are saved to the mandatory file\nexperiment_data.npy.  Only the embedding size changes; all other settings remain\nfixed.", "Hyperparam tuning name: batch_size. Below is a concise plan: we wrap the\noriginal training loop in a function, call it for different batch-sizes (32, 64,\n128, 256), collect losses / metrics in a nested experiment_data dictionary keyed\nby the tuning type \u201cbatch_size\u201d and finally save everything to\nexperiment_data.npy together with separate loss-curve pngs for each sweep value.", "Hyperparam tuning name: weight_decay. We will loop over the five candidate\nvalues of Adam\u2019s weight-decay (0, 1e-5, 5e-5, 1e-4, 5e-4), re-initialising model\nand optimiser each time while keeping every other setting fixed.   For every\ndecay value we record per-epoch training/validation losses and validation\nCWA-2D, store final predictions, and finally save everything into the\nhierarchical experiment_data dict under the top key \"weight_decay\". A single\nloss-curve figure comparing all settings is also produced.", "Hyperparam tuning name: classifier_hidden_dim. We iterate over hidden widths 64,\n128, 256 and 512, build a fresh MeanEncoder whose classifier uses the\ncorresponding hidden dimension, train for five epochs just like the baseline,\nevaluate CWA-2D on the dev set, and record losses, metrics, predictions and\nground-truth.   Each run is stored in\nexperiment_data['classifier_hidden_dim'][f'SPR_BENCH_hd{h}'] and a loss-curve\nPNG is written for every width. Finally the whole experiment_data dictionary is\nsaved to experiment_data.npy.", "Hyperparam tuning name: dropout_rate. We extend the baseline by making the mean-\npool encoder accept a dropout rate and inserting `nn.Dropout(p)` both on the\npooled sentence vector and inside the MLP classifier. A small loop trains the\nmodel for each probability in {0.0, 0.1, 0.2, 0.3, 0.5}, logging losses and\nCWA-2D at every epoch, storing the results in\n`experiment_data['dropout'][f'p={p}']`, saving everything to\n`experiment_data.npy`, and writing one loss-curve PNG per setting. This lets us\nlater pick the `p` that yields the best validation score without over-\nregularising.", "Hyperparam tuning name: gradient_clip_norm. We reload and train a fresh model\nfive times, each time applying a different global gradient-norm clipping value\n(off, 0.5, 1, 2, 5).  Before every optimiser step we perform\n`torch.nn.utils.clip_grad_norm_` when clipping is enabled.  For every setting we\nlog per-epoch train/validation loss and the complexity-weighted accuracy,\ntogether with final predictions and ground-truth; all results are stored in a\nsingle `experiment_data.npy` file under the top-level key `gradient_clip_norm`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)  # B,D\n        return self.classifier(mean)\n\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"num_epochs_sweep\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"losses\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"config_epochs\": [],  # list of tried max_epochs\n            \"best_val_metric\": [],  # best CWA-2D per config\n        }\n    }\n}\n\n\n# --------------- training function -----------\ndef train_model(max_epochs=20, patience=3, emb_dim=64, lr=1e-3, tol=1e-4):\n    model = MeanEncoder(len(vocab), emb_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    best_val_loss, best_state, best_metrics, best_epoch = float(\"inf\"), None, None, 0\n    train_losses, val_losses = [], []\n    for epoch in range(1, max_epochs + 1):\n        # -------- train ----------\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            loss = criterion(model(x, lens), y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_loader.dataset)\n        # -------- val ------------\n        model.eval()\n        vloss, preds, gtruth, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                vloss += loss.item() * x.size(0)\n                preds.extend(out.argmax(1).cpu().tolist())\n                gtruth.extend(y.cpu().tolist())\n                seqs.extend(raw)\n        vloss /= len(dev_loader.dataset)\n        cwa = complexity_weighted_accuracy(\n            seqs, [id2label[i] for i in gtruth], [id2label[i] for i in preds]\n        )\n        train_losses.append(train_loss)\n        val_losses.append(vloss)\n        print(\n            f\"[max_epochs={max_epochs}] Epoch {epoch}: val_loss={vloss:.4f} CWA-2D={cwa:.4f}\"\n        )\n        # early stopping bookkeeping\n        if vloss < best_val_loss - tol:\n            best_val_loss, best_state, best_metrics, best_epoch = (\n                vloss,\n                model.state_dict(),\n                (train_losses[:], val_losses[:], preds[:], gtruth[:], cwa),\n                epoch,\n            )\n            wait = 0\n        else:\n            wait += 1\n        if wait >= patience:\n            break\n    # restore best\n    model.load_state_dict(best_state)\n    return best_epoch, best_metrics\n\n\n# --------------- sweep over num_epochs -------\nepoch_grid = [10, 20, 30, 40, 50]\nbest_run_idx, best_cwa = None, -1.0\nfor idx, max_ep in enumerate(epoch_grid):\n    wait = 0  # reset global var for each run\n    best_epoch, bm = train_model(max_epochs=max_ep)\n    tr_losses, v_losses, preds, gtruth, cwa = bm\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"config_epochs\"].append(max_ep)\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"best_val_metric\"].append(cwa)\n    if cwa > best_cwa:\n        best_cwa, best_run_idx = cwa, idx\n        # keep curves & predictions of best run\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"] = tr_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"] = v_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"predictions\"] = preds\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"ground_truth\"] = gtruth\n\nprint(\n    f\"Best run: grid index {best_run_idx}, max_epochs={epoch_grid[best_run_idx]}, best CWA-2D={best_cwa:.4f}\"\n)\n\n# --------------- save experiment data --------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot (best run) -------------\nplt.figure()\nplt.plot(\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\"\n)\nplt.plot(experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Best run loss curves\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ---------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------- load SPR -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    for split in [\"train.csv\", \"dev.csv\", \"test.csv\"]:\n        dset[split.split(\".\")[0]] = _load(split)\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------- vocab ----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# ---------- labels ---------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\n\n\n# ---------- dataset class --------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds, dev_ds = SPRTorchDataset(spr[\"train\"]), SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------- model ----------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)\n        mask = (x != 0).unsqueeze(-1)\n        summed = (emb * mask).sum(1)\n        lens = mask.sum(1).clamp(min=1)\n        mean = summed / lens\n        return self.classifier(mean)\n\n\n# ---------- hyper-parameter sweep -------------\nlearning_rates = [1e-4, 5e-4, 1e-3, 2e-3]\nnum_epochs = 5\n\nexperiment_data = {\"learning_rate\": {}}\n\nfor lr in learning_rates:\n    print(f\"\\n=== Training with lr={lr} ===\")\n    model = MeanEncoder(len(vocab), 64, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n\n    exp_key = f\"{lr:.0e}\" if lr < 1e-3 else f\"{lr}\"\n    experiment_data[\"learning_rate\"][exp_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        running_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            out = model(x, lens)\n            loss = criterion(out, y)\n            loss.backward()\n            optimiser.step()\n            running_loss += loss.item() * x.size(0)\n        train_loss = running_loss / len(train_ds)\n        experiment_data[\"learning_rate\"][exp_key][\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validate ----\n        model.eval()\n        val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(y.cpu().tolist())\n                all_seq.extend(raw)\n        val_loss /= len(dev_ds)\n        cwa2d = complexity_weighted_accuracy(\n            all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n        )\n\n        experiment_data[\"learning_rate\"][exp_key][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"learning_rate\"][exp_key][\"metrics\"][\"val\"].append(cwa2d)\n        print(f\"Epoch {epoch}: val_loss={val_loss:.4f} | CWA-2D={cwa2d:.4f}\")\n\n    # store final preds/gt\n    experiment_data[\"learning_rate\"][exp_key][\"predictions\"] = all_pred\n    experiment_data[\"learning_rate\"][exp_key][\"ground_truth\"] = all_true\n\n    # plot loss curve\n    plt.figure()\n    plt.plot(\n        experiment_data[\"learning_rate\"][exp_key][\"losses\"][\"train\"], label=\"train\"\n    )\n    plt.plot(experiment_data[\"learning_rate\"][exp_key][\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"Loss (lr={lr})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"loss_lr_{exp_key}.png\"))\n    plt.close()\n\n# ---------- save experiment data -------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nFinished. Data & plots saved to ./working/\")\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# -------------- experiment dict --------------\nexperiment_data = {\n    \"embedding_dim\": {\"SPR_BENCH\": {}}  # hyper-parameter we tune  # dataset name\n}\n\n# -------------- working dir ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------- device -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------- load SPR ---------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# -------------- vocab build ------------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# -------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# -------------- Dataset class ----------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# -------------- model ------------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\n# -------------- training hyper-param sweep ----\nembed_dims = [32, 64, 128, 256]\nnum_epochs = 5\n\nfor emb_dim in embed_dims:\n    print(f\"\\n=== Training with embedding_dim={emb_dim} ===\")\n    model = MeanEncoder(len(vocab), emb_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    losses_train, losses_val, metrics_val = [], [], []\n    final_preds, final_truth = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        # ---- Training ----\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            out = model(x, lens)\n            loss = criterion(out, y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_ds)\n        losses_train.append(train_loss)\n\n        # ---- Validation ----\n        model.eval()\n        val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(y.cpu().tolist())\n                all_seq.extend(raw)\n        val_loss /= len(dev_ds)\n        losses_val.append(val_loss)\n\n        cwa2d = complexity_weighted_accuracy(\n            all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n        )\n        metrics_val.append(cwa2d)\n        print(\n            f\"Epoch {epoch}/{num_epochs}: train_loss={train_loss:.4f} | \"\n            f\"val_loss={val_loss:.4f} | CWA-2D={cwa2d:.4f}\"\n        )\n\n    # save last epoch predictions/ground_truth\n    final_preds, final_truth = all_pred, all_true\n\n    # store results\n    experiment_data[\"embedding_dim\"][\"SPR_BENCH\"][f\"dim_{emb_dim}\"] = {\n        \"metrics\": {\"train\": [], \"val\": metrics_val},\n        \"losses\": {\"train\": losses_train, \"val\": losses_val},\n        \"predictions\": final_preds,\n        \"ground_truth\": final_truth,\n    }\n\n    # free gpu\n    del model, optimiser, criterion\n    torch.cuda.empty_cache()\n\n# -------------- save data --------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nFinished. Results saved in ./working/experiment_data.npy\")\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ---------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- load SPR -------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = HFDD()\n    d[\"train\"], d[\"dev\"], d[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return d\n\n\ndef count_shape_variety(sequence: str):\n    return len({tok[0] for tok in sequence.strip().split() if tok})\n\n\ndef count_color_variety(sequence: str):\n    return len({tok[1] for tok in sequence.strip().split() if len(tok) > 1})\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(sum(w), 1)\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab ----------------\nvocab = {\"<pad>\": 0, \"<unk>\": 1}\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        vocab.setdefault(tok, len(vocab))\nprint(\"Vocab size:\", len(vocab))\n\n# ---------- label maps -----------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\n\n\n# ---------- dataset --------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, dsplit):\n        self.seqs = dsplit[\"sequence\"]\n        self.labels = [label2id[l] for l in dsplit[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode(self, seq):\n        return [vocab.get(tok, 1) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lens = [len(s) for s in seqs]\n    mlen = max(lens)\n    pad_arr = np.full((len(seqs), mlen), 0, np.int64)\n    for i, s in enumerate(seqs):\n        pad_arr[i, : len(s)] = s\n    return (torch.tensor(pad_arr), torch.tensor(lens), torch.tensor(labels)), raw\n\n\ntrain_ds, dev_ds = SPRTorchDataset(spr[\"train\"]), SPRTorchDataset(spr[\"dev\"])\n\n\n# ---------- model ----------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_sz, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n        self.clf = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lens):\n        e = self.emb(x)\n        mask = (x != 0).unsqueeze(-1)\n        mean = (e * mask).sum(1) / mask.sum(1).clamp(min=1)\n        return self.clf(mean)\n\n\n# ---------- experiment store -----\nexperiment_data = {\"batch_size\": {\"SPR_BENCH\": {}}}  # each bs will populate here\n\n\n# ---------- training routine -----\ndef run_experiment(batch_size, num_epochs=5, emb_dim=64, lr=1e-3):\n    print(f\"\\n=== Running batch_size={batch_size} ===\")\n    tr_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate\n    )\n    dv_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n    model = MeanEncoder(len(vocab), emb_dim, num_classes).to(device)\n    crit = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=lr)\n\n    losses_tr, losses_val, metrics_val = [], [], []\n    best_pred, best_true = None, None\n\n    for ep in range(1, num_epochs + 1):\n        model.train()\n        tot = 0\n        for (x, lens, y), _ in tr_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optim.zero_grad()\n            out = model(x, lens)\n            loss = crit(out, y)\n            loss.backward()\n            optim.step()\n            tot += loss.item() * x.size(0)\n        tr_loss = tot / len(train_ds)\n        losses_tr.append(tr_loss)\n\n        # ------ validation -----\n        model.eval()\n        tot = 0\n        preds, truth, seqs = [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dv_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = crit(out, y)\n                tot += loss.item() * x.size(0)\n                pr = out.argmax(1).cpu().tolist()\n                preds.extend(pr)\n                truth.extend(y.cpu().tolist())\n                seqs.extend(raw)\n        vl_loss = tot / len(dev_ds)\n        losses_val.append(vl_loss)\n        cwa = complexity_weighted_accuracy(\n            seqs, [id2label[i] for i in truth], [id2label[i] for i in preds]\n        )\n        metrics_val.append(cwa)\n        print(\n            f\"Epoch {ep}: train_loss={tr_loss:.4f}  val_loss={vl_loss:.4f}  CWA={cwa:.4f}\"\n        )\n        best_pred, best_true = (\n            preds,\n            truth,\n        )  # overwrite; after last epoch they correspond to final model\n\n    # ------ store ---------------\n    experiment_data[\"batch_size\"][\"SPR_BENCH\"][batch_size] = {\n        \"metrics\": {\"train\": [], \"val\": metrics_val},\n        \"losses\": {\"train\": losses_tr, \"val\": losses_val},\n        \"predictions\": best_pred,\n        \"ground_truth\": best_true,\n    }\n\n    # ------ plot -----------------\n    plt.figure()\n    plt.plot(losses_tr, label=\"train\")\n    plt.plot(losses_val, label=\"val\")\n    plt.title(f\"Loss (bs={batch_size})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_loss_bs{batch_size}.png\"))\n    plt.close()\n\n\n# ---------- sweep ---------------\nfor bs in [32, 64, 128, 256]:\n    run_experiment(bs)\n\n# ---------- save ---------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nAll runs finished. Results saved in ./working/\")\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict, load_dataset\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ----------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lens = [len(s) for s in seqs]\n    maxlen = max(lens)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lens), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model definition -------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\n# --------------- hyper-parameter grid ---------\nweight_decays = [0.0, 1e-5, 5e-5, 1e-4, 5e-4]\nnum_epochs = 5\n\n# --------------- experiment data dict ---------\nexperiment_data = {\n    \"weight_decay\": {\n        str(wd): {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for wd in weight_decays\n    }\n}\n\n# --------------- training loop ---------------\ncriterion = nn.CrossEntropyLoss()\nval_loss_curves = {}  # for plotting\n\nfor wd in weight_decays:\n    key = str(wd)\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    model = MeanEncoder(len(vocab), 64, num_classes).to(device)\n    optimiser = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            out = model(x, lens)\n            loss = criterion(out, y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_ds)\n        experiment_data[\"weight_decay\"][key][\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validate ----\n        model.eval()\n        val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(y.cpu().tolist())\n                all_seq.extend(raw)\n        val_loss /= len(dev_ds)\n        cwa2d = complexity_weighted_accuracy(\n            all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n        )\n        experiment_data[\"weight_decay\"][key][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"weight_decay\"][key][\"metrics\"][\"val\"].append(cwa2d)\n        print(f\"Epoch {epoch}: val_loss={val_loss:.4f} | CWA-2D={cwa2d:.4f}\")\n\n    # store last epoch predictions/ground truth\n    experiment_data[\"weight_decay\"][key][\"predictions\"] = all_pred\n    experiment_data[\"weight_decay\"][key][\"ground_truth\"] = all_true\n    val_loss_curves[key] = experiment_data[\"weight_decay\"][key][\"losses\"][\"val\"]\n\n# --------------- save experiment data ---------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot val loss curves ---------\nplt.figure()\nfor wd_key, losses in val_loss_curves.items():\n    plt.plot(losses, label=f\"wd={wd_key}\")\nplt.title(\"Validation Loss vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\n# --------------- experiment data -------------\nexperiment_data = {\"classifier_hidden_dim\": {}}\n\n# --------------- hyperparameter tuning -------\nhidden_dims = [64, 128, 256, 512]\nnum_epochs = 5\nfor hdim in hidden_dims:\n    print(f\"\\n=== Training with classifier_hidden_dim = {hdim} ===\")\n    run_key = f\"SPR_BENCH_hd{hdim}\"\n    experiment_data[\"classifier_hidden_dim\"][run_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    model = MeanEncoder(len(vocab), 64, hdim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, num_epochs + 1):\n        # -------- training ---------\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            out = model(x, lens)\n            loss = criterion(out, y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_ds)\n        experiment_data[\"classifier_hidden_dim\"][run_key][\"losses\"][\"train\"].append(\n            train_loss\n        )\n\n        # -------- validation --------\n        model.eval()\n        val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(y.cpu().tolist())\n                all_seq.extend(raw)\n        val_loss /= len(dev_ds)\n        cwa2d = complexity_weighted_accuracy(\n            all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n        )\n        experiment_data[\"classifier_hidden_dim\"][run_key][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        experiment_data[\"classifier_hidden_dim\"][run_key][\"metrics\"][\"val\"].append(\n            cwa2d\n        )\n        print(f\"Epoch {epoch}: val_loss = {val_loss:.4f} | CWA-2D = {cwa2d:.4f}\")\n\n    # store preds/gt of the last epoch\n    experiment_data[\"classifier_hidden_dim\"][run_key][\"predictions\"] = all_pred\n    experiment_data[\"classifier_hidden_dim\"][run_key][\"ground_truth\"] = all_true\n\n    # -------- plot losses ----------\n    plt.figure()\n    plt.plot(\n        experiment_data[\"classifier_hidden_dim\"][run_key][\"losses\"][\"train\"],\n        label=\"train\",\n    )\n    plt.plot(\n        experiment_data[\"classifier_hidden_dim\"][run_key][\"losses\"][\"val\"], label=\"val\"\n    )\n    plt.title(f\"Loss curves (hidden_dim={hdim})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"{run_key}_loss_curve.png\"))\n    plt.close()\n\n# --------------- save data -------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Finished. All data & plots saved in ./working/\")\n", "import os, pathlib, numpy as np, torch, matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path):\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\n    os.environ.get(\"SPR_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\n\n\n# --------------- helper metrics --------------\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lens = [len(s) for s in seqs]\n    maxlen = max(lens)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lens), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass, dropout_rate=0.0):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.pre_dropout = nn.Dropout(dropout_rate)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, nclass),\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        mean = self.pre_dropout(mean)\n        return self.classifier(mean)\n\n\n# --------------- hyperparameter tuning --------\ndropout_values = [0.0, 0.1, 0.2, 0.3, 0.5]\nnum_epochs = 5\n\nexperiment_data = {\"dropout\": {}}\n\ncriterion = nn.CrossEntropyLoss()\n\nfor p in dropout_values:\n    print(f\"\\n=== Training with dropout p={p} ===\")\n    model = MeanEncoder(len(vocab), 64, num_classes, dropout_rate=p).to(device)\n    optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for epoch in range(1, num_epochs + 1):\n        # ---- train ----\n        model.train()\n        running_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            out = model(x, lens)\n            loss = criterion(out, y)\n            loss.backward()\n            optimiser.step()\n            running_loss += loss.item() * x.size(0)\n        train_loss = running_loss / len(train_ds)\n        exp_rec[\"losses\"][\"train\"].append(train_loss)\n\n        # ---- validation ----\n        model.eval()\n        val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(y.cpu().tolist())\n                all_seq.extend(raw)\n        val_loss /= len(dev_ds)\n        exp_rec[\"losses\"][\"val\"].append(val_loss)\n        cwa2d = complexity_weighted_accuracy(\n            all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n        )\n        exp_rec[\"metrics\"][\"val\"].append(cwa2d)\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | CWA-2D={cwa2d:.4f}\"\n        )\n\n    # store predictions of final epoch\n    exp_rec[\"predictions\"] = all_pred\n    exp_rec[\"ground_truth\"] = all_true\n    experiment_data[\"dropout\"][f\"p={p}\"] = exp_rec\n\n    # ---- plot losses ----\n    plt.figure()\n    plt.plot(exp_rec[\"losses\"][\"train\"], label=\"train\")\n    plt.plot(exp_rec[\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"Loss curves (dropout p={p})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_loss_curve_p{p}.png\"))\n    plt.close()\n\n# --------------- save all experiment data ----\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nFinished. Data & plots saved in ./working/\")\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            vocab.setdefault(tok, len(vocab))\n    return vocab\n\n\nvocab = build_vocab(spr)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\n# -------- hyperparameter configurations -------\nclip_values = [None, 0.5, 1.0, 2.0, 5.0]  # None == off\n\n# --------------- experiment data -------------\nexperiment_data = {\"gradient_clip_norm\": {\"SPR_BENCH\": {}}}\n\n# --------------- training loop ---------------\nnum_epochs = 5\nfor clip_val in clip_values:\n    tag = f\"clip_{clip_val}\" if clip_val is not None else \"clip_off\"\n    print(f\"\\n=== Training with gradient clip = {clip_val} ===\")\n    model = MeanEncoder(len(vocab), 64, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    exp_entry = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(1, num_epochs + 1):\n        # ------ train ------\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            out = model(x, lens)\n            loss = criterion(out, y)\n            loss.backward()\n            if clip_val is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_ds)\n        exp_entry[\"losses\"][\"train\"].append(train_loss)\n\n        # ------ validation ------\n        model.eval()\n        val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(y.cpu().tolist())\n                all_seq.extend(raw)\n        val_loss /= len(dev_ds)\n        cwa2d = complexity_weighted_accuracy(\n            all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n        )\n        exp_entry[\"losses\"][\"val\"].append(val_loss)\n        exp_entry[\"metrics\"][\"val\"].append(cwa2d)\n        print(f\"Epoch {epoch}: val_loss={val_loss:.4f} | CWA-2D={cwa2d:.4f}\")\n\n    # Store last predictions / truths\n    exp_entry[\"predictions\"] = all_pred\n    exp_entry[\"ground_truth\"] = all_true\n    experiment_data[\"gradient_clip_norm\"][\"SPR_BENCH\"][tag] = exp_entry\n\n    # optional: plot loss curves\n    plt.figure()\n    plt.plot(exp_entry[\"losses\"][\"train\"], label=\"train\")\n    plt.plot(exp_entry[\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"Loss curves ({tag})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_loss_curve_{tag}.png\"))\n    plt.close()\n\n# --------------- save experiment data ---------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Finished. Data & plots saved in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)  # B,D\n        return self.classifier(mean)\n\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"num_epochs_sweep\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"losses\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"config_epochs\": [],  # list of tried max_epochs\n            \"best_val_metric\": [],  # best CWA-2D per config\n        }\n    }\n}\n\n\n# --------------- training function -----------\ndef train_model(max_epochs=20, patience=3, emb_dim=64, lr=1e-3, tol=1e-4):\n    model = MeanEncoder(len(vocab), emb_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    best_val_loss, best_state, best_metrics, best_epoch = float(\"inf\"), None, None, 0\n    train_losses, val_losses = [], []\n    for epoch in range(1, max_epochs + 1):\n        # -------- train ----------\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            loss = criterion(model(x, lens), y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_loader.dataset)\n        # -------- val ------------\n        model.eval()\n        vloss, preds, gtruth, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                vloss += loss.item() * x.size(0)\n                preds.extend(out.argmax(1).cpu().tolist())\n                gtruth.extend(y.cpu().tolist())\n                seqs.extend(raw)\n        vloss /= len(dev_loader.dataset)\n        cwa = complexity_weighted_accuracy(\n            seqs, [id2label[i] for i in gtruth], [id2label[i] for i in preds]\n        )\n        train_losses.append(train_loss)\n        val_losses.append(vloss)\n        print(\n            f\"[max_epochs={max_epochs}] Epoch {epoch}: val_loss={vloss:.4f} CWA-2D={cwa:.4f}\"\n        )\n        # early stopping bookkeeping\n        if vloss < best_val_loss - tol:\n            best_val_loss, best_state, best_metrics, best_epoch = (\n                vloss,\n                model.state_dict(),\n                (train_losses[:], val_losses[:], preds[:], gtruth[:], cwa),\n                epoch,\n            )\n            wait = 0\n        else:\n            wait += 1\n        if wait >= patience:\n            break\n    # restore best\n    model.load_state_dict(best_state)\n    return best_epoch, best_metrics\n\n\n# --------------- sweep over num_epochs -------\nepoch_grid = [10, 20, 30, 40, 50]\nbest_run_idx, best_cwa = None, -1.0\nfor idx, max_ep in enumerate(epoch_grid):\n    wait = 0  # reset global var for each run\n    best_epoch, bm = train_model(max_epochs=max_ep)\n    tr_losses, v_losses, preds, gtruth, cwa = bm\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"config_epochs\"].append(max_ep)\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"best_val_metric\"].append(cwa)\n    if cwa > best_cwa:\n        best_cwa, best_run_idx = cwa, idx\n        # keep curves & predictions of best run\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"] = tr_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"] = v_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"predictions\"] = preds\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"ground_truth\"] = gtruth\n\nprint(\n    f\"Best run: grid index {best_run_idx}, max_epochs={epoch_grid[best_run_idx]}, best CWA-2D={best_cwa:.4f}\"\n)\n\n# --------------- save experiment data --------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot (best run) -------------\nplt.figure()\nplt.plot(\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\"\n)\nplt.plot(experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Best run loss curves\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)  # B,D\n        return self.classifier(mean)\n\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"num_epochs_sweep\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"losses\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"config_epochs\": [],  # list of tried max_epochs\n            \"best_val_metric\": [],  # best CWA-2D per config\n        }\n    }\n}\n\n\n# --------------- training function -----------\ndef train_model(max_epochs=20, patience=3, emb_dim=64, lr=1e-3, tol=1e-4):\n    model = MeanEncoder(len(vocab), emb_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    best_val_loss, best_state, best_metrics, best_epoch = float(\"inf\"), None, None, 0\n    train_losses, val_losses = [], []\n    for epoch in range(1, max_epochs + 1):\n        # -------- train ----------\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            loss = criterion(model(x, lens), y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_loader.dataset)\n        # -------- val ------------\n        model.eval()\n        vloss, preds, gtruth, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                vloss += loss.item() * x.size(0)\n                preds.extend(out.argmax(1).cpu().tolist())\n                gtruth.extend(y.cpu().tolist())\n                seqs.extend(raw)\n        vloss /= len(dev_loader.dataset)\n        cwa = complexity_weighted_accuracy(\n            seqs, [id2label[i] for i in gtruth], [id2label[i] for i in preds]\n        )\n        train_losses.append(train_loss)\n        val_losses.append(vloss)\n        print(\n            f\"[max_epochs={max_epochs}] Epoch {epoch}: val_loss={vloss:.4f} CWA-2D={cwa:.4f}\"\n        )\n        # early stopping bookkeeping\n        if vloss < best_val_loss - tol:\n            best_val_loss, best_state, best_metrics, best_epoch = (\n                vloss,\n                model.state_dict(),\n                (train_losses[:], val_losses[:], preds[:], gtruth[:], cwa),\n                epoch,\n            )\n            wait = 0\n        else:\n            wait += 1\n        if wait >= patience:\n            break\n    # restore best\n    model.load_state_dict(best_state)\n    return best_epoch, best_metrics\n\n\n# --------------- sweep over num_epochs -------\nepoch_grid = [10, 20, 30, 40, 50]\nbest_run_idx, best_cwa = None, -1.0\nfor idx, max_ep in enumerate(epoch_grid):\n    wait = 0  # reset global var for each run\n    best_epoch, bm = train_model(max_epochs=max_ep)\n    tr_losses, v_losses, preds, gtruth, cwa = bm\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"config_epochs\"].append(max_ep)\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"best_val_metric\"].append(cwa)\n    if cwa > best_cwa:\n        best_cwa, best_run_idx = cwa, idx\n        # keep curves & predictions of best run\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"] = tr_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"] = v_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"predictions\"] = preds\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"ground_truth\"] = gtruth\n\nprint(\n    f\"Best run: grid index {best_run_idx}, max_epochs={epoch_grid[best_run_idx]}, best CWA-2D={best_cwa:.4f}\"\n)\n\n# --------------- save experiment data --------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot (best run) -------------\nplt.figure()\nplt.plot(\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\"\n)\nplt.plot(experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Best run loss curves\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"], dset[\"dev\"], dset[\"test\"] = (\n        _load(\"train.csv\"),\n        _load(\"dev.csv\"),\n        _load(\"test.csv\"),\n    )\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        mean = (emb * mask).sum(1) / mask.sum(1).clamp(min=1)  # B,D\n        return self.classifier(mean)\n\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"num_epochs_sweep\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"losses\": {\"train\": [], \"val\": []},  # epoch-wise for best run\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"config_epochs\": [],  # list of tried max_epochs\n            \"best_val_metric\": [],  # best CWA-2D per config\n        }\n    }\n}\n\n\n# --------------- training function -----------\ndef train_model(max_epochs=20, patience=3, emb_dim=64, lr=1e-3, tol=1e-4):\n    model = MeanEncoder(len(vocab), emb_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    best_val_loss, best_state, best_metrics, best_epoch = float(\"inf\"), None, None, 0\n    train_losses, val_losses = [], []\n    for epoch in range(1, max_epochs + 1):\n        # -------- train ----------\n        model.train()\n        epoch_loss = 0.0\n        for (x, lens, y), _ in train_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            optimiser.zero_grad()\n            loss = criterion(model(x, lens), y)\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.item() * x.size(0)\n        train_loss = epoch_loss / len(train_loader.dataset)\n        # -------- val ------------\n        model.eval()\n        vloss, preds, gtruth, seqs = 0.0, [], [], []\n        with torch.no_grad():\n            for (x, lens, y), raw in dev_loader:\n                x, lens, y = x.to(device), lens.to(device), y.to(device)\n                out = model(x, lens)\n                loss = criterion(out, y)\n                vloss += loss.item() * x.size(0)\n                preds.extend(out.argmax(1).cpu().tolist())\n                gtruth.extend(y.cpu().tolist())\n                seqs.extend(raw)\n        vloss /= len(dev_loader.dataset)\n        cwa = complexity_weighted_accuracy(\n            seqs, [id2label[i] for i in gtruth], [id2label[i] for i in preds]\n        )\n        train_losses.append(train_loss)\n        val_losses.append(vloss)\n        print(\n            f\"[max_epochs={max_epochs}] Epoch {epoch}: val_loss={vloss:.4f} CWA-2D={cwa:.4f}\"\n        )\n        # early stopping bookkeeping\n        if vloss < best_val_loss - tol:\n            best_val_loss, best_state, best_metrics, best_epoch = (\n                vloss,\n                model.state_dict(),\n                (train_losses[:], val_losses[:], preds[:], gtruth[:], cwa),\n                epoch,\n            )\n            wait = 0\n        else:\n            wait += 1\n        if wait >= patience:\n            break\n    # restore best\n    model.load_state_dict(best_state)\n    return best_epoch, best_metrics\n\n\n# --------------- sweep over num_epochs -------\nepoch_grid = [10, 20, 30, 40, 50]\nbest_run_idx, best_cwa = None, -1.0\nfor idx, max_ep in enumerate(epoch_grid):\n    wait = 0  # reset global var for each run\n    best_epoch, bm = train_model(max_epochs=max_ep)\n    tr_losses, v_losses, preds, gtruth, cwa = bm\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"config_epochs\"].append(max_ep)\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"best_val_metric\"].append(cwa)\n    if cwa > best_cwa:\n        best_cwa, best_run_idx = cwa, idx\n        # keep curves & predictions of best run\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"] = tr_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"] = v_losses\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"predictions\"] = preds\n        experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"ground_truth\"] = gtruth\n\nprint(\n    f\"Best run: grid index {best_run_idx}, max_epochs={epoch_grid[best_run_idx]}, best CWA-2D={best_cwa:.4f}\"\n)\n\n# --------------- save experiment data --------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot (best run) -------------\nplt.figure()\nplt.plot(\n    experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\"\n)\nplt.plot(experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Best run loss curves\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 572988.44\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 813574.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 984416.65\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', ' ', '2',\n'\\n', '[max_epochs=10] Epoch 1: val_loss=0.3182 CWA-2D=0.8896', '\\n',\n'[max_epochs=10] Epoch 2: val_loss=0.2117 CWA-2D=0.9348', '\\n', '[max_epochs=10]\nEpoch 3: val_loss=0.1899 CWA-2D=0.9442', '\\n', '[max_epochs=10] Epoch 4:\nval_loss=0.1805 CWA-2D=0.9448', '\\n', '[max_epochs=10] Epoch 5: val_loss=0.1760\nCWA-2D=0.9457', '\\n', '[max_epochs=10] Epoch 6: val_loss=0.1723 CWA-2D=0.9454',\n'\\n', '[max_epochs=10] Epoch 7: val_loss=0.1742 CWA-2D=0.9456', '\\n',\n'[max_epochs=10] Epoch 8: val_loss=0.1684 CWA-2D=0.9461', '\\n', '[max_epochs=10]\nEpoch 9: val_loss=0.1670 CWA-2D=0.9459', '\\n', '[max_epochs=10] Epoch 10:\nval_loss=0.1693 CWA-2D=0.9461', '\\n', '[max_epochs=20] Epoch 1: val_loss=0.3180\nCWA-2D=0.8735', '\\n', '[max_epochs=20] Epoch 2: val_loss=0.2181 CWA-2D=0.9324',\n'\\n', '[max_epochs=20] Epoch 3: val_loss=0.1908 CWA-2D=0.9448', '\\n',\n'[max_epochs=20] Epoch 4: val_loss=0.1798 CWA-2D=0.9440', '\\n', '[max_epochs=20]\nEpoch 5: val_loss=0.1770 CWA-2D=0.9450', '\\n', '[max_epochs=20] Epoch 6:\nval_loss=0.1758 CWA-2D=0.9446', '\\n', '[max_epochs=20] Epoch 7: val_loss=0.1693\nCWA-2D=0.9450', '\\n', '[max_epochs=20] Epoch 8: val_loss=0.1686 CWA-2D=0.9461',\n'\\n', '[max_epochs=20] Epoch 9: val_loss=0.1684 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 10: val_loss=0.1676 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 11: val_loss=0.1655 CWA-2D=0.9463', '\\n',\n'[max_epochs=20] Epoch 12: val_loss=0.1677 CWA-2D=0.9456', '\\n',\n'[max_epochs=20] Epoch 13: val_loss=0.1654 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 14: val_loss=0.1678 CWA-2D=0.9459', '\\n',\n'[max_epochs=20] Epoch 15: val_loss=0.1652 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 16: val_loss=0.1652 CWA-2D=0.9460', '\\n',\n'[max_epochs=20] Epoch 17: val_loss=0.1664 CWA-2D=0.9458', '\\n',\n'[max_epochs=20] Epoch 18: val_loss=0.1676 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 1: val_loss=0.3711 CWA-2D=0.8426', '\\n', '[max_epochs=30]\nEpoch 2: val_loss=0.2194 CWA-2D=0.9325', '\\n', '[max_epochs=30] Epoch 3:\nval_loss=0.1914 CWA-2D=0.9422', '\\n', '[max_epochs=30] Epoch 4: val_loss=0.1816\nCWA-2D=0.9450', '\\n', '[max_epochs=30] Epoch 5: val_loss=0.1762 CWA-2D=0.9450',\n'\\n', '[max_epochs=30] Epoch 6: val_loss=0.1726 CWA-2D=0.9452', '\\n',\n'[max_epochs=30] Epoch 7: val_loss=0.1719 CWA-2D=0.9461', '\\n', '[max_epochs=30]\nEpoch 8: val_loss=0.1712 CWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 9:\nval_loss=0.1696 CWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 10: val_loss=0.1681\nCWA-2D=0.9459', '\\n', '[max_epochs=30] Epoch 11: val_loss=0.1662 CWA-2D=0.9461',\n'\\n', '[max_epochs=30] Epoch 12: val_loss=0.1697 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 13: val_loss=0.1662 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 14: val_loss=0.1677 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 1: val_loss=0.3157 CWA-2D=0.8781', '\\n', '[max_epochs=40]\nEpoch 2: val_loss=0.2125 CWA-2D=0.9358', '\\n', '[max_epochs=40] Epoch 3:\nval_loss=0.1880 CWA-2D=0.9450', '\\n', '[max_epochs=40] Epoch 4: val_loss=0.1800\nCWA-2D=0.9445', '\\n', '[max_epochs=40] Epoch 5: val_loss=0.1757 CWA-2D=0.9457',\n'\\n', '[max_epochs=40] Epoch 6: val_loss=0.1736 CWA-2D=0.9457', '\\n',\n'[max_epochs=40] Epoch 7: val_loss=0.1709 CWA-2D=0.9455', '\\n', '[max_epochs=40]\nEpoch 8: val_loss=0.1724 CWA-2D=0.9445', '\\n', '[max_epochs=40] Epoch 9:\nval_loss=0.1696 CWA-2D=0.9459', '\\n', '[max_epochs=40] Epoch 10: val_loss=0.1672\nCWA-2D=0.9461', '\\n', '[max_epochs=40] Epoch 11: val_loss=0.1665 CWA-2D=0.9461',\n'\\n', '[max_epochs=40] Epoch 12: val_loss=0.1696 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 13: val_loss=0.1658 CWA-2D=0.9458', '\\n',\n'[max_epochs=40] Epoch 14: val_loss=0.1675 CWA-2D=0.9460', '\\n',\n'[max_epochs=40] Epoch 15: val_loss=0.1657 CWA-2D=0.9459', '\\n',\n'[max_epochs=40] Epoch 16: val_loss=0.1696 CWA-2D=0.9456', '\\n',\n'[max_epochs=40] Epoch 17: val_loss=0.1667 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 18: val_loss=0.1672 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 1: val_loss=0.3764 CWA-2D=0.8365', '\\n', '[max_epochs=50]\nEpoch 2: val_loss=0.2204 CWA-2D=0.9314', '\\n', '[max_epochs=50] Epoch 3:\nval_loss=0.1973 CWA-2D=0.9359', '\\n', '[max_epochs=50] Epoch 4: val_loss=0.1816\nCWA-2D=0.9436', '\\n', '[max_epochs=50] Epoch 5: val_loss=0.1786 CWA-2D=0.9448',\n'\\n', '[max_epochs=50] Epoch 6: val_loss=0.1724 CWA-2D=0.9450', '\\n',\n'[max_epochs=50] Epoch 7: val_loss=0.1718 CWA-2D=0.9457', '\\n', '[max_epochs=50]\nEpoch 8: val_loss=0.1697 CWA-2D=0.9454', '\\n', '[max_epochs=50] Epoch 9:\nval_loss=0.1699 CWA-2D=0.9452', '\\n', '[max_epochs=50] Epoch 10: val_loss=0.1678\nCWA-2D=0.9459', '\\n', '[max_epochs=50] Epoch 11: val_loss=0.1675 CWA-2D=0.9459',\n'\\n', '[max_epochs=50] Epoch 12: val_loss=0.1664 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 13: val_loss=0.1674 CWA-2D=0.9459', '\\n',\n'[max_epochs=50] Epoch 14: val_loss=0.1664 CWA-2D=0.9458', '\\n',\n'[max_epochs=50] Epoch 15: val_loss=0.1684 CWA-2D=0.9461', '\\n', 'Best run: grid\nindex 1, max_epochs=20, best CWA-2D=0.9461', '\\n', 'Finished. Data & plot saved\nin ./working/', '\\n', 'Execution time: 2 minutes seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 20000 examples [00:00,\n339683.02 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 451252.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 469151.03\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', '\\n=== Training with\nlr=0.0001 ===', '\\n', 'Epoch 1: val_loss=0.5886 | CWA-2D=0.7568', '\\n', 'Epoch\n2: val_loss=0.5247 | CWA-2D=0.7572', '\\n', 'Epoch 3: val_loss=0.4898 |\nCWA-2D=0.7683', '\\n', 'Epoch 4: val_loss=0.4629 | CWA-2D=0.7832', '\\n', 'Epoch\n5: val_loss=0.4334 | CWA-2D=0.7992', '\\n', '\\n=== Training with lr=0.0005 ===',\n'\\n', 'Epoch 1: val_loss=0.4665 | CWA-2D=0.7765', '\\n', 'Epoch 2:\nval_loss=0.3374 | CWA-2D=0.8605', '\\n', 'Epoch 3: val_loss=0.2513 |\nCWA-2D=0.9242', '\\n', 'Epoch 4: val_loss=0.2181 | CWA-2D=0.9298', '\\n', 'Epoch\n5: val_loss=0.1994 | CWA-2D=0.9435', '\\n', '\\n=== Training with lr=0.001 ===',\n'\\n', 'Epoch 1: val_loss=0.3055 | CWA-2D=0.8679', '\\n', 'Epoch 2:\nval_loss=0.2211 | CWA-2D=0.9281', '\\n', 'Epoch 3: val_loss=0.1960 |\nCWA-2D=0.9412', '\\n', 'Epoch 4: val_loss=0.1841 | CWA-2D=0.9433', '\\n', 'Epoch\n5: val_loss=0.1787 | CWA-2D=0.9443', '\\n', '\\n=== Training with lr=0.002 ===',\n'\\n', 'Epoch 1: val_loss=0.2188 | CWA-2D=0.9316', '\\n', 'Epoch 2:\nval_loss=0.1841 | CWA-2D=0.9435', '\\n', 'Epoch 3: val_loss=0.1748 |\nCWA-2D=0.9450', '\\n', 'Epoch 4: val_loss=0.1697 | CWA-2D=0.9450', '\\n', 'Epoch\n5: val_loss=0.1673 | CWA-2D=0.9461', '\\n', '\\nFinished. Data & plots saved to\n./working/', '\\n', 'Execution time: 8 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 273788.57\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 438093.17\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 511993.75\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', ' ', '2',\n'\\n', '\\n=== Training with embedding_dim=32 ===', '\\n', 'Epoch 1/5:\ntrain_loss=0.5338 | val_loss=0.4302 | CWA-2D=0.7977', '\\n', 'Epoch 2/5:\ntrain_loss=0.3394 | val_loss=0.2663 | CWA-2D=0.9159', '\\n', 'Epoch 3/5:\ntrain_loss=0.2413 | val_loss=0.2212 | CWA-2D=0.9294', '\\n', 'Epoch 4/5:\ntrain_loss=0.2110 | val_loss=0.2024 | CWA-2D=0.9389', '\\n', 'Epoch 5/5:\ntrain_loss=0.1957 | val_loss=0.1924 | CWA-2D=0.9418', '\\n', '\\n=== Training with\nembedding_dim=64 ===', '\\n', 'Epoch 1/5: train_loss=0.4415 | val_loss=0.2837 |\nCWA-2D=0.9063', '\\n', 'Epoch 2/5: train_loss=0.2479 | val_loss=0.2220 |\nCWA-2D=0.9333', '\\n', 'Epoch 3/5: train_loss=0.2072 | val_loss=0.1963 |\nCWA-2D=0.9417', '\\n', 'Epoch 4/5: train_loss=0.1886 | val_loss=0.1847 |\nCWA-2D=0.9450', '\\n', 'Epoch 5/5: train_loss=0.1802 | val_loss=0.1781 |\nCWA-2D=0.9455', '\\n', '\\n=== Training with embedding_dim=128 ===', '\\n', 'Epoch\n1/5: train_loss=0.4187 | val_loss=0.2665 | CWA-2D=0.9167', '\\n', 'Epoch 2/5:\ntrain_loss=0.2230 | val_loss=0.2056 | CWA-2D=0.9374', '\\n', 'Epoch 3/5:\ntrain_loss=0.1871 | val_loss=0.1804 | CWA-2D=0.9450', '\\n', 'Epoch 4/5:\ntrain_loss=0.1777 | val_loss=0.1748 | CWA-2D=0.9459', '\\n', 'Epoch 5/5:\ntrain_loss=0.1715 | val_loss=0.1715 | CWA-2D=0.9459', '\\n', '\\n=== Training with\nembedding_dim=256 ===', '\\n', 'Epoch 1/5: train_loss=0.3593 | val_loss=0.2185 |\nCWA-2D=0.9312', '\\n', 'Epoch 2/5: train_loss=0.1960 | val_loss=0.1854 |\nCWA-2D=0.9421', '\\n', 'Epoch 3/5: train_loss=0.1780 | val_loss=0.1748 |\nCWA-2D=0.9450', '\\n', 'Epoch 4/5: train_loss=0.1721 | val_loss=0.1725 |\nCWA-2D=0.9456', '\\n', 'Epoch 5/5: train_loss=0.1702 | val_loss=0.1668 |\nCWA-2D=0.9461', '\\n', '\\nFinished. Results saved in\n./working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 474297.09\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 387342.91\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 508320.39\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', '\\n=== Running batch_size=32\n===', '\\n', 'Epoch 1: train_loss=0.3313  val_loss=0.2034  CWA=0.9371', '\\n',\n'Epoch 2: train_loss=0.1888  val_loss=0.1780  CWA=0.9455', '\\n', 'Epoch 3:\ntrain_loss=0.1759  val_loss=0.1729  CWA=0.9459', '\\n', 'Epoch 4:\ntrain_loss=0.1705  val_loss=0.1673  CWA=0.9461', '\\n', 'Epoch 5:\ntrain_loss=0.1684  val_loss=0.1697  CWA=0.9458', '\\n', '\\n=== Running\nbatch_size=64 ===', '\\n', 'Epoch 1: train_loss=0.4085  val_loss=0.2447\nCWA=0.9216', '\\n', 'Epoch 2: train_loss=0.2108  val_loss=0.1903  CWA=0.9447',\n'\\n', 'Epoch 3: train_loss=0.1849  val_loss=0.1791  CWA=0.9445', '\\n', 'Epoch 4:\ntrain_loss=0.1760  val_loss=0.1747  CWA=0.9455', '\\n', 'Epoch 5:\ntrain_loss=0.1732  val_loss=0.1703  CWA=0.9450', '\\n', '\\n=== Running\nbatch_size=128 ===', '\\n', 'Epoch 1: train_loss=0.4559  val_loss=0.3055\nCWA=0.8679', '\\n', 'Epoch 2: train_loss=0.2507  val_loss=0.2211  CWA=0.9281',\n'\\n', 'Epoch 3: train_loss=0.2042  val_loss=0.1960  CWA=0.9412', '\\n', 'Epoch 4:\ntrain_loss=0.1873  val_loss=0.1841  CWA=0.9433', '\\n', 'Epoch 5:\ntrain_loss=0.1796  val_loss=0.1787  CWA=0.9443', '\\n', '\\n=== Running\nbatch_size=256 ===', '\\n', 'Epoch 1: train_loss=0.5274  val_loss=0.4317\nCWA=0.8006', '\\n', 'Epoch 2: train_loss=0.3565  val_loss=0.2824  CWA=0.8943',\n'\\n', 'Epoch 3: train_loss=0.2485  val_loss=0.2224  CWA=0.9278', '\\n', 'Epoch 4:\ntrain_loss=0.2104  val_loss=0.1996  CWA=0.9380', '\\n', 'Epoch 5:\ntrain_loss=0.1932  val_loss=0.1898  CWA=0.9445', '\\n', '\\nAll runs finished.\nResults saved in ./working/', '\\n', 'Execution time: a minute seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', '\\n=== Training with weight_decay=0.0 ===', '\\n', 'Epoch 1:\nval_loss=0.3182 | CWA-2D=0.8896', '\\n', 'Epoch 2: val_loss=0.2117 |\nCWA-2D=0.9348', '\\n', 'Epoch 3: val_loss=0.1899 | CWA-2D=0.9442', '\\n', 'Epoch\n4: val_loss=0.1805 | CWA-2D=0.9448', '\\n', 'Epoch 5: val_loss=0.1760 |\nCWA-2D=0.9457', '\\n', '\\n=== Training with weight_decay=1e-05 ===', '\\n', 'Epoch\n1: val_loss=0.3513 | CWA-2D=0.8448', '\\n', 'Epoch 2: val_loss=0.2197 |\nCWA-2D=0.9330', '\\n', 'Epoch 3: val_loss=0.1922 | CWA-2D=0.9436', '\\n', 'Epoch\n4: val_loss=0.1860 | CWA-2D=0.9448', '\\n', 'Epoch 5: val_loss=0.1770 |\nCWA-2D=0.9450', '\\n', '\\n=== Training with weight_decay=5e-05 ===', '\\n', 'Epoch\n1: val_loss=0.3063 | CWA-2D=0.8677', '\\n', 'Epoch 2: val_loss=0.2222 |\nCWA-2D=0.9281', '\\n', 'Epoch 3: val_loss=0.1969 | CWA-2D=0.9410', '\\n', 'Epoch\n4: val_loss=0.1846 | CWA-2D=0.9443', '\\n', 'Epoch 5: val_loss=0.1796 |\nCWA-2D=0.9443', '\\n', '\\n=== Training with weight_decay=0.0001 ===', '\\n',\n'Epoch 1: val_loss=0.3227 | CWA-2D=0.8647', '\\n', 'Epoch 2: val_loss=0.2148 |\nCWA-2D=0.9353', '\\n', 'Epoch 3: val_loss=0.1913 | CWA-2D=0.9414', '\\n', 'Epoch\n4: val_loss=0.1821 | CWA-2D=0.9443', '\\n', 'Epoch 5: val_loss=0.1771 |\nCWA-2D=0.9450', '\\n', '\\n=== Training with weight_decay=0.0005 ===', '\\n',\n'Epoch 1: val_loss=0.3744 | CWA-2D=0.8441', '\\n', 'Epoch 2: val_loss=0.2296 |\nCWA-2D=0.9248', '\\n', 'Epoch 3: val_loss=0.1982 | CWA-2D=0.9382', '\\n', 'Epoch\n4: val_loss=0.1875 | CWA-2D=0.9437', '\\n', 'Epoch 5: val_loss=0.1833 |\nCWA-2D=0.9429', '\\n', 'Finished. Data & plot saved in ./working/', '\\n',\n'Execution time: 59 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', '\\n=== Training with classifier_hidden_dim = 64 ===', '\\n', 'Epoch\n1: val_loss = 0.3920 | CWA-2D = 0.8170', '\\n', 'Epoch 2: val_loss = 0.2474 |\nCWA-2D = 0.9282', '\\n', 'Epoch 3: val_loss = 0.2061 | CWA-2D = 0.9357', '\\n',\n'Epoch 4: val_loss = 0.1912 | CWA-2D = 0.9431', '\\n', 'Epoch 5: val_loss =\n0.1811 | CWA-2D = 0.9440', '\\n', '\\n=== Training with classifier_hidden_dim =\n128 ===', '\\n', 'Epoch 1: val_loss = 0.3199 | CWA-2D = 0.8889', '\\n', 'Epoch 2:\nval_loss = 0.2147 | CWA-2D = 0.9298', '\\n', 'Epoch 3: val_loss = 0.1906 | CWA-2D\n= 0.9427', '\\n', 'Epoch 4: val_loss = 0.1820 | CWA-2D = 0.9448', '\\n', 'Epoch 5:\nval_loss = 0.1763 | CWA-2D = 0.9442', '\\n', '\\n=== Training with\nclassifier_hidden_dim = 256 ===', '\\n', 'Epoch 1: val_loss = 0.3235 | CWA-2D =\n0.8713', '\\n', 'Epoch 2: val_loss = 0.2116 | CWA-2D = 0.9356', '\\n', 'Epoch 3:\nval_loss = 0.1916 | CWA-2D = 0.9406', '\\n', 'Epoch 4: val_loss = 0.1828 | CWA-2D\n= 0.9450', '\\n', 'Epoch 5: val_loss = 0.1758 | CWA-2D = 0.9453', '\\n', '\\n===\nTraining with classifier_hidden_dim = 512 ===', '\\n', 'Epoch 1: val_loss =\n0.2473 | CWA-2D = 0.9222', '\\n', 'Epoch 2: val_loss = 0.1932 | CWA-2D = 0.9417',\n'\\n', 'Epoch 3: val_loss = 0.1794 | CWA-2D = 0.9457', '\\n', 'Epoch 4: val_loss =\n0.1809 | CWA-2D = 0.9452', '\\n', 'Epoch 5: val_loss = 0.1722 | CWA-2D = 0.9452',\n'\\n', 'Finished. All data & plots saved in ./working/', '\\n', 'Execution time:\n10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', '\\n=== Training with dropout p=0.0 ===', '\\n', 'Epoch 1:\ntrain_loss=0.4630 | val_loss=0.3182 | CWA-2D=0.8896', '\\n', 'Epoch 2:\ntrain_loss=0.2520 | val_loss=0.2117 | CWA-2D=0.9348', '\\n', 'Epoch 3:\ntrain_loss=0.1996 | val_loss=0.1899 | CWA-2D=0.9442', '\\n', 'Epoch 4:\ntrain_loss=0.1839 | val_loss=0.1805 | CWA-2D=0.9448', '\\n', 'Epoch 5:\ntrain_loss=0.1765 | val_loss=0.1760 | CWA-2D=0.9457', '\\n', '\\n=== Training with\ndropout p=0.1 ===', '\\n', 'Epoch 1: train_loss=0.5056 | val_loss=0.3770 |\nCWA-2D=0.8290', '\\n', 'Epoch 2: train_loss=0.3114 | val_loss=0.2380 |\nCWA-2D=0.9191', '\\n', 'Epoch 3: train_loss=0.2373 | val_loss=0.2049 |\nCWA-2D=0.9343', '\\n', 'Epoch 4: train_loss=0.2162 | val_loss=0.2007 |\nCWA-2D=0.9355', '\\n', 'Epoch 5: train_loss=0.2039 | val_loss=0.1855 |\nCWA-2D=0.9410', '\\n', '\\n=== Training with dropout p=0.2 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5016 | val_loss=0.3553 | CWA-2D=0.8436', '\\n', 'Epoch 2:\ntrain_loss=0.3252 | val_loss=0.2603 | CWA-2D=0.9057', '\\n', 'Epoch 3:\ntrain_loss=0.2701 | val_loss=0.2219 | CWA-2D=0.9230', '\\n', 'Epoch 4:\ntrain_loss=0.2504 | val_loss=0.2135 | CWA-2D=0.9258', '\\n', 'Epoch 5:\ntrain_loss=0.2326 | val_loss=0.2027 | CWA-2D=0.9320', '\\n', '\\n=== Training with\ndropout p=0.3 ===', '\\n', 'Epoch 1: train_loss=0.5262 | val_loss=0.4036 |\nCWA-2D=0.8133', '\\n', 'Epoch 2: train_loss=0.3761 | val_loss=0.2672 |\nCWA-2D=0.9119', '\\n', 'Epoch 3: train_loss=0.2987 | val_loss=0.2434 |\nCWA-2D=0.9120', '\\n', 'Epoch 4: train_loss=0.2724 | val_loss=0.2228 |\nCWA-2D=0.9192', '\\n', 'Epoch 5: train_loss=0.2536 | val_loss=0.2101 |\nCWA-2D=0.9256', '\\n', '\\n=== Training with dropout p=0.5 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5892 | val_loss=0.4921 | CWA-2D=0.7805', '\\n', 'Epoch 2:\ntrain_loss=0.5071 | val_loss=0.4224 | CWA-2D=0.8223', '\\n', 'Epoch 3:\ntrain_loss=0.4396 | val_loss=0.3303 | CWA-2D=0.8822', '\\n', 'Epoch 4:\ntrain_loss=0.3853 | val_loss=0.2784 | CWA-2D=0.9004', '\\n', 'Epoch 5:\ntrain_loss=0.3436 | val_loss=0.2525 | CWA-2D=0.9089', '\\n', '\\nFinished. Data &\nplots saved in ./working/', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\n=== Training with gradient clip = None ===',\n'\\n', 'Epoch 1: val_loss=0.3182 | CWA-2D=0.8896', '\\n', 'Epoch 2:\nval_loss=0.2117 | CWA-2D=0.9348', '\\n', 'Epoch 3: val_loss=0.1899 |\nCWA-2D=0.9442', '\\n', 'Epoch 4: val_loss=0.1805 | CWA-2D=0.9448', '\\n', 'Epoch\n5: val_loss=0.1760 | CWA-2D=0.9457', '\\n', '\\n=== Training with gradient clip =\n0.5 ===', '\\n', 'Epoch 1: val_loss=0.3510 | CWA-2D=0.8452', '\\n', 'Epoch 2:\nval_loss=0.2197 | CWA-2D=0.9326', '\\n', 'Epoch 3: val_loss=0.1920 |\nCWA-2D=0.9435', '\\n', 'Epoch 4: val_loss=0.1857 | CWA-2D=0.9450', '\\n', 'Epoch\n5: val_loss=0.1766 | CWA-2D=0.9452', '\\n', '\\n=== Training with gradient clip =\n1.0 ===', '\\n', 'Epoch 1: val_loss=0.3055 | CWA-2D=0.8679', '\\n', 'Epoch 2:\nval_loss=0.2211 | CWA-2D=0.9281', '\\n', 'Epoch 3: val_loss=0.1960 |\nCWA-2D=0.9412', '\\n', 'Epoch 4: val_loss=0.1841 | CWA-2D=0.9433', '\\n', 'Epoch\n5: val_loss=0.1787 | CWA-2D=0.9443', '\\n', '\\n=== Training with gradient clip =\n2.0 ===', '\\n', 'Epoch 1: val_loss=0.3198 | CWA-2D=0.8666', '\\n', 'Epoch 2:\nval_loss=0.2136 | CWA-2D=0.9361', '\\n', 'Epoch 3: val_loss=0.1898 |\nCWA-2D=0.9420', '\\n', 'Epoch 4: val_loss=0.1809 | CWA-2D=0.9439', '\\n', 'Epoch\n5: val_loss=0.1760 | CWA-2D=0.9450', '\\n', '\\n=== Training with gradient clip =\n5.0 ===', '\\n', 'Epoch 1: val_loss=0.3633 | CWA-2D=0.8499', '\\n', 'Epoch 2:\nval_loss=0.2194 | CWA-2D=0.9283', '\\n', 'Epoch 3: val_loss=0.1907 |\nCWA-2D=0.9404', '\\n', 'Epoch 4: val_loss=0.1805 | CWA-2D=0.9439', '\\n', 'Epoch\n5: val_loss=0.1762 | CWA-2D=0.9439', '\\n', 'Finished. Data & plots saved in\n./working/', '\\n', 'Execution time: 53 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', '[max_epochs=10] Epoch 1: val_loss=0.3485 CWA-2D=0.8581', '\\n',\n'[max_epochs=10] Epoch 2: val_loss=0.2200 CWA-2D=0.9370', '\\n', '[max_epochs=10]\nEpoch 3: val_loss=0.1905 CWA-2D=0.9450', '\\n', '[max_epochs=10] Epoch 4:\nval_loss=0.1803 CWA-2D=0.9445', '\\n', '[max_epochs=10] Epoch 5: val_loss=0.1755\nCWA-2D=0.9453', '\\n', '[max_epochs=10] Epoch 6: val_loss=0.1737 CWA-2D=0.9452',\n'\\n', '[max_epochs=10] Epoch 7: val_loss=0.1706 CWA-2D=0.9461', '\\n',\n'[max_epochs=10] Epoch 8: val_loss=0.1699 CWA-2D=0.9461', '\\n', '[max_epochs=10]\nEpoch 9: val_loss=0.1685 CWA-2D=0.9461', '\\n', '[max_epochs=10] Epoch 10:\nval_loss=0.1679 CWA-2D=0.9461', '\\n', '[max_epochs=20] Epoch 1: val_loss=0.3665\nCWA-2D=0.8496', '\\n', '[max_epochs=20] Epoch 2: val_loss=0.2179 CWA-2D=0.9400',\n'\\n', '[max_epochs=20] Epoch 3: val_loss=0.1888 CWA-2D=0.9450', '\\n',\n'[max_epochs=20] Epoch 4: val_loss=0.1818 CWA-2D=0.9453', '\\n', '[max_epochs=20]\nEpoch 5: val_loss=0.1770 CWA-2D=0.9450', '\\n', '[max_epochs=20] Epoch 6:\nval_loss=0.1735 CWA-2D=0.9450', '\\n', '[max_epochs=20] Epoch 7: val_loss=0.1711\nCWA-2D=0.9452', '\\n', '[max_epochs=20] Epoch 8: val_loss=0.1696 CWA-2D=0.9461',\n'\\n', '[max_epochs=20] Epoch 9: val_loss=0.1686 CWA-2D=0.9457', '\\n',\n'[max_epochs=20] Epoch 10: val_loss=0.1670 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 11: val_loss=0.1681 CWA-2D=0.9460', '\\n',\n'[max_epochs=20] Epoch 12: val_loss=0.1678 CWA-2D=0.9459', '\\n',\n'[max_epochs=20] Epoch 13: val_loss=0.1658 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 14: val_loss=0.1666 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 15: val_loss=0.1665 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 16: val_loss=0.1660 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 1: val_loss=0.3369 CWA-2D=0.8561', '\\n', '[max_epochs=30]\nEpoch 2: val_loss=0.2108 CWA-2D=0.9432', '\\n', '[max_epochs=30] Epoch 3:\nval_loss=0.1881 CWA-2D=0.9439', '\\n', '[max_epochs=30] Epoch 4: val_loss=0.1806\nCWA-2D=0.9436', '\\n', '[max_epochs=30] Epoch 5: val_loss=0.1775 CWA-2D=0.9448',\n'\\n', '[max_epochs=30] Epoch 6: val_loss=0.1740 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 7: val_loss=0.1710 CWA-2D=0.9459', '\\n', '[max_epochs=30]\nEpoch 8: val_loss=0.1689 CWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 9:\nval_loss=0.1692 CWA-2D=0.9459', '\\n', '[max_epochs=30] Epoch 10: val_loss=0.1666\nCWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 11: val_loss=0.1689 CWA-2D=0.9461',\n'\\n', '[max_epochs=30] Epoch 12: val_loss=0.1677 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 13: val_loss=0.1663 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 14: val_loss=0.1675 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 15: val_loss=0.1684 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 16: val_loss=0.1656 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 17: val_loss=0.1666 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 18: val_loss=0.1665 CWA-2D=0.9458', '\\n',\n'[max_epochs=30] Epoch 19: val_loss=0.1667 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 1: val_loss=0.3723 CWA-2D=0.8497', '\\n', '[max_epochs=40]\nEpoch 2: val_loss=0.2255 CWA-2D=0.9299', '\\n', '[max_epochs=40] Epoch 3:\nval_loss=0.1939 CWA-2D=0.9426', '\\n', '[max_epochs=40] Epoch 4: val_loss=0.1841\nCWA-2D=0.9448', '\\n', '[max_epochs=40] Epoch 5: val_loss=0.1778 CWA-2D=0.9450',\n'\\n', '[max_epochs=40] Epoch 6: val_loss=0.1750 CWA-2D=0.9436', '\\n',\n'[max_epochs=40] Epoch 7: val_loss=0.1699 CWA-2D=0.9457', '\\n', '[max_epochs=40]\nEpoch 8: val_loss=0.1705 CWA-2D=0.9450', '\\n', '[max_epochs=40] Epoch 9:\nval_loss=0.1673 CWA-2D=0.9461', '\\n', '[max_epochs=40] Epoch 10: val_loss=0.1681\nCWA-2D=0.9457', '\\n', '[max_epochs=40] Epoch 11: val_loss=0.1680 CWA-2D=0.9459',\n'\\n', '[max_epochs=40] Epoch 12: val_loss=0.1667 CWA-2D=0.9459', '\\n',\n'[max_epochs=40] Epoch 13: val_loss=0.1671 CWA-2D=0.9459', '\\n',\n'[max_epochs=40] Epoch 14: val_loss=0.1658 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 15: val_loss=0.1654 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 16: val_loss=0.1644 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 17: val_loss=0.1683 CWA-2D=0.9454', '\\n',\n'[max_epochs=40] Epoch 18: val_loss=0.1685 CWA-2D=0.9453', '\\n',\n'[max_epochs=40] Epoch 19: val_loss=0.1656 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 1: val_loss=0.3331 CWA-2D=0.8719', '\\n', '[max_epochs=50]\nEpoch 2: val_loss=0.2198 CWA-2D=0.9267', '\\n', '[max_epochs=50] Epoch 3:\nval_loss=0.1867 CWA-2D=0.9450', '\\n', '[max_epochs=50] Epoch 4: val_loss=0.1776\nCWA-2D=0.9447', '\\n', '[max_epochs=50] Epoch 5: val_loss=0.1754 CWA-2D=0.9456',\n'\\n', '[max_epochs=50] Epoch 6: val_loss=0.1717 CWA-2D=0.9459', '\\n',\n'[max_epochs=50] Epoch 7: val_loss=0.1690 CWA-2D=0.9459', '\\n', '[max_epochs=50]\nEpoch 8: val_loss=0.1683 CWA-2D=0.9461', '\\n', '[max_epochs=50] Epoch 9:\nval_loss=0.1686 CWA-2D=0.9461', '\\n', '[max_epochs=50] Epoch 10: val_loss=0.1663\nCWA-2D=0.9459', '\\n', '[max_epochs=50] Epoch 11: val_loss=0.1669 CWA-2D=0.9461',\n'\\n', '[max_epochs=50] Epoch 12: val_loss=0.1675 CWA-2D=0.9460', '\\n',\n'[max_epochs=50] Epoch 13: val_loss=0.1664 CWA-2D=0.9461', '\\n', 'Best run: grid\nindex 0, max_epochs=10, best CWA-2D=0.9461', '\\n', 'Finished. Data & plot saved\nin ./working/', '\\n', 'Execution time: 2 minutes seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', '[max_epochs=10] Epoch 1: val_loss=0.3081 CWA-2D=0.8864', '\\n',\n'[max_epochs=10] Epoch 2: val_loss=0.2076 CWA-2D=0.9369', '\\n', '[max_epochs=10]\nEpoch 3: val_loss=0.1878 CWA-2D=0.9423', '\\n', '[max_epochs=10] Epoch 4:\nval_loss=0.1807 CWA-2D=0.9439', '\\n', '[max_epochs=10] Epoch 5: val_loss=0.1763\nCWA-2D=0.9448', '\\n', '[max_epochs=10] Epoch 6: val_loss=0.1740 CWA-2D=0.9450',\n'\\n', '[max_epochs=10] Epoch 7: val_loss=0.1731 CWA-2D=0.9457', '\\n',\n'[max_epochs=10] Epoch 8: val_loss=0.1726 CWA-2D=0.9457', '\\n', '[max_epochs=10]\nEpoch 9: val_loss=0.1694 CWA-2D=0.9461', '\\n', '[max_epochs=10] Epoch 10:\nval_loss=0.1680 CWA-2D=0.9461', '\\n', '[max_epochs=20] Epoch 1: val_loss=0.3163\nCWA-2D=0.8772', '\\n', '[max_epochs=20] Epoch 2: val_loss=0.2103 CWA-2D=0.9367',\n'\\n', '[max_epochs=20] Epoch 3: val_loss=0.1893 CWA-2D=0.9429', '\\n',\n'[max_epochs=20] Epoch 4: val_loss=0.1789 CWA-2D=0.9446', '\\n', '[max_epochs=20]\nEpoch 5: val_loss=0.1742 CWA-2D=0.9448', '\\n', '[max_epochs=20] Epoch 6:\nval_loss=0.1756 CWA-2D=0.9456', '\\n', '[max_epochs=20] Epoch 7: val_loss=0.1717\nCWA-2D=0.9459', '\\n', '[max_epochs=20] Epoch 8: val_loss=0.1690 CWA-2D=0.9461',\n'\\n', '[max_epochs=20] Epoch 9: val_loss=0.1678 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 10: val_loss=0.1706 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 11: val_loss=0.1680 CWA-2D=0.9458', '\\n',\n'[max_epochs=20] Epoch 12: val_loss=0.1652 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 13: val_loss=0.1668 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 14: val_loss=0.1702 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 15: val_loss=0.1662 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 1: val_loss=0.3566 CWA-2D=0.8476', '\\n', '[max_epochs=30]\nEpoch 2: val_loss=0.2175 CWA-2D=0.9358', '\\n', '[max_epochs=30] Epoch 3:\nval_loss=0.1893 CWA-2D=0.9446', '\\n', '[max_epochs=30] Epoch 4: val_loss=0.1828\nCWA-2D=0.9450', '\\n', '[max_epochs=30] Epoch 5: val_loss=0.1758 CWA-2D=0.9453',\n'\\n', '[max_epochs=30] Epoch 6: val_loss=0.1727 CWA-2D=0.9455', '\\n',\n'[max_epochs=30] Epoch 7: val_loss=0.1704 CWA-2D=0.9455', '\\n', '[max_epochs=30]\nEpoch 8: val_loss=0.1698 CWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 9:\nval_loss=0.1743 CWA-2D=0.9445', '\\n', '[max_epochs=30] Epoch 10: val_loss=0.1666\nCWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 11: val_loss=0.1663 CWA-2D=0.9461',\n'\\n', '[max_epochs=30] Epoch 12: val_loss=0.1655 CWA-2D=0.9460', '\\n',\n'[max_epochs=30] Epoch 13: val_loss=0.1640 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 14: val_loss=0.1656 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 15: val_loss=0.1645 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 16: val_loss=0.1638 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 17: val_loss=0.1670 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 18: val_loss=0.1651 CWA-2D=0.9459', '\\n',\n'[max_epochs=30] Epoch 19: val_loss=0.1652 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 1: val_loss=0.3214 CWA-2D=0.8741', '\\n', '[max_epochs=40]\nEpoch 2: val_loss=0.2147 CWA-2D=0.9373', '\\n', '[max_epochs=40] Epoch 3:\nval_loss=0.1923 CWA-2D=0.9422', '\\n', '[max_epochs=40] Epoch 4: val_loss=0.1836\nCWA-2D=0.9446', '\\n', '[max_epochs=40] Epoch 5: val_loss=0.1804 CWA-2D=0.9432',\n'\\n', '[max_epochs=40] Epoch 6: val_loss=0.1753 CWA-2D=0.9458', '\\n',\n'[max_epochs=40] Epoch 7: val_loss=0.1718 CWA-2D=0.9458', '\\n', '[max_epochs=40]\nEpoch 8: val_loss=0.1706 CWA-2D=0.9460', '\\n', '[max_epochs=40] Epoch 9:\nval_loss=0.1694 CWA-2D=0.9459', '\\n', '[max_epochs=40] Epoch 10: val_loss=0.1680\nCWA-2D=0.9461', '\\n', '[max_epochs=40] Epoch 11: val_loss=0.1696 CWA-2D=0.9459',\n'\\n', '[max_epochs=40] Epoch 12: val_loss=0.1685 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 13: val_loss=0.1694 CWA-2D=0.9460', '\\n',\n'[max_epochs=50] Epoch 1: val_loss=0.3146 CWA-2D=0.8860', '\\n', '[max_epochs=50]\nEpoch 2: val_loss=0.2160 CWA-2D=0.9352', '\\n', '[max_epochs=50] Epoch 3:\nval_loss=0.1955 CWA-2D=0.9411', '\\n', '[max_epochs=50] Epoch 4: val_loss=0.1847\nCWA-2D=0.9429', '\\n', '[max_epochs=50] Epoch 5: val_loss=0.1806 CWA-2D=0.9445',\n'\\n', '[max_epochs=50] Epoch 6: val_loss=0.1753 CWA-2D=0.9447', '\\n',\n'[max_epochs=50] Epoch 7: val_loss=0.1739 CWA-2D=0.9457', '\\n', '[max_epochs=50]\nEpoch 8: val_loss=0.1701 CWA-2D=0.9461', '\\n', '[max_epochs=50] Epoch 9:\nval_loss=0.1685 CWA-2D=0.9461', '\\n', '[max_epochs=50] Epoch 10: val_loss=0.1761\nCWA-2D=0.9454', '\\n', '[max_epochs=50] Epoch 11: val_loss=0.1681 CWA-2D=0.9460',\n'\\n', '[max_epochs=50] Epoch 12: val_loss=0.1663 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 13: val_loss=0.1651 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 14: val_loss=0.1657 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 15: val_loss=0.1664 CWA-2D=0.9459', '\\n',\n'[max_epochs=50] Epoch 16: val_loss=0.1679 CWA-2D=0.9461', '\\n', 'Best run: grid\nindex 0, max_epochs=10, best CWA-2D=0.9461', '\\n', 'Finished. Data & plot saved\nin ./working/', '\\n', 'Execution time: 23 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', '[max_epochs=10] Epoch 1: val_loss=0.3287 CWA-2D=0.8750', '\\n',\n'[max_epochs=10] Epoch 2: val_loss=0.2144 CWA-2D=0.9332', '\\n', '[max_epochs=10]\nEpoch 3: val_loss=0.1917 CWA-2D=0.9429', '\\n', '[max_epochs=10] Epoch 4:\nval_loss=0.1822 CWA-2D=0.9444', '\\n', '[max_epochs=10] Epoch 5: val_loss=0.1773\nCWA-2D=0.9448', '\\n', '[max_epochs=10] Epoch 6: val_loss=0.1737 CWA-2D=0.9450',\n'\\n', '[max_epochs=10] Epoch 7: val_loss=0.1716 CWA-2D=0.9452', '\\n',\n'[max_epochs=10] Epoch 8: val_loss=0.1709 CWA-2D=0.9453', '\\n', '[max_epochs=10]\nEpoch 9: val_loss=0.1692 CWA-2D=0.9461', '\\n', '[max_epochs=10] Epoch 10:\nval_loss=0.1685 CWA-2D=0.9459', '\\n', '[max_epochs=20] Epoch 1: val_loss=0.3539\nCWA-2D=0.8603', '\\n', '[max_epochs=20] Epoch 2: val_loss=0.2197 CWA-2D=0.9290',\n'\\n', '[max_epochs=20] Epoch 3: val_loss=0.1915 CWA-2D=0.9430', '\\n',\n'[max_epochs=20] Epoch 4: val_loss=0.1837 CWA-2D=0.9439', '\\n', '[max_epochs=20]\nEpoch 5: val_loss=0.1792 CWA-2D=0.9448', '\\n', '[max_epochs=20] Epoch 6:\nval_loss=0.1744 CWA-2D=0.9450', '\\n', '[max_epochs=20] Epoch 7: val_loss=0.1725\nCWA-2D=0.9457', '\\n', '[max_epochs=20] Epoch 8: val_loss=0.1715 CWA-2D=0.9457',\n'\\n', '[max_epochs=20] Epoch 9: val_loss=0.1686 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 10: val_loss=0.1704 CWA-2D=0.9459', '\\n',\n'[max_epochs=20] Epoch 11: val_loss=0.1694 CWA-2D=0.9458', '\\n',\n'[max_epochs=20] Epoch 12: val_loss=0.1674 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 13: val_loss=0.1665 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 14: val_loss=0.1661 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 15: val_loss=0.1675 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 16: val_loss=0.1682 CWA-2D=0.9461', '\\n',\n'[max_epochs=20] Epoch 17: val_loss=0.1669 CWA-2D=0.9456', '\\n',\n'[max_epochs=30] Epoch 1: val_loss=0.3142 CWA-2D=0.8693', '\\n', '[max_epochs=30]\nEpoch 2: val_loss=0.2190 CWA-2D=0.9303', '\\n', '[max_epochs=30] Epoch 3:\nval_loss=0.1932 CWA-2D=0.9421', '\\n', '[max_epochs=30] Epoch 4: val_loss=0.1826\nCWA-2D=0.9448', '\\n', '[max_epochs=30] Epoch 5: val_loss=0.1767 CWA-2D=0.9445',\n'\\n', '[max_epochs=30] Epoch 6: val_loss=0.1760 CWA-2D=0.9448', '\\n',\n'[max_epochs=30] Epoch 7: val_loss=0.1727 CWA-2D=0.9452', '\\n', '[max_epochs=30]\nEpoch 8: val_loss=0.1683 CWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 9:\nval_loss=0.1675 CWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 10: val_loss=0.1671\nCWA-2D=0.9461', '\\n', '[max_epochs=30] Epoch 11: val_loss=0.1657 CWA-2D=0.9461',\n'\\n', '[max_epochs=30] Epoch 12: val_loss=0.1680 CWA-2D=0.9460', '\\n',\n'[max_epochs=30] Epoch 13: val_loss=0.1658 CWA-2D=0.9461', '\\n',\n'[max_epochs=30] Epoch 14: val_loss=0.1673 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 1: val_loss=0.3307 CWA-2D=0.8699', '\\n', '[max_epochs=40]\nEpoch 2: val_loss=0.2171 CWA-2D=0.9334', '\\n', '[max_epochs=40] Epoch 3:\nval_loss=0.1909 CWA-2D=0.9422', '\\n', '[max_epochs=40] Epoch 4: val_loss=0.1817\nCWA-2D=0.9437', '\\n', '[max_epochs=40] Epoch 5: val_loss=0.1764 CWA-2D=0.9450',\n'\\n', '[max_epochs=40] Epoch 6: val_loss=0.1736 CWA-2D=0.9452', '\\n',\n'[max_epochs=40] Epoch 7: val_loss=0.1716 CWA-2D=0.9457', '\\n', '[max_epochs=40]\nEpoch 8: val_loss=0.1691 CWA-2D=0.9461', '\\n', '[max_epochs=40] Epoch 9:\nval_loss=0.1699 CWA-2D=0.9458', '\\n', '[max_epochs=40] Epoch 10: val_loss=0.1665\nCWA-2D=0.9461', '\\n', '[max_epochs=40] Epoch 11: val_loss=0.1680 CWA-2D=0.9460',\n'\\n', '[max_epochs=40] Epoch 12: val_loss=0.1668 CWA-2D=0.9461', '\\n',\n'[max_epochs=40] Epoch 13: val_loss=0.1684 CWA-2D=0.9456', '\\n',\n'[max_epochs=50] Epoch 1: val_loss=0.3255 CWA-2D=0.8677', '\\n', '[max_epochs=50]\nEpoch 2: val_loss=0.2097 CWA-2D=0.9350', '\\n', '[max_epochs=50] Epoch 3:\nval_loss=0.1869 CWA-2D=0.9435', '\\n', '[max_epochs=50] Epoch 4: val_loss=0.1807\nCWA-2D=0.9450', '\\n', '[max_epochs=50] Epoch 5: val_loss=0.1753 CWA-2D=0.9450',\n'\\n', '[max_epochs=50] Epoch 6: val_loss=0.1736 CWA-2D=0.9450', '\\n',\n'[max_epochs=50] Epoch 7: val_loss=0.1703 CWA-2D=0.9461', '\\n', '[max_epochs=50]\nEpoch 8: val_loss=0.1693 CWA-2D=0.9453', '\\n', '[max_epochs=50] Epoch 9:\nval_loss=0.1671 CWA-2D=0.9461', '\\n', '[max_epochs=50] Epoch 10: val_loss=0.1668\nCWA-2D=0.9458', '\\n', '[max_epochs=50] Epoch 11: val_loss=0.1703 CWA-2D=0.9461',\n'\\n', '[max_epochs=50] Epoch 12: val_loss=0.1678 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 13: val_loss=0.1653 CWA-2D=0.9459', '\\n',\n'[max_epochs=50] Epoch 14: val_loss=0.1664 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 15: val_loss=0.1663 CWA-2D=0.9461', '\\n',\n'[max_epochs=50] Epoch 16: val_loss=0.1660 CWA-2D=0.9461', '\\n', 'Best run: grid\nindex 1, max_epochs=20, best CWA-2D=0.9461', '\\n', 'Finished. Data & plot saved\nin ./working/', '\\n', 'Execution time: 23 seconds seconds (time limit is 30\nminutes).']", ""], "analysis": ["", "", "The training script executed successfully without any errors or bugs. The\ntraining and validation losses decreased across epochs for all embedding\ndimensions, and the complexity-weighted accuracy (CWA-2D) improved consistently,\nindicating proper learning. The results were saved correctly in the specified\nlocation. No issues were detected in the implementation or execution.", "", "", "", "", "The training script executed successfully without any errors or bugs. The output\nindicates that the training and validation processes were completed for\ndifferent gradient clipping values, and the results (loss and CWA-2D metrics)\nwere logged for each epoch. The data and plots were saved as intended. No issues\nwere observed.", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating how well the model is learning.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1627, "best_value": 0.1627}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, used to evaluate the model on unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1652, "best_value": 0.1652}]}, {"metric_name": "validation CWA-2D", "lower_is_better": false, "description": "The CWA-2D metric during validation, measuring model performance on the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9461, "best_value": 0.9461}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "learning_rate=1e-04", "final_value": 0.449577, "best_value": 0.449577}, {"dataset_name": "learning_rate=5e-04", "final_value": 0.207086, "best_value": 0.207086}, {"dataset_name": "learning_rate=0.001", "final_value": 0.17964, "best_value": 0.17964}, {"dataset_name": "learning_rate=0.002", "final_value": 0.16948, "best_value": 0.16948}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "learning_rate=1e-04", "final_value": 0.433426, "best_value": 0.433426}, {"dataset_name": "learning_rate=5e-04", "final_value": 0.199444, "best_value": 0.199444}, {"dataset_name": "learning_rate=0.001", "final_value": 0.178717, "best_value": 0.178717}, {"dataset_name": "learning_rate=0.002", "final_value": 0.167326, "best_value": 0.167326}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset, weighted by complexity.", "data": [{"dataset_name": "learning_rate=1e-04", "final_value": 0.799184, "best_value": 0.799184}, {"dataset_name": "learning_rate=5e-04", "final_value": 0.94353, "best_value": 0.94353}, {"dataset_name": "learning_rate=0.001", "final_value": 0.944274, "best_value": 0.944274}, {"dataset_name": "learning_rate=0.002", "final_value": 0.94612, "best_value": 0.94612}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1702, "best_value": 0.1702}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1668, "best_value": 0.1668}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "The accuracy metric weighted by complexity, calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9461, "best_value": 0.9461}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error on the training dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1684, "best_value": 0.1684}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation dataset. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1697, "best_value": 0.1697}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy on the validation dataset, weighted by complexity. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9445, "best_value": 0.9461}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 0.1765, "best_value": 0.1765}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.178903, "best_value": 0.178903}, {"dataset_name": "weight_decay=5e-05", "final_value": 0.180486, "best_value": 0.180486}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.178831, "best_value": 0.178831}, {"dataset_name": "weight_decay=0.0005", "final_value": 0.18459, "best_value": 0.18459}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 0.176044, "best_value": 0.176044}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.177036, "best_value": 0.177036}, {"dataset_name": "weight_decay=5e-05", "final_value": 0.179558, "best_value": 0.179558}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.177082, "best_value": 0.177082}, {"dataset_name": "weight_decay=0.0005", "final_value": 0.183267, "best_value": 0.183267}]}, {"metric_name": "validation CWA-2D accuracy", "lower_is_better": false, "description": "The classification accuracy on the validation dataset for the CWA-2D metric.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 0.945673, "best_value": 0.945673}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.944989, "best_value": 0.944989}, {"dataset_name": "weight_decay=5e-05", "final_value": 0.944274, "best_value": 0.944274}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.944989, "best_value": 0.944989}, {"dataset_name": "weight_decay=0.0005", "final_value": 0.943709, "best_value": 0.943709}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The final training loss achieved by the model on each dataset.", "data": [{"dataset_name": "SPR_BENCH_hd64", "final_value": 0.1857, "best_value": 0.1857}, {"dataset_name": "SPR_BENCH_hd128", "final_value": 0.178529, "best_value": 0.178529}, {"dataset_name": "SPR_BENCH_hd256", "final_value": 0.177337, "best_value": 0.177337}, {"dataset_name": "SPR_BENCH_hd512", "final_value": 0.172245, "best_value": 0.172245}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The final validation loss achieved by the model on each dataset.", "data": [{"dataset_name": "SPR_BENCH_hd64", "final_value": 0.181052, "best_value": 0.181052}, {"dataset_name": "SPR_BENCH_hd128", "final_value": 0.176328, "best_value": 0.176328}, {"dataset_name": "SPR_BENCH_hd256", "final_value": 0.175796, "best_value": 0.175796}, {"dataset_name": "SPR_BENCH_hd512", "final_value": 0.172161, "best_value": 0.172161}]}, {"metric_name": "validation CWA-2D", "lower_is_better": false, "description": "The final validation CWA-2D metric achieved by the model on each dataset.", "data": [{"dataset_name": "SPR_BENCH_hd64", "final_value": 0.944036, "best_value": 0.944036}, {"dataset_name": "SPR_BENCH_hd128", "final_value": 0.944215, "best_value": 0.944215}, {"dataset_name": "SPR_BENCH_hd256", "final_value": 0.945346, "best_value": 0.945346}, {"dataset_name": "SPR_BENCH_hd512", "final_value": 0.945227, "best_value": 0.945227}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, where lower values indicate better model performance.", "data": [{"dataset_name": "p=0.0", "final_value": 0.1765, "best_value": 0.1765}, {"dataset_name": "p=0.1", "final_value": 0.203907, "best_value": 0.203907}, {"dataset_name": "p=0.2", "final_value": 0.232624, "best_value": 0.232624}, {"dataset_name": "p=0.3", "final_value": 0.253615, "best_value": 0.253615}, {"dataset_name": "p=0.5", "final_value": 0.343623, "best_value": 0.343623}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation set, where lower values indicate better model performance.", "data": [{"dataset_name": "p=0.0", "final_value": 0.176044, "best_value": 0.176044}, {"dataset_name": "p=0.1", "final_value": 0.18548, "best_value": 0.18548}, {"dataset_name": "p=0.2", "final_value": 0.202671, "best_value": 0.202671}, {"dataset_name": "p=0.3", "final_value": 0.210118, "best_value": 0.210118}, {"dataset_name": "p=0.5", "final_value": 0.252542, "best_value": 0.252542}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "The accuracy metric weighted by complexity on the validation set, where higher values indicate better model performance.", "data": [{"dataset_name": "p=0.0", "final_value": 0.945673, "best_value": 0.945673}, {"dataset_name": "p=0.1", "final_value": 0.941, "best_value": 0.941}, {"dataset_name": "p=0.2", "final_value": 0.93195, "best_value": 0.93195}, {"dataset_name": "p=0.3", "final_value": 0.92558, "best_value": 0.92558}, {"dataset_name": "p=0.5", "final_value": 0.90885, "best_value": 0.90885}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1775, "best_value": 0.1765}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1762, "best_value": 0.176}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "Measures the accuracy during validation, weighted by complexity. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9439, "best_value": 0.9457}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss computed on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1664, "best_value": 0.1664}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1679, "best_value": 0.1679}]}, {"metric_name": "validation CWA-2D", "lower_is_better": false, "description": "The Correct Within Acceptable range (CWA-2D) metric computed on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9461, "best_value": 0.9461}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training, indicating the error between predictions and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1662, "best_value": 0.1662}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation, indicating the error between predictions and actual values on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.168, "best_value": 0.168}]}, {"metric_name": "CWA-2D", "lower_is_better": false, "description": "The CWA-2D metric measures model performance in a specific domain, where higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9461, "best_value": 0.9461}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1644, "best_value": 0.1644}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1661, "best_value": 0.1661}]}, {"metric_name": "CWA-2D", "lower_is_better": false, "description": "The 2D Continuous Weighted Accuracy metric during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9461, "best_value": 0.9461}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_CWA_vs_max_epochs.png", "../../logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_1e-04.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_5e-04.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_0.001.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_0.002.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_1e-04.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_1e-04.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_5e-04.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_5e-04.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_0.001.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_0.001.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_0.002.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_0.002.png", "../../logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_final_cwa_comparison.png"], ["../../logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_cwa_curves.png", "../../logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_final_cwa_bar.png"], ["../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs32.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs64.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs128.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs256.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs32.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs64.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs128.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs256.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_CWA_curves.png", "../../logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_final_CWA_bar.png"], ["../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_1e-05.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_1e-05.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_5e-05.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_5e-05.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0001.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0001.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0005.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0005.png", "../../logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_final_CWA_vs_weight_decay.png"], ["../../logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd64_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd128_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd256_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd512_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.0.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.1.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.2.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.3.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.5.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_val_loss_across_dropout.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_val_cwa_across_dropout.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_final_cwa_bar.png", "../../logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_best_p=0.0_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_off.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_0.5.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_1.0.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_2.0.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_5.0.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_val_loss_vs_clip.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_final_cwa_bar.png", "../../logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_best_loss_curves_clip_off.png"], ["../../logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_CWA_vs_max_epochs.png", "../../logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_CWA_vs_max_epochs.png", "../../logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_CWA_vs_max_epochs.png", "../../logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_a3939b4f065c4a4a908e70fe9713d3c7/SPR_BENCH_aggregated_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_a3939b4f065c4a4a908e70fe9713d3c7/SPR_BENCH_aggregated_CWA_vs_max_epochs.png"]], "plot_paths": [["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_CWA_vs_max_epochs.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_1e-04.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_5e-04.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_0.001.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_0.002.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_1e-04.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_1e-04.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_5e-04.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_5e-04.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_0.001.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_0.001.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_0.002.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_0.002.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_final_cwa_comparison.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_cwa_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_final_cwa_bar.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs32.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs64.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs128.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs256.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs32.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs64.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs128.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs256.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_CWA_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_final_CWA_bar.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_1e-05.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_1e-05.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_5e-05.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_5e-05.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0001.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0001.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0005.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0005.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_final_CWA_vs_weight_decay.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd64_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd128_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd256_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd512_loss_curve.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.0.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.1.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.2.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.3.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.5.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_val_loss_across_dropout.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_val_cwa_across_dropout.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_final_cwa_bar.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_best_p=0.0_loss_curves.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_off.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_0.5.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_1.0.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_2.0.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_5.0.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_val_loss_vs_clip.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_final_cwa_bar.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_best_loss_curves_clip_off.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_CWA_vs_max_epochs.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_CWA_vs_max_epochs.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_CWA_vs_max_epochs.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a3939b4f065c4a4a908e70fe9713d3c7/SPR_BENCH_aggregated_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a3939b4f065c4a4a908e70fe9713d3c7/SPR_BENCH_aggregated_CWA_vs_max_epochs.png"]], "plot_analyses": [[{"analysis": "The loss curves for both training and validation demonstrate a steady decrease over the epochs, converging at around the same value. This indicates that the model is learning effectively without overfitting. The training loss starts higher but quickly aligns with the validation loss, suggesting good generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_loss_curve.png"}, {"analysis": "The training and validation loss curves show a consistent pattern of convergence, with validation loss closely following the training loss. This is a positive indication of the model's ability to generalize well to unseen data. The final loss values are low, which is desirable for the task.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_loss_curves.png"}, {"analysis": "The CWA-2D validation metric remains consistent across different maximum epoch settings. This suggests that the model's performance stabilizes early, and increasing the number of epochs does not lead to further improvement in the validation metric.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_CWA_vs_max_epochs.png"}, {"analysis": "The confusion matrix shows the distribution of predictions across the ground truth labels. The diagonal dominance indicates that the model is correctly predicting the majority of the samples. However, there are some off-diagonal elements, suggesting areas where the model could improve its classification accuracy.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fa3200d9bccd452fb729cbcfed97a296_proc_3099931/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows a steady decrease in both training and validation loss over the epochs, indicating good convergence. The learning rate of 0.0001 appears to be effective in reducing the loss but at a slower pace compared to higher learning rates. The gap between training and validation loss is minimal, suggesting no signs of overfitting.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_1e-04.png"}, {"analysis": "The plot demonstrates a faster reduction in loss compared to the previous learning rate. Both training and validation loss decrease steadily, with a small gap between them. This suggests that the learning rate of 0.0005 is leading to better convergence without overfitting.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_5e-04.png"}, {"analysis": "This plot shows a rapid decrease in both training and validation loss. The learning rate of 0.001 achieves faster convergence, and the training and validation losses are nearly identical by the end of the epochs, indicating excellent generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_0.001.png"}, {"analysis": "The plot illustrates a quick reduction in loss with a learning rate of 0.002. While the training and validation losses are closely aligned, the rapid decrease might indicate potential overfitting risks if training were extended further. However, the learning rate appears to be highly effective for short-term training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/loss_lr_0.002.png"}, {"analysis": "This plot replicates the first one, showing a steady decrease in both training and validation loss with minimal overfitting. The learning rate of 0.0001 is effective but slower in reducing loss compared to higher rates.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_1e-04.png"}, {"analysis": "This plot replicates the second one, showing faster reduction in loss with a learning rate of 0.0005. The convergence is steady with minimal overfitting, making this learning rate a strong candidate for effective training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_1e-04.png"}, {"analysis": "This plot replicates the third one, showing rapid convergence with a learning rate of 0.001. The small gap between training and validation losses indicates excellent generalization, making this learning rate highly effective.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_5e-04.png"}, {"analysis": "This plot replicates the fourth one, showing a rapid decrease in loss with a learning rate of 0.002. While effective for short-term training, the rapid reduction might lead to overfitting if training is prolonged.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_5e-04.png"}, {"analysis": "This plot shows a consistent increase in the CWA-2D metric over the epochs for a learning rate of 0.0001. The steady improvement indicates that the model is learning effectively and improving its performance on the metric.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_loss_lr_0.001.png"}, {"analysis": "This bar chart compares the final CWA-2D scores across different learning rates. Learning rates of 0.0005, 0.001, and 0.002 yield similar high scores, while 0.0001 achieves a slightly lower score. This suggests that higher learning rates are more effective for this task.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7d8991975baa465483d9d1c69e1c1db4_proc_3099932/SPR_BENCH_cwa_lr_0.001.png"}], [{"analysis": "The training and validation loss curves show a consistent decrease with increasing epochs across all embedding dimensions. Smaller embedding dimensions (e.g., 32) exhibit higher initial losses and slower convergence compared to larger dimensions (e.g., 256). However, by the fifth epoch, the losses stabilize across all dimensions, indicating that the model successfully learns the task regardless of the embedding size.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_loss_curves.png"}, {"analysis": "The complexity-weighted accuracy metric improves steadily across epochs for all embedding dimensions. Larger embedding dimensions (e.g., 256, 128) achieve higher accuracy earlier, indicating better feature representation. However, by the fifth epoch, the performance difference between dimensions narrows, with all dimensions converging to a high accuracy level of approximately 0.94.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_cwa_curves.png"}, {"analysis": "The final complexity-weighted accuracy bar chart shows minimal differences in performance across embedding dimensions at the last epoch. All dimensions achieve a similar level of accuracy, confirming that embedding size has a diminishing effect on performance as the model converges. This suggests that the task is not overly sensitive to embedding dimension size beyond a certain threshold.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_47d726ee5caa4581938c3ecb9eee4bc8_proc_3099933/spr_bench_final_cwa_bar.png"}], [{"analysis": "The loss curves for a batch size of 32 show a steady decline for both training and validation loss over the epochs. The training loss decreases faster than the validation loss, indicating effective learning. However, the convergence of both curves suggests that the model is not overfitting.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs32.png"}, {"analysis": "The loss curves for a batch size of 64 demonstrate a similar trend to batch size 32, with a slightly faster decrease in training loss. The validation loss also decreases steadily, showing good generalization capability. The final loss values are lower than for batch size 32, suggesting improved performance.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs64.png"}, {"analysis": "The loss curves for a batch size of 128 exhibit a slower initial decline in training loss compared to smaller batch sizes. However, validation loss decreases steadily and converges to a low value, indicating that this batch size balances learning efficiency and generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs128.png"}, {"analysis": "The loss curves for a batch size of 256 show the slowest decline in training loss initially, but both training and validation losses converge to low values. This indicates that while learning is slower, the model achieves good generalization at this batch size.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_bs256.png"}, {"analysis": "The cross-entropy loss curves for batch size 32 reaffirm the trends observed earlier, with both training and validation losses decreasing and converging effectively. This highlights the robustness of the model at this batch size.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs32.png"}, {"analysis": "The cross-entropy loss curves for batch size 64 confirm faster convergence and lower final loss values compared to batch size 32, indicating better performance at this batch size.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs64.png"}, {"analysis": "The cross-entropy loss curves for batch size 128 show steady convergence for both training and validation losses, with a balance between learning efficiency and generalization. This batch size appears to be optimal for achieving low loss values.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs128.png"}, {"analysis": "The cross-entropy loss curves for batch size 256 demonstrate slow initial learning but eventual convergence to low loss values, reaffirming the model's ability to generalize effectively at this batch size.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_loss_curve_bs256.png"}, {"analysis": "The validation complexity-weighted accuracy (CWA) plot shows that smaller batch sizes achieve higher accuracy earlier in training, but all batch sizes converge to similar final accuracy values. This indicates that while smaller batch sizes may learn faster, larger batch sizes can achieve comparable performance with adequate training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_CWA_curves.png"}, {"analysis": "The final validation CWA by batch size plot illustrates that all batch sizes achieve nearly identical final accuracy, confirming that the model's performance is robust across different batch sizes. This suggests that batch size selection can be guided by computational efficiency rather than accuracy concerns.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_710d8d7ccf404a95bb92a09e775ea5a6_proc_3099934/SPR_BENCH_final_CWA_bar.png"}], [{"analysis": "This plot compares the validation loss across different weight decay values over epochs. As weight decay increases, the initial loss reduction is faster, but all configurations converge to similar values around epoch 4. This implies that while weight decay helps in faster initial convergence, its effect diminishes as training progresses, suggesting that other factors dominate the loss minimization in later stages.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot shows the training and validation loss for a model trained with no weight decay. Both losses decrease steadily, with the validation loss being consistently lower than the training loss. This indicates effective generalization and no overfitting. The convergence of both losses suggests that the training process is stable.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0.png"}, {"analysis": "The accuracy metric improves sharply in the initial epochs and plateaus after epoch 3. This indicates that the model quickly learns the underlying patterns in the data and achieves near-optimal performance early in the training process. The stability of the accuracy in later epochs suggests robust learning.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0.png"}, {"analysis": "This plot shows the training and validation loss for a model trained with a weight decay of 1e-5. The trend is similar to the no-weight-decay case, but the validation loss is slightly lower, indicating that a small weight decay improves generalization. The stable convergence of both losses reflects effective training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_1e-05.png"}, {"analysis": "The validation accuracy increases rapidly in the first few epochs and stabilizes after epoch 3. This pattern is consistent across weight decay values, suggesting that the model's performance is not highly sensitive to small changes in weight decay at this scale.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_1e-05.png"}, {"analysis": "The training and validation loss curves for weight decay 5e-5 show similar trends to the other configurations, with steady decreases and eventual convergence. The validation loss remains slightly lower than the training loss, indicating good generalization. The impact of this weight decay is comparable to smaller values.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_5e-05.png"}, {"analysis": "The validation accuracy for weight decay 5e-5 also shows a rapid increase followed by stabilization. This consistency across different weight decay values highlights the robustness of the model's learning process.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_5e-05.png"}, {"analysis": "The training and validation loss curves for weight decay 0.0001 follow the same trends as the other configurations. The convergence of both losses reflects stable training, and the slightly lower validation loss compared to training loss indicates good generalization. The effect of this weight decay is similar to smaller values.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0001.png"}, {"analysis": "The validation accuracy for weight decay 0.0001 stabilizes after epoch 3, consistent with other weight decay values. This suggests that the model achieves its optimal performance early in training, irrespective of the specific weight decay value.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_CWA_curve_wd_0.0001.png"}, {"analysis": "This scatter plot summarizes the final validation accuracy for different weight decay values. The accuracy differences are minimal, with all configurations achieving similar performance. This indicates that weight decay has a limited impact on the final accuracy, and other hyperparameters may play a more significant role in optimizing performance.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_56c771f235d545ab85ef821807509748_proc_3099933/SPR_BENCH_loss_curve_wd_0.0005.png"}], [{"analysis": "The loss curves for hidden_dim=64 indicate a steady decrease in both training and validation loss over epochs, suggesting effective learning. However, the validation loss starts to plateau, indicating a potential limit to the improvement with the current configuration.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd64_loss_curve.png"}, {"analysis": "The loss curves for hidden_dim=128 show a similar trend to hidden_dim=64 but with slightly faster convergence in the initial epochs. This suggests that increasing the hidden dimension improves the model's ability to learn complex patterns.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd128_loss_curve.png"}, {"analysis": "The loss curves for hidden_dim=256 demonstrate even faster convergence in the early epochs compared to smaller hidden dimensions. Both training and validation losses are lower, indicating better learning capacity and generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd256_loss_curve.png"}, {"analysis": "The loss curves for hidden_dim=512 show the lowest overall loss values among all configurations. However, the gap between training and validation loss is slightly wider, suggesting a potential risk of overfitting as the hidden dimension increases.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_2b21ba1c79134a69893a5fdccfe74812_proc_3099932/SPR_BENCH_hd512_loss_curve.png"}], [{"analysis": "The loss curves for dropout p=0.0 indicate a steady decrease in both training and validation loss across epochs, with validation loss slightly lower than training loss. This suggests that the model is learning effectively without overfitting. However, the absence of dropout might lead to overfitting in more complex scenarios.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.0.png"}, {"analysis": "The loss curves for dropout p=0.1 show a similar trend to p=0.0, with a steady decrease in both training and validation loss. The slightly higher initial losses compared to p=0.0 indicate that the dropout introduces some regularization, which could be beneficial for generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.1.png"}, {"analysis": "For dropout p=0.2, the training and validation losses continue to decrease steadily, but the validation loss is slightly higher than p=0.1. This suggests that while dropout is providing regularization, the higher dropout rate might be slightly hindering the model's capacity to fit the data.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.2.png"}, {"analysis": "The loss curves for dropout p=0.3 exhibit a further increase in training and validation loss compared to lower dropout rates. This indicates that the regularization effect of dropout is becoming more pronounced, potentially limiting the model's ability to learn the training data effectively.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.3.png"}, {"analysis": "With dropout p=0.5, the training and validation losses are the highest among the dropout settings. This suggests that the dropout rate is too high, leading to underfitting and a reduced capacity to learn from the training data.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_loss_curve_p0.5.png"}, {"analysis": "The validation loss vs epoch plot shows that dropout p=0.0 achieves the lowest validation loss, followed by p=0.1 and p=0.2. Dropout rates of p=0.3 and p=0.5 result in noticeably higher validation losses, confirming that these rates might lead to underfitting.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_val_loss_across_dropout.png"}, {"analysis": "The validation CWA-2D vs epoch plot reveals that dropout p=0.0 achieves the highest validation CWA-2D accuracy, with p=0.1 and p=0.2 following closely. Dropout rates of p=0.3 and p=0.5 show a decline in accuracy, consistent with the trend observed in the loss curves.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_val_cwa_across_dropout.png"}, {"analysis": "The final validation CWA-2D per dropout bar chart shows that dropout p=0.0 achieves the highest final accuracy, with a slight decline as the dropout rate increases. This supports the conclusion that lower dropout rates are more effective for this task.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_final_cwa_bar.png"}, {"analysis": "The best dropout p=0.0 loss curves confirm that this dropout rate results in the lowest training and validation losses, reinforcing the observation that no dropout or minimal dropout is optimal for this model on the SPR_BENCH dataset.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_32f7e1a17b754591a6069aedc37f51b9_proc_3099934/SPR_BENCH_best_p=0.0_loss_curves.png"}], [{"analysis": "The loss curves for the 'clip_off' setting show a consistent decrease in both training and validation loss over the epochs, indicating that the model is learning effectively without overfitting. The gap between training and validation losses remains small, suggesting good generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_off.png"}, {"analysis": "The 'clip_0.5' setting also demonstrates a steady decline in both training and validation losses. The behavior is similar to 'clip_off', with minimal overfitting and effective training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_0.5.png"}, {"analysis": "For the 'clip_1.0' setting, the loss curves are almost identical to the previous settings, with both training and validation losses decreasing consistently. This indicates that gradient clipping at 1.0 does not significantly alter the learning dynamics compared to 'clip_off' or 'clip_0.5'.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_1.0.png"}, {"analysis": "The 'clip_2.0' setting exhibits similar trends, with steady declines in both training and validation losses. The results suggest that gradient clipping within this range has negligible impact on training performance.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_2.0.png"}, {"analysis": "The 'clip_5.0' setting shows a consistent reduction in losses, comparable to the other settings. This indicates that increasing the gradient clip norm to 5.0 does not adversely affect training or validation loss.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_loss_curve_clip_5.0.png"}, {"analysis": "The validation loss comparison plot across gradient clip norms shows minimal differences between the settings. All configurations reach a similar final loss, suggesting that gradient clipping has little impact on validation loss for this task.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_val_loss_vs_clip.png"}, {"analysis": "The bar plot for the final CWA-2D metric across gradient clip norms shows nearly identical performance for all configurations. This reinforces the observation that gradient clipping does not significantly affect the model's ability to generalize.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_final_cwa_bar.png"}, {"analysis": "The best model loss curves for the 'clip_off' setting highlight that this configuration achieves effective training and validation loss reduction, with minimal overfitting. It suggests that gradient clipping may not be necessary for this task.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_fc588d205c114935b12c87204ab71739_proc_3099931/SPR_BENCH_best_loss_curves_clip_off.png"}], [{"analysis": "The training and validation loss curves show a steady decrease over epochs, indicating that the model is learning effectively. The validation loss aligns closely with the training loss, suggesting minimal overfitting and good generalization. The convergence appears smooth, and the gap between the training and validation loss narrows as training progresses, which is a positive sign.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_loss_curve.png"}, {"analysis": "This loss curve confirms the earlier observations of smooth convergence. The alignment between training and validation loss indicates that the hyperparameter tuning has been effective in balancing model complexity and generalization. Both curves plateau around epoch 8-10, suggesting that additional training epochs may not yield significant improvements.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar plot shows that the best validation CWA-2D score remains consistent across different max_epochs settings. This indicates that the model's performance is not heavily influenced by the number of epochs, and it achieves its peak performance relatively early in training. A max_epochs setting of around 10-15 may be sufficient for optimal results, reducing unnecessary computation.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_CWA_vs_max_epochs.png"}, {"analysis": "The confusion matrix indicates a relatively balanced prediction performance across classes. The high intensity along the diagonal suggests that the model is making correct predictions for the majority of the samples. However, the presence of off-diagonal values indicates some misclassifications, which could be further analyzed to identify patterns or biases in the errors.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot illustrates the loss curves for both training and validation over 10 epochs. The training loss decreases steadily, indicating that the model is learning from the data. The validation loss also decreases and stabilizes, suggesting that the model is not overfitting and generalizes well to unseen data. The convergence of the two curves is a good sign of a balanced training process.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot shows the training and validation loss curves, similar to the previous one, but with a different title. The trends remain consistent, with a steady decrease in both losses. This further confirms the model's ability to learn effectively while maintaining generalization to the validation set.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_loss_curves.png"}, {"analysis": "This bar plot demonstrates the best validation CWA-2D scores across different maximum epoch settings, ranging from 10 to 50. The scores appear to be consistent and high across all settings, indicating that the model's performance is robust to changes in the maximum number of epochs. This suggests that the model reaches its optimal performance relatively early in the training process.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_CWA_vs_max_epochs.png"}, {"analysis": "The confusion matrix provides a breakdown of the model's predictions against the ground truth labels. The diagonal elements represent correct predictions, while the off-diagonal elements represent misclassifications. The matrix indicates that the model performs well overall, with the majority of predictions concentrated along the diagonal. However, there is still room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate a steady and consistent reduction in both training and validation loss over the epochs. The convergence of the curves suggests that the model is not overfitting and is learning effectively. The minimal gap between the training and validation loss indicates good generalization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot corroborates the earlier observation of consistent loss reduction for both training and validation sets. The alignment of the curves further supports the absence of overfitting and effective training dynamics.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart shows consistent best validation CWA-2D scores across different maximum epoch settings. This implies that increasing the number of epochs beyond a certain point does not significantly impact the model's performance, suggesting that the model may reach its optimal performance early in training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_CWA_vs_max_epochs.png"}, {"analysis": "The confusion matrix demonstrates the distribution of predictions against ground truth labels. The diagonal dominance indicates that the model performs well in correctly classifying the data. However, the off-diagonal values suggest some misclassification, which might be addressed through further hyperparameter tuning or data augmentation.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The plots collectively indicate that the model is learning effectively, with\ntraining and validation loss curves converging and showing no signs of\noverfitting. The stability of the CWA-2D metric across epochs suggests that the\nmodel's performance is robust to changes in training duration. The confusion\nmatrix highlights strong predictive performance but also points to areas for\npotential refinement.", "The plots collectively demonstrate the impact of different learning rates on\nmodel performance. Lower learning rates like 0.0001 show steady but slower\nconvergence, while higher rates like 0.001 and 0.002 achieve faster convergence\nwith excellent generalization. The CWA-2D metric improves consistently over\nepochs, with higher learning rates yielding better final scores. Overall, the\nresults highlight the importance of choosing an optimal learning rate to balance\nconvergence speed and generalization.", "The plots indicate that embedding dimension size influences initial training\ndynamics but has minimal impact on final performance. Larger dimensions converge\nfaster and achieve slightly better early results, but all dimensions perform\nsimilarly at the final epoch. The model demonstrates strong learning\ncapabilities across all tested configurations.", "The provided plots demonstrate consistent trends in loss reduction and accuracy\nimprovement across different batch sizes. Smaller batch sizes show faster\ninitial learning, while larger batch sizes achieve comparable performance with\nsufficient training. Final validation accuracy is robust across all batch sizes,\nindicating that the model generalizes well regardless of batch size.", "The plots indicate that weight decay has minimal impact on final validation\naccuracy, with all configurations achieving similar performance. Validation loss\nand accuracy stabilize after a few epochs, suggesting robust learning across\ndifferent weight decay values. The training process is stable, with consistent\ntrends in loss and accuracy metrics.", "The analysis indicates that increasing the hidden dimension improves the model's\nlearning capability and reduces loss. However, there is a trade-off with\npotential overfitting as the hidden dimension grows. Optimal performance might\nbe achieved with a balanced hidden dimension, such as 256, which combines\neffective learning with minimal overfitting.", "The analysis highlights that lower dropout rates (p=0.0 and p=0.1) result in\nbetter performance in terms of both loss and accuracy metrics. Higher dropout\nrates (p=0.3 and p=0.5) lead to underfitting, as evidenced by increased losses\nand reduced accuracies. The results suggest that minimal or no dropout is\noptimal for this task, and further tuning around these lower dropout rates may\nyield additional improvements.", "The plots indicate that gradient clipping has minimal impact on training and\nvalidation losses or on the final CWA-2D metric. All configurations perform\nsimilarly, suggesting that the model is robust to changes in gradient clip\nnorms. The 'clip_off' setting achieves strong performance, with no evidence of\noverfitting.", "The plots demonstrate effective training dynamics with smooth loss convergence\nand minimal overfitting. The model achieves consistent validation performance\nacross varying epochs, indicating robust training. The confusion matrix shows\ngood prediction accuracy with opportunities to analyze misclassifications for\nfurther improvement.", "The experimental plots demonstrate effective model training and generalization,\nas evidenced by converging loss curves and consistent validation scores across\nepoch settings. The confusion matrix highlights good predictive performance but\nalso identifies areas for reducing misclassifications.", "The experimental plots show effective training dynamics, with consistent loss\nreduction and good generalization. The confusion matrix indicates strong\nperformance with some room for improvement in misclassification rates.\nValidation performance appears stable across different epoch settings.", "[]"], "exec_time": [161.90641617774963, 8.205969095230103, 7.150194883346558, 71.57993459701538, 59.02755856513977, 10.550358533859253, 11.83504056930542, 53.81563901901245, 169.8787202835083, 23.382184505462646, 23.73636269569397, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['hidden_dim=256']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    spr_data = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data:\n    # ---------- metrics ----------\n    preds = np.array(spr_data.get(\"predictions\", []))\n    gtruth = np.array(spr_data.get(\"ground_truth\", []))\n    acc = (preds == gtruth).mean() if preds.size else float(\"nan\")\n    best_cwa = max(spr_data.get(\"best_val_metric\", [float(\"nan\")]))\n    print(f\"Overall accuracy (best run): {acc:.4f}\")\n    print(f\"Best validation CWA-2D (across sweeps): {best_cwa:.4f}\")\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(spr_data[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, spr_data[\"losses\"][\"train\"], label=\"train\")\n        plt.plot(epochs, spr_data[\"losses\"][\"val\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- 2. CWA vs max_epochs ----------\n    try:\n        plt.figure()\n        x = spr_data[\"config_epochs\"]\n        y = spr_data[\"best_val_metric\"]\n        plt.bar(x, y, color=\"skyblue\")\n        plt.xlabel(\"max_epochs setting\")\n        plt.ylabel(\"Best Validation CWA-2D\")\n        plt.title(\"SPR_BENCH \u2013 CWA-2D versus max_epochs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_max_epochs.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA bar plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix ----------\n    try:\n        if preds.size and gtruth.size:\n            num_cls = int(max(preds.max(), gtruth.max())) + 1\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for t, p in zip(gtruth, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.ylabel(\"Ground Truth label\")\n            plt.xlabel(\"Predicted label\")\n            plt.title(\"SPR_BENCH \u2013 Confusion Matrix\\n(rows = GT, cols = Pred)\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    lr_dict = experiment_data.get(\"learning_rate\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    lr_dict = {}\n\n# ---------- per-learning-rate plots ----------\nfor lr_key, lr_data in lr_dict.items():\n    # 1) Loss curves ----------------------------------------------------------\n    try:\n        train_losses = lr_data[\"losses\"].get(\"train\", [])\n        val_losses = lr_data[\"losses\"].get(\"val\", [])\n        if train_losses or val_losses:  # plot only if at least one list is non-empty\n            plt.figure()\n            if train_losses:\n                plt.plot(train_losses, label=\"train\")\n            if val_losses:\n                plt.plot(val_losses, label=\"val\")\n            plt.title(f\"SPR_BENCH Loss Curve (lr={lr_key})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = f\"SPR_BENCH_loss_lr_{lr_key}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for lr={lr_key}: {e}\")\n        plt.close()\n\n    # 2) Validation metric curves --------------------------------------------\n    try:\n        val_metrics = lr_data[\"metrics\"].get(\"val\", [])\n        if val_metrics:\n            plt.figure()\n            plt.plot(val_metrics, marker=\"o\")\n            plt.title(f\"SPR_BENCH CWA-2D Curve (lr={lr_key})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"CWA-2D\")\n            plt.grid(True)\n            fname = f\"SPR_BENCH_cwa_lr_{lr_key}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for lr={lr_key}: {e}\")\n        plt.close()\n\n# ---------- aggregate comparison of final metrics ---------------------------\ntry:\n    lrs, finals = [], []\n    for lr_key, lr_data in lr_dict.items():\n        vals = lr_data[\"metrics\"].get(\"val\", [])\n        if vals:  # take last epoch value\n            lrs.append(lr_key)\n            finals.append(vals[-1])\n    if finals:\n        plt.figure()\n        plt.bar(lrs, finals)\n        plt.title(\"SPR_BENCH Final CWA-2D vs Learning Rate\")\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Final CWA-2D\")\n        fname = \"SPR_BENCH_final_cwa_comparison.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregate plot: {e}\")\n    plt.close()\n\nprint(f\"Finished plotting. Figures saved to {working_dir}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"embedding_dim\", {}).get(\"SPR_BENCH\", {})\nif not spr_data:\n    print(\"No SPR_BENCH data found, nothing to plot.\")\n    exit()\n\ndims = sorted(int(k.split(\"_\")[-1]) for k in spr_data.keys())\nloss_train, loss_val, cwa_val = {}, {}, {}\nfor d in dims:\n    entry = spr_data[f\"dim_{d}\"]\n    loss_train[d] = entry[\"losses\"][\"train\"]\n    loss_val[d] = entry[\"losses\"][\"val\"]\n    cwa_val[d] = entry[\"metrics\"][\"val\"]\n\n# ----------------- figure 1: loss curves -----------------\ntry:\n    plt.figure()\n    for d in dims:\n        epochs = np.arange(1, len(loss_train[d]) + 1)\n        plt.plot(epochs, loss_train[d], \"--\", label=f\"Train dim={d}\")\n        plt.plot(epochs, loss_val[d], \"-\", label=f\"Val   dim={d}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ----------------- figure 2: CWA-2D curves -----------------\ntry:\n    plt.figure()\n    for d in dims:\n        epochs = np.arange(1, len(cwa_val[d]) + 1)\n        plt.plot(epochs, cwa_val[d], label=f\"dim={d}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CWA-2D\")\n    plt.title(\"SPR_BENCH Complexity-Weighted Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_cwa_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA curve plot: {e}\")\n    plt.close()\n\n# ---------------- figure 3: final-epoch CWA summary -------\ntry:\n    plt.figure()\n    final_scores = [cwa_val[d][-1] for d in dims]\n    plt.bar([str(d) for d in dims], final_scores, color=\"skyblue\")\n    plt.xlabel(\"Embedding Dimension\")\n    plt.ylabel(\"Final Epoch CWA-2D\")\n    plt.title(\"SPR_BENCH Final Complexity-Weighted Accuracy by Embedding Size\")\n    fname = os.path.join(working_dir, \"spr_bench_final_cwa_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA bar plot: {e}\")\n    plt.close()\n\n# ------------- print evaluation metrics -------------\nprint(\"Final-epoch CWA-2D scores:\")\nfor d, score in zip(dims, final_scores):\n    print(f\"  dim={d}: {score:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths -------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load --------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    bs_dict = experiment_data.get(\"batch_size\", {}).get(\"SPR_BENCH\", {})\n    batch_sizes = sorted(bs_dict.keys())\n\n    # --------- 1) loss curves per batch size ------------\n    for bs in batch_sizes:\n        try:\n            tr_loss = bs_dict[bs][\"losses\"][\"train\"]\n            vl_loss = bs_dict[bs][\"losses\"][\"val\"]\n            plt.figure()\n            plt.plot(tr_loss, label=\"Train\")\n            plt.plot(vl_loss, label=\"Validation\")\n            plt.title(f\"SPR_BENCH Loss vs Epoch (bs={bs})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = f\"SPR_BENCH_loss_curve_bs{bs}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for bs={bs}: {e}\")\n            plt.close()\n\n    # --------- 2) combined CWA curves -------------------\n    try:\n        plt.figure()\n        for bs in batch_sizes:\n            cwa = bs_dict[bs][\"metrics\"][\"val\"]\n            plt.plot(range(1, len(cwa) + 1), cwa, label=f\"bs={bs}\")\n        plt.title(\"SPR_BENCH Validation CWA vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Complexity-Weighted Accuracy\")\n        plt.legend()\n        fname = \"SPR_BENCH_CWA_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined CWA plot: {e}\")\n        plt.close()\n\n    # --------- 3) final CWA bar chart -------------------\n    try:\n        final_cwa = [bs_dict[bs][\"metrics\"][\"val\"][-1] for bs in batch_sizes]\n        plt.figure()\n        plt.bar([str(bs) for bs in batch_sizes], final_cwa, color=\"skyblue\")\n        plt.title(\"SPR_BENCH Final Validation CWA by Batch Size\")\n        plt.xlabel(\"Batch Size\")\n        plt.ylabel(\"Final CWA\")\n        fname = \"SPR_BENCH_final_CWA_bar.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final CWA bar plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------ load experiment data ------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ------------ helper to fetch values ----------\ndef get_vals(wd_key, kind, split):\n    \"\"\"kind: losses|metrics, split: train|val\"\"\"\n    return experiment_data[\"weight_decay\"][wd_key][kind][split]\n\n\n# ------------ per-weight-decay plots ----------\nfor wd_key in experiment_data.get(\"weight_decay\", {}):\n    # ----- loss curve -----\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(get_vals(wd_key, \"losses\", \"train\")) + 1)\n        plt.plot(epochs, get_vals(wd_key, \"losses\", \"train\"), label=\"Train Loss\")\n        plt.plot(epochs, get_vals(wd_key, \"losses\", \"val\"), label=\"Val Loss\")\n        plt.title(f\"SPR_BENCH Loss Curve (wd={wd_key})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = f\"SPR_BENCH_loss_curve_wd_{wd_key}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for wd={wd_key}: {e}\")\n        plt.close()\n\n    # ----- metric curve -----\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(get_vals(wd_key, \"metrics\", \"val\")) + 1)\n        plt.plot(epochs, get_vals(wd_key, \"metrics\", \"val\"), marker=\"o\")\n        plt.title(f\"SPR_BENCH CWA-2D Validation Curve (wd={wd_key})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Complexity-Weighted Accuracy\")\n        fname = f\"SPR_BENCH_CWA_curve_wd_{wd_key}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for wd={wd_key}: {e}\")\n        plt.close()\n\n# ------------ aggregate comparison ------------\ntry:\n    plt.figure()\n    wd_keys, final_scores = [], []\n    for wd_key, sub in experiment_data.get(\"weight_decay\", {}).items():\n        wd_keys.append(float(wd_key))\n        final_scores.append(sub[\"metrics\"][\"val\"][-1] if sub[\"metrics\"][\"val\"] else 0.0)\n    plt.scatter(wd_keys, final_scores)\n    for x, y in zip(wd_keys, final_scores):\n        plt.text(x, y, f\"{y:.3f}\")\n    plt.title(\"SPR_BENCH Final CWA-2D vs Weight Decay\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Validation CWA-2D\")\n    plt.xscale(\"log\")\n    fname = \"SPR_BENCH_final_CWA_vs_weight_decay.png\"\n    plt.savefig(os.path.join(working_dir, fname))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregate plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\n\nWe will load the saved experiment_data.npy, gather the loss and CWA-2D arrays for every hidden-dim run, and create three figures: (1) a 2\u00d72 grid of loss curves (train+val) for each hidden size, (2) a 2\u00d72 grid of CWA-2D validation curves, and (3) a bar-chart that compares the final epoch CWA-2D score across hidden sizes. Each figure is generated in its own try-except block, titled clearly, saved into the working directory with descriptive names, and the figure is always closed. We also print the final CWA-2D scores so they appear in the notebook / log. Only data already present in experiment_data.npy are used, and we limit ourselves to these three plots to stay within the 5-figure guideline.", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndropout_dict = experiment_data.get(\"dropout\", {})\nif not dropout_dict:\n    print(\"No dropout experiments found. Exiting.\")\n    quit()\n\n# ---------- collect arrays ----------\nepochs = None\nval_losses, train_losses, val_cwa = {}, {}, {}\nfor key, rec in dropout_dict.items():\n    train_losses[key] = rec[\"losses\"][\"train\"]\n    val_losses[key] = rec[\"losses\"][\"val\"]\n    val_cwa[key] = rec[\"metrics\"][\"val\"]\n    epochs = range(1, len(rec[\"losses\"][\"train\"]) + 1)\n\n# best dropout (lowest final val loss)\nbest_dp = min(val_losses.items(), key=lambda kv: kv[1][-1])[0]\n\n# ---------- PLOT 1: val loss vs epoch ----------\ntry:\n    plt.figure()\n    for k, vals in val_losses.items():\n        plt.plot(epochs, vals, label=k)\n    plt.title(\"Validation Loss vs Epoch (SPR_BENCH)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_loss_across_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val loss plot: {e}\")\n    plt.close()\n\n# ---------- PLOT 2: val CWA-2D vs epoch ----------\ntry:\n    plt.figure()\n    for k, vals in val_cwa.items():\n        plt.plot(epochs, vals, label=k)\n    plt.title(\"Validation CWA-2D vs Epoch (SPR_BENCH)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CWA-2D\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_cwa_across_dropout.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA plot: {e}\")\n    plt.close()\n\n# ---------- PLOT 3: final CWA bar chart ----------\ntry:\n    plt.figure()\n    dps = list(val_cwa.keys())\n    finals = [val_cwa[k][-1] for k in dps]\n    plt.bar(dps, finals)\n    plt.title(\"Final Validation CWA-2D per Dropout (SPR_BENCH)\")\n    plt.xlabel(\"Dropout setting\")\n    plt.ylabel(\"Final CWA-2D\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_cwa_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ---------- PLOT 4: train vs val loss for best dropout ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_losses[best_dp], label=\"train\")\n    plt.plot(epochs, val_losses[best_dp], label=\"val\")\n    plt.title(f\"Best Dropout {best_dp}: Loss Curves (SPR_BENCH)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"SPR_BENCH_best_{best_dp}_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating best-loss plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"\\nFinal Validation CWA-2D by dropout:\")\nfor k in sorted(val_cwa.keys()):\n    print(f\"{k}: {val_cwa[k][-1]:.4f}\")\nprint(f\"\\nBest configuration based on final val loss: {best_dp}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------- paths --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ load data -------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"gradient_clip_norm\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# ---------------- summarise metrics -------------\nsummary = {}\nfor tag, entry in exp.items():\n    vals = entry.get(\"metrics\", {}).get(\"val\", [])\n    summary[tag] = vals[-1] if vals else None\nprint(\"Final CWA-2D per clip setting:\", summary)\n\n# ------------------- plot 1 ---------------------\ntry:\n    plt.figure()\n    for tag, entry in exp.items():\n        val_losses = entry.get(\"losses\", {}).get(\"val\", [])\n        if val_losses:\n            plt.plot(range(1, len(val_losses) + 1), val_losses, label=tag)\n    plt.title(\"Validation Loss across Gradient Clip Norms\\nDataset: SPR_BENCH\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_val_loss_vs_clip.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val-loss plot: {e}\")\n    plt.close()\n\n# ------------------- plot 2 ---------------------\ntry:\n    plt.figure()\n    tags, cwas = zip(*[(k, v) for k, v in summary.items() if v is not None])\n    plt.bar(tags, cwas)\n    plt.title(\"Final CWA-2D by Gradient Clip Norm\\nDataset: SPR_BENCH\")\n    plt.ylabel(\"CWA-2D\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_cwa_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CWA bar plot: {e}\")\n    plt.close()\n\n# ------------------- plot 3 ---------------------\ntry:\n    # choose best tag by highest CWA\n    best_tag = max(\n        (t for t in summary if summary[t] is not None), key=lambda t: summary[t]\n    )\n    best_entry = exp[best_tag]\n    tr_losses = best_entry.get(\"losses\", {}).get(\"train\", [])\n    val_losses = best_entry.get(\"losses\", {}).get(\"val\", [])\n    plt.figure()\n    if tr_losses:\n        plt.plot(range(1, len(tr_losses) + 1), tr_losses, label=\"train\")\n    if val_losses:\n        plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"val\")\n    plt.title(\n        f\"Best Model Loss Curves (Train vs Val)\\nDataset: SPR_BENCH, Setting: {best_tag}\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_best_loss_curves_{best_tag}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating best-model loss plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    spr_data = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data:\n    # ---------- metrics ----------\n    preds = np.array(spr_data.get(\"predictions\", []))\n    gtruth = np.array(spr_data.get(\"ground_truth\", []))\n    acc = (preds == gtruth).mean() if preds.size else float(\"nan\")\n    best_cwa = max(spr_data.get(\"best_val_metric\", [float(\"nan\")]))\n    print(f\"Overall accuracy (best run): {acc:.4f}\")\n    print(f\"Best validation CWA-2D (across sweeps): {best_cwa:.4f}\")\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(spr_data[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, spr_data[\"losses\"][\"train\"], label=\"train\")\n        plt.plot(epochs, spr_data[\"losses\"][\"val\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- 2. CWA vs max_epochs ----------\n    try:\n        plt.figure()\n        x = spr_data[\"config_epochs\"]\n        y = spr_data[\"best_val_metric\"]\n        plt.bar(x, y, color=\"skyblue\")\n        plt.xlabel(\"max_epochs setting\")\n        plt.ylabel(\"Best Validation CWA-2D\")\n        plt.title(\"SPR_BENCH \u2013 CWA-2D versus max_epochs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_max_epochs.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA bar plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix ----------\n    try:\n        if preds.size and gtruth.size:\n            num_cls = int(max(preds.max(), gtruth.max())) + 1\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for t, p in zip(gtruth, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.ylabel(\"Ground Truth label\")\n            plt.xlabel(\"Predicted label\")\n            plt.title(\"SPR_BENCH \u2013 Confusion Matrix\\n(rows = GT, cols = Pred)\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    spr_data = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data:\n    # ---------- metrics ----------\n    preds = np.array(spr_data.get(\"predictions\", []))\n    gtruth = np.array(spr_data.get(\"ground_truth\", []))\n    acc = (preds == gtruth).mean() if preds.size else float(\"nan\")\n    best_cwa = max(spr_data.get(\"best_val_metric\", [float(\"nan\")]))\n    print(f\"Overall accuracy (best run): {acc:.4f}\")\n    print(f\"Best validation CWA-2D (across sweeps): {best_cwa:.4f}\")\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(spr_data[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, spr_data[\"losses\"][\"train\"], label=\"train\")\n        plt.plot(epochs, spr_data[\"losses\"][\"val\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- 2. CWA vs max_epochs ----------\n    try:\n        plt.figure()\n        x = spr_data[\"config_epochs\"]\n        y = spr_data[\"best_val_metric\"]\n        plt.bar(x, y, color=\"skyblue\")\n        plt.xlabel(\"max_epochs setting\")\n        plt.ylabel(\"Best Validation CWA-2D\")\n        plt.title(\"SPR_BENCH \u2013 CWA-2D versus max_epochs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_max_epochs.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA bar plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix ----------\n    try:\n        if preds.size and gtruth.size:\n            num_cls = int(max(preds.max(), gtruth.max())) + 1\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for t, p in zip(gtruth, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.ylabel(\"Ground Truth label\")\n            plt.xlabel(\"Predicted label\")\n            plt.title(\"SPR_BENCH \u2013 Confusion Matrix\\n(rows = GT, cols = Pred)\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    spr_data = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr_data = None\n\nif spr_data:\n    # ---------- metrics ----------\n    preds = np.array(spr_data.get(\"predictions\", []))\n    gtruth = np.array(spr_data.get(\"ground_truth\", []))\n    acc = (preds == gtruth).mean() if preds.size else float(\"nan\")\n    best_cwa = max(spr_data.get(\"best_val_metric\", [float(\"nan\")]))\n    print(f\"Overall accuracy (best run): {acc:.4f}\")\n    print(f\"Best validation CWA-2D (across sweeps): {best_cwa:.4f}\")\n\n    # ---------- 1. loss curves ----------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(spr_data[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, spr_data[\"losses\"][\"train\"], label=\"train\")\n        plt.plot(epochs, spr_data[\"losses\"][\"val\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------- 2. CWA vs max_epochs ----------\n    try:\n        plt.figure()\n        x = spr_data[\"config_epochs\"]\n        y = spr_data[\"best_val_metric\"]\n        plt.bar(x, y, color=\"skyblue\")\n        plt.xlabel(\"max_epochs setting\")\n        plt.ylabel(\"Best Validation CWA-2D\")\n        plt.title(\"SPR_BENCH \u2013 CWA-2D versus max_epochs\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_CWA_vs_max_epochs.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating CWA bar plot: {e}\")\n        plt.close()\n\n    # ---------- 3. confusion matrix ----------\n    try:\n        if preds.size and gtruth.size:\n            num_cls = int(max(preds.max(), gtruth.max())) + 1\n            cm = np.zeros((num_cls, num_cls), dtype=int)\n            for t, p in zip(gtruth, preds):\n                cm[t, p] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.ylabel(\"Ground Truth label\")\n            plt.xlabel(\"Predicted label\")\n            plt.title(\"SPR_BENCH \u2013 Confusion Matrix\\n(rows = GT, cols = Pred)\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------------------------------------------------------\n# basic set-up\n# -------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------------\n# 1. load every experiment_data.npy that the prompt lists\n# -------------------------------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_dca8557aa7b645b5ac3278e5ddee5fdb_proc_3099932/experiment_data.npy\",\n    \"experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c0758bb3d6144109759e7781358caba_proc_3099933/experiment_data.npy\",\n    \"experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_27499ebabc834f069d08b8930fc901ff_proc_3099934/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    root = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n    for exp_path in experiment_data_path_list:\n        abs_path = os.path.join(root, exp_path)\n        edata = np.load(abs_path, allow_pickle=True).item()\n        all_experiment_data.append(edata)\n    print(f\"Loaded {len(all_experiment_data)} experiment_data dicts.\")\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# -------------------------------------------------------\n# 2. aggregate per-dataset results\n# -------------------------------------------------------\naggregated = {}  # dataset -> dict\nfor exp in all_experiment_data:\n    if \"num_epochs_sweep\" not in exp:\n        continue\n    for dset, ddata in exp[\"num_epochs_sweep\"].items():\n        ag = aggregated.setdefault(\n            dset,\n            {\n                \"train_losses\": [],  # list of np arrays\n                \"val_losses\": [],\n                \"best_val\": {},  # config_epochs -> list of metrics\n            },\n        )\n        # store losses\n        try:\n            ag[\"train_losses\"].append(np.asarray(ddata[\"losses\"][\"train\"]))\n            ag[\"val_losses\"].append(np.asarray(ddata[\"losses\"][\"val\"]))\n        except Exception:\n            pass\n        # store best_val_metric per config_epochs\n        try:\n            for cfg_epochs, bval in zip(\n                ddata[\"config_epochs\"], ddata[\"best_val_metric\"]\n            ):\n                ag[\"best_val\"].setdefault(cfg_epochs, []).append(float(bval))\n        except Exception:\n            pass\n\n# -------------------------------------------------------\n# 3. plotting\n# -------------------------------------------------------\nfor dset, ag in aggregated.items():\n    # 3a. aggregated loss curves -------------------------------------------\n    try:\n        # align to shortest run so we can stack\n        if ag[\"train_losses\"] and ag[\"val_losses\"]:\n            min_len = min([len(a) for a in ag[\"train_losses\"] + ag[\"val_losses\"]])\n            train_stack = np.stack([tl[:min_len] for tl in ag[\"train_losses\"]])\n            val_stack = np.stack([vl[:min_len] for vl in ag[\"val_losses\"]])\n\n            epochs = np.arange(1, min_len + 1)\n            mean_train = train_stack.mean(axis=0)\n            se_train = train_stack.std(axis=0, ddof=1) / np.sqrt(train_stack.shape[0])\n\n            mean_val = val_stack.mean(axis=0)\n            se_val = val_stack.std(axis=0, ddof=1) / np.sqrt(val_stack.shape[0])\n\n            plt.figure()\n            plt.plot(epochs, mean_train, label=\"Train \u2013 mean\", color=\"tab:blue\")\n            plt.fill_between(\n                epochs,\n                mean_train - se_train,\n                mean_train + se_train,\n                color=\"tab:blue\",\n                alpha=0.3,\n                label=\"Train \u00b1 SE\",\n            )\n            plt.plot(epochs, mean_val, label=\"Val \u2013 mean\", color=\"tab:orange\")\n            plt.fill_between(\n                epochs,\n                mean_val - se_val,\n                mean_val + se_val,\n                color=\"tab:orange\",\n                alpha=0.3,\n                label=\"Val \u00b1 SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dset} \u2013 Aggregated Training / Validation Loss (mean \u00b1 SE)\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dset}_aggregated_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dset}: {e}\")\n        plt.close()\n\n    # 3b. aggregated best-val-metric vs max_epochs -------------------------\n    try:\n        if ag[\"best_val\"]:\n            cfg_vals = sorted(ag[\"best_val\"].keys())\n            means = [np.mean(ag[\"best_val\"][c]) for c in cfg_vals]\n            ses = [\n                np.std(ag[\"best_val\"][c], ddof=1) / np.sqrt(len(ag[\"best_val\"][c]))\n                for c in cfg_vals\n            ]\n\n            plt.figure()\n            plt.bar(\n                np.arange(len(cfg_vals)),\n                means,\n                yerr=ses,\n                capsize=5,\n                color=\"skyblue\",\n                alpha=0.8,\n                label=\"Mean \u00b1 SE\",\n            )\n            plt.xticks(np.arange(len(cfg_vals)), cfg_vals)\n            plt.xlabel(\"max_epochs setting\")\n            plt.ylabel(\"Best Validation CWA-2D\")\n            plt.title(f\"{dset} \u2013 Best Validation CWA-2D vs. max_epochs (mean \u00b1 SE)\")\n            plt.legend()\n            fname = os.path.join(\n                working_dir, f\"{dset}_aggregated_CWA_vs_max_epochs.png\"\n            )\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated CWA plot for {dset}: {e}\")\n        plt.close()\n"], "plot_plan": ["We load the saved experiment_data, pick the SPR_BENCH entry, and extract (1) the epoch-wise train/val loss lists, (2) the list of tried max_epoch values and the corresponding best validation CWA for each, and (3) the predictions/ground-truth arrays for the best run.  \nThree visualisations are produced, each wrapped in its own try-except block: a line plot of the training/validation loss curves, a bar plot that summarises how CWA varies with the different max_epoch settings, and a confusion-matrix heat-map comparing predicted vs. true labels.  \nAll plots are saved inside working_dir with descriptive names that include the dataset and plot type, and every figure is closed after saving to avoid memory leaks.  \nBasic matplotlib is used with titles and clearly-labelled axes; the confusion matrix title clarifies the orientation (\u201crows = ground truth, cols = predictions\u201d).  \nAfter plotting, we also compute and print two simple evaluation numbers for the best run: overall accuracy and the peak CWA.  \nNo synthetic data are generated\u2014everything comes directly from experiment_data.npy.  \nIf any expected key is missing we catch the exception, report it, and still ensure the figure is closed.", "We first load experiment_data.npy from the working directory and fetch the learning-rate sub-dictionary.  \nFor every learning rate we create two figures: (1) the train/val loss curve and (2) the validation CWA-2D curve; both curves are plotted only if the corresponding lists are non-empty.  \nAll figures are wrapped in separate try-except blocks so that an error in one plot does not stop the rest.  \nWe follow the naming convention \u201cSPR_BENCH_[plot_type]_lr_[lr].png\u201d and save every file into working_dir, always closing the figure afterward.  \nAfter plotting per-LR curves, we additionally aggregate the final CWA-2D value of each learning rate and show them together in a bar chart to make hyper-parameter comparison easy; this too is in its own try-except block.  \nNo data are fabricated\u2014everything comes directly from experiment_data.  \nThe code starts with the required imports and ensures working_dir exists before saving.  \nBecause there are only four learning rates, we remain under the 5-figure cap for similar plots.  \nFinally, the script prints a short confirmation message so users know where to find the images.", "We will load the saved experiment_data.npy, verify the structure, and extract the losses and CWA-2D validation metrics logged for each embedding dimension of SPR_BENCH. One figure will show training vs validation loss curves for all four embedding sizes, enabling easy comparison of convergence behaviour. A second figure will plot the CWA-2D validation curves over epochs for each embedding size, letting us see which dimensionality yields the best weighted accuracy trend. Finally, a bar chart summarises the final-epoch CWA-2D score for every embedding size so the optimal hyper-parameter is visually obvious. Each plot is wrapped in its own try-except block, titled clearly, saved into the working directory with descriptive filenames, and the figure is closed afterwards. We strictly rely on the arrays stored in experiment_data.npy and create no synthetic values. No more than three figures are produced (well under the 5-figure cap), satisfying the plotting interval constraint. The script ends by printing the final-epoch CWA-2D scores so they are visible in the console alongside the saved plots.", "We will load experiment_data.npy, extract the dictionary stored by the training script, and iterate over the four batch-sizes. For each batch size we will plot train vs. validation loss across epochs (four separate figures, all in try-except blocks). To avoid producing too many similar figures we will aggregate the validation Complexity-Weighted-Accuracy (CWA) of all batch sizes into one multi-line plot, and additionally make one bar chart that compares the final (last-epoch) CWA across batch sizes. All plots are saved into the working directory with descriptive names that include the dataset (SPR_BENCH) and the plot type. Every figure is explicitly closed whether the plot succeeds or fails.", "We will first load experiment_data.npy and retrieve the nested dict. For every weight-decay configuration we plot (a) the training-vs-validation loss curve and (b) the validation CWA-2D metric curve across epochs, saving each plot with clear, descriptive filenames that include the dataset (SPR_BENCH) and the weight-decay value. We additionally create one aggregate plot that overlays the final-epoch validation CWA-2D scores for all weight-decays so the best setting is obvious. Each plot is wrapped in its own try-except so one failure will not halt the others, and we always close the figure in both success and error cases. The code obeys the directory and naming rules, uses only data contained in experiment_data.npy, and never fabricates values. We limit ourselves to the five existing weight-decay settings, well below the maximum of five plot groups requested. Finally, the script prints nothing beyond potential error messages and automatically stores all PNGs in the working directory.", "", "The script first loads the serialized dictionary from experiment_data.npy, extracts per-dropout arrays for training/validation loss and the CWA-2D metric, and determines which dropout value gives the lowest final validation loss. It then creates up to four concise, informative figures: (1) validation loss vs. epoch for every dropout value; (2) validation CWA-2D vs. epoch for every dropout value; (3) a bar chart of the final CWA-2D achieved by each dropout; and (4) a combined train/validation loss curve for the single best-performing dropout. Each figure is wrapped in its own try-except block, saved with descriptive names inside working_dir, and properly closed. Finally, the script prints a small textual summary listing each dropout and its final CWA-2D, along with the best dropout configuration. No data outside of experiment_data.npy are used, and all mandatory plotting guidelines are respected.", "We will first load the saved experiment_data.npy, extract the sub-dictionary for the SPR_BENCH gradient-clipping sweep, and gather per-epoch train/val losses and the per-epoch CWA-2D metric.  \nThree figures will be produced, each wrapped in its own try-except block and saved in working_dir:  \n(1) \u201cValidation Loss across Gradient Clip Norms\u201d\u2014lines of val-loss vs. epoch for every clipping value.  \n(2) \u201cFinal CWA-2D by Gradient Clip Norm\u201d\u2014a bar chart of the last-epoch CWA-2D for each clipping value to reveal the best hyper-parameter.  \n(3) \u201cBest Model Loss Curves (Train vs Val)\u201d\u2014train/val losses across epochs for the clipping setting that achieved the highest final CWA-2D.  \nOnly data present in experiment_data is visualised, figures are closed after saving, and the code prints a small summary table of final CWA-2D scores so users can verify numerically.  \nIf any plot cannot be generated due to missing keys or empty lists, the try-except protects the run and reports the error, ensuring at most three figures are attempted.  \nAll figures are titled clearly and filenames include dataset name and plot type, following the prescribed conventions.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["num_epochs", "learning_rate", "embedding_dim", "batch_size", "weight_decay", "classifier_hidden_dim", "dropout_rate", "gradient_clip_norm", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved NumPy dictionary, pull out the sub-dictionary for\nthe sweep on SPR_BENCH, compute the last (i.e. final) training/validation losses\nfor the best run, locate the highest validation CWA-2D achieved across all tried\nepoch budgets, and finally print each metric with explicit, descriptive names.\nAll code runs immediately on execution and nothing is hidden behind an `if\n__name__ == \"__main__\":` guard.", "The script will locate the saved experiment_data.npy file in the \u201cworking\u201d\ndirectory, load it into memory, and iterate over every learning-rate sub-\nexperiment stored inside experiment_data[\"learning_rate\"].   For each learning-\nrate run (treated here as an individual \u201cdataset\u201d), it will compute:   \u2022 the\nfinal training loss (last epoch),   \u2022 the best (minimum) validation loss across\nepochs, and   \u2022 the best (maximum) validation Complexity-Weighted Accuracy.   It\nthen prints the learning-rate tag as the dataset name followed by the clearly\nlabelled metric values, satisfying all formatting and structural requirements\nwithout generating any plots.", "The script loads the saved NumPy file, navigates through its nested structure,\nand for each dataset (here \u201cSPR_BENCH\u201d) and each hyper-parameter setting\n(embedding dimension), it pulls out the last training loss, the lowest\nvalidation loss, and the highest validation complexity-weighted accuracy. These\nvalues are printed with explicit, descriptive labels so it is clear which split\nand metric each number refers to. The code is written at global scope so it runs\nimmediately on execution.", "The script loads the saved numpy file, navigates the nested dictionary to reach\neach dataset (\u201cSPR_BENCH\u201d) and its individual batch-size runs, and then prints\nconcise summaries.   For every batch size it reports:   \u2022 the final training\nloss (last epoch)   \u2022 the final validation loss (last epoch)   \u2022 the best\n(maximum) validation complexity-weighted accuracy obtained across epochs.   All\ninformation is printed immediately at import time, with clear, descriptive\nlabels.", "The script will load the saved numpy dictionary, loop over every weight\u2013decay\nsetting (treated here as separate \u201cdatasets\u201d), and compute the best value for\neach stored series: the minimum training loss, the minimum validation loss, and\nthe maximum validation CWA-2D accuracy. Each dataset\u2019s name and its three\nmetrics will then be printed with explicit labels so it is clear which value\ncorresponds to which metric. No plots or extra output are produced, and all code\nruns at the top level without any special entry point.", "The script will locate experiment_data.npy inside the \u201cworking\u201d directory, load\nit with NumPy, and iterate over every run stored under the\n\u201cclassifier_hidden_dim\u201d key. For each run (e.g., \u201cSPR_BENCH_hd64\u201d), it will\nfetch the final values of the recorded metrics/losses: training loss, validation\nloss, and validation complexity-weighted accuracy (CWA-2D). It then prints the\ndataset (run) name followed by these metrics, each clearly labelled. Execution\nis immediate because everything sits at top level.", "The solution loads the saved NumPy object, iterates through each dropout-based\nexperiment (treated here as separate \u201cdatasets\u201d), and prints the final/optimal\nvalues recorded for training loss, validation loss, and validation complexity-\nweighted accuracy (CWA-2D). It gracefully skips metrics that were never logged\nand keeps the code at the global scope so it executes immediately.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each dataset and its individual experiment variants, compute the\nbest (minimum or maximum) value for every recorded metric, and print them in a\nclear, labelled format. It relies on the structure defined in the original\ntraining script, namely the nested dictionary levels \u201cgradient_clip_norm \u279c\ndataset \u279c experiment-tag \u279c losses / metrics\u201d. No plots are produced and\neverything runs immediately on execution.", "The script will load the saved NumPy dictionary, pull out the sub-dictionary for\nthe sweep on SPR_BENCH, compute the last (i.e. final) training/validation losses\nfor the best run, locate the highest validation CWA-2D achieved across all tried\nepoch budgets, and finally print each metric with explicit, descriptive names.\nAll code runs immediately on execution and nothing is hidden behind an `if\n__name__ == \"__main__\":` guard.", "The script will load the saved NumPy dictionary, pull out the sub-dictionary for\nthe sweep on SPR_BENCH, compute the last (i.e. final) training/validation losses\nfor the best run, locate the highest validation CWA-2D achieved across all tried\nepoch budgets, and finally print each metric with explicit, descriptive names.\nAll code runs immediately on execution and nothing is hidden behind an `if\n__name__ == \"__main__\":` guard.", "The script will load the saved NumPy dictionary, pull out the sub-dictionary for\nthe sweep on SPR_BENCH, compute the last (i.e. final) training/validation losses\nfor the best run, locate the highest validation CWA-2D achieved across all tried\nepoch budgets, and finally print each metric with explicit, descriptive names.\nAll code runs immediately on execution and nothing is hidden behind an `if\n__name__ == \"__main__\":` guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------- locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- navigate dict ------------------\nspr_dict = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\n\n# metrics from the best run (already stored during the sweep)\ntrain_losses_curve = spr_dict[\"losses\"][\"train\"]\nval_losses_curve = spr_dict[\"losses\"][\"val\"]\nfinal_train_loss = train_losses_curve[-1] if train_losses_curve else None\nfinal_val_loss = val_losses_curve[-1] if val_losses_curve else None\n\n# metrics across all configurations in the sweep\nbest_val_metrics = spr_dict[\"best_val_metric\"]\nconfig_epochs = spr_dict[\"config_epochs\"]\n\nbest_cwa_2d = max(best_val_metrics) if best_val_metrics else None\nindex_of_best_cwa = best_val_metrics.index(best_cwa_2d) if best_val_metrics else None\nbest_epoch_configuration = (\n    config_epochs[index_of_best_cwa] if index_of_best_cwa is not None else None\n)\n\n# ---------- nicely formatted output ----------\nprint(\"Dataset: SPR_BENCH\")\nprint(f\"Final training loss (best run): {final_train_loss:.4f}\")\nprint(f\"Final validation loss (best run): {final_val_loss:.4f}\")\nprint(f\"Best validation CWA-2D: {best_cwa_2d:.4f}\")\nprint(f\"Epoch budget yielding best CWA-2D: {best_epoch_configuration} epochs\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ---------- iterate over learning-rate experiments ----------\nfor lr_key, run_data in experiment_data.get(\"learning_rate\", {}).items():\n    print(f\"Dataset: learning_rate={lr_key}\")\n\n    # ----- losses -----\n    train_losses = run_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = run_data.get(\"losses\", {}).get(\"val\", [])\n\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"Final training loss: {final_train_loss:.6f}\")\n\n    if val_losses:\n        best_val_loss = min(val_losses)\n        print(f\"Best validation loss: {best_val_loss:.6f}\")\n\n    # ----- validation metric (Complexity-Weighted Accuracy) -----\n    val_metrics = run_data.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        best_val_cwa = max(val_metrics)\n        print(f\"Best validation complexity weighted accuracy: {best_val_cwa:.6f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# --------- locate and load results ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# --------- iterate and display --------------\nfor sweep_name, sweep_dict in experiment_data.items():  # e.g. \"embedding_dim\"\n    for dataset_name, runs in sweep_dict.items():  # e.g. \"SPR_BENCH\"\n        print(f\"\\nDataset: {dataset_name}\")\n        for run_name, run_data in runs.items():  # e.g. \"dim_32\"\n            losses = run_data.get(\"losses\", {})\n            metrics = run_data.get(\"metrics\", {})\n            train_ls = losses.get(\"train\", [])\n            val_ls = losses.get(\"val\", [])\n            val_mt = metrics.get(\"val\", [])\n\n            # compute best/final values\n            final_train_loss = train_ls[-1] if train_ls else None\n            best_val_loss = min(val_ls) if val_ls else None\n            best_val_cwa2d = max(val_mt) if val_mt else None\n\n            # header for this hyper-parameter configuration\n            print(f\"  Embedding dimension setting: {run_name}\")\n\n            # print metrics with explicit labels\n            if final_train_loss is not None:\n                print(f\"    final training loss: {final_train_loss:.4f}\")\n            if best_val_loss is not None:\n                print(f\"    best validation loss: {best_val_loss:.4f}\")\n            if best_val_cwa2d is not None:\n                print(\n                    f\"    best validation complexity weighted accuracy: {best_val_cwa2d:.4f}\"\n                )\n", "import os\nimport numpy as np\n\n# ---------- locate and load saved results ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate through datasets and batch-size runs ----------\nfor dataset_name, batch_size_dict in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for bs, run_data in batch_size_dict.items():\n        losses_tr = run_data[\"losses\"][\"train\"]\n        losses_val = run_data[\"losses\"][\"val\"]\n        cwa_vals = run_data[\"metrics\"][\"val\"]\n\n        final_train_loss = losses_tr[-1] if losses_tr else None\n        final_val_loss = losses_val[-1] if losses_val else None\n        best_val_cwa = max(cwa_vals) if cwa_vals else None\n\n        print(f\"  Batch size {bs}:\")\n        print(f\"    final training loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n        print(f\"    best validation complexity weighted accuracy: {best_val_cwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------- locate and load the data ------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy in {working_dir}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ----------- iterate and report metrics ----------\nwd_dict = experiment_data.get(\"weight_decay\", {})\nfor wd_key, record in wd_dict.items():\n    losses = record.get(\"losses\", {})\n    metrics = record.get(\"metrics\", {})\n\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    val_cwa = metrics.get(\"val\", [])\n\n    # Skip if any list is empty (safety check)\n    if not (train_losses and val_losses and val_cwa):\n        continue\n\n    best_train_loss = min(train_losses)\n    best_val_loss = min(val_losses)\n    best_val_cwa = max(val_cwa)\n\n    print(f\"Dataset: weight_decay={wd_key}\")\n    print(f\"  best training loss: {best_train_loss:.6f}\")\n    print(f\"  best validation loss: {best_val_loss:.6f}\")\n    print(f\"  best validation CWA-2D accuracy: {best_val_cwa:.6f}\\n\")\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# Locate and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# -------------------------------------------------\n# Extract and print final metrics for each run\n# -------------------------------------------------\nruns = experiment_data.get(\"classifier_hidden_dim\", {})\n\nfor run_name, run_info in runs.items():\n    # Safely pull the lists; fallback to empty list if missing\n    train_losses = run_info.get(\"losses\", {}).get(\"train\", [])\n    val_losses = run_info.get(\"losses\", {}).get(\"val\", [])\n    val_metrics = run_info.get(\"metrics\", {}).get(\"val\", [])\n\n    # Determine final values (or None if list is empty)\n    final_train_loss = train_losses[-1] if train_losses else None\n    final_val_loss = val_losses[-1] if val_losses else None\n    final_val_cwa = val_metrics[-1] if val_metrics else None\n\n    # Print dataset/run name\n    print(f\"Dataset: {run_name}\")\n\n    # Print metrics with explicit names\n    if final_train_loss is not None:\n        print(f\"final training loss: {final_train_loss:.6f}\")\n    if final_val_loss is not None:\n        print(f\"final validation loss: {final_val_loss:.6f}\")\n    if final_val_cwa is not None:\n        print(f\"final validation CWA-2D: {final_val_cwa:.6f}\")\n\n    print()  # blank line between runs\n", "import os\nimport numpy as np\n\n# ---------------- locate file -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# ---------------- load data -------------------\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------- extract & print -------------\n# The top-level dict has a single key \"dropout\"\ndropout_runs = experiment_data.get(\"dropout\", {})\n\nfor run_name, record in dropout_runs.items():\n    print(f\"\\nDataset: {run_name}\")  # requirement 3\n\n    # ---- losses ----\n    train_losses = record.get(\"losses\", {}).get(\"train\", [])\n    val_losses = record.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"training loss: {train_losses[-1]:.6f}\")  # requirement 4\n    if val_losses:\n        print(f\"validation loss: {val_losses[-1]:.6f}\")\n\n    # ---- metrics ----\n    val_metrics = record.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        print(f\"validation complexity weighted accuracy: {val_metrics[-1]:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------- iterate & report ----------\nfor dataset_name, exp_dict in experiment_data[\"gradient_clip_norm\"].items():\n    print(f\"Dataset: {dataset_name}\")  # requirement #3\n    for tag, run in exp_dict.items():\n        train_losses = run[\"losses\"].get(\"train\", [])\n        val_losses = run[\"losses\"].get(\"val\", [])\n        val_cwas = run[\"metrics\"].get(\"val\", [])\n\n        best_train_loss = min(train_losses) if train_losses else None\n        best_val_loss = min(val_losses) if val_losses else None\n        best_cwa = max(val_cwas) if val_cwas else None\n\n        print(f\"  Experiment {tag}:\")\n        if best_train_loss is not None:\n            print(f\"    best training loss: {best_train_loss:.4f}\")  # requirement #4\n        if best_val_loss is not None:\n            print(f\"    best validation loss: {best_val_loss:.4f}\")\n        if best_cwa is not None:\n            print(f\"    best validation complexity weighted accuracy: {best_cwa:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- navigate dict ------------------\nspr_dict = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\n\n# metrics from the best run (already stored during the sweep)\ntrain_losses_curve = spr_dict[\"losses\"][\"train\"]\nval_losses_curve = spr_dict[\"losses\"][\"val\"]\nfinal_train_loss = train_losses_curve[-1] if train_losses_curve else None\nfinal_val_loss = val_losses_curve[-1] if val_losses_curve else None\n\n# metrics across all configurations in the sweep\nbest_val_metrics = spr_dict[\"best_val_metric\"]\nconfig_epochs = spr_dict[\"config_epochs\"]\n\nbest_cwa_2d = max(best_val_metrics) if best_val_metrics else None\nindex_of_best_cwa = best_val_metrics.index(best_cwa_2d) if best_val_metrics else None\nbest_epoch_configuration = (\n    config_epochs[index_of_best_cwa] if index_of_best_cwa is not None else None\n)\n\n# ---------- nicely formatted output ----------\nprint(\"Dataset: SPR_BENCH\")\nprint(f\"Final training loss (best run): {final_train_loss:.4f}\")\nprint(f\"Final validation loss (best run): {final_val_loss:.4f}\")\nprint(f\"Best validation CWA-2D: {best_cwa_2d:.4f}\")\nprint(f\"Epoch budget yielding best CWA-2D: {best_epoch_configuration} epochs\")\n", "import os\nimport numpy as np\n\n# ---------- locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- navigate dict ------------------\nspr_dict = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\n\n# metrics from the best run (already stored during the sweep)\ntrain_losses_curve = spr_dict[\"losses\"][\"train\"]\nval_losses_curve = spr_dict[\"losses\"][\"val\"]\nfinal_train_loss = train_losses_curve[-1] if train_losses_curve else None\nfinal_val_loss = val_losses_curve[-1] if val_losses_curve else None\n\n# metrics across all configurations in the sweep\nbest_val_metrics = spr_dict[\"best_val_metric\"]\nconfig_epochs = spr_dict[\"config_epochs\"]\n\nbest_cwa_2d = max(best_val_metrics) if best_val_metrics else None\nindex_of_best_cwa = best_val_metrics.index(best_cwa_2d) if best_val_metrics else None\nbest_epoch_configuration = (\n    config_epochs[index_of_best_cwa] if index_of_best_cwa is not None else None\n)\n\n# ---------- nicely formatted output ----------\nprint(\"Dataset: SPR_BENCH\")\nprint(f\"Final training loss (best run): {final_train_loss:.4f}\")\nprint(f\"Final validation loss (best run): {final_val_loss:.4f}\")\nprint(f\"Best validation CWA-2D: {best_cwa_2d:.4f}\")\nprint(f\"Epoch budget yielding best CWA-2D: {best_epoch_configuration} epochs\")\n", "import os\nimport numpy as np\n\n# ---------- locate & load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------- navigate dict ------------------\nspr_dict = experiment_data[\"num_epochs_sweep\"][\"SPR_BENCH\"]\n\n# metrics from the best run (already stored during the sweep)\ntrain_losses_curve = spr_dict[\"losses\"][\"train\"]\nval_losses_curve = spr_dict[\"losses\"][\"val\"]\nfinal_train_loss = train_losses_curve[-1] if train_losses_curve else None\nfinal_val_loss = val_losses_curve[-1] if val_losses_curve else None\n\n# metrics across all configurations in the sweep\nbest_val_metrics = spr_dict[\"best_val_metric\"]\nconfig_epochs = spr_dict[\"config_epochs\"]\n\nbest_cwa_2d = max(best_val_metrics) if best_val_metrics else None\nindex_of_best_cwa = best_val_metrics.index(best_cwa_2d) if best_val_metrics else None\nbest_epoch_configuration = (\n    config_epochs[index_of_best_cwa] if index_of_best_cwa is not None else None\n)\n\n# ---------- nicely formatted output ----------\nprint(\"Dataset: SPR_BENCH\")\nprint(f\"Final training loss (best run): {final_train_loss:.4f}\")\nprint(f\"Final validation loss (best run): {final_val_loss:.4f}\")\nprint(f\"Best validation CWA-2D: {best_cwa_2d:.4f}\")\nprint(f\"Epoch budget yielding best CWA-2D: {best_epoch_configuration} epochs\")\n", ""], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', 'Final training loss (best run): 0.1627', '\\n',\n'Final validation loss (best run): 0.1652', '\\n', 'Best validation CWA-2D:\n0.9461', '\\n', 'Epoch budget yielding best CWA-2D: 20 epochs', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: learning_rate=1e-04', '\\n', 'Final training loss: 0.449577', '\\n',\n'Best validation loss: 0.433426', '\\n', 'Best validation complexity weighted\naccuracy: 0.799184', '\\n', '\\n', 'Dataset: learning_rate=5e-04', '\\n', 'Final\ntraining loss: 0.207086', '\\n', 'Best validation loss: 0.199444', '\\n', 'Best\nvalidation complexity weighted accuracy: 0.943530', '\\n', '\\n', 'Dataset:\nlearning_rate=0.001', '\\n', 'Final training loss: 0.179640', '\\n', 'Best\nvalidation loss: 0.178717', '\\n', 'Best validation complexity weighted accuracy:\n0.944274', '\\n', '\\n', 'Dataset: learning_rate=0.002', '\\n', 'Final training\nloss: 0.169480', '\\n', 'Best validation loss: 0.167326', '\\n', 'Best validation\ncomplexity weighted accuracy: 0.946120', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Embedding dimension setting: dim_32', '\\n', '\nfinal training loss: 0.1957', '\\n', '    best validation loss: 0.1924', '\\n', '\nbest validation complexity weighted accuracy: 0.9418', '\\n', '  Embedding\ndimension setting: dim_64', '\\n', '    final training loss: 0.1802', '\\n', '\nbest validation loss: 0.1781', '\\n', '    best validation complexity weighted\naccuracy: 0.9455', '\\n', '  Embedding dimension setting: dim_128', '\\n', '\nfinal training loss: 0.1715', '\\n', '    best validation loss: 0.1715', '\\n', '\nbest validation complexity weighted accuracy: 0.9459', '\\n', '  Embedding\ndimension setting: dim_256', '\\n', '    final training loss: 0.1702', '\\n', '\nbest validation loss: 0.1668', '\\n', '    best validation complexity weighted\naccuracy: 0.9461', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Batch size 32:', '\\n', '    final training\nloss: 0.1684', '\\n', '    final validation loss: 0.1697', '\\n', '    best\nvalidation complexity weighted accuracy: 0.9461', '\\n', '  Batch size 64:',\n'\\n', '    final training loss: 0.1732', '\\n', '    final validation loss:\n0.1703', '\\n', '    best validation complexity weighted accuracy: 0.9455', '\\n',\n'  Batch size 128:', '\\n', '    final training loss: 0.1796', '\\n', '    final\nvalidation loss: 0.1787', '\\n', '    best validation complexity weighted\naccuracy: 0.9443', '\\n', '  Batch size 256:', '\\n', '    final training loss:\n0.1932', '\\n', '    final validation loss: 0.1898', '\\n', '    best validation\ncomplexity weighted accuracy: 0.9445', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: weight_decay=0.0', '\\n', '  best training loss: 0.176500', '\\n', '\nbest validation loss: 0.176044', '\\n', '  best validation CWA-2D accuracy:\n0.945673\\n', '\\n', 'Dataset: weight_decay=1e-05', '\\n', '  best training loss:\n0.178903', '\\n', '  best validation loss: 0.177036', '\\n', '  best validation\nCWA-2D accuracy: 0.944989\\n', '\\n', 'Dataset: weight_decay=5e-05', '\\n', '  best\ntraining loss: 0.180486', '\\n', '  best validation loss: 0.179558', '\\n', '\nbest validation CWA-2D accuracy: 0.944274\\n', '\\n', 'Dataset:\nweight_decay=0.0001', '\\n', '  best training loss: 0.178831', '\\n', '  best\nvalidation loss: 0.177082', '\\n', '  best validation CWA-2D accuracy:\n0.944989\\n', '\\n', 'Dataset: weight_decay=0.0005', '\\n', '  best training loss:\n0.184590', '\\n', '  best validation loss: 0.183267', '\\n', '  best validation\nCWA-2D accuracy: 0.943709\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH_hd64', '\\n', 'final training loss: 0.185700', '\\n', 'final\nvalidation loss: 0.181052', '\\n', 'final validation CWA-2D: 0.944036', '\\n',\n'\\n', 'Dataset: SPR_BENCH_hd128', '\\n', 'final training loss: 0.178529', '\\n',\n'final validation loss: 0.176328', '\\n', 'final validation CWA-2D: 0.944215',\n'\\n', '\\n', 'Dataset: SPR_BENCH_hd256', '\\n', 'final training loss: 0.177337',\n'\\n', 'final validation loss: 0.175796', '\\n', 'final validation CWA-2D:\n0.945346', '\\n', '\\n', 'Dataset: SPR_BENCH_hd512', '\\n', 'final training loss:\n0.172245', '\\n', 'final validation loss: 0.172161', '\\n', 'final validation\nCWA-2D: 0.945227', '\\n', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: p=0.0', '\\n', 'training loss: 0.176500', '\\n', 'validation loss:\n0.176044', '\\n', 'validation complexity weighted accuracy: 0.945673', '\\n',\n'\\nDataset: p=0.1', '\\n', 'training loss: 0.203907', '\\n', 'validation loss:\n0.185480', '\\n', 'validation complexity weighted accuracy: 0.941000', '\\n',\n'\\nDataset: p=0.2', '\\n', 'training loss: 0.232624', '\\n', 'validation loss:\n0.202671', '\\n', 'validation complexity weighted accuracy: 0.931950', '\\n',\n'\\nDataset: p=0.3', '\\n', 'training loss: 0.253615', '\\n', 'validation loss:\n0.210118', '\\n', 'validation complexity weighted accuracy: 0.925580', '\\n',\n'\\nDataset: p=0.5', '\\n', 'training loss: 0.343623', '\\n', 'validation loss:\n0.252542', '\\n', 'validation complexity weighted accuracy: 0.908850', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Experiment clip_off:', '\\n', '    best training\nloss: 0.1765', '\\n', '    best validation loss: 0.1760', '\\n', '    best\nvalidation complexity weighted accuracy: 0.9457', '\\n', '  Experiment\nclip_0.5:', '\\n', '    best training loss: 0.1789', '\\n', '    best validation\nloss: 0.1766', '\\n', '    best validation complexity weighted accuracy: 0.9452',\n'\\n', '  Experiment clip_1.0:', '\\n', '    best training loss: 0.1796', '\\n', '\nbest validation loss: 0.1787', '\\n', '    best validation complexity weighted\naccuracy: 0.9443', '\\n', '  Experiment clip_2.0:', '\\n', '    best training\nloss: 0.1777', '\\n', '    best validation loss: 0.1760', '\\n', '    best\nvalidation complexity weighted accuracy: 0.9450', '\\n', '  Experiment\nclip_5.0:', '\\n', '    best training loss: 0.1775', '\\n', '    best validation\nloss: 0.1762', '\\n', '    best validation complexity weighted accuracy: 0.9439',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training loss (best run): 0.1664', '\\n',\n'Final validation loss (best run): 0.1679', '\\n', 'Best validation CWA-2D:\n0.9461', '\\n', 'Epoch budget yielding best CWA-2D: 10 epochs', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training loss (best run): 0.1662', '\\n',\n'Final validation loss (best run): 0.1680', '\\n', 'Best validation CWA-2D:\n0.9461', '\\n', 'Epoch budget yielding best CWA-2D: 10 epochs', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training loss (best run): 0.1644', '\\n',\n'Final validation loss (best run): 0.1661', '\\n', 'Best validation CWA-2D:\n0.9461', '\\n', 'Epoch budget yielding best CWA-2D: 20 epochs', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
