<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[3, 5], [3, 7], [3, 6], [3, 4]], "layout": [[0.0, 0.0], [0.2222222222222222, 0.0], [0.4444444444444444, 0.0], [0.6666666666666666, 0.0], [0.3333333333333333, 1.0], [0.5555555555555556, 1.0], [0.7777777777777778, 1.0], [1.0, 1.0]], "plan": ["A practical first baseline is to train a light GRU-based sequence classifier\nthat directly consumes the symbolic SPR sequences.  We first build a minimal\nword-level vocabulary from the training split (space-separated tokens such as\n\u201cA1\u201d, \u201cB3\u201d\u2026), add <PAD>/<UNK>, and map labels to contiguous indices.  Sequences\nare padded on-the-fly in a custom collate function; an embedding layer followed\nby a single-layer bidirectional GRU produces a fixed representation (mean-pooled\nhidden states) that feeds a linear softmax head.  We train for only a few epochs\nwith Adam, track cross-entropy loss plus our three metrics (SWA, CWA and the\nrequired CWA-2D), and print validation results each epoch.  All tensors and the\nmodel are moved to GPU when available, and metrics / losses / predictions are\nlogged into the mandated experiment_data dict and saved to the working directory\nfor later analysis or plotting.  This provides a clean, fully working supervised\nbaseline against which later context-aware contrastive pre-training can be\ncompared.", "Our baseline first encodes each symbolic token (shape-color pair) into an index,\nthen trains a light bi-GRU encoder that produces two outputs per sequence: (i) a\nclassification logit for the SPR label, and (ii) a normalized projection used\nfor SimCLR-style contrastive learning.   During training every batch is turned\ninto two stochastically augmented views via token-dropout; the loss is a\nweighted sum of cross-entropy and the NT-Xent contrastive objective.   After\neach epoch we evaluate on the dev split and report Shape-Weighted Accuracy,\nColor-Weighted Accuracy and the requested Complexity-Weighted Accuracy (shape +\ncolor). The same metrics and losses are stored in a dictionary and saved to\n./working/experiment_data.npy for later analysis.   The code is self-contained,\nautomatically falls back to a small synthetic dataset if SPR_BENCH is not found,\nhonours GPU usage, and completes within minutes for a handful of epochs\u2014serving\nas a solid, extensible starting point for our context-aware contrastive study.", "Below is a minimal end-to-end baseline.   We (1) load SPR_BENCH or fall back to\na small synthetic set, (2) build a token-level vocabulary, (3) run a very small\nSimCLR-style contrastive pre-training (token-dropout + local swaps), (4) fine-\ntune the encoder with a linear classifier, and (5) report Shape-Weighted\nAccuracy, Color-Weighted Accuracy and the requested Complexity-Weighted Accuracy\n(SWA, CWA and CWA-2D) on the dev split while tracking & saving all metrics.\nThe model is deliberately tiny (embedding 64 / GRU 128) and trained only a few\nepochs so it finishes quickly; it can be scaled later.", "We start with a simple supervised baseline that tokenises each symbolic element,\nembeds tokens, mean-pools them, and adds a linear classifier. Sequences are\npadded per batch; padding is ignored in the mean operation. A small embedding\n(64-d) plus one hidden layer keeps training fast while still capturing co-\noccurrence structure. We train with cross-entropy on the provided 20 k training\nsamples for a few epochs, evaluate every epoch on the dev set, and report SWA,\nCWA, and the requested Complexity-Weighted Accuracy (CWA-2D). All metrics,\nlosses, and predictions are stored in the prescribed experiment_data dict and\nsaved as NumPy arrays; a simple loss-curve figure is also written to ./working.\nThis establishes a reproducible end-to-end pipeline (data-load \u2192 vocab build \u2192\nmodel \u2192 train/eval/visualise) that we will later enhance with context-aware\ncontrastive pre-training.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, math, random\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- working dir ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- helper metric functions ----------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(1, sum(w))\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(1, sum(w))\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    return sum(wi for wi, t, p in zip(w, y_true, y_pred) if t == p) / max(1, sum(w))\n\n\n# ---------- dataset load ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_path = pathlib.Path(os.getenv(\"SPR_PATH\", \"SPR_BENCH\"))\nspr = load_spr_bench(data_path)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- vocab & label maps ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = {PAD: 0, UNK: 1}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        add_token(tok)\nvocab_size = len(vocab)\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nnum_classes = len(label_set)\n\nprint(f\"Vocab size: {vocab_size}, num classes: {num_classes}\")\n\n\ndef encode_sequence(seq):\n    return [vocab.get(tok, vocab[UNK]) for tok in seq.split()]\n\n\ndef encode_label(lab):\n    return label2id[lab]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    spr[split] = spr[split].map(\n        lambda ex: {\n            \"input_ids\": encode_sequence(ex[\"sequence\"]),\n            \"labels\": encode_label(ex[\"label\"]),\n        },\n        remove_columns=[],\n    )\n\n\nclass TorchSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_dataset):\n        self.ds = hf_dataset\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long),\n            \"sequence\": item[\"sequence\"],\n        }\n\n\ndef collate_fn(batch):\n    seq_lens = [len(x[\"input_ids\"]) for x in batch]\n    max_len = max(seq_lens)\n    input_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        ids = b[\"input_ids\"]\n        input_ids[i, : len(ids)] = ids\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    sequences = [b[\"sequence\"] for b in batch]\n    return {\"input_ids\": input_ids, \"labels\": labels, \"sequence\": sequences}\n\n\ntrain_loader = DataLoader(\n    TorchSeqDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    TorchSeqDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    TorchSeqDataset(spr[\"test\"]), batch_size=256, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ---------- model ----------\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n    def forward(self, x):\n        emb = self.embedding(x)  # (B,L,E)\n        out, _ = self.gru(emb)  # (B,L,2H)\n        mask = (x != 0).unsqueeze(-1)  # (B,L,1)\n        out = out * mask  # zero out pads\n        summed = out.sum(1)  # (B,2H)\n        lens = mask.sum(1).clamp(min=1)\n        mean = summed / lens  # (B,2H)\n        return self.fc(mean)\n\n\nmodel = GRUClassifier(vocab_size, 64, 128, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------- experiment data dict ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ---------- training loop ----------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss, n = 0.0, 0\n    for batch in train_loader:\n        batch_t = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch_t[\"input_ids\"])\n        loss = criterion(logits, batch_t[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch_t[\"labels\"].size(0)\n        n += batch_t[\"labels\"].size(0)\n    train_loss = total_loss / n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n\n    # ---- validation ----\n    model.eval()\n    val_loss, n = 0.0, 0\n    all_preds, all_labels, all_seqs = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch_t = {\n                k: v.to(device) if isinstance(v, torch.Tensor) else v\n                for k, v in batch.items()\n            }\n            logits = model(batch_t[\"input_ids\"])\n            loss = criterion(logits, batch_t[\"labels\"])\n            val_loss += loss.item() * batch_t[\"labels\"].size(0)\n            n += batch_t[\"labels\"].size(0)\n            preds = logits.argmax(-1).cpu().tolist()\n            labels = batch_t[\"labels\"].cpu().tolist()\n            seqs = batch[\"sequence\"]\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            all_seqs.extend(seqs)\n    val_loss /= n\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa2d = complexity_weighted_accuracy(all_seqs, all_labels, all_preds)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"swa\": swa, \"cwa\": cwa, \"cwa2d\": cwa2d}\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(\n        {\"epoch\": epoch, \"time\": time.time()}\n    )\n\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | SWA={swa:.3f} CWA={cwa:.3f} CWA-2D={cwa2d:.3f}\"\n    )\n\n# ---------- final test evaluation ----------\nmodel.eval()\nall_preds, all_labels, all_seqs = [], [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch_t = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(batch_t[\"input_ids\"])\n        preds = logits.argmax(-1).cpu().tolist()\n        labels = batch_t[\"labels\"].cpu().tolist()\n        seqs = batch[\"sequence\"]\n        all_preds.extend(preds)\n        all_labels.extend(labels)\n        all_seqs.extend(seqs)\n\nswa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\ncwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\ncwa2d = complexity_weighted_accuracy(all_seqs, all_labels, all_preds)\nprint(f\"Test set results -> SWA={swa:.3f} | CWA={cwa:.3f} | CWA-2D={cwa2d:.3f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_labels\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, pathlib, time, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------\n# mandatory working directory & device announcement\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ---------------------------------------------------\n\n\n# ------------- helper functions & metrics ----------\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    return sum((wt if t == p else 0) for wt, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    return sum((wt if t == p else 0) for wt, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    return sum((wt if t == p else 0) for wt, t, p in zip(w, y_true, y_pred)) / max(\n        sum(w), 1\n    )\n\n\n# ---------------- dataset loading ------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(fname):\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef generate_synthetic(path: str):\n    os.makedirs(path, exist_ok=True)\n\n    def rand_seq():\n        shapes, colors = \"ABCDE\", \"12345\"\n        return \" \".join(\n            random.choice(shapes) + random.choice(colors)\n            for _ in range(random.randint(4, 9))\n        )\n\n    def make_csv(fname, n):\n        with open(os.path.join(path, fname), \"w\") as f:\n            f.write(\"id,sequence,label\\n\")\n            for i in range(n):\n                seq = rand_seq()\n                lbl = int(len(seq.replace(\" \", \"\")[::2]) % 3)  # simple rule\n                f.write(f\"{i},{seq},{lbl}\\n\")\n\n    make_csv(\"train.csv\", 2000)\n    make_csv(\"dev.csv\", 400)\n    make_csv(\"test.csv\", 600)\n\n\ndata_root = pathlib.Path(os.getcwd()) / \"SPR_BENCH\"\nif not data_root.exists():\n    print(\"SPR_BENCH not found \u2192 generating tiny synthetic data.\")\n    generate_synthetic(str(data_root))\n\ndsets = load_spr_bench(data_root)\nprint({k: len(v) for k, v in dsets.items()})\n\n# --------------- vocabulary & mapping --------------\nPAD, UNK = 0, 1\ntoken2id = {\"<PAD>\": PAD, \"<UNK>\": UNK}\n\n\ndef build_vocab(dataset):\n    for seq in dataset[\"sequence\"]:\n        for tok in seq.split():\n            if tok not in token2id:\n                token2id[tok] = len(token2id)\n\n\nbuild_vocab(dsets[\"train\"])\nvocab_size = len(token2id)\nprint(\"Vocab\", vocab_size)\n\nlabel2id = {}\nfor lbl in dsets[\"train\"][\"label\"]:\n    if lbl not in label2id:\n        label2id[lbl] = len(label2id)\nnum_classes = len(label2id)\n\n\ndef encode_sequence(seq):\n    return [token2id.get(tok, UNK) for tok in seq.split()]\n\n\n# ------------------ torch Dataset ------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = [label2id[lbl] for lbl in hf_split[\"label\"]]\n        self.ids = hf_split[\"id\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": encode_sequence(self.seqs[idx]),\n            \"label\": self.labels[idx],\n            \"raw\": self.seqs[idx],\n        }\n\n\ndef pad_batch(items):\n    maxlen = max(len(x[\"input_ids\"]) for x in items)\n    input_ids = []\n    labels = []\n    raws = []\n    for it in items:\n        arr = it[\"input_ids\"] + [PAD] * (maxlen - len(it[\"input_ids\"]))\n        input_ids.append(arr)\n        labels.append(it[\"label\"])\n        raws.append(it[\"raw\"])\n    return {\n        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n        \"labels\": torch.tensor(labels, dtype=torch.long),\n        \"raw\": raws,\n    }\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(dsets[\"train\"]), batch_size=128, shuffle=True, collate_fn=pad_batch\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(dsets[\"dev\"]), batch_size=256, shuffle=False, collate_fn=pad_batch\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(dsets[\"test\"]), batch_size=256, shuffle=False, collate_fn=pad_batch\n)\n\n\n# --------------- augmentation ----------------------\ndef dropout_tokens(tokens, p=0.15):\n    keep = [tok for tok in tokens if random.random() > p]\n    if not keep:\n        keep = [random.choice(tokens)]\n    return keep\n\n\ndef augment_batch(seqs):\n    return [dropout_tokens(s) for s in seqs]\n\n\n# ------------------- model -------------------------\nclass SPRModel(nn.Module):\n    def __init__(self, vocab, n_classes, emb=128, hid=128, proj=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, emb, padding_idx=PAD)\n        self.encoder = nn.GRU(emb, hid, batch_first=True, bidirectional=True)\n        self.classifier = nn.Linear(hid * 2, n_classes)\n        self.projector = nn.Sequential(\n            nn.Linear(hid * 2, proj), nn.ReLU(), nn.Linear(proj, proj)\n        )\n\n    def forward(self, x):\n        mask = x != PAD\n        emb = self.embed(x)\n        lengths = mask.sum(1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths, enforce_sorted=False, batch_first=True\n        )\n        _, h = self.encoder(packed)\n        h = torch.cat([h[-2], h[-1]], dim=-1)  # B x 2H\n        logits = self.classifier(h)\n        z = F.normalize(self.projector(h), dim=-1)\n        return logits, z\n\n\nmodel = SPRModel(vocab_size, num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------ SimCLR loss --------------------------\ndef nt_xent(z1, z2, temp=0.5):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2B x D (already normalized)\n    sim = (z @ z.T) / temp\n    mask = torch.eye(2 * B, dtype=torch.bool, device=z.device)\n    sim.masked_fill_(mask, -9e15)\n    targets = torch.arange(B, device=z.device)\n    targets = torch.cat([targets + B, targets])\n    loss = F.cross_entropy(sim, targets)\n    return loss\n\n\n# ------------- experiment data dict ---------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------- training ----------------------\nepochs = 5\nalpha = 0.5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    tr_loss = 0\n    for batch in train_loader:\n        ids = batch[\"input_ids\"]\n        labels = batch[\"labels\"]\n        orig = ids.to(device)\n        labels = labels.to(device)\n        # build two augmented views\n        seq_list = [seq.tolist() for seq in ids]\n        view1 = [\n            encode_sequence(\n                \" \".join(\n                    [list(token2id.keys())[tok] for tok in dropout_tokens(list_tokens)]\n                )\n            )\n            for list_tokens in seq_list\n        ]\n        view2 = [\n            encode_sequence(\n                \" \".join(\n                    [list(token2id.keys())[tok] for tok in dropout_tokens(list_tokens)]\n                )\n            )\n            for list_tokens in seq_list\n        ]\n\n        def pad(v):\n            ml = max(len(x) for x in v)\n            return torch.tensor(\n                [x + [PAD] * (ml - len(x)) for x in v], dtype=torch.long\n            ).to(device)\n\n        v1 = pad(view1)\n        v2 = pad(view2)\n\n        logits, _ = model(orig)\n        _, z1 = model(v1)\n        _, z2 = model(v2)\n        loss_cls = F.cross_entropy(logits, labels)\n        loss_con = nt_xent(z1, z2)\n        loss = loss_cls + alpha * loss_con\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item()\n    tr_loss /= len(train_loader)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n\n    # -------- validation -------------\n    model.eval()\n    val_loss = 0\n    all_preds = []\n    all_labels = []\n    all_seqs = []\n    with torch.no_grad():\n        for batch in dev_loader:\n            ids = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            logits, _ = model(ids)\n            val_loss += F.cross_entropy(logits, labels, reduction=\"sum\").item()\n            preds = logits.argmax(-1).cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(batch[\"labels\"].tolist())\n            all_seqs.extend(batch[\"raw\"])\n    val_loss /= len(dev_loader.dataset)\n    swa = shape_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa = color_weighted_accuracy(all_seqs, all_labels, all_preds)\n    cwa2 = complexity_weighted_accuracy(all_seqs, all_labels, all_preds)\n    metrics = {\"swa\": swa, \"cwa\": cwa, \"cwa2d\": cwa2}\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(metrics)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | SWA {swa:.3f} CWA {cwa:.3f} CWA-2D {cwa2:.3f}\"\n    )\n\n# -------- final test evaluation --------------------\nmodel.eval()\ntest_preds = []\ntest_labels = []\ntest_seqs = []\nwith torch.no_grad():\n    for batch in test_loader:\n        ids = batch[\"input_ids\"].to(device)\n        logits, _ = model(ids)\n        test_preds.extend(logits.argmax(-1).cpu().tolist())\n        test_labels.extend(batch[\"labels\"].tolist())\n        test_seqs.extend(batch[\"raw\"])\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_labels\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"] = {\n    \"swa\": shape_weighted_accuracy(test_seqs, test_labels, test_preds),\n    \"cwa\": color_weighted_accuracy(test_seqs, test_labels, test_preds),\n    \"cwa2d\": complexity_weighted_accuracy(test_seqs, test_labels, test_preds),\n}\nprint(\"Test metrics:\", experiment_data[\"SPR_BENCH\"][\"metrics\"][\"test\"])\n\n# -------------- save experiment data ---------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, random, time, math, itertools, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"spr_bench\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"pretrain\": [], \"ft_train\": [], \"ft_val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n# ---------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# ==================  DATA LOADING  ===================================\nfrom SPR import (\n    load_spr_bench,\n    shape_weighted_accuracy,\n    color_weighted_accuracy,\n    count_shape_variety,\n    count_color_variety,\n)\n\nDATA_PATH = pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"SPR_BENCH\"))\ntry:\n    spr_bench: DatasetDict = load_spr_bench(DATA_PATH)\nexcept Exception as e:\n    print(\"Dataset not found \u2013 building tiny synthetic set.\")\n\n    def synth(n):\n        shapes = \"ABC\"\n        colors = \"xyz\"\n        data = []\n        for i in range(n):\n            seq_len = random.randint(4, 8)\n            seq = \" \".join(\n                random.choice(shapes) + random.choice(colors) for _ in range(seq_len)\n            )\n            label = int(sum(ord(c) for c in seq) % 2)  # toy rule\n            data.append({\"id\": str(i), \"sequence\": seq, \"label\": label})\n        return data\n\n    spr_bench = DatasetDict()\n    from datasets import Dataset as HFDataset\n\n    spr_bench[\"train\"] = HFDataset.from_list(synth(4000))\n    spr_bench[\"dev\"] = HFDataset.from_list(synth(1000))\n    spr_bench[\"test\"] = HFDataset.from_list(synth(1000))\nprint({k: len(v) for k, v in spr_bench.items()})\n\n\n# -------------------  TOKENISATION -----------------------------------\ndef build_vocab(dsets):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    idx = 2\n    for seq in itertools.chain.from_iterable(ds[\"sequence\"] for ds in dsets):\n        for tok in seq.split():\n            if tok not in vocab:\n                vocab[tok] = idx\n                idx += 1\n    return vocab\n\n\nvocab = build_vocab([spr_bench[\"train\"], spr_bench[\"dev\"]])\npad_idx = vocab[\"<pad>\"]\nunk_idx = vocab[\"<unk>\"]\n\n\ndef encode(seq):\n    return [vocab.get(tok, unk_idx) for tok in seq.split()]\n\n\ndef collate(batch):\n    seqs = [torch.tensor(encode(b[\"sequence\"]), dtype=torch.long) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    lengths = torch.tensor([len(s) for s in seqs])\n    maxlen = max(lengths).item()\n    padded = torch.full((len(batch), maxlen), pad_idx, dtype=torch.long)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return {\n        \"input_ids\": padded,\n        \"lengths\": lengths,\n        \"labels\": labels,\n        \"sequences\": [b[\"sequence\"] for b in batch],\n    }\n\n\n# -------------------  DATA AUGMENTATION for CONTRASTIVE --------------\ndef augment(indices, drop_p=0.15, swap_p=0.10):\n    out = []\n    for tok in indices:\n        if random.random() < drop_p:\n            continue\n        out.append(tok)\n    i = 0\n    while i < len(out) - 1:\n        if random.random() < swap_p:\n            out[i], out[i + 1] = out[i + 1], out[i]\n        i += 1\n    if not out:\n        out = indices[:]  # avoid empty\n    return out\n\n\nclass ContrastiveDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx][\"sequence\"]\n        idxs = encode(seq)\n        v1 = torch.tensor(augment(idxs), dtype=torch.long)\n        v2 = torch.tensor(augment(idxs), dtype=torch.long)\n        return {\"v1\": v1, \"v2\": v2}\n\n\ndef collate_contrastive(batch):\n    def pad(seq_list):\n        lens = [len(s) for s in seq_list]\n        maxlen = max(lens)\n        padded = torch.full((len(seq_list), maxlen), pad_idx, dtype=torch.long)\n        for i, s in enumerate(seq_list):\n            padded[i, : len(s)] = s\n        return padded, torch.tensor(lens)\n\n    v1s = [b[\"v1\"] for b in batch]\n    v2s = [b[\"v2\"] for b in batch]\n    p1, l1 = pad(v1s)\n    p2, l2 = pad(v2s)\n    return {\"v1\": p1, \"l1\": l1, \"v2\": p2, \"l2\": l2}\n\n\n# -------------------  MODEL  -----------------------------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64, hid=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.gru = nn.GRU(emb_dim, hid, batch_first=True, bidirectional=True)\n\n    def forward(self, input_ids, lengths):\n        emb = self.embed(input_ids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h_n = self.gru(packed)\n        # h_n shape: (2, B, hid)\n        h = torch.cat([h_n[0], h_n[1]], dim=-1)  # (B, 2*hid)\n        return h\n\n\nclass SimCLRModel(nn.Module):\n    def __init__(self, encoder, proj_dim=128):\n        super().__init__()\n        self.encoder = encoder\n        self.proj = nn.Sequential(\n            nn.Linear(encoder.gru.hidden_size * 2, proj_dim),\n            nn.ReLU(),\n            nn.Linear(proj_dim, proj_dim),\n        )\n\n    def forward(self, x, lengths):\n        h = self.encoder(x, lengths)\n        z = self.proj(h)\n        z = nn.functional.normalize(z, dim=-1)\n        return z\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, n_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.fc = nn.Linear(encoder.gru.hidden_size * 2, n_classes)\n\n    def forward(self, x, lengths):\n        h = self.encoder(x, lengths)\n        return self.fc(h)\n\n\n# -------------------  CONTRASTIVE PRE-TRAINING -----------------------\ndef nt_xent(z1, z2, tau=0.1):\n    B = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)\n    sim = torch.matmul(z, z.T) / tau\n    mask = torch.eye(2 * B, device=z.device).bool()\n    sim.masked_fill_(mask, -1e9)\n    labels = torch.cat([torch.arange(B) + B, torch.arange(B)], dim=0).to(z.device)\n    loss = nn.functional.cross_entropy(sim, labels)\n    return loss\n\n\ndef pretrain_contrastive():\n    encoder = Encoder(len(vocab)).to(device)\n    model = SimCLRModel(encoder).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loader = DataLoader(\n        ContrastiveDataset(spr_bench[\"train\"]),\n        batch_size=256,\n        shuffle=True,\n        collate_fn=collate_contrastive,\n        drop_last=True,\n    )\n    epochs = 3\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tot = 0\n        cnt = 0\n        for batch in loader:\n            v1 = batch[\"v1\"].to(device)\n            l1 = batch[\"l1\"].to(device)\n            v2 = batch[\"v2\"].to(device)\n            l2 = batch[\"l2\"].to(device)\n            z1 = model(v1, l1)\n            z2 = model(v2, l2)\n            loss = nt_xent(z1, z2)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            tot += loss.item()\n            cnt += 1\n        avg = tot / cnt\n        experiment_data[\"spr_bench\"][\"losses\"][\"pretrain\"].append(avg)\n        print(f\"Pretrain epoch {epoch}: loss={avg:.4f}\")\n    return encoder\n\n\n# -------------------  SUPERVISED FINE-TUNING -------------------------\ndef compute_cwa2d(seqs, y_true, y_pred):\n    weights = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    correct = [w if yt == yp else 0 for w, yt, yp in zip(weights, y_true, y_pred)]\n    return sum(correct) / sum(weights) if sum(weights) > 0 else 0.0\n\n\ndef finetune(encoder):\n    model = Classifier(encoder, n_classes=len(set(spr_bench[\"train\"][\"label\"]))).to(\n        device\n    )\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    train_loader = DataLoader(\n        spr_bench[\"train\"], batch_size=128, shuffle=True, collate_fn=collate\n    )\n    val_loader = DataLoader(spr_bench[\"dev\"], batch_size=256, collate_fn=collate)\n    epochs = 5\n    for epoch in range(1, epochs + 1):\n        # ---- train\n        model.train()\n        tot_loss = 0\n        cnt = 0\n        for batch in train_loader:\n            ids = batch[\"input_ids\"].to(device)\n            lens = batch[\"lengths\"].to(device)\n            lbls = batch[\"labels\"].to(device)\n            logits = model(ids, lens)\n            loss = crit(logits, lbls)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            tot_loss += loss.item()\n            cnt += 1\n        avg_train = tot_loss / cnt\n        # ---- validate\n        model.eval()\n        val_loss = 0\n        vcnt = 0\n        all_pred = []\n        all_true = []\n        all_seq = []\n        with torch.no_grad():\n            for batch in val_loader:\n                ids = batch[\"input_ids\"].to(device)\n                lens = batch[\"lengths\"].to(device)\n                lbls = batch[\"labels\"].to(device)\n                logits = model(ids, lens)\n                loss = crit(logits, lbls)\n                val_loss += loss.item()\n                vcnt += 1\n                preds = logits.argmax(dim=-1).cpu().tolist()\n                all_pred.extend(preds)\n                all_true.extend(lbls.cpu().tolist())\n                all_seq.extend(batch[\"sequences\"])\n        avg_val = val_loss / vcnt\n        swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n        cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n        cwa2d = compute_cwa2d(all_seq, all_true, all_pred)\n        experiment_data[\"spr_bench\"][\"losses\"][\"ft_train\"].append(avg_train)\n        experiment_data[\"spr_bench\"][\"losses\"][\"ft_val\"].append(avg_val)\n        experiment_data[\"spr_bench\"][\"metrics\"][\"train\"].append(\n            swa\n        )  # storing swa as primary\n        experiment_data[\"spr_bench\"][\"metrics\"][\"val\"].append(cwa2d)  # store cwa2d\n        experiment_data[\"spr_bench\"][\"timestamps\"].append(time.time())\n        print(\n            f\"Epoch {epoch}: val_loss={avg_val:.4f} | SWA={swa:.3f} CWA={cwa:.3f} CWA-2D={cwa2d:.3f}\"\n        )\n    # save predictions for last epoch\n    experiment_data[\"spr_bench\"][\"predictions\"] = all_pred\n    experiment_data[\"spr_bench\"][\"ground_truth\"] = all_true\n    return model\n\n\n# =====================================================================\nencoder = pretrain_contrastive()\nclf_model = finetune(encoder)\n# ------------------- SAVE EXPERIMENT DATA ----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\nmodel = MeanEncoder(len(vocab), 64, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------- training --------------------\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    epoch_loss = 0.0\n    for (x, lens, y), _ in train_loader:\n        x, lens, y = x.to(device), lens.to(device), y.to(device)\n        optimiser.zero_grad()\n        out = model(x, lens)\n        loss = criterion(out, y)\n        loss.backward()\n        optimiser.step()\n        epoch_loss += loss.item() * x.size(0)\n    train_loss = epoch_loss / len(train_ds)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # ---------- validation ----------\n    model.eval()\n    val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n    with torch.no_grad():\n        for (x, lens, y), raw in dev_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            out = model(x, lens)\n            loss = criterion(out, y)\n            val_loss += loss.item() * x.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(y.cpu().tolist())\n            all_seq.extend(raw)\n    val_loss /= len(dev_ds)\n    train_metric = None  # optional\n    cwa2d = complexity_weighted_accuracy(\n        all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(cwa2d)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | CWA-2D = {cwa2d:.4f}\")\n\n# --------------- save predictions ------------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_true\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot losses -----------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Loss curves\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\nmodel = MeanEncoder(len(vocab), 64, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------- training --------------------\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    epoch_loss = 0.0\n    for (x, lens, y), _ in train_loader:\n        x, lens, y = x.to(device), lens.to(device), y.to(device)\n        optimiser.zero_grad()\n        out = model(x, lens)\n        loss = criterion(out, y)\n        loss.backward()\n        optimiser.step()\n        epoch_loss += loss.item() * x.size(0)\n    train_loss = epoch_loss / len(train_ds)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # ---------- validation ----------\n    model.eval()\n    val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n    with torch.no_grad():\n        for (x, lens, y), raw in dev_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            out = model(x, lens)\n            loss = criterion(out, y)\n            val_loss += loss.item() * x.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(y.cpu().tolist())\n            all_seq.extend(raw)\n    val_loss /= len(dev_ds)\n    train_metric = None  # optional\n    cwa2d = complexity_weighted_accuracy(\n        all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(cwa2d)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | CWA-2D = {cwa2d:.4f}\")\n\n# --------------- save predictions ------------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_true\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot losses -----------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Loss curves\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\nmodel = MeanEncoder(len(vocab), 64, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------- training --------------------\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    epoch_loss = 0.0\n    for (x, lens, y), _ in train_loader:\n        x, lens, y = x.to(device), lens.to(device), y.to(device)\n        optimiser.zero_grad()\n        out = model(x, lens)\n        loss = criterion(out, y)\n        loss.backward()\n        optimiser.step()\n        epoch_loss += loss.item() * x.size(0)\n    train_loss = epoch_loss / len(train_ds)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # ---------- validation ----------\n    model.eval()\n    val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n    with torch.no_grad():\n        for (x, lens, y), raw in dev_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            out = model(x, lens)\n            loss = criterion(out, y)\n            val_loss += loss.item() * x.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(y.cpu().tolist())\n            all_seq.extend(raw)\n    val_loss /= len(dev_ds)\n    train_metric = None  # optional\n    cwa2d = complexity_weighted_accuracy(\n        all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(cwa2d)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | CWA-2D = {cwa2d:.4f}\")\n\n# --------------- save predictions ------------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_true\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot losses -----------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Loss curves\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, numpy as np, torch, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\n\n# ---------------- working dir -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device ----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- load SPR --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset, DatasetDict as HFDD\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = HFDD()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef count_shape_variety(sequence: str):\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str):\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef complexity_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) + count_color_variety(s) for s in seqs]\n    cor = [wt if t == p else 0 for wt, t, p in zip(w, y_true, y_pred)]\n    return sum(cor) / sum(w) if sum(w) > 0 else 0.0\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---------------- vocab build -----------------\ndef build_vocab(dataset):\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for seq in dataset[\"train\"][\"sequence\"]:\n        for tok in seq.strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr)\nprint(\"Vocab size:\", len(vocab))\n\n# --------------- label mapping ----------------\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_classes = len(labels)\nprint(\"Num classes:\", num_classes)\n\n\n# --------------- Dataset class ---------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = [label2id[l] for l in split[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def encode_seq(self, seq):\n        return [vocab.get(tok, vocab[\"<unk>\"]) for tok in seq.strip().split()]\n\n    def __getitem__(self, idx):\n        return self.encode_seq(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\ndef collate(batch):\n    seqs, labels, raw = zip(*batch)\n    lengths = [len(s) for s in seqs]\n    maxlen = max(lengths)\n    padded = np.full((len(seqs), maxlen), vocab[\"<pad>\"], dtype=np.int64)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = s\n    return (torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)), raw\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# --------------- model -----------------------\nclass MeanEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, nclass):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.classifier = nn.Sequential(\n            nn.Linear(emb_dim, 128), nn.ReLU(), nn.Linear(128, nclass)\n        )\n\n    def forward(self, x, lengths):\n        emb = self.emb(x)  # B,L,D\n        mask = (x != 0).unsqueeze(-1)  # B,L,1\n        summed = (emb * mask).sum(1)  # B,D\n        lens = mask.sum(1).clamp(min=1)  # B,1\n        mean = summed / lens  # B,D\n        return self.classifier(mean)\n\n\nmodel = MeanEncoder(len(vocab), 64, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------------- experiment data -------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------- training --------------------\nnum_epochs = 5\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    epoch_loss = 0.0\n    for (x, lens, y), _ in train_loader:\n        x, lens, y = x.to(device), lens.to(device), y.to(device)\n        optimiser.zero_grad()\n        out = model(x, lens)\n        loss = criterion(out, y)\n        loss.backward()\n        optimiser.step()\n        epoch_loss += loss.item() * x.size(0)\n    train_loss = epoch_loss / len(train_ds)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # ---------- validation ----------\n    model.eval()\n    val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n    with torch.no_grad():\n        for (x, lens, y), raw in dev_loader:\n            x, lens, y = x.to(device), lens.to(device), y.to(device)\n            out = model(x, lens)\n            loss = criterion(out, y)\n            val_loss += loss.item() * x.size(0)\n            preds = out.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(y.cpu().tolist())\n            all_seq.extend(raw)\n    val_loss /= len(dev_ds)\n    train_metric = None  # optional\n    cwa2d = complexity_weighted_accuracy(\n        all_seq, [id2label[i] for i in all_true], [id2label[i] for i in all_pred]\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(cwa2d)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | CWA-2D = {cwa2d:.4f}\")\n\n# --------------- save predictions ------------\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = all_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = all_true\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# --------------- plot losses -----------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Loss curves\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\"))\nplt.close()\n\nprint(\"Finished. Data & plot saved in ./working/\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 59, in <module>\\n    spr = load_spr_bench(data_path)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 52, in load_spr_bench\\n\nd[\"train\"] = _load(\"train.csv\")\\n                 ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 44, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-16_02-31-\n48_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH/train.csv\\'\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2192 generating tiny synthetic\ndata.', '\\n', '\\rGenerating train split: 0 examples [00:00, ? examples/s]', '',\n'\\rGenerating train split: 2000 examples [00:00, 176490.81 examples/s]', '\\n',\n'\\rGenerating train split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating\ntrain split: 400 examples [00:00, 124904.82 examples/s]', '\\n', '\\rGenerating\ntrain split: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split:\n600 examples [00:00, 138296.55 examples/s]', '\\n', \"{'train': 2000, 'dev': 400,\n'test': 600}\", '\\n', 'Vocab', ' ', '27', '\\n', 'Epoch 1: validation_loss =\n1.1208 | SWA 0.321 CWA 0.312 CWA-2D 0.317', '\\n', 'Epoch 2: validation_loss =\n1.1273 | SWA 0.361 CWA 0.353 CWA-2D 0.357', '\\n', 'Epoch 3: validation_loss =\n1.1358 | SWA 0.342 CWA 0.334 CWA-2D 0.338', '\\n', 'Epoch 4: validation_loss =\n1.1320 | SWA 0.369 CWA 0.361 CWA-2D 0.365', '\\n', 'Epoch 5: validation_loss =\n1.1178 | SWA 0.393 CWA 0.393 CWA-2D 0.393', '\\n', 'Test metrics:', ' ', \"{'swa':\n0.39183123877917414, 'cwa': 0.3840612336785232, 'cwa2d': 0.3879523488424365}\",\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 22, in <module>\\n    from SPR import (\\nModuleNotFoundError:\nNo module named \\'SPR\\'\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 411751.24\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 516781.75\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 549683.37\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', ' ', '2',\n'\\n', 'Epoch 1: validation_loss = 0.3182 | CWA-2D = 0.8896', '\\n', 'Epoch 2:\nvalidation_loss = 0.2117 | CWA-2D = 0.9348', '\\n', 'Epoch 3: validation_loss =\n0.1899 | CWA-2D = 0.9442', '\\n', 'Epoch 4: validation_loss = 0.1805 | CWA-2D =\n0.9448', '\\n', 'Epoch 5: validation_loss = 0.1760 | CWA-2D = 0.9457', '\\n',\n'Finished. Data & plot saved in ./working/', '\\n', 'Execution time: 11 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 430670.91\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 533409.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 824400.81\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', ' ', '2',\n'\\n', 'Epoch 1: validation_loss = 0.3485 | CWA-2D = 0.8581', '\\n', 'Epoch 2:\nvalidation_loss = 0.2200 | CWA-2D = 0.9370', '\\n', 'Epoch 3: validation_loss =\n0.1905 | CWA-2D = 0.9450', '\\n', 'Epoch 4: validation_loss = 0.1803 | CWA-2D =\n0.9445', '\\n', 'Epoch 5: validation_loss = 0.1755 | CWA-2D = 0.9453', '\\n',\n'Finished. Data & plot saved in ./working/', '\\n', 'Execution time: 13 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 509094.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 740519.77\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 875601.02\nexamples/s]', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', ' ', '2',\n'\\n', 'Epoch 1: validation_loss = 0.3081 | CWA-2D = 0.8864', '\\n', 'Epoch 2:\nvalidation_loss = 0.2076 | CWA-2D = 0.9369', '\\n', 'Epoch 3: validation_loss =\n0.1878 | CWA-2D = 0.9423', '\\n', 'Epoch 4: validation_loss = 0.1807 | CWA-2D =\n0.9439', '\\n', 'Epoch 5: validation_loss = 0.1763 | CWA-2D = 0.9448', '\\n',\n'Finished. Data & plot saved in ./working/', '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', '\\n', 'Epoch 1: validation_loss = 0.3287 | CWA-2D = 0.8750', '\\n',\n'Epoch 2: validation_loss = 0.2144 | CWA-2D = 0.9332', '\\n', 'Epoch 3:\nvalidation_loss = 0.1917 | CWA-2D = 0.9429', '\\n', 'Epoch 4: validation_loss =\n0.1822 | CWA-2D = 0.9444', '\\n', 'Epoch 5: validation_loss = 0.1773 | CWA-2D =\n0.9448', '\\n', 'Finished. Data & plot saved in ./working/', '\\n', 'Execution\ntime: 4 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution failed because the script could not locate the dataset file\n'train.csv' in the specified path '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-16_02-31-\n48_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH/'. This indicates a path configuration issue or a missing dataset\nfile. To fix this, ensure that the dataset files (train.csv, dev.csv, test.csv)\nare correctly placed in the specified directory. Alternatively, update the\n'data_path' variable in the script to point to the correct location of the\ndataset files.", "", "The execution failed due to a missing module error: 'No module named SPR'. This\nindicates that the SPR.py file, which contains essential functions and classes\nfor the experiment, is not accessible or not located in the correct directory.\nTo fix this, ensure that the SPR.py file is in the same directory as the script\nbeing executed or is in the Python path. Alternatively, the file path to SPR.py\ncan be added to sys.path in the script to make it discoverable.", "", "", "", "", ""], "exc_type": ["FileNotFoundError", null, "ModuleNotFoundError", null, null, null, null, null], "exc_info": [{"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-1/SPR_BENCH/train.csv'"]}, null, {"args": ["No module named 'SPR'"], "name": "SPR", "msg": "No module named 'SPR'"}, null, null, null, null, null], "exc_stack": [[["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 59, "<module>", "spr = load_spr_bench(data_path)"], ["runfile.py", 52, "load_spr_bench", "d[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 44, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 22, "<module>", "from SPR import ("]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 3.0171, "best_value": 3.0171}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.1178, "best_value": 1.1178}]}, {"metric_name": "validation shape weighted accuracy", "lower_is_better": false, "description": "Shape weighted accuracy during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.393, "best_value": 0.393}]}, {"metric_name": "validation color weighted accuracy", "lower_is_better": false, "description": "Color weighted accuracy during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.393, "best_value": 0.393}]}, {"metric_name": "validation complexity weighted accuracy", "lower_is_better": false, "description": "Complexity weighted accuracy during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.393, "best_value": 0.393}]}, {"metric_name": "test shape weighted accuracy", "lower_is_better": false, "description": "Shape weighted accuracy during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.392, "best_value": 0.392}]}, {"metric_name": "test color weighted accuracy", "lower_is_better": false, "description": "Color weighted accuracy during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.384, "best_value": 0.384}]}, {"metric_name": "test complexity weighted accuracy", "lower_is_better": false, "description": "Complexity weighted accuracy during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.388, "best_value": 0.388}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error or discrepancy between predicted and actual values during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1765, "best_value": 0.1765}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error or discrepancy between predicted and actual values during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.176, "best_value": 0.176}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "Accuracy metric weighted by complexity during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9457, "best_value": 0.9457}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correctly predicted instances during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9488, "best_value": 0.9488}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures how well the model fits the training data, with lower values indicating a better fit.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1772, "best_value": 0.1772}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model generalizes to unseen data, with lower values indicating better generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1755, "best_value": 0.1755}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "Evaluates the model's accuracy on the validation set, weighted by complexity, with higher values indicating better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9453, "best_value": 0.9453}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Evaluates the model's accuracy on the validation set, with higher values indicating better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9484, "best_value": 0.9484}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1763, "best_value": 0.1763}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1763, "best_value": 0.1763}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset, weighted by complexity.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9448, "best_value": 0.9448}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.948, "best_value": 0.948}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures how well the model is fitting the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1785, "best_value": 0.1785}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1773, "best_value": 0.1773}]}, {"metric_name": "validation complexity-weighted accuracy", "lower_is_better": false, "description": "Evaluates the accuracy of the model while considering the complexity of the classification task.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9448, "best_value": 0.9448}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.948, "best_value": 0.948}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, true, false, false, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_val_metric_curves.png", "../../logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_test_metrics_bar.png", "../../logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_class_histogram.png"], [], ["../../logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_val_metric_curve.png", "../../logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_val_metric_curve.png", "../../logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_val_metric_curve.png", "../../logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_val_metric_curve.png", "../../logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_class_distribution.png"], ["../../logs/0-run/experiment_results/seed_aggregation_f68b378776114ddb884d5aa8ad97df70/SPR_BENCH_agg_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_f68b378776114ddb884d5aa8ad97df70/SPR_BENCH_agg_val_metric.png", "../../logs/0-run/experiment_results/seed_aggregation_f68b378776114ddb884d5aa8ad97df70/SPR_BENCH_agg_class_distribution.png"]], "plot_paths": [[], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_val_metric_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_test_metrics_bar.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_class_histogram.png"], [], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_val_metric_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_class_distribution.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_val_metric_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_class_distribution.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_val_metric_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_class_distribution.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_loss_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_val_metric_curve.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_class_distribution.png"], ["experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_f68b378776114ddb884d5aa8ad97df70/SPR_BENCH_agg_loss_curves.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_f68b378776114ddb884d5aa8ad97df70/SPR_BENCH_agg_val_metric.png", "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_f68b378776114ddb884d5aa8ad97df70/SPR_BENCH_agg_class_distribution.png"]], "plot_analyses": [[], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning from the training data. However, the validation loss remains almost constant and significantly lower than the training loss, suggesting potential underfitting or an issue with the validation set not being representative of the training set.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation weighted accuracies (SWA, CWA, and CWA2D) show a consistent upward trend across epochs, indicating gradual improvement in the model's performance. The difference between SWA and CWA shows the model's sensitivity to shape and color information in the sequences, while CWA2D aligns closely with the other metrics, confirming its reliability.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_val_metric_curves.png"}, {"analysis": "The test weighted accuracies for SWA, CWA, and CWA2D are approximately equal, around 0.4. This indicates that the model's performance on the test set is consistent across different weighted accuracy metrics but is still far from the state-of-the-art benchmarks (65.0% SWA and 70.0% CWA). This implies that the model requires further optimization or architectural improvements.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_test_metrics_bar.png"}, {"analysis": "The confusion matrix reveals that the model struggles with misclassification across all classes. For example, class 0 is often misclassified as class 2, and class 1 has significant overlap with both classes 0 and 2. This highlights the need for better feature separation or a more discriminative learning approach.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The class distribution plot shows a discrepancy between the ground truth and predicted distributions, particularly for class 2, which is over-represented in predictions. This suggests a bias in the model's predictions, possibly due to imbalanced training data or insufficient regularization.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_244870cc62d24db49e42f5dcc39db07d_proc_3096999/SPR_BENCH_class_histogram.png"}], [], [{"analysis": "The plot shows the training and validation loss curves across epochs. Both losses decrease consistently, indicating that the model is learning effectively and there is no overfitting. The validation loss closely follows the training loss, which suggests a good generalization capability of the model.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot is similar to the previous one, showing training and validation loss across epochs. The consistent decrease in loss for both training and validation suggests effective learning, and the close alignment between the curves indicates good generalization. The model appears to converge around epoch 4 or 5.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the validation Color-Weighted Accuracy (CWA) over epochs. The CWA improves steadily, with a significant increase in the initial epochs and a plateauing trend after epoch 3. This indicates the model's increasing ability to correctly classify sequences with varying color complexity.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_val_metric_curve.png"}, {"analysis": "This plot compares the ground truth class distribution with the predicted class distribution. The predictions align closely with the ground truth, indicating that the model is performing well in maintaining class balance and is not biased toward any specific class.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_0c7e877a1d17458c8af237c0146db33e_proc_3097001/SPR_BENCH_class_distribution.png"}], [{"analysis": "The loss curves show consistent improvement in both training and validation losses across epochs. This indicates that the model is learning effectively and generalizing well to the validation set. The gap between training and validation loss remains small, suggesting minimal overfitting.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_loss_curve.png"}, {"analysis": "Similar to the previous plot, this set of loss curves demonstrates steady convergence of both training and validation losses. The model appears to maintain a good balance between fitting the training data and generalizing to the validation set.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation Color-Weighted Accuracy (CWA) improves significantly in the initial epochs and plateaus after epoch 3. This suggests that the model quickly learns the task and reaches a stable performance level, indicating effective training.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_val_metric_curve.png"}, {"analysis": "The class distribution plot comparing ground truth and predictions shows that the model performs well in predicting class 1, with predictions closely matching the ground truth. However, for class 0, there is a slight underprediction, suggesting room for improvement in balancing predictions across classes.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/SPR_BENCH_class_distribution.png"}], [{"analysis": "The loss curves indicate a steady reduction in loss for both the training and validation sets over the epochs. This suggests that the model is learning effectively without significant overfitting, as the validation loss closely follows the training loss trajectory. The rapid initial decrease in loss indicates that the model is able to capture patterns in the data early in the training process.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_loss_curve.png"}, {"analysis": "The loss curves for the SPR_BENCH dataset further confirm effective model training. The training and validation losses decrease consistently, with the validation loss closely tracking the training loss. This suggests that the model generalizes well to unseen data and is not overfitting.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation CWA-2D metric shows a steady improvement over the epochs, reaching a high value of approximately 0.94 by epoch 5. This indicates that the model's ability to recognize color-weighted patterns in the dataset improves consistently as training progresses. The saturation of the curve towards the end suggests that the model is nearing its optimal performance.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_val_metric_curve.png"}, {"analysis": "The class distribution plot comparing ground truth and predictions shows a close alignment between the two. While there is a slight underestimation of class 0 and overestimation of class 1, the overall distribution indicates that the model is performing well in capturing the underlying class distributions. This suggests a balanced performance across different classes.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/SPR_BENCH_class_distribution.png"}], [{"analysis": "The training and validation loss curves demonstrate a steady and consistent decrease over epochs, indicating effective learning. The validation loss is lower than the training loss after the first epoch, which may suggest appropriate regularization or that the model is not overfitting at this stage.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_loss_curve.png"}, {"analysis": "The loss curves for the SPR_BENCH dataset show a similar trend, with both training and validation loss decreasing steadily and converging. The validation loss being consistently lower than the training loss suggests that the model generalizes well to unseen data.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_loss_curves.png"}, {"analysis": "The validation CWA-2D metric shows a clear upward trend, reaching a plateau by epoch 4 or 5. This indicates that the model improves in capturing color-weighted accuracy over epochs and stabilizes at a high performance level.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_val_metric_curve.png"}, {"analysis": "The class distribution plot reveals an imbalance between the predicted and ground truth counts for class 0, with fewer predictions for this class. For class 1, the predictions slightly exceed the ground truth counts. This suggests that the model may have a bias toward predicting class 1 more frequently, which could impact overall performance and fairness.", "plot_path": "experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/SPR_BENCH_class_distribution.png"}], []], "vlm_feedback_summary": ["[]", "The plots indicate that the model is learning but is far from optimal. Training\nloss decreases, but validation loss remains constant, suggesting underfitting or\nvalidation issues. Validation accuracies improve but are far from the state-of-\nthe-art benchmarks. Test accuracies are consistent but low. The confusion matrix\nand class distribution reveal misclassification and bias issues, respectively.\nFurther optimization and architectural improvements are necessary.", "[]", "The plots demonstrate effective model learning with consistent decreases in loss\nand improvements in accuracy metrics. The alignment between training and\nvalidation loss curves, steady improvement in validation CWA, and the close\nmatch between predicted and ground truth class distributions highlight the\nrobustness and generalization capability of the model.", "The plots indicate effective training with steady convergence of loss and good\ngeneralization, as shown by the loss curves. Validation accuracy improves\nrapidly and stabilizes, reflecting effective learning. Class distribution\nanalysis reveals slight underprediction for one class, suggesting potential\nareas for improvement.", "The plots indicate effective training with consistent improvements in loss and\nvalidation metrics. The validation CWA-2D metric demonstrates significant\nprogress, and the class distribution plot shows balanced predictions with\nminimal bias. Overall, the results suggest that the model is learning\neffectively and generalizing well.", "The plots indicate steady learning progress, good generalization, and high\nperformance on validation metrics. However, there is a slight class prediction\nimbalance that may need to be addressed to improve the model's fairness and\nrobustness.", "[]"], "exec_time": [0.6561691761016846, 4.16834282875061, 0.4518446922302246, 11.709563255310059, 13.542164325714111, 4.272675037384033, 4.065829753875732, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], ["[]"], [], ["[\"experiment_data\"]"], ["[dname]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], []], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load data ---------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata_key = \"SPR_BENCH\"\nif data_key not in experiment_data:\n    print(f\"{data_key} not found in experiment_data\")\n    exit()\n\ndata = experiment_data[data_key]\nloss_tr = data[\"losses\"].get(\"train\", [])\nloss_val = data[\"losses\"].get(\"val\", [])\nval_metrics = data[\"metrics\"].get(\"val\", [])\ntest_metrics = data[\"metrics\"].get(\"test\", {})\ny_true = np.array(data.get(\"ground_truth\", []))\ny_pred = np.array(data.get(\"predictions\", []))\n\n\n# helper to extract metric curve -----------------------------------------\ndef metric_curve(metric_name):\n    return [m.get(metric_name, np.nan) for m in val_metrics]\n\n\nepochs = np.arange(1, len(loss_tr) + 1)\n\n# 1. train / val loss -----------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_tr, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"SPR_BENCH - Training vs Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 2. validation metric curves --------------------------------------------\ntry:\n    plt.figure()\n    for m in [\"swa\", \"cwa\", \"cwa2d\"]:\n        plt.plot(epochs, metric_curve(m), label=m.upper())\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH - Validation Weighted Accuracies\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_metric_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation metric plot: {e}\")\n    plt.close()\n\n# 3. bar chart of test metrics -------------------------------------------\ntry:\n    plt.figure()\n    names, vals = zip(*test_metrics.items()) if test_metrics else ([], [])\n    plt.bar(names, vals, color=\"skyblue\")\n    plt.ylim(0, 1)\n    plt.title(\"SPR_BENCH - Test Weighted Accuracies\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_metrics_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metric bar plot: {e}\")\n    plt.close()\n\n# 4. confusion matrix -----------------------------------------------------\ntry:\n    if y_true.size and y_pred.size:\n        num_classes = int(max(y_true.max(), y_pred.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(y_true, y_pred):\n            cm[t, p] += 1\n        plt.figure()\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH - Confusion Matrix\")\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# 5. class frequency histogram -------------------------------------------\ntry:\n    if y_true.size and y_pred.size:\n        plt.figure()\n        bins = np.arange(int(max(num_classes, 1)) + 1) - 0.5\n        plt.hist(y_true, bins=bins, alpha=0.6, label=\"Ground Truth\")\n        plt.hist(y_pred, bins=bins, alpha=0.6, label=\"Predicted\")\n        plt.xlabel(\"Class ID\")\n        plt.ylabel(\"Count\")\n        plt.title(\"SPR_BENCH - Class Distribution (True vs Predicted)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_class_histogram.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating histogram: {e}\")\n    plt.close()\n\n# print test metrics ------------------------------------------------------\nprint(\"Test metrics:\", test_metrics)\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper to pick dataset key\ndname = next(iter(experiment_data.keys()), None)\nif dname is None:\n    print(\"No experiment data found.\")\n    exit()\n\ndata = experiment_data[dname]\nepochs = range(1, len(data[\"losses\"].get(\"train\", [])) + 1)\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    if data[\"losses\"].get(\"train\"):\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=\"train\")\n    if data[\"losses\"].get(\"val\"):\n        plt.plot(epochs, data[\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"{dname}: Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: validation metric curve ----------\ntry:\n    if data[\"metrics\"].get(\"val\"):\n        plt.figure()\n        plt.plot(\n            epochs[: len(data[\"metrics\"][\"val\"])], data[\"metrics\"][\"val\"], marker=\"o\"\n        )\n        plt.title(f\"{dname}: Validation CWA-2D\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA-2D\")\n        fname = os.path.join(working_dir, f\"{dname}_val_metric_curve.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: class distribution ----------\ntry:\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    if preds and gts:\n        classes = sorted(set(gts + preds))\n        pred_counts = [preds.count(c) for c in classes]\n        gt_counts = [gts.count(c) for c in classes]\n\n        x = np.arange(len(classes))\n        width = 0.35\n        plt.figure()\n        plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        plt.xticks(x, classes)\n        plt.title(f\"{dname}: Class Distribution (GT vs Pred)\")\n        plt.xlabel(\"Class ID\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_class_distribution.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper to pick dataset key\ndname = next(iter(experiment_data.keys()), None)\nif dname is None:\n    print(\"No experiment data found.\")\n    exit()\n\ndata = experiment_data[dname]\nepochs = range(1, len(data[\"losses\"].get(\"train\", [])) + 1)\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    if data[\"losses\"].get(\"train\"):\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=\"train\")\n    if data[\"losses\"].get(\"val\"):\n        plt.plot(epochs, data[\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"{dname}: Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: validation metric curve ----------\ntry:\n    if data[\"metrics\"].get(\"val\"):\n        plt.figure()\n        plt.plot(\n            epochs[: len(data[\"metrics\"][\"val\"])], data[\"metrics\"][\"val\"], marker=\"o\"\n        )\n        plt.title(f\"{dname}: Validation CWA-2D\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA-2D\")\n        fname = os.path.join(working_dir, f\"{dname}_val_metric_curve.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: class distribution ----------\ntry:\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    if preds and gts:\n        classes = sorted(set(gts + preds))\n        pred_counts = [preds.count(c) for c in classes]\n        gt_counts = [gts.count(c) for c in classes]\n\n        x = np.arange(len(classes))\n        width = 0.35\n        plt.figure()\n        plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        plt.xticks(x, classes)\n        plt.title(f\"{dname}: Class Distribution (GT vs Pred)\")\n        plt.xlabel(\"Class ID\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_class_distribution.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper to pick dataset key\ndname = next(iter(experiment_data.keys()), None)\nif dname is None:\n    print(\"No experiment data found.\")\n    exit()\n\ndata = experiment_data[dname]\nepochs = range(1, len(data[\"losses\"].get(\"train\", [])) + 1)\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    if data[\"losses\"].get(\"train\"):\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=\"train\")\n    if data[\"losses\"].get(\"val\"):\n        plt.plot(epochs, data[\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"{dname}: Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: validation metric curve ----------\ntry:\n    if data[\"metrics\"].get(\"val\"):\n        plt.figure()\n        plt.plot(\n            epochs[: len(data[\"metrics\"][\"val\"])], data[\"metrics\"][\"val\"], marker=\"o\"\n        )\n        plt.title(f\"{dname}: Validation CWA-2D\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA-2D\")\n        fname = os.path.join(working_dir, f\"{dname}_val_metric_curve.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: class distribution ----------\ntry:\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    if preds and gts:\n        classes = sorted(set(gts + preds))\n        pred_counts = [preds.count(c) for c in classes]\n        gt_counts = [gts.count(c) for c in classes]\n\n        x = np.arange(len(classes))\n        width = 0.35\n        plt.figure()\n        plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        plt.xticks(x, classes)\n        plt.title(f\"{dname}: Class Distribution (GT vs Pred)\")\n        plt.xlabel(\"Class ID\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_class_distribution.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper to pick dataset key\ndname = next(iter(experiment_data.keys()), None)\nif dname is None:\n    print(\"No experiment data found.\")\n    exit()\n\ndata = experiment_data[dname]\nepochs = range(1, len(data[\"losses\"].get(\"train\", [])) + 1)\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    if data[\"losses\"].get(\"train\"):\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=\"train\")\n    if data[\"losses\"].get(\"val\"):\n        plt.plot(epochs, data[\"losses\"][\"val\"], label=\"val\")\n    plt.title(f\"{dname}: Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n    plt.savefig(fname)\n    print(f\"Saved {fname}\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: validation metric curve ----------\ntry:\n    if data[\"metrics\"].get(\"val\"):\n        plt.figure()\n        plt.plot(\n            epochs[: len(data[\"metrics\"][\"val\"])], data[\"metrics\"][\"val\"], marker=\"o\"\n        )\n        plt.title(f\"{dname}: Validation CWA-2D\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CWA-2D\")\n        fname = os.path.join(working_dir, f\"{dname}_val_metric_curve.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: class distribution ----------\ntry:\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    if preds and gts:\n        classes = sorted(set(gts + preds))\n        pred_counts = [preds.count(c) for c in classes]\n        gt_counts = [gts.count(c) for c in classes]\n\n        x = np.arange(len(classes))\n        width = 0.35\n        plt.figure()\n        plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        plt.xticks(x, classes)\n        plt.title(f\"{dname}: Class Distribution (GT vs Pred)\")\n        plt.xlabel(\"Class ID\")\n        plt.ylabel(\"Count\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_class_distribution.png\")\n        plt.savefig(fname)\n        print(f\"Saved {fname}\")\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom math import sqrt\n\n# ---- basic setup -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# paths provided by prompt\nexperiment_data_path_list = [\n    \"experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_8e17faf107614e4d9be882114528ec16_proc_3096998/experiment_data.npy\",\n    \"experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_789e750a7306425194fa2d83fef49800_proc_3097000/experiment_data.npy\",\n    \"experiments/2025-08-16_02-31-48_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b5faed0fe34848dc9afb65919550e1cc_proc_3097001/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp_dict = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dict)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nif not all_experiment_data:\n    print(\"No experiment data loaded.\")\n    exit()\n\n\n# ---- utilities -------------------------------------------------------------\ndef compute_mean_se(list_of_lists):\n    \"\"\"Stack 1-D lists to 2-D array, truncate to shortest length, return mean & SE.\"\"\"\n    min_len = min(len(l) for l in list_of_lists)\n    if min_len == 0:\n        return None, None\n    arr = np.array([l[:min_len] for l in list_of_lists], dtype=float)\n    mean = arr.mean(axis=0)\n    se = (\n        arr.std(axis=0, ddof=1) / np.sqrt(arr.shape[0])\n        if arr.shape[0] > 1\n        else np.zeros_like(mean)\n    )\n    return mean, se\n\n\n# assume every run uses the same dataset key(s); grab intersection\ndataset_keys = set.intersection(*(set(d.keys()) for d in all_experiment_data))\nif not dataset_keys:\n    print(\"No common dataset key across runs.\")\n    exit()\n\n# ---------------------------------------------------------------------------\nfor dname in dataset_keys:\n    # gather per-run arrays ---------------------------------------------------\n    train_losses_runs, val_losses_runs, val_metric_runs = [], [], []\n    preds_runs, gts_runs = [], []\n    for run in all_experiment_data:\n        data = run[dname]\n        train_losses_runs.append(data.get(\"losses\", {}).get(\"train\", []))\n        val_losses_runs.append(data.get(\"losses\", {}).get(\"val\", []))\n        if \"val\" in data.get(\"metrics\", {}):\n            val_metric_runs.append(data[\"metrics\"][\"val\"])\n        preds_runs.append(data.get(\"predictions\", []))\n        gts_runs.append(data.get(\"ground_truth\", []))\n\n    # ---- plot 1: aggregated loss curves ------------------------------------\n    try:\n        mean_train, se_train = compute_mean_se(train_losses_runs)\n        mean_val, se_val = compute_mean_se(val_losses_runs)\n        if mean_train is not None or mean_val is not None:\n            plt.figure()\n            if mean_train is not None:\n                x = np.arange(1, len(mean_train) + 1)\n                plt.plot(x, mean_train, label=\"Train Mean\")\n                plt.fill_between(\n                    x,\n                    mean_train - se_train,\n                    mean_train + se_train,\n                    alpha=0.3,\n                    label=\"Train \u00b1 SE\",\n                )\n            if mean_val is not None:\n                x = np.arange(1, len(mean_val) + 1)\n                plt.plot(x, mean_val, label=\"Val Mean\")\n                plt.fill_between(\n                    x, mean_val - se_val, mean_val + se_val, alpha=0.3, label=\"Val \u00b1 SE\"\n                )\n            plt.title(\n                f\"{dname}: Aggregated Loss Curves\\n(Mean \u00b1 Standard Error across {len(all_experiment_data)} runs)\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_agg_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dname}: {e}\")\n        plt.close()\n\n    # ---- plot 2: aggregated validation metric ------------------------------\n    try:\n        if val_metric_runs and all(len(v) for v in val_metric_runs):\n            mean_m, se_m = compute_mean_se(val_metric_runs)\n            if mean_m is not None:\n                x = np.arange(1, len(mean_m) + 1)\n                plt.figure()\n                plt.plot(x, mean_m, marker=\"o\", label=\"Val Metric Mean\")\n                plt.fill_between(\n                    x, mean_m - se_m, mean_m + se_m, alpha=0.3, label=\"Val \u00b1 SE\"\n                )\n                plt.title(\n                    f\"{dname}: Aggregated Validation Metric\\n(Mean \u00b1 SE across {len(all_experiment_data)} runs)\"\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Metric value\")\n                plt.legend()\n                fname = os.path.join(working_dir, f\"{dname}_agg_val_metric.png\")\n                plt.savefig(fname)\n                print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated metric plot for {dname}: {e}\")\n        plt.close()\n\n    # ---- plot 3: aggregated class distribution -----------------------------\n    try:\n        # Build union of class ids over all runs\n        classes = sorted(set().union(*gts_runs, *preds_runs))\n        if classes:\n            gt_counts_runs = []\n            pred_counts_runs = []\n            for gt, pr in zip(gts_runs, preds_runs):\n                gt_counts_runs.append([gt.count(c) for c in classes])\n                pred_counts_runs.append([pr.count(c) for c in classes])\n\n            gt_mean = np.mean(gt_counts_runs, axis=0)\n            pred_mean = np.mean(pred_counts_runs, axis=0)\n            gt_se = (\n                np.std(gt_counts_runs, ddof=1, axis=0) / sqrt(len(gt_counts_runs))\n                if len(gt_counts_runs) > 1\n                else np.zeros_like(gt_mean)\n            )\n            pred_se = (\n                np.std(pred_counts_runs, ddof=1, axis=0) / sqrt(len(pred_counts_runs))\n                if len(pred_counts_runs) > 1\n                else np.zeros_like(pred_mean)\n            )\n\n            x = np.arange(len(classes))\n            width = 0.35\n            plt.figure(figsize=(max(6, len(classes) * 0.8), 4.5))\n            plt.bar(\n                x - width / 2,\n                gt_mean,\n                width,\n                yerr=gt_se,\n                label=\"Ground Truth\",\n                capsize=3,\n            )\n            plt.bar(\n                x + width / 2,\n                pred_mean,\n                width,\n                yerr=pred_se,\n                label=\"Predictions\",\n                capsize=3,\n            )\n            plt.xticks(x, classes, rotation=45, ha=\"right\")\n            plt.title(f\"{dname}: Aggregated Class Distribution (Mean \u00b1 SE)\")\n            plt.xlabel(\"Class ID\")\n            plt.ylabel(\"Count\")\n            plt.legend()\n            plt.tight_layout()\n            fname = os.path.join(working_dir, f\"{dname}_agg_class_distribution.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated class distribution for {dname}: {e}\")\n        plt.close()\n"], "plot_plan": [null, "This script loads the saved experiment_data.npy, extracts the losses, validation metrics by epoch, and final test metrics for the SPR_BENCH dataset, and produces up to five concise visualizations. Figure 1 plots training vs. validation loss curves; Figure 2 plots validation SWA, CWA and CWA-2D curves; Figure 3 is a bar chart of the three weighted accuracies on the test set; Figure 4 shows a confusion matrix between predicted and true labels; Figure 5 compares class-frequency histograms of predictions and ground-truth. Each figure is built in its own try-except block, saved under a descriptive name in the working directory, and the figure is closed afterwards to avoid memory leaks. All numbers come directly from experiment_data.npy, respecting the \u201conly plot existing data\u201d rule, and at most five figures are produced. The script ends by printing the test metrics dictionary so results are visible in the execution log. Basic matplotlib is used without custom styles, and all titles clearly indicate dataset and plot type. The code starts with the mandated imports and working_dir setup, keeping everything compact and self-contained.", null, "We will first load experiment_data.npy and safely fetch the only dataset key it contains (SPR_BENCH).  From the nested dict we retrieve the existing arrays: losses[train/val] and metrics[val]; if any field is missing we skip that plot.  For each available curve (losses and metrics) a dedicated try-except block will create a figure, add epoch numbers on the x-axis, add a clear title that specifies \u201cSPR_BENCH : Loss (train vs val)\u201d etc., save it to working_dir with an explicit name, and close it.  If both ground_truth and predictions are stored, we will also build a simple bar chart of class counts (ground vs pred) to spot imbalance; this too is wrapped in try-except.  We respect the \u22645-figure rule: loss curve, metric curve, counts bar chart\u2014total \u22643.  Every figure is closed after saving, and any error is printed without halting later plots.  Finally, we print confirmation lines so the user knows which files were written.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["", "The script will load the NumPy file, iterate over every dataset contained in the\ndictionary, pick the final (last-epoch) value for every stored loss and metric,\nand print them with explicit names such as \u201ctrain loss\u201d, \u201cvalidation shape\nweighted accuracy\u201d, and so on. No plots are produced and all code is executed at\nthe top level so the file runs immediately.", "", "The script loads the serialized experiment data from the working directory,\niterates over every dataset stored inside, and extracts the lists of losses and\nmetrics that were logged during training. For each dataset it reports (1) the\nfinal training loss, (2) the best (lowest) validation loss, (3) the best\n(highest) validation complexity-weighted accuracy, and (4) the final validation\naccuracy computed from the saved predictions. All values are printed with\ndescriptive metric names so their meaning is unambiguous, and the script\nexecutes immediately without any special entry point.", "The script loads the serialized experiment data from the working directory,\niterates over every dataset stored inside, and extracts the lists of losses and\nmetrics that were logged during training. For each dataset it reports (1) the\nfinal training loss, (2) the best (lowest) validation loss, (3) the best\n(highest) validation complexity-weighted accuracy, and (4) the final validation\naccuracy computed from the saved predictions. All values are printed with\ndescriptive metric names so their meaning is unambiguous, and the script\nexecutes immediately without any special entry point.", "The script loads the serialized experiment data from the working directory,\niterates over every dataset stored inside, and extracts the lists of losses and\nmetrics that were logged during training. For each dataset it reports (1) the\nfinal training loss, (2) the best (lowest) validation loss, (3) the best\n(highest) validation complexity-weighted accuracy, and (4) the final validation\naccuracy computed from the saved predictions. All values are printed with\ndescriptive metric names so their meaning is unambiguous, and the script\nexecutes immediately without any special entry point.", "The script loads the serialized experiment data from the working directory,\niterates over every dataset stored inside, and extracts the lists of losses and\nmetrics that were logged during training. For each dataset it reports (1) the\nfinal training loss, (2) the best (lowest) validation loss, (3) the best\n(highest) validation complexity-weighted accuracy, and (4) the final validation\naccuracy computed from the saved predictions. All values are printed with\ndescriptive metric names so their meaning is unambiguous, and the script\nexecutes immediately without any special entry point.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\n\n# ---------------------------------------------------\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------\ndef print_final_metrics(data_dict):\n    \"\"\"\n    Print the final (last-epoch) value for every stored loss/metric\n    plus the test metrics for a single dataset entry.\n    \"\"\"\n    # final / last-epoch losses\n    final_train_loss = (\n        data_dict[\"losses\"][\"train\"][-1] if data_dict[\"losses\"][\"train\"] else None\n    )\n    final_val_loss = (\n        data_dict[\"losses\"][\"val\"][-1] if data_dict[\"losses\"][\"val\"] else None\n    )\n\n    # final / last-epoch validation metrics (SWA, CWA, CWA-2D)\n    if data_dict[\"metrics\"][\"val\"]:\n        final_val_metrics = data_dict[\"metrics\"][\"val\"][-1]\n        final_val_swa = final_val_metrics.get(\"swa\")\n        final_val_cwa = final_val_metrics.get(\"cwa\")\n        final_val_cwa2 = final_val_metrics.get(\"cwa2d\")\n    else:\n        final_val_swa = final_val_cwa = final_val_cwa2 = None\n\n    # test metrics\n    test_metrics = data_dict[\"metrics\"].get(\"test\", {})\n    test_swa = test_metrics.get(\"swa\")\n    test_cwa = test_metrics.get(\"cwa\")\n    test_cwa2 = test_metrics.get(\"cwa2d\")\n\n    # ---------- printing ----------\n    if final_train_loss is not None:\n        print(f\"train loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"validation loss: {final_val_loss:.4f}\")\n\n    if final_val_swa is not None:\n        print(f\"validation shape weighted accuracy: {final_val_swa:.3f}\")\n    if final_val_cwa is not None:\n        print(f\"validation color weighted accuracy: {final_val_cwa:.3f}\")\n    if final_val_cwa2 is not None:\n        print(f\"validation complexity weighted accuracy: {final_val_cwa2:.3f}\")\n\n    if test_swa is not None:\n        print(f\"test shape weighted accuracy: {test_swa:.3f}\")\n    if test_cwa is not None:\n        print(f\"test color weighted accuracy: {test_cwa:.3f}\")\n    if test_cwa2 is not None:\n        print(f\"test complexity weighted accuracy: {test_cwa2:.3f}\")\n\n\n# ---------------------------------------------------\n# iterate over datasets contained in the file\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    print_final_metrics(dataset_dict)\n", "", "import os\nimport numpy as np\n\n# ------------- locate & load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------- helpers -----------------------\ndef accuracy(preds, trues):\n    preds = np.asarray(preds)\n    trues = np.asarray(trues)\n    return (preds == trues).mean() if len(trues) else float(\"nan\")\n\n\n# ------------- reporting ---------------------\nfor dname, dct in experiment_data.items():\n    print(dname)  # dataset name header\n\n    # losses\n    train_losses = dct.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dct.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"  final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"  best validation loss: {min(val_losses):.4f}\")\n\n    # validation metrics (complexity-weighted accuracy)\n    val_cwa = dct.get(\"metrics\", {}).get(\"val\", [])\n    if val_cwa:\n        print(f\"  best validation complexity-weighted accuracy: {max(val_cwa):.4f}\")\n\n    # plain validation accuracy from stored predictions / ground-truth\n    preds = dct.get(\"predictions\", [])\n    trues = dct.get(\"ground_truth\", [])\n    if preds and trues:\n        print(f\"  final validation accuracy: {accuracy(preds, trues):.4f}\")\n", "import os\nimport numpy as np\n\n# ------------- locate & load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------- helpers -----------------------\ndef accuracy(preds, trues):\n    preds = np.asarray(preds)\n    trues = np.asarray(trues)\n    return (preds == trues).mean() if len(trues) else float(\"nan\")\n\n\n# ------------- reporting ---------------------\nfor dname, dct in experiment_data.items():\n    print(dname)  # dataset name header\n\n    # losses\n    train_losses = dct.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dct.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"  final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"  best validation loss: {min(val_losses):.4f}\")\n\n    # validation metrics (complexity-weighted accuracy)\n    val_cwa = dct.get(\"metrics\", {}).get(\"val\", [])\n    if val_cwa:\n        print(f\"  best validation complexity-weighted accuracy: {max(val_cwa):.4f}\")\n\n    # plain validation accuracy from stored predictions / ground-truth\n    preds = dct.get(\"predictions\", [])\n    trues = dct.get(\"ground_truth\", [])\n    if preds and trues:\n        print(f\"  final validation accuracy: {accuracy(preds, trues):.4f}\")\n", "import os\nimport numpy as np\n\n# ------------- locate & load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------- helpers -----------------------\ndef accuracy(preds, trues):\n    preds = np.asarray(preds)\n    trues = np.asarray(trues)\n    return (preds == trues).mean() if len(trues) else float(\"nan\")\n\n\n# ------------- reporting ---------------------\nfor dname, dct in experiment_data.items():\n    print(dname)  # dataset name header\n\n    # losses\n    train_losses = dct.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dct.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"  final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"  best validation loss: {min(val_losses):.4f}\")\n\n    # validation metrics (complexity-weighted accuracy)\n    val_cwa = dct.get(\"metrics\", {}).get(\"val\", [])\n    if val_cwa:\n        print(f\"  best validation complexity-weighted accuracy: {max(val_cwa):.4f}\")\n\n    # plain validation accuracy from stored predictions / ground-truth\n    preds = dct.get(\"predictions\", [])\n    trues = dct.get(\"ground_truth\", [])\n    if preds and trues:\n        print(f\"  final validation accuracy: {accuracy(preds, trues):.4f}\")\n", "import os\nimport numpy as np\n\n# ------------- locate & load -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------- helpers -----------------------\ndef accuracy(preds, trues):\n    preds = np.asarray(preds)\n    trues = np.asarray(trues)\n    return (preds == trues).mean() if len(trues) else float(\"nan\")\n\n\n# ------------- reporting ---------------------\nfor dname, dct in experiment_data.items():\n    print(dname)  # dataset name header\n\n    # losses\n    train_losses = dct.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dct.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"  final training loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"  best validation loss: {min(val_losses):.4f}\")\n\n    # validation metrics (complexity-weighted accuracy)\n    val_cwa = dct.get(\"metrics\", {}).get(\"val\", [])\n    if val_cwa:\n        print(f\"  best validation complexity-weighted accuracy: {max(val_cwa):.4f}\")\n\n    # plain validation accuracy from stored predictions / ground-truth\n    preds = dct.get(\"predictions\", [])\n    trues = dct.get(\"ground_truth\", [])\n    if preds and trues:\n        print(f\"  final validation accuracy: {accuracy(preds, trues):.4f}\")\n", ""], "parse_term_out": ["", "['\\nDataset: SPR_BENCH', '\\n', 'train loss: 3.0171', '\\n', 'validation loss:\n1.1178', '\\n', 'validation shape weighted accuracy: 0.393', '\\n', 'validation\ncolor weighted accuracy: 0.393', '\\n', 'validation complexity weighted accuracy:\n0.393', '\\n', 'test shape weighted accuracy: 0.392', '\\n', 'test color weighted\naccuracy: 0.384', '\\n', 'test complexity weighted accuracy: 0.388', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH', '\\n', '  final training loss: 0.1765', '\\n', '  best validation\nloss: 0.1760', '\\n', '  best validation complexity-weighted accuracy: 0.9457',\n'\\n', '  final validation accuracy: 0.9488', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  final training loss: 0.1772', '\\n', '  best validation\nloss: 0.1755', '\\n', '  best validation complexity-weighted accuracy: 0.9453',\n'\\n', '  final validation accuracy: 0.9484', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  final training loss: 0.1763', '\\n', '  best validation\nloss: 0.1763', '\\n', '  best validation complexity-weighted accuracy: 0.9448',\n'\\n', '  final validation accuracy: 0.9480', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  final training loss: 0.1785', '\\n', '  best validation\nloss: 0.1773', '\\n', '  best validation complexity-weighted accuracy: 0.9448',\n'\\n', '  final validation accuracy: 0.9480', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
