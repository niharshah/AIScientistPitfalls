<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[1, 7], [1, 6], [1, 4], [1, 5]], "layout": [[0.14285714285714285, 0.0], [0.42857142857142855, 0.0], [0.7142857142857143, 0.0], [1.0, 0.0], [0.0, 1.0], [0.2857142857142857, 1.0], [0.5714285714285714, 1.0], [0.8571428571428571, 1.0]], "plan": ["We start by loading the SPR_BENCH splits with the provided helper, build a\nsimple vocabulary from training tokens, and transform every sequence into a\nfixed-length list of token IDs. 0 is reserved for PAD, 1 for UNK.  A small Bi-\nGRU encoder with an average-pooled output is used for classification.  After\neach epoch we evaluate on the dev set, computing Shape-Weighted Accuracy (SWA),\nColor-Weighted Accuracy (CWA) and their harmonic mean (SCHM), printing and\nstoring them together with the validation loss.  The script respects all\nGPU/CPU, saving and data-logging requirements, and stores metrics in\n\u201c./working/experiment_data.npy\u201d.  Ten quick epochs give us a clean, reproducible\nbaseline that we will later augment with contrastive pre-training.", "We first load the SPR_BENCH splits with the helper in SPR.py, build a token-\nlevel vocabulary from all sequences, and encode each example as an integer array\npadded per batch.   A light Bi-GRU encoder (embedding \u2192 bi-GRU \u2192 projection)\nyields fixed-width representations.   During self-supervised pre-training every\nsequence is turned into two augmented \u201cviews\u201d through random token drop/masking;\na SimCLR-style NT-Xent loss (temperature = 0.5) is minimized for a few epochs.\nAfterwards a small linear classifier is attached on top of the encoder and the\nwhole network is fine-tuned with cross-entropy on the training labels.   We\nevaluate after every epoch on the dev split, computing Shape-Weighted Accuracy\n(SWA), Color-Weighted Accuracy (CWA) and their harmonic mean SCHM, printing and\nstoring them together with losses.   All tensors and models are moved to GPU\nwhen available, and metrics/losses are stored in experiment_data then saved to\n\u201c./working/experiment_data.npy\u201d.   Finally a simple learning-curve plot (loss\nvs. epoch) is saved for quick visual inspection.", "Here we build a minimal baseline that (1) tokenises SPR sequences, (2) performs\none epoch of very light SimCLR-style contrastive pre-training using random\ntoken-mask augmentation, (3) fine-tunes the same encoder with a classification\nhead, and (4) reports SWA, CWA and their harmonic mean (SCHM) on the dev set.\nThe model is a tiny Transformer encoder so training finishes in minutes on\nCPU/GPU. All metrics, losses and predictions are stored in\n./working/experiment_data.npy for later analysis.", "We start by loading SPR_BENCH with the provided helper, build a vocabulary over\nall unique shape-color tokens, and turn each sequence into an index list.  A\nsmall PyTorch Dataset / DataLoader pair pads these lists to the longest length\nin a batch.  The model is a light baseline: token embeddings \u2192 mean-pooling \u2192\ntwo-layer MLP \u2192 logits, trained with cross-entropy.  After every epoch we\nevaluate on the dev split, computing SWA, CWA and their harmonic mean SCHM;\nthese together with losses are stored in the experiment_data dict and printed.\nEverything (model, data, loss, tensors) is moved to GPU when available.  At the\nend we also evaluate on the held-out test split and save predictions, metrics\nand losses into ./working/experiment_data.npy for later analysis.  This gives a\nclean, runnable first baseline that respects all device-handling, logging and\nsaving requirements while being simple enough to finish quickly.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom datasets import DatasetDict\nfrom typing import List, Dict\nfrom datetime import datetime\n\n# ---------------- working dir -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- device handling --------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- load SPR_BENCH ---------------------------------------------\nfrom SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if needed\ndsets: DatasetDict = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n# ---------------- vocabulary --------------------------------------------------\nPAD, UNK = 0, 1\ntoken_freq = {}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        token_freq[tok] = token_freq.get(tok, 0) + 1\nvocab = {tok: idx + 2 for idx, tok in enumerate(token_freq)}  # +2 to skip PAD/UNK\nvocab_size = len(vocab) + 2\nprint(f\"vocab_size={vocab_size}\")\n\n\ndef encode(seq: str, max_len: int = 50) -> List[int]:\n    ids = [vocab.get(tok, UNK) for tok in seq.strip().split()][:max_len]\n    if len(ids) < max_len:\n        ids += [PAD] * (max_len - len(ids))\n    return ids\n\n\nmax_len = 50\nfor split in dsets.keys():\n    dsets[split] = dsets[split].map(\n        lambda ex: {\"input_ids\": encode(ex[\"sequence\"], max_len)}, remove_columns=[]\n    )\n    dsets[split].set_format(type=\"torch\", columns=[\"input_ids\", \"label\"])\n\n\n# ---------------- dataloaders -------------------------------------------------\ndef collate(batch):\n    ids = torch.stack([b[\"input_ids\"] for b in batch])\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    return {\"input_ids\": ids, \"labels\": labels}\n\n\nbatch_size = 256\ntrain_loader = DataLoader(\n    dsets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    dsets[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------- model -------------------------------------------------------\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_sz, embed_dim=64, hid_dim=128, n_classes=None):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, embed_dim, padding_idx=PAD)\n        self.gru = nn.GRU(embed_dim, hid_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(2 * hid_dim, n_classes)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        _, h = self.gru(emb)  # h: (2, B, H)\n        h = torch.cat([h[0], h[1]], dim=-1)  # (B, 2H)\n        return self.fc(h)\n\n\nn_classes = len(set(dsets[\"train\"][\"label\"]))\nmodel = GRUClassifier(vocab_size, n_classes=n_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------------- experiment data store --------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n\n# ---------------- training loop ----------------------------------------------\ndef evaluate(loader):\n    model.eval()\n    y_true, y_pred, seqs = [], [], []\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(dim=-1).cpu().tolist()\n            y_pred.extend(preds)\n            y_true.extend(batch[\"labels\"].cpu().tolist())\n            # recover original sequences for metrics\n            seqs.extend(\n                [\n                    \" \".join(\n                        [\n                            list(vocab.keys())[id - 2] if id > 1 else \"\"\n                            for id in ids\n                            if id > 1\n                        ]\n                    )\n                    for ids in batch[\"input_ids\"].cpu().tolist()\n                ]\n            )\n    avg_loss = total_loss / len(loader.dataset)\n    swa = shape_weighted_accuracy(seqs, y_true, y_pred)\n    cwa = color_weighted_accuracy(seqs, y_true, y_pred)\n    schm = 2 * swa * cwa / (swa + cwa) if (swa + cwa) > 0 else 0.0\n    return avg_loss, swa, cwa, schm, y_true, y_pred\n\n\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"labels\"].size(0)\n    train_loss = running_loss / len(train_loader.dataset)\n\n    val_loss, swa, cwa, schm, y_true, y_pred = evaluate(dev_loader)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, SWA={swa:.3f}, CWA={cwa:.3f}, SCHM={schm:.3f}\"\n    )\n\n    ts = datetime.utcnow().isoformat()\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"epoch\": epoch, \"swa\": None, \"cwa\": None, \"schm\": None}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"swa\": swa, \"cwa\": cwa, \"schm\": schm}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        {\"epoch\": epoch, \"loss\": train_loss}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        {\"epoch\": epoch, \"loss\": val_loss}\n    )\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(ts)\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = y_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_true\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, math, time, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport matplotlib.pyplot as plt\n\n# -------------------------------------------------\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\n# Device handling (MUST)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------\n# ----- SPR helper functions (copied from SPR.py) --\nfrom datasets import load_dataset, DatasetDict\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# -------------------------------------------------\n# --------------- Dataset preparation -------------\ndata_root_candidates = [\n    pathlib.Path(\"SPR_BENCH\"),\n    pathlib.Path(\"./data/SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nspr_bench = None\nfor p in data_root_candidates:\n    if (p / \"train.csv\").exists():\n        spr_bench = load_spr_bench(p)\n        print(f\"Loaded data from {p}\")\n        break\nif spr_bench is None:\n    # fallback small synthetic dataset to keep script runnable\n    print(\"SPR_BENCH not found, generating tiny synthetic data.\")\n\n    def synth(n):\n        seqs, labels = [], []\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        for _ in range(n):\n            L = random.randint(4, 9)\n            seq = []\n            for _ in range(L):\n                seq.append(random.choice(shapes) + random.choice(colors))\n            seqs.append(\" \".join(seq))\n            labels.append(random.randint(0, 1))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    spr_bench = DatasetDict()\n    for split, n in [(\"train\", 500), (\"dev\", 100), (\"test\", 100)]:\n        spr_bench[split] = load_dataset(\n            \"json\", data_files={\"train\": None}, split=\"train\", data=synth(n)\n        )\n\n\n# Build vocabulary\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = {\"<PAD>\": 0, \"<MASK>\": 1}\nfor split in spr_bench.values():\n    for seq in split[\"sequence\"]:\n        for tok in tokenize(seq):\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nmask_id = vocab[\"<MASK>\"]\nprint(f\"Vocab size = {vocab_size}\")\n\n\n# -------------------------------------------------\n# --------- PyTorch datasets ----------------------\nclass SPRContrastiveDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _augment(self, toks, p=0.15):\n        out = []\n        for t in toks:\n            r = random.random()\n            if r < p:  # drop\n                continue\n            out.append(t)\n        if len(out) == 0:\n            out.append(random.choice(toks))\n        return out\n\n    def __getitem__(self, idx):\n        toks = tokenize(self.seqs[idx])\n        view1 = self._augment(toks)\n        view2 = self._augment(toks)\n        return view1, view2\n\n\nclass SPRClassifierDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return tokenize(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\n# Collate fns\ndef collate_contrastive(batch):\n    v1, v2 = zip(*batch)\n\n    def encode(list_tokens):\n        ids = [\n            torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n            for toks in list_tokens\n        ]\n        lens = [len(i) for i in ids]\n        ids = pad_sequence(ids, batch_first=True, padding_value=0)\n        return ids, torch.tensor(lens, dtype=torch.long)\n\n    ids1, len1 = encode(v1)\n    ids2, len2 = encode(v2)\n    return {\n        \"ids1\": ids1.to(device),\n        \"len1\": len1.to(device),\n        \"ids2\": ids2.to(device),\n        \"len2\": len2.to(device),\n    }\n\n\ndef collate_classifier(batch):\n    toks, labels, raw_seq = zip(*batch)\n    ids = [torch.tensor([vocab[t] for t in t], dtype=torch.long) for t in toks]\n    lens = [len(i) for i in ids]\n    ids = pad_sequence(ids, batch_first=True, padding_value=0)\n    return {\n        \"ids\": ids.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"label\": torch.tensor(labels, dtype=torch.long).to(device),\n        \"sequence\": raw_seq,\n    }\n\n\n# DataLoaders\npretrain_loader = DataLoader(\n    SPRContrastiveDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\ntrain_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_classifier,\n)\ndev_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"dev\"]),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate_classifier,\n)\n\n\n# -------------------------------------------------\n# --------- Model definition ----------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(hidden * 2, hidden)\n\n    def forward(self, ids, lens):\n        x = self.emb(ids)\n        packed = pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)  # (B, 2H)\n        z = self.proj(h)\n        return z\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.head = nn.Linear(encoder.proj.out_features, num_classes)\n\n    def forward(self, ids, lens):\n        z = self.encoder(ids, lens)\n        return self.head(z)\n\n\n# Instantiate\nencoder = Encoder(vocab).to(device)\nclf_model = Classifier(encoder, num_classes=len(set(spr_bench[\"train\"][\"label\"]))).to(\n    device\n)\n\n\n# -------------------------------------------------\n# --------- Contrastive loss ----------------------\ndef nt_xent_loss(z1, z2, temp=0.5):\n    z1 = nn.functional.normalize(z1, dim=1)\n    z2 = nn.functional.normalize(z2, dim=1)\n    N = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2N x D\n    sim = torch.matmul(z, z.T) / temp  # 2N x 2N\n    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim.masked_fill_(mask, -9e15)\n    targets = torch.arange(N, 2 * N, device=z.device)\n    logits_12 = sim[:N, N:]  # positives across views\n    logits_21 = sim[N:, :N]\n    denom_12 = torch.logsumexp(sim[:N], dim=1)\n    denom_21 = torch.logsumexp(sim[N:], dim=1)\n    loss = (-logits_12.diag() + denom_12 + -logits_21.diag() + denom_21).mean() * 0.5\n    return loss\n\n\n# -------------------------------------------------\n# --------- Training utilities --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"SWA\": [], \"CWA\": [], \"SCHM\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --- Pretraining ---------------------------------\noptimizer_pt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npretrain_epochs = 2\nfor epoch in range(1, pretrain_epochs + 1):\n    encoder.train()\n    epoch_loss = 0.0\n    for batch in pretrain_loader:\n        optimizer_pt.zero_grad()\n        z1 = encoder(batch[\"ids1\"], batch[\"len1\"])\n        z2 = encoder(batch[\"ids2\"], batch[\"len2\"])\n        loss = nt_xent_loss(z1, z2)\n        loss.backward()\n        optimizer_pt.step()\n        epoch_loss += loss.item() * batch[\"ids1\"].size(0)\n    epoch_loss /= len(pretrain_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(epoch_loss)\n    print(f\"Pretrain Epoch {epoch}: loss={epoch_loss:.4f}\")\n\n# --- Fine-tuning ---------------------------------\noptimizer_ft = torch.optim.Adam(clf_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nft_epochs = 3\nfor epoch in range(1, ft_epochs + 1):\n    # train\n    clf_model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer_ft.zero_grad()\n        logits = clf_model(batch[\"ids\"], batch[\"len\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer_ft.step()\n        running_loss += loss.item() * batch[\"ids\"].size(0)\n    train_loss = running_loss / len(train_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # eval\n    clf_model.eval()\n    val_loss = 0.0\n    all_pred, all_true, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            logits = clf_model(batch[\"ids\"], batch[\"len\"])\n            loss = criterion(logits, batch[\"label\"])\n            val_loss += loss.item() * batch[\"ids\"].size(0)\n            preds = logits.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(batch[\"label\"].cpu().tolist())\n            all_seq.extend(batch[\"sequence\"])\n    val_loss /= len(dev_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    schm = 2 * swa * cwa / (swa + cwa) if (swa + cwa) > 0 else 0.0\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SWA\"].append(swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"CWA\"].append(cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SCHM\"].append(schm)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} | SWA={swa:.3f} CWA={cwa:.3f} SCHM={schm:.3f}\"\n    )\n\n# -------------------------------------------------\n# ---------- Save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- Visualization ------------------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Fine-tune loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nprint(\"Saved experiment data and plot in ./working/\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib, random, itertools, time, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import DatasetDict\n\n\n# ---------- copy utility funcs from SPR.py ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_shape_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_t, y_p):\n    w = [count_color_variety(s) for s in seqs]\n    c = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_t, y_p)]\n    return sum(c) / sum(w) if sum(w) > 0 else 0.0\n\n\n# ---------- load data ----------\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")\nspr = load_spr_bench(DATA_PATH)\n\ntrain_seqs = spr[\"train\"][\"sequence\"]\nlabels = list(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(labels)\n\n\n# ---------- vocab ----------\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = {\"[PAD]\": 0, \"[MASK]\": 1}\nfor s in train_seqs:\n    for tok in tokenize(s):\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\npad_id, mask_id = vocab[\"[PAD]\"], vocab[\"[MASK]\"]\n\n\ndef encode(seq):\n    return [vocab[t] for t in tokenize(seq)]\n\n\nmax_len = max(\n    len(tokenize(s))\n    for s in itertools.chain(spr[\"train\"][\"sequence\"], spr[\"dev\"][\"sequence\"])\n)\n\n\n# ---------- datasets ----------\nclass SPRDataset(torch.utils.data.Dataset):\n    def __init__(self, split):\n        self.seqs = spr[split][\"sequence\"]\n        self.labels = [label2id[l] for l in spr[split][\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input\": torch.tensor(encode(self.seqs[idx]), dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n            \"raw_seq\": self.seqs[idx],\n        }\n\n\ndef collate(batch):\n    lens = [len(x[\"input\"]) for x in batch]\n    maxl = max(lens)\n    inp = torch.full((len(batch), maxl), pad_id, dtype=torch.long)\n    for i, b in enumerate(batch):\n        inp[i, : lens[i]] = b[\"input\"]\n    lab = torch.stack([b[\"label\"] for b in batch])\n    raws = [b[\"raw_seq\"] for b in batch]\n    return {\"input_ids\": inp, \"labels\": lab, \"raw_seq\": raws}\n\n\ntrain_ds, dev_ds = SPRDataset(\"train\"), SPRDataset(\"dev\")\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------- model ----------\nclass Encoder(nn.Module):\n    def __init__(self, d_model=128, nhead=4, num_layers=2):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(max_len, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=256, dropout=0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        h = self.tok_emb(x) + self.pos_emb(positions)\n        h = self.transformer(h, src_key_padding_mask=(x == pad_id))\n        h = h.masked_fill((x == pad_id).unsqueeze(-1), 0)\n        return h.mean(1)  # simple average pooling\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_labels):\n        super().__init__()\n        self.encoder = encoder\n        self.head = nn.Linear(encoder.tok_emb.embedding_dim, num_labels)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        return self.head(z), z\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------- augmentation for contrastive ----------\ndef augment(seq_ids, mask_prob=0.15):\n    seq = seq_ids.clone()\n    mask = torch.rand(seq.shape, device=seq.device) < mask_prob\n    seq[mask] = mask_id\n    return seq\n\n\n# ---------- training utils ----------\ntemperature = 0.1\ncriterion_ce = nn.CrossEntropyLoss()\n\n\ndef contrastive_loss(z1, z2):\n    z1 = nn.functional.normalize(z1, dim=1)\n    z2 = nn.functional.normalize(z2, dim=1)\n    logits = torch.matmul(z1, z2.T) / temperature\n    targets = torch.arange(z1.size(0), device=z1.device)\n    return criterion_ce(logits, targets)\n\n\n# ---------- build model ----------\nenc = Encoder()\nmodel = Classifier(enc, num_labels).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n# ---------- 1. light contrastive pretrain ----------\nmodel.train()\nfor batch in train_loader:\n    x = batch[\"input_ids\"].to(device)\n    x1 = augment(x)\n    x2 = augment(x)\n    _, z1 = model(x1)\n    _, z2 = model(x2)\n    loss = contrastive_loss(z1, z2)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\nprint(\"Contrastive pre-training finished.\")\n\n# ---------- 2. supervised fine-tuning ----------\nn_epochs = 5\nfor epoch in range(1, n_epochs + 1):\n    model.train()\n    tr_losses = []\n    for batch in train_loader:\n        x = batch[\"input_ids\"].to(device)\n        y = batch[\"labels\"].to(device)\n        logits, _ = model(x)\n        loss = criterion_ce(logits, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tr_losses.append(loss.item())\n    model.eval()\n    val_losses = []\n    preds = []\n    gts = []\n    rawseq = []\n    with torch.no_grad():\n        for batch in dev_loader:\n            x = batch[\"input_ids\"].to(device)\n            y = batch[\"labels\"].to(device)\n            logits, _ = model(x)\n            val_losses.append(criterion_ce(logits, y).item())\n            preds.extend(logits.argmax(-1).cpu().tolist())\n            gts.extend(y.cpu().tolist())\n            rawseq.extend(batch[\"raw_seq\"])\n    swa = shape_weighted_accuracy(rawseq, gts, preds)\n    cwa = color_weighted_accuracy(rawseq, gts, preds)\n    schm = 2 * swa * cwa / (swa + cwa + 1e-8)\n    print(\n        f\"Epoch {epoch}: validation_loss = {np.mean(val_losses):.4f} | SWA={swa:.3f} CWA={cwa:.3f} SCHM={schm:.3f}\"\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"epoch\": epoch, \"loss\": np.mean(tr_losses)}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\n            \"epoch\": epoch,\n            \"loss\": np.mean(val_losses),\n            \"SWA\": swa,\n            \"CWA\": cwa,\n            \"SCHM\": schm,\n        }\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(np.mean(tr_losses))\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(np.mean(val_losses))\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ---------- save experiment data ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy\n\n# ---- required working dir scaffolding -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---- 1. Load dataset ---------------------------------------------------------------------------\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if necessary\nspr = load_spr_bench(DATA_PATH)\n\n\n# ---- 2. Build vocabulary & label map -----------------------------------------------------------\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nall_tokens = set()\nall_labels = set()\nfor ex in spr[\"train\"]:\n    all_tokens.update(tokenize(ex[\"sequence\"]))\n    all_labels.add(ex[\"label\"])\ntok2idx = {tok: i + 1 for i, tok in enumerate(sorted(all_tokens))}  # 0 reserved PAD\nidx2tok = {i: t for t, i in tok2idx.items()}\nlabel2idx = {lab: i for i, lab in enumerate(sorted(all_labels))}\nidx2label = {i: lab for lab, i in label2idx.items()}\n\nvocab_size = len(tok2idx) + 1\nnum_classes = len(label2idx)\nprint(f\"Vocab size={vocab_size-1}, classes={num_classes}\")\n\n\n# ---- 3. PyTorch Dataset ------------------------------------------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        seq = self.data[i][\"sequence\"]\n        label = self.data[i][\"label\"]\n        ids = [tok2idx[tok] for tok in tokenize(seq)]\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"label\": torch.tensor(label2idx[label], dtype=torch.long),\n            \"raw_seq\": seq,\n        }\n\n\ndef collate(batch):\n    lens = [len(b[\"input_ids\"]) for b in batch]\n    max_len = max(lens)\n    padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, b in enumerate(batch):\n        padded[i, : lens[i]] = b[\"input_ids\"]\n    labels = torch.stack([b[\"label\"] for b in batch])\n    raw_seq = [b[\"raw_seq\"] for b in batch]\n    return {\"input_ids\": padded, \"label\": labels, \"raw_seq\": raw_seq}\n\n\nbatch_size = 256\ndl_train = DataLoader(\n    SPRTorch(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\ndl_val = DataLoader(\n    SPRTorch(spr[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ndl_test = DataLoader(\n    SPRTorch(spr[\"test\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ---- 4. Model -----------------------------------------------------------------------------------\nclass MeanPoolMLP(nn.Module):\n    def __init__(self, vocab, n_cls, emb_dim=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=0)\n        self.net = nn.Sequential(\n            nn.Linear(emb_dim, hid), nn.ReLU(), nn.Linear(hid, n_cls)\n        )\n\n    def forward(self, x):\n        mask = (x != 0).unsqueeze(-1)\n        summed = (self.emb(x) * mask).sum(1)\n        denom = mask.sum(1).clamp(min=1)\n        mean = summed / denom\n        return self.net(mean)\n\n\nmodel = MeanPoolMLP(vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---- 5. Train -----------------------------------------------------------------------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    for batch in dl_train:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"label\"].size(0)\n    train_loss = running_loss / len(dl_train.dataset)\n\n    # ---- validation ------------------------------------------------------------------\n    model.eval()\n    val_loss, all_pred, all_true, all_seq = 0.0, [], [], []\n    with torch.no_grad():\n        for batch in dl_val:\n            batch_gpu = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits = model(batch_gpu[\"input_ids\"])\n            loss = criterion(logits, batch_gpu[\"label\"])\n            val_loss += loss.item() * batch_gpu[\"label\"].size(0)\n            preds = logits.argmax(1).cpu().numpy()\n            labels = batch_gpu[\"label\"].cpu().numpy()\n            all_pred.extend(preds)\n            all_true.extend(labels)\n            all_seq.extend(batch[\"raw_seq\"])\n    val_loss /= len(dl_val.dataset)\n    # Metrics\n    pred_labels = [idx2label[i] for i in all_pred]\n    true_labels = [idx2label[i] for i in all_true]\n    swa = shape_weighted_accuracy(all_seq, true_labels, pred_labels)\n    cwa = color_weighted_accuracy(all_seq, true_labels, pred_labels)\n    schm = 2 * swa * cwa / (swa + cwa + 1e-12)\n\n    # Logging\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(None)  # not computed\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"SWA\": swa, \"CWA\": cwa, \"SCHM\": schm}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"SWA={swa:.4f} CWA={cwa:.4f} SCHM={schm:.4f}\"\n    )\n\n# ---- 6. Final test evaluation -------------------------------------------------------------------\nmodel.eval()\nall_pred, all_true, all_seq = [], [], []\nwith torch.no_grad():\n    for batch in dl_test:\n        batch_gpu = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        logits = model(batch_gpu[\"input_ids\"])\n        preds = logits.argmax(1).cpu().numpy()\n        labels = batch_gpu[\"label\"].cpu().numpy()\n        all_pred.extend(preds)\n        all_true.extend(labels)\n        all_seq.extend(batch[\"raw_seq\"])\npred_labels = [idx2label[i] for i in all_pred]\ntrue_labels = [idx2label[i] for i in all_true]\nswa = shape_weighted_accuracy(all_seq, true_labels, pred_labels)\ncwa = color_weighted_accuracy(all_seq, true_labels, pred_labels)\nschm = 2 * swa * cwa / (swa + cwa + 1e-12)\nprint(f\"TEST  =>  SWA={swa:.4f} CWA={cwa:.4f} SCHM={schm:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = pred_labels\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = true_labels\nexperiment_data[\"SPR_BENCH\"][\"test_metrics\"] = {\"SWA\": swa, \"CWA\": cwa, \"SCHM\": schm}\n\n# ---- 7. Save all data ---------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, time, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport matplotlib.pyplot as plt\n\n# -------------------------------------------------\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\n# Device handling (MUST)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------\n# ----- SPR helper functions (copied from SPR.py) --\nfrom datasets import load_dataset, DatasetDict\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# -------------------------------------------------\n# --------------- Dataset preparation -------------\ndata_root_candidates = [\n    pathlib.Path(\"SPR_BENCH\"),\n    pathlib.Path(\"./data/SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nspr_bench = None\nfor p in data_root_candidates:\n    if (p / \"train.csv\").exists():\n        spr_bench = load_spr_bench(p)\n        print(f\"Loaded data from {p}\")\n        break\nif spr_bench is None:\n    # fallback small synthetic dataset to keep script runnable\n    print(\"SPR_BENCH not found, generating tiny synthetic data.\")\n\n    def synth(n):\n        seqs, labels = [], []\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        for _ in range(n):\n            L = random.randint(4, 9)\n            seq = []\n            for _ in range(L):\n                seq.append(random.choice(shapes) + random.choice(colors))\n            seqs.append(\" \".join(seq))\n            labels.append(random.randint(0, 1))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    spr_bench = DatasetDict()\n    for split, n in [(\"train\", 500), (\"dev\", 100), (\"test\", 100)]:\n        spr_bench[split] = load_dataset(\n            \"json\", data_files={\"train\": None}, split=\"train\", data=synth(n)\n        )\n\n\n# Build vocabulary\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = {\"<PAD>\": 0, \"<MASK>\": 1}\nfor split in spr_bench.values():\n    for seq in split[\"sequence\"]:\n        for tok in tokenize(seq):\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nmask_id = vocab[\"<MASK>\"]\nprint(f\"Vocab size = {vocab_size}\")\n\n\n# -------------------------------------------------\n# --------- PyTorch datasets ----------------------\nclass SPRContrastiveDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _augment(self, toks, p=0.15):\n        out = []\n        for t in toks:\n            r = random.random()\n            if r < p:  # drop\n                continue\n            out.append(t)\n        if len(out) == 0:\n            out.append(random.choice(toks))\n        return out\n\n    def __getitem__(self, idx):\n        toks = tokenize(self.seqs[idx])\n        view1 = self._augment(toks)\n        view2 = self._augment(toks)\n        return view1, view2\n\n\nclass SPRClassifierDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return tokenize(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\n# Collate fns\ndef collate_contrastive(batch):\n    v1, v2 = zip(*batch)\n\n    def encode(list_tokens):\n        ids = [\n            torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n            for toks in list_tokens\n        ]\n        lens = [len(i) for i in ids]\n        ids = pad_sequence(ids, batch_first=True, padding_value=0)\n        return ids, torch.tensor(lens, dtype=torch.long)\n\n    ids1, len1 = encode(v1)\n    ids2, len2 = encode(v2)\n    return {\n        \"ids1\": ids1.to(device),\n        \"len1\": len1.to(device),\n        \"ids2\": ids2.to(device),\n        \"len2\": len2.to(device),\n    }\n\n\ndef collate_classifier(batch):\n    toks, labels, raw_seq = zip(*batch)\n    ids = [torch.tensor([vocab[t] for t in t], dtype=torch.long) for t in toks]\n    lens = [len(i) for i in ids]\n    ids = pad_sequence(ids, batch_first=True, padding_value=0)\n    return {\n        \"ids\": ids.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"label\": torch.tensor(labels, dtype=torch.long).to(device),\n        \"sequence\": raw_seq,\n    }\n\n\n# DataLoaders\npretrain_loader = DataLoader(\n    SPRContrastiveDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\ntrain_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_classifier,\n)\ndev_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"dev\"]),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate_classifier,\n)\n\n\n# -------------------------------------------------\n# --------- Model definition ----------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(hidden * 2, hidden)\n\n    def forward(self, ids, lens):\n        x = self.emb(ids)\n        packed = pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)  # (B, 2H)\n        z = self.proj(h)\n        return z\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.head = nn.Linear(encoder.proj.out_features, num_classes)\n\n    def forward(self, ids, lens):\n        z = self.encoder(ids, lens)\n        return self.head(z)\n\n\n# Instantiate\nencoder = Encoder(vocab).to(device)\nclf_model = Classifier(encoder, num_classes=len(set(spr_bench[\"train\"][\"label\"]))).to(\n    device\n)\n\n\n# -------------------------------------------------\n# --------- Contrastive loss ----------------------\ndef nt_xent_loss(z1, z2, temp=0.5):\n    z1 = nn.functional.normalize(z1, dim=1)\n    z2 = nn.functional.normalize(z2, dim=1)\n    N = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2N x D\n    sim = torch.matmul(z, z.T) / temp  # 2N x 2N\n    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim.masked_fill_(mask, -9e15)\n    targets = torch.arange(N, 2 * N, device=z.device)\n    logits_12 = sim[:N, N:]  # positives across views\n    logits_21 = sim[N:, :N]\n    denom_12 = torch.logsumexp(sim[:N], dim=1)\n    denom_21 = torch.logsumexp(sim[N:], dim=1)\n    loss = (-logits_12.diag() + denom_12 + -logits_21.diag() + denom_21).mean() * 0.5\n    return loss\n\n\n# -------------------------------------------------\n# --------- Training utilities --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"SWA\": [], \"CWA\": [], \"SCHM\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --- Pretraining ---------------------------------\noptimizer_pt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npretrain_epochs = 2\nfor epoch in range(1, pretrain_epochs + 1):\n    encoder.train()\n    epoch_loss = 0.0\n    for batch in pretrain_loader:\n        optimizer_pt.zero_grad()\n        z1 = encoder(batch[\"ids1\"], batch[\"len1\"])\n        z2 = encoder(batch[\"ids2\"], batch[\"len2\"])\n        loss = nt_xent_loss(z1, z2)\n        loss.backward()\n        optimizer_pt.step()\n        epoch_loss += loss.item() * batch[\"ids1\"].size(0)\n    epoch_loss /= len(pretrain_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(epoch_loss)\n    print(f\"Pretrain Epoch {epoch}: loss={epoch_loss:.4f}\")\n\n# --- Fine-tuning ---------------------------------\noptimizer_ft = torch.optim.Adam(clf_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nft_epochs = 3\nfor epoch in range(1, ft_epochs + 1):\n    # train\n    clf_model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer_ft.zero_grad()\n        logits = clf_model(batch[\"ids\"], batch[\"len\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer_ft.step()\n        running_loss += loss.item() * batch[\"ids\"].size(0)\n    train_loss = running_loss / len(train_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # eval\n    clf_model.eval()\n    val_loss = 0.0\n    all_pred, all_true, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            logits = clf_model(batch[\"ids\"], batch[\"len\"])\n            loss = criterion(logits, batch[\"label\"])\n            val_loss += loss.item() * batch[\"ids\"].size(0)\n            preds = logits.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(batch[\"label\"].cpu().tolist())\n            all_seq.extend(batch[\"sequence\"])\n    val_loss /= len(dev_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    schm = 2 * swa * cwa / (swa + cwa) if (swa + cwa) > 0 else 0.0\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SWA\"].append(swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"CWA\"].append(cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SCHM\"].append(schm)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} | SWA={swa:.3f} CWA={cwa:.3f} SCHM={schm:.3f}\"\n    )\n\n# -------------------------------------------------\n# ---------- Save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- Visualization ------------------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Fine-tune loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nprint(\"Saved experiment data and plot in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, time, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport matplotlib.pyplot as plt\n\n# -------------------------------------------------\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\n# Device handling (MUST)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------\n# ----- SPR helper functions (copied from SPR.py) --\nfrom datasets import load_dataset, DatasetDict\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# -------------------------------------------------\n# --------------- Dataset preparation -------------\ndata_root_candidates = [\n    pathlib.Path(\"SPR_BENCH\"),\n    pathlib.Path(\"./data/SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nspr_bench = None\nfor p in data_root_candidates:\n    if (p / \"train.csv\").exists():\n        spr_bench = load_spr_bench(p)\n        print(f\"Loaded data from {p}\")\n        break\nif spr_bench is None:\n    # fallback small synthetic dataset to keep script runnable\n    print(\"SPR_BENCH not found, generating tiny synthetic data.\")\n\n    def synth(n):\n        seqs, labels = [], []\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        for _ in range(n):\n            L = random.randint(4, 9)\n            seq = []\n            for _ in range(L):\n                seq.append(random.choice(shapes) + random.choice(colors))\n            seqs.append(\" \".join(seq))\n            labels.append(random.randint(0, 1))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    spr_bench = DatasetDict()\n    for split, n in [(\"train\", 500), (\"dev\", 100), (\"test\", 100)]:\n        spr_bench[split] = load_dataset(\n            \"json\", data_files={\"train\": None}, split=\"train\", data=synth(n)\n        )\n\n\n# Build vocabulary\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = {\"<PAD>\": 0, \"<MASK>\": 1}\nfor split in spr_bench.values():\n    for seq in split[\"sequence\"]:\n        for tok in tokenize(seq):\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nmask_id = vocab[\"<MASK>\"]\nprint(f\"Vocab size = {vocab_size}\")\n\n\n# -------------------------------------------------\n# --------- PyTorch datasets ----------------------\nclass SPRContrastiveDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _augment(self, toks, p=0.15):\n        out = []\n        for t in toks:\n            r = random.random()\n            if r < p:  # drop\n                continue\n            out.append(t)\n        if len(out) == 0:\n            out.append(random.choice(toks))\n        return out\n\n    def __getitem__(self, idx):\n        toks = tokenize(self.seqs[idx])\n        view1 = self._augment(toks)\n        view2 = self._augment(toks)\n        return view1, view2\n\n\nclass SPRClassifierDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return tokenize(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\n# Collate fns\ndef collate_contrastive(batch):\n    v1, v2 = zip(*batch)\n\n    def encode(list_tokens):\n        ids = [\n            torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n            for toks in list_tokens\n        ]\n        lens = [len(i) for i in ids]\n        ids = pad_sequence(ids, batch_first=True, padding_value=0)\n        return ids, torch.tensor(lens, dtype=torch.long)\n\n    ids1, len1 = encode(v1)\n    ids2, len2 = encode(v2)\n    return {\n        \"ids1\": ids1.to(device),\n        \"len1\": len1.to(device),\n        \"ids2\": ids2.to(device),\n        \"len2\": len2.to(device),\n    }\n\n\ndef collate_classifier(batch):\n    toks, labels, raw_seq = zip(*batch)\n    ids = [torch.tensor([vocab[t] for t in t], dtype=torch.long) for t in toks]\n    lens = [len(i) for i in ids]\n    ids = pad_sequence(ids, batch_first=True, padding_value=0)\n    return {\n        \"ids\": ids.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"label\": torch.tensor(labels, dtype=torch.long).to(device),\n        \"sequence\": raw_seq,\n    }\n\n\n# DataLoaders\npretrain_loader = DataLoader(\n    SPRContrastiveDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\ntrain_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_classifier,\n)\ndev_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"dev\"]),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate_classifier,\n)\n\n\n# -------------------------------------------------\n# --------- Model definition ----------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(hidden * 2, hidden)\n\n    def forward(self, ids, lens):\n        x = self.emb(ids)\n        packed = pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)  # (B, 2H)\n        z = self.proj(h)\n        return z\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.head = nn.Linear(encoder.proj.out_features, num_classes)\n\n    def forward(self, ids, lens):\n        z = self.encoder(ids, lens)\n        return self.head(z)\n\n\n# Instantiate\nencoder = Encoder(vocab).to(device)\nclf_model = Classifier(encoder, num_classes=len(set(spr_bench[\"train\"][\"label\"]))).to(\n    device\n)\n\n\n# -------------------------------------------------\n# --------- Contrastive loss ----------------------\ndef nt_xent_loss(z1, z2, temp=0.5):\n    z1 = nn.functional.normalize(z1, dim=1)\n    z2 = nn.functional.normalize(z2, dim=1)\n    N = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2N x D\n    sim = torch.matmul(z, z.T) / temp  # 2N x 2N\n    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim.masked_fill_(mask, -9e15)\n    targets = torch.arange(N, 2 * N, device=z.device)\n    logits_12 = sim[:N, N:]  # positives across views\n    logits_21 = sim[N:, :N]\n    denom_12 = torch.logsumexp(sim[:N], dim=1)\n    denom_21 = torch.logsumexp(sim[N:], dim=1)\n    loss = (-logits_12.diag() + denom_12 + -logits_21.diag() + denom_21).mean() * 0.5\n    return loss\n\n\n# -------------------------------------------------\n# --------- Training utilities --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"SWA\": [], \"CWA\": [], \"SCHM\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --- Pretraining ---------------------------------\noptimizer_pt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npretrain_epochs = 2\nfor epoch in range(1, pretrain_epochs + 1):\n    encoder.train()\n    epoch_loss = 0.0\n    for batch in pretrain_loader:\n        optimizer_pt.zero_grad()\n        z1 = encoder(batch[\"ids1\"], batch[\"len1\"])\n        z2 = encoder(batch[\"ids2\"], batch[\"len2\"])\n        loss = nt_xent_loss(z1, z2)\n        loss.backward()\n        optimizer_pt.step()\n        epoch_loss += loss.item() * batch[\"ids1\"].size(0)\n    epoch_loss /= len(pretrain_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(epoch_loss)\n    print(f\"Pretrain Epoch {epoch}: loss={epoch_loss:.4f}\")\n\n# --- Fine-tuning ---------------------------------\noptimizer_ft = torch.optim.Adam(clf_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nft_epochs = 3\nfor epoch in range(1, ft_epochs + 1):\n    # train\n    clf_model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer_ft.zero_grad()\n        logits = clf_model(batch[\"ids\"], batch[\"len\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer_ft.step()\n        running_loss += loss.item() * batch[\"ids\"].size(0)\n    train_loss = running_loss / len(train_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # eval\n    clf_model.eval()\n    val_loss = 0.0\n    all_pred, all_true, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            logits = clf_model(batch[\"ids\"], batch[\"len\"])\n            loss = criterion(logits, batch[\"label\"])\n            val_loss += loss.item() * batch[\"ids\"].size(0)\n            preds = logits.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(batch[\"label\"].cpu().tolist())\n            all_seq.extend(batch[\"sequence\"])\n    val_loss /= len(dev_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    schm = 2 * swa * cwa / (swa + cwa) if (swa + cwa) > 0 else 0.0\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SWA\"].append(swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"CWA\"].append(cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SCHM\"].append(schm)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} | SWA={swa:.3f} CWA={cwa:.3f} SCHM={schm:.3f}\"\n    )\n\n# -------------------------------------------------\n# ---------- Save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- Visualization ------------------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Fine-tune loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nprint(\"Saved experiment data and plot in ./working/\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, time, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport matplotlib.pyplot as plt\n\n# -------------------------------------------------\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\n# Device handling (MUST)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------\n# ----- SPR helper functions (copied from SPR.py) --\nfrom datasets import load_dataset, DatasetDict\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\ndef count_shape_variety(sequence: str) -> int:\n    return len(set(tok[0] for tok in sequence.strip().split() if tok))\n\n\ndef count_color_variety(sequence: str) -> int:\n    return len(set(tok[1] for tok in sequence.strip().split() if len(tok) > 1))\n\n\ndef shape_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_shape_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\ndef color_weighted_accuracy(seqs, y_true, y_pred):\n    w = [count_color_variety(s) for s in seqs]\n    corr = [wi if yt == yp else 0 for wi, yt, yp in zip(w, y_true, y_pred)]\n    return sum(corr) / sum(w) if sum(w) else 0.0\n\n\n# -------------------------------------------------\n# --------------- Dataset preparation -------------\ndata_root_candidates = [\n    pathlib.Path(\"SPR_BENCH\"),\n    pathlib.Path(\"./data/SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nspr_bench = None\nfor p in data_root_candidates:\n    if (p / \"train.csv\").exists():\n        spr_bench = load_spr_bench(p)\n        print(f\"Loaded data from {p}\")\n        break\nif spr_bench is None:\n    # fallback small synthetic dataset to keep script runnable\n    print(\"SPR_BENCH not found, generating tiny synthetic data.\")\n\n    def synth(n):\n        seqs, labels = [], []\n        shapes = \"ABCD\"\n        colors = \"1234\"\n        for _ in range(n):\n            L = random.randint(4, 9)\n            seq = []\n            for _ in range(L):\n                seq.append(random.choice(shapes) + random.choice(colors))\n            seqs.append(\" \".join(seq))\n            labels.append(random.randint(0, 1))\n        return {\"id\": list(range(n)), \"sequence\": seqs, \"label\": labels}\n\n    spr_bench = DatasetDict()\n    for split, n in [(\"train\", 500), (\"dev\", 100), (\"test\", 100)]:\n        spr_bench[split] = load_dataset(\n            \"json\", data_files={\"train\": None}, split=\"train\", data=synth(n)\n        )\n\n\n# Build vocabulary\ndef tokenize(seq):\n    return seq.strip().split()\n\n\nvocab = {\"<PAD>\": 0, \"<MASK>\": 1}\nfor split in spr_bench.values():\n    for seq in split[\"sequence\"]:\n        for tok in tokenize(seq):\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nmask_id = vocab[\"<MASK>\"]\nprint(f\"Vocab size = {vocab_size}\")\n\n\n# -------------------------------------------------\n# --------- PyTorch datasets ----------------------\nclass SPRContrastiveDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def _augment(self, toks, p=0.15):\n        out = []\n        for t in toks:\n            r = random.random()\n            if r < p:  # drop\n                continue\n            out.append(t)\n        if len(out) == 0:\n            out.append(random.choice(toks))\n        return out\n\n    def __getitem__(self, idx):\n        toks = tokenize(self.seqs[idx])\n        view1 = self._augment(toks)\n        view2 = self._augment(toks)\n        return view1, view2\n\n\nclass SPRClassifierDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return tokenize(self.seqs[idx]), self.labels[idx], self.seqs[idx]\n\n\n# Collate fns\ndef collate_contrastive(batch):\n    v1, v2 = zip(*batch)\n\n    def encode(list_tokens):\n        ids = [\n            torch.tensor([vocab[t] for t in toks], dtype=torch.long)\n            for toks in list_tokens\n        ]\n        lens = [len(i) for i in ids]\n        ids = pad_sequence(ids, batch_first=True, padding_value=0)\n        return ids, torch.tensor(lens, dtype=torch.long)\n\n    ids1, len1 = encode(v1)\n    ids2, len2 = encode(v2)\n    return {\n        \"ids1\": ids1.to(device),\n        \"len1\": len1.to(device),\n        \"ids2\": ids2.to(device),\n        \"len2\": len2.to(device),\n    }\n\n\ndef collate_classifier(batch):\n    toks, labels, raw_seq = zip(*batch)\n    ids = [torch.tensor([vocab[t] for t in t], dtype=torch.long) for t in toks]\n    lens = [len(i) for i in ids]\n    ids = pad_sequence(ids, batch_first=True, padding_value=0)\n    return {\n        \"ids\": ids.to(device),\n        \"len\": torch.tensor(lens, dtype=torch.long).to(device),\n        \"label\": torch.tensor(labels, dtype=torch.long).to(device),\n        \"sequence\": raw_seq,\n    }\n\n\n# DataLoaders\npretrain_loader = DataLoader(\n    SPRContrastiveDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_contrastive,\n)\ntrain_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"train\"]),\n    batch_size=128,\n    shuffle=True,\n    collate_fn=collate_classifier,\n)\ndev_loader = DataLoader(\n    SPRClassifierDataset(spr_bench[\"dev\"]),\n    batch_size=256,\n    shuffle=False,\n    collate_fn=collate_classifier,\n)\n\n\n# -------------------------------------------------\n# --------- Model definition ----------------------\nclass Encoder(nn.Module):\n    def __init__(self, vocab, emb_dim=64, hidden=128):\n        super().__init__()\n        self.emb = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n        self.gru = nn.GRU(emb_dim, hidden, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(hidden * 2, hidden)\n\n    def forward(self, ids, lens):\n        x = self.emb(ids)\n        packed = pack_padded_sequence(\n            x, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h = self.gru(packed)\n        h = torch.cat([h[-2], h[-1]], dim=1)  # (B, 2H)\n        z = self.proj(h)\n        return z\n\n\nclass Classifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.head = nn.Linear(encoder.proj.out_features, num_classes)\n\n    def forward(self, ids, lens):\n        z = self.encoder(ids, lens)\n        return self.head(z)\n\n\n# Instantiate\nencoder = Encoder(vocab).to(device)\nclf_model = Classifier(encoder, num_classes=len(set(spr_bench[\"train\"][\"label\"]))).to(\n    device\n)\n\n\n# -------------------------------------------------\n# --------- Contrastive loss ----------------------\ndef nt_xent_loss(z1, z2, temp=0.5):\n    z1 = nn.functional.normalize(z1, dim=1)\n    z2 = nn.functional.normalize(z2, dim=1)\n    N = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)  # 2N x D\n    sim = torch.matmul(z, z.T) / temp  # 2N x 2N\n    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n    sim.masked_fill_(mask, -9e15)\n    targets = torch.arange(N, 2 * N, device=z.device)\n    logits_12 = sim[:N, N:]  # positives across views\n    logits_21 = sim[N:, :N]\n    denom_12 = torch.logsumexp(sim[:N], dim=1)\n    denom_21 = torch.logsumexp(sim[N:], dim=1)\n    loss = (-logits_12.diag() + denom_12 + -logits_21.diag() + denom_21).mean() * 0.5\n    return loss\n\n\n# -------------------------------------------------\n# --------- Training utilities --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"pretrain\": [], \"train\": [], \"val\": []},\n        \"metrics\": {\"SWA\": [], \"CWA\": [], \"SCHM\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --- Pretraining ---------------------------------\noptimizer_pt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\npretrain_epochs = 2\nfor epoch in range(1, pretrain_epochs + 1):\n    encoder.train()\n    epoch_loss = 0.0\n    for batch in pretrain_loader:\n        optimizer_pt.zero_grad()\n        z1 = encoder(batch[\"ids1\"], batch[\"len1\"])\n        z2 = encoder(batch[\"ids2\"], batch[\"len2\"])\n        loss = nt_xent_loss(z1, z2)\n        loss.backward()\n        optimizer_pt.step()\n        epoch_loss += loss.item() * batch[\"ids1\"].size(0)\n    epoch_loss /= len(pretrain_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"pretrain\"].append(epoch_loss)\n    print(f\"Pretrain Epoch {epoch}: loss={epoch_loss:.4f}\")\n\n# --- Fine-tuning ---------------------------------\noptimizer_ft = torch.optim.Adam(clf_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nft_epochs = 3\nfor epoch in range(1, ft_epochs + 1):\n    # train\n    clf_model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        optimizer_ft.zero_grad()\n        logits = clf_model(batch[\"ids\"], batch[\"len\"])\n        loss = criterion(logits, batch[\"label\"])\n        loss.backward()\n        optimizer_ft.step()\n        running_loss += loss.item() * batch[\"ids\"].size(0)\n    train_loss = running_loss / len(train_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    # eval\n    clf_model.eval()\n    val_loss = 0.0\n    all_pred, all_true, all_seq = [], [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            logits = clf_model(batch[\"ids\"], batch[\"len\"])\n            loss = criterion(logits, batch[\"label\"])\n            val_loss += loss.item() * batch[\"ids\"].size(0)\n            preds = logits.argmax(1).cpu().tolist()\n            all_pred.extend(preds)\n            all_true.extend(batch[\"label\"].cpu().tolist())\n            all_seq.extend(batch[\"sequence\"])\n    val_loss /= len(dev_loader.dataset)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    swa = shape_weighted_accuracy(all_seq, all_true, all_pred)\n    cwa = color_weighted_accuracy(all_seq, all_true, all_pred)\n    schm = 2 * swa * cwa / (swa + cwa) if (swa + cwa) > 0 else 0.0\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SWA\"].append(swa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"CWA\"].append(cwa)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"SCHM\"].append(schm)\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} | SWA={swa:.3f} CWA={cwa:.3f} SCHM={schm:.3f}\"\n    )\n\n# -------------------------------------------------\n# ---------- Save experiment data -----------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n# ---------- Visualization ------------------------\nplt.figure()\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train\")\nplt.plot(experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val\")\nplt.title(\"Fine-tune loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nprint(\"Saved experiment data and plot in ./working/\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 16, in <module>\\n    from SPR import load_spr_bench,\nshape_weighted_accuracy, color_weighted_accuracy\\nModuleNotFoundError: No module\nnamed \\'SPR\\'\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 510541.67\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 526499.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 468098.61\nexamples/s]', '\\n', 'Loaded data from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size = 18', '\\n', 'Pretrain Epoch 1:\nloss=4.0394', '\\n', 'Pretrain Epoch 2: loss=4.0177', '\\n', 'Epoch 1:\ntrain_loss=0.1949 val_loss=0.1242 | SWA=0.965 CWA=0.962 SCHM=0.963', '\\n',\n'Epoch 2: train_loss=0.0537 val_loss=0.0215 | SWA=0.993 CWA=0.993 SCHM=0.993',\n'\\n', 'Epoch 3: train_loss=0.0125 val_loss=0.0061 | SWA=0.999 CWA=0.999\nSCHM=0.999', '\\n', 'Saved experiment data and plot in ./working/', '\\n',\n'Execution time: 8 seconds seconds (time limit is 30 minutes).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 53, in\n<module>\\n    spr = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 25, in load_spr_bench\\n\nd[\"train\"] = _load(\"train.csv\")\\n                 ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 17, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_22-24-\n43_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-\n3/SPR_BENCH/train.csv\\'\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 3, in <module>\\n\nfrom SPR import load_spr_bench, shape_weighted_accuracy,\ncolor_weighted_accuracy\\nModuleNotFoundError: No module named \\'SPR\\'\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 397487.12\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 407316.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 597317.54\nexamples/s]', '\\n', 'Loaded data from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size = 18', '\\n', 'Pretrain Epoch 1:\nloss=4.0424', '\\n', 'Pretrain Epoch 2: loss=4.0243', '\\n', 'Epoch 1:\ntrain_loss=0.1930 val_loss=0.1405 | SWA=0.958 CWA=0.956 SCHM=0.957', '\\n',\n'Epoch 2: train_loss=0.0573 val_loss=0.0239 | SWA=0.993 CWA=0.993 SCHM=0.993',\n'\\n', 'Epoch 3: train_loss=0.0144 val_loss=0.0074 | SWA=0.997 CWA=0.997\nSCHM=0.997', '\\n', 'Saved experiment data and plot in ./working/', '\\n',\n'Execution time: 9 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 270203.67\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 154288.57\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 506087.82\nexamples/s]', '\\n', 'Loaded data from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size = 18', '\\n', 'Pretrain Epoch 1:\nloss=4.0459', '\\n', 'Pretrain Epoch 2: loss=4.0195', '\\n', 'Epoch 1:\ntrain_loss=0.1833 val_loss=0.0982 | SWA=0.967 CWA=0.967 SCHM=0.967', '\\n',\n'Epoch 2: train_loss=0.0393 val_loss=0.0128 | SWA=0.996 CWA=0.996 SCHM=0.996',\n'\\n', 'Epoch 3: train_loss=0.0087 val_loss=0.0102 | SWA=0.996 CWA=0.997\nSCHM=0.996', '\\n', 'Saved experiment data and plot in ./working/', '\\n',\n'Execution time: 8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 20000 examples [00:00, 400540.89\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 5000 examples [00:00, 488323.01\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 10000 examples [00:00, 636165.69\nexamples/s]', '\\n', 'Loaded data from /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size = 18', '\\n', 'Pretrain Epoch 1:\nloss=4.0378', '\\n', 'Pretrain Epoch 2: loss=4.0200', '\\n', 'Epoch 1:\ntrain_loss=0.1994 val_loss=0.1011 | SWA=0.974 CWA=0.972 SCHM=0.973', '\\n',\n'Epoch 2: train_loss=0.0473 val_loss=0.0294 | SWA=0.993 CWA=0.993 SCHM=0.993',\n'\\n', 'Epoch 3: train_loss=0.0132 val_loss=0.0113 | SWA=0.998 CWA=0.998\nSCHM=0.998', '\\n', 'Saved experiment data and plot in ./working/', '\\n',\n'Execution time: 9 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution failed due to a 'ModuleNotFoundError: No module named 'SPR''. This\nindicates that the script could not locate the 'SPR' module, which is essential\nfor loading the SPR_BENCH dataset and performing related operations.   Proposed\nFix: Ensure that the 'SPR.py' file is located in the same directory as the\nscript being executed or adjust the Python path to include the directory\ncontaining 'SPR.py'. Alternatively, verify that the file is correctly named as\n'SPR.py' and not something else.", "", "The execution failed due to a FileNotFoundError. The script attempted to load\nthe dataset from a path ('/home/zxl240011/AI-\nScientist-v2/experiments/.../SPR_BENCH/train.csv') that does not exist. Ensure\nthat the dataset files (train.csv, dev.csv, test.csv) are correctly placed in\nthe specified directory or update the 'DATA_PATH' variable to point to the\ncorrect dataset location.", "The script failed to execute because the module 'SPR' could not be found. This\nindicates that the script relies on a local or external module named 'SPR',\nwhich is either missing or not properly installed in the environment. To fix\nthis, ensure that the 'SPR' module is available in the working directory or\ninstalled in the Python environment. Additionally, verify the PYTHONPATH to\ninclude the directory containing 'SPR.py' or adjust the import statement to\npoint to the correct location of the module.", "", "", "", ""], "exc_type": ["ModuleNotFoundError", null, "FileNotFoundError", "ModuleNotFoundError", null, null, null, null], "exc_info": [{"args": ["No module named 'SPR'"], "name": "SPR", "msg": "No module named 'SPR'"}, null, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/0-run/process_ForkProcess-3/SPR_BENCH/train.csv'"]}, {"args": ["No module named 'SPR'"], "name": "SPR", "msg": "No module named 'SPR'"}, null, null, null, null], "exc_stack": [[["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 16, "<module>", "from SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy"]], null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 53, "<module>", "spr = load_spr_bench(DATA_PATH)"], ["runfile.py", 25, "load_spr_bench", "d[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 17, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 3, "<module>", "from SPR import load_spr_bench, shape_weighted_accuracy, color_weighted_accuracy"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "The loss during the pretraining phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.017728, "best_value": 4.017728}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.012461, "best_value": 0.012461}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.006089, "best_value": 0.006089}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape-related factors.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.998663, "best_value": 0.998663}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by color-related factors.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.998719, "best_value": 0.998719}]}, {"metric_name": "SCHM score", "lower_is_better": false, "description": "The SCHM score for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.998691, "best_value": 0.998691}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "The loss during the pretraining phase of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.02428, "best_value": 4.02428}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss during the training phase of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.014444, "best_value": 0.014444}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during the validation phase of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.007433, "best_value": 0.007433}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by shape categories.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996919, "best_value": 0.996919}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "The accuracy weighted by color categories.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.997194, "best_value": 0.997194}]}, {"metric_name": "SCHM score", "lower_is_better": false, "description": "The SCHM score, a composite metric evaluating model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.997056, "best_value": 0.997056}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 4.019487, "best_value": 4.019487}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.008735, "best_value": 0.008735}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.010233, "best_value": 0.010233}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy for shape classification.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996396, "best_value": 0.996396}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy for color classification.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996583, "best_value": 0.996583}]}, {"metric_name": "SCHM score", "lower_is_better": false, "description": "SCHM score metric.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.99649, "best_value": 0.99649}]}]}, {"metric_names": [{"metric_name": "pretraining loss", "lower_is_better": true, "description": "Loss during the pretraining phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": 4.020039}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during the training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.013209, "best_value": null}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during the validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": 0.011277}]}, {"metric_name": "shape-weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy based on shape", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": 0.997558}]}, {"metric_name": "color-weighted accuracy", "lower_is_better": false, "description": "Weighted accuracy based on color", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": 0.997682}]}, {"metric_name": "SCHM score", "lower_is_better": false, "description": "Aggregated metric score for SCHM", "data": [{"dataset_name": "SPR_BENCH", "final_value": null, "best_value": 0.99762}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/loss_curve.png", "../../logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/SPR_BENCH_metric_curves.png"], [], [], ["../../logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/loss_curve.png", "../../logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/SPR_BENCH_metric_curves.png"], ["../../logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/loss_curve.png", "../../logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/SPR_BENCH_metric_curves.png"], ["../../logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/loss_curve.png", "../../logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/SPR_BENCH_metric_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_5457a31a5475479da083153a604447da/SPR_BENCH_aggregate_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_5457a31a5475479da083153a604447da/SPR_BENCH_aggregate_metric_curves.png"]], "plot_paths": [[], ["experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/loss_curve.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/SPR_BENCH_metric_curves.png"], [], [], ["experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/loss_curve.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/SPR_BENCH_metric_curves.png"], ["experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/loss_curve.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/SPR_BENCH_metric_curves.png"], ["experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/loss_curve.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/SPR_BENCH_loss_curves.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/SPR_BENCH_metric_curves.png"], ["experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_5457a31a5475479da083153a604447da/SPR_BENCH_aggregate_loss_curves.png", "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_5457a31a5475479da083153a604447da/SPR_BENCH_aggregate_metric_curves.png"]], "plot_analyses": [[], [{"analysis": "This plot depicts the fine-tune loss over epochs for both training and validation datasets. The loss decreases consistently for both datasets, with the validation loss being lower than the training loss throughout the epochs. This indicates that the model is learning effectively during fine-tuning and there is no sign of overfitting at this stage. The convergence of the two curves suggests that the model might generalize well on unseen data.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/loss_curve.png"}, {"analysis": "This plot compares the loss during pre-training and fine-tuning phases. The pre-training loss remains constant at a high value, while the training and validation losses during fine-tuning decrease steadily over epochs. This behavior suggests that the pre-trained model starts with a high loss but fine-tunes effectively to adapt to the SPR_BENCH dataset. The sharp decline in fine-tuning losses highlights the importance of this phase in improving model performance.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the validation metrics (SWA, CWA, and SCHM) across epochs. All three metrics improve consistently, approaching a score of 1.0 by the final epoch. This demonstrates the model's increasing ability to correctly classify symbolic sequences while accounting for shape and color variations. The alignment of the three curves indicates that the model performs uniformly well across the different evaluation criteria.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_9d5e551b878c4fd5aef7531126758612_proc_2983500/SPR_BENCH_metric_curves.png"}], [], [], [{"analysis": "This plot shows the fine-tuning loss for both training and validation datasets over two epochs. The loss values decrease steadily for both datasets, indicating successful training. The validation loss is slightly lower than the training loss, which suggests that the model generalizes well and is not overfitting. However, the small number of epochs may not fully reveal potential overfitting or underfitting trends. Further training over more epochs is recommended to confirm these observations.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/loss_curve.png"}, {"analysis": "This plot compares the loss curves for pre-training and fine-tuning phases over three epochs. The pre-training loss remains constant at a high value, likely because the pre-training phase is not actively being optimized during these epochs. In contrast, the training and validation losses decrease significantly, demonstrating effective fine-tuning. The close alignment of training and validation losses suggests good generalization capability of the model during fine-tuning.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the validation performance of the model using three metrics: SWA, CWA, and SCHM, over three epochs. All three metrics show a consistent improvement and converge to near-perfect scores by the third epoch. This indicates that the model is learning effectively and achieving high accuracy across different evaluation criteria. The close alignment of the metrics suggests that the model performs uniformly well across these different aspects of the task.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/SPR_BENCH_metric_curves.png"}], [{"analysis": "The plot shows the fine-tuning loss for both training and validation sets over three epochs. The training loss decreases rapidly, indicating that the model is learning effectively. The validation loss also decreases in tandem, suggesting that the model generalizes well to unseen data without overfitting. The convergence of both curves towards zero is a positive indicator of the model's performance during fine-tuning.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/loss_curve.png"}, {"analysis": "This plot compares the loss curves of pre-training, training, and validation phases. The pre-training loss remains constant, likely because pre-training was completed before this phase. The training and validation losses decrease steadily, with minimal divergence between them, implying that the fine-tuning process is stable and effective. The gap between pre-training and fine-tuning losses suggests that fine-tuning significantly improves task-specific performance.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/SPR_BENCH_loss_curves.png"}, {"analysis": "The plot displays the validation metrics (SWA, CWA, and SCHM) over three epochs. All three metrics show a consistent improvement, with scores approaching 1.0, indicating near-perfect performance. The alignment of the three curves suggests that the model performs uniformly well across different metrics, showcasing its robustness and effectiveness in the SPR task.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/SPR_BENCH_metric_curves.png"}], [{"analysis": "The plot shows the fine-tuning loss for both training and validation datasets over two epochs. The loss decreases steadily for both datasets, indicating that the model is learning effectively during fine-tuning. The validation loss is consistently lower than the training loss, suggesting that the model generalizes well to unseen data and is not overfitting.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/loss_curve.png"}, {"analysis": "This plot compares the loss curves for pre-training, training, and validation phases over three epochs. The pre-training loss remains constant and high, indicating that it is not being optimized further during this phase. In contrast, both the training and validation losses decrease steadily and converge to near-zero values, demonstrating that the fine-tuning phase effectively improves the model's performance on the SPR_BENCH dataset.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the validation metrics (SWA, CWA, and SCHM) over three epochs. All three metrics improve steadily and converge to approximately 0.995, indicating that the model achieves high accuracy and robustness in recognizing symbolic patterns. The close alignment of the three metrics suggests that the model performs consistently across different evaluation criteria.", "plot_path": "experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/SPR_BENCH_metric_curves.png"}], []], "vlm_feedback_summary": ["[]", "The plots collectively show that the fine-tuning process is effective in\nreducing loss and improving validation metrics. The model demonstrates strong\ngeneralization capabilities and consistent performance across different\nevaluation metrics (SWA, CWA, and SCHM).", "[]", "[]", "The plots provide clear insights into the model's training and validation\nperformance. The decreasing loss curves indicate effective training and fine-\ntuning, while the validation metrics demonstrate strong performance across\nevaluation criteria. The results suggest that the proposed approach is\nsuccessfully improving model performance on the SPR task.", "The plots collectively indicate that the context-aware contrastive learning\nframework is effective. The model demonstrates steady improvements in loss and\nvalidation metrics, with no signs of overfitting. The results suggest that the\nproposed approach is on track to surpass the SOTA benchmarks for the SPR task.", "The plots indicate that the context-aware contrastive learning approach is\neffective for the SPR task. The fine-tuning phase results in decreasing loss and\nhigh validation performance, with metrics converging to near-perfect scores,\ndemonstrating the robustness and generalization of the model.", "[]"], "exec_time": [0.4271821975708008, 8.672101020812988, 0.4410672187805176, 0.002359628677368164, 9.062712669372559, 8.40219235420227, 9.028416872024536, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], ["['SPR_BENCH']"], [], [], ["['All Datasets']"], ["[\"experiment_data\"]"], ["['SPR_BENCH']"], []], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper: safe fetch\ndef get(dic, *keys, default=None):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic if dic else default\n\n\n# -------------------- iterate datasets ------------\nfor dname, dct in experiment_data.items():\n    # -------- loss curves -----------\n    try:\n        plt.figure()\n        # plot only if series exist\n        for tag in [\"pretrain\", \"train\", \"val\"]:\n            series = get(dct, \"losses\", tag, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=tag)\n        plt.title(f\"{dname} Loss Curves\\nLeft: Pre-training vs Fine-tuning\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- metric curves ----------\n    try:\n        plt.figure()\n        plotted = False\n        for metric in [\"SWA\", \"CWA\", \"SCHM\"]:\n            series = get(dct, \"metrics\", metric, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=metric)\n                plotted = True\n        if plotted:\n            plt.title(f\"{dname} Validation Metrics\\nLeft: SWA, CWA, Right: SCHM\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_metric_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- print final metrics ----\n    final_metrics = {\n        m: get(dct, \"metrics\", m, default=[None])[-1]\n        for m in [\"SWA\", \"CWA\", \"SCHM\"]\n        if get(dct, \"metrics\", m)\n    }\n    print(f\"Final metrics for {dname}: {final_metrics}\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper: safe fetch\ndef get(dic, *keys, default=None):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic if dic else default\n\n\n# -------------------- iterate datasets ------------\nfor dname, dct in experiment_data.items():\n    # -------- loss curves -----------\n    try:\n        plt.figure()\n        # plot only if series exist\n        for tag in [\"pretrain\", \"train\", \"val\"]:\n            series = get(dct, \"losses\", tag, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=tag)\n        plt.title(f\"{dname} Loss Curves\\nLeft: Pre-training vs Fine-tuning\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- metric curves ----------\n    try:\n        plt.figure()\n        plotted = False\n        for metric in [\"SWA\", \"CWA\", \"SCHM\"]:\n            series = get(dct, \"metrics\", metric, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=metric)\n                plotted = True\n        if plotted:\n            plt.title(f\"{dname} Validation Metrics\\nLeft: SWA, CWA, Right: SCHM\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_metric_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- print final metrics ----\n    final_metrics = {\n        m: get(dct, \"metrics\", m, default=[None])[-1]\n        for m in [\"SWA\", \"CWA\", \"SCHM\"]\n        if get(dct, \"metrics\", m)\n    }\n    print(f\"Final metrics for {dname}: {final_metrics}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper: safe fetch\ndef get(dic, *keys, default=None):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic if dic else default\n\n\n# -------------------- iterate datasets ------------\nfor dname, dct in experiment_data.items():\n    # -------- loss curves -----------\n    try:\n        plt.figure()\n        # plot only if series exist\n        for tag in [\"pretrain\", \"train\", \"val\"]:\n            series = get(dct, \"losses\", tag, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=tag)\n        plt.title(f\"{dname} Loss Curves\\nLeft: Pre-training vs Fine-tuning\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- metric curves ----------\n    try:\n        plt.figure()\n        plotted = False\n        for metric in [\"SWA\", \"CWA\", \"SCHM\"]:\n            series = get(dct, \"metrics\", metric, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=metric)\n                plotted = True\n        if plotted:\n            plt.title(f\"{dname} Validation Metrics\\nLeft: SWA, CWA, Right: SCHM\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_metric_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- print final metrics ----\n    final_metrics = {\n        m: get(dct, \"metrics\", m, default=[None])[-1]\n        for m in [\"SWA\", \"CWA\", \"SCHM\"]\n        if get(dct, \"metrics\", m)\n    }\n    print(f\"Final metrics for {dname}: {final_metrics}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper: safe fetch\ndef get(dic, *keys, default=None):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic if dic else default\n\n\n# -------------------- iterate datasets ------------\nfor dname, dct in experiment_data.items():\n    # -------- loss curves -----------\n    try:\n        plt.figure()\n        # plot only if series exist\n        for tag in [\"pretrain\", \"train\", \"val\"]:\n            series = get(dct, \"losses\", tag, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=tag)\n        plt.title(f\"{dname} Loss Curves\\nLeft: Pre-training vs Fine-tuning\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{dname}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- metric curves ----------\n    try:\n        plt.figure()\n        plotted = False\n        for metric in [\"SWA\", \"CWA\", \"SCHM\"]:\n            series = get(dct, \"metrics\", metric, default=None)\n            if series is not None and len(series):\n                plt.plot(range(1, len(series) + 1), series, label=metric)\n                plotted = True\n        if plotted:\n            plt.title(f\"{dname} Validation Metrics\\nLeft: SWA, CWA, Right: SCHM\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_metric_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric plot for {dname}: {e}\")\n        plt.close()\n\n    # -------- print final metrics ----\n    final_metrics = {\n        m: get(dct, \"metrics\", m, default=[None])[-1]\n        for m in [\"SWA\", \"CWA\", \"SCHM\"]\n        if get(dct, \"metrics\", m)\n    }\n    print(f\"Final metrics for {dname}: {final_metrics}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------- load all experiment files -----------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_c53ede312b8b4af7942f0a4209d14bd5_proc_2983499/experiment_data.npy\",\n    \"experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_7405fcb2acf44d2f98ecf2659de8a4fc_proc_2983502/experiment_data.npy\",\n    \"experiments/2025-08-15_22-24-43_context_aware_contrastive_learning_attempt_0/logs/0-run/experiment_results/experiment_b49cc689da4745b1a3b001353789e01d_proc_2983501/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nnum_runs = len(all_experiment_data)\nif num_runs == 0:\n    print(\"No experiment data could be loaded \u2013 aborting plotting.\")\n    exit()\n\n\n# ---------------- helper --------------------\ndef safe_get(dic, *keys):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic if dic else None\n\n\ndef aggregate_across_runs(key_chain):\n    \"\"\"Return list of np.arrays (one per run) following the key_chain\"\"\"\n    series_list = []\n    for exp in all_experiment_data:\n        val = safe_get(exp, *key_chain)\n        if isinstance(val, (list, tuple)):\n            val = np.asarray(val, dtype=np.float32)\n        if val is not None and val.size:\n            series_list.append(val)\n    return series_list  # may be []\n\n\n# ---------------- iterate over datasets ------------------\n# collect union of dataset names\ndataset_names = set()\nfor exp in all_experiment_data:\n    dataset_names.update(exp.keys())\n\nfor dname in dataset_names:\n    # ------------------ aggregated loss curves ---------------\n    try:\n        tags = [\"pretrain\", \"train\", \"val\"]\n        plotted_any = False\n        plt.figure()\n        for tag in tags:\n            series_per_run = aggregate_across_runs([dname, \"losses\", tag])\n            if len(series_per_run) >= 1:\n                # align length\n                min_len = min(len(s) for s in series_per_run)\n                trimmed = np.stack([s[:min_len] for s in series_per_run], axis=0)\n                mean = trimmed.mean(axis=0)\n                se = trimmed.std(axis=0, ddof=1) / np.sqrt(trimmed.shape[0])\n                epochs = np.arange(1, min_len + 1)\n                plt.plot(epochs, mean, label=f\"{tag} mean\")\n                plt.fill_between(epochs, mean - se, mean + se, alpha=0.25)\n                plotted_any = True\n        if plotted_any:\n            plt.title(\n                f\"{dname} Aggregated Loss Curves (mean \u00b1 SE)\\nAcross {num_runs} runs\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_aggregate_loss_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dname}: {e}\")\n        plt.close()\n\n    # ------------------ aggregated metric curves -------------\n    try:\n        metrics = [\"SWA\", \"CWA\", \"SCHM\"]\n        plotted_any = False\n        plt.figure()\n        for metric in metrics:\n            series_per_run = aggregate_across_runs([dname, \"metrics\", metric])\n            if len(series_per_run) >= 1:\n                min_len = min(len(s) for s in series_per_run)\n                trimmed = np.stack([s[:min_len] for s in series_per_run], axis=0)\n                mean = trimmed.mean(axis=0)\n                se = trimmed.std(axis=0, ddof=1) / np.sqrt(trimmed.shape[0])\n                epochs = np.arange(1, min_len + 1)\n                plt.plot(epochs, mean, label=f\"{metric} mean\")\n                plt.fill_between(epochs, mean - se, mean + se, alpha=0.25)\n                plotted_any = True\n        if plotted_any:\n            plt.title(\n                f\"{dname} Aggregated Validation Metrics (mean \u00b1 SE)\\nAcross {num_runs} runs\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Score\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname}_aggregate_metric_curves.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated metric plot for {dname}: {e}\")\n        plt.close()\n\n    # --------------- print final aggregated metrics ----------\n    final_stats = {}\n    for metric in [\"SWA\", \"CWA\", \"SCHM\"]:\n        final_vals = []\n        for exp in all_experiment_data:\n            vals = safe_get(exp, dname, \"metrics\", metric)\n            if vals:\n                final_vals.append(vals[-1])\n        if final_vals:\n            arr = np.asarray(final_vals, dtype=np.float32)\n            final_stats[metric] = (arr.mean(), arr.std(ddof=1))\n    if final_stats:\n        print(f\"{dname} final epoch metrics (mean \u00b1 std over {num_runs} runs):\")\n        for m, (mu, sd) in final_stats.items():\n            print(f\"  {m}: {mu:.4f} \u00b1 {sd:.4f}\")\n"], "plot_plan": [null, "The script loads the serialized experiment_data.npy, detects all datasets stored inside, and for each one generates up-to-date visual summaries. First, it builds a loss-curve figure that overlays pre-training, fine-tuning, and validation losses when they exist; the X-axis is epoch number (each series starts at 1). Next, it draws a metric-curve figure containing SWA, CWA, and SCHM across fine-tuning epochs. Each plot is wrapped in its own try-except block to guarantee figures are closed even on error and to keep the total number of figures reasonable (\u2264 5). The figures are titled with clear subtitles and saved to the mandatory working directory using descriptive file names. After plotting, the script prints the final value of every tracked metric so users can quickly inspect numerical results. Only data present in experiment_data.npy are accessed; nothing is fabricated. The code is concise, relies solely on basic matplotlib, and follows all provided directory, naming, and cleanup conventions.", null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["", "The script will locate the \u201cworking\u201d directory, load the saved\nexperiment_data.npy file, and convert it to a Python dict.   For each dataset\nkey (e.g., \u201cSPR_BENCH\u201d) it will compute the best value (minimum for losses,\nmaximum for accuracies) or the final value where requested, and print them with\nexplicit, readable names.   The code executes immediately at import time,\ncontains no `if __name__ == \"__main__\":` guard, and produces no plots\u2014only\nneatly formatted metric summaries.", "", "", "The script will locate the \u201cworking\u201d directory, load the saved\nexperiment_data.npy file, and convert it to a Python dict.   For each dataset\nkey (e.g., \u201cSPR_BENCH\u201d) it will compute the best value (minimum for losses,\nmaximum for accuracies) or the final value where requested, and print them with\nexplicit, readable names.   The code executes immediately at import time,\ncontains no `if __name__ == \"__main__\":` guard, and produces no plots\u2014only\nneatly formatted metric summaries.", "The script will locate the \u201cworking\u201d directory, load the saved\nexperiment_data.npy file, and convert it to a Python dict.   For each dataset\nkey (e.g., \u201cSPR_BENCH\u201d) it will compute the best value (minimum for losses,\nmaximum for accuracies) or the final value where requested, and print them with\nexplicit, readable names.   The code executes immediately at import time,\ncontains no `if __name__ == \"__main__\":` guard, and produces no plots\u2014only\nneatly formatted metric summaries.", "The script will locate the \u201cworking\u201d directory, load the saved\nexperiment_data.npy file, and convert it to a Python dict.   For each dataset\nkey (e.g., \u201cSPR_BENCH\u201d) it will compute the best value (minimum for losses,\nmaximum for accuracies) or the final value where requested, and print them with\nexplicit, readable names.   The code executes immediately at import time,\ncontains no `if __name__ == \"__main__\":` guard, and produces no plots\u2014only\nneatly formatted metric summaries.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper functions to pick best/final numbers\ndef best_min(values):\n    \"\"\"Return the minimum value (best for loss).\"\"\"\n    return min(values) if values else None\n\n\ndef best_max(values):\n    \"\"\"Return the maximum value (best for accuracies/metrics).\"\"\"\n    return max(values) if values else None\n\n\ndef final_val(values):\n    \"\"\"Return the last value in a list (final epoch).\"\"\"\n    return values[-1] if values else None\n\n\n# ------------------------------------------------------------------\n# Iterate through datasets and print metrics\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # Dataset title\n\n    # ----- Losses -----\n    losses = content.get(\"losses\", {})\n    pretrain_losses = losses.get(\"pretrain\", [])\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n\n    if pretrain_losses:\n        print(f\"best pretraining loss: {best_min(pretrain_losses):.6f}\")\n    if train_losses:\n        print(f\"final training loss: {final_val(train_losses):.6f}\")\n    if val_losses:\n        print(f\"best validation loss: {best_min(val_losses):.6f}\")\n\n    # ----- Additional metrics -----\n    metrics = content.get(\"metrics\", {})\n    swa = metrics.get(\"SWA\", [])\n    cwa = metrics.get(\"CWA\", [])\n    schm = metrics.get(\"SCHM\", [])\n\n    if swa:\n        print(f\"best shape-weighted accuracy: {best_max(swa):.6f}\")\n    if cwa:\n        print(f\"best color-weighted accuracy: {best_max(cwa):.6f}\")\n    if schm:\n        print(f\"best SCHM score: {best_max(schm):.6f}\")\n", "", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper functions to pick best/final numbers\ndef best_min(values):\n    \"\"\"Return the minimum value (best for loss).\"\"\"\n    return min(values) if values else None\n\n\ndef best_max(values):\n    \"\"\"Return the maximum value (best for accuracies/metrics).\"\"\"\n    return max(values) if values else None\n\n\ndef final_val(values):\n    \"\"\"Return the last value in a list (final epoch).\"\"\"\n    return values[-1] if values else None\n\n\n# ------------------------------------------------------------------\n# Iterate through datasets and print metrics\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # Dataset title\n\n    # ----- Losses -----\n    losses = content.get(\"losses\", {})\n    pretrain_losses = losses.get(\"pretrain\", [])\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n\n    if pretrain_losses:\n        print(f\"best pretraining loss: {best_min(pretrain_losses):.6f}\")\n    if train_losses:\n        print(f\"final training loss: {final_val(train_losses):.6f}\")\n    if val_losses:\n        print(f\"best validation loss: {best_min(val_losses):.6f}\")\n\n    # ----- Additional metrics -----\n    metrics = content.get(\"metrics\", {})\n    swa = metrics.get(\"SWA\", [])\n    cwa = metrics.get(\"CWA\", [])\n    schm = metrics.get(\"SCHM\", [])\n\n    if swa:\n        print(f\"best shape-weighted accuracy: {best_max(swa):.6f}\")\n    if cwa:\n        print(f\"best color-weighted accuracy: {best_max(cwa):.6f}\")\n    if schm:\n        print(f\"best SCHM score: {best_max(schm):.6f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper functions to pick best/final numbers\ndef best_min(values):\n    \"\"\"Return the minimum value (best for loss).\"\"\"\n    return min(values) if values else None\n\n\ndef best_max(values):\n    \"\"\"Return the maximum value (best for accuracies/metrics).\"\"\"\n    return max(values) if values else None\n\n\ndef final_val(values):\n    \"\"\"Return the last value in a list (final epoch).\"\"\"\n    return values[-1] if values else None\n\n\n# ------------------------------------------------------------------\n# Iterate through datasets and print metrics\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # Dataset title\n\n    # ----- Losses -----\n    losses = content.get(\"losses\", {})\n    pretrain_losses = losses.get(\"pretrain\", [])\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n\n    if pretrain_losses:\n        print(f\"best pretraining loss: {best_min(pretrain_losses):.6f}\")\n    if train_losses:\n        print(f\"final training loss: {final_val(train_losses):.6f}\")\n    if val_losses:\n        print(f\"best validation loss: {best_min(val_losses):.6f}\")\n\n    # ----- Additional metrics -----\n    metrics = content.get(\"metrics\", {})\n    swa = metrics.get(\"SWA\", [])\n    cwa = metrics.get(\"CWA\", [])\n    schm = metrics.get(\"SCHM\", [])\n\n    if swa:\n        print(f\"best shape-weighted accuracy: {best_max(swa):.6f}\")\n    if cwa:\n        print(f\"best color-weighted accuracy: {best_max(cwa):.6f}\")\n    if schm:\n        print(f\"best SCHM score: {best_max(schm):.6f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper functions to pick best/final numbers\ndef best_min(values):\n    \"\"\"Return the minimum value (best for loss).\"\"\"\n    return min(values) if values else None\n\n\ndef best_max(values):\n    \"\"\"Return the maximum value (best for accuracies/metrics).\"\"\"\n    return max(values) if values else None\n\n\ndef final_val(values):\n    \"\"\"Return the last value in a list (final epoch).\"\"\"\n    return values[-1] if values else None\n\n\n# ------------------------------------------------------------------\n# Iterate through datasets and print metrics\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # Dataset title\n\n    # ----- Losses -----\n    losses = content.get(\"losses\", {})\n    pretrain_losses = losses.get(\"pretrain\", [])\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n\n    if pretrain_losses:\n        print(f\"best pretraining loss: {best_min(pretrain_losses):.6f}\")\n    if train_losses:\n        print(f\"final training loss: {final_val(train_losses):.6f}\")\n    if val_losses:\n        print(f\"best validation loss: {best_min(val_losses):.6f}\")\n\n    # ----- Additional metrics -----\n    metrics = content.get(\"metrics\", {})\n    swa = metrics.get(\"SWA\", [])\n    cwa = metrics.get(\"CWA\", [])\n    schm = metrics.get(\"SCHM\", [])\n\n    if swa:\n        print(f\"best shape-weighted accuracy: {best_max(swa):.6f}\")\n    if cwa:\n        print(f\"best color-weighted accuracy: {best_max(cwa):.6f}\")\n    if schm:\n        print(f\"best SCHM score: {best_max(schm):.6f}\")\n", ""], "parse_term_out": ["", "['\\nSPR_BENCH', '\\n', 'best pretraining loss: 4.017728', '\\n', 'final training\nloss: 0.012461', '\\n', 'best validation loss: 0.006089', '\\n', 'best shape-\nweighted accuracy: 0.998663', '\\n', 'best color-weighted accuracy: 0.998719',\n'\\n', 'best SCHM score: 0.998691', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "", "", "['\\nSPR_BENCH', '\\n', 'best pretraining loss: 4.024280', '\\n', 'final training\nloss: 0.014444', '\\n', 'best validation loss: 0.007433', '\\n', 'best shape-\nweighted accuracy: 0.996919', '\\n', 'best color-weighted accuracy: 0.997194',\n'\\n', 'best SCHM score: 0.997056', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'best pretraining loss: 4.019487', '\\n', 'final training\nloss: 0.008735', '\\n', 'best validation loss: 0.010233', '\\n', 'best shape-\nweighted accuracy: 0.996396', '\\n', 'best color-weighted accuracy: 0.996583',\n'\\n', 'best SCHM score: 0.996490', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nSPR_BENCH', '\\n', 'best pretraining loss: 4.020039', '\\n', 'final training\nloss: 0.013209', '\\n', 'best validation loss: 0.011277', '\\n', 'best shape-\nweighted accuracy: 0.997558', '\\n', 'best color-weighted accuracy: 0.997682',\n'\\n', 'best SCHM score: 0.997620', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
