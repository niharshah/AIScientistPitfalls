{"edges": [[0, 7], [0, 5], [0, 4], [0, 6]], "layout": [[0.3333333333333333, 0.0], [0.5555555555555556, 0.0], [0.7777777777777778, 0.0], [1.0, 0.0], [0.0, 1.0], [0.2222222222222222, 1.0], [0.4444444444444444, 1.0], [0.6666666666666666, 1.0]], "plan": ["We will build a simple, fully-interpretable baseline that treats every SPR\nsequence as a bag-of-character 1-grams and 2-grams, then trains a multinomial\nlogistic-regression model (one linear layer + soft-max) in PyTorch. Because the\nmodel weights directly correspond to n-gram importance, we can derive symbolic\nrules by keeping only the top-K highest-weight n-grams for each class and\nrecomputing the class scores with those few rules; Rule Fidelity is the\npercentage of samples whose class from the truncated-rule system matches the\nfull model\u2019s prediction. Training will run for a handful of epochs with Adam,\ntracking validation loss, accuracy and fidelity at every epoch, and saving\neverything (metrics, losses, predictions) into ./working for future analysis.\nAll tensors, model and optimizer strictly follow the GPU/CPU device rules. This\ngives a fast, transparent baseline that already yields explicit weighted n-gram\nrules and a quantitative interpretability score.", "We will build a lightweight character-level classifier (tiny embedding + mean-\npooling MLP) to obtain strong baseline accuracy, then train a shallow\nDecisionTree surrogate on n-gram bags of the same sequences; the tree\u2019s symbolic\nrules serve as our interpretable component.  Rule Fidelity is simply the\nagreement between the tree and the neural model on the validation split,\nguaranteeing a meaningful metric even at this preliminary stage.  All mandatory\nGPU handling, metric tracking, numpy logging and saving are included, and the\nscript runs end-to-end on a single file without entry-point guards.  The whole\npipeline\u2014data loading, tokenisation, PyTorch training, tree extraction, fidelity\ncomputation, metric persistence\u2014executes in well under 30 minutes on modest\nhardware while producing an easily inspectable set of rules via sklearn.  This\nprovides a clean, functioning foundation for later, more sophisticated rule-\nlearning architectures.  Now the code:", "This baseline trains a lightweight character-level GRU classifier on SPR_BENCH\nto obtain solid predictive performance.   Sequences are mapped to integer\nindices via a learned vocabulary; padding and packed sequences keep training\nefficient on GPU when available.   After several epochs we collect the model\u2019s\npredictions on the train split and fit a shallow decision tree on easy-to-\nextract character-count features.   Because the tree is forced to mimic the\nnetwork, its induced IF-THEN rules serve as an explicit symbolic surrogate;\ntheir agreement with the original network constitutes the Rule Fidelity metric.\nWe report validation loss, accuracy, and rule fidelity each epoch, and store all\nintermediate data in the prescribed experiment_data structure.   The whole\nscript is self-contained, runs in <30\u2009min, respects device handling rules, and\nsaves results under ./working.   This gives us a working end-to-end\npipeline\u2014from data loading to interpretable rule extraction\u2014on which we can\niterate with more sophisticated methods later.", "We will build a very lightweight baseline that still satisfies the\n\u201cinterpretable-rule\u201d requirement.  Character sequences are mapped to integers\nand padded to the maximal training-set length.  A tiny neural model (Embedding \u2192\nmean-pool \u2192 Linear) is trained with cross-entropy on GPU if available, giving us\na reasonably strong black-box predictor.  After training, we freeze the network\nand obtain its predictions for the complete training split; using those\npredictions as pseudo-labels, we train a scikit-learn DecisionTreeClassifier on\nsimple bag-of-characters counts.  The decision tree\u2019s if-then-else paths\nconstitute explicit symbolic rules, and we measure Rule Fidelity on the dev set\nby comparing tree outputs to the neural model\u2019s outputs.  Because the tree is\ndirectly fitted to mimic the network, fidelity should already be high and is a\nmeaningful \u201cexplanation faithfulness\u201d metric.  All losses and metrics are\ntracked epoch-wise, saved to ./working/experiment_data.npy, and validation loss\nis printed every epoch.  This provides a clean, runnable end-to-end pipeline\nthat we can later refine for better accuracy and more sophisticated rule\nextraction.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict\n\n# ---------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------- DATA -----------------------\n# loader supplied in prompt (copied here to keep single-file)\nfrom datasets import load_dataset\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ---------------- n-gram vectoriser ----------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        chars = list(s)\n        unis.update(chars)\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    idx = {tok: i for i, tok in enumerate(vocab)}\n    return idx\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    chars = list(seq)\n    for c in chars:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    return v\n\n\n# Build vocab on train only for fairness\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\n# Label encoding\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\n\n\n# -------------------- MODEL ------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = LogReg(num_feats, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# ----------------- EXPERIMENT STORE ---------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ----------------- TRAINING LOOP ------------------\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    acc = correct / total\n    loss = loss_sum / total\n    return acc, loss, torch.cat(all_logits)\n\n\nepochs = 10\ntop_k = 10  # number of n-grams kept per class for rule fidelity\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    seen = 0\n    correct = 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"y\"].size(0)\n        _, preds = torch.max(logits, 1)\n        correct += (preds == batch[\"y\"]).sum().item()\n        seen += batch[\"y\"].size(0)\n    train_loss = running_loss / seen\n    train_acc = correct / seen\n\n    val_acc, val_loss, val_logits = evaluate(dev_loader)\n\n    # ---------- Rule Fidelity ----------\n    W = model.linear.weight.detach().cpu().numpy()  # shape [C,F]\n    b = model.linear.bias.detach().cpu().numpy()\n    # truncate weights to top_k abs weight per class\n    W_trunc = np.zeros_like(W)\n    for c in range(num_classes):\n        idxs = np.argsort(-np.abs(W[c]))[:top_k]\n        W_trunc[c, idxs] = W[c, idxs]\n    # compute rule predictions on dev set\n    X_dev_tensor = torch.from_numpy(X_dev).float()\n    lin_full = torch.from_numpy((X_dev @ W.T) + b)\n    lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n    rule_pred = torch.argmax(lin_trunc, 1)\n    model_pred = torch.argmax(lin_full, 1)\n    rule_fid = (rule_pred == model_pred).float().mean().item()\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(rule_fid)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n    )\n\n# --------------- FINAL EVAL & SAVE -----------------\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\ntest_acc, test_loss, test_logits = evaluate(test_loader)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.argmax(test_logits, 1).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_test\nprint(f\"Test_acc={test_acc:.3f} test_loss={test_loss:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier, export_text\n\n# ----------------- working dir & global experiment store -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": {\"val_nn\": [], \"val_rule\": []},\n        \"ground_truth\": [],\n        \"timestamps\": [],\n    }\n}\n\n# ----------------- GPU / device handling ---------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------- load dataset ------------------------------------------\nfrom datasets import DatasetDict\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"./SPR_BENCH\")  # adjust if needed\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded SPR_BENCH:\", {k: len(v) for k, v in spr.items()})\n\n\n# ----------------- basic char-level vocab --------------------------------\ndef build_vocab(dataset):\n    chars = set()\n    for seq in dataset[\"sequence\"]:\n        chars.update(list(seq))\n    vocab = {c: i + 1 for i, c in enumerate(sorted(list(chars)))}  # 0 is pad\n    vocab[\"<PAD>\"] = 0\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\")\n\nmax_len = max(len(s) for s in spr[\"train\"][\"sequence\"])  # full length\n\n\ndef encode(seq):\n    ids = [vocab[c] for c in seq]\n    if len(ids) < max_len:\n        ids += [0] * (max_len - len(ids))\n    else:\n        ids = ids[:max_len]\n    return ids\n\n\n# ----------------- torch dataset ----------------------------------------\nclass SPRTorchSet(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(encode(self.seqs[idx]), dtype=torch.long)\n        y = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\"x\": x, \"y\": y}\n\n\ntrain_ds = SPRTorchSet(spr[\"train\"])\ndev_ds = SPRTorchSet(spr[\"dev\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False)\n\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# ----------------- simple neural model -----------------------------------\nclass CharMeanNet(nn.Module):\n    def __init__(self, vocab, emb_dim=32, hidden=64, num_classes=2):\n        super().__init__()\n        self.emb = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n        self.fc1 = nn.Linear(emb_dim, hidden)\n        self.fc2 = nn.Linear(hidden, num_classes)\n\n    def forward(self, x):\n        emb = self.emb(x)  # [B,L,E]\n        mask = (x != 0).unsqueeze(-1).float()\n        summed = (emb * mask).sum(1)\n        lens = mask.sum(1).clamp(min=1)\n        mean = summed / lens  # [B,E]\n        h = torch.relu(self.fc1(mean))\n        return self.fc2(h)\n\n\nmodel = CharMeanNet(vocab, num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ----------------- training loop ----------------------------------------\ndef run_epoch(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, correct, n = 0.0, 0, 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            xb = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * len(xb)\n            preds = logits.argmax(1)\n            correct += (preds == yb).sum().item()\n            n += len(xb)\n    return total_loss / n, correct / n\n\n\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n    dv_loss, dv_acc = run_epoch(dev_loader, train=False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(dv_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(tr_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(dv_acc)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n    print(\n        f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={dv_loss:.4f} \"\n        f\"train_acc={tr_acc*100:.2f}% val_acc={dv_acc*100:.2f}%\"\n    )\n\n# ----------------- surrogate rule extraction -----------------------------\n# Vectorise sequences into n-gram bag for tree\nvectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1, 2))\nX_train_txt = vectorizer.fit_transform(spr[\"train\"][\"sequence\"])\nwith torch.no_grad():\n    model.eval()\n    X_train_pred = []\n    for s in spr[\"train\"][\"sequence\"]:\n        t = torch.tensor(encode(s))[None, :].to(device)\n        X_train_pred.append(model(t).argmax(1).item())\ny_surrogate = np.array(X_train_pred)\n\ntree = DecisionTreeClassifier(max_depth=6)\ntree.fit(X_train_txt, y_surrogate)\n\n# fidelity on dev\nX_dev_txt = vectorizer.transform(spr[\"dev\"][\"sequence\"])\nwith torch.no_grad():\n    nn_preds = []\n    for s in spr[\"dev\"][\"sequence\"]:\n        t = torch.tensor(encode(s))[None, :].to(device)\n        nn_preds.append(model(t).argmax(1).item())\nrule_preds = tree.predict(X_dev_txt)\nfidelity = (rule_preds == np.array(nn_preds)).mean()\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(fidelity)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"val_nn\"] = nn_preds\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"val_rule\"] = rule_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = spr[\"dev\"][\"label\"]\nprint(f\"Rule Fidelity on dev: {fidelity*100:.2f}%\")\n\n# optional: print rules\nrule_text = export_text(tree, feature_names=vectorizer.get_feature_names_out())\nwith open(os.path.join(working_dir, \"decision_tree_rules.txt\"), \"w\") as f:\n    f.write(rule_text)\nprint(\"Saved symbolic rules to decision_tree_rules.txt\")\n\n# ----------------- save experiment data ----------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved.\")\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------#\n# Working dir and experiment data container\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"val_acc\": [],\n            \"rule_fidelity\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": {\"train\": [], \"val\": []},\n        \"ground_truth\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n    }\n}\n# ------------------------------------------------------------------#\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------#\n# Data loader utility (from prompt)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndataset_root = pathlib.Path(os.getenv(\"SPR_PATH\", \"SPR_BENCH\"))\nspr = load_spr_bench(dataset_root)\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------------#\n# Vocabulary\nPAD = \"<PAD>\"\nchars = {c for seq in spr[\"train\"][\"sequence\"] for c in seq}\nchar2idx = {PAD: 0, **{c: i + 1 for i, c in enumerate(sorted(chars))}}\nvocab_size = len(char2idx)\nprint(f\"Vocab size: {vocab_size}\")\n\n# Label mapping\nunique_labels = sorted({l for l in spr[\"train\"][\"label\"]})\nlabel2idx = {l: i for i, l in enumerate(unique_labels)}\nnum_classes = len(label2idx)\nprint(f\"Num classes: {num_classes}\")\n\n\n# ------------------------------------------------------------------#\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = [label2idx[l] for l in hf_ds[\"label\"]]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = [char2idx[c] for c in self.seqs[idx]]\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = torch.tensor([len(b[\"input_ids\"]) for b in batch], dtype=torch.long)\n    maxlen = lengths.max().item()\n    inputs = []\n    for b in batch:\n        pad_len = maxlen - len(b[\"input_ids\"])\n        padded = torch.cat([b[\"input_ids\"], torch.zeros(pad_len, dtype=torch.long)])\n        inputs.append(padded)\n    return {\n        \"input_ids\": torch.stack(inputs),\n        \"lengths\": lengths,\n        \"label\": torch.stack([b[\"label\"] for b in batch]),\n    }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------#\nclass GRUClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, input_ids, lengths):\n        emb = self.embedding(input_ids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, h_n = self.gru(packed)\n        logits = self.fc(h_n[-1])\n        return logits\n\n\nmodel = GRUClassifier(vocab_size, 16, 32, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------------------------------------------------------#\ndef run_epoch(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    all_preds, all_labels = [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"label\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"label\"].size(0)\n        preds = logits.argmax(1)\n        correct += (preds == batch[\"label\"]).sum().item()\n        total += batch[\"label\"].size(0)\n        all_preds.extend(preds.cpu().tolist())\n        all_labels.extend(batch[\"label\"].cpu().tolist())\n    avg_loss = total_loss / total\n    accuracy = correct / total\n    return avg_loss, accuracy, all_preds, all_labels\n\n\n# ------------------------------------------------------------------#\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    start = time.time()\n    train_loss, train_acc, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_acc, val_preds, val_labels = run_epoch(val_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_loss\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_loss\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n        f\"val_acc={val_acc:.4f}, time={time.time()-start:.1f}s\"\n    )\n\n# ------------------------------------------------------------------#\n# Gather network predictions on train & dev for rule extraction\ntrain_logits = []\ntrain_sequences = spr[\"train\"][\"sequence\"]\ntrain_loader_no_shuffle = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\nwith torch.no_grad():\n    for batch in train_loader_no_shuffle:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        train_logits.append(model(batch[\"input_ids\"], batch[\"lengths\"]).cpu())\ntrain_logits = torch.cat(train_logits, 0)\ntrain_net_preds = train_logits.argmax(1).numpy()\n\n# Decision-tree surrogate (rule extractor)\nvectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1, 2))\nX_train_feat = vectorizer.fit_transform(train_sequences)\nrule_model = DecisionTreeClassifier(max_depth=5, random_state=42)\nrule_model.fit(X_train_feat, train_net_preds)\n\n# Rule fidelity on dev\ndev_sequences = spr[\"dev\"][\"sequence\"]\nX_dev_feat = vectorizer.transform(dev_sequences)\nnet_dev_preds = np.array(val_preds)\nrule_preds = rule_model.predict(X_dev_feat)\nrule_fidelity = (rule_preds == net_dev_preds).mean()\nprint(f\"Rule Fidelity on dev: {rule_fidelity:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(rule_fidelity)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"val\"] = net_dev_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"val\"] = val_labels\n\n# ------------------------------------------------------------------#\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# -------------------------------------------------\n# working directory & experiment-data container\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": {\"val\": [], \"val_tree\": []},\n        \"ground_truth\": {\"val\": []},\n        \"timestamps\": [],\n    }\n}\n\n# -------------------------------------------------\n# device handling\n# -------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------\n# small helper -------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndata_root = pathlib.Path(os.environ.get(\"SPR_DATA_PATH\", \"./SPR_BENCH\"))\nspr = load_spr_bench(data_root)\n\n# -------------------------------------------------\n# vocabulary build -------------------------------------------------\nall_chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(all_chars))}\npad_idx = 0\nvocab_size = len(char2idx) + 1\n\n\ndef encode(seq, max_len):\n    seq_idx = [char2idx.get(c, 0) for c in seq][:max_len]\n    return seq_idx + [pad_idx] * (max_len - len(seq_idx))\n\n\nmax_len = max(len(s) for s in spr[\"train\"][\"sequence\"])\n\n\n# -------------------------------------------------\n# torch Dataset -------------------------------------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(encode(self.seqs[idx], max_len), dtype=torch.long)\n        y = torch.tensor(self.labels[idx], dtype=torch.long)\n        return {\"input_ids\": x, \"labels\": y}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\nval_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n\n# -------------------------------------------------\n# simple model -------------------------------------------------\nclass MeanPoolClassifier(nn.Module):\n    def __init__(self, vocab, emb_dim, n_classes):\n        super().__init__()\n        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=pad_idx)\n        self.fc = nn.Linear(emb_dim, n_classes)\n\n    def forward(self, x):\n        # x: (B, L)\n        mask = (x != pad_idx).unsqueeze(-1)\n        emb = self.emb(x) * mask\n        summed = emb.sum(dim=1)\n        lens = mask.sum(dim=1).clamp(min=1)\n        mean = summed / lens\n        return self.fc(mean)\n\n\nn_classes = len(set(spr[\"train\"][\"label\"]))\nmodel = MeanPoolClassifier(vocab_size, 32, n_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# -------------------------------------------------\n# training -------------------------------------------------\ndef run_epoch(loader, train=True):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, total_correct, total_count = 0.0, 0, 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            preds = logits.argmax(dim=1)\n            total_loss += loss.item() * len(preds)\n            total_correct += (preds == batch[\"labels\"]).sum().item()\n            total_count += len(preds)\n    return total_loss / total_count, total_correct / total_count\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    train_loss, train_acc = run_epoch(train_loader, True)\n    val_loss, val_acc = run_epoch(val_loader, False)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n\n# -------------------------------------------------\n# Rule extraction (decision tree surrogate) -------\n# -------------------------------------------------\ndef bag_of_chars(seqs, chars):\n    mat = np.zeros((len(seqs), len(chars)), dtype=np.float32)\n    c2i = {c: i for i, c in enumerate(chars)}\n    for i, s in enumerate(seqs):\n        for ch in s:\n            if ch in c2i:\n                mat[i, c2i[ch]] += 1\n    return mat\n\n\n# Prepare data for surrogate training\ntrain_seqs = spr[\"train\"][\"sequence\"]\nwith torch.no_grad():\n    # neural preds on train\n    train_logits = []\n    for i in range(0, len(train_seqs), 512):\n        enc = torch.tensor(\n            [encode(s, max_len) for s in train_seqs[i : i + 512]], dtype=torch.long\n        ).to(device)\n        train_logits.append(model(enc).cpu())\n    train_pred = torch.cat(train_logits).argmax(dim=1).numpy()\n\nX_train_tree = bag_of_chars(train_seqs, sorted(all_chars))\ntree = DecisionTreeClassifier(max_depth=5)\ntree.fit(X_train_tree, train_pred)\n\n# Evaluate rule fidelity on dev split\nval_seqs = spr[\"dev\"][\"sequence\"]\nX_val_tree = bag_of_chars(val_seqs, sorted(all_chars))\ntree_pred = tree.predict(X_val_tree)\n\n# neural preds on val\nwith torch.no_grad():\n    val_logits = []\n    for i in range(0, len(val_seqs), 512):\n        enc = torch.tensor(\n            [encode(s, max_len) for s in val_seqs[i : i + 512]], dtype=torch.long\n        ).to(device)\n        val_logits.append(model(enc).cpu())\n    val_pred = torch.cat(val_logits).argmax(dim=1).numpy()\n\nrule_fidelity = accuracy_score(val_pred, tree_pred)\nprint(f\"Rule Fidelity on dev split: {rule_fidelity*100:.2f}%\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(rule_fidelity)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"val\"] = val_pred\nexperiment_data[\"SPR_BENCH\"][\"predictions\"][\"val_tree\"] = tree_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"][\"val\"] = spr[\"dev\"][\"label\"]\n\n# -------------------------------------------------\n# save data ---------------------------------------\n# -------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict\n\n# ---------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------- DATA -----------------------\n# loader supplied in prompt (copied here to keep single-file)\nfrom datasets import load_dataset\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ---------------- n-gram vectoriser ----------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        chars = list(s)\n        unis.update(chars)\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    idx = {tok: i for i, tok in enumerate(vocab)}\n    return idx\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    chars = list(seq)\n    for c in chars:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    return v\n\n\n# Build vocab on train only for fairness\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\n# Label encoding\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\n\n\n# -------------------- MODEL ------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = LogReg(num_feats, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# ----------------- EXPERIMENT STORE ---------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ----------------- TRAINING LOOP ------------------\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    acc = correct / total\n    loss = loss_sum / total\n    return acc, loss, torch.cat(all_logits)\n\n\nepochs = 10\ntop_k = 10  # number of n-grams kept per class for rule fidelity\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    seen = 0\n    correct = 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"y\"].size(0)\n        _, preds = torch.max(logits, 1)\n        correct += (preds == batch[\"y\"]).sum().item()\n        seen += batch[\"y\"].size(0)\n    train_loss = running_loss / seen\n    train_acc = correct / seen\n\n    val_acc, val_loss, val_logits = evaluate(dev_loader)\n\n    # ---------- Rule Fidelity ----------\n    W = model.linear.weight.detach().cpu().numpy()  # shape [C,F]\n    b = model.linear.bias.detach().cpu().numpy()\n    # truncate weights to top_k abs weight per class\n    W_trunc = np.zeros_like(W)\n    for c in range(num_classes):\n        idxs = np.argsort(-np.abs(W[c]))[:top_k]\n        W_trunc[c, idxs] = W[c, idxs]\n    # compute rule predictions on dev set\n    X_dev_tensor = torch.from_numpy(X_dev).float()\n    lin_full = torch.from_numpy((X_dev @ W.T) + b)\n    lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n    rule_pred = torch.argmax(lin_trunc, 1)\n    model_pred = torch.argmax(lin_full, 1)\n    rule_fid = (rule_pred == model_pred).float().mean().item()\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(rule_fid)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n    )\n\n# --------------- FINAL EVAL & SAVE -----------------\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\ntest_acc, test_loss, test_logits = evaluate(test_loader)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.argmax(test_logits, 1).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_test\nprint(f\"Test_acc={test_acc:.3f} test_loss={test_loss:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict\n\n# ---------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------- DATA -----------------------\n# loader supplied in prompt (copied here to keep single-file)\nfrom datasets import load_dataset\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ---------------- n-gram vectoriser ----------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        chars = list(s)\n        unis.update(chars)\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    idx = {tok: i for i, tok in enumerate(vocab)}\n    return idx\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    chars = list(seq)\n    for c in chars:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    return v\n\n\n# Build vocab on train only for fairness\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\n# Label encoding\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\n\n\n# -------------------- MODEL ------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = LogReg(num_feats, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# ----------------- EXPERIMENT STORE ---------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ----------------- TRAINING LOOP ------------------\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    acc = correct / total\n    loss = loss_sum / total\n    return acc, loss, torch.cat(all_logits)\n\n\nepochs = 10\ntop_k = 10  # number of n-grams kept per class for rule fidelity\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    seen = 0\n    correct = 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"y\"].size(0)\n        _, preds = torch.max(logits, 1)\n        correct += (preds == batch[\"y\"]).sum().item()\n        seen += batch[\"y\"].size(0)\n    train_loss = running_loss / seen\n    train_acc = correct / seen\n\n    val_acc, val_loss, val_logits = evaluate(dev_loader)\n\n    # ---------- Rule Fidelity ----------\n    W = model.linear.weight.detach().cpu().numpy()  # shape [C,F]\n    b = model.linear.bias.detach().cpu().numpy()\n    # truncate weights to top_k abs weight per class\n    W_trunc = np.zeros_like(W)\n    for c in range(num_classes):\n        idxs = np.argsort(-np.abs(W[c]))[:top_k]\n        W_trunc[c, idxs] = W[c, idxs]\n    # compute rule predictions on dev set\n    X_dev_tensor = torch.from_numpy(X_dev).float()\n    lin_full = torch.from_numpy((X_dev @ W.T) + b)\n    lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n    rule_pred = torch.argmax(lin_trunc, 1)\n    model_pred = torch.argmax(lin_full, 1)\n    rule_fid = (rule_pred == model_pred).float().mean().item()\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(rule_fid)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n    )\n\n# --------------- FINAL EVAL & SAVE -----------------\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\ntest_acc, test_loss, test_logits = evaluate(test_loader)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.argmax(test_logits, 1).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_test\nprint(f\"Test_acc={test_acc:.3f} test_loss={test_loss:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict\n\n# ---------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------- DATA -----------------------\n# loader supplied in prompt (copied here to keep single-file)\nfrom datasets import load_dataset\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# ---------------- n-gram vectoriser ----------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        chars = list(s)\n        unis.update(chars)\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    idx = {tok: i for i, tok in enumerate(vocab)}\n    return idx\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    chars = list(seq)\n    for c in chars:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    return v\n\n\n# Build vocab on train only for fairness\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\n# Label encoding\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\n\n\n# -------------------- MODEL ------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = LogReg(num_feats, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# ----------------- EXPERIMENT STORE ---------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ----------------- TRAINING LOOP ------------------\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    acc = correct / total\n    loss = loss_sum / total\n    return acc, loss, torch.cat(all_logits)\n\n\nepochs = 10\ntop_k = 10  # number of n-grams kept per class for rule fidelity\nfor epoch in range(1, epochs + 1):\n    model.train()\n    running_loss = 0.0\n    seen = 0\n    correct = 0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"x\"])\n        loss = criterion(logits, batch[\"y\"])\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * batch[\"y\"].size(0)\n        _, preds = torch.max(logits, 1)\n        correct += (preds == batch[\"y\"]).sum().item()\n        seen += batch[\"y\"].size(0)\n    train_loss = running_loss / seen\n    train_acc = correct / seen\n\n    val_acc, val_loss, val_logits = evaluate(dev_loader)\n\n    # ---------- Rule Fidelity ----------\n    W = model.linear.weight.detach().cpu().numpy()  # shape [C,F]\n    b = model.linear.bias.detach().cpu().numpy()\n    # truncate weights to top_k abs weight per class\n    W_trunc = np.zeros_like(W)\n    for c in range(num_classes):\n        idxs = np.argsort(-np.abs(W[c]))[:top_k]\n        W_trunc[c, idxs] = W[c, idxs]\n    # compute rule predictions on dev set\n    X_dev_tensor = torch.from_numpy(X_dev).float()\n    lin_full = torch.from_numpy((X_dev @ W.T) + b)\n    lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n    rule_pred = torch.argmax(lin_trunc, 1)\n    model_pred = torch.argmax(lin_full, 1)\n    rule_fid = (rule_pred == model_pred).float().mean().item()\n\n    # store metrics\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(rule_fid)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n    )\n\n# --------------- FINAL EVAL & SAVE -----------------\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\ntest_acc, test_loss, test_logits = evaluate(test_loader)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.argmax(test_logits, 1).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = y_test\nprint(f\"Test_acc={test_acc:.3f} test_loss={test_loss:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 113966.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 106589.68\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 180952.76\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', 'Epoch 1: train_loss=1.4462\nval_loss=0.8908 train_acc=0.549 val_acc=0.588 rule_fid=0.566', '\\n', 'Epoch 2:\ntrain_loss=0.7264 val_loss=0.8662 train_acc=0.658 val_acc=0.622 rule_fid=0.698',\n'\\n', 'Epoch 3: train_loss=0.5713 val_loss=0.7895 train_acc=0.721 val_acc=0.646\nrule_fid=0.594', '\\n', 'Epoch 4: train_loss=0.4738 val_loss=0.7602\ntrain_acc=0.776 val_acc=0.684 rule_fid=0.648', '\\n', 'Epoch 5: train_loss=0.3999\nval_loss=0.7416 train_acc=0.821 val_acc=0.696 rule_fid=0.606', '\\n', 'Epoch 6:\ntrain_loss=0.3394 val_loss=0.7318 train_acc=0.856 val_acc=0.726 rule_fid=0.656',\n'\\n', 'Epoch 7: train_loss=0.2947 val_loss=0.7316 train_acc=0.883 val_acc=0.746\nrule_fid=0.620', '\\n', 'Epoch 8: train_loss=0.2585 val_loss=0.7345\ntrain_acc=0.911 val_acc=0.756 rule_fid=0.624', '\\n', 'Epoch 9: train_loss=0.2309\nval_loss=0.7401 train_acc=0.930 val_acc=0.762 rule_fid=0.636', '\\n', 'Epoch 10:\ntrain_loss=0.2080 val_loss=0.7525 train_acc=0.939 val_acc=0.762 rule_fid=0.652',\n'\\n', 'Test_acc=0.754 test_loss=0.7323', '\\n', 'Execution time: 3 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 48, in <module>\\n    spr = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 41, in load_spr_bench\\n\ndset[\"train\"] = _load(\"train.csv\")\\n                    ^^^^^^^^^^^^^^^^^^\\n\nFile \"runfile.py\", line 33, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n2/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 51, in <module>\\n    spr = load_spr_bench(dataset_root)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 44, in load_spr_bench\\n\ndset[\"train\"] = _load(\"train.csv\")\\n                    ^^^^^^^^^^^^^^^^^^\\n\nFile \"runfile.py\", line 36, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n3/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 51, in <module>\\n    spr = load_spr_bench(data_root)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 44, in load_spr_bench\\n\nd[\"train\"] = _load(\"train.csv\")\\n                 ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 36, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n4/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 44087.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 15918.39\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 146408.27\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', 'Epoch 1: train_loss=0.7436\nval_loss=0.7394 train_acc=0.597 val_acc=0.602 rule_fid=0.692', '\\n', 'Epoch 2:\ntrain_loss=0.5482 val_loss=0.6748 train_acc=0.721 val_acc=0.656 rule_fid=0.594',\n'\\n', 'Epoch 3: train_loss=0.4312 val_loss=0.6377 train_acc=0.820 val_acc=0.692\nrule_fid=0.508', '\\n', 'Epoch 4: train_loss=0.3503 val_loss=0.6262\ntrain_acc=0.882 val_acc=0.728 rule_fid=0.544', '\\n', 'Epoch 5: train_loss=0.2934\nval_loss=0.6221 train_acc=0.915 val_acc=0.742 rule_fid=0.540', '\\n', 'Epoch 6:\ntrain_loss=0.2538 val_loss=0.6304 train_acc=0.934 val_acc=0.750 rule_fid=0.550',\n'\\n', 'Epoch 7: train_loss=0.2236 val_loss=0.6397 train_acc=0.950 val_acc=0.766\nrule_fid=0.542', '\\n', 'Epoch 8: train_loss=0.2020 val_loss=0.6552\ntrain_acc=0.956 val_acc=0.764 rule_fid=0.550', '\\n', 'Epoch 9: train_loss=0.1854\nval_loss=0.6666 train_acc=0.958 val_acc=0.768 rule_fid=0.548', '\\n', 'Epoch 10:\ntrain_loss=0.1714 val_loss=0.6880 train_acc=0.962 val_acc=0.764 rule_fid=0.554',\n'\\n', 'Test_acc=0.773 test_loss=0.6569', '\\n', 'Execution time: 3 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 46517.84\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 114755.24\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 111860.04\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', 'Epoch 1: train_loss=2.7990\nval_loss=1.2676 train_acc=0.499 val_acc=0.514 rule_fid=0.274', '\\n', 'Epoch 2:\ntrain_loss=0.9341 val_loss=0.9695 train_acc=0.473 val_acc=0.464 rule_fid=0.860',\n'\\n', 'Epoch 3: train_loss=0.7299 val_loss=0.7474 train_acc=0.588 val_acc=0.578\nrule_fid=0.438', '\\n', 'Epoch 4: train_loss=0.5753 val_loss=0.6985\ntrain_acc=0.684 val_acc=0.628 rule_fid=0.482', '\\n', 'Epoch 5: train_loss=0.4790\nval_loss=0.6748 train_acc=0.778 val_acc=0.664 rule_fid=0.576', '\\n', 'Epoch 6:\ntrain_loss=0.4081 val_loss=0.6531 train_acc=0.839 val_acc=0.694 rule_fid=0.590',\n'\\n', 'Epoch 7: train_loss=0.3568 val_loss=0.6494 train_acc=0.875 val_acc=0.716\nrule_fid=0.788', '\\n', 'Epoch 8: train_loss=0.3157 val_loss=0.6464\ntrain_acc=0.905 val_acc=0.730 rule_fid=0.816', '\\n', 'Epoch 9: train_loss=0.2831\nval_loss=0.6533 train_acc=0.925 val_acc=0.740 rule_fid=0.832', '\\n', 'Epoch 10:\ntrain_loss=0.2572 val_loss=0.6571 train_acc=0.936 val_acc=0.748 rule_fid=0.842',\n'\\n', 'Test_acc=0.768 test_loss=0.6049', '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 54511.48\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 104726.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 201998.84\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', 'Epoch 1: train_loss=0.9057\nval_loss=0.6405 train_acc=0.631 val_acc=0.676 rule_fid=0.368', '\\n', 'Epoch 2:\ntrain_loss=0.4884 val_loss=0.6015 train_acc=0.759 val_acc=0.716 rule_fid=0.518',\n'\\n', 'Epoch 3: train_loss=0.3862 val_loss=0.5955 train_acc=0.852 val_acc=0.736\nrule_fid=0.532', '\\n', 'Epoch 4: train_loss=0.3355 val_loss=0.5877\ntrain_acc=0.887 val_acc=0.756 rule_fid=0.522', '\\n', 'Epoch 5: train_loss=0.2989\nval_loss=0.5912 train_acc=0.909 val_acc=0.764 rule_fid=0.546', '\\n', 'Epoch 6:\ntrain_loss=0.2680 val_loss=0.5953 train_acc=0.931 val_acc=0.768 rule_fid=0.542',\n'\\n', 'Epoch 7: train_loss=0.2435 val_loss=0.6052 train_acc=0.940 val_acc=0.770\nrule_fid=0.536', '\\n', 'Epoch 8: train_loss=0.2232 val_loss=0.6134\ntrain_acc=0.952 val_acc=0.774 rule_fid=0.532', '\\n', 'Epoch 9: train_loss=0.2065\nval_loss=0.6251 train_acc=0.957 val_acc=0.776 rule_fid=0.886', '\\n', 'Epoch 10:\ntrain_loss=0.1926 val_loss=0.6376 train_acc=0.963 val_acc=0.776 rule_fid=0.922',\n'\\n', 'Test_acc=0.778 test_loss=0.6096', '\\n', 'Execution time: 3 seconds\nseconds (time limit is 30 minutes).']", ""], "analysis": ["", "The script failed to execute because it could not locate the dataset file\n'train.csv' in the specified directory '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH/'. This indicates that either the datasets are not\npresent in the expected path or the path provided in the script is incorrect.\n**Proposed Fix:** 1. Verify that the dataset files ('train.csv', 'dev.csv',\n'test.csv') are correctly placed in the '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH/' directory. 2. If the datasets are located in a\ndifferent directory, update the `DATA_PATH` variable in the script to point to\nthe correct path. 3. Ensure the directory and file permissions allow the script\nto access the datasets. 4. Add error handling in the script to provide a more\nuser-friendly error message when the dataset files are not found.", "The execution failed due to a FileNotFoundError. Specifically, the script\nattempted to load the dataset from '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n3/SPR_BENCH/train.csv', but the file was not found at that location. This\nsuggests that the dataset path is incorrect or the dataset files are missing. To\nfix this issue, verify that the dataset is correctly placed in the specified\ndirectory. If the dataset is located elsewhere, update the 'dataset_root'\nvariable to point to the correct path. Additionally, ensure that the environment\nvariable 'SPR_PATH' is set correctly if it is being used to define the dataset\npath dynamically.", "The execution failed because the dataset files (train.csv, dev.csv, test.csv)\nwere not found in the specified directory '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n4/SPR_BENCH/'. This likely happened because the dataset path is incorrect or the\ndataset files are missing. To fix this issue, ensure that the dataset files are\ncorrectly placed in the specified directory or update the DATA_PATH variable to\npoint to the correct location of the dataset files.", "The execution of the training script was successful, with no bugs observed. The\nmodel was trained on the SPR_BENCH dataset, achieving a test accuracy of 77.3%\nand a validation accuracy of 76.8% at its peak. The rule fidelity metric was\nalso computed, showing reasonable alignment between the model's predictions and\nthe rule-based approximations. The implementation and results are consistent\nwith the stated objectives of the experiment.", "", "The execution output indicates that the training script ran successfully without\nany bugs. The model achieved a test accuracy of 77.8%, with a corresponding test\nloss of 0.6096. Rule fidelity improved significantly in the later epochs,\ndemonstrating interpretable rule learning. The implementation is functional and\naligns with the initial goals of the experiment.", ""], "exc_type": [null, "FileNotFoundError", "FileNotFoundError", "FileNotFoundError", null, null, null, null], "exc_info": [null, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-2/SPR_BENCH/train.csv'"]}, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-3/SPR_BENCH/train.csv'"]}, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-4/SPR_BENCH/train.csv'"]}, null, null, null, null], "exc_stack": [null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 48, "<module>", "spr = load_spr_bench(DATA_PATH)"], ["runfile.py", 41, "load_spr_bench", "dset[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 33, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 51, "<module>", "spr = load_spr_bench(dataset_root)"], ["runfile.py", 44, "load_spr_bench", "dset[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 36, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 51, "<module>", "spr = load_spr_bench(data_root)"], ["runfile.py", 44, "load_spr_bench", "d[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 36, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9385, "best_value": 0.9385}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.762, "best_value": 0.762}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.208, "best_value": 0.208}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7316, "best_value": 0.7316}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.962, "best_value": 0.962}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.692, "best_value": 0.692}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1714, "best_value": 0.1714}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6221, "best_value": 0.6221}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions over total predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.748, "best_value": 0.9365}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Indicates how well the rules generated align with the observed data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.86, "best_value": 0.86}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error in predictions. Lower values indicate better model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6464, "best_value": 0.2572}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.963, "best_value": 0.963}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.776, "best_value": 0.776}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.922, "best_value": 0.922}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1926, "best_value": 0.1926}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5877, "best_value": 0.5877}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_confusion_matrix.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_loss_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_accuracy_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_rule_fidelity_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_confusion_matrix_agg.png"]], "plot_paths": [["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_confusion_matrix.png"], [], [], [], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_loss_agg.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_accuracy_agg.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_rule_fidelity_agg.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_8521215cf8f644269bd4fc0e11b921e2/SPR_BENCH_confusion_matrix_agg.png"]], "plot_analyses": [[{"analysis": "The loss curves indicate that the training loss decreases steadily over the epochs, which is expected as the model optimizes its parameters. The validation loss initially decreases but starts to plateau and slightly increase after a certain point. This suggests that the model may be starting to overfit to the training data, and regularization techniques or early stopping could be considered to improve generalization.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_loss_curves.png"}, {"analysis": "The accuracy curves show a steady improvement in both training and validation accuracy over the epochs. However, the gap between the training and validation accuracy widens as training progresses, which further supports the observation of potential overfitting. The validation accuracy plateaus around 0.75, indicating that the model is nearing its performance limit on the validation set under the current configuration.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The rule fidelity plot shows fluctuations over the epochs, with an initial spike followed by a drop and subsequent oscillations. This suggests that the model's ability to learn interpretable rules is inconsistent across epochs. The overall trend appears to improve slightly towards the end, indicating that the model might require more epochs or additional refinement to stabilize rule fidelity.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix indicates that the model performs better on one class compared to the other. With 401 correct predictions for one class and 353 for the other, there is a noticeable imbalance in performance. The misclassification rates (145 and 101) suggest that the model has room for improvement in handling both classes more evenly. Techniques such as class weighting or data augmentation could help address this imbalance.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_ddab37f1e45045bab8b6fc11a82d2a1b_proc_3198316/SPR_BENCH_confusion_matrix.png"}], [], [], [], [{"analysis": "The training loss decreases steadily over epochs, indicating that the model is learning effectively on the training data. However, the validation loss decreases initially but starts to increase slightly after epoch 6, suggesting potential overfitting. Regularization techniques or early stopping might help mitigate this issue.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_loss_curves.png"}, {"analysis": "The training accuracy improves consistently, reaching over 95%, which reflects good performance on the training set. However, validation accuracy plateaus around 80% after epoch 6, indicating that the model may be overfitting to the training data and not generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The rule fidelity starts high at the beginning but drops sharply after the first epoch and stabilizes around 0.55. This indicates that while the model is improving its classification accuracy, it is losing some fidelity in terms of the interpretability or adherence to the underlying rules. Further investigation into the trade-off between performance and rule fidelity is needed.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix shows that the model performs reasonably well in classifying both classes, with a higher true positive rate for class 1 than class 0. However, there is a noticeable number of false positives and false negatives, indicating room for improvement in the model's classification precision and recall.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that the model is learning effectively. The training loss decreases steadily, suggesting the model is optimizing well on the training data. The validation loss decreases initially but plateaus after a few epochs, which is a common behavior when the model begins to converge. There is no significant overfitting observed, as the validation loss does not increase dramatically.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_loss_curves.png"}, {"analysis": "The accuracy curves show that the model improves in classification performance over epochs. Training accuracy increases steadily and reaches near-optimal levels, while validation accuracy also improves but at a slower rate, suggesting that the model generalizes reasonably well to unseen data. However, the gap between training and validation accuracy indicates some room for improvement in generalization.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The rule fidelity plot shows an increase in fidelity over epochs, indicating that the model is progressively learning to represent the underlying rules governing the SPR task. The fidelity stabilizes at a high value, suggesting that the model achieves good interpretability in terms of rule representation by the end of training.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix shows the classification performance on the test set. The model correctly classifies a majority of the samples for both classes. However, there is some misclassification (129 and 103 instances in the two off-diagonal cells), indicating areas where the model could be further improved for better accuracy.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation loss over 10 epochs. The training loss decreases steadily, indicating that the model is learning effectively during training. However, the validation loss initially decreases but then plateaus and slightly increases after epoch 6, suggesting potential overfitting. This behavior indicates that the model might be memorizing the training data instead of generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation accuracy over 10 epochs. The training accuracy increases rapidly and approaches close to 100%, while the validation accuracy improves initially but starts plateauing around 80% after epoch 6. This is consistent with the validation loss behavior and further supports the observation of potential overfitting. The model achieves a validation accuracy close to the state-of-the-art benchmark but does not show further improvement in generalization.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot shows the fidelity of the rules learned by the model over 10 epochs. The fidelity starts at a low value, indicating that the initial rules are not aligned with the true underlying rules. However, there is a significant jump in fidelity after epoch 8, suggesting that the model learns and represents the rules more effectively in later epochs. This improvement in rule fidelity aligns with the goal of enhancing interpretability in the proposed research.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix for the test set indicates that the model correctly classifies a majority of the samples in both classes. However, there are 127 false positives and 95 false negatives, suggesting that the model has a slightly higher tendency to misclassify the negative class as positive. Overall, the performance is reasonable, but there is room for improvement in reducing misclassifications to achieve better balance and accuracy.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The results indicate that the model achieves reasonable performance but shows\nsigns of overfitting and inconsistent rule fidelity. The validation accuracy\nplateaus below the target state-of-the-art accuracy, and the confusion matrix\nreveals class imbalance in predictions. Further refinement is needed to enhance\ngeneralization and interpretability, possibly through regularization, additional\ntraining epochs, or hyperparameter tuning.", "[]", "[]", "[]", "The plots indicate that the model is learning effectively but exhibits signs of\noverfitting after epoch 6. While classification accuracy is high, the rule\nfidelity decreases significantly, raising concerns about interpretability. The\nconfusion matrix highlights a reasonable classification performance but also\npoints to areas for improvement in reducing misclassifications.", "The plots indicate that the model is learning effectively, with steady\nimprovements in loss, accuracy, and rule fidelity. There is some gap between\ntraining and validation performance, suggesting potential for better\ngeneralization. The confusion matrix highlights reasonable classification\nperformance with some room for improvement in reducing misclassifications.", "The plots indicate that the model is learning effectively during training but\nshows signs of overfitting after epoch 6. While the accuracy on the validation\nset approaches the state-of-the-art benchmark, there is limited improvement in\ngeneralization. The rule fidelity improves significantly in later epochs,\naligning with the research goal of interpretability. The confusion matrix\nhighlights a slight imbalance in misclassification, suggesting areas for further\noptimization.", "[]"], "exec_time": [3.509493350982666, 1.224114179611206, 1.1360464096069336, 1.1073212623596191, 3.7696340084075928, 4.025714635848999, 3.978062152862549, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], [], [], [], ["[]"], ["[\"experiment_data\"]"], ["[\"experiment_data\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper: confusion matrix without sklearn\ndef confusion_matrix_np(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    losses = ds_dict.get(\"losses\", {})\n    preds = ds_dict.get(\"predictions\", None)\n    gt = ds_dict.get(\"ground_truth\", None)\n\n    epochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n    # 1) Loss curves ---------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Loss Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) Accuracy curves -----------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, metrics[\"val_acc\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{ds_name} Accuracy Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) Rule fidelity -------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"rule_fidelity\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Fidelity\")\n        plt.title(f\"{ds_name} Rule Fidelity over Epochs\")\n        fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix ----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            num_classes = len(np.unique(gt))\n            cm = confusion_matrix_np(gt, preds, num_classes)\n            plt.figure(figsize=(4, 4))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{ds_name} Confusion Matrix\\nTest Set\")\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # Print final metric -----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            test_acc = (preds == gt).mean()\n            print(f\"{ds_name} final test accuracy: {test_acc:.3f}\")\n    except Exception as e:\n        print(f\"Error computing final accuracy for {ds_name}: {e}\")\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper: confusion matrix without sklearn\ndef confusion_matrix_np(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    losses = ds_dict.get(\"losses\", {})\n    preds = ds_dict.get(\"predictions\", None)\n    gt = ds_dict.get(\"ground_truth\", None)\n\n    epochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n    # 1) Loss curves ---------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Loss Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) Accuracy curves -----------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, metrics[\"val_acc\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{ds_name} Accuracy Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) Rule fidelity -------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"rule_fidelity\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Fidelity\")\n        plt.title(f\"{ds_name} Rule Fidelity over Epochs\")\n        fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix ----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            num_classes = len(np.unique(gt))\n            cm = confusion_matrix_np(gt, preds, num_classes)\n            plt.figure(figsize=(4, 4))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{ds_name} Confusion Matrix\\nTest Set\")\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # Print final metric -----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            test_acc = (preds == gt).mean()\n            print(f\"{ds_name} final test accuracy: {test_acc:.3f}\")\n    except Exception as e:\n        print(f\"Error computing final accuracy for {ds_name}: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper: confusion matrix without sklearn\ndef confusion_matrix_np(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    losses = ds_dict.get(\"losses\", {})\n    preds = ds_dict.get(\"predictions\", None)\n    gt = ds_dict.get(\"ground_truth\", None)\n\n    epochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n    # 1) Loss curves ---------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Loss Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) Accuracy curves -----------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, metrics[\"val_acc\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{ds_name} Accuracy Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) Rule fidelity -------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"rule_fidelity\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Fidelity\")\n        plt.title(f\"{ds_name} Rule Fidelity over Epochs\")\n        fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix ----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            num_classes = len(np.unique(gt))\n            cm = confusion_matrix_np(gt, preds, num_classes)\n            plt.figure(figsize=(4, 4))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{ds_name} Confusion Matrix\\nTest Set\")\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # Print final metric -----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            test_acc = (preds == gt).mean()\n            print(f\"{ds_name} final test accuracy: {test_acc:.3f}\")\n    except Exception as e:\n        print(f\"Error computing final accuracy for {ds_name}: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper: confusion matrix without sklearn\ndef confusion_matrix_np(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\nfor ds_name, ds_dict in experiment_data.items():\n    metrics = ds_dict.get(\"metrics\", {})\n    losses = ds_dict.get(\"losses\", {})\n    preds = ds_dict.get(\"predictions\", None)\n    gt = ds_dict.get(\"ground_truth\", None)\n\n    epochs = np.arange(1, len(losses.get(\"train\", [])) + 1)\n\n    # 1) Loss curves ---------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Loss Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 2) Accuracy curves -----------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"train_acc\"], label=\"Train Acc\")\n        plt.plot(epochs, metrics[\"val_acc\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"{ds_name} Accuracy Curves\\nLeft: Train, Right: Val\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 3) Rule fidelity -------------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"rule_fidelity\"], marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Fidelity\")\n        plt.title(f\"{ds_name} Rule Fidelity over Epochs\")\n        fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot for {ds_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix ----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            num_classes = len(np.unique(gt))\n            cm = confusion_matrix_np(gt, preds, num_classes)\n            plt.figure(figsize=(4, 4))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{ds_name} Confusion Matrix\\nTest Set\")\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # Print final metric -----------------------------------------------------\n    try:\n        if preds is not None and gt is not None:\n            test_acc = (preds == gt).mean()\n            print(f\"{ds_name} final test accuracy: {test_acc:.3f}\")\n    except Exception as e:\n        print(f\"Error computing final accuracy for {ds_name}: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------------------------------------------------\nimport matplotlib\n\nmatplotlib.rcParams.update({\"figure.max_open_warning\": 0})\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# All experiment files to aggregate\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_b729f3272edc4e1e99a903c7218a2c0f_proc_3198317/experiment_data.npy\",\n    \"experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_39f76a7592064842840ca3a2981390e5_proc_3198319/experiment_data.npy\",\n    \"experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_befe90ecadd74cdbbf6d6ab06c34bbbe_proc_3198318/experiment_data.npy\",\n]\n\nall_runs = []\nroot = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\nfor p in experiment_data_path_list:\n    try:\n        data = np.load(os.path.join(root, p), allow_pickle=True).item()\n        all_runs.append(data)\n    except Exception as e:\n        print(f\"Error loading experiment data {p}: {e}\")\n\n\n# ------------------------------------------------------------------\n# Helper functions\ndef confusion_matrix_np(y_true, y_pred, num_classes):\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    return cm\n\n\ndef collect_series(run_list, ds_name, outer_key, inner_key):\n    \"\"\"Return stacked np.array shape (n_runs, min_len) for a series present in every run.\"\"\"\n    series_list = []\n    for run in run_list:\n        if ds_name not in run:  # dataset missing in that run\n            return None\n        container = run[ds_name].get(outer_key, {})\n        if inner_key not in container:\n            return None\n        series_list.append(np.asarray(container[inner_key]))\n    if not series_list:\n        return None\n    # truncate to shortest length\n    min_len = min(len(s) for s in series_list)\n    series_list = [s[:min_len] for s in series_list]\n    return np.vstack(series_list)\n\n\ndef aggregate_and_plot(ds_name, run_list):\n    # --------------- LOSS CURVES -----------------------------------\n    for outer, left_key, right_key, ylabel, ftag in [\n        (\"losses\", \"train\", \"val\", \"Loss\", \"loss\"),\n        (\"metrics\", \"train_acc\", \"val_acc\", \"Accuracy\", \"accuracy\"),\n    ]:\n        try:\n            left_mat = collect_series(run_list, ds_name, outer, left_key)\n            right_mat = collect_series(run_list, ds_name, outer, right_key)\n            if left_mat is None or right_mat is None:\n                continue\n            epochs = np.arange(1, left_mat.shape[1] + 1)\n            n_runs = left_mat.shape[0]\n\n            left_mean, left_sem = left_mat.mean(0), left_mat.std(0, ddof=1) / np.sqrt(\n                n_runs\n            )\n            right_mean, right_sem = right_mat.mean(0), right_mat.std(\n                0, ddof=1\n            ) / np.sqrt(n_runs)\n\n            plt.figure()\n            plt.plot(epochs, left_mean, label=f\"Train mean ({n_runs} runs)\")\n            plt.fill_between(\n                epochs,\n                left_mean - left_sem,\n                left_mean + left_sem,\n                alpha=0.3,\n                label=\"Train \u00b1 SEM\",\n            )\n            plt.plot(epochs, right_mean, label=\"Val mean\")\n            plt.fill_between(\n                epochs,\n                right_mean - right_sem,\n                right_mean + right_sem,\n                alpha=0.3,\n                label=\"Val \u00b1 SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(ylabel)\n            plt.title(f\"{ds_name} {ylabel} (mean \u00b1 SEM)\\nLeft: Train, Right: Val\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_{ftag}_agg.png\")\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated {ftag} plot for {ds_name}: {e}\")\n            plt.close()\n\n    # --------------- RULE FIDELITY -----------------------------------\n    try:\n        rf_mat = collect_series(run_list, ds_name, \"metrics\", \"rule_fidelity\")\n        if rf_mat is not None:\n            epochs = np.arange(1, rf_mat.shape[1] + 1)\n            n_runs = rf_mat.shape[0]\n            rf_mean = rf_mat.mean(0)\n            rf_sem = rf_mat.std(0, ddof=1) / np.sqrt(n_runs)\n\n            plt.figure()\n            plt.plot(epochs, rf_mean, marker=\"o\", label=f\"Mean ({n_runs} runs)\")\n            plt.fill_between(\n                epochs, rf_mean - rf_sem, rf_mean + rf_sem, alpha=0.3, label=\"\u00b1 SEM\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Fidelity\")\n            plt.title(f\"{ds_name} Rule Fidelity (mean \u00b1 SEM)\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity_agg.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated rule fidelity plot for {ds_name}: {e}\")\n        plt.close()\n\n    # --------------- CONFUSION MATRIX -----------------------------------\n    try:\n        # collect predictions and gt\n        cm_total = None\n        for run in run_list:\n            ds_dict = run.get(ds_name, {})\n            preds = ds_dict.get(\"predictions\", None)\n            gt = ds_dict.get(\"ground_truth\", None)\n            if preds is None or gt is None:\n                cm_total = None\n                break\n            num_classes = len(np.unique(gt))\n            cm = confusion_matrix_np(gt, preds, num_classes)\n            cm_total = cm if cm_total is None else cm_total + cm\n        if cm_total is not None:\n            cm_percent = cm_total / cm_total.sum() * 100.0\n            plt.figure(figsize=(4, 4))\n            plt.imshow(cm_percent, cmap=\"Blues\")\n            plt.colorbar(label=\"%\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name} Confusion Matrix\\nAggregated over {len(run_list)} runs\"\n            )\n            for i in range(num_classes):\n                for j in range(num_classes):\n                    plt.text(\n                        j,\n                        i,\n                        f\"{cm_percent[i, j]:.1f}\",\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\",\n                        fontsize=8,\n                    )\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix_agg.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # --------------- FINAL TEST ACCURACY -----------------------------------\n    try:\n        acc_list = []\n        for run in run_list:\n            ds_dict = run.get(ds_name, {})\n            preds = ds_dict.get(\"predictions\", None)\n            gt = ds_dict.get(\"ground_truth\", None)\n            if preds is not None and gt is not None:\n                acc_list.append((preds == gt).mean())\n        if acc_list:\n            acc_arr = np.asarray(acc_list)\n            mean_acc = acc_arr.mean()\n            sem_acc = acc_arr.std(ddof=1) / np.sqrt(len(acc_arr))\n            print(\n                f\"{ds_name} test accuracy: {mean_acc:.3f} \u00b1 {sem_acc:.3f} (SEM) over {len(acc_arr)} runs\"\n            )\n    except Exception as e:\n        print(f\"Error computing aggregated accuracy for {ds_name}: {e}\")\n\n\n# ------------------------------------------------------------------\n# Group runs by dataset and plot\ndatasets = set()\nfor r in all_runs:\n    datasets.update(r.keys())\n\nfor ds in datasets:\n    runs_with_ds = [r for r in all_runs if ds in r]\n    if runs_with_ds:\n        aggregate_and_plot(ds, runs_with_ds)\n"], "plot_plan": ["We will first load the saved experiment_data.npy file, recover the SPR_BENCH sub-dictionary, and extract its losses, accuracies, rule-fidelity, predictions and ground-truth arrays. We then create up to four key visualizations: (1) train/val loss over epochs, (2) train/val accuracy over epochs, (3) rule fidelity over epochs, and (4) a test-set confusion matrix. Each plot is wrapped in an independent try-except block to avoid halting the script if any data element is missing. All figures are saved in the \u201cworking\u201d folder using descriptive names that include the dataset and plot type, with clear titles and (when relevant) subtitles. After plotting we always close the figure to free resources. Finally, we print the final test accuracy so the evaluation metric is visible in the script output. The code respects the constraint of using only values present in experiment_data.npy and producing \u22645 plots total.", null, null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved NumPy dictionary, loop through each dataset\nentry, and for every recorded metric or loss it will compute either the maximum\n(for accuracies and rule fidelity) or the minimum (for losses). It then prints\nthe dataset name followed by clearly-labelled best values such as \u201cbest train\naccuracy,\u201d \u201cbest validation loss,\u201d and \u201cbest rule fidelity.\u201d All code runs\nimmediately on execution, with no special entry point guards, and adheres to the\ndirectory layout described in the prompt.", "", "", "", "The script will load the saved NumPy dictionary, loop through each dataset\nentry, and for every recorded metric or loss it will compute either the maximum\n(for accuracies and rule fidelity) or the minimum (for losses). It then prints\nthe dataset name followed by clearly-labelled best values such as \u201cbest train\naccuracy,\u201d \u201cbest validation loss,\u201d and \u201cbest rule fidelity.\u201d All code runs\nimmediately on execution, with no special entry point guards, and adheres to the\ndirectory layout described in the prompt.", "The script will load the saved NumPy dictionary, loop through each dataset\nentry, and for every recorded metric or loss it will compute either the maximum\n(for accuracies and rule fidelity) or the minimum (for losses). It then prints\nthe dataset name followed by clearly-labelled best values such as \u201cbest train\naccuracy,\u201d \u201cbest validation loss,\u201d and \u201cbest rule fidelity.\u201d All code runs\nimmediately on execution, with no special entry point guards, and adheres to the\ndirectory layout described in the prompt.", "The script will load the saved NumPy dictionary, loop through each dataset\nentry, and for every recorded metric or loss it will compute either the maximum\n(for accuracies and rule fidelity) or the minimum (for losses). It then prints\nthe dataset name followed by clearly-labelled best values such as \u201cbest train\naccuracy,\u201d \u201cbest validation loss,\u201d and \u201cbest rule fidelity.\u201d All code runs\nimmediately on execution, with no special entry point guards, and adheres to the\ndirectory layout described in the prompt.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------\ndef is_accuracy_key(k: str) -> bool:\n    \"\"\"Heuristic to decide if the metric should be maximised.\"\"\"\n    return any(\n        word in k.lower() for word in [\"acc\", \"fidelity\", \"f1\", \"precision\", \"recall\"]\n    )\n\n\n# 2\u20135. Traverse and print\nfor dataset_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle 'metrics' block (e.g., accuracies, rule fidelity, etc.)\n    if \"metrics\" in ds_dict:\n        for metric_name, series in ds_dict[\"metrics\"].items():\n            if len(series) == 0:\n                continue\n            series = np.asarray(series)\n            if is_accuracy_key(metric_name):\n                best_val = series.max()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n            else:  # in case some metric should be minimised\n                best_val = series.min()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n\n    # Handle 'losses' block\n    if \"losses\" in ds_dict:\n        for loss_name, series in ds_dict[\"losses\"].items():\n            if len(series) == 0:\n                continue\n            best_loss = np.asarray(series).min()  # lower is better for losses\n            pretty_name = (\n                loss_name + \" loss\" if \"loss\" not in loss_name.lower() else loss_name\n            )\n            print(f\"best {pretty_name}: {best_loss:.4f}\")\n\n    # Any other blocks with a list of scalar values can be added similarly\n", "", "", "", "import os\nimport numpy as np\n\n# ---------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------\ndef is_accuracy_key(k: str) -> bool:\n    \"\"\"Heuristic to decide if the metric should be maximised.\"\"\"\n    return any(\n        word in k.lower() for word in [\"acc\", \"fidelity\", \"f1\", \"precision\", \"recall\"]\n    )\n\n\n# 2\u20135. Traverse and print\nfor dataset_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle 'metrics' block (e.g., accuracies, rule fidelity, etc.)\n    if \"metrics\" in ds_dict:\n        for metric_name, series in ds_dict[\"metrics\"].items():\n            if len(series) == 0:\n                continue\n            series = np.asarray(series)\n            if is_accuracy_key(metric_name):\n                best_val = series.max()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n            else:  # in case some metric should be minimised\n                best_val = series.min()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n\n    # Handle 'losses' block\n    if \"losses\" in ds_dict:\n        for loss_name, series in ds_dict[\"losses\"].items():\n            if len(series) == 0:\n                continue\n            best_loss = np.asarray(series).min()  # lower is better for losses\n            pretty_name = (\n                loss_name + \" loss\" if \"loss\" not in loss_name.lower() else loss_name\n            )\n            print(f\"best {pretty_name}: {best_loss:.4f}\")\n\n    # Any other blocks with a list of scalar values can be added similarly\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------\ndef is_accuracy_key(k: str) -> bool:\n    \"\"\"Heuristic to decide if the metric should be maximised.\"\"\"\n    return any(\n        word in k.lower() for word in [\"acc\", \"fidelity\", \"f1\", \"precision\", \"recall\"]\n    )\n\n\n# 2\u20135. Traverse and print\nfor dataset_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle 'metrics' block (e.g., accuracies, rule fidelity, etc.)\n    if \"metrics\" in ds_dict:\n        for metric_name, series in ds_dict[\"metrics\"].items():\n            if len(series) == 0:\n                continue\n            series = np.asarray(series)\n            if is_accuracy_key(metric_name):\n                best_val = series.max()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n            else:  # in case some metric should be minimised\n                best_val = series.min()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n\n    # Handle 'losses' block\n    if \"losses\" in ds_dict:\n        for loss_name, series in ds_dict[\"losses\"].items():\n            if len(series) == 0:\n                continue\n            best_loss = np.asarray(series).min()  # lower is better for losses\n            pretty_name = (\n                loss_name + \" loss\" if \"loss\" not in loss_name.lower() else loss_name\n            )\n            print(f\"best {pretty_name}: {best_loss:.4f}\")\n\n    # Any other blocks with a list of scalar values can be added similarly\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------\n# 0. Locate working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------\ndef is_accuracy_key(k: str) -> bool:\n    \"\"\"Heuristic to decide if the metric should be maximised.\"\"\"\n    return any(\n        word in k.lower() for word in [\"acc\", \"fidelity\", \"f1\", \"precision\", \"recall\"]\n    )\n\n\n# 2\u20135. Traverse and print\nfor dataset_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Handle 'metrics' block (e.g., accuracies, rule fidelity, etc.)\n    if \"metrics\" in ds_dict:\n        for metric_name, series in ds_dict[\"metrics\"].items():\n            if len(series) == 0:\n                continue\n            series = np.asarray(series)\n            if is_accuracy_key(metric_name):\n                best_val = series.max()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n            else:  # in case some metric should be minimised\n                best_val = series.min()\n                print(f\"best {metric_name.replace('_', ' ')}: {best_val:.4f}\")\n\n    # Handle 'losses' block\n    if \"losses\" in ds_dict:\n        for loss_name, series in ds_dict[\"losses\"].items():\n            if len(series) == 0:\n                continue\n            best_loss = np.asarray(series).min()  # lower is better for losses\n            pretty_name = (\n                loss_name + \" loss\" if \"loss\" not in loss_name.lower() else loss_name\n            )\n            print(f\"best {pretty_name}: {best_loss:.4f}\")\n\n    # Any other blocks with a list of scalar values can be added similarly\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', 'best train acc: 0.9385', '\\n', 'best val acc:\n0.7620', '\\n', 'best rule fidelity: 0.6980', '\\n', 'best train loss: 0.2080',\n'\\n', 'best val loss: 0.7316', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "", "", "", "['\\nDataset: SPR_BENCH', '\\n', 'best train acc: 0.9620', '\\n', 'best val acc:\n0.7680', '\\n', 'best rule fidelity: 0.6920', '\\n', 'best train loss: 0.1714',\n'\\n', 'best val loss: 0.6221', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best train acc: 0.9365', '\\n', 'best val acc:\n0.7480', '\\n', 'best rule fidelity: 0.8600', '\\n', 'best train loss: 0.2572',\n'\\n', 'best val loss: 0.6464', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best train acc: 0.9630', '\\n', 'best val acc:\n0.7760', '\\n', 'best rule fidelity: 0.9220', '\\n', 'best train loss: 0.1926',\n'\\n', 'best val loss: 0.5877', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]}