{"edges": [[0, 4], [0, 2], [0, 1], [0, 3]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.3333333333333333, 1.0], [0.6666666666666666, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: optimizer_type (Adam \u2192 SGD with momentum). We extend the\nbaseline by running several training runs that only differ in the optimiser: the\noriginal Adam run and multiple SGD runs whose momentum \u2208 {0.0, 0.5, 0.9}.   For\nevery configuration we train the same logistic-regression model for 10 epochs,\nkeep per-epoch metrics, and finally pick the configuration with the best dev\naccuracy.   All results (per-epoch losses / accuracies, rule-fidelity values,\ntest predictions, etc.) are stored inside the mandatory experiment_data dict\nunder the hyper-parameter key \u201coptimizer_type\u201d and eventually saved to\n\u201cexperiment_data.npy\u201d.", "Ablation name: Unigram-only Feature Ablation. We perform the \u201cUnigram-only\u201d\nablation by changing the vectoriser so it builds a vocabulary of single\ncharacters and counts only those when encoding sequences; all bigram logic is\nremoved.  The data splits, logistic-regression model, optimiser grid, training\nloop, and rule-fidelity calculation remain identical to the baseline, so any\nchange in metrics is attributable solely to the absence of bigram context.\nResults for every optimiser configuration are collected in the prescribed\nexperiment_data structure and saved to \u2018experiment_data.npy\u2019.", "Ablation name: Length-Normalised n-gram Feature Ablation. We normalise each\nsequence\u2019s unigram + bigram feature vector by dividing all counts by (len(seq) +\nlen(seq)\u22121), thereby eliminating explicit sequence-length information while\npreserving relative n-gram distributions. The rest of the pipeline (vocab,\nloaders, model, training, evaluation, rule-fidelity computation, and result\nsaving) is kept identical, now stored under the ablation tag \u201clen_norm_ngram\u201d.", "Ablation name: No-Bias Logistic Regression Ablation. The ablation is implemented\nby defining `LogRegNoBias`, which uses `nn.Linear(..., bias=False)`.   The\nscript reuses the full data-processing and training pipeline from the baseline,\nbut trains this bias-free variant across the same optimizer grid, logging\naccuracy, loss, and rule fidelity (now computed without an intercept).   All\nresults are stored under the key `no_bias_log_reg -> SPR_BENCH` in\n`experiment_data`, and finally saved to `experiment_data.npy` for later plotting\nor comparison.", "Ablation name: TF-IDF Weighted n-gram Feature Ablation. The new implementation\nfirst collects document-frequency statistics for every unigram and bigram in the\ntraining split, converts them into idf scores (log-smoothed), and then\nmultiplies raw term counts by these idf weights when vectorising sequences.\nThis TF-IDF representation replaces the old frequency-only vectors, while all\ndownstream model-training, evaluation, and result-saving logic is kept\nidentical.  Results for every optimiser configuration are stored under a new\nablation key \u201ctfidf_ngram\u201d and written to experiment_data.npy for later\nanalysis."], "code": ["import os, pathlib, random, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------- REPRODUCIBILITY -------------------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------- WORK DIR --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- DATA ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# --------------- n-gram vectoriser ---------------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        chars = list(s)\n        unis.update(chars)\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    return {tok: i for i, tok in enumerate(vocab)}\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    for c in seq:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    return v\n\n\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\n\n\n# -------------------- MODEL ----------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# ------------- EXPERIMENT / H-PARAM STORE --------------\nexperiment_data = {\n    \"optimizer_type\": {\n        \"SPR_BENCH\": {\n            \"configs\": [],  # names of runs\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],  # best config only\n            \"ground_truth\": y_test.tolist(),\n            \"best_config\": \"\",\n        }\n    }\n}\n\n\n# ------------- HELPER: EVALUATION ----------------------\ndef evaluate(model, loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    return correct / total, loss_sum / total, torch.cat(all_logits)\n\n\n# ------------- HYPER-PARAM GRID ------------------------\ngrid = [(\"adam\", None)] + [(\"sgd\", m) for m in (0.0, 0.5, 0.9)]\nlr_map = {\"adam\": 1e-3, \"sgd\": 0.1}  # simple LR choice\nepochs, top_k = 10, 10\n\nbest_val_acc, best_pred, best_cfg_name = -1.0, None, \"\"\n\nfor opt_name, momentum in grid:\n    cfg_name = opt_name if opt_name == \"adam\" else f\"sgd_m{momentum}\"\n    experiment_data[\"optimizer_type\"][\"SPR_BENCH\"][\"configs\"].append(cfg_name)\n    print(f\"\\n===== Training with {cfg_name} =====\")\n\n    model = LogReg(num_feats, num_classes).to(device)\n    if opt_name == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr_map[\"adam\"])\n    else:\n        optimizer = optim.SGD(model.parameters(), lr=lr_map[\"sgd\"], momentum=momentum)\n\n    run_train_acc, run_val_acc, run_rule_fid = [], [], []\n    run_train_loss, run_val_loss = [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        seen, correct, running_loss = 0, 0, 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"y\"].size(0)\n            _, preds = torch.max(logits, 1)\n            correct += (preds == batch[\"y\"]).sum().item()\n            seen += batch[\"y\"].size(0)\n\n        train_loss = running_loss / seen\n        train_acc = correct / seen\n        val_acc, val_loss, val_logits = evaluate(model, dev_loader)\n\n        # ----- rule fidelity -----\n        W = model.linear.weight.detach().cpu().numpy()\n        b = model.linear.bias.detach().cpu().numpy()\n        W_trunc = np.zeros_like(W)\n        for c in range(num_classes):\n            idxs = np.argsort(-np.abs(W[c]))[:top_k]\n            W_trunc[c, idxs] = W[c, idxs]\n        lin_full = torch.from_numpy((X_dev @ W.T) + b)\n        lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n        rule_fid = (\n            (torch.argmax(lin_trunc, 1) == torch.argmax(lin_full, 1))\n            .float()\n            .mean()\n            .item()\n        )\n\n        # store per-epoch\n        run_train_acc.append(train_acc)\n        run_val_acc.append(val_acc)\n        run_rule_fid.append(rule_fid)\n        run_train_loss.append(train_loss)\n        run_val_loss.append(val_loss)\n\n        print(\n            f\"Epoch {epoch}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n        )\n\n    # ----- aggregate & save run -----\n    ed = experiment_data[\"optimizer_type\"][\"SPR_BENCH\"]\n    ed[\"metrics\"][\"train_acc\"].append(run_train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(run_val_acc)\n    ed[\"metrics\"][\"rule_fidelity\"].append(run_rule_fid)\n    ed[\"losses\"][\"train\"].append(run_train_loss)\n    ed[\"losses\"][\"val\"].append(run_val_loss)\n\n    if run_val_acc[-1] > best_val_acc:\n        best_val_acc = run_val_acc[-1]\n        best_cfg_name = cfg_name\n        test_acc, test_loss, test_logits = evaluate(model, test_loader)\n        best_pred = torch.argmax(test_logits, 1).cpu().numpy()\n        print(f\"*** New best config: {cfg_name} with test_acc={test_acc:.3f}\")\n\n# ------------ FINAL SAVE --------------------------------\ned = experiment_data[\"optimizer_type\"][\"SPR_BENCH\"]\ned[\"predictions\"] = best_pred.tolist()\ned[\"best_config\"] = best_cfg_name\n\n\n# convert lists to numpy for saving\ndef to_np(o):\n    return np.array(o, dtype=object)\n\n\nfor k in [\"train_acc\", \"val_acc\", \"rule_fidelity\"]:\n    ed[\"metrics\"][k] = to_np(ed[\"metrics\"][k])\nfor k in [\"train\", \"val\"]:\n    ed[\"losses\"][k] = to_np(ed[\"losses\"][k])\ned[\"predictions\"] = np.array(ed[\"predictions\"])\ned[\"ground_truth\"] = np.array(ed[\"ground_truth\"])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nBest configuration: {best_cfg_name} with dev_acc={best_val_acc:.3f}\")\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------- REPRODUCIBILITY -------------------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------- WORK DIR --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- DATA ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# --------------- unigram vectoriser --------------------\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    unigrams = sorted(list({ch for s in seqs for ch in s}))\n    return {tok: i for i, tok in enumerate(unigrams)}\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    for ch in seq:\n        if ch in idx:\n            v[idx[ch]] += 1.0\n    return v\n\n\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size (unigram only): {num_feats}\")\n\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\n\n\n# -------------------- MODEL ----------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# ------------- EXPERIMENT STORE ------------------------\nexperiment_data = {\n    \"unigram_only\": {\n        \"SPR_BENCH\": {\n            \"configs\": [],\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_test.tolist(),\n            \"best_config\": \"\",\n        }\n    }\n}\n\n\n# ------------- HELPER: EVALUATION ----------------------\ndef evaluate(model, loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    return correct / total, loss_sum / total, torch.cat(all_logits)\n\n\n# ------------- HYPER-PARAM GRID ------------------------\ngrid = [(\"adam\", None)] + [(\"sgd\", m) for m in (0.0, 0.5, 0.9)]\nlr_map = {\"adam\": 1e-3, \"sgd\": 0.1}\nepochs, top_k = 10, 10\n\nbest_val_acc, best_pred, best_cfg_name = -1.0, None, \"\"\ned = experiment_data[\"unigram_only\"][\"SPR_BENCH\"]\n\nfor opt_name, momentum in grid:\n    cfg_name = opt_name if opt_name == \"adam\" else f\"sgd_m{momentum}\"\n    ed[\"configs\"].append(cfg_name)\n    print(f\"\\n===== Training with {cfg_name} =====\")\n    model = LogReg(num_feats, num_classes).to(device)\n    optimizer = (\n        optim.Adam(model.parameters(), lr=lr_map[\"adam\"])\n        if opt_name == \"adam\"\n        else optim.SGD(model.parameters(), lr=lr_map[\"sgd\"], momentum=momentum)\n    )\n\n    run_train_acc, run_val_acc, run_rule_fid = [], [], []\n    run_train_loss, run_val_loss = [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        seen, correct, running_loss = 0, 0, 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"y\"].size(0)\n            _, preds = torch.max(logits, 1)\n            correct += (preds == batch[\"y\"]).sum().item()\n            seen += batch[\"y\"].size(0)\n\n        train_loss = running_loss / seen\n        train_acc = correct / seen\n        val_acc, val_loss, _ = evaluate(model, dev_loader)\n\n        # ---- rule fidelity (top-k weights) ----\n        W = model.linear.weight.detach().cpu().numpy()\n        b = model.linear.bias.detach().cpu().numpy()\n        W_trunc = np.zeros_like(W)\n        for c in range(num_classes):\n            idxs = np.argsort(-np.abs(W[c]))[:top_k]\n            W_trunc[c, idxs] = W[c, idxs]\n        lin_full = torch.from_numpy((X_dev @ W.T) + b)\n        lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n        rule_fid = (\n            (torch.argmax(lin_trunc, 1) == torch.argmax(lin_full, 1))\n            .float()\n            .mean()\n            .item()\n        )\n\n        run_train_acc.append(train_acc)\n        run_val_acc.append(val_acc)\n        run_rule_fid.append(rule_fid)\n        run_train_loss.append(train_loss)\n        run_val_loss.append(val_loss)\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n        )\n\n    ed[\"metrics\"][\"train_acc\"].append(run_train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(run_val_acc)\n    ed[\"metrics\"][\"rule_fidelity\"].append(run_rule_fid)\n    ed[\"losses\"][\"train\"].append(run_train_loss)\n    ed[\"losses\"][\"val\"].append(run_val_loss)\n\n    if run_val_acc[-1] > best_val_acc:\n        best_val_acc = run_val_acc[-1]\n        best_cfg_name = cfg_name\n        test_acc, test_loss, test_logits = evaluate(model, test_loader)\n        best_pred = torch.argmax(test_logits, 1).cpu().numpy()\n        print(f\"*** New best config: {cfg_name} with test_acc={test_acc:.3f}\")\n\n# ------------ FINAL SAVE --------------------------------\ned[\"predictions\"] = best_pred.tolist()\ned[\"best_config\"] = best_cfg_name\n\n\ndef to_np(o):\n    return np.array(o, dtype=object)\n\n\nfor k in [\"train_acc\", \"val_acc\", \"rule_fidelity\"]:\n    ed[\"metrics\"][k] = to_np(ed[\"metrics\"][k])\nfor k in [\"train\", \"val\"]:\n    ed[\"losses\"][k] = to_np(ed[\"losses\"][k])\ned[\"predictions\"] = np.array(ed[\"predictions\"])\ned[\"ground_truth\"] = np.array(ed[\"ground_truth\"])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nBest configuration: {best_cfg_name} with dev_acc={best_val_acc:.3f}\")\nprint(\"Saved results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------- REPRODUCIBILITY -------------------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------- WORK DIR --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- DATA ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# --------------- n-gram vectoriser ---------------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        unis.update(list(s))\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    return {tok: i for i, tok in enumerate(vocab)}\n\n\ndef vectorise_len_norm(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    for c in seq:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    denom = len(seq) + max(len(seq) - 1, 0)\n    if denom:\n        v /= denom\n    return v\n\n\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise_len_norm(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\n\n\n# -------------------- MODEL ----------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# ------------- EXPERIMENT / H-PARAM STORE --------------\nexperiment_data = {\n    \"len_norm_ngram\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_test.tolist(),\n            \"configs\": [],\n            \"best_config\": \"\",\n        }\n    }\n}\n\n\n# ------------- HELPER: EVALUATION ----------------------\ndef evaluate(model, loader):\n    model.eval()\n    total = correct = loss_sum = 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    return correct / total, loss_sum / total, torch.cat(all_logits)\n\n\n# ------------- HYPER-PARAM GRID ------------------------\ngrid = [(\"adam\", None)] + [(\"sgd\", m) for m in (0.0, 0.5, 0.9)]\nlr_map = {\"adam\": 1e-3, \"sgd\": 0.1}\nepochs, top_k = 10, 10\nbest_val_acc, best_pred, best_cfg_name = -1.0, None, \"\"\ned = experiment_data[\"len_norm_ngram\"][\"SPR_BENCH\"]\n\nfor opt_name, momentum in grid:\n    cfg_name = opt_name if opt_name == \"adam\" else f\"sgd_m{momentum}\"\n    ed[\"configs\"].append(cfg_name)\n    print(f\"\\n===== Training with {cfg_name} =====\")\n    model = LogReg(num_feats, num_classes).to(device)\n    optimizer = (\n        optim.Adam(model.parameters(), lr=lr_map[\"adam\"])\n        if opt_name == \"adam\"\n        else optim.SGD(model.parameters(), lr=lr_map[\"sgd\"], momentum=momentum)\n    )\n\n    run_train_acc, run_val_acc, run_rule_fid, run_train_loss, run_val_loss = (\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        seen = correct = running_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"y\"].size(0)\n            _, preds = torch.max(logits, 1)\n            correct += (preds == batch[\"y\"]).sum().item()\n            seen += batch[\"y\"].size(0)\n\n        train_loss = running_loss / seen\n        train_acc = correct / seen\n        val_acc, val_loss, _ = evaluate(model, dev_loader)\n\n        # rule fidelity\n        W = model.linear.weight.detach().cpu().numpy()\n        b = model.linear.bias.detach().cpu().numpy()\n        W_trunc = np.zeros_like(W)\n        for c in range(num_classes):\n            idxs = np.argsort(-np.abs(W[c]))[:top_k]\n            W_trunc[c, idxs] = W[c, idxs]\n        lin_full = torch.from_numpy((X_dev @ W.T) + b)\n        lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n        rule_fid = (\n            (torch.argmax(lin_trunc, 1) == torch.argmax(lin_full, 1))\n            .float()\n            .mean()\n            .item()\n        )\n\n        run_train_acc.append(train_acc)\n        run_val_acc.append(val_acc)\n        run_rule_fid.append(rule_fid)\n        run_train_loss.append(train_loss)\n        run_val_loss.append(val_loss)\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n        )\n\n    ed[\"metrics\"][\"train_acc\"].append(run_train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(run_val_acc)\n    ed[\"metrics\"][\"rule_fidelity\"].append(run_rule_fid)\n    ed[\"losses\"][\"train\"].append(run_train_loss)\n    ed[\"losses\"][\"val\"].append(run_val_loss)\n\n    if run_val_acc[-1] > best_val_acc:\n        best_val_acc = run_val_acc[-1]\n        best_cfg_name = cfg_name\n        test_acc, test_loss, test_logits = evaluate(model, test_loader)\n        best_pred = torch.argmax(test_logits, 1).cpu().numpy()\n        print(f\"*** New best config: {cfg_name} with test_acc={test_acc:.3f}\")\n\ned[\"predictions\"] = best_pred.tolist()\ned[\"best_config\"] = best_cfg_name\n\n\n# convert lists to numpy and save\ndef to_np(o):\n    return np.array(o, dtype=object)\n\n\nfor k in [\"train_acc\", \"val_acc\", \"rule_fidelity\"]:\n    ed[\"metrics\"][k] = to_np(ed[\"metrics\"][k])\nfor k in [\"train\", \"val\"]:\n    ed[\"losses\"][k] = to_np(ed[\"losses\"][k])\ned[\"predictions\"] = np.array(ed[\"predictions\"])\ned[\"ground_truth\"] = np.array(ed[\"ground_truth\"])\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nBest configuration: {best_cfg_name} with dev_acc={best_val_acc:.3f}\")\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------- REPRODUCIBILITY -------------------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------- WORK DIR --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- DATA ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# --------------- n-gram vectoriser ---------------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        unis.update(list(s))\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    return {tok: i for i, tok in enumerate(vocab)}\n\n\ndef vectorise(seq: str, idx: Dict[str, int]) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    for c in seq:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    return v\n\n\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack([vectorise(s, vocab_idx) for s in dsets[split][\"sequence\"]])\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\n\n\n# -------------------- NO-BIAS MODEL --------------------\nclass LogRegNoBias(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=False)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# ------------- EXPERIMENT / H-PARAM STORE --------------\nexperiment_data = {\n    \"no_bias_log_reg\": {\n        \"SPR_BENCH\": {\n            \"configs\": [],\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_test.tolist(),\n            \"best_config\": \"\",\n        }\n    }\n}\n\n\n# ------------- HELPER: EVALUATION ----------------------\ndef evaluate(model, loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    return correct / total, loss_sum / total, torch.cat(all_logits)\n\n\n# ------------- HYPER-PARAM GRID ------------------------\ngrid = [(\"adam\", None)] + [(\"sgd\", m) for m in (0.0, 0.5, 0.9)]\nlr_map = {\"adam\": 1e-3, \"sgd\": 0.1}\nepochs, top_k = 10, 10\n\nbest_val_acc, best_pred, best_cfg_name = -1.0, None, \"\"\nfor opt_name, momentum in grid:\n    cfg_name = opt_name if opt_name == \"adam\" else f\"sgd_m{momentum}\"\n    ed = experiment_data[\"no_bias_log_reg\"][\"SPR_BENCH\"]\n    ed[\"configs\"].append(cfg_name)\n    print(f\"\\n===== Training (no-bias) with {cfg_name} =====\")\n\n    model = LogRegNoBias(num_feats, num_classes).to(device)\n    optimizer = (\n        optim.Adam(model.parameters(), lr=lr_map[\"adam\"])\n        if opt_name == \"adam\"\n        else optim.SGD(model.parameters(), lr=lr_map[\"sgd\"], momentum=momentum)\n    )\n\n    run_train_acc, run_val_acc, run_rule_fid = [], [], []\n    run_train_loss, run_val_loss = [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        seen, correct, running_loss = 0, 0, 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"y\"].size(0)\n            _, preds = torch.max(logits, 1)\n            correct += (preds == batch[\"y\"]).sum().item()\n            seen += batch[\"y\"].size(0)\n        train_loss = running_loss / seen\n        train_acc = correct / seen\n        val_acc, val_loss, val_logits = evaluate(model, dev_loader)\n\n        # rule fidelity (without bias)\n        W = model.linear.weight.detach().cpu().numpy()\n        W_trunc = np.zeros_like(W)\n        for c in range(num_classes):\n            idxs = np.argsort(-np.abs(W[c]))[:top_k]\n            W_trunc[c, idxs] = W[c, idxs]\n        lin_full = torch.from_numpy(X_dev @ W.T)\n        lin_trunc = torch.from_numpy(X_dev @ W_trunc.T)\n        rule_fid = (\n            (torch.argmax(lin_trunc, 1) == torch.argmax(lin_full, 1))\n            .float()\n            .mean()\n            .item()\n        )\n\n        run_train_acc.append(train_acc)\n        run_val_acc.append(val_acc)\n        run_rule_fid.append(rule_fid)\n        run_train_loss.append(train_loss)\n        run_val_loss.append(val_loss)\n\n        print(\n            f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n        )\n\n    # store run metrics\n    ed[\"metrics\"][\"train_acc\"].append(run_train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(run_val_acc)\n    ed[\"metrics\"][\"rule_fidelity\"].append(run_rule_fid)\n    ed[\"losses\"][\"train\"].append(run_train_loss)\n    ed[\"losses\"][\"val\"].append(run_val_loss)\n\n    if run_val_acc[-1] > best_val_acc:\n        best_val_acc = run_val_acc[-1]\n        best_cfg_name = cfg_name\n        test_acc, test_loss, test_logits = evaluate(model, test_loader)\n        best_pred = torch.argmax(test_logits, 1).cpu().numpy()\n        print(f\"*** New best config: {cfg_name} with test_acc={test_acc:.3f}\")\n\n# ------------ FINAL SAVE --------------------------------\ned = experiment_data[\"no_bias_log_reg\"][\"SPR_BENCH\"]\ned[\"predictions\"] = best_pred.tolist()\ned[\"best_config\"] = best_cfg_name\n\n\ndef to_np(o):\n    return np.array(o, dtype=object)\n\n\nfor k in [\"train_acc\", \"val_acc\", \"rule_fidelity\"]:\n    ed[\"metrics\"][k] = to_np(ed[\"metrics\"][k])\nfor k in [\"train\", \"val\"]:\n    ed[\"losses\"][k] = to_np(ed[\"losses\"][k])\ned[\"predictions\"] = np.array(ed[\"predictions\"])\ned[\"ground_truth\"] = np.array(ed[\"ground_truth\"])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f\"\\nBest configuration (no-bias): {best_cfg_name} with dev_acc={best_val_acc:.3f}\"\n)\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nfrom datasets import DatasetDict, load_dataset\n\n# ------------------- REPRODUCIBILITY -------------------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ------------------- WORK DIR --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- DATA ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# change this path if necessary\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\ndsets = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\n\n# --------------- n-gram TF-IDF vectoriser -------------\ndef build_vocab(seqs: List[str]):\n    unis, bis = set(), set()\n    for s in seqs:\n        unis.update(list(s))\n        bis.update([s[i : i + 2] for i in range(len(s) - 1)])\n    vocab = sorted(list(unis)) + sorted(list(bis))\n    return {tok: i for i, tok in enumerate(vocab)}\n\n\ndef compute_idf(seqs: List[str], idx: Dict[str, int]) -> np.ndarray:\n    N = len(seqs)\n    df = np.zeros(len(idx), dtype=np.int32)\n    for s in seqs:\n        seen = set()\n        for c in s:\n            if c in idx:\n                seen.add(idx[c])\n        for i in range(len(s) - 1):\n            bg = s[i : i + 2]\n            if bg in idx:\n                seen.add(idx[bg])\n        for j in seen:\n            df[j] += 1\n    idf = np.log((N + 1) / (df + 1)) + 1.0\n    return idf.astype(np.float32)\n\n\ndef vectorise_tfidf(seq: str, idx: Dict[str, int], idf_vec: np.ndarray) -> np.ndarray:\n    v = np.zeros(len(idx), dtype=np.float32)\n    for c in seq:\n        if c in idx:\n            v[idx[c]] += 1.0\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        if bg in idx:\n            v[idx[bg]] += 1.0\n    v *= idf_vec\n    return v\n\n\ntrain_seqs = dsets[\"train\"][\"sequence\"]\nvocab_idx = build_vocab(train_seqs)\nidf_vec = compute_idf(train_seqs, vocab_idx)\nnum_feats = len(vocab_idx)\nprint(f\"Feature size: {num_feats}\")\n\nlabels = sorted(list(set(dsets[\"train\"][\"label\"])))\nlabel2id = {l: i for i, l in enumerate(labels)}\nnum_classes = len(labels)\nprint(f\"Classes: {labels}\")\n\n\ndef encode_split(split):\n    X = np.stack(\n        [vectorise_tfidf(s, vocab_idx, idf_vec) for s in dsets[split][\"sequence\"]]\n    )\n    y = np.array([label2id[l] for l in dsets[split][\"label\"]], dtype=np.int64)\n    return X, y\n\n\nX_train, y_train = encode_split(\"train\")\nX_dev, y_dev = encode_split(\"dev\")\nX_test, y_test = encode_split(\"test\")\n\n\nclass NgramDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = X, y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": torch.from_numpy(self.X[idx]), \"y\": torch.tensor(self.y[idx])}\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    NgramDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(NgramDataset(X_dev, y_dev), batch_size=batch_size)\ntest_loader = DataLoader(NgramDataset(X_test, y_test), batch_size=batch_size)\n\n\n# -------------------- MODEL ----------------------------\nclass LogReg(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_classes, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# ---------- EXPERIMENT DATA CONTAINER -----------------\nexperiment_data = {\n    \"tfidf_ngram\": {\n        \"SPR_BENCH\": {\n            \"configs\": [],\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_test.tolist(),\n            \"best_config\": \"\",\n        }\n    }\n}\n\n\n# ---------------- HELPER: EVALUATION -------------------\ndef evaluate(model, loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    all_logits = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            _, preds = torch.max(logits, 1)\n            total += batch[\"y\"].size(0)\n            correct += (preds == batch[\"y\"]).sum().item()\n            loss_sum += loss.item() * batch[\"y\"].size(0)\n            all_logits.append(logits.cpu())\n    return correct / total, loss_sum / total, torch.cat(all_logits)\n\n\n# ------------- HYPER-PARAM GRID ------------------------\ngrid = [(\"adam\", None)] + [(\"sgd\", m) for m in (0.0, 0.5, 0.9)]\nlr_map = {\"adam\": 1e-3, \"sgd\": 0.1}\nepochs, top_k = 10, 10\n\nbest_val_acc, best_pred, best_cfg_name = -1.0, None, \"\"\n\nfor opt_name, momentum in grid:\n    cfg_name = opt_name if opt_name == \"adam\" else f\"sgd_m{momentum}\"\n    ed_branch = experiment_data[\"tfidf_ngram\"][\"SPR_BENCH\"]\n    ed_branch[\"configs\"].append(cfg_name)\n    print(f\"\\n===== Training with {cfg_name} =====\")\n\n    model = LogReg(num_feats, num_classes).to(device)\n    optimizer = (\n        optim.Adam(model.parameters(), lr=lr_map[\"adam\"])\n        if opt_name == \"adam\"\n        else optim.SGD(model.parameters(), lr=lr_map[\"sgd\"], momentum=momentum)\n    )\n\n    run_train_acc, run_val_acc, run_rule_fid = [], [], []\n    run_train_loss, run_val_loss = [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        seen, correct, running_loss = 0, 0, 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            logits = model(batch[\"x\"])\n            loss = criterion(logits, batch[\"y\"])\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * batch[\"y\"].size(0)\n            _, preds = torch.max(logits, 1)\n            correct += (preds == batch[\"y\"]).sum().item()\n            seen += batch[\"y\"].size(0)\n\n        train_loss = running_loss / seen\n        train_acc = correct / seen\n        val_acc, val_loss, val_logits = evaluate(model, dev_loader)\n\n        # rule fidelity\n        W = model.linear.weight.detach().cpu().numpy()\n        b = model.linear.bias.detach().cpu().numpy()\n        W_trunc = np.zeros_like(W)\n        for c in range(num_classes):\n            idxs = np.argsort(-np.abs(W[c]))[:top_k]\n            W_trunc[c, idxs] = W[c, idxs]\n        lin_full = torch.from_numpy((X_dev @ W.T) + b)\n        lin_trunc = torch.from_numpy((X_dev @ W_trunc.T) + b)\n        rule_fid = (\n            (torch.argmax(lin_trunc, 1) == torch.argmax(lin_full, 1))\n            .float()\n            .mean()\n            .item()\n        )\n\n        run_train_acc.append(train_acc)\n        run_val_acc.append(val_acc)\n        run_rule_fid.append(rule_fid)\n        run_train_loss.append(train_loss)\n        run_val_loss.append(val_loss)\n\n        print(\n            f\"Epoch {epoch}: \"\n            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f} rule_fid={rule_fid:.3f}\"\n        )\n\n    # ----- aggregate & save run -----\n    ed_branch[\"metrics\"][\"train_acc\"].append(run_train_acc)\n    ed_branch[\"metrics\"][\"val_acc\"].append(run_val_acc)\n    ed_branch[\"metrics\"][\"rule_fidelity\"].append(run_rule_fid)\n    ed_branch[\"losses\"][\"train\"].append(run_train_loss)\n    ed_branch[\"losses\"][\"val\"].append(run_val_loss)\n\n    if run_val_acc[-1] > best_val_acc:\n        best_val_acc = run_val_acc[-1]\n        best_cfg_name = cfg_name\n        test_acc, test_loss, test_logits = evaluate(model, test_loader)\n        best_pred = torch.argmax(test_logits, 1).cpu().numpy()\n        print(f\"*** New best config: {cfg_name} with test_acc={test_acc:.3f}\")\n\n# ---------------- FINAL SAVE ---------------------------\ned_branch[\"predictions\"] = best_pred.tolist()\ned_branch[\"best_config\"] = best_cfg_name\n\n\ndef to_np(o):\n    return np.array(o, dtype=object)\n\n\nfor k in [\"train_acc\", \"val_acc\", \"rule_fidelity\"]:\n    ed_branch[\"metrics\"][k] = to_np(ed_branch[\"metrics\"][k])\nfor k in [\"train\", \"val\"]:\n    ed_branch[\"losses\"][k] = to_np(ed_branch[\"losses\"][k])\ned_branch[\"predictions\"] = np.array(ed_branch[\"predictions\"])\ned_branch[\"ground_truth\"] = np.array(ed_branch[\"ground_truth\"])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nBest configuration: {best_cfg_name} with dev_acc={best_val_acc:.3f}\")\nprint(\"Saved all results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Feature size: 33', '\\n', 'Classes: [0, 1]', '\\n', '\\n===== Training with adam\n=====', '\\n', 'Epoch 1: train_loss=0.7436 val_loss=0.7394 train_acc=0.597\nval_acc=0.602 rule_fid=0.692', '\\n', 'Epoch 2: train_loss=0.5482 val_loss=0.6748\ntrain_acc=0.721 val_acc=0.656 rule_fid=0.594', '\\n', 'Epoch 3: train_loss=0.4312\nval_loss=0.6377 train_acc=0.820 val_acc=0.692 rule_fid=0.508', '\\n', 'Epoch 4:\ntrain_loss=0.3503 val_loss=0.6262 train_acc=0.882 val_acc=0.728 rule_fid=0.544',\n'\\n', 'Epoch 5: train_loss=0.2934 val_loss=0.6221 train_acc=0.915 val_acc=0.742\nrule_fid=0.540', '\\n', 'Epoch 6: train_loss=0.2538 val_loss=0.6304\ntrain_acc=0.934 val_acc=0.750 rule_fid=0.550', '\\n', 'Epoch 7: train_loss=0.2236\nval_loss=0.6397 train_acc=0.950 val_acc=0.766 rule_fid=0.542', '\\n', 'Epoch 8:\ntrain_loss=0.2020 val_loss=0.6552 train_acc=0.956 val_acc=0.764 rule_fid=0.550',\n'\\n', 'Epoch 9: train_loss=0.1854 val_loss=0.6666 train_acc=0.958 val_acc=0.768\nrule_fid=0.548', '\\n', 'Epoch 10: train_loss=0.1714 val_loss=0.6880\ntrain_acc=0.962 val_acc=0.764 rule_fid=0.554', '\\n', '*** New best config: adam\nwith test_acc=0.773', '\\n', '\\n===== Training with sgd_m0.0 =====', '\\n', 'Epoch\n1: train_loss=39.0111 val_loss=15.1629 train_acc=0.514 val_acc=0.524\nrule_fid=0.992', '\\n', 'Epoch 2: train_loss=10.0341 val_loss=11.7155\ntrain_acc=0.772 val_acc=0.786 rule_fid=0.972', '\\n', 'Epoch 3: train_loss=0.1667\nval_loss=12.1988 train_acc=0.983 val_acc=0.722 rule_fid=0.716', '\\n', 'Epoch 4:\ntrain_loss=0.3151 val_loss=11.8325 train_acc=0.973 val_acc=0.794\nrule_fid=0.994', '\\n', 'Epoch 5: train_loss=0.0887 val_loss=11.7407\ntrain_acc=0.991 val_acc=0.794 rule_fid=0.996', '\\n', 'Epoch 6: train_loss=0.1367\nval_loss=11.6721 train_acc=0.984 val_acc=0.796 rule_fid=0.998', '\\n', 'Epoch 7:\ntrain_loss=0.0849 val_loss=11.5397 train_acc=0.991 val_acc=0.782\nrule_fid=0.528', '\\n', 'Epoch 8: train_loss=0.0734 val_loss=11.5750\ntrain_acc=0.992 val_acc=0.796 rule_fid=0.498', '\\n', 'Epoch 9: train_loss=0.1669\nval_loss=11.4931 train_acc=0.985 val_acc=0.788 rule_fid=0.516', '\\n', 'Epoch 10:\ntrain_loss=0.0789 val_loss=12.6834 train_acc=0.989 val_acc=0.772\nrule_fid=0.982', '\\n', '*** New best config: sgd_m0.0 with test_acc=0.782',\n'\\n', '\\n===== Training with sgd_m0.5 =====', '\\n', 'Epoch 1: train_loss=27.2884\nval_loss=13.9427 train_acc=0.593 val_acc=0.750 rule_fid=0.974', '\\n', 'Epoch 2:\ntrain_loss=4.7124 val_loss=28.6372 train_acc=0.801 val_acc=0.636\nrule_fid=0.966', '\\n', 'Epoch 3: train_loss=2.8910 val_loss=17.9485\ntrain_acc=0.916 val_acc=0.792 rule_fid=0.978', '\\n', 'Epoch 4: train_loss=0.1335\nval_loss=19.8039 train_acc=0.987 val_acc=0.768 rule_fid=0.948', '\\n', 'Epoch 5:\ntrain_loss=0.2181 val_loss=17.8316 train_acc=0.989 val_acc=0.790\nrule_fid=0.974', '\\n', 'Epoch 6: train_loss=0.1924 val_loss=17.9536\ntrain_acc=0.980 val_acc=0.796 rule_fid=0.960', '\\n', 'Epoch 7: train_loss=0.1707\nval_loss=17.9166 train_acc=0.986 val_acc=0.796 rule_fid=0.996', '\\n', 'Epoch 8:\ntrain_loss=0.0910 val_loss=18.5828 train_acc=0.991 val_acc=0.788\nrule_fid=0.968', '\\n', 'Epoch 9: train_loss=0.1096 val_loss=17.7809\ntrain_acc=0.991 val_acc=0.796 rule_fid=0.996', '\\n', 'Epoch 10:\ntrain_loss=0.4904 val_loss=18.6188 train_acc=0.967 val_acc=0.792\nrule_fid=0.964', '\\n', '*** New best config: sgd_m0.5 with test_acc=0.796',\n'\\n', '\\n===== Training with sgd_m0.9 =====', '\\n', 'Epoch 1: train_loss=71.6328\nval_loss=72.5873 train_acc=0.577 val_acc=0.652 rule_fid=0.924', '\\n', 'Epoch 2:\ntrain_loss=8.0266 val_loss=69.6533 train_acc=0.890 val_acc=0.762\nrule_fid=0.654', '\\n', 'Epoch 3: train_loss=1.9766 val_loss=75.6620\ntrain_acc=0.977 val_acc=0.770 rule_fid=0.650', '\\n', 'Epoch 4: train_loss=0.5616\nval_loss=76.2565 train_acc=0.989 val_acc=0.784 rule_fid=0.716', '\\n', 'Epoch 5:\ntrain_loss=0.5909 val_loss=75.9738 train_acc=0.992 val_acc=0.782\nrule_fid=0.664', '\\n', 'Epoch 6: train_loss=0.4162 val_loss=76.0323\ntrain_acc=0.989 val_acc=0.796 rule_fid=0.648', '\\n', 'Epoch 7: train_loss=0.4936\nval_loss=75.3461 train_acc=0.993 val_acc=0.784 rule_fid=0.620', '\\n', 'Epoch 8:\ntrain_loss=0.3376 val_loss=76.0324 train_acc=0.988 val_acc=0.796\nrule_fid=0.706', '\\n', 'Epoch 9: train_loss=0.4990 val_loss=75.7492\ntrain_acc=0.990 val_acc=0.796 rule_fid=0.708', '\\n', 'Epoch 10:\ntrain_loss=0.5044 val_loss=78.6547 train_acc=0.987 val_acc=0.788\nrule_fid=0.964', '\\n', '\\nBest configuration: sgd_m0.5 with dev_acc=0.792',\n'\\n', 'Saved all results to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n15/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 42917.48\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 118590.36\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 97739.71\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize (unigram only): 9', '\\n', 'Classes: [0, 1]', '\\n', '\\n===== Training with\nadam =====', '\\n', 'Epoch 1: train_loss=1.0026 val_loss=0.9034 train_acc=0.438\nval_acc=0.474 rule_fid=1.000', '\\n', 'Epoch 2: train_loss=0.8893 val_loss=0.8380\ntrain_acc=0.484 val_acc=0.512 rule_fid=1.000', '\\n', 'Epoch 3: train_loss=0.7912\nval_loss=0.7737 train_acc=0.531 val_acc=0.530 rule_fid=1.000', '\\n', 'Epoch 4:\ntrain_loss=0.7034 val_loss=0.7213 train_acc=0.585 val_acc=0.552 rule_fid=1.000',\n'\\n', 'Epoch 5: train_loss=0.6296 val_loss=0.6841 train_acc=0.645 val_acc=0.608\nrule_fid=1.000', '\\n', 'Epoch 6: train_loss=0.5640 val_loss=0.6546\ntrain_acc=0.703 val_acc=0.644 rule_fid=1.000', '\\n', 'Epoch 7: train_loss=0.5095\nval_loss=0.6241 train_acc=0.766 val_acc=0.676 rule_fid=1.000', '\\n', 'Epoch 8:\ntrain_loss=0.4627 val_loss=0.6147 train_acc=0.825 val_acc=0.694 rule_fid=1.000',\n'\\n', 'Epoch 9: train_loss=0.4239 val_loss=0.5923 train_acc=0.860 val_acc=0.710\nrule_fid=1.000', '\\n', 'Epoch 10: train_loss=0.3907 val_loss=0.5867\ntrain_acc=0.898 val_acc=0.728 rule_fid=1.000', '\\n', '*** New best config: adam\nwith test_acc=0.726', '\\n', '\\n===== Training with sgd_m0.0 =====', '\\n', 'Epoch\n1: train_loss=32.7054 val_loss=23.1402 train_acc=0.486 val_acc=0.520\nrule_fid=1.000', '\\n', 'Epoch 2: train_loss=24.1351 val_loss=8.3509\ntrain_acc=0.523 val_acc=0.764 rule_fid=1.000', '\\n', 'Epoch 3: train_loss=0.1961\nval_loss=7.6665 train_acc=0.977 val_acc=0.790 rule_fid=1.000', '\\n', 'Epoch 4:\ntrain_loss=0.2035 val_loss=7.6836 train_acc=0.973 val_acc=0.792 rule_fid=1.000',\n'\\n', 'Epoch 5: train_loss=0.1290 val_loss=7.5791 train_acc=0.983 val_acc=0.792\nrule_fid=1.000', '\\n', 'Epoch 6: train_loss=0.0976 val_loss=7.5231\ntrain_acc=0.987 val_acc=0.784 rule_fid=1.000', '\\n', 'Epoch 7: train_loss=0.0808\nval_loss=7.6230 train_acc=0.990 val_acc=0.792 rule_fid=1.000', '\\n', 'Epoch 8:\ntrain_loss=0.0823 val_loss=7.4873 train_acc=0.987 val_acc=0.782 rule_fid=1.000',\n'\\n', 'Epoch 9: train_loss=0.0922 val_loss=7.5497 train_acc=0.991 val_acc=0.796\nrule_fid=1.000', '\\n', 'Epoch 10: train_loss=0.1399 val_loss=7.5228\ntrain_acc=0.984 val_acc=0.796 rule_fid=1.000', '\\n', '*** New best config:\nsgd_m0.0 with test_acc=0.796', '\\n', '\\n===== Training with sgd_m0.5 =====',\n'\\n', 'Epoch 1: train_loss=20.2091 val_loss=10.9562 train_acc=0.633\nval_acc=0.520 rule_fid=1.000', '\\n', 'Epoch 2: train_loss=6.7328\nval_loss=18.3904 train_acc=0.721 val_acc=0.622 rule_fid=1.000', '\\n', 'Epoch 3:\ntrain_loss=2.2829 val_loss=10.3240 train_acc=0.864 val_acc=0.790\nrule_fid=1.000', '\\n', 'Epoch 4: train_loss=0.1145 val_loss=10.3334\ntrain_acc=0.986 val_acc=0.792 rule_fid=1.000', '\\n', 'Epoch 5: train_loss=0.1155\nval_loss=10.2034 train_acc=0.990 val_acc=0.796 rule_fid=1.000', '\\n', 'Epoch 6:\ntrain_loss=0.1690 val_loss=10.1212 train_acc=0.981 val_acc=0.776\nrule_fid=1.000', '\\n', 'Epoch 7: train_loss=0.0862 val_loss=10.1314\ntrain_acc=0.992 val_acc=0.796 rule_fid=1.000', '\\n', 'Epoch 8: train_loss=0.1464\nval_loss=10.3318 train_acc=0.982 val_acc=0.796 rule_fid=1.000', '\\n', 'Epoch 9:\ntrain_loss=0.0953 val_loss=17.1608 train_acc=0.983 val_acc=0.650\nrule_fid=1.000', '\\n', 'Epoch 10: train_loss=6.1302 val_loss=12.7330\ntrain_acc=0.777 val_acc=0.716 rule_fid=1.000', '\\n', '\\n===== Training with\nsgd_m0.9 =====', '\\n', 'Epoch 1: train_loss=31.6405 val_loss=41.2886\ntrain_acc=0.578 val_acc=0.590 rule_fid=1.000', '\\n', 'Epoch 2: train_loss=6.8140\nval_loss=35.5518 train_acc=0.862 val_acc=0.762 rule_fid=1.000', '\\n', 'Epoch 3:\ntrain_loss=1.1743 val_loss=34.6705 train_acc=0.971 val_acc=0.788\nrule_fid=1.000', '\\n', 'Epoch 4: train_loss=0.4718 val_loss=35.6294\ntrain_acc=0.980 val_acc=0.790 rule_fid=1.000', '\\n', 'Epoch 5: train_loss=0.5364\nval_loss=35.5988 train_acc=0.982 val_acc=0.792 rule_fid=1.000', '\\n', 'Epoch 6:\ntrain_loss=0.3595 val_loss=34.9069 train_acc=0.991 val_acc=0.796\nrule_fid=1.000', '\\n', 'Epoch 7: train_loss=0.2363 val_loss=34.1379\ntrain_acc=0.991 val_acc=0.794 rule_fid=1.000', '\\n', 'Epoch 8: train_loss=1.0925\nval_loss=47.9337 train_acc=0.941 val_acc=0.692 rule_fid=1.000', '\\n', 'Epoch 9:\ntrain_loss=4.8658 val_loss=39.3582 train_acc=0.915 val_acc=0.786\nrule_fid=1.000', '\\n', 'Epoch 10: train_loss=5.6592 val_loss=58.5819\ntrain_acc=0.870 val_acc=0.692 rule_fid=1.000', '\\n', '\\nBest configuration:\nsgd_m0.0 with dev_acc=0.796', '\\n', 'Saved results to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n23/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 108642.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 119946.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 189256.57\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', '\\n===== Training with adam =====',\n'\\n', 'Epoch 1: train_loss=0.6923 val_loss=0.6929 train_acc=0.500 val_acc=0.480\nrule_fid=1.000', '\\n', 'Epoch 2: train_loss=0.6910 val_loss=0.6920\ntrain_acc=0.501 val_acc=0.480 rule_fid=1.000', '\\n', 'Epoch 3: train_loss=0.6897\nval_loss=0.6914 train_acc=0.512 val_acc=0.484 rule_fid=0.996', '\\n', 'Epoch 4:\ntrain_loss=0.6883 val_loss=0.6906 train_acc=0.524 val_acc=0.496 rule_fid=0.968',\n'\\n', 'Epoch 5: train_loss=0.6870 val_loss=0.6898 train_acc=0.582 val_acc=0.538\nrule_fid=0.960', '\\n', 'Epoch 6: train_loss=0.6857 val_loss=0.6891\ntrain_acc=0.615 val_acc=0.540 rule_fid=0.944', '\\n', 'Epoch 7: train_loss=0.6844\nval_loss=0.6884 train_acc=0.634 val_acc=0.582 rule_fid=0.940', '\\n', 'Epoch 8:\ntrain_loss=0.6832 val_loss=0.6875 train_acc=0.727 val_acc=0.640 rule_fid=0.938',\n'\\n', 'Epoch 9: train_loss=0.6819 val_loss=0.6871 train_acc=0.715 val_acc=0.604\nrule_fid=0.964', '\\n', 'Epoch 10: train_loss=0.6806 val_loss=0.6863\ntrain_acc=0.725 val_acc=0.640 rule_fid=0.944', '\\n', '*** New best config: adam\nwith test_acc=0.642', '\\n', '\\n===== Training with sgd_m0.0 =====', '\\n', 'Epoch\n1: train_loss=0.6931 val_loss=0.6937 train_acc=0.500 val_acc=0.480\nrule_fid=1.000', '\\n', 'Epoch 2: train_loss=0.6916 val_loss=0.6925\ntrain_acc=0.525 val_acc=0.480 rule_fid=1.000', '\\n', 'Epoch 3: train_loss=0.6907\nval_loss=0.6916 train_acc=0.564 val_acc=0.574 rule_fid=0.816', '\\n', 'Epoch 4:\ntrain_loss=0.6902 val_loss=0.6907 train_acc=0.561 val_acc=0.522 rule_fid=0.994',\n'\\n', 'Epoch 5: train_loss=0.6895 val_loss=0.6906 train_acc=0.583 val_acc=0.668\nrule_fid=0.812', '\\n', 'Epoch 6: train_loss=0.6890 val_loss=0.6904\ntrain_acc=0.690 val_acc=0.696 rule_fid=0.814', '\\n', 'Epoch 7: train_loss=0.6883\nval_loss=0.6900 train_acc=0.766 val_acc=0.700 rule_fid=0.708', '\\n', 'Epoch 8:\ntrain_loss=0.6877 val_loss=0.6896 train_acc=0.764 val_acc=0.710 rule_fid=0.744',\n'\\n', 'Epoch 9: train_loss=0.6871 val_loss=0.6895 train_acc=0.778 val_acc=0.690\nrule_fid=0.712', '\\n', 'Epoch 10: train_loss=0.6864 val_loss=0.6892\ntrain_acc=0.771 val_acc=0.714 rule_fid=0.748', '\\n', '*** New best config:\nsgd_m0.0 with test_acc=0.734', '\\n', '\\n===== Training with sgd_m0.5 =====',\n'\\n', 'Epoch 1: train_loss=0.6942 val_loss=0.6932 train_acc=0.498 val_acc=0.458\nrule_fid=0.106', '\\n', 'Epoch 2: train_loss=0.6931 val_loss=0.6928\ntrain_acc=0.494 val_acc=0.480 rule_fid=0.204', '\\n', 'Epoch 3: train_loss=0.6917\nval_loss=0.6920 train_acc=0.540 val_acc=0.500 rule_fid=0.134', '\\n', 'Epoch 4:\ntrain_loss=0.6909 val_loss=0.6918 train_acc=0.530 val_acc=0.480 rule_fid=0.988',\n'\\n', 'Epoch 5: train_loss=0.6893 val_loss=0.6909 train_acc=0.693 val_acc=0.488\nrule_fid=0.846', '\\n', 'Epoch 6: train_loss=0.6880 val_loss=0.6904\ntrain_acc=0.608 val_acc=0.484 rule_fid=0.992', '\\n', 'Epoch 7: train_loss=0.6870\nval_loss=0.6900 train_acc=0.630 val_acc=0.480 rule_fid=0.996', '\\n', 'Epoch 8:\ntrain_loss=0.6856 val_loss=0.6881 train_acc=0.549 val_acc=0.746 rule_fid=0.614',\n'\\n', 'Epoch 9: train_loss=0.6847 val_loss=0.6879 train_acc=0.741 val_acc=0.662\nrule_fid=0.894', '\\n', 'Epoch 10: train_loss=0.6833 val_loss=0.6874\ntrain_acc=0.750 val_acc=0.660 rule_fid=0.946', '\\n', '\\n===== Training with\nsgd_m0.9 =====', '\\n', 'Epoch 1: train_loss=0.6948 val_loss=0.6922\ntrain_acc=0.518 val_acc=0.520 rule_fid=1.000', '\\n', 'Epoch 2: train_loss=0.6916\nval_loss=0.6897 train_acc=0.546 val_acc=0.604 rule_fid=0.800', '\\n', 'Epoch 3:\ntrain_loss=0.6843 val_loss=0.6877 train_acc=0.660 val_acc=0.484 rule_fid=0.984',\n'\\n', 'Epoch 4: train_loss=0.6783 val_loss=0.6827 train_acc=0.755 val_acc=0.760\nrule_fid=0.988', '\\n', 'Epoch 5: train_loss=0.6732 val_loss=0.6796\ntrain_acc=0.801 val_acc=0.754 rule_fid=0.990', '\\n', 'Epoch 6: train_loss=0.6675\nval_loss=0.6779 train_acc=0.748 val_acc=0.640 rule_fid=0.984', '\\n', 'Epoch 7:\ntrain_loss=0.6617 val_loss=0.6738 train_acc=0.850 val_acc=0.732 rule_fid=0.982',\n'\\n', 'Epoch 8: train_loss=0.6565 val_loss=0.6696 train_acc=0.685 val_acc=0.538\nrule_fid=0.986', '\\n', 'Epoch 9: train_loss=0.6531 val_loss=0.6665\ntrain_acc=0.769 val_acc=0.748 rule_fid=0.918', '\\n', 'Epoch 10:\ntrain_loss=0.6448 val_loss=0.6665 train_acc=0.841 val_acc=0.670 rule_fid=0.968',\n'\\n', '\\nBest configuration: sgd_m0.0 with dev_acc=0.714', '\\n', 'Saved all\nresults to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n24/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 76860.99\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 17105.37\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 165156.09\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', '\\n===== Training (no-bias) with adam\n=====', '\\n', 'Epoch 1: train_loss=0.7664 val_loss=0.7503 train_acc=0.584\nval_acc=0.592 rule_fid=0.698', '\\n', 'Epoch 2: train_loss=0.5662 val_loss=0.6760\ntrain_acc=0.708 val_acc=0.650 rule_fid=0.588', '\\n', 'Epoch 3: train_loss=0.4439\nval_loss=0.6359 train_acc=0.812 val_acc=0.696 rule_fid=0.520', '\\n', 'Epoch 4:\ntrain_loss=0.3581 val_loss=0.6216 train_acc=0.880 val_acc=0.728 rule_fid=0.540',\n'\\n', 'Epoch 5: train_loss=0.2989 val_loss=0.6199 train_acc=0.915 val_acc=0.742\nrule_fid=0.552', '\\n', 'Epoch 6: train_loss=0.2577 val_loss=0.6268\ntrain_acc=0.935 val_acc=0.750 rule_fid=0.550', '\\n', 'Epoch 7: train_loss=0.2273\nval_loss=0.6348 train_acc=0.946 val_acc=0.766 rule_fid=0.542', '\\n', 'Epoch 8:\ntrain_loss=0.2050 val_loss=0.6482 train_acc=0.955 val_acc=0.764 rule_fid=0.550',\n'\\n', 'Epoch 9: train_loss=0.1875 val_loss=0.6631 train_acc=0.957 val_acc=0.768\nrule_fid=0.546', '\\n', 'Epoch 10: train_loss=0.1740 val_loss=0.6780\ntrain_acc=0.960 val_acc=0.768 rule_fid=0.554', '\\n', '*** New best config: adam\nwith test_acc=0.779', '\\n', '\\n===== Training (no-bias) with sgd_m0.0 =====',\n'\\n', 'Epoch 1: train_loss=34.6692 val_loss=27.0375 train_acc=0.551\nval_acc=0.520 rule_fid=1.000', '\\n', 'Epoch 2: train_loss=13.8986\nval_loss=12.2800 train_acc=0.702 val_acc=0.792 rule_fid=0.970', '\\n', 'Epoch 3:\ntrain_loss=0.1453 val_loss=12.3523 train_acc=0.982 val_acc=0.796\nrule_fid=0.986', '\\n', 'Epoch 4: train_loss=0.0920 val_loss=12.1749\ntrain_acc=0.991 val_acc=0.788 rule_fid=0.774', '\\n', 'Epoch 5: train_loss=0.0890\nval_loss=12.1338 train_acc=0.991 val_acc=0.778 rule_fid=0.746', '\\n', 'Epoch 6:\ntrain_loss=0.0834 val_loss=12.2068 train_acc=0.992 val_acc=0.796\nrule_fid=0.790', '\\n', 'Epoch 7: train_loss=0.0762 val_loss=12.3038\ntrain_acc=0.992 val_acc=0.796 rule_fid=0.986', '\\n', 'Epoch 8: train_loss=0.1075\nval_loss=12.0968 train_acc=0.990 val_acc=0.796 rule_fid=0.726', '\\n', 'Epoch 9:\ntrain_loss=0.1142 val_loss=12.1228 train_acc=0.986 val_acc=0.796\nrule_fid=0.712', '\\n', 'Epoch 10: train_loss=0.0766 val_loss=12.0366\ntrain_acc=0.989 val_acc=0.790 rule_fid=0.668', '\\n', '*** New best config:\nsgd_m0.0 with test_acc=0.794', '\\n', '\\n===== Training (no-bias) with sgd_m0.5\n=====', '\\n', 'Epoch 1: train_loss=28.5634 val_loss=12.6520 train_acc=0.630\nval_acc=0.756 rule_fid=0.982', '\\n', 'Epoch 2: train_loss=0.3670\nval_loss=11.5248 train_acc=0.964 val_acc=0.788 rule_fid=0.958', '\\n', 'Epoch 3:\ntrain_loss=0.1606 val_loss=11.9496 train_acc=0.978 val_acc=0.782\nrule_fid=0.964', '\\n', 'Epoch 4: train_loss=0.1368 val_loss=11.5043\ntrain_acc=0.985 val_acc=0.788 rule_fid=1.000', '\\n', 'Epoch 5: train_loss=0.8360\nval_loss=11.7375 train_acc=0.946 val_acc=0.796 rule_fid=0.998', '\\n', 'Epoch 6:\ntrain_loss=7.2346 val_loss=16.9893 train_acc=0.762 val_acc=0.774\nrule_fid=0.958', '\\n', 'Epoch 7: train_loss=3.5325 val_loss=20.2241\ntrain_acc=0.869 val_acc=0.784 rule_fid=0.970', '\\n', 'Epoch 8: train_loss=0.2570\nval_loss=19.1400 train_acc=0.982 val_acc=0.796 rule_fid=0.500', '\\n', 'Epoch 9:\ntrain_loss=0.0977 val_loss=19.0769 train_acc=0.993 val_acc=0.796\nrule_fid=0.496', '\\n', 'Epoch 10: train_loss=0.0907 val_loss=18.9742\ntrain_acc=0.993 val_acc=0.792 rule_fid=0.508', '\\n', '*** New best config:\nsgd_m0.5 with test_acc=0.792', '\\n', '\\n===== Training (no-bias) with sgd_m0.9\n=====', '\\n', 'Epoch 1: train_loss=15.6713 val_loss=24.0961 train_acc=0.773\nval_acc=0.776 rule_fid=0.956', '\\n', 'Epoch 2: train_loss=1.7352\nval_loss=51.2843 train_acc=0.934 val_acc=0.652 rule_fid=0.970', '\\n', 'Epoch 3:\ntrain_loss=8.2146 val_loss=44.4464 train_acc=0.858 val_acc=0.782\nrule_fid=0.962', '\\n', 'Epoch 4: train_loss=0.7109 val_loss=46.3976\ntrain_acc=0.984 val_acc=0.794 rule_fid=0.956', '\\n', 'Epoch 5: train_loss=0.2495\nval_loss=46.1412 train_acc=0.993 val_acc=0.794 rule_fid=0.762', '\\n', 'Epoch 6:\ntrain_loss=0.2728 val_loss=46.6085 train_acc=0.988 val_acc=0.796\nrule_fid=0.992', '\\n', 'Epoch 7: train_loss=0.3149 val_loss=45.9179\ntrain_acc=0.992 val_acc=0.794 rule_fid=0.752', '\\n', 'Epoch 8: train_loss=0.2157\nval_loss=46.7414 train_acc=0.989 val_acc=0.796 rule_fid=0.958', '\\n', 'Epoch 9:\ntrain_loss=0.2603 val_loss=46.8781 train_acc=0.991 val_acc=0.794\nrule_fid=0.960', '\\n', 'Epoch 10: train_loss=0.2836 val_loss=45.4890\ntrain_acc=0.991 val_acc=0.792 rule_fid=0.500', '\\n', '\\nBest configuration (no-\nbias): sgd_m0.5 with dev_acc=0.792', '\\n', 'Saved all results to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n25/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 112654.71\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 113176.04\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 205774.62\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\nsize: 33', '\\n', 'Classes: [0, 1]', '\\n', '\\n===== Training with adam =====',\n'\\n', 'Epoch 1: train_loss=0.7090 val_loss=0.7350 train_acc=0.627 val_acc=0.626\nrule_fid=0.762', '\\n', 'Epoch 2: train_loss=0.5106 val_loss=0.6769\ntrain_acc=0.746 val_acc=0.664 rule_fid=0.640', '\\n', 'Epoch 3: train_loss=0.3961\nval_loss=0.6483 train_acc=0.840 val_acc=0.700 rule_fid=0.558', '\\n', 'Epoch 4:\ntrain_loss=0.3189 val_loss=0.6441 train_acc=0.893 val_acc=0.738 rule_fid=0.556',\n'\\n', 'Epoch 5: train_loss=0.2662 val_loss=0.6461 train_acc=0.922 val_acc=0.750\nrule_fid=0.564', '\\n', 'Epoch 6: train_loss=0.2301 val_loss=0.6590\ntrain_acc=0.941 val_acc=0.752 rule_fid=0.562', '\\n', 'Epoch 7: train_loss=0.2026\nval_loss=0.6718 train_acc=0.954 val_acc=0.764 rule_fid=0.566', '\\n', 'Epoch 8:\ntrain_loss=0.1831 val_loss=0.6901 train_acc=0.958 val_acc=0.764 rule_fid=0.570',\n'\\n', 'Epoch 9: train_loss=0.1683 val_loss=0.7043 train_acc=0.961 val_acc=0.770\nrule_fid=0.570', '\\n', 'Epoch 10: train_loss=0.1556 val_loss=0.7274\ntrain_acc=0.965 val_acc=0.764 rule_fid=0.584', '\\n', '*** New best config: adam\nwith test_acc=0.774', '\\n', '\\n===== Training with sgd_m0.0 =====', '\\n', 'Epoch\n1: train_loss=38.2472 val_loss=17.2714 train_acc=0.515 val_acc=0.522\nrule_fid=0.998', '\\n', 'Epoch 2: train_loss=8.3591 val_loss=12.2269\ntrain_acc=0.815 val_acc=0.794 rule_fid=0.974', '\\n', 'Epoch 3: train_loss=0.1686\nval_loss=12.4415 train_acc=0.984 val_acc=0.738 rule_fid=0.630', '\\n', 'Epoch 4:\ntrain_loss=0.2447 val_loss=12.2408 train_acc=0.978 val_acc=0.794\nrule_fid=0.930', '\\n', 'Epoch 5: train_loss=0.0936 val_loss=12.1414\ntrain_acc=0.990 val_acc=0.794 rule_fid=0.932', '\\n', 'Epoch 6: train_loss=0.1078\nval_loss=12.7868 train_acc=0.987 val_acc=0.786 rule_fid=0.994', '\\n', 'Epoch 7:\ntrain_loss=0.1026 val_loss=11.9746 train_acc=0.990 val_acc=0.780\nrule_fid=0.876', '\\n', 'Epoch 8: train_loss=0.0765 val_loss=12.0012\ntrain_acc=0.991 val_acc=0.796 rule_fid=0.904', '\\n', 'Epoch 9: train_loss=0.1588\nval_loss=11.9232 train_acc=0.986 val_acc=0.786 rule_fid=0.872', '\\n', 'Epoch 10:\ntrain_loss=0.0823 val_loss=13.1994 train_acc=0.988 val_acc=0.768\nrule_fid=0.994', '\\n', '*** New best config: sgd_m0.0 with test_acc=0.780',\n'\\n', '\\n===== Training with sgd_m0.5 =====', '\\n', 'Epoch 1: train_loss=26.3343\nval_loss=16.1286 train_acc=0.595 val_acc=0.742 rule_fid=0.986', '\\n', 'Epoch 2:\ntrain_loss=0.4299 val_loss=13.9706 train_acc=0.967 val_acc=0.790\nrule_fid=0.970', '\\n', 'Epoch 3: train_loss=0.1183 val_loss=13.6556\ntrain_acc=0.988 val_acc=0.796 rule_fid=0.968', '\\n', 'Epoch 4: train_loss=0.1914\nval_loss=14.5203 train_acc=0.983 val_acc=0.782 rule_fid=0.978', '\\n', 'Epoch 5:\ntrain_loss=0.1363 val_loss=13.4378 train_acc=0.989 val_acc=0.782\nrule_fid=0.958', '\\n', 'Epoch 6: train_loss=0.4372 val_loss=16.6872\ntrain_acc=0.959 val_acc=0.746 rule_fid=0.982', '\\n', 'Epoch 7: train_loss=0.3798\nval_loss=14.1311 train_acc=0.974 val_acc=0.796 rule_fid=0.966', '\\n', 'Epoch 8:\ntrain_loss=0.1164 val_loss=14.2995 train_acc=0.990 val_acc=0.794\nrule_fid=0.968', '\\n', 'Epoch 9: train_loss=0.0979 val_loss=14.3904\ntrain_acc=0.989 val_acc=0.792 rule_fid=0.970', '\\n', 'Epoch 10:\ntrain_loss=0.7499 val_loss=14.9874 train_acc=0.944 val_acc=0.794\nrule_fid=0.968', '\\n', '*** New best config: sgd_m0.5 with test_acc=0.796',\n'\\n', '\\n===== Training with sgd_m0.9 =====', '\\n', 'Epoch 1: train_loss=69.4129\nval_loss=76.5471 train_acc=0.574 val_acc=0.654 rule_fid=0.878', '\\n', 'Epoch 2:\ntrain_loss=7.6466 val_loss=79.1604 train_acc=0.903 val_acc=0.724\nrule_fid=0.712', '\\n', 'Epoch 3: train_loss=1.4568 val_loss=81.7807\ntrain_acc=0.976 val_acc=0.766 rule_fid=0.570', '\\n', 'Epoch 4: train_loss=0.7127\nval_loss=82.6099 train_acc=0.988 val_acc=0.794 rule_fid=0.966', '\\n', 'Epoch 5:\ntrain_loss=0.5079 val_loss=82.2350 train_acc=0.988 val_acc=0.792\nrule_fid=0.966', '\\n', 'Epoch 6: train_loss=0.4512 val_loss=82.1686\ntrain_acc=0.990 val_acc=0.794 rule_fid=0.498', '\\n', 'Epoch 7: train_loss=0.5142\nval_loss=81.4458 train_acc=0.992 val_acc=0.784 rule_fid=0.516', '\\n', 'Epoch 8:\ntrain_loss=0.4638 val_loss=82.7683 train_acc=0.990 val_acc=0.796\nrule_fid=0.970', '\\n', 'Epoch 9: train_loss=0.4427 val_loss=81.2404\ntrain_acc=0.991 val_acc=0.796 rule_fid=0.496', '\\n', 'Epoch 10:\ntrain_loss=0.3179 val_loss=81.0545 train_acc=0.991 val_acc=0.796\nrule_fid=0.496', '\\n', '*** New best config: sgd_m0.9 with test_acc=0.796',\n'\\n', '\\nBest configuration: sgd_m0.9 with dev_acc=0.796', '\\n', 'Saved all\nresults to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n40_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n26/working/experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']"], "analysis": ["", "", "", "", ""], "exc_type": [null, null, null, null, null], "exc_info": [null, null, null, null, null], "exc_stack": [null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9665, "best_value": 0.9665}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.964, "best_value": 0.964}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4904, "best_value": 0.4904}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 18.6188, "best_value": 18.6188}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9835, "best_value": 0.9835}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1399, "best_value": 0.1399}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 7.5228, "best_value": 7.5228}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions out of total predictions.", "data": [{"dataset_name": "train", "final_value": 0.7715, "best_value": 0.7715}, {"dataset_name": "validation", "final_value": 0.714, "best_value": 0.714}, {"dataset_name": "test", "final_value": 0.734, "best_value": 0.734}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Indicates how closely the rules mimic the model's predictions.", "data": [{"dataset_name": "test", "final_value": 0.748, "best_value": 0.748}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error of the model during training or validation.", "data": [{"dataset_name": "train", "final_value": 0.6864, "best_value": 0.6864}, {"dataset_name": "validation", "final_value": 0.6892, "best_value": 0.6892}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9925, "best_value": 0.9925}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0907, "best_value": 0.0907}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 18.9742, "best_value": 18.9742}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.508, "best_value": 0.508}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9905, "best_value": 0.9905}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.496, "best_value": 0.496}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3179, "best_value": 0.3179}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 81.0545, "best_value": 81.0545}]}]}], "is_best_node": [false, true, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_train_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_gt_vs_pred_distribution.png"], ["../../logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_train_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_rule_fidelity.png"], ["../../logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_rule_fidelity_curves.png", "../../logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_accuracy.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_loss.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_rule_fid.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_accuracy.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_loss.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_rule_fid.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_accuracy.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_loss.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_rule_fid.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_accuracy.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_loss.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_rule_fid.png", "../../logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_confusion_matrix_best.png"]], "plot_paths": [["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_train_val_accuracy.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_train_val_loss.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_gt_vs_pred_distribution.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_train_val_accuracy.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_train_val_loss.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_rule_fidelity.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_rule_fidelity_curves.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_accuracy.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_loss.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_rule_fid.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_accuracy.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_loss.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_rule_fid.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_accuracy.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_loss.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_rule_fid.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_accuracy.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_loss.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_rule_fid.png", "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_confusion_matrix_best.png"]], "plot_analyses": [[{"analysis": "This plot shows the training and validation accuracy for different optimizers and momentum values. The Adam optimizer achieves the highest validation accuracy, with a smooth convergence pattern. For SGD with varying momentum, higher momentum values (e.g., 0.9) lead to faster initial learning but result in unstable validation accuracy, suggesting potential overfitting or sensitivity to momentum. The Adam optimizer appears to be a better choice for this task as it provides consistent and high validation accuracy.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_train_val_accuracy.png"}, {"analysis": "The training and validation loss curves indicate that the Adam optimizer achieves rapid convergence with low loss values, maintaining a stable validation loss. SGD with momentum values of 0.0 and 0.5 shows some stability but higher loss values compared to Adam. SGD with momentum 0.9 exhibits a high validation loss, indicating poor generalization. These trends reinforce that Adam is the most effective optimizer for this task, ensuring low loss and better generalization.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_train_val_loss.png"}, {"analysis": "This plot evaluates the rule fidelity across epochs for different optimizers and momentum settings. The Adam optimizer struggles initially but gradually improves rule fidelity, stabilizing at around 0.8. SGD with momentum 0.0 and 0.5 achieves high rule fidelity above 0.99 consistently, making them more interpretable. SGD with momentum 0.9 shows significant fluctuations, indicating poor rule fidelity and instability. For maintaining interpretability, SGD with momentum 0.0 or 0.5 is preferable.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The class distribution plot compares the ground truth and predicted class distributions. The distributions are nearly identical, indicating that the model achieves balanced predictions across all classes. This suggests that the model does not show bias toward any specific class and maintains fairness in its predictions.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_162cce3d8f5a4216b9124739abf80f7e_proc_3207485/SPR_BENCH_gt_vs_pred_distribution.png"}], [{"analysis": "The first plot shows the accuracy trends over epochs for different optimizers and momentum configurations. The Adam optimizer demonstrates a gradual increase in both training and validation accuracy, with a slower convergence compared to SGD with momentum. Among the SGD configurations, higher momentum values (e.g., 0.9) lead to faster convergence to high training and validation accuracy, but there is a noticeable overfitting pattern as the validation accuracy dips after initial improvement. Lower momentum values (e.g., 0.0) result in slower convergence and lower final accuracy. The plot indicates that while SGD with momentum can achieve high accuracy quickly, careful tuning is required to avoid overfitting.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_train_val_accuracy.png"}, {"analysis": "The second plot illustrates the cross-entropy loss over epochs. Adam exhibits a steady decline in training loss, while validation loss remains relatively stable, suggesting good generalization. SGD with momentum (e.g., 0.9) achieves rapid loss reduction in training but struggles with validation loss, which increases after an initial drop, indicating overfitting. Lower momentum values (e.g., 0.0) lead to slower loss reduction and higher final loss values, reflecting poorer optimization. This plot highlights the trade-offs between optimization speed and generalization for different momentum settings in SGD.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_train_val_loss.png"}, {"analysis": "The third plot depicts rule fidelity over epochs for different optimizers and momentum settings. Surprisingly, the fidelity remains constant at 1.0 for all configurations, suggesting that all models perfectly adhere to the rules throughout training. This may indicate that rule fidelity is not sensitive to the choice of optimizer or momentum, or it might be a limitation of the metric used to measure fidelity. Further investigation would be needed to confirm these observations.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d37630869725437ea9f59b112c3f8288_proc_3220426/SPR_BENCH_rule_fidelity.png"}], [{"analysis": "This plot compares training and validation accuracy across different optimization configurations (Adam and SGD with varying momentum values). The key observations are: (1) SGD with momentum 0.9 achieves the highest training and validation accuracy, indicating its superior performance for this task. (2) Adam optimizer shows stable but relatively lower accuracy compared to SGD with momentum. (3) Validation accuracy for SGD with momentum exhibits some oscillations, suggesting potential overfitting or instability in learning. (4) SGD with momentum 0.5 shows a significant gap between training and validation accuracy, indicating possible overfitting.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot shows the cross-entropy loss for training and validation across different configurations. Key findings: (1) SGD with momentum 0.9 exhibits the steepest loss reduction, aligning with its superior accuracy performance. (2) Adam optimizer shows slower but steady loss reduction, reflecting its stability but relatively lower performance. (3) SGD with momentum 0.5 has a sharp decrease in training loss but a slower decrease in validation loss, further indicating overfitting. (4) The validation loss for SGD with momentum 0.0 remains relatively high, showing its ineffectiveness compared to other configurations.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the rule fidelity metric over epochs for different configurations. Key observations: (1) Adam optimizer maintains high rule fidelity throughout training, indicating strong interpretability. (2) SGD with momentum 0.5 and 0.9 show some fluctuations but achieve relatively high rule fidelity by the end of training. (3) SGD with momentum 0.0 exhibits significant variability and lower rule fidelity, suggesting poor interpretability for this configuration. (4) The trade-off between accuracy and rule fidelity is apparent as the most accurate configuration (SGD with momentum 0.9) does not consistently achieve the highest rule fidelity.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_rule_fidelity.png"}, {"analysis": "This confusion matrix represents the performance of the best configuration (likely SGD with momentum 0.9) on the test set. Observations: (1) The model correctly classifies a majority of examples in both classes, as shown by the high diagonal values. (2) There are more false positives (189) compared to false negatives (77), indicating a bias in the model's predictions. (3) The overall performance is strong, but addressing this bias could further improve the model's precision and recall for the underrepresented class.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d7cb04b92f2b427d84d6fbfe89c0ce0a_proc_3220427/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot compares training and validation accuracy across different optimizers and momentum settings over 10 epochs. The Adam optimizer shows a steady and consistent increase in both training and validation accuracy, indicating good generalization. The SGD optimizer with momentum settings (m=0.5 and m=0.9) shows rapid fluctuations in accuracy, suggesting instability in learning. SGD without momentum (m=0.0) achieves high accuracy quickly but plateaus early, indicating potential overfitting or limited generalization. Overall, Adam appears to be the most stable and reliable optimizer for this task.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot shows the training and validation loss for different optimizers and momentum settings. The Adam optimizer maintains low and stable loss values for both training and validation, reflecting effective learning. SGD with momentum settings (m=0.5 and m=0.9) exhibits high loss fluctuations, particularly in validation, indicating unstable convergence. SGD without momentum (m=0.0) achieves low loss early but does not improve further, which may suggest overfitting. Adam demonstrates the most consistent loss reduction and stability.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot examines rule fidelity over epochs for different optimizers and momentum settings. Rule fidelity measures the model's ability to learn and adhere to the underlying rules of the task. Adam shows a gradual decline in fidelity, possibly due to overfitting to the data at the expense of rule adherence. SGD with m=0.0 and m=0.5 shows high fidelity initially, but both experience sharp drops and fluctuations, indicating instability. SGD with m=0.9 maintains high fidelity for most epochs but drops significantly towards the end, suggesting overfitting or instability. This indicates a trade-off between performance and rule fidelity.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_rule_fidelity_curves.png"}, {"analysis": "The confusion matrix evaluates the best-performing configuration's classification accuracy. The diagonal values (0.79) indicate that the model correctly classifies 79% of the examples in both classes. The off-diagonal values (0.21) represent misclassifications, which are relatively high. This suggests that while the model performs reasonably well, there is room for improvement in reducing misclassifications and increasing overall accuracy.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f5c3cf4f29a0450fb08830172a9ab8d2_proc_3220428/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation accuracy curves for the Adam optimizer. Training accuracy improves steadily and reaches above 95%, while validation accuracy plateaus around 75% after epoch 6. This indicates potential overfitting, as the model performs significantly better on the training set compared to the validation set.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_accuracy.png"}, {"analysis": "This plot depicts the training and validation loss for the Adam optimizer. Training loss decreases consistently, while validation loss stagnates and slightly increases after epoch 6. This further supports the overfitting observation from the accuracy plot.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_loss.png"}, {"analysis": "This plot illustrates rule fidelity for the Adam optimizer. Rule fidelity starts high but decreases significantly in the first few epochs and stabilizes around 0.55. This could indicate that the model struggles to maintain rule fidelity as it optimizes for accuracy.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_adam_rule_fid.png"}, {"analysis": "This plot presents training and validation accuracy for the SGD optimizer with momentum 0.0. Training accuracy reaches near 100% by epoch 4, while validation accuracy stabilizes around 80%. This suggests better generalization compared to the Adam optimizer, as the validation performance is closer to the training performance.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_accuracy.png"}, {"analysis": "This plot shows training and validation loss for the SGD optimizer with momentum 0.0. Training loss decreases sharply and stabilizes near zero, while validation loss stabilizes at a higher value. This indicates the model's ability to fit the training data well while maintaining relatively stable validation performance.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_loss.png"}, {"analysis": "This plot displays rule fidelity for the SGD optimizer with momentum 0.0. Rule fidelity fluctuates significantly, dropping sharply in the early epochs but recovering and stabilizing around 0.9. This suggests that the model maintains better rule fidelity compared to Adam, albeit with some instability.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.0_rule_fid.png"}, {"analysis": "This plot shows training and validation accuracy for the SGD optimizer with momentum 0.5. Training accuracy reaches near 100% quickly, while validation accuracy stabilizes around 80%. This is similar to the performance with momentum 0.0, indicating consistent generalization performance.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_accuracy.png"}, {"analysis": "This plot depicts training and validation loss for the SGD optimizer with momentum 0.5. Training loss decreases to near zero, while validation loss stabilizes at a higher value, indicating good training performance and stable validation performance.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_loss.png"}, {"analysis": "This plot illustrates rule fidelity for the SGD optimizer with momentum 0.5. Rule fidelity fluctuates throughout training, peaking around 0.98 but with significant variations. This suggests that while the model achieves high rule fidelity at times, it lacks stability.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.5_rule_fid.png"}, {"analysis": "This confusion matrix for the SGD optimizer with momentum 0.9 shows the model's classification performance. The true positive and true negative counts are high, with relatively low false positives and false negatives. This indicates good classification performance overall.", "plot_path": "experiments/2025-08-17_02-43-40_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_f46893fa9e204b79a977591a23e7ed07_proc_3220429/SPR_BENCH_sgd_m0.9_accuracy.png"}]], "vlm_feedback_summary": ["The experimental results highlight that the Adam optimizer provides the best\nperformance in terms of accuracy and loss, while SGD with momentum 0.0 or 0.5\nensures high rule fidelity. The model achieves balanced class predictions,\nindicating fairness. A trade-off exists between performance and\ninterpretability, requiring careful consideration of the task's priorities.", "The plots reveal key insights into the performance and behavior of different\noptimizers and momentum settings on the SPR task. Adam provides steady and\ngeneralizable performance, while SGD with high momentum achieves rapid\noptimization but risks overfitting. Rule fidelity remains constant, suggesting\nrobustness but potentially limited interpretability of the metric.", "The plots provide valuable insights into the performance and interpretability of\ndifferent optimization configurations. SGD with momentum 0.9 stands out as the\nbest-performing configuration in terms of accuracy and loss reduction, though it\nhas some interpretability trade-offs. Adam optimizer offers high rule fidelity\nbut lower accuracy. The confusion matrix highlights areas for improvement in\nclass balance.", "The provided plots reveal insights into optimizer performance, loss stability,\nrule fidelity, and classification accuracy. The Adam optimizer demonstrates the\nmost consistent performance, while SGD shows instability, particularly with\nhigher momentum settings. Rule fidelity analysis highlights a trade-off between\ntask performance and interpretability. The confusion matrix indicates reasonable\nclassification accuracy but with notable misclassifications, suggesting\npotential areas for model improvement.", "The experimental plots reveal insights into the performance and interpretability\ntrade-offs of different optimization strategies. The Adam optimizer exhibits\noverfitting, while SGD optimizers show better generalization and higher rule\nfidelity. However, rule fidelity stability remains a challenge across all\nconfigurations."], "exec_time": [4.15961766242981, 3.6128549575805664, 4.235713958740234, 4.7882537841796875, 4.239712953567505], "exec_time_feedback": ["", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"]], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- SETUP ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- LOAD EXP DATA ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"optimizer_type\"][\"SPR_BENCH\"]\n    cfgs = list(ed[\"configs\"])\n    train_acc = ed[\"metrics\"][\"train_acc\"]\n    val_acc = ed[\"metrics\"][\"val_acc\"]\n    rule_fid = ed[\"metrics\"][\"rule_fidelity\"]\n    train_loss = ed[\"losses\"][\"train\"]\n    val_loss = ed[\"losses\"][\"val\"]\n    preds = ed[\"predictions\"]\n    gts = ed[\"ground_truth\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\n# ---------- ACCURACY PLOT ----------\ntry:\n    plt.figure()\n    for i, cfg in enumerate(cfgs):\n        epochs = np.arange(1, len(train_acc[i]) + 1)\n        plt.plot(epochs, train_acc[i], label=f\"{cfg}-train\")\n        plt.plot(epochs, val_acc[i], \"--\", label=f\"{cfg}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH Training vs Validation Accuracy\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_train_val_accuracy.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# ---------- LOSS PLOT ----------\ntry:\n    plt.figure()\n    for i, cfg in enumerate(cfgs):\n        epochs = np.arange(1, len(train_loss[i]) + 1)\n        plt.plot(epochs, train_loss[i], label=f\"{cfg}-train\")\n        plt.plot(epochs, val_loss[i], \"--\", label=f\"{cfg}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Training vs Validation Loss\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_train_val_loss.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- RULE FIDELITY PLOT ----------\ntry:\n    plt.figure()\n    for i, cfg in enumerate(cfgs):\n        epochs = np.arange(1, len(rule_fid[i]) + 1)\n        plt.plot(epochs, rule_fid[i], label=f\"{cfg}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Rule Fidelity\")\n    plt.title(\"SPR_BENCH Rule Fidelity per Epoch\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_rule_fidelity.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating rule fidelity plot: {e}\")\n    plt.close()\n\n# ---------- GROUND-TRUTH vs PREDICTION DISTRIBUTION ----------\ntry:\n    classes = np.sort(np.unique(np.concatenate([gts, preds])))\n    gt_counts = np.array([np.sum(gts == c) for c in classes])\n    pred_counts = np.array([np.sum(preds == c) for c in classes])\n\n    bar_w = 0.4\n    x = np.arange(len(classes))\n    plt.figure(figsize=(8, 4))\n    plt.bar(x - bar_w / 2, gt_counts, width=bar_w, label=\"Ground Truth\")\n    plt.bar(x + bar_w / 2, pred_counts, width=bar_w, label=\"Predicted\")\n    plt.xlabel(\"Class ID\")\n    plt.ylabel(\"Count\")\n    plt.title(\n        \"SPR_BENCH Class Distribution\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_gt_vs_pred_distribution.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating GT vs Pred plot: {e}\")\n    plt.close()\n\n# ---------- PRINT TEST ACCURACY ----------\ntest_acc = (preds == gts).mean()\nprint(f\"Best config: {ed['best_config']}  |  Test accuracy: {test_acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----- setup -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load experiment results -----\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to safely fetch data\ndef safe_get(dic, *keys):\n    for k in keys:\n        dic = dic.get(k, {})\n    return dic\n\n\ned = safe_get(experiment_data, \"unigram_only\", \"SPR_BENCH\")\n\nconfigs = ed.get(\"configs\", [])\ntrain_acc = np.array(ed.get(\"metrics\", {}).get(\"train_acc\", []), dtype=object)\nval_acc = np.array(ed.get(\"metrics\", {}).get(\"val_acc\", []), dtype=object)\nrule_fid = np.array(ed.get(\"metrics\", {}).get(\"rule_fidelity\", []), dtype=object)\ntrain_loss = np.array(ed.get(\"losses\", {}).get(\"train\", []), dtype=object)\nval_loss = np.array(ed.get(\"losses\", {}).get(\"val\", []), dtype=object)\n\n# ----- evaluation metric -----\npreds = ed.get(\"predictions\", np.array([]))\ngolds = ed.get(\"ground_truth\", np.array([]))\nif preds.size and golds.size:\n    test_acc = (preds == golds).mean()\n    print(\n        f\"Final test accuracy (best config={ed.get('best_config','')}): {test_acc:.3f}\"\n    )\n\n# ----- plotting -----\nepochs = np.arange(1, train_acc.shape[1] + 1) if train_acc.size else np.array([])\n\n# 1. Accuracy curves\ntry:\n    if epochs.size:\n        plt.figure(figsize=(8, 4))\n        for i, cfg in enumerate(configs):\n            plt.plot(epochs, train_acc[i], label=f\"{cfg}-train\")\n            plt.plot(epochs, val_acc[i], linestyle=\"--\", label=f\"{cfg}-val\")\n        plt.title(\"SPR_BENCH: Accuracy over Epochs\\nTrain vs Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_val_accuracy.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# 2. Loss curves\ntry:\n    if epochs.size:\n        plt.figure(figsize=(8, 4))\n        for i, cfg in enumerate(configs):\n            plt.plot(epochs, train_loss[i], label=f\"{cfg}-train\")\n            plt.plot(epochs, val_loss[i], linestyle=\"--\", label=f\"{cfg}-val\")\n        plt.title(\"SPR_BENCH: Loss over Epochs\\nTrain vs Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_train_val_loss.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 3. Rule fidelity curves\ntry:\n    if epochs.size:\n        plt.figure(figsize=(6, 4))\n        for i, cfg in enumerate(configs):\n            plt.plot(epochs, rule_fid[i], label=cfg)\n        plt.title(\"SPR_BENCH: Rule Fidelity over Epochs\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Fidelity\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_rule_fidelity.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating rule fidelity plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# IO --------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Short-circuit if loading failed\nif experiment_data:\n    ds_key = \"len_norm_ngram\"\n    dataset = \"SPR_BENCH\"\n    ed = experiment_data[ds_key][dataset]\n    cfgs = ed[\"configs\"]\n    epochs = len(ed[\"metrics\"][\"train_acc\"][0])\n    x_axis = np.arange(1, epochs + 1)\n\n    # 1) ACCURACY CURVES ------------------------------------------------------\n    try:\n        plt.figure()\n        for cfg, tr, va in zip(\n            cfgs, ed[\"metrics\"][\"train_acc\"], ed[\"metrics\"][\"val_acc\"]\n        ):\n            plt.plot(x_axis, tr, label=f\"{cfg}-train\", alpha=0.8)\n            plt.plot(x_axis, va, \"--\", label=f\"{cfg}-val\", alpha=0.8)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Train vs Validation Accuracy per Config\")\n        plt.legend(fontsize=8)\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\"),\n            dpi=150,\n            bbox_inches=\"tight\",\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # 2) LOSS CURVES ----------------------------------------------------------\n    try:\n        plt.figure()\n        for cfg, tr, va in zip(cfgs, ed[\"losses\"][\"train\"], ed[\"losses\"][\"val\"]):\n            plt.plot(x_axis, tr, label=f\"{cfg}-train\", alpha=0.8)\n            plt.plot(x_axis, va, \"--\", label=f\"{cfg}-val\", alpha=0.8)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Train vs Validation Loss per Config\")\n        plt.legend(fontsize=8)\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"),\n            dpi=150,\n            bbox_inches=\"tight\",\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # 3) RULE FIDELITY CURVES -------------------------------------------------\n    try:\n        plt.figure()\n        for cfg, rf in zip(cfgs, ed[\"metrics\"][\"rule_fidelity\"]):\n            plt.plot(x_axis, rf, label=cfg, alpha=0.8)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity\")\n        plt.title(\"SPR_BENCH: Rule Fidelity over Epochs\")\n        plt.legend(fontsize=8)\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_rule_fidelity.png\"),\n            dpi=150,\n            bbox_inches=\"tight\",\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot: {e}\")\n        plt.close()\n\n    # 4) CONFUSION MATRIX -----------------------------------------------------\n    try:\n        gt = ed[\"ground_truth\"]\n        pred = ed[\"predictions\"]\n        n_cls = len(np.unique(gt))\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for t, p in zip(gt, pred):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"SPR_BENCH: Confusion Matrix (Best Config)\")\n        for i in range(n_cls):\n            for j in range(n_cls):\n                plt.text(\n                    j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=6\n                )\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"),\n            dpi=150,\n            bbox_inches=\"tight\",\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ------------------------------------------------------------------------\n    print(\n        f\"Best configuration: {ed['best_config']}  \"\n        f\"Dev accuracy (final epoch): {ed['metrics']['val_acc'][cfgs.index(ed['best_config'])][-1]:.3f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------- load experiment results ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ned = experiment_data.get(\"no_bias_log_reg\", {}).get(\"SPR_BENCH\", None)\n\nif ed:\n    cfgs = ed[\"configs\"]\n    metrics = ed[\"metrics\"]\n    losses = ed[\"losses\"]\n    epochs = np.arange(1, len(metrics[\"train_acc\"][0]) + 1)\n\n    # --------- accuracy curves ----------\n    try:\n        plt.figure()\n        for c, tr, va in zip(cfgs, metrics[\"train_acc\"], metrics[\"val_acc\"]):\n            plt.plot(epochs, tr, label=f\"{c}-train\")\n            plt.plot(epochs, va, \"--\", label=f\"{c}-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Training vs Validation Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # --------- loss curves ----------\n    try:\n        plt.figure()\n        for c, tr, va in zip(cfgs, losses[\"train\"], losses[\"val\"]):\n            plt.plot(epochs, tr, label=f\"{c}-train\")\n            plt.plot(epochs, va, \"--\", label=f\"{c}-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # --------- rule fidelity ----------\n    try:\n        plt.figure()\n        for c, rf in zip(cfgs, metrics[\"rule_fidelity\"]):\n            plt.plot(epochs, rf, label=c)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Rule Fidelity\")\n        plt.title(\"SPR_BENCH: Rule Fidelity over Epochs\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_rule_fidelity_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating rule fidelity plot: {e}\")\n        plt.close()\n\n    # --------- confusion matrix ----------\n    try:\n        preds = np.asarray(ed[\"predictions\"])\n        gts = np.asarray(ed[\"ground_truth\"])\n        if preds.size == gts.size:\n            n_cls = len(np.unique(gts))\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            cm_pct = cm / cm.sum(axis=1, keepdims=True)\n            plt.figure()\n            im = plt.imshow(cm_pct, cmap=\"Blues\", vmin=0, vmax=1)\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(\"SPR_BENCH: Confusion Matrix (Best Config)\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            for i in range(n_cls):\n                for j in range(n_cls):\n                    plt.text(\n                        j,\n                        i,\n                        f\"{cm_pct[i, j]:.2f}\",\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"red\" if cm_pct[i, j] > 0.5 else \"black\",\n                        fontsize=8,\n                    )\n            plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # --------- print key metrics ----------\n    try:\n        best_idx = cfgs.index(ed[\"best_config\"])\n        dev_acc = metrics[\"val_acc\"][best_idx][-1]\n        test_acc = (preds == gts).mean()\n        print(f\"Best config: {ed['best_config']}\")\n        print(f\"Dev accuracy (last epoch): {dev_acc:.3f}\")\n        print(f\"Test accuracy: {test_acc:.3f}\")\n    except Exception as e:\n        print(f\"Error printing metrics: {e}\")\nelse:\n    print(\"No experiment data found for SPR_BENCH.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------- SETUP ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------- LOAD DATA -------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    ed = experiment_data[\"tfidf_ngram\"][\"SPR_BENCH\"]\n    cfgs = ed[\"configs\"]\n    epochs = len(ed[\"metrics\"][\"train_acc\"][0])\n    xs = np.arange(1, epochs + 1)\n\n    # ----- PER-CONFIG CURVES -----\n    for i, cfg in enumerate(cfgs):\n        # Accuracy\n        try:\n            plt.figure()\n            plt.plot(xs, ed[\"metrics\"][\"train_acc\"][i], label=\"Train\")\n            plt.plot(xs, ed[\"metrics\"][\"val_acc\"][i], label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"SPR_BENCH Accuracy \u2013 {cfg}\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_{cfg}_accuracy.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating accuracy plot ({cfg}): {e}\")\n            plt.close()\n\n        # Loss\n        try:\n            plt.figure()\n            plt.plot(xs, ed[\"losses\"][\"train\"][i], label=\"Train\")\n            plt.plot(xs, ed[\"losses\"][\"val\"][i], label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"SPR_BENCH Loss \u2013 {cfg}\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_{cfg}_loss.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot ({cfg}): {e}\")\n            plt.close()\n\n        # Rule fidelity\n        try:\n            plt.figure()\n            plt.plot(xs, ed[\"metrics\"][\"rule_fidelity\"][i], label=\"Rule Fidelity\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Fidelity\")\n            plt.title(f\"SPR_BENCH Rule Fidelity \u2013 {cfg}\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_{cfg}_rule_fid.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating rule fidelity plot ({cfg}): {e}\")\n            plt.close()\n\n    # ----- CONFUSION MATRIX FOR BEST CONFIG -----\n    try:\n        preds = np.array(ed[\"predictions\"])\n        gts = np.array(ed[\"ground_truth\"])\n        n_cls = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f'SPR_BENCH Confusion Matrix \u2013 {ed[\"best_config\"]}')\n        for r in range(n_cls):\n            for c in range(n_cls):\n                plt.text(\n                    c, r, cm[r, c], va=\"center\", ha=\"center\", color=\"black\", fontsize=7\n                )\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix_best.png\"))\n        plt.close()\n\n        test_acc = (preds == gts).mean()\n        print(f'Best config: {ed[\"best_config\"]} | Test accuracy: {test_acc:.3f}')\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n"], "plot_plan": ["The code will first load experiment_data.npy, locate the SPR_BENCH block, and unpack per-epoch metrics and configuration names.  \nFor every optimizer configuration it will retrieve training/validation accuracy, loss, and rule-fidelity curves (all stored as object arrays), then plot them so that each metric type has its own figure; this yields three plots.  \nA fourth figure compares class-level counts for ground-truth versus best-configuration predictions using a side-by-side bar chart, satisfying the \u201cLeft: Ground Truth, Right: Generated Samples\u201d subtitle guideline.  \nEach figure is wrapped in a try-except block, carries a descriptive title, is saved to working_dir with an informative filename that includes the dataset name and metric type, and is closed in both success and exception branches to avoid resource leakage.  \nOnly data present in experiment_data.npy are used\u2014no values are fabricated.  \nTo keep output manageable we plot all epochs but no more than four total figures, well within the \u22645 limit.  \nFinally, the script computes and prints overall test accuracy of the best configuration so users can quickly verify performance.", "We first load the serialized experiment_data.npy, extract the unigram_only/SPR_BENCH block, and unpack train/val accuracy, loss, and rule-fidelity arrays as well as the config names.  Using numpy we compute the final test accuracy by comparing stored predictions with ground-truth labels and print it as an evaluation metric.  For visualization, we create at most three figures: (1) training vs validation accuracy across epochs, (2) training vs validation loss across epochs, and (3) rule fidelity across epochs.  Each figure contains one line per configuration so we stay below the five-plot limit, and each is wrapped in its own try-except block to guard against missing data.  We follow the plotting guidelines by starting with the prescribed imports, setting/creating the working directory, titling every plot, adding clear subtitles, saving under descriptive names that include the dataset and plot type, and always closing figures.  We limit ourselves strictly to the data present in experiment_data.npy; no values are fabricated.  After execution, three .png files will reside in working_dir alongside the printed test accuracy.  This meets all specification points, including reproducibility, error handling, and concise coding.", "We first load the stored experiment_data.npy, extract the only run group (model \u201clen_norm_ngram\u201d on \u201cSPR_BENCH\u201d), and recover the list of configuration names as well as epoch\u2013wise arrays for train/val accuracy, loss, and rule fidelity.  \nFor each metric we create a single figure where every configuration is a separate curve; train and validation values are shown with different linestyles to keep the plot count \u2264 5.  \nA fourth figure visualises the final-epoch confusion matrix between ground-truth and the predictions produced by the best configuration that the training script already saved.  \nAll plots are enclosed in independent try-except blocks, always closed, titled, and saved under descriptive names into working_dir.  \nEpoch indices are generated by `np.arange(1, len(curve)+1)`.  \nWe respect the plotting guideline: plain matplotlib, \u2264 5 figures, data only from the .npy file, and clear titles including the dataset name.  \nFinally, the script prints the best configuration name and its dev accuracy which are stored in the file; no additional evaluation is run.", "The script first loads the saved experiment_data.npy file, extracts the metrics for the SPR_BENCH dataset, and prepares an epoch index. It then creates up to four figures: (1) training/validation accuracy curves, (2) training/validation loss curves, (3) rule-fidelity curves, and (4) a confusion matrix for the best configuration. Each plot is wrapped in its own try-except block, saved with descriptive names inside the working directory, given clear titles, and the figure is always closed afterward. Lines and legends distinguish different optimiser settings in the curves. The confusion matrix is normalised row-wise to percentages and annotated for readability. Finally, the script prints the best configuration along with its final dev accuracy and computed test accuracy so users can verify performance without opening the plots.", "Load experiment_data.npy, extract the tfidf_ngram\u2006/\u2006SPR_BENCH branch, and iterate over all saved optimisation configs.  \nFor every config, draw three separate figures: (1) training vs validation accuracy, (2) training vs validation loss, and (3) rule-fidelity across epochs\u2014each in its own try-except block, with clear titles and legends, then save to working_dir and close.  \nBecause there are only four configs, this stays below the five-figure limit per plot type.  \nAfter the per-config curves, build a confusion matrix for the best_config using the stored ground-truth and prediction arrays, visualise it with imshow\u200a+\u200atext overlay, save, and close.  \nAll figures are titled with the dataset name and plot type (e.g. \u201cSPR_BENCH Accuracy \u2013 adam\u201d).  \nFinally, compute and print the test accuracy of the best model so the script outputs an evaluation metric.  \nThe code rigorously follows the plotting guidelines: imports, directory creation, try-except wrappers, titles, legend, colour-bar for the matrix, and explicit plt.close() after every save."], "ablation_name": [null, "Unigram-only Feature Ablation", "Length-Normalised n-gram Feature Ablation", "No-Bias Logistic Regression Ablation", "TF-IDF Weighted n-gram Feature Ablation"], "hyperparam_name": ["optimizer_type (Adam \u2192 SGD with momentum)", null, null, null, null], "is_seed_node": [false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false], "parse_metrics_plan": ["Below is a small script that immediately loads the saved NumPy file from the\nworking directory, identifies the best configuration for every dataset (using\nthe stored \u201cbest_config\u201d field), and prints the final values for every recorded\nmetric in a clear, labeled fashion. It also computes and prints the test\naccuracy from the saved predictions and ground-truth labels.", "The code will locate the saved NumPy file inside the \u201cworking\u201d directory, load\nit into memory, and loop through every experiment and dataset it contains.   For\neach dataset it finds the index of the recorded best configuration, then\nextracts the final epoch\u2019s training/validation accuracies and losses, the final\nrule-fidelity score, and computes the held-out test accuracy from the stored\npredictions.   All metrics are printed in a readable form, always preceded by\nthe dataset name and an explicit metric label so nothing is ambiguous.", "We will load experiment_data.npy from the working directory, iterate through\nevery stored model/dataset combination, and locate the index that corresponds to\nthe best configuration recorded during training. Using this index we grab the\nfinal\u2010epoch values for each metric (train accuracy, validation accuracy, rule\nfidelity, training loss, validation loss) and compute the test accuracy from the\nsaved predictions and ground-truth labels. The script then prints the dataset\nname followed by each metric name and its corresponding value, adhering to the\nrequired naming conventions and without creating any plots.", "The script will load the saved NumPy file from the working directory, locate the\nresults for each dataset (here, \u201cSPR_BENCH\u201d), identify the index corresponding\nto the best configuration, extract the last-epoch value for every stored\nmetric/loss, compute the held-out test accuracy from saved predictions, and\nprint everything in a clear, labelled way. No code is wrapped in an if-main\nguard, so it runs immediately.", "Below I load the serialized dictionary, locate the best-performing run for every\ndataset (using the \u201cbest_config\u201d field) and print the final epoch\u2019s values for\neach stored metric and loss with clear, explicit labels. Everything is executed\nimmediately at global scope."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -----------------------------------------------------\n# 0. Locate and load the experiment data\n# -----------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -----------------------------------------------------\n# 1. Iterate over every \u201ctask\u201d key (e.g. optimizer_type)\n#    and then every dataset contained within it\n# -----------------------------------------------------\nfor task_name, task_dict in experiment_data.items():\n    for dataset_name, ds_data in task_dict.items():\n        print(f\"\\nDataset: {dataset_name}\")\n\n        # ----------------- locate best run -----------------\n        configs = ds_data[\"configs\"]\n        best_cfg_name = ds_data[\"best_config\"]\n        best_idx = configs.index(best_cfg_name)\n\n        # ----------------- fetch metrics -------------------\n        final_train_acc = float(ds_data[\"metrics\"][\"train_acc\"][best_idx][-1])\n        final_val_acc = float(ds_data[\"metrics\"][\"val_acc\"][best_idx][-1])\n        final_rule_fid = float(ds_data[\"metrics\"][\"rule_fidelity\"][best_idx][-1])\n        final_train_loss = float(ds_data[\"losses\"][\"train\"][best_idx][-1])\n        final_val_loss = float(ds_data[\"losses\"][\"val\"][best_idx][-1])\n\n        # ----------------- compute test accuracy -----------\n        preds, gold = ds_data[\"predictions\"], ds_data[\"ground_truth\"]\n        test_accuracy = (preds == gold).mean()\n\n        # ----------------- print all results ---------------\n        print(f\"  Best configuration: {best_cfg_name}\")\n        print(f\"  training accuracy:     {final_train_acc:.4f}\")\n        print(f\"  validation accuracy:   {final_val_acc:.4f}\")\n        print(f\"  rule fidelity:         {final_rule_fid:.4f}\")\n        print(f\"  training loss:         {final_train_loss:.4f}\")\n        print(f\"  validation loss:       {final_val_loss:.4f}\")\n        print(f\"  test accuracy:         {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load the stored results ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to fetch final metric for best config ----------\ndef final_metric(metric_array, cfg_index):\n    \"\"\"\n    metric_array is a 1-D numpy object array where each element is the\n    list/array of values recorded across epochs for one config.\n    \"\"\"\n    return metric_array[cfg_index][-1]\n\n\n# ---------- iterate through every dataset and report metrics ----------\nfor experiment_name, datasets in experiment_data.items():\n    # e.g. experiment_name = 'unigram_only'; datasets is a dict keyed by dataset name\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        configs = data[\"configs\"]  # list of config names\n        best_cfg = data[\"best_config\"]  # name of best config\n        best_idx = configs.index(best_cfg)  # its index\n\n        # metrics\n        train_acc = final_metric(data[\"metrics\"][\"train_acc\"], best_idx)\n        val_acc = final_metric(data[\"metrics\"][\"val_acc\"], best_idx)\n        rule_fidelity = final_metric(data[\"metrics\"][\"rule_fidelity\"], best_idx)\n        train_loss = final_metric(data[\"losses\"][\"train\"], best_idx)\n        val_loss = final_metric(data[\"losses\"][\"val\"], best_idx)\n\n        # test accuracy from saved predictions / ground-truth\n        preds = np.asarray(data[\"predictions\"])\n        ground_truth = np.asarray(data[\"ground_truth\"])\n        test_accuracy = (preds == ground_truth).mean()\n\n        # -------- print nicely --------\n        print(f\"best configuration: {best_cfg}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"training loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"rule fidelity: {rule_fidelity:.4f}\")\n        print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Load saved experiment results\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Traverse structure and report metrics\n# ------------------------------------------------------------------\nfor model_name, datasets in experiment_data.items():  # e.g. \"len_norm_ngram\"\n    for dataset_name, data_dict in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(dataset_name)  # Dataset header\n\n        # Identify the best configuration index\n        best_cfg_name = data_dict[\"best_config\"]\n        best_idx = data_dict[\"configs\"].index(best_cfg_name)\n\n        # Extract final-epoch metric values for that configuration\n        train_acc = data_dict[\"metrics\"][\"train_acc\"][best_idx][-1]\n        val_acc = data_dict[\"metrics\"][\"val_acc\"][best_idx][-1]\n        rule_fid = data_dict[\"metrics\"][\"rule_fidelity\"][best_idx][-1]\n\n        train_loss = data_dict[\"losses\"][\"train\"][best_idx][-1]\n        val_loss = data_dict[\"losses\"][\"val\"][best_idx][-1]\n\n        # Compute test accuracy from stored predictions / ground truth\n        preds = np.asarray(data_dict[\"predictions\"])\n        gts = np.asarray(data_dict[\"ground_truth\"])\n        test_acc = (preds == gts).mean() if preds.size and gts.size else float(\"nan\")\n\n        # Print metrics with explicit names\n        print(f\"  train accuracy: {train_acc:.4f}\")\n        print(f\"  validation accuracy: {val_acc:.4f}\")\n        print(f\"  test accuracy: {test_acc:.4f}\")\n        print(f\"  rule fidelity: {rule_fid:.4f}\")\n        print(f\"  training loss: {train_loss:.4f}\")\n        print(f\"  validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------ LOAD EXPERIMENT DATA ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------ EXTRACT & PRINT METRICS ---------------\nmodel_key = \"no_bias_log_reg\"\nfor dataset_name, data in experiment_data[model_key].items():\n    # Best configuration index\n    best_cfg_name = data[\"best_config\"]\n    best_idx = data[\"configs\"].index(best_cfg_name)\n\n    # Metrics (arrays: shape = [n_configs, n_epochs])\n    final_train_acc = data[\"metrics\"][\"train_acc\"][best_idx][-1]\n    final_val_acc = data[\"metrics\"][\"val_acc\"][best_idx][-1]\n    final_rule_fid = data[\"metrics\"][\"rule_fidelity\"][best_idx][-1]\n\n    # Losses\n    final_train_loss = data[\"losses\"][\"train\"][best_idx][-1]\n    final_val_loss = data[\"losses\"][\"val\"][best_idx][-1]\n\n    # Test accuracy from saved predictions\n    preds = np.array(data[\"predictions\"])\n    gtruth = np.array(data[\"ground_truth\"])\n    test_accuracy = (preds == gtruth).mean()\n\n    # -------------------- PRINT ---------------------------\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Best configuration: {best_cfg_name}\")\n    print(f\"Final train accuracy: {final_train_acc:.4f}\")\n    print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(f\"Final rule fidelity: {final_rule_fid:.4f}\")\n    print(f\"Test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------- LOAD DATA -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# top level usually holds model-type branches (e.g. 'tfidf_ngram')\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, data_dict in datasets.items():\n        # identify the best configuration index\n        configs = data_dict[\"configs\"]\n        best_cfg = data_dict[\"best_config\"]\n        best_idx = (\n            configs.index(best_cfg) if best_cfg in configs else -1\n        )  # fallback to last\n\n        print(f\"{dataset_name}\")  # dataset header\n\n        # ---------- METRICS ----------\n        metric_key_to_label = {\n            \"train_acc\": \"train accuracy\",\n            \"val_acc\": \"validation accuracy\",\n            \"rule_fidelity\": \"rule fidelity\",\n        }\n        for k, label in metric_key_to_label.items():\n            # metrics[k] is an array with shape (runs, epochs)\n            value = data_dict[\"metrics\"][k][best_idx][-1]\n            print(f\"{label}: {value:.4f}\")\n\n        # ---------- LOSSES ----------\n        loss_key_to_label = {\n            \"train\": \"train loss\",\n            \"val\": \"validation loss\",\n        }\n        for k, label in loss_key_to_label.items():\n            value = data_dict[\"losses\"][k][best_idx][-1]\n            print(f\"{label}: {value:.4f}\")\n\n        # separator (optional newline for readability)\n        print()\n"], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', '  Best configuration: sgd_m0.5', '\\n', '\ntraining accuracy:     0.9665', '\\n', '  validation accuracy:   0.7920', '\\n', '\nrule fidelity:         0.9640', '\\n', '  training loss:         0.4904', '\\n', '\nvalidation loss:       18.6188', '\\n', '  test accuracy:         0.7960', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best configuration: sgd_m0.0', '\\n', 'train\naccuracy: 0.9835', '\\n', 'validation accuracy: 0.7960', '\\n', 'training loss:\n0.1399', '\\n', 'validation loss: 7.5228', '\\n', 'rule fidelity: 1.0000', '\\n',\n'test accuracy: 0.7960\\n', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', '  train accuracy: 0.7715', '\\n', '  validation accuracy:\n0.7140', '\\n', '  test accuracy: 0.7340', '\\n', '  rule fidelity: 0.7480', '\\n',\n'  training loss: 0.6864', '\\n', '  validation loss: 0.6892', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best configuration: sgd_m0.5', '\\n', 'Final train\naccuracy: 0.9925', '\\n', 'Final validation accuracy: 0.7920', '\\n', 'Final train\nloss: 0.0907', '\\n', 'Final validation loss: 18.9742', '\\n', 'Final rule\nfidelity: 0.5080', '\\n', 'Test accuracy: 0.7920', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.9905', '\\n', 'validation accuracy:\n0.7960', '\\n', 'rule fidelity: 0.4960', '\\n', 'train loss: 0.3179', '\\n',\n'validation loss: 81.0545', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null], "parse_exc_info": [null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}