{
  "best node": {
    "overall_plan": "The overall plan began with optimizing the Char-BiLSTM model through hyperparameter tuning, particularly focusing on the learning rate, and selecting models based on validation Macro-F1 scores. This involved comprehensive logging and serialization of metrics. The strategy was then enhanced by incorporating a soft attention layer to improve interpretability, allowing for the extraction of 'key-character rules' for each class and introducing the Rule-Extraction-Accuracy (REA) metric alongside Macro-F1. Building on this, the current plan merges an interpretable bag-of-characters (BoC) classifier with a neural Attn-BiLSTM. The BoC branch, a single linear layer, provides explicit human-readable rules through its learned weights, while the BiLSTM branch offers contextual reasoning. Both branches are jointly optimized using the average of their logits for cross-entropy loss and an L1 penalty on BoC weights for sparsity. During training, validation loss and F1 scores are printed, metrics recorded, and post-training evaluations include extracting one-token rules, computing REA, and evaluating final Macro-F1 on the test split. All results and learned rules are stored for further analysis. This comprehensive strategy aims to develop effective models that are interpretable and provide insight into their decision-making processes.",
    "analysis": "",
    "metric": {
      "value": {
        "metric_names": [
          {
            "metric_name": "training loss",
            "lower_is_better": true,
            "description": "The loss value during the training phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.0109,
                "best_value": 0.0109
              }
            ]
          },
          {
            "metric_name": "validation loss",
            "lower_is_better": true,
            "description": "The loss value during the validation phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.5924,
                "best_value": 0.5924
              }
            ]
          },
          {
            "metric_name": "training F1 score",
            "lower_is_better": false,
            "description": "The F1 score during the training phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.9985,
                "best_value": 0.9985
              }
            ]
          },
          {
            "metric_name": "validation F1 score",
            "lower_is_better": false,
            "description": "The F1 score during the validation phase.",
            "data": [
              {
                "dataset_name": "SPR_BENCH",
                "final_value": 0.7979,
                "best_value": 0.7979
              }
            ]
          },
          {
            "metric_name": "rule extraction accuracy",
            "lower_is_better": false,
            "description": "The accuracy of rule extraction.",
            "data": [
              {
                "dataset_name": "development set",
                "final_value": 0.564,
                "best_value": 0.564
              },
              {
                "dataset_name": "test set",
                "final_value": 0.572,
                "best_value": 0.572
              }
            ]
          },
          {
            "metric_name": "hybrid model macro-F1 score",
            "lower_is_better": false,
            "description": "The macro-F1 score of the hybrid model on the test set.",
            "data": [
              {
                "dataset_name": "test set",
                "final_value": 0.797,
                "best_value": 0.797
              }
            ]
          }
        ]
      },
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "import os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# --------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- data loading --------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\nspr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_root = spr_root if spr_root.exists() else pathlib.Path(\"SPR_BENCH/\")\ndsets = load_spr_bench(spr_root)\nprint({k: len(v) for k, v in dsets.items()})\n\n\ndef build_vocab(split):\n    charset = set()\n    for seq in split[\"sequence\"]:\n        charset.update(seq)\n    stoi = {c: i + 1 for i, c in enumerate(sorted(charset))}\n    stoi[\"<PAD>\"] = 0\n    return stoi\n\n\nvocab = build_vocab(dsets[\"train\"])\nitos = {i: s for s, i in vocab.items()}\nvocab_size = len(vocab)\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\n\n\n# ---------------------- dataset & loader ----------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab[c] for c in self.seqs[idx]]\n        return {\n            \"seq\": torch.tensor(seq, dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lens = torch.tensor([len(b[\"seq\"]) for b in batch])\n    maxlen = lens.max()\n    padded = torch.zeros(len(batch), maxlen, dtype=torch.long)\n    bags = torch.zeros(len(batch), vocab_size, dtype=torch.float32)\n    for i, b in enumerate(batch):\n        l = lens[i]\n        padded[i, :l] = b[\"seq\"]\n        bags[i].index_add_(0, b[\"seq\"], torch.ones(l))\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input\": padded, \"lengths\": lens, \"bag\": bags, \"label\": labels}\n\n\nbs = 128\ntrain_dl = DataLoader(\n    SPRDataset(dsets[\"train\"], vocab), bs, shuffle=True, collate_fn=collate\n)\ndev_dl = DataLoader(\n    SPRDataset(dsets[\"dev\"], vocab), bs, shuffle=False, collate_fn=collate\n)\ntest_dl = DataLoader(\n    SPRDataset(dsets[\"test\"], vocab), bs, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------------- model --------------------------------------\nclass AttnBiLSTM(nn.Module):\n    def __init__(self, vocab_sz, emb=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, bidirectional=True, batch_first=True)\n        self.attn_vec = nn.Parameter(torch.randn(hid * 2))\n        self.fc = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens, need_attn=False):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        out, _ = self.lstm(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        attn = torch.tanh(out) @ self.attn_vec\n        mask = x != 0\n        attn[~mask] = -1e9\n        w = torch.softmax(attn, 1).unsqueeze(-1)\n        ctx = (out * w).sum(1)\n        logits = self.fc(ctx)\n        return (logits, w.squeeze(-1)) if need_attn else logits\n\n\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.lin = nn.Linear(vocab_sz, num_classes)\n\n    def forward(self, bag):\n        return self.lin(bag)\n\n\nclass HybridModel(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.bilstm = AttnBiLSTM(vocab_sz)\n        self.bag = BagClassifier(vocab_sz)\n\n    def forward(self, inp, lens, bag):\n        log_lstm = self.bilstm(inp, lens)\n        log_bag = self.bag(bag)\n        return (log_lstm + log_bag) / 2, log_bag  # ensemble logits and raw BoC logits\n\n\nmodel = HybridModel(vocab_size).to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\nl1_lambda = 1e-4\n\n\n# ------------------- training & evaluation helpers ------------------\ndef epoch_pass(model, dl, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in dl:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits, logits_bag = model(batch[\"input\"], batch[\"lengths\"], batch[\"bag\"])\n            loss = nn.functional.cross_entropy(logits, batch[\"label\"])\n            # sparse penalty on bag weights\n            loss += l1_lambda * model.bag.lin.weight.abs().mean()\n            if train:\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(batch[\"label\"].cpu().tolist())\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------------- experiment tracking ----------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"REA_dev\": None, \"REA_test\": None},\n        \"rules\": {},\n        \"preds_test\": [],\n        \"gts_test\": [],\n    }\n}\n\nbest_f1 = 0\nbest_state = None\nepochs = 12\nfor ep in range(1, epochs + 1):\n    tr_loss, tr_f1, _, _ = epoch_pass(model, train_dl, True)\n    val_loss, val_f1, _, _ = epoch_pass(model, dev_dl, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_state = model.state_dict()\n    print(f\"Epoch {ep}: validation_loss = {val_loss:.4f}  val_f1={val_f1:.4f}\")\n\nmodel.load_state_dict(best_state)\n\n# -------------------- rule extraction -------------------------------\nwith torch.no_grad():\n    weights = model.bag.lin.weight.detach().cpu().numpy()  # (C,V)\nrules = {}\nfor c in range(num_classes):\n    tok = int(np.argmax(weights[c, 1:]) + 1)  # ignore PAD\n    rules[c] = tok\nexperiment_data[\"SPR_BENCH\"][\"rules\"] = {c: itos[tok] for c, tok in rules.items()}\nprint(\"Learned single-token rules:\", experiment_data[\"SPR_BENCH\"][\"rules\"])\n\n\ndef rule_predict(seq, rules):\n    for ch in seq:\n        vid = vocab.get(ch, 0)\n        for cls, tok in rules.items():\n            if vid == tok:\n                return cls\n    return None\n\n\ndef evaluate_rules(hf_split, rules, model):\n    correct = 0\n    total = len(hf_split)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for seq, label in zip(hf_split[\"sequence\"], hf_split[\"label\"]):\n            rp = rule_predict(seq, rules)\n            if rp is None:\n                # fallback to hybrid model\n                inp = torch.tensor([[vocab[c] for c in seq]], dtype=torch.long).to(\n                    device\n                )\n                lens = torch.tensor([inp.size(1)]).to(device)\n                bag = torch.zeros(1, vocab_size, device=device)\n                bag[0].index_add_(\n                    1, inp.squeeze(0), torch.ones(inp.size(1), device=device)\n                )\n                logit, _ = model(inp, lens, bag)\n                rp = int(torch.argmax(logit, 1).item())\n            preds.append(rp)\n            if rp == label:\n                correct += 1\n    return correct / total, preds\n\n\nREA_dev, _ = evaluate_rules(dsets[\"dev\"], rules, model)\nREA_test, preds_test = evaluate_rules(dsets[\"test\"], rules, model)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_dev\"] = REA_dev\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_test\"] = REA_test\nprint(f\"Rule Extraction Accuracy (dev): {REA_dev:.4f}\")\nprint(f\"Rule Extraction Accuracy (test): {REA_test:.4f}\")\n\n# -------------------- final test F1 ---------------------------------\ntest_loss, test_f1, preds, gts = epoch_pass(model, test_dl, False)\nexperiment_data[\"SPR_BENCH\"][\"preds_test\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"gts_test\"] = gts\nprint(f\"Hybrid Model Test Macro-F1: {test_f1:.4f}\")\n\n# -------------------- save ------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- directories ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helpers ----------\ndef macro_f1(y_true, y_pred):\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    f1s = []\n    for lb in labels:\n        tp = np.sum((y_true == lb) & (y_pred == lb))\n        fp = np.sum((y_true != lb) & (y_pred == lb))\n        fn = np.sum((y_true == lb) & (y_pred != lb))\n        prec = tp / (tp + fp + 1e-12)\n        rec = tp / (tp + fn + 1e-12)\n        f1s.append(0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# ---------- plotting ----------\nplot_cap = 5  # maximum total figures\nplotted = 0\n\nfor dname, dct in experiment_data.items():\n    if plotted >= plot_cap:\n        break\n\n    # ---- 1) loss curves ----\n    try:\n        if plotted < plot_cap:\n            tr_loss = dct[\"losses\"][\"train\"]\n            val_loss = dct[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr_loss) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Loss Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting loss for {dname}: {e}\")\n        plt.close()\n\n    # ---- 2) F1 curves ----\n    try:\n        if plotted < plot_cap:\n            tr_f1 = dct[\"metrics\"][\"train_f1\"]\n            val_f1 = dct[\"metrics\"][\"val_f1\"]\n            epochs = np.arange(1, len(tr_f1) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train F1\")\n            plt.plot(epochs, val_f1, label=\"Validation F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting F1 for {dname}: {e}\")\n        plt.close()\n\n    # ---- 3) confusion matrix ----\n    try:\n        if plotted < plot_cap and dct.get(\"gts_test\") and dct.get(\"preds_test\"):\n            gts = np.array(dct[\"gts_test\"])\n            preds = np.array(dct[\"preds_test\"])\n            print(f\"{dname} Test Macro-F1:\", macro_f1(gts, preds))\n            labels = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (Test Set)\")\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=7)\n            fname = os.path.join(working_dir, f\"{dname.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting CM for {dname}: {e}\")\n        plt.close()\n\n    # ---- 4) rule accuracy bar ----\n    try:\n        if (\n            plotted < plot_cap\n            and isinstance(dct[\"metrics\"].get(\"REA_dev\"), (int, float))\n            and isinstance(dct[\"metrics\"].get(\"REA_test\"), (int, float))\n        ):\n            rea_dev = dct[\"metrics\"][\"REA_dev\"]\n            rea_test = dct[\"metrics\"][\"REA_test\"]\n            plt.figure()\n            plt.bar([\"Dev\", \"Test\"], [rea_dev, rea_test], color=[\"skyblue\", \"salmon\"])\n            plt.ylim(0, 1)\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Rule Extraction Accuracy\\nLeft: Dev, Right: Test\")\n            fname = os.path.join(working_dir, f\"{dname.lower()}_rule_accuracy.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting REA for {dname}: {e}\")\n        plt.close()\n",
    "plot_plan": "We will load experiment_data.npy, iterate over every dataset entry, and collect existing fields such as train/val losses, F1 scores, rule\u2013extraction accuracies, and test predictions/labels. For each dataset we create up to four plots, each inside its own try-except: (1) train vs. validation loss curve, (2) train vs. validation macro-F1 curve, (3) confusion matrix on the test set, and (4) bar chart comparing REA_dev and REA_test. We keep a global counter so that the total number of figures never exceeds five, satisfying the \u201cat most 5\u201d requirement when multiple datasets are present. All plots are saved into working_dir with descriptive file names including the dataset and plot type, and every figure is closed afterward. Titles are explicit, and subtitles clarify axes or data splits. A helper function computes macro-F1 so we can print it before plotting. No data are invented; everything comes directly from experiment_data.npy. The code is concise, self-contained, and uses only base matplotlib + numpy.",
    "plot_analyses": [
      {
        "analysis": "The loss curves indicate a rapid convergence of the training loss, reaching near-zero values by around epoch 8. However, the validation loss starts increasing after epoch 4, suggesting potential overfitting. This behavior indicates that the model is learning the training data well but struggles to generalize to unseen data.",
        "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_loss_curves.png"
      },
      {
        "analysis": "The macro-F1 curves demonstrate that the training F1 score quickly converges to nearly 1, indicating excellent performance on the training set. The validation F1 score shows slower growth, plateauing around 0.8 after epoch 6. This further supports the hypothesis of overfitting as the model performs better on the training data compared to the validation set.",
        "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_f1_curves.png"
      },
      {
        "analysis": "The confusion matrix for the test set shows a reasonable balance in predictions, with 396 true positives, 102 false negatives, 101 false positives, and 401 true negatives. The model appears to perform slightly better on class 1 compared to class 0, but the misclassification rates are relatively low overall.",
        "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_confusion_matrix.png"
      },
      {
        "analysis": "The rule extraction accuracy indicates moderate success, with accuracy slightly above 0.6 for both the dev and test sets. This suggests that the model is able to extract and represent rules to some extent, though there is room for improvement to enhance interpretability and alignment with ground truth rules.",
        "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_rule_accuracy.png"
      }
    ],
    "plot_paths": [
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_loss_curves.png",
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_f1_curves.png",
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_confusion_matrix.png",
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/spr_bench_rule_accuracy.png"
    ],
    "vlm_feedback_summary": "The plots collectively indicate that while the model demonstrates strong performance on the training set, there are signs of overfitting as evidenced by the divergence between training and validation losses and F1 scores. The confusion matrix shows reasonable classification performance with slightly better handling of one class over the other. Rule extraction accuracy is moderate, suggesting that the interpretability aspect of the model is functional but not optimal.",
    "exp_results_dir": "experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857",
    "exp_results_npy_files": [
      "experiment_results/experiment_77518c893aff4bd4ad649b2e6558fda5_proc_3305857/experiment_data.npy"
    ]
  },
  "best node with different seeds": [
    {
      "overall_plan": "The overall plan began with optimizing the Char-BiLSTM model through hyperparameter tuning, with a focus on learning rate and selection based on validation Macro-F1 scores, accompanied by comprehensive logging and serialization of metrics. The strategy was enhanced by incorporating a soft attention layer to improve interpretability, enabling the extraction of 'key-character rules' for each class and introducing the Rule-Extraction-Accuracy (REA) metric alongside Macro-F1. Building on this, the plan merged an interpretable bag-of-characters (BoC) classifier with a neural Attn-BiLSTM. The BoC branch, a single linear layer, provides explicit human-readable rules through its learned weights, while the BiLSTM branch offers contextual reasoning. Both branches are jointly optimized using the average of their logits for cross-entropy loss and an L1 penalty on BoC weights for sparsity. During training, validation loss and F1 scores are printed, metrics recorded, and post-training evaluations include extracting one-token rules, computing REA, and evaluating final Macro-F1 on the test split. All results and learned rules are stored for further analysis. This comprehensive strategy aims to develop effective models that are interpretable and provide insight into their decision-making processes. The current node, being a seed node, does not introduce new changes or ideas, thus the focus remains on the previously outlined comprehensive strategy.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "training loss",
              "lower_is_better": true,
              "description": "The loss value during training phase.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.0449,
                  "best_value": 0.0449
                }
              ]
            },
            {
              "metric_name": "validation loss",
              "lower_is_better": true,
              "description": "The loss value during validation phase.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.654,
                  "best_value": 0.654
                }
              ]
            },
            {
              "metric_name": "training F1 score",
              "lower_is_better": false,
              "description": "F1 score during training phase.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.99,
                  "best_value": 0.99
                }
              ]
            },
            {
              "metric_name": "validation F1 score",
              "lower_is_better": false,
              "description": "F1 score during validation phase.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.7879,
                  "best_value": 0.7879
                }
              ]
            },
            {
              "metric_name": "rule extraction accuracy on development set",
              "lower_is_better": false,
              "description": "Accuracy of rule extraction on development set.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.414,
                  "best_value": 0.414
                }
              ]
            },
            {
              "metric_name": "rule extraction accuracy on test set",
              "lower_is_better": false,
              "description": "Accuracy of rule extraction on test set.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.416,
                  "best_value": 0.416
                }
              ]
            },
            {
              "metric_name": "hybrid model macro-F1 score on test set",
              "lower_is_better": false,
              "description": "Macro F1 score of hybrid model on test set.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.79,
                  "best_value": 0.79
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# --------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- data loading --------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\nspr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_root = spr_root if spr_root.exists() else pathlib.Path(\"SPR_BENCH/\")\ndsets = load_spr_bench(spr_root)\nprint({k: len(v) for k, v in dsets.items()})\n\n\ndef build_vocab(split):\n    charset = set()\n    for seq in split[\"sequence\"]:\n        charset.update(seq)\n    stoi = {c: i + 1 for i, c in enumerate(sorted(charset))}\n    stoi[\"<PAD>\"] = 0\n    return stoi\n\n\nvocab = build_vocab(dsets[\"train\"])\nitos = {i: s for s, i in vocab.items()}\nvocab_size = len(vocab)\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\n\n\n# ---------------------- dataset & loader ----------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab[c] for c in self.seqs[idx]]\n        return {\n            \"seq\": torch.tensor(seq, dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lens = torch.tensor([len(b[\"seq\"]) for b in batch])\n    maxlen = lens.max()\n    padded = torch.zeros(len(batch), maxlen, dtype=torch.long)\n    bags = torch.zeros(len(batch), vocab_size, dtype=torch.float32)\n    for i, b in enumerate(batch):\n        l = lens[i]\n        padded[i, :l] = b[\"seq\"]\n        bags[i].index_add_(0, b[\"seq\"], torch.ones(l))\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input\": padded, \"lengths\": lens, \"bag\": bags, \"label\": labels}\n\n\nbs = 128\ntrain_dl = DataLoader(\n    SPRDataset(dsets[\"train\"], vocab), bs, shuffle=True, collate_fn=collate\n)\ndev_dl = DataLoader(\n    SPRDataset(dsets[\"dev\"], vocab), bs, shuffle=False, collate_fn=collate\n)\ntest_dl = DataLoader(\n    SPRDataset(dsets[\"test\"], vocab), bs, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------------- model --------------------------------------\nclass AttnBiLSTM(nn.Module):\n    def __init__(self, vocab_sz, emb=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, bidirectional=True, batch_first=True)\n        self.attn_vec = nn.Parameter(torch.randn(hid * 2))\n        self.fc = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens, need_attn=False):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        out, _ = self.lstm(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        attn = torch.tanh(out) @ self.attn_vec\n        mask = x != 0\n        attn[~mask] = -1e9\n        w = torch.softmax(attn, 1).unsqueeze(-1)\n        ctx = (out * w).sum(1)\n        logits = self.fc(ctx)\n        return (logits, w.squeeze(-1)) if need_attn else logits\n\n\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.lin = nn.Linear(vocab_sz, num_classes)\n\n    def forward(self, bag):\n        return self.lin(bag)\n\n\nclass HybridModel(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.bilstm = AttnBiLSTM(vocab_sz)\n        self.bag = BagClassifier(vocab_sz)\n\n    def forward(self, inp, lens, bag):\n        log_lstm = self.bilstm(inp, lens)\n        log_bag = self.bag(bag)\n        return (log_lstm + log_bag) / 2, log_bag  # ensemble logits and raw BoC logits\n\n\nmodel = HybridModel(vocab_size).to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\nl1_lambda = 1e-4\n\n\n# ------------------- training & evaluation helpers ------------------\ndef epoch_pass(model, dl, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in dl:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits, logits_bag = model(batch[\"input\"], batch[\"lengths\"], batch[\"bag\"])\n            loss = nn.functional.cross_entropy(logits, batch[\"label\"])\n            # sparse penalty on bag weights\n            loss += l1_lambda * model.bag.lin.weight.abs().mean()\n            if train:\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(batch[\"label\"].cpu().tolist())\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------------- experiment tracking ----------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"REA_dev\": None, \"REA_test\": None},\n        \"rules\": {},\n        \"preds_test\": [],\n        \"gts_test\": [],\n    }\n}\n\nbest_f1 = 0\nbest_state = None\nepochs = 12\nfor ep in range(1, epochs + 1):\n    tr_loss, tr_f1, _, _ = epoch_pass(model, train_dl, True)\n    val_loss, val_f1, _, _ = epoch_pass(model, dev_dl, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_state = model.state_dict()\n    print(f\"Epoch {ep}: validation_loss = {val_loss:.4f}  val_f1={val_f1:.4f}\")\n\nmodel.load_state_dict(best_state)\n\n# -------------------- rule extraction -------------------------------\nwith torch.no_grad():\n    weights = model.bag.lin.weight.detach().cpu().numpy()  # (C,V)\nrules = {}\nfor c in range(num_classes):\n    tok = int(np.argmax(weights[c, 1:]) + 1)  # ignore PAD\n    rules[c] = tok\nexperiment_data[\"SPR_BENCH\"][\"rules\"] = {c: itos[tok] for c, tok in rules.items()}\nprint(\"Learned single-token rules:\", experiment_data[\"SPR_BENCH\"][\"rules\"])\n\n\ndef rule_predict(seq, rules):\n    for ch in seq:\n        vid = vocab.get(ch, 0)\n        for cls, tok in rules.items():\n            if vid == tok:\n                return cls\n    return None\n\n\ndef evaluate_rules(hf_split, rules, model):\n    correct = 0\n    total = len(hf_split)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for seq, label in zip(hf_split[\"sequence\"], hf_split[\"label\"]):\n            rp = rule_predict(seq, rules)\n            if rp is None:\n                # fallback to hybrid model\n                inp = torch.tensor([[vocab[c] for c in seq]], dtype=torch.long).to(\n                    device\n                )\n                lens = torch.tensor([inp.size(1)]).to(device)\n                bag = torch.zeros(1, vocab_size, device=device)\n                bag[0].index_add_(\n                    1, inp.squeeze(0), torch.ones(inp.size(1), device=device)\n                )\n                logit, _ = model(inp, lens, bag)\n                rp = int(torch.argmax(logit, 1).item())\n            preds.append(rp)\n            if rp == label:\n                correct += 1\n    return correct / total, preds\n\n\nREA_dev, _ = evaluate_rules(dsets[\"dev\"], rules, model)\nREA_test, preds_test = evaluate_rules(dsets[\"test\"], rules, model)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_dev\"] = REA_dev\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_test\"] = REA_test\nprint(f\"Rule Extraction Accuracy (dev): {REA_dev:.4f}\")\nprint(f\"Rule Extraction Accuracy (test): {REA_test:.4f}\")\n\n# -------------------- final test F1 ---------------------------------\ntest_loss, test_f1, preds, gts = epoch_pass(model, test_dl, False)\nexperiment_data[\"SPR_BENCH\"][\"preds_test\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"gts_test\"] = gts\nprint(f\"Hybrid Model Test Macro-F1: {test_f1:.4f}\")\n\n# -------------------- save ------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- directories ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helpers ----------\ndef macro_f1(y_true, y_pred):\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    f1s = []\n    for lb in labels:\n        tp = np.sum((y_true == lb) & (y_pred == lb))\n        fp = np.sum((y_true != lb) & (y_pred == lb))\n        fn = np.sum((y_true == lb) & (y_pred != lb))\n        prec = tp / (tp + fp + 1e-12)\n        rec = tp / (tp + fn + 1e-12)\n        f1s.append(0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# ---------- plotting ----------\nplot_cap = 5  # maximum total figures\nplotted = 0\n\nfor dname, dct in experiment_data.items():\n    if plotted >= plot_cap:\n        break\n\n    # ---- 1) loss curves ----\n    try:\n        if plotted < plot_cap:\n            tr_loss = dct[\"losses\"][\"train\"]\n            val_loss = dct[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr_loss) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Loss Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting loss for {dname}: {e}\")\n        plt.close()\n\n    # ---- 2) F1 curves ----\n    try:\n        if plotted < plot_cap:\n            tr_f1 = dct[\"metrics\"][\"train_f1\"]\n            val_f1 = dct[\"metrics\"][\"val_f1\"]\n            epochs = np.arange(1, len(tr_f1) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train F1\")\n            plt.plot(epochs, val_f1, label=\"Validation F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting F1 for {dname}: {e}\")\n        plt.close()\n\n    # ---- 3) confusion matrix ----\n    try:\n        if plotted < plot_cap and dct.get(\"gts_test\") and dct.get(\"preds_test\"):\n            gts = np.array(dct[\"gts_test\"])\n            preds = np.array(dct[\"preds_test\"])\n            print(f\"{dname} Test Macro-F1:\", macro_f1(gts, preds))\n            labels = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (Test Set)\")\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=7)\n            fname = os.path.join(working_dir, f\"{dname.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting CM for {dname}: {e}\")\n        plt.close()\n\n    # ---- 4) rule accuracy bar ----\n    try:\n        if (\n            plotted < plot_cap\n            and isinstance(dct[\"metrics\"].get(\"REA_dev\"), (int, float))\n            and isinstance(dct[\"metrics\"].get(\"REA_test\"), (int, float))\n        ):\n            rea_dev = dct[\"metrics\"][\"REA_dev\"]\n            rea_test = dct[\"metrics\"][\"REA_test\"]\n            plt.figure()\n            plt.bar([\"Dev\", \"Test\"], [rea_dev, rea_test], color=[\"skyblue\", \"salmon\"])\n            plt.ylim(0, 1)\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Rule Extraction Accuracy\\nLeft: Dev, Right: Test\")\n            fname = os.path.join(working_dir, f\"{dname.lower()}_rule_accuracy.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting REA for {dname}: {e}\")\n        plt.close()\n",
      "plot_analyses": [
        {
          "analysis": "The loss curves indicate a significant divergence between training and validation losses after a few epochs. This suggests potential overfitting, as the training loss continues to decrease while the validation loss increases consistently after epoch 4. The model might be over-parameterized or insufficiently regularized. Strategies like early stopping, dropout, or weight regularization could help mitigate this issue.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_loss_curves.png"
        },
        {
          "analysis": "The Macro-F1 curves show that the training F1 score quickly reaches near-perfect levels, while the validation F1 score plateaus at a lower value and fluctuates slightly. This discrepancy between training and validation performance further supports the observation of overfitting. The model is likely capturing noise in the training data rather than generalizable patterns.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_f1_curves.png"
        },
        {
          "analysis": "The confusion matrix for the test set shows a balanced performance between the two classes, with 395 correct predictions for each class and 103 and 107 misclassifications for classes 0 and 1, respectively. This indicates that the model is not biased toward either class but still has room for improvement in overall accuracy.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_confusion_matrix.png"
        },
        {
          "analysis": "The rule extraction accuracy is approximately 40% for both the development and test sets, indicating that the interpretability aspect of the model is underperforming. This low accuracy suggests that the rule-based layer is not effectively capturing or representing the underlying poly-factor rules. Further refinement of the rule extraction mechanism is necessary to improve its utility and interpretability.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_rule_accuracy.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_loss_curves.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_f1_curves.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_confusion_matrix.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/spr_bench_rule_accuracy.png"
      ],
      "vlm_feedback_summary": "The analysis highlights overfitting as a significant issue, as evidenced by the divergence in loss and F1 curves between training and validation. While the model demonstrates balanced performance across classes in the test set, the overall accuracy and interpretability, particularly in rule extraction, require substantial improvement. Refinements in regularization, rule-based layer design, and interpretability strategies are recommended.",
      "exp_results_dir": "experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855",
      "exp_results_npy_files": [
        "experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/experiment_data.npy"
      ]
    },
    {
      "overall_plan": "The overall plan remains focused on optimizing the Char-BiLSTM model by tuning hyperparameters, particularly the learning rate, and selecting models based on validation Macro-F1 scores. A soft attention layer was incorporated to enhance interpretability, allowing for the extraction of 'key-character rules' for each class and introducing the Rule-Extraction-Accuracy (REA) metric alongside Macro-F1. The strategy evolved by merging an interpretable bag-of-characters (BoC) classifier with a neural Attn-BiLSTM, where the BoC provides explicit human-readable rules and the BiLSTM offers contextual reasoning. Both branches are optimized using averaged logits for cross-entropy loss with an L1 penalty on BoC weights for sparsity. Training involves monitoring validation loss and F1 scores, while post-training evaluations include rule extraction, REA computation, and final Macro-F1 assessment on test data. All results and rules are stored for analysis. The current node is a 'Seed node,' indicating a foundational point in a sequence, but does not alter the existing comprehensive plan.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "training loss",
              "lower_is_better": true,
              "description": "The loss value during model training.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.2347,
                  "best_value": 0.2347
                }
              ]
            },
            {
              "metric_name": "validation loss",
              "lower_is_better": true,
              "description": "The loss value during model validation.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.6247,
                  "best_value": 0.6247
                }
              ]
            },
            {
              "metric_name": "training F1 score",
              "lower_is_better": false,
              "description": "The F1 score during model training.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.9474,
                  "best_value": 0.9474
                }
              ]
            },
            {
              "metric_name": "validation F1 score",
              "lower_is_better": false,
              "description": "The F1 score during model validation.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.7678,
                  "best_value": 0.7678
                }
              ]
            },
            {
              "metric_name": "rule extraction accuracy",
              "lower_is_better": false,
              "description": "The accuracy of rule extraction.",
              "data": [
                {
                  "dataset_name": "development set",
                  "final_value": 0.532,
                  "best_value": 0.532
                },
                {
                  "dataset_name": "test set",
                  "final_value": 0.517,
                  "best_value": 0.517
                }
              ]
            },
            {
              "metric_name": "hybrid model macro-F1 score",
              "lower_is_better": false,
              "description": "The macro-F1 score of the hybrid model on the test set.",
              "data": [
                {
                  "dataset_name": "test set",
                  "final_value": 0.7804,
                  "best_value": 0.7804
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# --------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- data loading --------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\nspr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_root = spr_root if spr_root.exists() else pathlib.Path(\"SPR_BENCH/\")\ndsets = load_spr_bench(spr_root)\nprint({k: len(v) for k, v in dsets.items()})\n\n\ndef build_vocab(split):\n    charset = set()\n    for seq in split[\"sequence\"]:\n        charset.update(seq)\n    stoi = {c: i + 1 for i, c in enumerate(sorted(charset))}\n    stoi[\"<PAD>\"] = 0\n    return stoi\n\n\nvocab = build_vocab(dsets[\"train\"])\nitos = {i: s for s, i in vocab.items()}\nvocab_size = len(vocab)\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\n\n\n# ---------------------- dataset & loader ----------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab[c] for c in self.seqs[idx]]\n        return {\n            \"seq\": torch.tensor(seq, dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lens = torch.tensor([len(b[\"seq\"]) for b in batch])\n    maxlen = lens.max()\n    padded = torch.zeros(len(batch), maxlen, dtype=torch.long)\n    bags = torch.zeros(len(batch), vocab_size, dtype=torch.float32)\n    for i, b in enumerate(batch):\n        l = lens[i]\n        padded[i, :l] = b[\"seq\"]\n        bags[i].index_add_(0, b[\"seq\"], torch.ones(l))\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input\": padded, \"lengths\": lens, \"bag\": bags, \"label\": labels}\n\n\nbs = 128\ntrain_dl = DataLoader(\n    SPRDataset(dsets[\"train\"], vocab), bs, shuffle=True, collate_fn=collate\n)\ndev_dl = DataLoader(\n    SPRDataset(dsets[\"dev\"], vocab), bs, shuffle=False, collate_fn=collate\n)\ntest_dl = DataLoader(\n    SPRDataset(dsets[\"test\"], vocab), bs, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------------- model --------------------------------------\nclass AttnBiLSTM(nn.Module):\n    def __init__(self, vocab_sz, emb=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, bidirectional=True, batch_first=True)\n        self.attn_vec = nn.Parameter(torch.randn(hid * 2))\n        self.fc = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens, need_attn=False):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        out, _ = self.lstm(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        attn = torch.tanh(out) @ self.attn_vec\n        mask = x != 0\n        attn[~mask] = -1e9\n        w = torch.softmax(attn, 1).unsqueeze(-1)\n        ctx = (out * w).sum(1)\n        logits = self.fc(ctx)\n        return (logits, w.squeeze(-1)) if need_attn else logits\n\n\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.lin = nn.Linear(vocab_sz, num_classes)\n\n    def forward(self, bag):\n        return self.lin(bag)\n\n\nclass HybridModel(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.bilstm = AttnBiLSTM(vocab_sz)\n        self.bag = BagClassifier(vocab_sz)\n\n    def forward(self, inp, lens, bag):\n        log_lstm = self.bilstm(inp, lens)\n        log_bag = self.bag(bag)\n        return (log_lstm + log_bag) / 2, log_bag  # ensemble logits and raw BoC logits\n\n\nmodel = HybridModel(vocab_size).to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\nl1_lambda = 1e-4\n\n\n# ------------------- training & evaluation helpers ------------------\ndef epoch_pass(model, dl, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in dl:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits, logits_bag = model(batch[\"input\"], batch[\"lengths\"], batch[\"bag\"])\n            loss = nn.functional.cross_entropy(logits, batch[\"label\"])\n            # sparse penalty on bag weights\n            loss += l1_lambda * model.bag.lin.weight.abs().mean()\n            if train:\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(batch[\"label\"].cpu().tolist())\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------------- experiment tracking ----------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"REA_dev\": None, \"REA_test\": None},\n        \"rules\": {},\n        \"preds_test\": [],\n        \"gts_test\": [],\n    }\n}\n\nbest_f1 = 0\nbest_state = None\nepochs = 12\nfor ep in range(1, epochs + 1):\n    tr_loss, tr_f1, _, _ = epoch_pass(model, train_dl, True)\n    val_loss, val_f1, _, _ = epoch_pass(model, dev_dl, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_state = model.state_dict()\n    print(f\"Epoch {ep}: validation_loss = {val_loss:.4f}  val_f1={val_f1:.4f}\")\n\nmodel.load_state_dict(best_state)\n\n# -------------------- rule extraction -------------------------------\nwith torch.no_grad():\n    weights = model.bag.lin.weight.detach().cpu().numpy()  # (C,V)\nrules = {}\nfor c in range(num_classes):\n    tok = int(np.argmax(weights[c, 1:]) + 1)  # ignore PAD\n    rules[c] = tok\nexperiment_data[\"SPR_BENCH\"][\"rules\"] = {c: itos[tok] for c, tok in rules.items()}\nprint(\"Learned single-token rules:\", experiment_data[\"SPR_BENCH\"][\"rules\"])\n\n\ndef rule_predict(seq, rules):\n    for ch in seq:\n        vid = vocab.get(ch, 0)\n        for cls, tok in rules.items():\n            if vid == tok:\n                return cls\n    return None\n\n\ndef evaluate_rules(hf_split, rules, model):\n    correct = 0\n    total = len(hf_split)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for seq, label in zip(hf_split[\"sequence\"], hf_split[\"label\"]):\n            rp = rule_predict(seq, rules)\n            if rp is None:\n                # fallback to hybrid model\n                inp = torch.tensor([[vocab[c] for c in seq]], dtype=torch.long).to(\n                    device\n                )\n                lens = torch.tensor([inp.size(1)]).to(device)\n                bag = torch.zeros(1, vocab_size, device=device)\n                bag[0].index_add_(\n                    1, inp.squeeze(0), torch.ones(inp.size(1), device=device)\n                )\n                logit, _ = model(inp, lens, bag)\n                rp = int(torch.argmax(logit, 1).item())\n            preds.append(rp)\n            if rp == label:\n                correct += 1\n    return correct / total, preds\n\n\nREA_dev, _ = evaluate_rules(dsets[\"dev\"], rules, model)\nREA_test, preds_test = evaluate_rules(dsets[\"test\"], rules, model)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_dev\"] = REA_dev\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_test\"] = REA_test\nprint(f\"Rule Extraction Accuracy (dev): {REA_dev:.4f}\")\nprint(f\"Rule Extraction Accuracy (test): {REA_test:.4f}\")\n\n# -------------------- final test F1 ---------------------------------\ntest_loss, test_f1, preds, gts = epoch_pass(model, test_dl, False)\nexperiment_data[\"SPR_BENCH\"][\"preds_test\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"gts_test\"] = gts\nprint(f\"Hybrid Model Test Macro-F1: {test_f1:.4f}\")\n\n# -------------------- save ------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- directories ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helpers ----------\ndef macro_f1(y_true, y_pred):\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    f1s = []\n    for lb in labels:\n        tp = np.sum((y_true == lb) & (y_pred == lb))\n        fp = np.sum((y_true != lb) & (y_pred == lb))\n        fn = np.sum((y_true == lb) & (y_pred != lb))\n        prec = tp / (tp + fp + 1e-12)\n        rec = tp / (tp + fn + 1e-12)\n        f1s.append(0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# ---------- plotting ----------\nplot_cap = 5  # maximum total figures\nplotted = 0\n\nfor dname, dct in experiment_data.items():\n    if plotted >= plot_cap:\n        break\n\n    # ---- 1) loss curves ----\n    try:\n        if plotted < plot_cap:\n            tr_loss = dct[\"losses\"][\"train\"]\n            val_loss = dct[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr_loss) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Loss Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting loss for {dname}: {e}\")\n        plt.close()\n\n    # ---- 2) F1 curves ----\n    try:\n        if plotted < plot_cap:\n            tr_f1 = dct[\"metrics\"][\"train_f1\"]\n            val_f1 = dct[\"metrics\"][\"val_f1\"]\n            epochs = np.arange(1, len(tr_f1) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train F1\")\n            plt.plot(epochs, val_f1, label=\"Validation F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting F1 for {dname}: {e}\")\n        plt.close()\n\n    # ---- 3) confusion matrix ----\n    try:\n        if plotted < plot_cap and dct.get(\"gts_test\") and dct.get(\"preds_test\"):\n            gts = np.array(dct[\"gts_test\"])\n            preds = np.array(dct[\"preds_test\"])\n            print(f\"{dname} Test Macro-F1:\", macro_f1(gts, preds))\n            labels = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (Test Set)\")\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=7)\n            fname = os.path.join(working_dir, f\"{dname.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting CM for {dname}: {e}\")\n        plt.close()\n\n    # ---- 4) rule accuracy bar ----\n    try:\n        if (\n            plotted < plot_cap\n            and isinstance(dct[\"metrics\"].get(\"REA_dev\"), (int, float))\n            and isinstance(dct[\"metrics\"].get(\"REA_test\"), (int, float))\n        ):\n            rea_dev = dct[\"metrics\"][\"REA_dev\"]\n            rea_test = dct[\"metrics\"][\"REA_test\"]\n            plt.figure()\n            plt.bar([\"Dev\", \"Test\"], [rea_dev, rea_test], color=[\"skyblue\", \"salmon\"])\n            plt.ylim(0, 1)\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Rule Extraction Accuracy\\nLeft: Dev, Right: Test\")\n            fname = os.path.join(working_dir, f\"{dname.lower()}_rule_accuracy.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting REA for {dname}: {e}\")\n        plt.close()\n",
      "plot_analyses": [
        {
          "analysis": "The loss curves show that the training loss decreases steadily and stabilizes, indicating successful convergence of the model. However, the validation loss decreases initially but starts to increase after epoch 6, suggesting overfitting. This indicates that the model is learning to fit the training data well but struggles to generalize to unseen data. Regularization techniques or early stopping might help mitigate this issue.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_loss_curves.png"
        },
        {
          "analysis": "The Macro-F1 curves demonstrate that the model achieves high performance on the training data, with an F1 score approaching 1.0. The validation F1 score increases until around epoch 6, where it plateaus, supporting the observation of potential overfitting. The validation F1 stabilizes at a value close to the benchmark, indicating competitive performance but room for improvement in generalization.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_f1_curves.png"
        },
        {
          "analysis": "The confusion matrix for the test set reveals that the model performs better in predicting one class over the other. Specifically, it shows a higher number of true positives (416) compared to false negatives (86). However, the false positives (133) suggest that the model occasionally misclassifies sequences as belonging to this class. Balancing the class predictions and reducing false positives would improve overall performance.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_confusion_matrix.png"
        },
        {
          "analysis": "The rule extraction accuracy plot indicates that the model achieves moderate interpretability, with rule extraction accuracy around 50% on both the dev and test sets. This suggests that while the model can extract interpretable rules, there is significant room for improvement in making the rules more accurate and representative of the underlying data patterns.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_rule_accuracy.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_loss_curves.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_f1_curves.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_confusion_matrix.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/spr_bench_rule_accuracy.png"
      ],
      "vlm_feedback_summary": "The plots reveal that the model demonstrates strong performance on the training data but struggles with generalization, as evidenced by the divergence in loss and F1 score trends between training and validation. The confusion matrix highlights class imbalance issues, and the rule extraction accuracy suggests moderate interpretability but with potential for enhancement.",
      "exp_results_dir": "experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857",
      "exp_results_npy_files": [
        "experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/experiment_data.npy"
      ]
    },
    {
      "overall_plan": "The overall plan began with optimizing the Char-BiLSTM model through hyperparameter tuning, particularly focusing on the learning rate, and selecting models based on validation Macro-F1 scores. This involved comprehensive logging and serialization of metrics. The strategy was then enhanced by incorporating a soft attention layer to improve interpretability, allowing for the extraction of 'key-character rules' for each class and introducing the Rule-Extraction-Accuracy (REA) metric alongside Macro-F1. Building on this, the plan merges an interpretable bag-of-characters (BoC) classifier with a neural Attn-BiLSTM. The BoC branch, a single linear layer, provides explicit human-readable rules through its learned weights, while the BiLSTM branch offers contextual reasoning. Both branches are jointly optimized using the average of their logits for cross-entropy loss and an L1 penalty on BoC weights for sparsity. During training, validation loss and F1 scores are printed, metrics recorded, and post-training evaluations include extracting one-token rules, computing REA, and evaluating final Macro-F1 on the test split. All results and learned rules are stored for further analysis. This comprehensive strategy aims to develop effective models that are interpretable and provide insight into their decision-making processes. The current plan is a seed node, indicating no new additions or changes, maintaining the focus on these objectives.",
      "analysis": "",
      "metric": {
        "value": {
          "metric_names": [
            {
              "metric_name": "training loss",
              "lower_is_better": true,
              "description": "Measures the error during training. Lower values indicate better training performance.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.031,
                  "best_value": 0.031
                }
              ]
            },
            {
              "metric_name": "validation loss",
              "lower_is_better": true,
              "description": "Measures the error on the validation set. Lower values indicate better generalization.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.6118,
                  "best_value": 0.6118
                }
              ]
            },
            {
              "metric_name": "training F1 score",
              "lower_is_better": false,
              "description": "Measures the harmonic mean of precision and recall during training. Higher values are better.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.9935,
                  "best_value": 0.9935
                }
              ]
            },
            {
              "metric_name": "validation F1 score",
              "lower_is_better": false,
              "description": "Measures the harmonic mean of precision and recall on the validation set. Higher values are better.",
              "data": [
                {
                  "dataset_name": "SPR_BENCH",
                  "final_value": 0.7959,
                  "best_value": 0.7959
                }
              ]
            },
            {
              "metric_name": "rule extraction accuracy",
              "lower_is_better": false,
              "description": "Measures the accuracy of rule extraction for the model.",
              "data": [
                {
                  "dataset_name": "development",
                  "final_value": 0.542,
                  "best_value": 0.542
                },
                {
                  "dataset_name": "test",
                  "final_value": 0.585,
                  "best_value": 0.585
                }
              ]
            },
            {
              "metric_name": "hybrid model macro-F1 score",
              "lower_is_better": false,
              "description": "Measures the macro-averaged F1 score of the hybrid model on the test set. Higher values are better.",
              "data": [
                {
                  "dataset_name": "test",
                  "final_value": 0.795,
                  "best_value": 0.795
                }
              ]
            }
          ]
        },
        "maximize": null,
        "name": null,
        "description": null
      },
      "code": "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# --------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------- data loading --------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        d[s] = _load(f\"{s}.csv\")\n    return d\n\n\nspr_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr_root = spr_root if spr_root.exists() else pathlib.Path(\"SPR_BENCH/\")\ndsets = load_spr_bench(spr_root)\nprint({k: len(v) for k, v in dsets.items()})\n\n\ndef build_vocab(split):\n    charset = set()\n    for seq in split[\"sequence\"]:\n        charset.update(seq)\n    stoi = {c: i + 1 for i, c in enumerate(sorted(charset))}\n    stoi[\"<PAD>\"] = 0\n    return stoi\n\n\nvocab = build_vocab(dsets[\"train\"])\nitos = {i: s for s, i in vocab.items()}\nvocab_size = len(vocab)\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\n\n\n# ---------------------- dataset & loader ----------------------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = [self.vocab[c] for c in self.seqs[idx]]\n        return {\n            \"seq\": torch.tensor(seq, dtype=torch.long),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lens = torch.tensor([len(b[\"seq\"]) for b in batch])\n    maxlen = lens.max()\n    padded = torch.zeros(len(batch), maxlen, dtype=torch.long)\n    bags = torch.zeros(len(batch), vocab_size, dtype=torch.float32)\n    for i, b in enumerate(batch):\n        l = lens[i]\n        padded[i, :l] = b[\"seq\"]\n        bags[i].index_add_(0, b[\"seq\"], torch.ones(l))\n    labels = torch.stack([b[\"label\"] for b in batch])\n    return {\"input\": padded, \"lengths\": lens, \"bag\": bags, \"label\": labels}\n\n\nbs = 128\ntrain_dl = DataLoader(\n    SPRDataset(dsets[\"train\"], vocab), bs, shuffle=True, collate_fn=collate\n)\ndev_dl = DataLoader(\n    SPRDataset(dsets[\"dev\"], vocab), bs, shuffle=False, collate_fn=collate\n)\ntest_dl = DataLoader(\n    SPRDataset(dsets[\"test\"], vocab), bs, shuffle=False, collate_fn=collate\n)\n\n\n# ----------------------- model --------------------------------------\nclass AttnBiLSTM(nn.Module):\n    def __init__(self, vocab_sz, emb=64, hid=128):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_sz, emb, padding_idx=0)\n        self.lstm = nn.LSTM(emb, hid, bidirectional=True, batch_first=True)\n        self.attn_vec = nn.Parameter(torch.randn(hid * 2))\n        self.fc = nn.Linear(hid * 2, num_classes)\n\n    def forward(self, x, lens, need_attn=False):\n        e = self.emb(x)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            e, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        out, _ = self.lstm(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        attn = torch.tanh(out) @ self.attn_vec\n        mask = x != 0\n        attn[~mask] = -1e9\n        w = torch.softmax(attn, 1).unsqueeze(-1)\n        ctx = (out * w).sum(1)\n        logits = self.fc(ctx)\n        return (logits, w.squeeze(-1)) if need_attn else logits\n\n\nclass BagClassifier(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.lin = nn.Linear(vocab_sz, num_classes)\n\n    def forward(self, bag):\n        return self.lin(bag)\n\n\nclass HybridModel(nn.Module):\n    def __init__(self, vocab_sz):\n        super().__init__()\n        self.bilstm = AttnBiLSTM(vocab_sz)\n        self.bag = BagClassifier(vocab_sz)\n\n    def forward(self, inp, lens, bag):\n        log_lstm = self.bilstm(inp, lens)\n        log_bag = self.bag(bag)\n        return (log_lstm + log_bag) / 2, log_bag  # ensemble logits and raw BoC logits\n\n\nmodel = HybridModel(vocab_size).to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\nl1_lambda = 1e-4\n\n\n# ------------------- training & evaluation helpers ------------------\ndef epoch_pass(model, dl, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in dl:\n            batch = {\n                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n                for k, v in batch.items()\n            }\n            logits, logits_bag = model(batch[\"input\"], batch[\"lengths\"], batch[\"bag\"])\n            loss = nn.functional.cross_entropy(logits, batch[\"label\"])\n            # sparse penalty on bag weights\n            loss += l1_lambda * model.bag.lin.weight.abs().mean()\n            if train:\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n            total_loss += loss.item() * batch[\"label\"].size(0)\n            preds.extend(torch.argmax(logits, 1).cpu().tolist())\n            gts.extend(batch[\"label\"].cpu().tolist())\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------------- experiment tracking ----------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"REA_dev\": None, \"REA_test\": None},\n        \"rules\": {},\n        \"preds_test\": [],\n        \"gts_test\": [],\n    }\n}\n\nbest_f1 = 0\nbest_state = None\nepochs = 12\nfor ep in range(1, epochs + 1):\n    tr_loss, tr_f1, _, _ = epoch_pass(model, train_dl, True)\n    val_loss, val_f1, _, _ = epoch_pass(model, dev_dl, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_state = model.state_dict()\n    print(f\"Epoch {ep}: validation_loss = {val_loss:.4f}  val_f1={val_f1:.4f}\")\n\nmodel.load_state_dict(best_state)\n\n# -------------------- rule extraction -------------------------------\nwith torch.no_grad():\n    weights = model.bag.lin.weight.detach().cpu().numpy()  # (C,V)\nrules = {}\nfor c in range(num_classes):\n    tok = int(np.argmax(weights[c, 1:]) + 1)  # ignore PAD\n    rules[c] = tok\nexperiment_data[\"SPR_BENCH\"][\"rules\"] = {c: itos[tok] for c, tok in rules.items()}\nprint(\"Learned single-token rules:\", experiment_data[\"SPR_BENCH\"][\"rules\"])\n\n\ndef rule_predict(seq, rules):\n    for ch in seq:\n        vid = vocab.get(ch, 0)\n        for cls, tok in rules.items():\n            if vid == tok:\n                return cls\n    return None\n\n\ndef evaluate_rules(hf_split, rules, model):\n    correct = 0\n    total = len(hf_split)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for seq, label in zip(hf_split[\"sequence\"], hf_split[\"label\"]):\n            rp = rule_predict(seq, rules)\n            if rp is None:\n                # fallback to hybrid model\n                inp = torch.tensor([[vocab[c] for c in seq]], dtype=torch.long).to(\n                    device\n                )\n                lens = torch.tensor([inp.size(1)]).to(device)\n                bag = torch.zeros(1, vocab_size, device=device)\n                bag[0].index_add_(\n                    1, inp.squeeze(0), torch.ones(inp.size(1), device=device)\n                )\n                logit, _ = model(inp, lens, bag)\n                rp = int(torch.argmax(logit, 1).item())\n            preds.append(rp)\n            if rp == label:\n                correct += 1\n    return correct / total, preds\n\n\nREA_dev, _ = evaluate_rules(dsets[\"dev\"], rules, model)\nREA_test, preds_test = evaluate_rules(dsets[\"test\"], rules, model)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_dev\"] = REA_dev\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"REA_test\"] = REA_test\nprint(f\"Rule Extraction Accuracy (dev): {REA_dev:.4f}\")\nprint(f\"Rule Extraction Accuracy (test): {REA_test:.4f}\")\n\n# -------------------- final test F1 ---------------------------------\ntest_loss, test_f1, preds, gts = epoch_pass(model, test_dl, False)\nexperiment_data[\"SPR_BENCH\"][\"preds_test\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"gts_test\"] = gts\nprint(f\"Hybrid Model Test Macro-F1: {test_f1:.4f}\")\n\n# -------------------- save ------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n",
      "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- directories ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helpers ----------\ndef macro_f1(y_true, y_pred):\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    f1s = []\n    for lb in labels:\n        tp = np.sum((y_true == lb) & (y_pred == lb))\n        fp = np.sum((y_true != lb) & (y_pred == lb))\n        fn = np.sum((y_true == lb) & (y_pred != lb))\n        prec = tp / (tp + fp + 1e-12)\n        rec = tp / (tp + fn + 1e-12)\n        f1s.append(0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# ---------- plotting ----------\nplot_cap = 5  # maximum total figures\nplotted = 0\n\nfor dname, dct in experiment_data.items():\n    if plotted >= plot_cap:\n        break\n\n    # ---- 1) loss curves ----\n    try:\n        if plotted < plot_cap:\n            tr_loss = dct[\"losses\"][\"train\"]\n            val_loss = dct[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr_loss) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Loss Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting loss for {dname}: {e}\")\n        plt.close()\n\n    # ---- 2) F1 curves ----\n    try:\n        if plotted < plot_cap:\n            tr_f1 = dct[\"metrics\"][\"train_f1\"]\n            val_f1 = dct[\"metrics\"][\"val_f1\"]\n            epochs = np.arange(1, len(tr_f1) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train F1\")\n            plt.plot(epochs, val_f1, label=\"Validation F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dname}: Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_f1_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting F1 for {dname}: {e}\")\n        plt.close()\n\n    # ---- 3) confusion matrix ----\n    try:\n        if plotted < plot_cap and dct.get(\"gts_test\") and dct.get(\"preds_test\"):\n            gts = np.array(dct[\"gts_test\"])\n            preds = np.array(dct[\"preds_test\"])\n            print(f\"{dname} Test Macro-F1:\", macro_f1(gts, preds))\n            labels = np.unique(np.concatenate([gts, preds]))\n            cm = np.zeros((len(labels), len(labels)), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{dname}: Confusion Matrix (Test Set)\")\n            plt.xticks(labels)\n            plt.yticks(labels)\n            for i in range(len(labels)):\n                for j in range(len(labels)):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=7)\n            fname = os.path.join(working_dir, f\"{dname.lower()}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting CM for {dname}: {e}\")\n        plt.close()\n\n    # ---- 4) rule accuracy bar ----\n    try:\n        if (\n            plotted < plot_cap\n            and isinstance(dct[\"metrics\"].get(\"REA_dev\"), (int, float))\n            and isinstance(dct[\"metrics\"].get(\"REA_test\"), (int, float))\n        ):\n            rea_dev = dct[\"metrics\"][\"REA_dev\"]\n            rea_test = dct[\"metrics\"][\"REA_test\"]\n            plt.figure()\n            plt.bar([\"Dev\", \"Test\"], [rea_dev, rea_test], color=[\"skyblue\", \"salmon\"])\n            plt.ylim(0, 1)\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Rule Extraction Accuracy\\nLeft: Dev, Right: Test\")\n            fname = os.path.join(working_dir, f\"{dname.lower()}_rule_accuracy.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting REA for {dname}: {e}\")\n        plt.close()\n",
      "plot_analyses": [
        {
          "analysis": "The training loss decreases steadily and converges to a low value, indicating that the model learns effectively from the training data. However, the validation loss initially decreases but starts fluctuating and increasing slightly after a certain point, suggesting potential overfitting. This highlights the need for regularization techniques or early stopping to prevent overfitting and improve generalization.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_loss_curves.png"
        },
        {
          "analysis": "The macro-F1 score for the training set improves rapidly, reaching close to 1.0, which indicates excellent performance on the training data. The validation F1 score also improves but plateaus around 0.8, suggesting that the model achieves good but not perfect generalization. The gap between the training and validation F1 scores may indicate overfitting, necessitating further fine-tuning or architectural adjustments.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_f1_curves.png"
        },
        {
          "analysis": "The confusion matrix shows that the model performs reasonably well on the test set, with 392 true negatives and 403 true positives. However, there are 106 false positives and 99 false negatives. This indicates that while the model is effective, there is room for improvement, especially in reducing false positives and negatives to enhance overall accuracy and reliability.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_confusion_matrix.png"
        },
        {
          "analysis": "The rule extraction accuracy is higher on the test set compared to the development set. While this is a positive outcome, the overall accuracy values suggest that there is still significant room for improvement in the interpretability aspect of the model. Enhancing the rule-based layer or refining the rule extraction process could lead to better results.",
          "plot_path": "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_rule_accuracy.png"
        }
      ],
      "plot_paths": [
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_loss_curves.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_f1_curves.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_confusion_matrix.png",
        "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/spr_bench_rule_accuracy.png"
      ],
      "vlm_feedback_summary": "The plots provide valuable insights into the model's performance and interpretability. The training and validation loss curves reveal potential overfitting, while the macro-F1 curves show good but not perfect generalization. The confusion matrix highlights the model's strengths and weaknesses in classification, and the rule extraction accuracy underscores the need for further improvements in interpretability. Overall, the results are promising but indicate areas for refinement.",
      "exp_results_dir": "experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854",
      "exp_results_npy_files": [
        "experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/experiment_data.npy"
      ]
    }
  ],
  "aggregated results of nodes with different seeds": {
    "overall_plan": "The overall plan began with optimizing the Char-BiLSTM model through hyperparameter tuning, particularly focusing on the learning rate, and selecting models based on validation Macro-F1 scores. This involved comprehensive logging and serialization of metrics. The strategy was then enhanced by incorporating a soft attention layer to improve interpretability, allowing for the extraction of 'key-character rules' for each class and introducing the Rule-Extraction-Accuracy (REA) metric alongside Macro-F1. Building on this, the plan merged an interpretable bag-of-characters (BoC) classifier with a neural Attn-BiLSTM. The BoC branch, a single linear layer, provides explicit human-readable rules through its learned weights, while the BiLSTM branch offers contextual reasoning. Both branches are jointly optimized using the average of their logits for cross-entropy loss and an L1 penalty on BoC weights for sparsity. During training, validation loss and F1 scores are printed, metrics recorded, and post-training evaluations include extracting one-token rules, computing REA, and evaluating final Macro-F1 on the test split. All results and learned rules are stored for further analysis. This comprehensive strategy aims to develop effective models that are interpretable and provide insight into their decision-making processes. The current plan focuses on aggregating results from multiple seeds to ensure robustness and reliability, providing a stable estimate of model performance and validating the effectiveness of the strategies implemented.",
    "metric": {
      "value": null,
      "maximize": null,
      "name": null,
      "description": null
    },
    "code": "# plotting aggregation code",
    "plot_code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- directories ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- experiment paths ----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_42547dbfef184ebb923db3cc24284519_proc_3305854/experiment_data.npy\",\n    \"experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_aeef92afb53d4b2291b184d257cd1b7b_proc_3305855/experiment_data.npy\",\n    \"experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9deb35bbf816471d8d030c19bfbe48d2_proc_3305857/experiment_data.npy\",\n]\n\n# ---------- load data ----------\nall_experiment_data = []\nfor exp_path in experiment_data_path_list:\n    try:\n        abs_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), exp_path)\n        exp_dct = np.load(abs_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dct)\n    except Exception as e:\n        print(f\"Error loading {exp_path}: {e}\")\n\n# ---------- aggregate by dataset ----------\naggregated = {}  # {dataset : [run_dict, ...]}\nfor run in all_experiment_data:\n    for dname, dct in run.items():\n        aggregated.setdefault(dname, []).append(dct)\n\nplot_cap, plotted = 5, 0\n\n\n# ---------- helper ----------\ndef stack_and_trim(list_of_lists):\n    \"\"\"Stack ragged list of 1-D arrays by trimming to shortest length.\"\"\"\n    min_len = min(len(a) for a in list_of_lists)\n    arr = np.vstack([np.asarray(a[:min_len]) for a in list_of_lists])\n    return arr, min_len\n\n\ndef sem(x, axis=0):\n    n = x.shape[axis]\n    return np.std(x, axis=axis, ddof=1) / np.sqrt(n) if n > 1 else np.zeros(x.shape[1])\n\n\ndef macro_f1(y_true, y_pred):\n    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    f1s = []\n    for lb in labels:\n        tp = np.sum((y_true == lb) & (y_pred == lb))\n        fp = np.sum((y_true != lb) & (y_pred == lb))\n        fn = np.sum((y_true == lb) & (y_pred != lb))\n        prec = tp / (tp + fp + 1e-12)\n        rec = tp / (tp + fn + 1e-12)\n        f1s.append(0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# ---------- plotting aggregated results ----------\nfor dname, runs in aggregated.items():\n    if plotted >= plot_cap:\n        break\n    n_runs = len(runs)\n\n    # ========== Aggregated Loss Curves ==========\n    try:\n        tr_losses = [\n            run[\"losses\"][\"train\"]\n            for run in runs\n            if \"losses\" in run and \"train\" in run[\"losses\"]\n        ]\n        val_losses = [\n            run[\"losses\"][\"val\"]\n            for run in runs\n            if \"losses\" in run and \"val\" in run[\"losses\"]\n        ]\n        if tr_losses and val_losses:\n            tr_arr, L = stack_and_trim(tr_losses)\n            val_arr, _ = stack_and_trim(val_losses)\n            tr_mean, tr_sem = tr_arr.mean(0), sem(tr_arr)\n            val_mean, val_sem = val_arr.mean(0), sem(val_arr)\n\n            epochs = np.arange(1, L + 1)\n            plt.figure()\n            plt.plot(epochs, tr_mean, label=\"Train Mean\", color=\"steelblue\")\n            plt.fill_between(\n                epochs,\n                tr_mean - tr_sem,\n                tr_mean + tr_sem,\n                alpha=0.3,\n                color=\"steelblue\",\n                label=\"Train \u00b1SEM\",\n            )\n            plt.plot(epochs, val_mean, label=\"Validation Mean\", color=\"darkorange\")\n            plt.fill_between(\n                epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                alpha=0.3,\n                color=\"darkorange\",\n                label=\"Val \u00b1SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(\n                f\"{dname}: Aggregated Loss Curves\\nMean with Shaded Standard Error over {n_runs} runs\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_aggregated_loss.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting aggregated loss for {dname}: {e}\")\n        plt.close()\n\n    # ========== Aggregated F1 Curves ==========\n    try:\n        tr_f1s = [\n            run[\"metrics\"][\"train_f1\"]\n            for run in runs\n            if \"metrics\" in run and \"train_f1\" in run[\"metrics\"]\n        ]\n        val_f1s = [\n            run[\"metrics\"][\"val_f1\"]\n            for run in runs\n            if \"metrics\" in run and \"val_f1\" in run[\"metrics\"]\n        ]\n        if tr_f1s and val_f1s and plotted < plot_cap:\n            tr_arr, Lf = stack_and_trim(tr_f1s)\n            val_arr, _ = stack_and_trim(val_f1s)\n            tr_mean, tr_sem = tr_arr.mean(0), sem(tr_arr)\n            val_mean, val_sem = val_arr.mean(0), sem(val_arr)\n\n            epochs = np.arange(1, Lf + 1)\n            plt.figure()\n            plt.plot(epochs, tr_mean, label=\"Train Mean\", color=\"seagreen\")\n            plt.fill_between(\n                epochs,\n                tr_mean - tr_sem,\n                tr_mean + tr_sem,\n                alpha=0.3,\n                color=\"seagreen\",\n                label=\"Train \u00b1SEM\",\n            )\n            plt.plot(epochs, val_mean, label=\"Validation Mean\", color=\"tomato\")\n            plt.fill_between(\n                epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                alpha=0.3,\n                color=\"tomato\",\n                label=\"Val \u00b1SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(\n                f\"{dname}: Aggregated Macro-F1 Curves\\nMean with Shaded Standard Error over {n_runs} runs\"\n            )\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{dname.lower()}_aggregated_f1.png\")\n            plt.savefig(fname)\n            plt.close()\n            plotted += 1\n    except Exception as e:\n        print(f\"Error plotting aggregated F1 for {dname}: {e}\")\n        plt.close()\n\n    # ========== Aggregated Rule-Extraction Accuracy ==========\n    try:\n        if plotted < plot_cap:\n            rea_dev_vals = [\n                run[\"metrics\"].get(\"REA_dev\") for run in runs if \"metrics\" in run\n            ]\n            rea_test_vals = [\n                run[\"metrics\"].get(\"REA_test\") for run in runs if \"metrics\" in run\n            ]\n            if all(isinstance(v, (int, float)) for v in rea_dev_vals) and all(\n                isinstance(v, (int, float)) for v in rea_test_vals\n            ):\n                dev_mean, dev_sem = np.mean(rea_dev_vals), (\n                    np.std(rea_dev_vals, ddof=1) / np.sqrt(n_runs)\n                    if n_runs > 1\n                    else 0.0\n                )\n                test_mean, test_sem = np.mean(rea_test_vals), (\n                    np.std(rea_test_vals, ddof=1) / np.sqrt(n_runs)\n                    if n_runs > 1\n                    else 0.0\n                )\n\n                plt.figure()\n                plt.bar(\n                    [\"Dev\", \"Test\"],\n                    [dev_mean, test_mean],\n                    yerr=[dev_sem, test_sem],\n                    color=[\"skyblue\", \"salmon\"],\n                    capsize=5,\n                )\n                plt.ylim(0, 1)\n                plt.ylabel(\"Accuracy\")\n                plt.title(\n                    f\"{dname}: Aggregated Rule-Extraction Accuracy\\nBars show mean \u00b1SEM over {n_runs} runs\"\n                )\n                fname = os.path.join(working_dir, f\"{dname.lower()}_aggregated_rea.png\")\n                plt.savefig(fname)\n                plt.close()\n                plotted += 1\n    except Exception as e:\n        print(f\"Error plotting aggregated REA for {dname}: {e}\")\n        plt.close()\n\n    # ========== Aggregated Confusion Matrix ==========\n    try:\n        if plotted < plot_cap:\n            gts_all, preds_all = [], []\n            for run in runs:\n                if (\n                    run.get(\"gts_test\") is not None\n                    and run.get(\"preds_test\") is not None\n                ):\n                    gts_all.append(np.asarray(run[\"gts_test\"]))\n                    preds_all.append(np.asarray(run[\"preds_test\"]))\n\n            if gts_all and preds_all:\n                gts_concat = np.concatenate(gts_all)\n                preds_concat = np.concatenate(preds_all)\n                labels = np.unique(np.concatenate([gts_concat, preds_concat]))\n                cm = np.zeros((len(labels), len(labels)), dtype=int)\n                for t, p in zip(gts_concat, preds_concat):\n                    cm[t, p] += 1\n\n                # print aggregated macro-F1\n                mean_f1 = macro_f1(gts_concat, preds_concat)\n                print(\n                    f\"{dname} Aggregated Test Macro-F1 over {n_runs} runs: {mean_f1:.4f}\"\n                )\n\n                plt.figure(figsize=(6, 5))\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im)\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"True\")\n                plt.title(\n                    f\"{dname}: Aggregated Confusion Matrix (Test)\\nCounts summed over {n_runs} runs\"\n                )\n                plt.xticks(labels)\n                plt.yticks(labels)\n                for i in range(len(labels)):\n                    for j in range(len(labels)):\n                        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=7)\n                fname = os.path.join(\n                    working_dir, f\"{dname.lower()}_aggregated_confusion_matrix.png\"\n                )\n                plt.savefig(fname)\n                plt.close()\n                plotted += 1\n    except Exception as e:\n        print(f\"Error plotting aggregated confusion matrix for {dname}: {e}\")\n        plt.close()\n",
    "plot_analyses": [],
    "plot_paths": [
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ddf7945c0d5f44f09a05921edc990192/spr_bench_aggregated_loss.png",
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ddf7945c0d5f44f09a05921edc990192/spr_bench_aggregated_f1.png",
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ddf7945c0d5f44f09a05921edc990192/spr_bench_aggregated_rea.png",
      "experiments/2025-08-17_17-27-17_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_ddf7945c0d5f44f09a05921edc990192/spr_bench_aggregated_confusion_matrix.png"
    ],
    "vlm_feedback_summary": [],
    "exp_results_dir": "experiment_results/seed_aggregation_ddf7945c0d5f44f09a05921edc990192",
    "exp_results_npy_files": []
  }
}