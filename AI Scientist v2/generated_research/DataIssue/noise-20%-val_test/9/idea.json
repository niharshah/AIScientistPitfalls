{
    "Name": "interpretable_neural_rule_learning",
    "Title": "Interpretable Neural Rule Learning for Synthetic PolyRule Reasoning",
    "Short Hypothesis": "Can we design an interpretable neural network model that can learn and explicitly represent the underlying poly-factor rules governing the Synthetic PolyRule Reasoning (SPR) task, enhancing both performance and interpretability?",
    "Related Work": "1. Neural Rule Learning: Neural Logic Machines (Dong et al., 2018) and RL-Net (Dierckx et al., 2023) focus on learning logical rules but often lack interpretability and explicit rule representation. 2. Symbolic Reasoning Models: Deep Concept Reasoner (Barbiero et al., 2023) builds syntactic rule structures but still relies on high-dimensional concept embeddings. 3. Interpretable AI: Existing methods like LIME and SHAP offer post-hoc explanations but do not inherently learn interpretable rules. Our proposal aims to develop a neural network model that inherently learns and represents poly-factor rules in an interpretable manner, addressing the performance-interpretability trade-off.",
    "Abstract": "The Synthetic PolyRule Reasoning (SPR) task involves classifying symbolic sequences based on latent poly-factor rules. Current approaches in neural rule learning and symbolic reasoning either lack interpretability or are domain-specific. This proposal aims to develop an interpretable neural network model that learns and explicitly represents the underlying poly-factor rules governing the SPR task. By integrating rule-based learning with neural networks, we aim to create a model that not only achieves high classification accuracy but also provides interpretable rule representations. Our approach will be evaluated on the SPR_BENCH benchmark from HuggingFace, aiming to surpass the state-of-the-art accuracy of 80.0% while providing clear rule explanations for each classification decision.",
    "Experiments": [
        {
            "Description": "Model Design",
            "Details": "Develop a neural network model that incorporates a rule-based layer designed to learn and represent poly-factor rules. This layer will output explicit rules in human-readable format."
        },
        {
            "Description": "Training and Evaluation",
            "Details": "Train the model on the Train split of the SPR_BENCH benchmark. Tune the model on the Dev split. Evaluate final accuracy on the Test split and compare against the SOTA accuracy of 80.0%."
        },
        {
            "Description": "Interpretability Analysis",
            "Details": "Extract and present the learned rules for a subset of sequences. Conduct user studies to evaluate the interpretability of the rules."
        },
        {
            "Description": "Ablation Studies",
            "Details": "Compare performance with and without the rule-based layer. Evaluate the impact of different rule complexities on model performance."
        }
    ],
    "Risk Factors and Limitations": [
        "Complexity of Rule Learning: The model might struggle to learn highly complex rules, leading to reduced performance.",
        "Interpretability Trade-offs: Balancing performance and interpretability may be challenging, potentially leading to trade-offs.",
        "Generalization: The model's ability to generalize to unseen rules and sequences may be limited."
    ],
    "Code": "\"\"\"\nSPR.py\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nUtility to load the SPR_BENCH benchmark datasets\nUsing HuggingFace\u2019s `datasets` library.\n\nDirectory layout expected\nSPR_BENCH/\n \u251c\u2500 train.csv   (20000 rows)\n \u251c\u2500 dev.csv     (5000 rows)\n \u2514\u2500 test.csv    (10000 rows)\n\nEach CSV has header:  id,sequence,label\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n$ pip install datasets   # once\n\"\"\"\nimport pathlib\nfrom typing import Dict\n\nfrom datasets import load_dataset, DatasetDict                                         # <- no pandas import\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"\n    Return a DatasetDict {'train':\u2026, 'dev':\u2026, 'test':\u2026} for one SPR ID folder.\n    \"\"\"\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",           # treat csv as a single split\n            cache_dir=\".cache_dsets\" # optional; keeps HF cache tidy\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"]   = _load(\"dev.csv\")\n    dset[\"test\"]  = _load(\"test.csv\")\n    return dset\n\n\ndef main():\n\n    ## Absolute path of the datasets\n    DATA_PATH = pathlib.Path('/home/zxl240011/AI-Scientist-v2/SPR_BENCH/')\n    spr_bench = load_spr_bench(DATA_PATH)\n\n    print(\"Benchmarks split:\", spr_bench.keys())\n\n    # Demo: show first example from SPR_BENCH\u2011train\n    ex = spr_bench[\"train\"][0]\n    print(\"\\nExample row:\")\n    print(ex)          \n\n\nif __name__ == \"__main__\":\n    main()\n"
}