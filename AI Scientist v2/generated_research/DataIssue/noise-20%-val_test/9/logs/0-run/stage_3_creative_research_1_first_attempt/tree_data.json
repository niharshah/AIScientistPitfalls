{"edges": [[0, 1], [0, 2], [0, 4], [0, 3], [1, 7], [3, 8], [3, 5], [3, 6], [7, 10], [7, 11], [7, 12], [7, 9]], "layout": [[0.6666666666666666, 0.0], [0.3333333333333333, 0.33333333333333337], [0.5555555555555556, 0.33333333333333337], [0.7777777777777778, 0.33333333333333337], [1.0, 0.33333333333333337], [0.5555555555555556, 0.6666666666666667], [0.7777777777777778, 0.6666666666666667], [0.3333333333333333, 0.6666666666666667], [1.0, 0.6666666666666667], [0.0, 1.0], [0.2222222222222222, 1.0], [0.4444444444444444, 1.0], [0.6666666666666666, 1.0]], "plan": ["The previous script halted during training because the batch dictionary was\nbuilt incorrectly and no optimization step was performed. I rewrote the training\nloop to correctly unpack TensorDataset batches, move tensors to the selected\ndevice, perform forward / backward passes, and update the optimizer. I also\nnormalised bag-of-character inputs by sequence length, added validation\nmonitoring, stored metrics (including a simple Rule-Based Accuracy that uses the\nmost influential character per class as an extracted \u201crule\u201d), and persisted all\ncollected data to the working directory. The code now runs end-to-end, printing\nepoch-level losses and accuracies, and saving the experiment data for later\nanalysis.", "We enhance the baseline by coupling an interpretable linear \u201crule\u201d head (bag-of-\ncharacter with L1 regularisation) with a small CNN text encoder.   The linear\nhead supplies explicit per-class character weights that can be read as rules,\nwhile the CNN captures higher-order poly-factor interactions, the two logits\nbeing summed.   Rule Fidelity Score is computed as the proportion of instances\nwhere the linear rules\u2019 prediction agrees with the hybrid model\u2019s final\nprediction.   Data are represented both as (i) length-normalised bag-of-\ncharacter vectors and (ii) padded index sequences for the CNN.   All tensors and\nthe model are moved to GPU when available, metrics are tracked every epoch and\npersisted.   The code below trains the hybrid model on SPR_BENCH, monitors\nvalidation accuracy, loss and Rule Fidelity, then evaluates on the test split\nand saves everything to ./working.", "We switch to a sparsity\u2013promoting linear model whose weights are driven toward\none-hot \u201crule vectors\u201d by a strong L1 penalty.  Each class therefore learns to\nrely on only a handful of characters, making post-hoc rule extraction trivial:\nfor every class we keep the top-K characters with largest (positive) weights.\nTraining still uses a standard cross-entropy objective, but we add \u03bb\u2006\u2211\u2006|W| for\ninterpretability.  After every epoch we compute (i) normal validation accuracy,\n(ii) Rule Fidelity \u2013 the agreement between the model\u2019s own predictions and the\nlightweight rule engine that only looks at the extracted characters.  Because\nthe main model is itself sparse, fidelity should climb toward 1.0 while accuracy\nstays competitive with the previous baseline.  The code below loads SPR_BENCH,\nbuilds the bag-of-chars representation, trains the sparse model, tracks all\nmetrics, saves them to ./working, and prints final test scores together with\nRule Fidelity.  The whole script is self-contained, honours GPU/CPU handling,\nand finishes within minutes.", "We replace the bag-of-chars linear baseline with a small \u201cmax-pool-over-tokens\u201d\nnetwork.   Each character has a learnable embedding; a class-projection turns\nevery token into class-specific logits.   A masked max-pool across the sequence\nimplements an OR-like aggregation that naturally models \u201ca rule is satisfied if\nany token fires\u201d, capturing poly-factor rules while keeping the model fully\ninterpretable: the contribution of every character to every class is simply\nproj(embedding[char]).   Human-readable rules are extracted by taking, for every\nclass, the top-K characters with the highest learned logit; inference with only\nthese rules gives a symbolic classifier.   We train the network with cross-\nentropy, report accuracy, and compute the Rule Fidelity Score \u2013 the fraction of\ninstances where the symbolic rules agree with the neural model.   The code below\nloads SPR_BENCH, builds padded DataLoaders, trains for a few epochs, prints per-\nepoch metrics, evaluates on test, saves everything in ./working, and runs on GPU\nif available.", "We broaden interpretability and capacity by extending the earlier bag-of-\ncharacters model to a sparse bag-of-ngrams (1\u20133-gram) linear network.   The\nn-gram layer lets the classifier capture true \u201cpoly-factor rules\u201d while still\nexposing explicit, human-readable weights.   We (1) scan training data to keep\nonly the N most frequent uni/bi/tri-grams, (2) vectorise sequences into\nfrequency vectors, (3) train a single linear layer with light L1 regularisation\nfor sparsity, and (4) extract, per class, the top-K weighted n-grams as rules.\nRule Fidelity Score is then the share of instances whose label obtained from\nthese symbolic rules matches the model\u2019s own prediction.   The code below loads\nSPR_BENCH, builds the n-gram vocabulary, trains for a few epochs, prints\nvalidation loss/accuracy plus Rule Fidelity each epoch, evaluates on the test\nsplit, and finally saves all metrics for later analysis.   Everything respects\nthe GPU/CPU handling, metric tracking, and data-persistence requirements.", "The poor accuracy came from conflating the real character that happened to get\nindex 0 with the padding symbol.   Because index 0 tokens were simultaneously\nvalid characters and \u201cpads\u201d, the model learned spurious weights and could never\nexceed chance-level performance even though the rule extractor always matched\nthe network (Rule Fidelity = 1.0).   The fix is to introduce an explicit <PAD>\ntoken at index 0, shift every real character index by +1, and tell\n`nn.Embedding` to keep the padding row frozen via `padding_idx=0`.   Nothing\nelse in the architecture has to change; once padding is separated, the same max-\npool classifier can train properly.   The code below implements this correction,\nre-trains, evaluates accuracy, Rule Fidelity, and saves everything to\n`./working/experiment_data.npy`.", "The original model mapped every character independently to class logits and then\ntook a max over time. Because the padding index overlapped with a real character\nand no position-dependent features were learned, the network could only memorise\nsingle characters, converging to chance accuracy.   We fix this by (1)\nintroducing an explicit PAD token so that padding never collides with real\nsymbols, and (2) replacing the single-character projection with a small CNN\n(kernels 1-3) whose global-max-pooled features can capture character\nconjunctions.   For interpretability we still derive simple character-level\nrules from the kernel-size-1 filters and compute rule fidelity against the full\nmodel.   The rest of the training / logging pipeline is kept intact and now\nfollows the mandatory GPU-handling and data-saving guidelines.", "We introduce a gated-hybrid architecture that learns two complementary experts:\na sparse linear rule head operating on character-frequency bags and a deep CNN\nexpert over token embeddings. A per-sample gate (sigmoid of the bag features)\nblends both sets of logits, allowing the network to trust explicit rules when\nsufficient and defer to the neural expert otherwise. L1 regularization shrinks\nrule weights for interpretability, while the gate regularizer encourages\ndecisive (near-binary) gating. This design typically raises accuracy and\nmaintains high Rule-Fidelity because the rule head remains an active component\nin the final prediction path. The script below loads SPR_BENCH, trains the model\nwith metric tracking, prints epoch-wise validation losses, and saves all\nexperiment data for later analysis.", "The constant 1.00 \u201cRule Fidelity\u201d arose because the auxiliary rule predictor was\nusing the *exact* projection weights of the neural model, so its outputs were\nmathematically identical to the model\u2019s own predictions.   The fix is to let the\nrule predictor rely only on a *human-readable* rule set: for every class keep\nthe top-K characters judged most indicative by the model, then base rule\npredictions solely on the presence of these characters in a sequence.   We\ntherefore (1) extract the top-K characters per class after each epoch, (2)\nrewrite `rule_predict` to use nothing except these characters, and (3) track\nrule fidelity with this truly simplified rule set. The rest of the training\npipeline is unchanged and still satisfies all device-handling, metric-logging\nand saving requirements.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time\nimport pathlib\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- Device ---------------- #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- Hyper-params ---------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nLR = 1e-2\nEPOCHS = 10\nRULE_TOP_K = 1  # characters per class that constitute the symbolic \u201crule\u201d\n\n\n# ---------------- Dataset loading ---------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr_bench = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr_bench.keys())\n\n# ---------------- Vocabulary ---------------- #\nall_chars = set()\nfor seq in spr_bench[\"train\"][\"sequence\"]:\n    all_chars.update(seq)\nchar2idx = {c: i for i, c in enumerate(sorted(all_chars))}\nidx2char = {i: c for c, i in char2idx.items()}\nvocab_size = len(char2idx)\nprint(f\"Vocab size = {vocab_size}\")\n\n\ndef seq_to_vec(seq: str) -> np.ndarray:\n    \"\"\"Bag-of-chars (frequency).\"\"\"\n    vec = np.zeros(vocab_size, dtype=np.float32)\n    for ch in seq:\n        vec[char2idx[ch]] += 1.0\n    if len(seq) > 0:  # normalise\n        vec /= len(seq)\n    return vec\n\n\ndef prepare_split(split):\n    X = np.stack([seq_to_vec(s) for s in split[\"sequence\"]])\n    y = np.array(split[\"label\"], dtype=np.int64)\n    return torch.from_numpy(X), torch.from_numpy(y)\n\n\nX_train, y_train = prepare_split(spr_bench[\"train\"])\nX_dev, y_dev = prepare_split(spr_bench[\"dev\"])\nX_test, y_test = prepare_split(spr_bench[\"test\"])\n\nnum_classes = int(max(y_train.max(), y_dev.max(), y_test.max()) + 1)\nprint(f\"Number of classes: {num_classes}\")\n\n# ---------------- DataLoaders ---------------- #\ntrain_loader = DataLoader(\n    TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = DataLoader(TensorDataset(X_dev, y_dev), batch_size=VAL_BATCH)\ntest_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=VAL_BATCH)\n\n\n# ---------------- Model ---------------- #\nclass CharBagLinear(nn.Module):\n    def __init__(self, in_dim: int, num_cls: int):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, num_cls)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = CharBagLinear(vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# ---------------- Experiment store ---------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"RBA\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ---------------- Helper: evaluation ---------------- #\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            preds = logits.argmax(1)\n            total += yb.size(0)\n            correct += (preds == yb).sum().item()\n            loss_sum += loss.item() * yb.size(0)\n    return correct / total, loss_sum / total\n\n\n# ---------------- Helper: Rule-Based Accuracy (RBA) ---------------- #\ndef compute_rule_accuracy(loader):\n    \"\"\"\n    Build a trivial symbolic rule set: for each class, pick the top-K\n    characters with highest weight. Predict by counting occurrences of those\n    characters and picking the class with max count.\n    \"\"\"\n    with torch.no_grad():\n        W = model.linear.weight.detach().cpu().numpy()  # [C, V]\n    # top K indices per class\n    top_idx = np.argsort(W, axis=1)[:, -RULE_TOP_K:]  # [C, K]\n\n    total, correct = 0, 0\n    for xb, yb in loader:\n        seq_vectors = xb.numpy()  # frequencies\n        # revert to counts to avoid floating\n        counts = (seq_vectors * 1000).astype(int)  # arbitrary scaling\n        preds = []\n        for count_vec in counts:\n            votes = []\n            for cls in range(num_classes):\n                votes.append(count_vec[top_idx[cls]].sum())\n            preds.append(int(np.argmax(votes)))\n        preds = torch.tensor(preds)\n        correct += (preds == yb).sum().item()\n        total += yb.size(0)\n    return correct / total\n\n\n# ---------------- Training loop ---------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss, running_correct, seen = 0.0, 0, 0\n    start_t = time.time()\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        logits = model(xb)\n        loss = criterion(logits, yb)\n        loss.backward()\n        optimizer.step()\n\n        preds = logits.argmax(1)\n        running_loss += loss.item() * yb.size(0)\n        running_correct += (preds == yb).sum().item()\n        seen += yb.size(0)\n\n    train_acc = running_correct / seen\n    train_loss = running_loss / seen\n    val_acc, val_loss = evaluate(val_loader)\n    rba = compute_rule_accuracy(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"RBA\"].append(rba)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | \"\n        f\"RBA={rba:.3f} | \"\n        f\"epoch_time={time.time()-start_t:.1f}s\"\n    )\n\n# ---------------- Final test evaluation ---------------- #\ntest_acc, test_loss = evaluate(test_loader)\nrba_test = compute_rule_accuracy(test_loader)\nprint(f\"\\nTest set: loss={test_loss:.4f}, acc={test_acc:.3f}, RBA={rba_test:.3f}\")\n\n# Store final predictions for interpretability\nmodel.eval()\nall_preds, all_gts = [], []\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb = xb.to(device)\n        logits = model(xb)\n        preds = logits.argmax(1).cpu()\n        all_preds.append(preds)\n        all_gts.append(yb)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(all_preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(all_gts).numpy()\n\n# ---------------- Save everything ---------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nExperiment data saved to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- mandatory working dir ---------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- device ---------- #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- hyper-parameters ---------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 8\nLR = 3e-3\nWEIGHT_DECAY = 1e-4  # slight L2\nL1_LAMBDA = 1e-3  # sparsity for rule head\nEMB_DIM = 32\nCONV_CH = 64\nKERNELS = [3, 4, 5]\nPAD_IDX = 0\nMAX_LEN = 128  # truncate / pad sequences\nRULE_TOP_K = 1  # shown but not needed now\n\n\n# ---------- load SPR_BENCH ---------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", spr.keys())\n\n# ---------- vocabulary ---------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # +1 to reserve 0\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1  # inc PAD\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = (\n    SPRDataset(spr[\"train\"]),\n    SPRDataset(spr[\"dev\"]),\n    SPRDataset(spr[\"test\"]),\n)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLS = int(max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1)\nprint(\"Vocab size=\", VOCAB - 1, \"Num classes=\", NUM_CLS)\n\n\n# ---------- model ---------- #\nclass HybridRuleCNN(nn.Module):\n    def __init__(self, vocab, num_cls):\n        super().__init__()\n        self.rule = nn.Linear(vocab - 1, num_cls)  # interpretable head\n        self.emb = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.fc = nn.Linear(CONV_CH * len(KERNELS), num_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule(bag)\n        x = self.emb(seq).transpose(1, 2)  # B,E,L\n        feats = []\n        for conv in self.convs:\n            c = torch.relu(conv(x))\n            feats.append(torch.amax(c, dim=2))  # global max-pool\n        cnn_feat = torch.cat(feats, 1)\n        cnn_logits = self.fc(cnn_feat)\n        return rule_logits + cnn_logits, rule_logits  # combined, rule-only\n\n\nmodel = HybridRuleCNN(VOCAB, NUM_CLS).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# ---------- metrics store ---------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ---------- helpers ---------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            tot += y.size(0)\n            loss_sum += loss.item() * y.size(0)\n    acc = corr / tot\n    rfs = fid_match / fid_tot\n    loss = loss_sum / tot\n    return acc, loss, rfs\n\n\n# ---------- training ---------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits = model(bag, seq)\n        loss = criterion(logits, y) + L1_LAMBDA * model.rule.weight.abs().mean()\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        seen += y.size(0)\n    train_loss, train_acc = run_loss / seen, run_corr / seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# ---------- final test ---------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_rfs:.3f}\")\n\n# store predictions & gts\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# ---------- save ---------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"All experiment data saved.\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------------------------------------------------------------- #\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------------------------------------------------------------- #\n# Hyper-parameters\nBATCH = 256\nVAL_BATCH = 512\nEPOCHS = 12\nLR = 1e-2\nL1_LAMBDA = 1e-3  # strength of sparsity penalty\nTOP_K_RULE = 1  # number of chars per class kept in rule\n\n\n# --------------------------------------------------------------------------- #\n# Load SPR_BENCH\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nds = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in ds.items()})\n\n# --------------------------------------------------------------------------- #\n# Char vocabulary + bag-of-chars vectors\nvocab = sorted({ch for seq in ds[\"train\"][\"sequence\"] for ch in seq})\nchar2idx = {c: i for i, c in enumerate(vocab)}\nV = len(vocab)\n\n\ndef seq2vec(s: str) -> np.ndarray:\n    v = np.zeros(V, np.float32)\n    for ch in s:\n        v[char2idx[ch]] += 1\n    if len(s):\n        v /= len(s)\n    return v\n\n\ndef split_to_tensors(split):\n    X = np.stack([seq2vec(s) for s in split[\"sequence\"]])\n    y = np.array(split[\"label\"], np.int64)\n    return torch.from_numpy(X), torch.from_numpy(y)\n\n\nXtr, ytr = split_to_tensors(ds[\"train\"])\nXdv, ydv = split_to_tensors(ds[\"dev\"])\nXte, yte = split_to_tensors(ds[\"test\"])\nC = int(max(ytr.max(), ydv.max(), yte.max()) + 1)\n\ntrain_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=BATCH, shuffle=True)\nval_loader = DataLoader(TensorDataset(Xdv, ydv), batch_size=VAL_BATCH)\ntest_loader = DataLoader(TensorDataset(Xte, yte), batch_size=VAL_BATCH)\n\n\n# --------------------------------------------------------------------------- #\n# Sparse linear classifier\nclass SparseLinear(nn.Module):\n    def __init__(self, in_dim, num_cls):\n        super().__init__()\n        self.W = nn.Linear(in_dim, num_cls, bias=True)\n\n    def forward(self, x):\n        return self.W(x)\n\n\nmodel = SparseLinear(V, C).to(device)\ncriterion = nn.CrossEntropyLoss()\nopt = torch.optim.Adam(model.parameters(), lr=LR)\n\n# --------------------------------------------------------------------------- #\n# Metrics container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"rule_fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n    }\n}\n\n\n# --------------------------------------------------------------------------- #\n# Helper: evaluation + rule fidelity ---------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, correct, loss_sum = 0, 0, 0.0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            pred = logits.argmax(1)\n            tot += yb.size(0)\n            correct += (pred == yb).sum().item()\n            loss_sum += loss.item() * yb.size(0)\n    return correct / tot, loss_sum / tot\n\n\ndef extract_rules():  # returns list[List[int]] indices per class\n    with torch.no_grad():\n        W = model.W.weight.detach().cpu().numpy()  # [C,V]\n    topk = np.argsort(W, axis=1)[:, -TOP_K_RULE:]\n    return topk\n\n\ndef rule_predict(x_batch, rules):  # x_batch: [B,V] numpy counts\n    votes = np.zeros((x_batch.shape[0], C), np.float32)\n    for cls in range(C):\n        votes[:, cls] = x_batch[:, rules[cls]].sum(axis=1)\n    return votes.argmax(1)\n\n\ndef compute_rule_fidelity(loader):\n    rules = extract_rules()\n    model.eval()\n    agree, total = 0, 0\n    with torch.no_grad():\n        for xb, _ in loader:\n            logits = model(xb.to(device))\n            model_preds = logits.argmax(1).cpu().numpy()\n            counts = (xb.numpy() * 1000).astype(int)  # rescale to counts\n            rule_preds = rule_predict(counts, rules)\n            agree += (rule_preds == model_preds).sum()\n            total += xb.shape[0]\n    return agree / total\n\n\n# --------------------------------------------------------------------------- #\n# Training loop ------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    start = time.time()\n    seen, correct, loss_accum = 0, 0, 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        opt.zero_grad()\n        logits = model(xb)\n        loss_ce = criterion(logits, yb)\n        l1_pen = sum(p.abs().sum() for p in model.parameters())\n        loss = loss_ce + L1_LAMBDA * l1_pen\n        loss.backward()\n        opt.step()\n        pred = logits.argmax(1)\n        seen += yb.size(0)\n        correct += (pred == yb).sum().item()\n        loss_accum += loss_ce.item() * yb.size(0)  # record pure CE loss\n    train_acc = correct / seen\n    train_loss = loss_accum / seen\n    val_acc, val_loss = evaluate(val_loader)\n    fidelity = compute_rule_fidelity(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"rule_fidelity\"].append(fidelity)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}, \"\n        f\"fidelity={fidelity:.3f}, val_loss={val_loss:.4f}, \"\n        f\"time={time.time()-start:.1f}s\"\n    )\n\n# --------------------------------------------------------------------------- #\n# Final evaluation ---------------------------------------------------------- #\ntest_acc, test_loss = evaluate(test_loader)\ntest_fidelity = compute_rule_fidelity(test_loader)\nprint(\n    f\"\\nTEST  \u2013 acc: {test_acc:.3f}, loss: {test_loss:.4f}, \"\n    f\"Rule Fidelity: {test_fidelity:.3f}\"\n)\n\n# --------------------------------------------------------------------------- #\n# Save everything ----------------------------------------------------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved metrics -> {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time\nimport pathlib\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- Device ---------------- #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- Hyper-parameters ---------------- #\nEMBED_DIM = 32\nBATCH_SIZE = 256\nVAL_BATCH = 512\nLR = 3e-3\nEPOCHS = 12\nRULE_TOP_K = 3  # characters per class kept for human rule\nSEED = 2024\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n\n# ---------------- Dataset loading ---------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_l(\"train.csv\"), dev=_l(\"dev.csv\"), test=_l(\"test.csv\"))\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\ndset = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", list(dset.keys()))\n\n# ---------------- Vocabulary ---------------- #\nall_chars = set(ch for seq in dset[\"train\"][\"sequence\"] for ch in seq)\nchar2idx = {c: i for i, c in enumerate(sorted(all_chars))}\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB_SIZE = len(char2idx)\nprint(\"Vocab size:\", VOCAB_SIZE)\n\n\n# ---------------- Utilities ---------------- #\ndef encode(seq: str) -> List[int]:\n    return [char2idx[c] for c in seq]\n\n\ndef collate(batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    # batch = list of dicts from HF dataset {'id':..,'sequence':..,'label':..}\n    seqs = [encode(b[\"sequence\"]) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n    mask = torch.zeros(len(seqs), max_len, dtype=torch.bool)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        mask[i, : len(s)] = True\n    return padded.to(device), mask.to(device), labels.to(device)\n\n\ntrain_loader = DataLoader(\n    dset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(dset[\"dev\"], batch_size=VAL_BATCH, collate_fn=collate)\ntest_loader = DataLoader(dset[\"test\"], batch_size=VAL_BATCH, collate_fn=collate)\n\nNUM_CLASSES = int(max(dset[\"train\"][\"label\"]) + 1)\nprint(\"Classes:\", NUM_CLASSES)\n\n\n# ---------------- Model ---------------- #\nclass MaxPoolCharClassifier(nn.Module):\n    def __init__(self, vocab: int, classes: int, dim: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab, dim)\n        self.proj = nn.Linear(dim, classes, bias=False)\n\n    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):\n        emb = self.embedding(tokens)  # [B,L,D]\n        logits_pos = self.proj(emb)  # [B,L,C]\n        logits_pos = logits_pos.masked_fill(~mask.unsqueeze(-1), -1e9)\n        logits, _ = logits_pos.max(dim=1)  # [B,C]\n        return logits  # unnormalised\n\n\nmodel = MaxPoolCharClassifier(VOCAB_SIZE, NUM_CLASSES, EMBED_DIM).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# ---------------- Experiment store ---------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ---------------- Rule helper ---------------- #\n@torch.no_grad()\ndef extract_char_logits() -> torch.Tensor:\n    # returns [V,C] char \u2192 class logit matrix\n    return model.proj(model.embedding.weight).detach().cpu()  # [V,C]\n\n\n@torch.no_grad()\ndef rule_predict(tokens: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    char_cls = extract_char_logits().to(tokens.device)  # [V,C]\n    batch, L = tokens.size()\n    logits = torch.full((batch, NUM_CLASSES), -1e9, device=tokens.device)\n    for i in range(batch):\n        seq_tokens = tokens[i][mask[i]]\n        if seq_tokens.numel() == 0:\n            logits[i] = 0\n        else:\n            per_cls = char_cls[seq_tokens]  # [len, C]\n            logits[i], _ = per_cls.max(dim=0)  # OR over chars\n    return logits.argmax(1)  # [B]\n\n\n# ---------------- Evaluation ---------------- #\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    rule_match = 0\n    for tokens, mask, labels in loader:\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        preds = logits.argmax(1)\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n        # rule fidelity\n        r_preds = rule_predict(tokens, mask)\n        rule_match += (r_preds == preds).sum().item()\n    acc = correct / total\n    rule_fidelity = rule_match / total\n    return acc, loss_sum / total, rule_fidelity\n\n\n# ---------------- Training loop ---------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    seen, correct, loss_sum = 0, 0, 0.0\n    start = time.time()\n    for tokens, mask, labels in train_loader:\n        optimizer.zero_grad()\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        preds = logits.argmax(1)\n        seen += labels.size(0)\n        correct += (preds == labels).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n    train_acc = correct / seen\n    train_loss = loss_sum / seen\n\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | \"\n        f\"RuleFid={val_rf:.3f}\"\n    )\n\n# ---------------- Test evaluation ---------------- #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_rf:.3f}\")\n\n# Store predictions / gts for interpretability\nmodel.eval()\nall_preds, all_gts = [], []\nwith torch.no_grad():\n    for tokens, mask, labels in test_loader:\n        logits = model(tokens, mask)\n        all_preds.append(logits.argmax(1).cpu())\n        all_gts.append(labels.cpu())\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(all_preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(all_gts).numpy()\n\n# ---------------- Save ---------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n\n# ---------------- Print learned human-readable rules ---------------- #\nchar_logits = extract_char_logits().numpy()  # [V,C]\nfor cls in range(NUM_CLASSES):\n    top_idx = char_logits[:, cls].argsort()[-RULE_TOP_K:][::-1]\n    rule_chars = [idx2char[i] for i in top_idx]\n    print(f\"Class {cls} rule chars: {rule_chars}\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time\nimport pathlib\nfrom collections import Counter, defaultdict\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------ Device ------------------ #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------ Hyper-parameters ------------------ #\nMAX_UNI, MAX_BI, MAX_TRI = 100, 1000, 2000  # size of kept n-gram sets\nBATCH_SIZE, VAL_BATCH = 256, 512\nLR, EPOCHS = 5e-3, 12\nL1_LAMBDA = 1e-4  # sparsity weight\nTOPK_RULE = 3  # n-grams per class for rule\nrng = np.random.default_rng(42)\n\n\n# ------------------ Dataset loading ------------------ #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _ld(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n\n# ------------------ Build n-gram vocabulary ------------------ #\ndef get_ngrams(seq: str, n: int) -> List[str]:\n    return [seq[i : i + n] for i in range(len(seq) - n + 1)]\n\n\nuni_ctr, bi_ctr, tri_ctr = Counter(), Counter(), Counter()\nfor s in spr[\"train\"][\"sequence\"]:\n    uni_ctr.update(get_ngrams(s, 1))\n    bi_ctr.update(get_ngrams(s, 2))\n    tri_ctr.update(get_ngrams(s, 3))\n\nuni_vocab = [g for g, _ in uni_ctr.most_common(MAX_UNI)]\nbi_vocab = [g for g, _ in bi_ctr.most_common(MAX_BI)]\ntri_vocab = [g for g, _ in tri_ctr.most_common(MAX_TRI)]\nngram2idx = {g: i for i, g in enumerate(uni_vocab + bi_vocab + tri_vocab)}\nidx2ngram = {i: g for g, i in ngram2idx.items()}\nfeat_dim = len(ngram2idx)\nprint(f\"Feature dimension: {feat_dim}\")\n\n\n# ------------------ Vectorisation ------------------ #\ndef seq_to_vec(seq: str) -> np.ndarray:\n    vec = np.zeros(feat_dim, dtype=np.float32)\n    L = len(seq)\n    if L == 0:\n        return vec\n    for g in get_ngrams(seq, 1):\n        if g in ngram2idx:\n            vec[ngram2idx[g]] += 1.0\n    for g in get_ngrams(seq, 2):\n        if g in ngram2idx:\n            vec[ngram2idx[g]] += 1.0\n    for g in get_ngrams(seq, 3):\n        if g in ngram2idx:\n            vec[ngram2idx[g]] += 1.0\n    vec /= L\n    return vec\n\n\ndef prep_split(split):\n    X = np.stack([seq_to_vec(s) for s in split[\"sequence\"]])\n    y = np.array(split[\"label\"], dtype=np.int64)\n    return torch.from_numpy(X), torch.from_numpy(y)\n\n\nX_tr, y_tr = prep_split(spr[\"train\"])\nX_dev, y_dev = prep_split(spr[\"dev\"])\nX_tst, y_tst = prep_split(spr[\"test\"])\nn_classes = int(max(y_tr.max(), y_dev.max(), y_tst.max()) + 1)\nprint(f\"Classes: {n_classes}\")\n\ntrain_loader = DataLoader(\n    TensorDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True\n)\nval_loader = DataLoader(TensorDataset(X_dev, y_dev), batch_size=VAL_BATCH)\ntest_loader = DataLoader(TensorDataset(X_tst, y_tst), batch_size=VAL_BATCH)\n\n\n# ------------------ Model ------------------ #\nclass NGramLinear(nn.Module):\n    def __init__(self, in_dim: int, n_cls: int):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, n_cls)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = NGramLinear(feat_dim, n_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss()\n\n# ------------------ Metric store ------------------ #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ------------------ Utils ------------------ #\ndef eval_split(loader):\n    model.eval()\n    tot, corr, loss_sum = 0, 0, 0.0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            preds = logits.argmax(1)\n            tot += yb.size(0)\n            corr += (preds == yb).sum().item()\n            loss_sum += loss.item() * yb.size(0)\n    return corr / tot, loss_sum / tot\n\n\ndef extract_rules(topk: int = TOPK_RULE) -> List[List[int]]:\n    W = model.linear.weight.detach().cpu().numpy()  # (C,D)\n    top_idx = np.argsort(W, axis=1)[:, -topk:]  # indices of largest weights\n    return top_idx\n\n\ndef rule_predict(xvecs: torch.Tensor, rules: List[List[int]]) -> torch.Tensor:\n    # xvecs : (B,D) float32\n    counts = xvecs.cpu().numpy()\n    preds = []\n    for row in counts:\n        scores = []\n        for cls, idxs in enumerate(rules):\n            scores.append(row[idxs].sum())\n        preds.append(int(np.argmax(scores)))\n    return torch.tensor(preds)\n\n\ndef rule_fidelity(loader):\n    rules = extract_rules()\n    total, match = 0, 0\n    model.eval()\n    with torch.no_grad():\n        for xb, _ in loader:\n            xb_d = xb.to(device)\n            logits = model(xb_d)\n            full_preds = logits.argmax(1).cpu()\n            rule_preds = rule_predict(xb, rules)\n            match += (rule_preds == full_preds).sum().item()\n            total += xb.size(0)\n    return match / total\n\n\n# ------------------ Training loop ------------------ #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    ep_loss, ep_corr, ep_seen = 0.0, 0, 0\n    t0 = time.time()\n    for xb, yb in train_loader:\n        xb = xb.to(device)\n        yb = yb.to(device)\n        opt.zero_grad()\n        outs = model(xb)\n        ce_loss = criterion(outs, yb)\n        l1_pen = sum(p.abs().sum() for p in model.parameters())\n        loss = ce_loss + L1_LAMBDA * l1_pen\n        loss.backward()\n        opt.step()\n        preds = outs.argmax(1)\n        ep_loss += ce_loss.item() * yb.size(0)\n        ep_corr += (preds == yb).sum().item()\n        ep_seen += yb.size(0)\n    train_acc = ep_corr / ep_seen\n    train_loss = ep_loss / ep_seen\n    val_acc, val_loss = eval_split(val_loader)\n    fid = rule_fidelity(val_loader)\n    # store\n    ed = experiment_data[\"SPR_BENCH\"]\n    ed[\"metrics\"][\"train_acc\"].append(train_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"Rule_Fidelity\"].append(fid)\n    ed[\"losses\"][\"train\"].append(train_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"timestamps\"].append(time.time())\n    print(\n        f\"Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={val_acc:.3f}, RuleFid={fid:.3f}\"\n    )\n\n# ------------------ Test evaluation ------------------ #\ntest_acc, test_loss = eval_split(test_loader)\ntest_fid = rule_fidelity(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_fid:.3f}\")\n\n# Predictions for persistence\nmodel.eval()\nall_p, all_g = [], []\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb = xb.to(device)\n        preds = model(xb).argmax(1).cpu()\n        all_p.append(preds)\n        all_g.append(yb)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(all_p).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(all_g).numpy()\n\n# ------------------ Save experiment data ------------------ #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data.\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time\nimport pathlib\nfrom typing import List, Tuple\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# --------------- Device ------------------ #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------- Hyper-parameters --------- #\nEMBED_DIM = 64\nBATCH_SIZE = 512\nVAL_BATCH = 1024\nLR = 3e-3\nEPOCHS = 15\nRULE_TOP_K = 3\nSEED = 2024\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n\n# --------------- Dataset loading ---------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\ndset = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in dset.items()})\n\n# --------------- Vocabulary with PAD ------- #\nPAD_TOKEN = \"<PAD>\"\nall_chars = sorted({ch for seq in dset[\"train\"][\"sequence\"] for ch in seq})\nchar2idx = {PAD_TOKEN: 0}\nchar2idx.update({c: i + 1 for i, c in enumerate(all_chars)})  # shift by +1\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB_SIZE = len(char2idx)\nprint(f\"Vocab (incl. PAD) size: {VOCAB_SIZE}\")\n\n\n# --------------- Encode / Collate ---------- #\ndef encode(seq: str) -> List[int]:\n    return [char2idx[c] for c in seq]\n\n\ndef collate(batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    seqs = [encode(b[\"sequence\"]) for b in batch]\n    labels = torch.tensor([int(b[\"label\"]) for b in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    tokens = torch.full((len(seqs), max_len), 0, dtype=torch.long)  # PAD=0\n    mask = torch.zeros((len(seqs), max_len), dtype=torch.bool)\n    for i, s in enumerate(seqs):\n        tokens[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        mask[i, : len(s)] = True\n    return tokens.to(device), mask.to(device), labels.to(device)\n\n\ntrain_loader = DataLoader(\n    dset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    dset[\"dev\"], batch_size=VAL_BATCH, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dset[\"test\"], batch_size=VAL_BATCH, shuffle=False, collate_fn=collate\n)\n\nNUM_CLASSES = int(max(dset[\"train\"][\"label\"]) + 1)\nprint(f\"Num classes: {NUM_CLASSES}\")\n\n\n# --------------- Model --------------------- #\nclass MaxPoolCharClassifier(nn.Module):\n    def __init__(self, vocab: int, classes: int, dim: int, pad_idx: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab, dim, padding_idx=pad_idx)\n        self.proj = nn.Linear(dim, classes, bias=False)\n\n    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):\n        emb = self.embedding(tokens)  # [B,L,D]\n        logits_pos = self.proj(emb)  # [B,L,C]\n        logits_pos = logits_pos.masked_fill(~mask.unsqueeze(-1), -1e9)\n        logits, _ = logits_pos.max(dim=1)  # [B,C]\n        return logits\n\n\nmodel = MaxPoolCharClassifier(VOCAB_SIZE, NUM_CLASSES, EMBED_DIM, pad_idx=0).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# --------------- Experiment data store ----- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# --------------- Rule helpers -------------- #\n@torch.no_grad()\ndef extract_char_logits() -> torch.Tensor:\n    # returns [V,C] matrix (PAD row included)\n    return model.proj(model.embedding.weight).detach().cpu()\n\n\n@torch.no_grad()\ndef rule_predict(tokens: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    char_cls = extract_char_logits().to(tokens.device)  # [V,C]\n    batch, _ = tokens.shape\n    out = torch.full((batch, NUM_CLASSES), -1e9, device=tokens.device)\n    for i in range(batch):\n        valid_tokens = tokens[i][mask[i]]\n        if valid_tokens.numel() == 0:\n            out[i] = 0\n        else:\n            out[i], _ = char_cls[valid_tokens].max(dim=0)\n    return out.argmax(1)\n\n\n# --------------- Evaluation ---------------- #\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum, rule_match = 0, 0, 0.0, 0\n    for tokens, mask, labels in loader:\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        preds = logits.argmax(1)\n        r_preds = rule_predict(tokens, mask)\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n        rule_match += (r_preds == preds).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n    return correct / total, loss_sum / total, rule_match / total\n\n\n# --------------- Training loop ------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    seen, correct, loss_sum = 0, 0, 0.0\n    for tokens, mask, labels in train_loader:\n        optimizer.zero_grad()\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        preds = logits.argmax(1)\n        seen += labels.size(0)\n        correct += (preds == labels).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n    train_acc = correct / seen\n    train_loss = loss_sum / seen\n\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | \"\n        f\"RuleFid={val_rf:.3f}\"\n    )\n\n# --------------- Test evaluation ----------- #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_rf:.3f}\")\n\n# --------------- Store predictions ---------- #\nmodel.eval()\nall_preds, all_gts = [], []\nwith torch.no_grad():\n    for tokens, mask, labels in test_loader:\n        logits = model(tokens, mask)\n        all_preds.append(logits.argmax(1).cpu())\n        all_gts.append(labels.cpu())\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(all_preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(all_gts).numpy()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n\n# --------------- Human-readable rules ------- #\nchar_logits = extract_char_logits().numpy()\nfor cls in range(NUM_CLASSES):\n    top_idx = char_logits[1:, cls].argsort()[-RULE_TOP_K:][::-1] + 1  # skip PAD\n    rule_chars = [idx2char[i] for i in top_idx]\n    print(f\"Class {cls} top chars: {rule_chars}\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time\nimport pathlib\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict, disable_caching\n\n# ------------------------------------------------------------------ #\n#                        DEVICE & SEED                               #\n# ------------------------------------------------------------------ #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nSEED = 2024\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# ------------------------------------------------------------------ #\n#                     EXPERIMENT DATA STORE                          #\n# ------------------------------------------------------------------ #\nexperiment_data: Dict = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ------------------------------------------------------------------ #\n#                     DATASET LOADING HELPER                         #\n# ------------------------------------------------------------------ #\ndisable_caching()  # avoid filling ~/.cache unnecessarily\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# absolute path configurable via env\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nassert DATA_PATH.exists(), f\"Data path {DATA_PATH} does not exist\"\n\ndset = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", list(dset.keys()))\n\n# ------------------------------------------------------------------ #\n#                          VOCABULARY                                #\n# ------------------------------------------------------------------ #\nPAD_IDX = 0  # reserve 0 for padding\nall_chars = set(ch for seq in dset[\"train\"][\"sequence\"] for ch in seq)\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(all_chars))}  # start at 1\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB_SIZE = len(char2idx) + 1  # + PAD\nprint(\"Vocab size (with PAD):\", VOCAB_SIZE)\n\n\n# ------------------------------------------------------------------ #\n#                         ENCODING UTILITIES                         #\n# ------------------------------------------------------------------ #\ndef encode(seq: str) -> List[int]:\n    return [char2idx[c] for c in seq]\n\n\ndef collate_fn(batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    seqs = [encode(b[\"sequence\"]) for b in batch]\n    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.full((len(seqs), max_len), PAD_IDX, dtype=torch.long)\n    mask = torch.zeros((len(seqs), max_len), dtype=torch.bool)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        mask[i, : len(s)] = True\n    return padded.to(device), mask.to(device), labels.to(device)\n\n\n# ------------------------------------------------------------------ #\n#                         DATA LOADERS                               #\n# ------------------------------------------------------------------ #\nBATCH_SIZE = 256\nVAL_BATCH = 512\n\ntrain_loader = DataLoader(\n    dset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(dset[\"dev\"], batch_size=VAL_BATCH, collate_fn=collate_fn)\ntest_loader = DataLoader(dset[\"test\"], batch_size=VAL_BATCH, collate_fn=collate_fn)\n\nNUM_CLASSES = int(max(dset[\"train\"][\"label\"]) + 1)\nprint(\"Number of classes:\", NUM_CLASSES)\n\n\n# ------------------------------------------------------------------ #\n#                              MODEL                                 #\n# ------------------------------------------------------------------ #\nclass SmallCNN(nn.Module):\n    \"\"\"\n    Char-CNN with kernel sizes 1-3 and global-max pooling.\n    kernel size 1 keeps character-level features (used for simple rules).\n    \"\"\"\n\n    def __init__(self, vocab_size: int, embed_dim: int, n_classes: int, pad_idx: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        kernel_sizes = [1, 2, 3]\n        n_filters = 64\n        self.convs = nn.ModuleList(\n            [nn.Conv1d(embed_dim, n_filters, k, padding=0) for k in kernel_sizes]\n        )\n        self.activation = nn.ReLU()\n        self.fc = nn.Linear(n_filters * len(kernel_sizes), n_classes)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x: [B,L] long\n        mask: [B,L] bool\n        \"\"\"\n        emb = self.embedding(x)  # [B,L,D]\n        emb = emb.transpose(1, 2)  # [B,D,L] for conv1d\n        feats = []\n        for conv in self.convs:\n            z = self.activation(conv(emb))  # [B,F,L']\n            pooled = torch.max(z, dim=2).values  # global max pool -> [B,F]\n            feats.append(pooled)\n        feats = torch.cat(feats, dim=1)  # [B, F*k]\n        logits = self.fc(feats)  # [B,C]\n        return logits\n\n\nEMBED_DIM = 64\nmodel = SmallCNN(VOCAB_SIZE, EMBED_DIM, NUM_CLASSES, PAD_IDX).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n\n\n# ------------------------------------------------------------------ #\n#                  RULE EXTRACTION  (char-level)                     #\n# ------------------------------------------------------------------ #\n@torch.no_grad()\ndef extract_char_rules(top_k: int = 3) -> List[List[str]]:\n    \"\"\"\n    Use kernel size 1 filters: they map directly from characters (after embedding*conv weight)\n    We approximate char contribution by multiplying embedding matrix with conv weight.\n    For each class we take FC weight @ conv filter to get class-specific importance.\n    \"\"\"\n    # first conv is kernel size 1 by construction\n    conv1: nn.Conv1d = model.convs[0]\n    W_conv = conv1.weight.squeeze(-1)  # [F, D]\n    W_embed = model.embedding.weight  # [V,D]\n    char_feat = torch.matmul(W_embed, W_conv.T)  # [V, F]\n    # class importance = char_feat @ fc_weight (only filters of conv1)\n    fc_weights = model.fc.weight[:, : W_conv.size(0)]  # [C, F]\n    char_cls_score = torch.matmul(char_feat, fc_weights.T)  # [V, C]\n    top_chars = []\n    for c in range(NUM_CLASSES):\n        idxs = torch.topk(char_cls_score[:, c], top_k).indices.cpu().tolist()\n        top_chars.append([idx2char[i] for i in idxs if i in idx2char])\n    return top_chars  # list of list[str]\n\n\n@torch.no_grad()\ndef rule_based_predict(tokens: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Very simple heuristic: if sequence contains any of the top characters\n    for class c (and not for others), predict c else use fallback model.\n    \"\"\"\n    rules = extract_char_rules(top_k=3)\n    batch_preds = []\n    seq_np = tokens.cpu().numpy()\n    for seq in seq_np:\n        chars = {idx for idx in seq if idx != PAD_IDX}\n        matched = None\n        for c, chars_c in enumerate(rules):\n            if any(char2idx[ch] in chars for ch in chars_c):\n                matched = c\n                break\n        if matched is None:\n            matched = -1  # signal fallback\n        batch_preds.append(matched)\n    return torch.tensor(batch_preds, device=tokens.device)\n\n\n# ------------------------------------------------------------------ #\n#                            EVALUATION                              #\n# ------------------------------------------------------------------ #\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    tot, correct, loss_sum = 0, 0, 0.0\n    rule_match = 0\n    for tokens, mask, labels in loader:\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        preds = logits.argmax(1)\n        tot += labels.size(0)\n        correct += (preds == labels).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n\n        r_preds = rule_based_predict(tokens)\n        mask_valid = r_preds != -1\n        rule_match += (r_preds[mask_valid] == preds[mask_valid]).sum().item()\n    acc = correct / tot\n    rf = rule_match / tot\n    return acc, loss_sum / tot, rf\n\n\n# ------------------------------------------------------------------ #\n#                          TRAINING LOOP                             #\n# ------------------------------------------------------------------ #\nEPOCHS = 8\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    seen, correct, loss_sum = 0, 0, 0.0\n    for tokens, mask, labels in train_loader:\n        optimizer.zero_grad()\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = logits.argmax(1)\n        seen += labels.size(0)\n        correct += (preds == labels).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n\n    train_acc = correct / seen\n    train_loss = loss_sum / seen\n\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | \"\n        f\"RuleFid={val_rf:.3f}\"\n    )\n\n# ------------------------------------------------------------------ #\n#                        TEST EVALUATION                             #\n# ------------------------------------------------------------------ #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_rf:.3f}\")\n\n# store predictions & gts\nmodel.eval()\nall_preds, all_gts = [], []\nwith torch.no_grad():\n    for tokens, mask, labels in test_loader:\n        out = model(tokens, mask).argmax(1).cpu()\n        all_preds.append(out)\n        all_gts.append(labels.cpu())\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(all_preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(all_gts).numpy()\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n\n# Print simple human-readable rules\nfor cls, chars in enumerate(extract_char_rules(top_k=5)):\n    print(f\"Class {cls} indicative characters: {chars}\")\n", "import os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport time\nimport pathlib\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------- Device ---------------- #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- Hyper-parameters ---------------- #\nEMBED_DIM = 32\nBATCH_SIZE = 256\nVAL_BATCH = 512\nLR = 3e-3\nEPOCHS = 12\nRULE_TOP_K = 3  # characters kept per class to form the human rule\nSEED = 2024\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n\n# ---------------- Dataset loading ---------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_l(\"train.csv\"), dev=_l(\"dev.csv\"), test=_l(\"test.csv\"))\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\ndset = load_spr_bench(DATA_PATH)\nprint(\"Loaded splits:\", list(dset.keys()))\n\n# ---------------- Vocabulary ---------------- #\nall_chars = set(\n    ch for split in dset.keys() for seq in dset[split][\"sequence\"] for ch in seq\n)\nchar2idx = {\n    c: i + 1 for i, c in enumerate(sorted(all_chars))\n}  # start at 1, reserve 0 for PAD\nPAD_IDX = 0\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB_SIZE = len(char2idx) + 1\nprint(\"Vocab size (incl. PAD):\", VOCAB_SIZE)\n\n\n# ---------------- Utilities ---------------- #\ndef encode(seq: str) -> List[int]:\n    return [char2idx[c] for c in seq]\n\n\ndef collate(batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    seqs = [encode(b[\"sequence\"]) for b in batch]\n    labels = torch.tensor([int(b[\"label\"]) for b in batch], dtype=torch.long)\n    max_len = max(len(s) for s in seqs)\n    padded = torch.full((len(seqs), max_len), PAD_IDX, dtype=torch.long)\n    mask = torch.zeros((len(seqs), max_len), dtype=torch.bool)\n    for i, s in enumerate(seqs):\n        padded[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n        mask[i, : len(s)] = True\n    return padded.to(device), mask.to(device), labels.to(device)\n\n\ntrain_loader = DataLoader(\n    dset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    dset[\"dev\"], batch_size=VAL_BATCH, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    dset[\"test\"], batch_size=VAL_BATCH, shuffle=False, collate_fn=collate\n)\n\nNUM_CLASSES = int(max(dset[\"train\"][\"label\"]) + 1)\nprint(\"Classes:\", NUM_CLASSES)\n\n\n# ---------------- Model ---------------- #\nclass MaxPoolCharClassifier(nn.Module):\n    def __init__(self, vocab: int, classes: int, dim: int, pad_idx: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab, dim, padding_idx=pad_idx)\n        self.proj = nn.Linear(dim, classes, bias=False)\n\n    def forward(self, tokens: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        emb = self.embedding(tokens)  # [B,L,D]\n        logits_pos = self.proj(emb)  # [B,L,C]\n        logits_pos = logits_pos.masked_fill(~mask.unsqueeze(-1), -1e9)\n        logits, _ = logits_pos.max(dim=1)  # [B,C]\n        return logits\n\n\nmodel = MaxPoolCharClassifier(VOCAB_SIZE, NUM_CLASSES, EMBED_DIM, PAD_IDX).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# ---------------- Experiment store ---------------- #\nexperiment_data: Dict[str, Dict] = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# ---------------- Rule helpers ---------------- #\nrule_char_sets: List[List[int]] = [[] for _ in range(NUM_CLASSES)]  # updated each epoch\n\n\n@torch.no_grad()\ndef update_rule_chars() -> None:\n    \"\"\"Extract top-K indicative characters per class to form human-readable rules.\"\"\"\n    global rule_char_sets\n    char_logits = model.proj(model.embedding.weight).detach().cpu().numpy()  # [V,C]\n    rule_char_sets = []\n    for cls in range(NUM_CLASSES):\n        top_idx = char_logits[:, cls].argsort()[-RULE_TOP_K:][::-1]\n        rule_char_sets.append(list(top_idx))\n\n\n@torch.no_grad()\ndef rule_predict(tokens: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Predict via simple char-presence rules built from `rule_char_sets`.\"\"\"\n    batch = tokens.size(0)\n    preds = torch.zeros(batch, dtype=torch.long, device=tokens.device)\n    for i in range(batch):\n        seq_tokens = set(tokens[i][mask[i]].tolist())\n        votes = [\n            len(seq_tokens.intersection(set(rule_char_sets[c])))\n            for c in range(NUM_CLASSES)\n        ]\n        preds[i] = int(np.argmax(votes))\n    return preds\n\n\n# ---------------- Evaluation ---------------- #\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    total, correct, loss_sum, rule_match = 0, 0, 0.0, 0\n    for tokens, mask, labels in loader:\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        preds = logits.argmax(1)\n        r_preds = rule_predict(tokens, mask)\n\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n        rule_match += (r_preds == preds).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n\n    acc = correct / total\n    rule_fid = rule_match / total\n    avg_loss = loss_sum / total\n    return acc, avg_loss, rule_fid\n\n\n# ---------------- Training loop ---------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    seen, correct, loss_sum = 0, 0, 0.0\n    start = time.time()\n    for tokens, mask, labels in train_loader:\n        optimizer.zero_grad()\n        logits = model(tokens, mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = logits.argmax(1)\n        seen += labels.size(0)\n        correct += (preds == labels).sum().item()\n        loss_sum += loss.item() * labels.size(0)\n\n    train_acc = correct / seen\n    train_loss = loss_sum / seen\n\n    # update rule chars after each epoch so validation uses fresh rules\n    update_rule_chars()\n\n    val_acc, val_loss, val_rf = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rf)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch:02d}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | \"\n        f\"RuleFid={val_rf:.3f}\"\n    )\n\n# ---------------- Test evaluation ---------------- #\ntest_acc, test_loss, test_rf = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFid={test_rf:.3f}\")\n\n# Store predictions / ground truths for interpretability\nmodel.eval()\nall_preds, all_gts = [], []\nwith torch.no_grad():\n    for tokens, mask, labels in test_loader:\n        logits = model(tokens, mask)\n        all_preds.append(logits.argmax(1).cpu())\n        all_gts.append(labels.cpu())\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(all_preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(all_gts).numpy()\n\n# ---------------- Save ---------------- #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n\n# ---------------- Print learned human-readable rules ---------------- #\nfor cls in range(NUM_CLASSES):\n    readable = [idx2char[i] if i in idx2char else \"<PAD>\" for i in rule_char_sets[cls]]\n    print(f\"Class {cls} rule chars: {readable}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, time, pathlib, itertools, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------- #\n#                              HOUSE-KEEPING                                 #\n# -------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------- #\n#                             HYPER-PARAMETERS                               #\n# -------------------------------------------------------------------------- #\nBATCH_SIZE = 256\nVAL_BATCH = 512\nEPOCHS = 10\nLR = 3e-3\nWEIGHT_DECAY = 1e-4\nL1_LAMBDA = 2e-3  # sparsity on rule head\nGATE_LAMBDA = 1e-3  # encourage confident gate (towards 0 or 1)\nEMB_DIM = 48\nCONV_CH = 96\nKERNELS = [3, 4, 5]\nMAX_LEN = 128\nPAD_IDX = 0\n\n\n# -------------------------------------------------------------------------- #\n#                              LOAD DATASET                                  #\n# -------------------------------------------------------------------------- #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\nDATA_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATASET_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n)\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------- #\n#                               VOCAB                                        #\n# -------------------------------------------------------------------------- #\nchars = set(itertools.chain.from_iterable(spr[\"train\"][\"sequence\"]))\nchar2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}  # 0 reserved for PAD\nidx2char = {i: c for c, i in char2idx.items()}\nVOCAB = len(char2idx) + 1\n\n\ndef seq_to_bag(seq: str) -> np.ndarray:\n    v = np.zeros(VOCAB - 1, dtype=np.float32)\n    for ch in seq[:MAX_LEN]:\n        v[char2idx[ch] - 1] += 1\n    if len(seq) > 0:\n        v /= len(seq)\n    return v\n\n\ndef seq_to_idx(seq: str) -> np.ndarray:\n    arr = np.full(MAX_LEN, PAD_IDX, dtype=np.int64)\n    for i, ch in enumerate(seq[:MAX_LEN]):\n        arr[i] = char2idx[ch]\n    return arr\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.bags = np.stack([seq_to_bag(s) for s in split[\"sequence\"]])\n        self.seqs = np.stack([seq_to_idx(s) for s in split[\"sequence\"]])\n        self.labels = np.array(split[\"label\"], dtype=np.int64)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            torch.from_numpy(self.bags[idx]),\n            torch.from_numpy(self.seqs[idx]),\n            torch.tensor(self.labels[idx]),\n        )\n\n\ntrain_ds, val_ds, test_ds = [SPRDataset(spr[s]) for s in (\"train\", \"dev\", \"test\")]\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH)\ntest_loader = DataLoader(test_ds, batch_size=VAL_BATCH)\n\nNUM_CLASSES = int(\n    max(train_ds.labels.max(), val_ds.labels.max(), test_ds.labels.max()) + 1\n)\nprint(\"Vocab\", VOCAB - 1, \"classes\", NUM_CLASSES)\n\n\n# -------------------------------------------------------------------------- #\n#                               MODEL                                        #\n# -------------------------------------------------------------------------- #\nclass GatedHybridRuleCNN(nn.Module):\n    def __init__(self, vocab: int, n_cls: int):\n        super().__init__()\n        self.rule_head = nn.Linear(vocab - 1, n_cls)  # interpretable rules\n        self.gate = nn.Sequential(nn.Linear(vocab - 1, 1), nn.Sigmoid())  # scalar gate\n        self.embed = nn.Embedding(vocab, EMB_DIM, padding_idx=PAD_IDX)\n        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CONV_CH, k) for k in KERNELS])\n        self.cnn_head = nn.Linear(CONV_CH * len(KERNELS), n_cls)\n\n    def forward(self, bag, seq):\n        rule_logits = self.rule_head(bag)  # (B,C)\n        gate = self.gate(bag)  # (B,1)\n        x = self.embed(seq).transpose(1, 2)  # (B,E,L)\n        features = [torch.amax(torch.relu(conv(x)), dim=2) for conv in self.convs]\n        cnn_logits = self.cnn_head(torch.cat(features, dim=1))\n        final_logits = gate * rule_logits + (1 - gate) * cnn_logits\n        return final_logits, rule_logits, gate.squeeze(1)\n\n\nmodel = GatedHybridRuleCNN(VOCAB, NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# -------------------------------------------------------------------------- #\n#                           METRIC STORAGE                                   #\n# -------------------------------------------------------------------------- #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"Rule_Fidelity\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"timestamps\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------------- #\n#                               EVALUATION                                   #\n# -------------------------------------------------------------------------- #\ndef evaluate(loader):\n    model.eval()\n    tot, corr, loss_sum, fid_tot, fid_match = 0, 0, 0.0, 0, 0\n    with torch.no_grad():\n        for bag, seq, y in loader:\n            bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n            logits, rule_logits, _ = model(bag, seq)\n            loss = criterion(logits, y)\n            preds = logits.argmax(1)\n            rule_preds = rule_logits.argmax(1)\n            corr += (preds == y).sum().item()\n            fid_match += (preds == rule_preds).sum().item()\n            fid_tot += preds.size(0)\n            loss_sum += loss.item() * y.size(0)\n            tot += y.size(0)\n    return corr / tot, loss_sum / tot, fid_match / fid_tot\n\n\n# -------------------------------------------------------------------------- #\n#                               TRAIN LOOP                                   #\n# -------------------------------------------------------------------------- #\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    run_loss, run_corr, n_seen = 0.0, 0, 0\n    t0 = time.time()\n    for bag, seq, y in train_loader:\n        bag, seq, y = bag.to(device), seq.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits, rule_logits, gate = model(bag, seq)\n        ce_loss = criterion(logits, y)\n        l1_loss = model.rule_head.weight.abs().mean()\n        gate_reg = torch.mean(gate * (1 - gate))  # small when gate ~0/1\n        loss = ce_loss + L1_LAMBDA * l1_loss + GATE_LAMBDA * gate_reg\n        loss.backward()\n        optimizer.step()\n        run_loss += loss.item() * y.size(0)\n        run_corr += (logits.argmax(1) == y).sum().item()\n        n_seen += y.size(0)\n\n    train_loss, train_acc = run_loss / n_seen, run_corr / n_seen\n    val_acc, val_loss, val_rfs = evaluate(val_loader)\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(train_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(val_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"Rule_Fidelity\"].append(val_rfs)\n    experiment_data[\"SPR_BENCH\"][\"timestamps\"].append(time.time())\n\n    print(\n        f\"Epoch {epoch}: \"\n        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} | RFS={val_rfs:.3f}\"\n    )\n\n# -------------------------------------------------------------------------- #\n#                               FINAL TEST                                   #\n# -------------------------------------------------------------------------- #\ntest_acc, test_loss, test_rfs = evaluate(test_loader)\nprint(f\"\\nTest: loss={test_loss:.4f}, acc={test_acc:.3f}, RuleFidelity={test_rfs:.3f}\")\n\n# store detailed predictions\nmodel.eval()\npreds, gts = [], []\nwith torch.no_grad():\n    for bag, seq, y in test_loader:\n        bag, seq = bag.to(device), seq.to(device)\n        logits, _, _ = model(bag, seq)\n        preds.append(logits.argmax(1).cpu())\n        gts.append(y)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = torch.cat(preds).numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = torch.cat(gts).numpy()\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved to\", working_dir)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train', 'dev',\n'test'])\", '\\n', 'Vocab size = 9', '\\n', 'Number of classes: 2', '\\n', 'Epoch\n01: train_loss=0.6910, train_acc=0.513 | val_loss=0.6881, val_acc=0.678 |\nRBA=0.562 | epoch_time=0.2s', '\\n', 'Epoch 02: train_loss=0.6827,\ntrain_acc=0.662 | val_loss=0.6875, val_acc=0.496 | RBA=0.662 | epoch_time=0.0s',\n'\\n', 'Epoch 03: train_loss=0.6785, train_acc=0.587 | val_loss=0.6844,\nval_acc=0.630 | RBA=0.662 | epoch_time=0.0s', '\\n', 'Epoch 04:\ntrain_loss=0.6741, train_acc=0.829 | val_loss=0.6811, val_acc=0.752 | RBA=0.662\n| epoch_time=0.0s', '\\n', 'Epoch 05: train_loss=0.6695, train_acc=0.904 |\nval_loss=0.6789, val_acc=0.730 | RBA=0.662 | epoch_time=0.0s', '\\n', 'Epoch 06:\ntrain_loss=0.6651, train_acc=0.892 | val_loss=0.6764, val_acc=0.740 | RBA=0.662\n| epoch_time=0.0s', '\\n', 'Epoch 07: train_loss=0.6607, train_acc=0.913 |\nval_loss=0.6740, val_acc=0.744 | RBA=0.662 | epoch_time=0.0s', '\\n', 'Epoch 08:\ntrain_loss=0.6563, train_acc=0.919 | val_loss=0.6716, val_acc=0.746 | RBA=0.662\n| epoch_time=0.0s', '\\n', 'Epoch 09: train_loss=0.6521, train_acc=0.900 |\nval_loss=0.6695, val_acc=0.732 | RBA=0.662 | epoch_time=0.0s', '\\n', 'Epoch 10:\ntrain_loss=0.6477, train_acc=0.933 | val_loss=0.6663, val_acc=0.764 | RBA=0.662\n| epoch_time=0.0s', '\\n', '\\nTest set: loss=0.6655, acc=0.773, RBA=0.662', '\\n',\n'\\nExperiment data saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_02-43-53_interpretable_neural_rule_learning_attempt_0/0-\nrun/process_ForkProcess-3/working/experiment_data.npy', '\\n', 'Execution time: 2\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 94519.53\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 112063.27\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 143956.07\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"dict_keys(['train', 'dev',\n'test'])\", '\\n', 'Vocab size=', ' ', '9', ' ', 'Num classes=', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.9141, train_acc=0.524 | val_loss=0.8593, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.6123, train_acc=0.613 |\nval_loss=0.5727, val_acc=0.744 | RFS=0.440', '\\n', 'Epoch 3: train_loss=0.4219,\ntrain_acc=0.836 | val_loss=0.6035, val_acc=0.702 | RFS=0.386', '\\n', 'Epoch 4:\ntrain_loss=0.3082, train_acc=0.941 | val_loss=0.5699, val_acc=0.760 |\nRFS=0.472', '\\n', 'Epoch 5: train_loss=0.2141, train_acc=0.935 |\nval_loss=0.6411, val_acc=0.766 | RFS=0.490', '\\n', 'Epoch 6: train_loss=0.1655,\ntrain_acc=0.958 | val_loss=0.7118, val_acc=0.756 | RFS=0.476', '\\n', 'Epoch 7:\ntrain_loss=0.1404, train_acc=0.961 | val_loss=0.7730, val_acc=0.762 |\nRFS=0.482', '\\n', 'Epoch 8: train_loss=0.1231, train_acc=0.964 |\nval_loss=0.8177, val_acc=0.774 | RFS=0.498', '\\n', '\\nTest: loss=0.7844,\nacc=0.780, RuleFid=0.490', '\\n', 'All experiment data saved.', '\\n', 'Execution\ntime: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 131546.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 41764.29\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 151934.51\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Epoch\n01: train_acc=0.511, val_acc=0.674, fidelity=0.768, val_loss=0.6881, time=0.2s',\n'\\n', 'Epoch 02: train_acc=0.675, val_acc=0.512, fidelity=0.778,\nval_loss=0.6873, time=0.0s', '\\n', 'Epoch 03: train_acc=0.617, val_acc=0.652,\nfidelity=0.902, val_loss=0.6842, time=0.0s', '\\n', 'Epoch 04: train_acc=0.874,\nval_acc=0.758, fidelity=0.828, val_loss=0.6811, time=0.0s', '\\n', 'Epoch 05:\ntrain_acc=0.924, val_acc=0.730, fidelity=0.876, val_loss=0.6792, time=0.0s',\n'\\n', 'Epoch 06: train_acc=0.885, val_acc=0.728, fidelity=0.870,\nval_loss=0.6771, time=0.0s', '\\n', 'Epoch 07: train_acc=0.909, val_acc=0.736,\nfidelity=0.854, val_loss=0.6748, time=0.0s', '\\n', 'Epoch 08: train_acc=0.913,\nval_acc=0.740, fidelity=0.846, val_loss=0.6726, time=0.0s', '\\n', 'Epoch 09:\ntrain_acc=0.891, val_acc=0.726, fidelity=0.872, val_loss=0.6708, time=0.0s',\n'\\n', 'Epoch 10: train_acc=0.926, val_acc=0.760, fidelity=0.820,\nval_loss=0.6679, time=0.0s', '\\n', 'Epoch 11: train_acc=0.953, val_acc=0.750,\nfidelity=0.830, val_loss=0.6662, time=0.0s', '\\n', 'Epoch 12: train_acc=0.911,\nval_acc=0.738, fidelity=0.846, val_loss=0.6645, time=0.0s', '\\n', '\\nTEST  \u2013\nacc: 0.741, loss: 0.6623, Rule Fidelity: 0.837', '\\n', 'Saved metrics ->\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 41702.83\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 105347.47\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 148350.16\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"['train', 'dev', 'test']\", '\\n',\n'Vocab size:', ' ', '9', '\\n', 'Classes:', ' ', '2', '\\n', 'Epoch 01:\ntrain_loss=0.7121, train_acc=0.495 | val_loss=0.7057, val_acc=0.480 |\nRuleFid=1.000', '\\n', 'Epoch 02: train_loss=0.6985, train_acc=0.498 |\nval_loss=0.6926, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 03:\ntrain_loss=0.6946, train_acc=0.500 | val_loss=0.6926, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 04: train_loss=0.6945, train_acc=0.496 |\nval_loss=0.6960, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 05:\ntrain_loss=0.6940, train_acc=0.505 | val_loss=0.6924, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 06: train_loss=0.6941, train_acc=0.485 |\nval_loss=0.6951, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 07:\ntrain_loss=0.6942, train_acc=0.484 | val_loss=0.6926, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 08: train_loss=0.6938, train_acc=0.490 |\nval_loss=0.6945, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 09:\ntrain_loss=0.6932, train_acc=0.502 | val_loss=0.6925, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 10: train_loss=0.6937, train_acc=0.494 |\nval_loss=0.6935, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 11:\ntrain_loss=0.6953, train_acc=0.500 | val_loss=0.6931, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 12: train_loss=0.6945, train_acc=0.500 |\nval_loss=0.6924, val_acc=0.520 | RuleFid=1.000', '\\n', '\\nTest: loss=0.6944,\nacc=0.498, RuleFid=1.000', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', \"Class 0 rule chars: ['r', '\u25a0', '\u25cf']\",\n'\\n', \"Class 1 rule chars: [' ', '\u25cf', 'b']\", '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 114147.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 103064.28\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 126704.65\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Feature\ndimension: 81', '\\n', 'Classes: 2', '\\n', 'Epoch 1: val_loss=0.6893,\nval_acc=0.478, RuleFid=0.958', '\\n', 'Epoch 2: val_loss=0.6815, val_acc=0.560,\nRuleFid=0.896', '\\n', 'Epoch 3: val_loss=0.6732, val_acc=0.726, RuleFid=0.814',\n'\\n', 'Epoch 4: val_loss=0.6666, val_acc=0.722, RuleFid=0.806', '\\n', 'Epoch 5:\nval_loss=0.6606, val_acc=0.712, RuleFid=0.822', '\\n', 'Epoch 6: val_loss=0.6543,\nval_acc=0.734, RuleFid=0.808', '\\n', 'Epoch 7: val_loss=0.6493, val_acc=0.716,\nRuleFid=0.818', '\\n', 'Epoch 8: val_loss=0.6429, val_acc=0.744, RuleFid=0.810',\n'\\n', 'Epoch 9: val_loss=0.6374, val_acc=0.742, RuleFid=0.804', '\\n', 'Epoch 10:\nval_loss=0.6325, val_acc=0.742, RuleFid=0.804', '\\n', 'Epoch 11:\nval_loss=0.6279, val_acc=0.742, RuleFid=0.804', '\\n', 'Epoch 12:\nval_loss=0.6226, val_acc=0.746, RuleFid=0.800', '\\n', '\\nTest: loss=0.6182,\nacc=0.764, RuleFid=0.768', '\\n', 'Saved experiment data.', '\\n', 'Execution\ntime: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab (incl. PAD) size: 10', '\\n', 'Num classes: 2', '\\n', 'Epoch 01:\ntrain_loss=0.7021, train_acc=0.507 | val_loss=0.6984, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 02: train_loss=0.6995, train_acc=0.500 |\nval_loss=0.6950, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 03:\ntrain_loss=0.6957, train_acc=0.500 | val_loss=0.6981, val_acc=0.480 |\nRuleFid=1.000', '\\n', 'Epoch 04: train_loss=0.6937, train_acc=0.503 |\nval_loss=0.6924, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 05:\ntrain_loss=0.6941, train_acc=0.500 | val_loss=0.6923, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 06: train_loss=0.6932, train_acc=0.500 |\nval_loss=0.6941, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 07:\ntrain_loss=0.6939, train_acc=0.500 | val_loss=0.6946, val_acc=0.480 |\nRuleFid=1.000', '\\n', 'Epoch 08: train_loss=0.6935, train_acc=0.507 |\nval_loss=0.6924, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 09:\ntrain_loss=0.6936, train_acc=0.500 | val_loss=0.6933, val_acc=0.480 |\nRuleFid=1.000', '\\n', 'Epoch 10: train_loss=0.6932, train_acc=0.489 |\nval_loss=0.6934, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 11:\ntrain_loss=0.6932, train_acc=0.500 | val_loss=0.6935, val_acc=0.480 |\nRuleFid=1.000', '\\n', 'Epoch 12: train_loss=0.6935, train_acc=0.495 |\nval_loss=0.6927, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 13:\ntrain_loss=0.6932, train_acc=0.501 | val_loss=0.6938, val_acc=0.480 |\nRuleFid=1.000', '\\n', 'Epoch 14: train_loss=0.6933, train_acc=0.500 |\nval_loss=0.6934, val_acc=0.480 | RuleFid=1.000', '\\n', 'Epoch 15:\ntrain_loss=0.6937, train_acc=0.500 | val_loss=0.6932, val_acc=0.480 |\nRuleFid=1.000', '\\n', '\\nTest: loss=0.6931, acc=0.502, RuleFid=1.000', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n13/working/experiment_data.npy', '\\n', \"Class 0 top chars: ['g', '\u25c6', ' ']\",\n'\\n', \"Class 1 top chars: [' ', 'b', '\u25c6']\", '\\n', 'Execution time: 5 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size (with PAD):', ' ', '10', '\\n', 'Number of classes:', ' ', '2',\n'\\n', 'Epoch 01: train_loss=0.8713, train_acc=0.521 | val_loss=0.9045,\nval_acc=0.480 | RuleFid=0.000', '\\n', 'Epoch 02: train_loss=0.6933,\ntrain_acc=0.542 | val_loss=0.6686, val_acc=0.488 | RuleFid=0.032', '\\n', 'Epoch\n03: train_loss=0.5813, train_acc=0.633 | val_loss=0.5910, val_acc=0.702 |\nRuleFid=0.650', '\\n', 'Epoch 04: train_loss=0.4702, train_acc=0.845 |\nval_loss=0.5713, val_acc=0.692 | RuleFid=0.640', '\\n', 'Epoch 05:\ntrain_loss=0.3702, train_acc=0.892 | val_loss=0.5572, val_acc=0.744 |\nRuleFid=0.532', '\\n', 'Epoch 06: train_loss=0.2865, train_acc=0.930 |\nval_loss=0.5773, val_acc=0.756 | RuleFid=0.516', '\\n', 'Epoch 07:\ntrain_loss=0.2340, train_acc=0.934 | val_loss=0.6119, val_acc=0.764 |\nRuleFid=0.500', '\\n', 'Epoch 08: train_loss=0.1975, train_acc=0.943 |\nval_loss=0.6560, val_acc=0.764 | RuleFid=0.504', '\\n', '\\nTest: loss=0.6715,\nacc=0.777, RuleFid=0.487', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', \"Class 0 indicative characters: ['b',\n'\u25c6', ' ', '\u25b2']\", '\\n', \"Class 1 indicative characters: ['g', '\u25a0', ' ', 'r']\",\n'\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.8398, train_acc=0.544 | val_loss=0.7889, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.6050, train_acc=0.600 |\nval_loss=0.5734, val_acc=0.792 | RFS=0.512', '\\n', 'Epoch 3: train_loss=0.4324,\ntrain_acc=0.849 | val_loss=0.5727, val_acc=0.720 | RFS=0.412', '\\n', 'Epoch 4:\ntrain_loss=0.3009, train_acc=0.948 | val_loss=0.5722, val_acc=0.746 |\nRFS=0.458', '\\n', 'Epoch 5: train_loss=0.2098, train_acc=0.935 |\nval_loss=0.6643, val_acc=0.766 | RFS=0.490', '\\n', 'Epoch 6: train_loss=0.1624,\ntrain_acc=0.953 | val_loss=0.7531, val_acc=0.762 | RFS=0.478', '\\n', 'Epoch 7:\ntrain_loss=0.1307, train_acc=0.965 | val_loss=0.8187, val_acc=0.764 |\nRFS=0.480', '\\n', 'Epoch 8: train_loss=0.1083, train_acc=0.969 |\nval_loss=0.8595, val_acc=0.776 | RFS=0.496', '\\n', 'Epoch 9: train_loss=0.0907,\ntrain_acc=0.974 | val_loss=0.8979, val_acc=0.778 | RFS=0.482', '\\n', 'Epoch 10:\ntrain_loss=0.0800, train_acc=0.977 | val_loss=0.9330, val_acc=0.784 |\nRFS=0.488', '\\n', '\\nTest: loss=0.9244, acc=0.784, RuleFidelity=0.490', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n12/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Loaded splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size (incl. PAD):', ' ', '10', '\\n', 'Classes:', ' ', '2', '\\n',\n'Epoch 01: train_loss=0.7092, train_acc=0.498 | val_loss=0.6923, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 02: train_loss=0.6941, train_acc=0.500 |\nval_loss=0.6927, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 03:\ntrain_loss=0.6957, train_acc=0.491 | val_loss=0.6947, val_acc=0.480 |\nRuleFid=0.000', '\\n', 'Epoch 04: train_loss=0.6940, train_acc=0.506 |\nval_loss=0.6924, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 05:\ntrain_loss=0.6935, train_acc=0.502 | val_loss=0.6950, val_acc=0.480 |\nRuleFid=0.000', '\\n', 'Epoch 06: train_loss=0.6947, train_acc=0.500 |\nval_loss=0.6928, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 07:\ntrain_loss=0.6937, train_acc=0.500 | val_loss=0.6929, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 08: train_loss=0.6933, train_acc=0.500 |\nval_loss=0.6932, val_acc=0.480 | RuleFid=0.000', '\\n', 'Epoch 09:\ntrain_loss=0.6934, train_acc=0.484 | val_loss=0.6934, val_acc=0.480 |\nRuleFid=0.000', '\\n', 'Epoch 10: train_loss=0.6934, train_acc=0.492 |\nval_loss=0.6927, val_acc=0.520 | RuleFid=1.000', '\\n', 'Epoch 11:\ntrain_loss=0.6934, train_acc=0.495 | val_loss=0.6931, val_acc=0.520 |\nRuleFid=1.000', '\\n', 'Epoch 12: train_loss=0.6939, train_acc=0.501 |\nval_loss=0.6937, val_acc=0.480 | RuleFid=0.000', '\\n', '\\nTest: loss=0.6932,\nacc=0.502, RuleFid=0.000', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', \"Class 0 rule chars: ['\u25c6', 'y', ' ']\",\n'\\n', \"Class 1 rule chars: ['b', ' ', 'g']\", '\\n', 'Execution time: 4 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.7258, train_acc=0.531 | val_loss=0.6675, val_acc=0.500 |\nRFS=0.964', '\\n', 'Epoch 2: train_loss=0.5030, train_acc=0.785 |\nval_loss=0.5864, val_acc=0.718 | RFS=0.598', '\\n', 'Epoch 3: train_loss=0.3522,\ntrain_acc=0.938 | val_loss=0.5666, val_acc=0.746 | RFS=0.558', '\\n', 'Epoch 4:\ntrain_loss=0.2228, train_acc=0.950 | val_loss=0.6560, val_acc=0.754 |\nRFS=0.534', '\\n', 'Epoch 5: train_loss=0.1558, train_acc=0.958 |\nval_loss=0.7902, val_acc=0.754 | RFS=0.530', '\\n', 'Epoch 6: train_loss=0.1220,\ntrain_acc=0.966 | val_loss=0.8836, val_acc=0.766 | RFS=0.506', '\\n', 'Epoch 7:\ntrain_loss=0.1056, train_acc=0.971 | val_loss=0.9521, val_acc=0.772 |\nRFS=0.492', '\\n', 'Epoch 8: train_loss=0.0954, train_acc=0.972 |\nval_loss=0.9965, val_acc=0.774 | RFS=0.486', '\\n', 'Epoch 9: train_loss=0.0818,\ntrain_acc=0.976 | val_loss=1.0551, val_acc=0.776 | RFS=0.472', '\\n', 'Epoch 10:\ntrain_loss=0.0734, train_acc=0.978 | val_loss=1.0887, val_acc=0.780 |\nRFS=0.484', '\\n', '\\nTest: loss=1.0875, acc=0.779, RuleFidelity=0.489', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n11/working', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.7679, train_acc=0.540 | val_loss=0.6982, val_acc=0.520 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.5532, train_acc=0.763 |\nval_loss=0.5841, val_acc=0.748 | RFS=0.580', '\\n', 'Epoch 3: train_loss=0.4018,\ntrain_acc=0.948 | val_loss=0.5496, val_acc=0.770 | RFS=0.506', '\\n', 'Epoch 4:\ntrain_loss=0.2640, train_acc=0.957 | val_loss=0.5898, val_acc=0.772 |\nRFS=0.492', '\\n', 'Epoch 5: train_loss=0.1722, train_acc=0.959 |\nval_loss=0.7062, val_acc=0.770 | RFS=0.490', '\\n', 'Epoch 6: train_loss=0.1262,\ntrain_acc=0.966 | val_loss=0.8272, val_acc=0.776 | RFS=0.496', '\\n', 'Epoch 7:\ntrain_loss=0.1000, train_acc=0.971 | val_loss=0.9109, val_acc=0.778 |\nRFS=0.494', '\\n', 'Epoch 8: train_loss=0.0860, train_acc=0.974 |\nval_loss=0.9668, val_acc=0.784 | RFS=0.496', '\\n', 'Epoch 9: train_loss=0.0756,\ntrain_acc=0.977 | val_loss=1.0126, val_acc=0.786 | RFS=0.494', '\\n', 'Epoch 10:\ntrain_loss=0.0654, train_acc=0.983 | val_loss=1.0517, val_acc=0.786 |\nRFS=0.494', '\\n', '\\nTest: loss=1.0565, acc=0.788, RuleFidelity=0.488', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n13/working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n', 'Vocab', ' ', '9', ' ', 'classes', ' ', '2', '\\n',\n'Epoch 1: train_loss=0.7722, train_acc=0.558 | val_loss=0.7278, val_acc=0.480 |\nRFS=1.000', '\\n', 'Epoch 2: train_loss=0.5505, train_acc=0.711 |\nval_loss=0.5932, val_acc=0.730 | RFS=0.598', '\\n', 'Epoch 3: train_loss=0.3936,\ntrain_acc=0.948 | val_loss=0.5441, val_acc=0.774 | RFS=0.530', '\\n', 'Epoch 4:\ntrain_loss=0.2504, train_acc=0.971 | val_loss=0.5870, val_acc=0.778 |\nRFS=0.518', '\\n', 'Epoch 5: train_loss=0.1568, train_acc=0.967 |\nval_loss=0.7186, val_acc=0.784 | RFS=0.504', '\\n', 'Epoch 6: train_loss=0.1123,\ntrain_acc=0.971 | val_loss=0.8522, val_acc=0.784 | RFS=0.504', '\\n', 'Epoch 7:\ntrain_loss=0.0915, train_acc=0.972 | val_loss=0.9508, val_acc=0.784 |\nRFS=0.504', '\\n', 'Epoch 8: train_loss=0.0772, train_acc=0.978 |\nval_loss=1.0128, val_acc=0.788 | RFS=0.504', '\\n', 'Epoch 9: train_loss=0.0680,\ntrain_acc=0.982 | val_loss=1.0600, val_acc=0.788 | RFS=0.504', '\\n', 'Epoch 10:\ntrain_loss=0.0604, train_acc=0.983 | val_loss=1.1089, val_acc=0.782 |\nRFS=0.490', '\\n', '\\nTest: loss=1.1226, acc=0.788, RuleFidelity=0.496', '\\n',\n'Experiment data saved to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_02-43-\n53_interpretable_neural_rule_learning_attempt_0/0-run/process_ForkProcess-\n12/working', '\\n', 'Execution time: 4 seconds seconds (time limit is 30\nminutes).']", ""], "analysis": ["The execution of the training script was successful, and no bugs were detected.\nThe model was trained on the SPR_BENCH dataset, achieving a final test accuracy\nof 77.3% and a Rule-Based Accuracy (RBA) of 66.2%. The experiment data was\nsuccessfully saved for further analysis. The implementation demonstrates basic\nfunctional correctness and meets the goals of the preliminary sub-stage.", "The training script executed successfully without any bugs. The model achieved a\ntest accuracy of 78.0%, which is close to the state-of-the-art (80.0%) on the\nSPR_BENCH benchmark. Rule Fidelity (RFS) was also measured, with a final value\nof 0.490, indicating moderate alignment between the rule-based and combined\npredictions. All experiment data was saved successfully for further analysis. No\nissues were observed in the implementation or execution.", "", "The model failed to achieve meaningful learning during training. Both training\nand validation accuracies hovered around 50%, which is equivalent to random\nguessing for a binary classification task. Additionally, the Rule Fidelity\n(RuleFid) metric remained at 1.000 throughout, suggesting that the rule-based\npredictions perfectly matched the model's predictions, but this did not\ntranslate to actual classification performance. This indicates that the model's\narchitecture or initialization might not be suitable for learning the task.\nProposed Fix: 1. Investigate the dataset to ensure proper preprocessing and\nencoding steps. 2. Experiment with a more complex model architecture that can\nbetter capture the relationships in the data. 3. Adjust hyperparameters such as\nlearning rate or embedding dimensions. 4. Use a different initialization\nstrategy for weights. 5. Consider debugging the rule-based layer to ensure it is\nfunctioning as intended.", "", "The model's training and validation accuracy are stuck around 50%, which is\nclose to random guessing for a binary classification task. This indicates that\nthe model is not learning meaningful patterns from the data. Additionally, the\nRule Fidelity metric is consistently at 1.000, which might be due to the\nsimplistic rule-based predictions always aligning with the model's outputs, but\nthis doesn't indicate actual learning.  Potential issues: 1. The dataset might\nbe too small (2000 training samples) for the model to learn effectively. 2. The\nmodel architecture (MaxPoolCharClassifier) might be too simplistic for the task.\n3. The learning rate or other hyperparameters might not be optimal. 4. There\nmight be issues with the dataset encoding or preprocessing.  Proposed fixes: 1.\nIncrease the dataset size or use data augmentation techniques. 2. Experiment\nwith more complex model architectures, such as adding recurrent or transformer\nlayers. 3. Tune the hyperparameters, especially the learning rate. 4. Verify the\ndataset encoding and preprocessing steps to ensure they are correct.", "The training script executed successfully without any errors or bugs. The model\nachieved a test accuracy of 77.7%, which is close to the state-of-the-art\nbenchmark of 80.0% for the SPR_BENCH dataset. Additionally, the rule fidelity\nmetric was calculated, and indicative characters for each class were extracted,\ndemonstrating the interpretability aspect of the model. The experiment data was\nsaved successfully for future analysis. Overall, the implementation is\nfunctional and meets the objectives of the first attempt.", "", "The training script for the interpretable neural rule learning model did not\nachieve meaningful learning. The training and validation accuracies remained\nclose to 50%, indicating that the model is failing to learn the task.\nAdditionally, the Rule Fidelity (RuleFid) metric alternates between 1.000 and\n0.000, which suggests instability in the rule-based predictions. This could be\ndue to issues such as insufficient model complexity, improper rule extraction\nlogic, or a lack of meaningful features in the dataset.  Proposed Fix: 1.\nIncrease the embedding dimension (EMBED_DIM) and experiment with deeper\narchitectures to improve model capacity. 2. Revisit the rule extraction logic to\nensure that the rules are updated meaningfully and are being applied correctly.\n3. Check the dataset for any potential issues, such as incorrect labels or\ninsufficient diversity in the sequences. 4. Experiment with different learning\nrates and optimization strategies to stabilize training. 5. Add monitoring for\nthe distribution of rule-based predictions to debug the RuleFid instability.", "", "The execution of the training script was successful, with the model achieving a\ntest accuracy of 78.8% and Rule Fidelity of 0.488. While the accuracy is close\nto the SOTA benchmark of 80.0%, the Rule Fidelity metric indicates room for\nimprovement in the interpretability of the learned rules. No bugs were\nidentified in the execution process.", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9335, "best_value": 0.9335}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.764, "best_value": 0.764}]}, {"metric_name": "rule-based accuracy", "lower_is_better": false, "description": "The accuracy of the rule-based approach for the dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.662, "best_value": 0.662}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.647695, "best_value": 0.647695}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.666328, "best_value": 0.666328}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.773, "best_value": 0.773}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9635, "best_value": 0.9635}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.774, "best_value": 0.774}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1231, "best_value": 0.1231}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5699, "best_value": 0.5699}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.78, "best_value": 0.78}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.953, "best_value": 0.953}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.76, "best_value": 0.76}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.902, "best_value": 0.902}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.64244, "best_value": 0.64244}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.664509, "best_value": 0.664509}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.52, "best_value": 0.52}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The consistency of the model's rules with the expected behavior.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6945, "best_value": 0.6945}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6924, "best_value": 0.6924}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.498, "best_value": 0.498}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.938, "best_value": 0.938}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.746, "best_value": 0.746}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the model's rules.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.958, "best_value": 0.958}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6909, "best_value": 0.6909}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6893, "best_value": 0.6893}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.764, "best_value": 0.764}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Final accuracy on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.48, "best_value": 0.48}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Final fidelity of the rules generated.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6937365822792053, "best_value": 0.6937365822792053}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6932250261306763, "best_value": 0.6932250261306763}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.502, "best_value": 0.502}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9425, "best_value": 0.9425}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1975, "best_value": 0.1975}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.764, "best_value": 0.764}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5572, "best_value": 0.5572}]}, {"metric_name": "validation Rule_Fidelity", "lower_is_better": false, "description": "Rule fidelity on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.65, "best_value": 0.65}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.777, "best_value": 0.777}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9765, "best_value": 0.9765}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Indicates how closely the rules align with the model's decisions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Measures the loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.08, "best_value": 0.08}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5722, "best_value": 0.5722}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.784, "best_value": 0.784}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.501, "best_value": 0.501}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.48, "best_value": 0.48}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6937, "best_value": 0.6937}]}, {"metric_name": "Rule Fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.502, "best_value": 0.502}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9775, "best_value": 0.9775}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.78, "best_value": 0.78}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "The fidelity of the rules generated by the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.964, "best_value": 0.964}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0734, "best_value": 0.0734}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5666, "best_value": 0.5666}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.779, "best_value": 0.779}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correct predictions.", "data": [{"dataset_name": "train", "final_value": 0.9825, "best_value": 0.9825}, {"dataset_name": "validation", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "test", "final_value": 0.788, "best_value": 0.788}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Measures how well the rules align with the model's predictions.", "data": [{"dataset_name": "train", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error rate of the model.", "data": [{"dataset_name": "train", "final_value": 0.0654, "best_value": 0.0654}, {"dataset_name": "validation", "final_value": 0.5496, "best_value": 0.5496}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified instances.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.788, "best_value": 0.788}]}, {"metric_name": "rule fidelity", "lower_is_better": false, "description": "Measures the alignment of the rule predictions with the actual predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or deviation from the true values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5441, "best_value": 0.5441}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_rba_vs_val.png", "../../logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_val_vs_rulefid.png", "../../logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_val_vs_fidelity.png"], [], ["../../logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_val_vs_fidelity.png", "../../logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/comparison_final_val_accuracy.png"], [], ["../../logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_rulefid_vs_val.png", "../../logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_rule_fidelity.png", "../../logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_a8aaa73f686043cf93e2d7f9127bea11/SPR_BENCH_agg_accuracy_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_a8aaa73f686043cf93e2d7f9127bea11/SPR_BENCH_agg_rule_fidelity.png", "../../logs/0-run/experiment_results/seed_aggregation_a8aaa73f686043cf93e2d7f9127bea11/SPR_BENCH_agg_confusion_matrix.png"]], "plot_paths": [["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_rba_vs_val.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_val_vs_rulefid.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_val_vs_fidelity.png"], [], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_val_vs_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/comparison_final_val_accuracy.png"], [], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_rulefid_vs_val.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a8aaa73f686043cf93e2d7f9127bea11/SPR_BENCH_agg_accuracy_curves.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a8aaa73f686043cf93e2d7f9127bea11/SPR_BENCH_agg_rule_fidelity.png", "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/seed_aggregation_a8aaa73f686043cf93e2d7f9127bea11/SPR_BENCH_agg_confusion_matrix.png"]], "plot_analyses": [[{"analysis": "The training accuracy increases steadily and reaches above 90% by the 10th epoch, indicating that the model is learning the task effectively on the training data. However, the validation accuracy shows a less pronounced improvement, plateauing around 80%. This suggests potential overfitting, as the model performs better on the training data than on unseen validation data.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The training loss decreases consistently, showing that the model is effectively minimizing the objective function on the training data. The validation loss also decreases but at a slower rate, which aligns with the validation accuracy plateau observed earlier. This supports the potential overfitting hypothesis.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_loss_curves.png"}, {"analysis": "Validation accuracy outperforms the rule-based accuracy (RBA) starting from the 5th epoch, indicating that the neural model is learning patterns beyond the baseline rules. However, the fluctuations in validation accuracy suggest that the model's generalization ability may still need improvement.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_rba_vs_val.png"}, {"analysis": "The confusion matrix shows that the model performs reasonably well on the test set, with a high number of correct predictions (diagonal elements). However, there is still room for improvement as some misclassifications are evident, particularly in one of the classes.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_95dbf5af5c80492d9c949248fc35830e_proc_3198575/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot demonstrates the training and validation accuracy over 8 epochs. The training accuracy increases steadily and reaches close to 1.0, indicating that the model has learned the training data effectively. However, the validation accuracy plateaus around 0.8 after epoch 4, suggesting that the model's performance on unseen data has reached its limit. The gap between training and validation accuracy after epoch 4 may indicate some overfitting.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot shows the training and validation loss over 8 epochs. The training loss decreases consistently, reaching a very low value by the final epoch, which aligns with the high training accuracy observed. However, the validation loss decreases initially but starts increasing after epoch 4, indicating overfitting. This suggests that the model is memorizing the training data rather than generalizing.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares validation accuracy with rule fidelity over 8 epochs. Validation accuracy improves initially and stabilizes around 0.8, while rule fidelity drops significantly in the first few epochs and then slightly recovers but remains low. This indicates a trade-off between achieving high classification accuracy and maintaining interpretable rule fidelity. The model may prioritize accuracy over interpretability.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_val_vs_rulefid.png"}, {"analysis": "The confusion matrix for the test set shows the distribution of predicted versus true labels. The diagonal elements are darker, indicating a higher number of correct predictions. The off-diagonal elements are lighter, suggesting fewer misclassifications. This indicates the model performs well on the test set, with most predictions aligning with the ground truth.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_d279bfe788eb421988fd6aa1ac262f05_proc_3211630/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation accuracy over epochs. Training accuracy improves steadily and reaches over 90%, while validation accuracy stabilizes around 75-80% after early fluctuations. This indicates that the model is learning but may be overfitting slightly, as the gap between training and validation accuracy widens in later epochs.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The plot depicts the training and validation loss over epochs. Both losses decrease consistently, with training loss declining more sharply. Validation loss decreases at a slower rate, suggesting the model is improving but may face challenges generalizing to unseen data. The consistent decline in loss indicates stable training.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates rule fidelity over epochs. Rule fidelity improves significantly in the initial epochs, peaking early, but fluctuates in subsequent epochs. The fluctuations suggest that while the model can learn interpretable rules, its ability to consistently maintain high fidelity varies during training.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_rule_fidelity.png"}, {"analysis": "This plot compares validation accuracy with rule fidelity over epochs. Validation accuracy stabilizes around 75-80%, while rule fidelity remains higher, fluctuating around 85-90%. The disparity suggests that the model's interpretability (rule fidelity) is strong but does not directly translate to improved validation performance, indicating room for optimization in aligning interpretability and accuracy.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_be31c82c8862473ebb886a8dc8095838_proc_3211631/SPR_BENCH_val_vs_fidelity.png"}], [], [{"analysis": "The plot shows the training and validation accuracy over 12 epochs. Training accuracy increases rapidly and stabilizes around 0.95, indicating that the model is learning the training data effectively. Validation accuracy peaks at around 0.8 and remains stable, suggesting that the model generalizes well to unseen data and achieves state-of-the-art performance on the SPR_BENCH task.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot depicts the cross-entropy loss for both training and validation datasets over 12 epochs. Training loss decreases steadily, indicating effective learning. Validation loss also decreases, though at a slower rate, suggesting that the model is not overfitting and maintains good generalization.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares validation accuracy with rule fidelity over the epochs. Validation accuracy stabilizes around 0.8, while rule fidelity starts high and decreases slightly, leveling off. This suggests a trade-off between rule fidelity and accuracy, with the model balancing interpretability and performance.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_val_vs_fidelity.png"}, {"analysis": "The confusion matrix shows the model's performance on the test set. The diagonal entries indicate the number of correct predictions, while off-diagonal entries represent misclassifications. The model appears to perform well, with the majority of predictions concentrated on the diagonal, indicating high accuracy.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/SPR_BENCH_confusion_matrix.png"}, {"analysis": "This bar chart compares the final validation accuracy of the model across datasets. The model achieves a final validation accuracy of approximately 0.75 on the SPR_BENCH dataset, which is slightly below the state-of-the-art benchmark of 0.8, indicating room for improvement.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_1c19417c63154a0ab744add9f67f5bee_proc_3211633/comparison_final_val_accuracy.png"}], [], [{"analysis": "The training accuracy steadily increases and plateaus close to 100%, indicating that the model fits the training data well. However, the validation accuracy plateaus around 80% after epoch 5, suggesting potential overfitting. The gap between training and validation accuracy highlights the need for regularization or other techniques to improve generalization.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The training loss decreases consistently, showing effective learning on the training data. However, the validation loss decreases initially but starts increasing after epoch 5, suggesting overfitting. This behavior indicates that the model starts memorizing the training data rather than generalizing.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_loss_curves.png"}, {"analysis": "Validation accuracy increases steadily and stabilizes around 80%, which matches the state-of-the-art performance. Rule fidelity also increases initially but starts declining after epoch 4. This suggests that while the model achieves high accuracy, its ability to generate interpretable rules deteriorates as training progresses, which might require balancing interpretability and accuracy.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_rulefid_vs_val.png"}, {"analysis": "The confusion matrix shows that the model performs well on both classes, with a balanced number of correct predictions for each. However, there are still some misclassifications, indicating room for improvement in the model's decision boundary or feature representation.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_8181bef7bf9a479ca2d852b9b163bd24_proc_3211633/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation accuracy over epochs. The training accuracy increases steadily, reaching close to 100%, indicating that the model is learning effectively on the training data. However, the validation accuracy plateaus around 80% after an initial rise, suggesting potential overfitting as the gap between training and validation accuracy widens.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot displays the training and validation loss over epochs. The training loss decreases steadily, demonstrating that the model is optimizing on the training data. However, the validation loss begins to increase after an initial decrease, which is a classic sign of overfitting. The model fails to generalize well to unseen data beyond a certain point.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot depicts the rule fidelity over epochs. Rule fidelity starts at a high value, indicating that the model initially adheres well to the extracted rules. However, it drops significantly by the second epoch and remains relatively low and stable afterward. This suggests that the model's rule-based interpretability diminishes as training progresses, potentially due to overfitting to the data rather than adhering to interpretable rules.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_rule_fidelity.png"}, {"analysis": "This confusion matrix for the test set shows that the model performs well in distinguishing between the two classes, with a strong diagonal indicating correct predictions. However, the exact counts of false positives and false negatives are not visible in this heatmap, making it hard to assess specific areas of misclassification.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_c9210a1fbb4d478194939a388c3eed1a_proc_3211631/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The training accuracy improves steadily and reaches near 100% by the final epoch, indicating that the model is capable of learning the training data effectively. However, the validation accuracy plateaus around 80%, suggesting the model is reaching the benchmark's state-of-the-art accuracy but may be overfitting to the training data. This highlights a potential need for additional regularization or data augmentation to improve generalization.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The training loss decreases consistently, reflecting the model's ability to minimize error on the training set. However, the validation loss begins to increase after an initial decrease, suggesting overfitting as the model starts to memorize the training data rather than generalizing to unseen data. Early stopping or regularization techniques could help mitigate this issue.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_loss_curves.png"}, {"analysis": "Rule fidelity decreases sharply after the first epoch and stabilizes at around 0.5. This indicates that while the model is learning to classify sequences accurately, it may not be maintaining high fidelity in the explicit rule representations. Improvements in the rule-based layer design or additional constraints could help enhance fidelity while preserving accuracy.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix reveals that the model performs well overall, but there may be some misclassifications. The darker diagonal indicates a high number of correct predictions, while lighter off-diagonal cells suggest areas where the model struggles. Further analysis of these misclassifications could provide insights into specific sequence patterns or rule complexities that the model finds challenging.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation accuracy over 10 epochs. The training accuracy quickly increases and plateaus near 1.0, indicating the model is fitting the training data well. Validation accuracy starts to plateau around 0.8, suggesting the model is performing well on unseen data but may be slightly overfitting as the gap between training and validation accuracy widens after epoch 4.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot depicts the training and validation loss over 10 epochs. The training loss decreases steadily, indicating effective learning on the training data. However, the validation loss begins to increase after epoch 4, suggesting overfitting as the model starts to perform worse on the validation set despite improving on the training set.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_loss_curves.png"}, {"analysis": "The plot shows rule fidelity over epochs. Rule fidelity drops sharply after the first epoch and stabilizes around 0.5. This suggests that while the model is learning to classify sequences accurately, the explicit rule representations are becoming less faithful to the true underlying rules as training progresses. This could indicate a trade-off between accuracy and interpretability.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix for the test set shows the distribution of predictions versus ground truth. The diagonal dominance indicates good overall performance, but there are noticeable misclassifications. Further analysis is needed to understand the specific errors and whether they are due to systematic issues or inherent ambiguity in the data.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the training and validation accuracy over 10 epochs. The training accuracy rapidly increases and approaches near-perfect accuracy by epoch 3, which suggests that the model is highly capable of fitting the training data. However, the validation accuracy plateaus around 80% after epoch 3 and does not improve further, indicating that the model may be overfitting to the training data and not generalizing well to unseen validation data. This trend suggests the need for regularization techniques or additional tuning to improve generalization.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot depicts the training and validation loss over 10 epochs. The training loss consistently decreases, indicating that the model is learning effectively on the training data. However, the validation loss starts to increase after epoch 4, which is a classic sign of overfitting. The divergence between training and validation loss further supports the observation that the model is not generalizing well to the validation data. Early stopping or other regularization methods could help mitigate this issue.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_loss_curves.png"}, {"analysis": "The rule fidelity metric decreases significantly after the first epoch and stabilizes around 50% from epoch 4 onwards. This suggests that the learned rules do not align well with the true underlying rules of the dataset as training progresses. The decline in rule fidelity could indicate that the model is prioritizing accuracy over interpretability, which is contrary to the research goal of achieving interpretable rule learning. Adjustments to the model architecture or loss function may be necessary to better balance accuracy with interpretability.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_rule_fidelity.png"}, {"analysis": "The confusion matrix for the test set shows that the model performs reasonably well, with a high number of correct predictions for both classes. However, there are some misclassifications, as indicated by the off-diagonal elements. The model's performance on the test set aligns with the plateau observed in validation accuracy, suggesting that the model's generalization capabilities are limited and need to be improved to achieve state-of-the-art performance.", "plot_path": "experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The experimental results indicate that the model is learning effectively on the\ntraining data but shows signs of overfitting as the validation performance\nplateaus. The neural model surpasses the rule-based baseline in validation\naccuracy, demonstrating its capability to learn complex patterns. However,\nfurther efforts are needed to improve generalization and reduce\nmisclassifications, as indicated by the confusion matrix.", "The plots reveal that the model achieves high training accuracy but suffers from\noverfitting, as seen in the divergence of validation loss and accuracy after\nepoch 4. There is a trade-off between accuracy and interpretability, with rule\nfidelity being compromised for higher validation accuracy. The confusion matrix\nhighlights good overall performance on the test set, with minimal\nmisclassifications.", "The plots indicate that the model shows promising results in both accuracy and\ninterpretability. Training is stable, but there are signs of overfitting and\nfluctuations in rule fidelity. Validation accuracy is close to the SOTA\nbenchmark, and rule fidelity is high, but further optimization is needed to\nalign interpretability with performance.", "[]", "The plots indicate that the model performs well on the SPR_BENCH task, achieving\nhigh training and validation accuracy. However, the final validation accuracy\nslightly underperforms the state-of-the-art benchmark. The results suggest a\ngood balance between interpretability and performance, as seen in the rule\nfidelity analysis. Further optimization may help surpass the benchmark.", "[]", "The results indicate that the model achieves high accuracy, matching the state-\nof-the-art performance on the SPR_BENCH benchmark. However, overfitting is\nevident from the divergence between training and validation metrics,\nparticularly after epoch 5. Additionally, while the model achieves good\nperformance, its rule fidelity diminishes as training progresses, highlighting a\ntrade-off between accuracy and interpretability. Future work should focus on\naddressing overfitting and improving the balance between accuracy and rule\nfidelity.", "The plots indicate that while the model achieves high training accuracy, it\nsuffers from overfitting, as evidenced by the widening gap between training and\nvalidation accuracy and the increasing validation loss. Rule fidelity diminishes\nover epochs, suggesting a trade-off between interpretability and performance.\nThe confusion matrix shows strong performance but lacks detailed\nmisclassification insights.", "[]", "The plots indicate that the model achieves state-of-the-art accuracy on the\nbenchmark but suffers from overfitting and suboptimal rule fidelity. While the\nclassification performance is strong, further work is needed to enhance\ninterpretability and generalization.", "The plots provide insights into the model's performance and interpretability.\nWhile the model achieves high accuracy, there are signs of overfitting and a\ndecline in rule fidelity, suggesting a trade-off between accuracy and\ninterpretability. The confusion matrix highlights areas for further analysis to\nimprove classification performance.", "The plots indicate that the model achieves high training accuracy but struggles\nwith generalization, as evidenced by the plateau in validation accuracy and\nincreasing validation loss. Rule fidelity is low, suggesting poor\ninterpretability of the learned rules. The confusion matrix reveals reasonable\nbut suboptimal classification performance on the test set. Regularization\ntechniques, adjustments to the model architecture, and a better balance between\naccuracy and interpretability are recommended for improvement.", "[]"], "exec_time": [2.9251508712768555, 3.5280396938323975, 3.4776408672332764, 4.601354122161865, 4.125895738601685, 5.6067726612091064, 3.474526882171631, 3.7457423210144043, 4.416305780410767, 3.9999475479125977, 4.052133321762085, 4.147414684295654, null], "exec_time_feedback": ["", "", "", "", "Implementation works but runs too quickly (0.06 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["['All datasets processed in the code']"], ["[]"], [], ["['SPR_BENCH']"], [], ["['SPR_BENCH']"], ["[]"], [], ["['All datasets processed in the code']"], ["[\"experiment_data\"]"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----- paths -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load data -----\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"SPR_BENCH\", {})\n\n\n# Helper to safely fetch arrays\ndef get(arr, key):\n    return arr.get(key, [])\n\n\nmetrics = data.get(\"metrics\", {})\nlosses = data.get(\"losses\", {})\n\n# Plot 1: Accuracy curves\ntry:\n    train_acc = np.array(get(metrics, \"train_acc\"))\n    val_acc = np.array(get(metrics, \"val_acc\"))\n    if train_acc.size and val_acc.size:\n        plt.figure()\n        epochs = np.arange(1, len(train_acc) + 1)\n        plt.plot(epochs, train_acc, label=\"Train Acc\")\n        plt.plot(epochs, val_acc, label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Train vs Validation Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot 2: Loss curves\ntry:\n    train_loss = np.array(get(losses, \"train\"))\n    val_loss = np.array(get(losses, \"val\"))\n    if train_loss.size and val_loss.size:\n        plt.figure()\n        epochs = np.arange(1, len(train_loss) + 1)\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Train vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot 3: RBA vs Validation Accuracy\ntry:\n    rba = np.array(get(metrics, \"RBA\"))\n    val_acc = np.array(get(metrics, \"val_acc\"))\n    if rba.size and val_acc.size:\n        plt.figure()\n        epochs = np.arange(1, len(rba) + 1)\n        plt.plot(epochs, val_acc, label=\"Val Acc\")\n        plt.plot(epochs, rba, label=\"RBA\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Validation Accuracy vs Rule-Based Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_rba_vs_val.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating RBA plot: {e}\")\n    plt.close()\n\n# Plot 4: Confusion Matrix\ntry:\n    preds = np.array(data.get(\"predictions\", []))\n    gts = np.array(data.get(\"ground_truth\", []))\n    if preds.size and gts.size and preds.shape == gts.shape:\n        n_cls = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((n_cls, n_cls), dtype=int)\n        for p, t in zip(preds, gts):\n            cm[t, p] += 1\n        plt.figure(figsize=(6, 5))\n        plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar()\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"SPR_BENCH: Confusion Matrix (Test Set)\")\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ----- evaluation metric -----\nif \"predictions\" in data and \"ground_truth\" in data:\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n    if preds.size and preds.shape == gts.shape:\n        test_acc = (preds == gts).mean()\n        print(f\"Test Accuracy: {test_acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----- paths -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load data -----\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# helper\ndef safe(arr, key):\n    return np.array(arr.get(key, []))\n\n\nall_val_acc = {}\n# -------- per-dataset plots -------- #\nfor dname, data in experiment_data.items():\n    metrics, losses = data.get(\"metrics\", {}), data.get(\"losses\", {})\n    train_acc, val_acc = safe(metrics, \"train_acc\"), safe(metrics, \"val_acc\")\n    train_loss, val_loss = safe(losses, \"train\"), safe(losses, \"val\")\n    rule_fid = safe(metrics, \"Rule_Fidelity\")\n    preds, gts = np.array(data.get(\"predictions\", [])), np.array(\n        data.get(\"ground_truth\", [])\n    )\n    all_val_acc[dname] = val_acc\n\n    # Plot 1: accuracy\n    try:\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            ep = np.arange(1, len(train_acc) + 1)\n            plt.plot(ep, train_acc, label=\"Train Acc\")\n            plt.plot(ep, val_acc, label=\"Val Acc\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Train vs Validation Accuracy\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_accuracy_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error accuracy plot for {dname}: {e}\")\n        plt.close()\n\n    # Plot 2: loss\n    try:\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            ep = np.arange(1, len(train_loss) + 1)\n            plt.plot(ep, train_loss, label=\"Train Loss\")\n            plt.plot(ep, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dname}: Train vs Validation Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error loss plot for {dname}: {e}\")\n        plt.close()\n\n    # Plot 3: rule fidelity vs val acc\n    try:\n        if rule_fid.size and val_acc.size:\n            plt.figure()\n            ep = np.arange(1, len(rule_fid) + 1)\n            plt.plot(ep, val_acc, label=\"Val Acc\")\n            plt.plot(ep, rule_fid, label=\"Rule Fidelity\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Validation Accuracy vs Rule Fidelity\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_val_vs_rulefid.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error rule fidelity plot for {dname}: {e}\")\n        plt.close()\n\n    # Plot 4: confusion matrix\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{dname}: Confusion Matrix (Test)\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error confusion matrix for {dname}: {e}\")\n        plt.close()\n\n    # Print test accuracy\n    if preds.size and gts.size and preds.shape == gts.shape:\n        print(f\"{dname} Test Accuracy: {(preds==gts).mean():.3f}\")\n\n# -------- comparison plot across datasets -------- #\ntry:\n    if len(all_val_acc) >= 2:\n        plt.figure()\n        for dname, v in all_val_acc.items():\n            if v.size:\n                plt.plot(np.arange(1, len(v) + 1), v, label=f\"{dname} Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"Dataset Comparison: Validation Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_val_accuracy.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error comparison plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef safe(arr_dict, key):  # returns np.ndarray (possibly size 0)\n    return np.array(arr_dict.get(key, []))\n\n\n# ---------- iterate over datasets ----------\nfor dset, dct in experiment_data.items():\n    metrics, losses = dct.get(\"metrics\", {}), dct.get(\"losses\", {})\n\n    # 1) Accuracy curves ------------------------------------------------------\n    try:\n        tr, va = safe(metrics, \"train_acc\"), safe(metrics, \"val_acc\")\n        if tr.size and va.size:\n            plt.figure()\n            ep = np.arange(1, len(tr) + 1)\n            plt.plot(ep, tr, label=\"Train Acc\")\n            plt.plot(ep, va, label=\"Val Acc\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dset}: Train vs Validation Accuracy\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_accuracy_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {dset}: {e}\")\n        plt.close()\n\n    # 2) Loss curves ----------------------------------------------------------\n    try:\n        tl, vl = safe(losses, \"train\"), safe(losses, \"val\")\n        if tl.size and vl.size:\n            plt.figure()\n            ep = np.arange(1, len(tl) + 1)\n            plt.plot(ep, tl, label=\"Train Loss\")\n            plt.plot(ep, vl, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # 3) Rule-fidelity curve --------------------------------------------------\n    try:\n        rf = safe(metrics, \"rule_fidelity\")\n        if rf.size:\n            plt.figure()\n            ep = np.arange(1, len(rf) + 1)\n            plt.plot(ep, rf, label=\"Rule Fidelity\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Fidelity\")\n            plt.title(f\"{dset}: Rule Fidelity Over Epochs\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_rule_fidelity.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating fidelity plot for {dset}: {e}\")\n        plt.close()\n\n    # 4) Fidelity vs Val-Accuracy --------------------------------------------\n    try:\n        rf, va = safe(metrics, \"rule_fidelity\"), safe(metrics, \"val_acc\")\n        if rf.size and va.size:\n            plt.figure()\n            ep = np.arange(1, len(rf) + 1)\n            plt.plot(ep, va, label=\"Val Acc\")\n            plt.plot(ep, rf, label=\"Rule Fidelity\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy / Fidelity\")\n            plt.title(f\"{dset}: Validation Accuracy vs Rule Fidelity\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_val_vs_fidelity.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating val-vs-fidelity plot for {dset}: {e}\")\n        plt.close()\n\n    # 5) Confusion matrix (test set) ------------------------------------------\n    try:\n        preds, gts = safe(dct, \"predictions\"), safe(dct, \"ground_truth\")\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{dset}: Confusion Matrix (Test Set)\")\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ---------- evaluation metric ----------\n    if \"predictions\" in dct and \"ground_truth\" in dct:\n        preds, gts = np.array(dct[\"predictions\"]), np.array(dct[\"ground_truth\"])\n        if preds.size and preds.shape == gts.shape:\n            acc = (preds == gts).mean()\n            print(f\"{dset} \u2013 Test Accuracy: {acc:.3f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----- paths -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----- load data -----\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe(arr, key):\n    return np.array(arr.get(key, []))\n\n\nfinal_val_accs = {}\n\nfor dset, blob in experiment_data.items():\n    metrics, losses = blob.get(\"metrics\", {}), blob.get(\"losses\", {})\n    # Plot 1: Accuracy curves\n    try:\n        tr_acc, vl_acc = safe(metrics, \"train_acc\"), safe(metrics, \"val_acc\")\n        if tr_acc.size and vl_acc.size:\n            epochs = np.arange(1, len(tr_acc) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_acc, label=\"Train Acc\")\n            plt.plot(epochs, vl_acc, label=\"Val Acc\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dset}: Train vs Validation Accuracy\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_accuracy_curves.png\"))\n            plt.close()\n            final_val_accs[dset] = vl_acc[-1]\n    except Exception as e:\n        print(f\"{dset}: accuracy plot error: {e}\")\n        plt.close()\n\n    # Plot 2: Loss curves\n    try:\n        tr_ls, vl_ls = safe(losses, \"train\"), safe(losses, \"val\")\n        if tr_ls.size and vl_ls.size:\n            epochs = np.arange(1, len(tr_ls) + 1)\n            plt.figure()\n            plt.plot(epochs, tr_ls, label=\"Train Loss\")\n            plt.plot(epochs, vl_ls, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"CE Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dset}: loss plot error: {e}\")\n        plt.close()\n\n    # Plot 3: Rule Fidelity vs Val Acc\n    try:\n        fid = safe(metrics, \"Rule_Fidelity\")\n        vl_acc = safe(metrics, \"val_acc\")\n        if fid.size and vl_acc.size:\n            epochs = np.arange(1, len(fid) + 1)\n            plt.figure()\n            plt.plot(epochs, vl_acc, label=\"Val Acc\")\n            plt.plot(epochs, fid, label=\"Rule Fidelity\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dset}: Validation Accuracy vs Rule Fidelity\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_val_vs_fidelity.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dset}: fidelity plot error: {e}\")\n        plt.close()\n\n    # Plot 4: Confusion matrix\n    try:\n        preds, gts = safe(blob, \"predictions\"), safe(blob, \"ground_truth\")\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{dset}: Confusion Matrix (Test Set)\")\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dset}: confusion plot error: {e}\")\n        plt.close()\n\n    # Print test accuracy if available\n    if preds.size and gts.size and preds.shape == gts.shape:\n        print(f\"{dset} Test Accuracy: {(preds==gts).mean():.3f}\")\n\n# Cross-dataset comparison bar chart\ntry:\n    if final_val_accs:\n        plt.figure()\n        names = list(final_val_accs.keys())\n        vals = [final_val_accs[k] for k in names]\n        plt.bar(names, vals)\n        plt.ylabel(\"Final Validation Accuracy\")\n        plt.title(\"Comparison of Final Validation Accuracy Across Datasets\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"comparison_final_val_accuracy.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Comparison plot error: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# ---------- helper ----------\ndef safe(arr, key):\n    return np.array(arr.get(key, []))\n\n\n# ---------- per-dataset plots ----------\nall_datasets = list(experiment_data.keys())\nval_acc_dict = {}\n\nfor dname, ddata in experiment_data.items():\n    metrics, losses = ddata.get(\"metrics\", {}), ddata.get(\"losses\", {})\n    train_acc, val_acc = safe(metrics, \"train_acc\"), safe(metrics, \"val_acc\")\n    train_loss, val_loss = safe(losses, \"train\"), safe(losses, \"val\")\n    rule_fid = safe(metrics, \"Rule_Fidelity\")\n    preds, gts = np.array(ddata.get(\"predictions\", [])), np.array(\n        ddata.get(\"ground_truth\", [])\n    )\n\n    # 1. Accuracy curves\n    try:\n        if train_acc.size and val_acc.size:\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.figure()\n            plt.plot(epochs, train_acc, label=\"Train Acc\")\n            plt.plot(epochs, val_acc, label=\"Val Acc\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{dname}: Train vs Validation Accuracy\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_accuracy_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dname}: error accuracy plot {e}\")\n        plt.close()\n\n    # 2. Loss curves\n    try:\n        if train_loss.size and val_loss.size:\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.figure()\n            plt.plot(epochs, train_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dname}: Train vs Validation Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dname}: error loss plot {e}\")\n        plt.close()\n\n    # 3. Rule Fidelity vs Val Acc\n    try:\n        if rule_fid.size and val_acc.size:\n            epochs = np.arange(1, len(rule_fid) + 1)\n            plt.figure()\n            plt.plot(epochs, val_acc, label=\"Val Acc\")\n            plt.plot(epochs, rule_fid, label=\"Rule Fidelity\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy / Fidelity\")\n            plt.title(f\"{dname}: Validation Accuracy vs Rule Fidelity\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dname}_rulefid_vs_val.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dname}: error rule fidelity plot {e}\")\n        plt.close()\n\n    # 4. Confusion Matrix\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{dname}: Confusion Matrix (Test)\")\n            plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"{dname}: error confusion matrix {e}\")\n        plt.close()\n\n    # Store val_acc for cross-dataset comparison\n    if val_acc.size:\n        val_acc_dict[dname] = val_acc\n\n    # Print evaluation metric\n    if preds.size and preds.shape == gts.shape:\n        print(f\"{dname} Test Accuracy: {(preds == gts).mean():.3f}\")\n\n# ---------- Cross-dataset comparison plot ----------\ntry:\n    if len(val_acc_dict) > 1:\n        plt.figure()\n        for dname, v in val_acc_dict.items():\n            epochs = np.arange(1, len(v) + 1)\n            plt.plot(epochs, v, label=f\"{dname}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Validation Accuracy\")\n        plt.title(\"Dataset Comparison: Validation Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_val_accuracy.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ---------------- iterate over datasets ----------------\nfor ds_name, ds_blob in experiment_data.items():\n    metrics = ds_blob.get(\"metrics\", {})\n    losses = ds_blob.get(\"losses\", {})\n    preds = np.array(ds_blob.get(\"predictions\", []))\n    gts = np.array(ds_blob.get(\"ground_truth\", []))\n\n    # -- Plot 1: Accuracy curves --\n    try:\n        train_acc = safe_get(metrics, \"train_acc\")\n        val_acc = safe_get(metrics, \"val_acc\")\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_acc) + 1)\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 2: Loss curves --\n    try:\n        train_loss = safe_get(losses, \"train\")\n        val_loss = safe_get(losses, \"val\")\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            epochs = np.arange(1, len(train_loss) + 1)\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{ds_name}: Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 3: Rule Fidelity curves --\n    try:\n        rf = safe_get(metrics, \"Rule_Fidelity\")\n        if rf.size:\n            plt.figure()\n            epochs = np.arange(1, len(rf) + 1)\n            plt.plot(epochs, rf, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Rule Fidelity over Epochs\")\n            fname = os.path.join(working_dir, f\"{ds_name}_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # -- Plot 4: Confusion matrix (Test) --\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            n_cls = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((n_cls, n_cls), dtype=int)\n            for p, t in zip(preds, gts):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{ds_name}: Confusion Matrix (Test)\")\n            fname = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # -------- evaluation metric printout --------\n    if preds.size and gts.size and preds.shape == gts.shape:\n        acc = (preds == gts).mean()\n        print(f\"{ds_name} Test Accuracy: {acc:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------- paths & load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# All experiment files provided in the prompt (relative to $AI_SCIENTIST_ROOT)\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_500cbfde893946b0af2d5e5124d82a11_proc_3211630/experiment_data.npy\",\n    \"experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_dbd07256943f4aa88286744ade4951d2_proc_3211632/experiment_data.npy\",\n    \"experiments/2025-08-17_02-43-53_interpretable_neural_rule_learning_attempt_0/logs/0-run/experiment_results/experiment_9639da53be0744838f7a198e6ce77643_proc_3211631/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor exp_path in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), exp_path)\n        exp_blob = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_blob)\n    except Exception as e:\n        print(f\"Error loading {exp_path}: {e}\")\n\n\ndef safe_get(dct, *keys):\n    cur = dct\n    for k in keys:\n        cur = cur.get(k, {})\n    return np.array(cur)\n\n\n# ------------- Collect all dataset names -------------\ndataset_names = set()\nfor run_blob in all_experiment_data:\n    dataset_names.update(run_blob.keys())\n\n# ------------- Aggregate & plot -------------\nfor ds_name in dataset_names:\n    # Gather data across runs for this dataset\n    runs_for_ds = [\n        run_blob.get(ds_name, {})\n        for run_blob in all_experiment_data\n        if ds_name in run_blob\n    ]\n\n    # Helper to stack metric curves (trim to min length)\n    def stack_metric(metric_key_tuple):\n        curves = []\n        for blob in runs_for_ds:\n            arr = safe_get(blob.get(\"metrics\", {}), *metric_key_tuple)\n            if arr.size:\n                curves.append(arr.astype(float))\n        if not curves:\n            return None  # metric absent in all runs\n        min_len = min(len(c) for c in curves)\n        curves = [c[:min_len] for c in curves]\n        stacked = np.vstack(curves)\n        mean = np.nanmean(stacked, axis=0)\n        stderr = np.nanstd(stacked, axis=0, ddof=0) / np.sqrt(stacked.shape[0])\n        return mean, stderr\n\n    # Accuracy curves ----------------------------------------------------------\n    try:\n        res = stack_metric((\"train_acc\",))\n        res_val = stack_metric((\"val_acc\",))\n        if res and res_val:\n            mean_train, se_train = res\n            mean_val, se_val = res_val\n            epochs = np.arange(1, len(mean_train) + 1)\n            plt.figure()\n            plt.plot(epochs, mean_train, label=\"Train (mean)\")\n            plt.fill_between(\n                epochs,\n                mean_train - se_train,\n                mean_train + se_train,\n                alpha=0.3,\n                label=\"Train \u00b1 SE\",\n            )\n            plt.plot(epochs, mean_val, label=\"Validation (mean)\")\n            plt.fill_between(\n                epochs,\n                mean_val - se_val,\n                mean_val + se_val,\n                alpha=0.3,\n                label=\"Val \u00b1 SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.title(f\"{ds_name}: Aggregated Train vs Validation Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_accuracy_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error aggregating accuracy for {ds_name}: {e}\")\n        plt.close()\n\n    # Loss curves --------------------------------------------------------------\n    try:\n        res_loss_tr = stack_metric((\"train\",))\n        res_loss_val = stack_metric((\"val\",))\n        if res_loss_tr and res_loss_val:\n            mean_tr, se_tr = res_loss_tr\n            mean_val, se_val = res_loss_val\n            epochs = np.arange(1, len(mean_tr) + 1)\n            plt.figure()\n            plt.plot(epochs, mean_tr, label=\"Train (mean)\")\n            plt.fill_between(\n                epochs, mean_tr - se_tr, mean_tr + se_tr, alpha=0.3, label=\"Train \u00b1 SE\"\n            )\n            plt.plot(epochs, mean_val, label=\"Validation (mean)\")\n            plt.fill_between(\n                epochs,\n                mean_val - se_val,\n                mean_val + se_val,\n                alpha=0.3,\n                label=\"Val \u00b1 SE\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name}: Aggregated Train vs Validation Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_loss_curves.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error aggregating loss for {ds_name}: {e}\")\n        plt.close()\n\n    # Rule Fidelity ------------------------------------------------------------\n    try:\n        res_rf = stack_metric((\"Rule_Fidelity\",))\n        if res_rf:\n            mean_rf, se_rf = res_rf\n            epochs = np.arange(1, len(mean_rf) + 1)\n            plt.figure()\n            plt.plot(epochs, mean_rf, marker=\"o\", label=\"Mean\")\n            plt.fill_between(\n                epochs, mean_rf - se_rf, mean_rf + se_rf, alpha=0.3, label=\"\u00b1 SE\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Rule Fidelity\")\n            plt.title(f\"{ds_name}: Aggregated Rule Fidelity\")\n            plt.legend()\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_rule_fidelity.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error aggregating rule fidelity for {ds_name}: {e}\")\n        plt.close()\n\n    # Confusion Matrix ---------------------------------------------------------\n    try:\n        # Sum confusion matrices over runs\n        aggregated_cm = None\n        for blob in runs_for_ds:\n            preds = np.array(blob.get(\"predictions\", []))\n            gts = np.array(blob.get(\"ground_truth\", []))\n            if preds.size and gts.size and preds.shape == gts.shape:\n                n_cls = int(max(preds.max(), gts.max()) + 1)\n                cm = np.zeros((n_cls, n_cls), dtype=int)\n                for p, t in zip(preds, gts):\n                    cm[t, p] += 1\n                if aggregated_cm is None:\n                    aggregated_cm = cm\n                else:\n                    # Resize if needed\n                    max_dim = max(aggregated_cm.shape[0], cm.shape[0])\n                    if aggregated_cm.shape[0] < max_dim:\n                        tmp = np.zeros((max_dim, max_dim), int)\n                        tmp[: aggregated_cm.shape[0], : aggregated_cm.shape[1]] = (\n                            aggregated_cm\n                        )\n                        aggregated_cm = tmp\n                    if cm.shape[0] < max_dim:\n                        tmp = np.zeros((max_dim, max_dim), int)\n                        tmp[: cm.shape[0], : cm.shape[1]] = cm\n                        cm = tmp\n                    aggregated_cm += cm\n        if aggregated_cm is not None:\n            plt.figure(figsize=(6, 5))\n            plt.imshow(aggregated_cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(\n                f\"{ds_name}: Aggregated Confusion Matrix\\n(Left: Ground Truth, Right: Generated Samples)\"\n            )\n            fname = os.path.join(working_dir, f\"{ds_name}_agg_confusion_matrix.png\")\n            plt.savefig(fname)\n            plt.close()\n    except Exception as e:\n        print(f\"Error aggregating confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # Test Accuracy printout ---------------------------------------------------\n    try:\n        accs = []\n        for blob in runs_for_ds:\n            preds = np.array(blob.get(\"predictions\", []))\n            gts = np.array(blob.get(\"ground_truth\", []))\n            if preds.size and gts.size and preds.shape == gts.shape:\n                accs.append((preds == gts).mean())\n        if accs:\n            accs = np.array(accs, dtype=float)\n            print(\n                f\"{ds_name} Test Accuracy: {accs.mean():.3f} \u00b1 {accs.std(ddof=0):.3f}\"\n            )\n    except Exception as e:\n        print(f\"Error computing aggregated accuracy for {ds_name}: {e}\")\n"], "plot_plan": ["We load experiment_data.npy, extract the single entry SPR_BENCH, and retrieve the arrays for train/val accuracy, train/val loss, and rule-based accuracy (RBA). Using matplotlib we create up to four figures: (1) train vs validation accuracy across epochs, (2) train vs validation loss across epochs, (3) comparison of validation accuracy and RBA, and (4) a confusion matrix of final test predictions vs ground truth. Each figure is built in its own try-except block, saved to working_dir with descriptive names, titled clearly, and the figure is always closed. If the required data for a plot is missing, the corresponding except block reports the issue without stopping the script. After plotting, we compute and print the final test accuracy directly from stored predictions/ground_truth. All code is concise, relies only on numpy and matplotlib, and respects the directory and plotting guidelines. Finally, the script prints the computed evaluation metric(s) so users can verify performance without opening the plots.", "We load the stored experiment_data.npy, iterate over each dataset entry, and for every dataset we plot training/validation accuracy and loss curves, rule-fidelity versus validation accuracy, and a confusion matrix based on saved test predictions. All figures are enclosed in try-except blocks, saved to working_dir with descriptive names, and closed after saving. After plotting, the script prints test accuracy for each dataset and, if two or more datasets exist, additionally plots a comparison of their validation accuracies in a single figure. Only real values from the numpy file are used; plots are skipped when the requisite arrays are missing.", "The script will first create/confirm the working directory, load experiment_data.npy, and safely fetch the metrics/losses it contains. It then iterates through each dataset key (only \u201cSPR_BENCH\u201d is expected, but the loop keeps it generic for future datasets) and produces up to five plots: (1) train vs validation accuracy, (2) train vs validation loss, (3) rule-fidelity over epochs, (4) rule-fidelity compared to validation accuracy, and (5) a confusion matrix if predictions are saved. Every figure is wrapped in its own try-except block to guarantee graceful failure and always closes with plt.close(). Naming of files and titles clearly specify dataset and plot type. Finally, if ground-truth and prediction arrays exist, the script prints the resulting test accuracy to stdout. The code follows all formatting, saving, and style requirements and remains concise and self-contained.", null, "We will load experiment_data.npy, iterate over each dataset key, and inside independent try-except blocks generate four standard plots\u2014accuracy, loss, rule-fidelity vs validation accuracy, and confusion matrix\u2014saving each to working_dir with descriptive filenames. While looping, we accumulate each dataset\u2019s final validation accuracy to later create a single cross-dataset bar chart for quick comparison. All plots are closed after saving and carry explicit titles that embed the dataset name. Arrays are fetched defensively, so missing entries won\u2019t break execution. After plotting we compute and print the test accuracy (or another available metric) for every dataset. The code uses only numpy and matplotlib, stays under the 5-figure limit per dataset, and follows all directory and naming conventions. Finally, figures are saved only if the relevant data exist.", null, "We will load the saved NumPy dictionary, walk through every dataset key it contains, and for each one plot (1) train/validation accuracy, (2) train/validation loss, (3) rule-fidelity versus validation accuracy, and (4) a test-set confusion matrix\u2014each wrapped in its own try/except with proper closing.  File names and titles always embed the dataset name for clarity.  After looping over individual datasets we build a cross-dataset comparison plot of validation accuracy curves (only if more than one dataset is present).  All figures are saved into the working/ directory.  Finally, for every dataset that provides predictions and ground-truth vectors we compute and print its test accuracy so users immediately see quantitative results.  No values are invented: everything is pulled from experiment_data.npy exactly as stored.  The code follows the supplied base structure, largely re-using its helpers, and respects the requirement to limit plots to existing data (at most 5 epochs intervals if extremely long).  Every figure is titled clearly, e.g. \u201cSPR_BENCH: Train vs Validation Accuracy\u201d, and given a descriptive filename.  This compact script can be executed as-is to generate all requested visualizations and metrics.", "We will load experiment_data.npy from the working directory, loop over every dataset present, and for each dataset we will: (1) extract metric/loss arrays safely, (2) generate up-to-four plots\u2014accuracy, loss, rule fidelity, and confusion matrix\u2014each wrapped in its own try-except block, (3) save each figure with a descriptive filename containing the dataset name and plot type, and (4) always close figures.  For datasets that have predictions and ground-truth available, we will compute and print the final test accuracy (or other metrics) after plotting.  Helper functions ensure missing keys gracefully return empty arrays so plots are only attempted when data exist; this keeps the script robust across arbitrary experiment dumps.  All plots are created with basic matplotlib, titled clearly, and saved into the pre-made \u201cworking\u201d folder.  The number of generated figures is bounded (max five per dataset).  Finally, the code is concise and self-contained, starting with the required import statements and printing the evaluation metric(s).", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The code will load the saved NumPy dictionary from the working directory,\niterate over every dataset it contains, and for each dataset print the last\nrecorded (i.e., final) value for every metric and loss that the training loop\nstored. It will also compute a final test accuracy by comparing the stored\ntest\u2010set predictions with the ground-truth labels.", "The script loads the saved numpy file from the working directory, retrieves the\nstored dictionaries, and for every dataset (only \u201cSPR_BENCH\u201d here) prints the\nfinal training metrics, the best validation metrics, the best rule-fidelity\nscore, and the test accuracy calculated from the saved predictions and ground-\ntruth labels. All values are clearly labelled so that \u201ctrain accuracy,\u201d\n\u201cvalidation loss,\u201d etc., are unambiguous.", "Below is a small utility that immediately loads the saved NumPy file, extracts\nthe lists of metrics saved during training, determines the best (highest for\naccuracies/fidelity, lowest for losses) value for each metric, and prints them\nwith clear labels for every dataset contained in the file.", "The script will locate the saved NumPy file inside the \u201cworking\u201d directory, load\nit into a Python dictionary, and then iterate through each stored dataset. For\nevery dataset it will fetch the final (last-epoch) values of each recorded\nmetric or loss and output them with explicit, human-readable names such as\n\u201cfinal training accuracy,\u201d \u201cfinal validation loss,\u201d or \u201ctest accuracy.\u201d It also\nrecomputes the overall test accuracy by comparing the saved test predictions\nwith the ground-truth labels.", "The script will load the saved NumPy dictionary, loop over every dataset key\n(e.g., \u201cSPR_BENCH\u201d), and for each stored list of metric values it will select an\nappropriate summary: the maximum for accuracy-type metrics and rule fidelity,\nand the minimum for losses.  It will also compute the final test accuracy\ndirectly from the stored prediction and ground-truth arrays if present.  Every\npiece of information is printed with an explicit, human-readable label, and the\ncode runs immediately on execution without relying on any special entry point.", "The script will locate the working directory, load the saved NumPy dictionary,\nand iterate over each stored dataset. For every dataset it will pull out the\nlists of accuracy, loss, and rule-fidelity values that were logged during\ntraining, take the final entry in each list, and print these with explicit\nmetric names. It will also compute test accuracy directly from the stored\npredictions and ground-truth arrays. All code is placed at the top level so the\nfile executes immediately when run.", "The script loads the numpy file from the working directory, retrieves the stored\nexperiment dictionary, and iterates over every dataset it contains. For each\ndataset it extracts the metric/loss lists, finds either the final value (for\ntraining quantities) or the best value (max accuracy / Rule Fidelity, min loss)\non the validation set, and computes the test accuracy directly from the saved\npredictions and ground-truth labels. Each metric is printed with a clear,\ndescriptive label immediately after the dataset\u2019s name; no plots or additional\noutput are generated.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", "The script will locate the working directory, load the numpy file containing the\nexperiment record, and iterate over every dataset stored in that dictionary. For\neach dataset it will pull the last element of every metric/loss list\n(representing the final-epoch value), compute test accuracy from the saved\npredictions and ground-truth arrays, and then print all these numbers with\nexplicit, descriptive labels. There are no plots or special entry\npoints\u2014everything executes immediately at the global scope.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", "The script will load the numpy file from the \u201cworking\u201d directory, iterate over\neach stored dataset (e.g., \u201cSPR_BENCH\u201d), and determine the best value for every\nrecorded metric: highest value for accuracies and rule-fidelity, lowest value\nfor losses. It also derives test accuracy directly from the saved predictions\nand ground-truth labels. Results are printed in a clear, dataset-first order\nwith explicit metric names such as \u201cbest train accuracy\u201d or \u201cbest validation\nloss\u201d, satisfying the formatting rules and avoiding any plots or special entry\npoints.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------- Locate and load experiment data ---------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------- Extract and print metrics ---------------- #\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}\")  # dataset header\n\n    # ----- Final (last) metrics ----- #\n    metrics = data.get(\"metrics\", {})\n    losses = data.get(\"losses\", {})\n\n    # Accuracy-related metrics\n    if metrics.get(\"train_acc\"):\n        print(f\"final train accuracy: {metrics['train_acc'][-1]:.4f}\")\n    if metrics.get(\"val_acc\"):\n        print(f\"final validation accuracy: {metrics['val_acc'][-1]:.4f}\")\n    if metrics.get(\"RBA\"):\n        print(f\"final rule-based accuracy: {metrics['RBA'][-1]:.4f}\")\n\n    # Losses\n    if losses.get(\"train\"):\n        print(f\"final training loss: {losses['train'][-1]:.6f}\")\n    if losses.get(\"val\"):\n        print(f\"final validation loss: {losses['val'][-1]:.6f}\")\n\n    # ----- Compute test accuracy from stored predictions ----- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if preds is not None and gts is not None and len(preds) == len(gts):\n        test_acc = (preds == gts).mean()\n        print(f\"test accuracy: {test_acc:.4f}\")\n\n    print(\"\")  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ---------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- utility helpers ---------- #\ndef _best(values, higher_is_better=True):\n    \"\"\"Return best (max or min) value from a list.\"\"\"\n    return max(values) if higher_is_better else min(values)\n\n\n# ---------- iterate over datasets and print metrics ---------- #\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}:\")\n\n    # final / best values\n    final_train_acc = data[\"metrics\"][\"train_acc\"][-1]\n    best_val_acc = _best(data[\"metrics\"][\"val_acc\"], higher_is_better=True)\n    best_rule_fid = _best(data[\"metrics\"][\"Rule_Fidelity\"], higher_is_better=True)\n\n    final_train_loss = data[\"losses\"][\"train\"][-1]\n    best_val_loss = _best(data[\"losses\"][\"val\"], higher_is_better=False)\n\n    # compute test accuracy from stored predictions\n    preds = data.get(\"predictions\", None)\n    gts = data.get(\"ground_truth\", None)\n    test_acc = (\n        float((preds == gts).mean()) if preds is not None and gts is not None else None\n    )\n\n    # ---------- print with clear labels ---------- #\n    print(f\"  train accuracy: {final_train_acc:.4f}\")\n    print(f\"  validation accuracy (best): {best_val_acc:.4f}\")\n    print(f\"  train loss: {final_train_loss:.4f}\")\n    print(f\"  validation loss (best): {best_val_loss:.4f}\")\n    print(f\"  rule fidelity (best): {best_rule_fid:.4f}\")\n    if test_acc is not None:\n        print(f\"  test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------------- #\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------------------------------- #\n# Helper to pick best value depending on metric type\ndef best_value(values, higher_is_better=True):\n    return max(values) if higher_is_better else min(values)\n\n\n# --------------------------------------------------------------------------- #\n# Iterate over all datasets and print best metrics\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    metrics = content.get(\"metrics\", {})\n    losses = content.get(\"losses\", {})\n\n    if metrics:\n        if \"train_acc\" in metrics:\n            val = best_value(metrics[\"train_acc\"], higher_is_better=True)\n            print(f\"Best train accuracy: {val:.4f}\")\n\n        if \"val_acc\" in metrics:\n            val = best_value(metrics[\"val_acc\"], higher_is_better=True)\n            print(f\"Best validation accuracy: {val:.4f}\")\n\n        if \"rule_fidelity\" in metrics:\n            val = best_value(metrics[\"rule_fidelity\"], higher_is_better=True)\n            print(f\"Best rule fidelity: {val:.4f}\")\n\n    if losses:\n        if \"train\" in losses:\n            val = best_value(losses[\"train\"], higher_is_better=False)\n            print(f\"Lowest train loss: {val:.6f}\")\n\n        if \"val\" in losses:\n            val = best_value(losses[\"val\"], higher_is_better=False)\n            print(f\"Lowest validation loss: {val:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- Locate and load the stored experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n\n# ---------- Helper to print a metric if it exists ----------\ndef _print_metric(name: str, value):\n    if value is not None:\n        print(f\"{name}: {value:.4f}\")\n\n\n# ---------- Iterate through datasets and report metrics ----------\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    metrics = content.get(\"metrics\", {})\n    losses = content.get(\"losses\", {})\n\n    # Final (last-epoch) metrics\n    _print_metric(\n        \"Final training accuracy\",\n        metrics.get(\"train_acc\", [None])[-1] if metrics.get(\"train_acc\") else None,\n    )\n    _print_metric(\n        \"Final validation accuracy\",\n        metrics.get(\"val_acc\", [None])[-1] if metrics.get(\"val_acc\") else None,\n    )\n    _print_metric(\n        \"Final rule fidelity\",\n        (\n            metrics.get(\"Rule_Fidelity\", [None])[-1]\n            if metrics.get(\"Rule_Fidelity\")\n            else None\n        ),\n    )\n\n    # Final (last-epoch) losses\n    _print_metric(\n        \"Final training loss\",\n        losses.get(\"train\", [None])[-1] if losses.get(\"train\") else None,\n    )\n    _print_metric(\n        \"Final validation loss\",\n        losses.get(\"val\", [None])[-1] if losses.get(\"val\") else None,\n    )\n\n    # Compute test accuracy from saved predictions / ground truth, if available\n    preds = content.get(\"predictions\")\n    gts = content.get(\"ground_truth\")\n    if (\n        preds is not None\n        and gts is not None\n        and len(preds) == len(gts)\n        and len(preds) > 0\n    ):\n        test_accuracy = float((preds == gts).mean())\n        _print_metric(\"Test accuracy\", test_accuracy)\n", "import os\nimport numpy as np\n\n# ------------------ Locate and load experiment data ------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------ Helper to choose best / final value ------------------ #\ndef select_summary(values, name):\n    \"\"\"\n    Pick a representative scalar for a metric series.\n    - For losses: return the minimum value.\n    - For everything else (accuracies, fidelities, etc.): return the maximum value.\n    If 'values' is not a list/array, it is returned unchanged.\n    \"\"\"\n    if not isinstance(values, (list, tuple, np.ndarray)):\n        return values\n    return min(values) if \"loss\" in name.lower() else max(values)\n\n\n# ------------------ Iterate and report ------------------ #\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metrics accumulated during training\n    for metric_name, metric_series in ds_dict.get(\"metrics\", {}).items():\n        best_val = select_summary(metric_series, metric_name)\n        pretty_name = metric_name.replace(\"_\", \" \")\n        print(f\"Best {pretty_name}: {best_val:.4f}\")\n\n    # Losses accumulated during training\n    for loss_name, loss_series in ds_dict.get(\"losses\", {}).items():\n        best_val = select_summary(loss_series, loss_name)\n        pretty_name = loss_name.replace(\"_\", \" \")\n        print(f\"Best {pretty_name} loss: {best_val:.4f}\")\n\n    # Optional test accuracy from stored predictions\n    preds = ds_dict.get(\"predictions\")\n    gts = ds_dict.get(\"ground_truth\")\n    if (\n        preds is not None\n        and gts is not None\n        and len(preds) == len(gts)\n        and len(preds) > 0\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"Test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Locate the working directory and the file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# 1. Load the experiment data\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2\u20135. Extract and print final values for each metric\nfor dataset_name, content in experiment_data.items():\n    print(dataset_name)  # Dataset header\n\n    # Metrics\n    metrics = content.get(\"metrics\", {})\n    if \"train_acc\" in metrics and metrics[\"train_acc\"]:\n        print(\"Final train accuracy:\", metrics[\"train_acc\"][-1])\n    if \"val_acc\" in metrics and metrics[\"val_acc\"]:\n        print(\"Final validation accuracy:\", metrics[\"val_acc\"][-1])\n    if \"Rule_Fidelity\" in metrics and metrics[\"Rule_Fidelity\"]:\n        print(\"Final rule fidelity:\", metrics[\"Rule_Fidelity\"][-1])\n\n    # Losses\n    losses = content.get(\"losses\", {})\n    if \"train\" in losses and losses[\"train\"]:\n        print(\"Final training loss:\", losses[\"train\"][-1])\n    if \"val\" in losses and losses[\"val\"]:\n        print(\"Final validation loss:\", losses[\"val\"][-1])\n\n    # Test accuracy (from stored predictions and ground truth)\n    preds = content.get(\"predictions\")\n    gts = content.get(\"ground_truth\")\n    if preds is not None and gts is not None and len(preds) == len(gts):\n        test_accuracy = np.mean(preds == gts)\n        print(\"Test accuracy:\", float(test_accuracy))\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ #\n#                 LOAD EXPERIMENT DATA FROM .npy FILE                #\n# ------------------------------------------------------------------ #\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------ #\n#               HELPER FUNCTIONS FOR METRIC EXTRACTION               #\n# ------------------------------------------------------------------ #\ndef safe_final(lst):\n    \"\"\"Return the last element of a list if it exists, else None.\"\"\"\n    return lst[-1] if lst else None\n\n\ndef safe_best(lst, mode=\"max\"):\n    \"\"\"Return best element according to mode from a list, else None.\"\"\"\n    if not lst:\n        return None\n    return max(lst) if mode == \"max\" else min(lst)\n\n\n# ------------------------------------------------------------------ #\n#                           PRINT RESULTS                            #\n# ------------------------------------------------------------------ #\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Metric & loss histories\n    metrics = content.get(\"metrics\", {})\n    losses = content.get(\"losses\", {})\n\n    # Training metrics (final value)\n    final_train_acc = safe_final(metrics.get(\"train_acc\", []))\n    final_train_loss = safe_final(losses.get(\"train\", []))\n\n    # Validation metrics (best value)\n    best_val_acc = safe_best(metrics.get(\"val_acc\", []), mode=\"max\")\n    best_val_loss = safe_best(losses.get(\"val\", []), mode=\"min\")\n    best_rule_fidelity = safe_best(metrics.get(\"Rule_Fidelity\", []), mode=\"max\")\n\n    # Test metrics (computed from saved predictions)\n    preds = content.get(\"predictions\", np.array([]))\n    gts = content.get(\"ground_truth\", np.array([]))\n    test_accuracy = float((preds == gts).mean()) if preds.size and gts.size else None\n\n    # ---------------------- PRINT WITH CLEAR LABELS ---------------------- #\n    if final_train_acc is not None:\n        print(f\"final train accuracy: {final_train_acc:.4f}\")\n    if final_train_loss is not None:\n        print(f\"final train loss: {final_train_loss:.4f}\")\n\n    if best_val_acc is not None:\n        print(f\"best validation accuracy: {best_val_acc:.4f}\")\n    if best_val_loss is not None:\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n    if best_rule_fidelity is not None:\n        print(f\"best validation Rule_Fidelity: {best_rule_fidelity:.4f}\")\n\n    if test_accuracy is not None:\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# -------- Locate and load the stored experiment data -------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------- Helper to print a metric if it exists -------- #\ndef _print_metric(label: str, value):\n    if value is not None:\n        # If it's a scalar numerical value, format it; otherwise just print raw\n        if isinstance(value, (int, float, np.floating)):\n            print(f\"  {label}: {value:.4f}\")\n        else:\n            print(f\"  {label}: {value}\")\n\n\n# -------- Iterate over datasets and report final metrics -------- #\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Safely fetch the final value for each stored list-metric\n    final_train_acc = (\n        data[\"metrics\"][\"train_acc\"][-1] if data[\"metrics\"][\"train_acc\"] else None\n    )\n    final_val_acc = (\n        data[\"metrics\"][\"val_acc\"][-1] if data[\"metrics\"][\"val_acc\"] else None\n    )\n    final_rule_fid = (\n        data[\"metrics\"][\"Rule_Fidelity\"][-1]\n        if data[\"metrics\"][\"Rule_Fidelity\"]\n        else None\n    )\n    final_train_loss = data[\"losses\"][\"train\"][-1] if data[\"losses\"][\"train\"] else None\n    final_val_loss = data[\"losses\"][\"val\"][-1] if data[\"losses\"][\"val\"] else None\n\n    # Compute test accuracy from saved predictions / ground truths, if present\n    test_accuracy = None\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    if len(preds) and len(gts) and len(preds) == len(gts):\n        test_accuracy = (preds == gts).mean()\n\n    # -------- Print metrics -------- #\n    _print_metric(\"final train accuracy\", final_train_acc)\n    _print_metric(\"final validation accuracy\", final_val_acc)\n    _print_metric(\"final training loss\", final_train_loss)\n    _print_metric(\"final validation loss\", final_val_loss)\n    _print_metric(\"final Rule Fidelity\", final_rule_fid)\n    _print_metric(\"test accuracy\", test_accuracy)\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------- #\n#                           LOAD EXPERIMENT DATA                         #\n# ---------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------- #\n#                         HELPER: PRETTY METRIC NAME                     #\n# ---------------------------------------------------------------------- #\ndef prettify(key: str, is_loss=False) -> str:\n    \"\"\"\n    Turn raw dictionary keys into human\u2013readable metric names.\n    Examples:\n        train_acc   -> train accuracy\n        val_acc     -> validation accuracy\n        Rule_Fidelity -> rule fidelity\n        train (loss) -> train loss\n    \"\"\"\n    key_low = key.lower()\n    if key_low == \"rule_fidelity\" or key_low == \"rule fidelity\":\n        return \"rule fidelity\"\n    replacements = {\n        \"train_acc\": \"train accuracy\",\n        \"val_acc\": \"validation accuracy\",\n        \"val\": \"validation\",\n        \"train\": \"train\",\n    }\n    if key in replacements:\n        return f\"{replacements[key]}{' loss' if is_loss else ' accuracy'}\"\n    # Fallback: replace underscores, keep casing lower\n    name = key.replace(\"_\", \" \").lower()\n    if \"acc\" in name:\n        name = name.replace(\"acc\", \"accuracy\")\n    if is_loss and \"loss\" not in name:\n        name += \" loss\"\n    return name.strip()\n\n\n# ---------------------------------------------------------------------- #\n#                           PRINT BEST METRICS                           #\n# ---------------------------------------------------------------------- #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # ---- accuracy / fidelity type metrics ---- #\n    metrics_dict = data.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.max() if \"loss\" not in metric_key.lower() else arr.min()\n        pretty = prettify(metric_key)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- loss metrics ---- #\n    losses_dict = data.get(\"losses\", {})\n    for loss_split, values in losses_dict.items():\n        if len(values) == 0:\n            continue\n        arr = np.asarray(values, dtype=float)\n        best_val = arr.min()  # lower loss is better\n        pretty = prettify(loss_split, is_loss=True)\n        print(f\"  best {pretty}: {best_val:.4f}\")\n\n    # ---- test accuracy from saved predictions ---- #\n    preds = data.get(\"predictions\")\n    gts = data.get(\"ground_truth\")\n    if (\n        preds is not None\n        and len(preds) > 0\n        and gts is not None\n        and len(gts) == len(preds)\n    ):\n        test_accuracy = (preds == gts).mean()\n        print(f\"  test accuracy: {test_accuracy:.4f}\")\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'final train accuracy: 0.9335', '\\n', 'final validation\naccuracy: 0.7640', '\\n', 'final rule-based accuracy: 0.6620', '\\n', 'final\ntraining loss: 0.647695', '\\n', 'final validation loss: 0.666328', '\\n', 'test\naccuracy: 0.7730', '\\n', '', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH:', '\\n', '  train accuracy: 0.9635', '\\n', '  validation accuracy\n(best): 0.7740', '\\n', '  train loss: 0.1231', '\\n', '  validation loss (best):\n0.5699', '\\n', '  rule fidelity (best): 1.0000', '\\n', '  test accuracy:\n0.7800', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best train accuracy: 0.9530', '\\n', 'Best\nvalidation accuracy: 0.7600', '\\n', 'Best rule fidelity: 0.9020', '\\n', 'Lowest\ntrain loss: 0.642440', '\\n', 'Lowest validation loss: 0.664509', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Final training accuracy: 0.5000', '\\n', 'Final\nvalidation accuracy: 0.5200', '\\n', 'Final rule fidelity: 1.0000', '\\n', 'Final\ntraining loss: 0.6945', '\\n', 'Final validation loss: 0.6924', '\\n', 'Test\naccuracy: 0.4980', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best train acc: 0.9380', '\\n', 'Best val acc:\n0.7460', '\\n', 'Best Rule Fidelity: 0.9580', '\\n', 'Best train loss: 0.6909',\n'\\n', 'Best val loss: 0.6893', '\\n', 'Test accuracy: 0.7640', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Final train accuracy:', ' ', '0.5', '\\n', 'Final validation\naccuracy:', ' ', '0.48', '\\n', 'Final rule fidelity:', ' ', '1.0', '\\n', 'Final\ntraining loss:', ' ', '0.6937365822792053', '\\n', 'Final validation loss:', ' ',\n'0.6932250261306763', '\\n', 'Test accuracy:', ' ', '0.502', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final train accuracy: 0.9425', '\\n', 'final\ntrain loss: 0.1975', '\\n', 'best validation accuracy: 0.7640', '\\n', 'best\nvalidation loss: 0.5572', '\\n', 'best validation Rule_Fidelity: 0.6500', '\\n',\n'test accuracy: 0.7770', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9765', '\\n', '  best\nvalidation accuracy accuracy: 0.7920', '\\n', '  best rule fidelity: 1.0000',\n'\\n', '  best train loss: 0.0800', '\\n', '  best validation loss: 0.5722', '\\n',\n'  test accuracy: 0.7840', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  final train accuracy: 0.5010', '\\n', '  final\nvalidation accuracy: 0.4800', '\\n', '  final training loss: 0.6939', '\\n', '\nfinal validation loss: 0.6937', '\\n', '  final Rule Fidelity: 0.0000', '\\n', '\ntest accuracy: 0.5020', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9775', '\\n', '  best\nvalidation accuracy accuracy: 0.7800', '\\n', '  best rule fidelity: 0.9640',\n'\\n', '  best train loss: 0.0734', '\\n', '  best validation loss: 0.5666', '\\n',\n'  test accuracy: 0.7790', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9825', '\\n', '  best\nvalidation accuracy accuracy: 0.7860', '\\n', '  best rule fidelity: 1.0000',\n'\\n', '  best train loss: 0.0654', '\\n', '  best validation loss: 0.5496', '\\n',\n'  test accuracy: 0.7880', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', '  best train accuracy accuracy: 0.9830', '\\n', '  best\nvalidation accuracy accuracy: 0.7880', '\\n', '  best rule fidelity: 1.0000',\n'\\n', '  best train loss: 0.0604', '\\n', '  best validation loss: 0.5441', '\\n',\n'  test accuracy: 0.7880', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}