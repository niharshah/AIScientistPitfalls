\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{natbib}

\graphicspath{{figures/}}

\title{When Accuracy Clashes with Interpretability: Surprising Shortcomings in Deep Classifiers}
\author{Anonymous Submission}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Despite remarkable improvements on benchmark datasets, deep learning models can exhibit surprising failures in real-world scenarios. This paper explores how seemingly high-performing classifiers violate interpretability constraints, leading to pitfalls in practical deployments. We present inconclusive and sometimes negative findings that caution against overreliance on standard strategies when interpretability is required.
\end{abstract}

\section{Introduction}
Deep neural networks achieve exceptional results in vision (\citep{goodfellow2014explaining}), language, and other domains. Yet real-world use cases often demand more than raw accuracy, especially in safety-critical or regulated applications. We explore settings where classifiers must align with explicit reasoning rules, highlighting how subtle discrepancies foil purely data-driven methods. Our central contribution is a set of experiments demonstrating that typical approaches not only fail to respect human-crafted rules but can mask these failures behind high test accuracies.

\section{Related Work}
Models that combine symbolic rules with deep representations have attracted recent attention, aiming to balance interpretability and performance (\citep{kingma2015adam}). However, many standard training procedures disregard explicit logical constraints, resulting in partial or misleading compliance. Our investigation extends the critique of black-box models by showing how small domain shifts can exacerbate rule violations, undermining user trust.

\section{Method}
We conduct controlled experiments where each dataset instance contains features crucial for classification alongside auxiliary patterns that frequently override symbolic rules. While the training set suggests that the network has learned both patterns, out-of-distribution examples reveal large gaps in rule fidelity. We employ an architecture loosely based on prior classification backbones, with additional input transformations to highlight rules vs.\ data-driven cues.

\section{Experiments}
We trained models across multiple seeds and varied hyperparameters. Inconclusive or even negative results emerged: although final accuracies often reached high values, per-class confusion analyses showed consistent rule violations. This discrepancy was particularly evident on data samples reflecting real-world distortions.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{example_performance.png}
\caption{Overall classification performance remains high, but does not guarantee faithful rule adherence.}
\label{fig:example}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{example_misclassification.png}
\caption{Example misclassification scenario where logical rules are violated, despite correct features being present.}
\label{fig:misclass}
\end{figure}

These figures illustrate that while the model frequently identifies correct class labels, it does so for the wrong reasons. Our results expose how deep networks can exploit correlations that overshadow explicit, human-designed rules. This mismatch raises critical concerns for deployment in domains like healthcare, where interpretability can be as important as raw predictive metrics.

\section{Conclusion}
Our findings emphasize the need to validate interpretability in conjunction with accuracy. Models must be tested on challenging conditions that expose alignment failures. We encourage the community to develop joint learning protocols integrating robust rule adherence, as high accuracy alone remains insufficient to ensure reliable decision-making. We hope these results stimulate discussion on reconciling rules with the flexible but opaque reasoning of deep networks.

\bibliographystyle{plain}
\bibliography{references}

\begin{filecontents}{references.bib}
@article{goodfellow2014explaining,
  title={Explaining and Harnessing Adversarial Examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@article{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations (ICLR)},
  year={2015}
}
\end{filecontents}

\end{document}