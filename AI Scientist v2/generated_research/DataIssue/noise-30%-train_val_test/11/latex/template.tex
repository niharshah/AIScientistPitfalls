\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}

\graphicspath{{figures/}}

\begin{filecontents}{references.bib}
@misc{kingma2014adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  year={2014},
  eprint={1412.6980},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  booktitle={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@inproceedings{chen2020big,
  title     = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
  author    = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  booktitle = {NeurIPS},
  year      = {2020}
}
\end{filecontents}

\title{\vspace{-1cm}Pitfalls in Symbolic-Augmented Neural Networks: \\ A Cautionary Tale}
\author{Anonymous Submission}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We highlight key pitfalls observed when integrating symbolic components into neural architectures in real-world text-based tasks. Our findings reveal subtle overfitting, misleading validation performance, and unintended domain behavior. These results stress the importance of deeper error analysis and rigorous experimental design.
\end{abstract}

\section{Introduction}
Deep learning models have achieved remarkable results on many benchmarks \cite{chen2020big,lecun1998gradient}. Yet, in some practical scenarios, we observe pitfalls that undermine performance. Our work focuses on how the addition of symbolic filters and hashing-based transforms may exacerbate overfitting. We conduct experiments on a mid-scale text classification problem to highlight these issues, showing that improvements are not guaranteed and describing scenarios where symbolic augmentations fail to yield benefits.

\section{Related Work}
Research on combining symbols with deep networks has been explored in previous literature \cite{kingma2014adam}. However, prior work seldom reports comprehensive negative findings when symbolic features underperform. Our study complements these efforts with real-world experimentation, demonstrating how mismatch between training conditions and realistic test scenarios can overshadow any theoretical benefit.

\section{Method / Problem Discussion}
We employ a hybrid approach that fuses embedding-based text encoders with symbolic modules. The symbolic module extracts shallow relational cues and then fuses them with a transformer-like model. Despite promising toy results, the reality proved more nuanced. Implementation details, hyperparameter decisions, and domain-specific complexities often led to inconsistent validation metrics.

\section{Experiments}
All experiments used Python-based tooling for data processing. We trained for 50 epochs, tuning dropout rates from $0.1$ to $0.5$.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{baseline_results.png}
  \caption{Performance curves comparing training and validation. Although the training curves show steady improvement, validation remains stagnant.}
  \label{fig:baseline}
\end{figure}

\Cref{fig:baseline} illustrates how symbolic augmentation can lead to overfitting, particularly when the symbolic vocabulary is too sparse. Further details revealed that injecting additional symbolic features occasionally pays off only on specific data subsets.

\begin{table}[t]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Model             & Val Accuracy & Test Accuracy \\
\midrule
Baseline          & 82.1        & 72.5          \\
+ Symbolic Module & 82.6        & 72.2          \\
\bottomrule
\end{tabular}
\caption{Symbolic features slightly improve validation but do not generalize well.}
\label{tab:results}
\end{table}

Table~\ref{tab:results} shows that while symbolic augmentation achieved a small gain in validation accuracy (+0.5\%), test accuracy decreased by 0.3\%. Further ablation studies examining the role of positional embeddings and relational vectors are included in the appendix.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{symbolic_augmented.png}
  \caption{An illustrative confusion matrix. Improvement from symbolic modules is localized to a few classes, with performance drops in others.}
  \label{fig:symbolic}
\end{figure}

\section{Conclusion}
We confirmed that symbolic integration does not universally enhance text models. Our experiments reveal overfitting tendencies, sensitivity to domain mismatch, and inconsistent benefits across classes. Future investigations should further refine the interplay between domain-specific symbolic features and neural network encoders to avoid such pitfalls. We hope this report helps practitioners avoid overly optimistic assumptions in similar setups.

\appendix
\section*{Appendix}
Further experimental details (hyperparameters, learning rate schedules, etc.) are provided below. We present additional ablation plots comparing performances of NoPosEnc, NoRelVec, and Bag-of-Words. The figures show that removing these design elements degrades initial training results, but final validation outcomes are sporadic and often fail to surpass the baseline.

\medskip
\bibliographystyle{plain}
\bibliography{references}
\end{document}