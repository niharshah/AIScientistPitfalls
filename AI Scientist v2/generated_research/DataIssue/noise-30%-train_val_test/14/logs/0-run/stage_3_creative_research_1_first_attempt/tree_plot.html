<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 1], [0, 2], [0, 3], [3, 8], [3, 7], [3, 6], [3, 5], [8, 10], [8, 9], [8, 11], [8, 12]], "layout": [[0.3, 0.0], [0.0, 0.33333333333333337], [0.2, 0.33333333333333337], [0.4, 0.33333333333333337], [0.6, 0.33333333333333337], [0.1, 0.6666666666666667], [0.3, 0.6666666666666667], [0.5, 0.6666666666666667], [0.7, 0.6666666666666667], [0.4, 1.0], [0.6, 1.0], [0.8, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: d_model (Transformer hidden/embedding size). We load the\nSPR benchmark, build a whitespace vocabulary, and create PyTorch\ndatasets/dataloaders exactly as in the baseline.   A grid search is then\nperformed over d_model \u2208 {64, 128, 256, 384}.   For every d_model we pick a\ncompatible nhead, build a fresh SimpleTransformer, train for five epochs,\nevaluate after each epoch, and log losses/F1.   All results are stored in the\nnested experiment_data structure under the key 'd_model_tuning' and finally\nsaved to working/experiment_data.npy.", "Our first attempt adds a lightweight \u201csymbolic feature\u201d branch to the vanilla\nTransformer.   For every sequence we compute three inexpensive symbolic\nstatistics (length, number of unique tokens, and uniqueness-ratio).   An MLP\nembeds these features and concatenates them with the [CLS] representation before\nclassification.   Thus the model can combine distributed sequence\nrepresentations with explicit symbolic cues.   We train both baseline and\naugmented models for three epochs and monitor macro-F1 (proxy for Rule\nGeneralization Accuracy) plus loss on the development set.   All metrics,\nlosses, predictions and ground-truth labels are stored in `experiment_data.npy`\ninside the working directory.   If the symbolic branch helps, we should observe\nhigher dev F1/RGA than the baseline; otherwise ablation studies will guide\nfurther refinement.", "We extend the previous grid-search setup with an explicit \u201csymbolic\u201d feature\nthat approximates rule\u2013level reasoning: besides the standard CLS vector from the\nTransformer encoder, we form an order-invariant bag-of-symbol representation by\nsimply averaging token embeddings over all non-pad positions (a cheap surrogate\nfor explicit rule counts).  A small gating MLP fuses the CLS and BoW vectors\nbefore classification.  This hybrid model (SimpleTransformer+BoW) injects\nexplicit set-level statistics that a pure sequence model must learn implicitly,\noften boosting generalization on rule-driven data.  We train both the baseline\nand the hybrid model (d_model = 128) for a few epochs and track loss, macro-F1\nand Rule Generalisation Accuracy (RGA, here identical to dev accuracy because\nthe benchmark does not expose hidden-rule ids).  All metrics are stored in the\nprescribed experiment_data structure and saved for later analysis.  The code is\nself-contained, GPU-aware and finishes in <30 minutes on a single GPU.", "We extend the baseline token-level Transformer with an explicit symbolic branch\nthat reasons over a bag-of-symbols representation (token-count vector).   The\nmodel embeds the input sequence as usual, takes the [CLS] token output, and\nconcatenates it with a compact symbolic vector produced by an MLP over the\ncounts.   This hybrid representation is fed to a final classifier, encouraging\nthe network to combine continuous pattern extraction with discrete counting\nlogic\u2014an ability crucial for PolyRule abstractions.   We train both the baseline\nand the new \u201cSymbolic-Aug\u201d variant for a few epochs on SPR_BENCH and log loss,\nmacro-F1 and Rule Generalization Accuracy (here measured as plain accuracy on\nthe dev split).   All metrics are stored in `experiment_data.npy` for later\nanalysis.", "We extend the baseline by adding a light-weight SymbolicReasoningModule that\nextracts a fixed-length bag-of-ngrams signature from every sequence, passes it\nthrough an MLP, and concatenates the resulting symbolic vector with the CLS\nembedding produced by the Transformer. This hybrid representation is then\nclassified. Although simple, it injects explicit symbolic pattern statistics\nthat a pure Transformer might overlook and lets us test whether neural-symbolic\nintegration helps rule generalization. We keep most hyper-parameters from the\nwinning baseline (d_model = 128) and train for a few epochs on a subset of the\ntraining set to ensure quick turnaround. We report macro-F1 and the requested\nRule Generalization Accuracy (measured as plain accuracy on the dev split, which\ncontains unseen rules). Both metrics are logged every epoch and stored for later\nanalysis.", "We propose to boost conceptual generalisation by jointly training a single\nTransformer on two complementary objectives: (1) sequence-level rule\nclassification (SPR label) and (2) masked-token reconstruction (MLM).   Self-\nsupervised MLM regularises internal representations to capture symbol-level\nregularities, while the supervised head focuses them toward rule discrimination.\nA single shared encoder feeds two lightweight output heads; the total loss is\nL = Lcls + \u03bb\u00b7Lmlm  (\u03bb=0.5).   We compare this MultiTask-Transformer (MTT) to the\nprevious Baseline on the SPR_BENCH dev split and report macro-F1 + Rule-\nGeneralisation-Accuracy each epoch. The whole script is self-contained, GPU-\naware, saves metrics and runs in <30 min.", "We keep the original token-level Transformer baseline, but add an auxiliary\nmasked-language-model (MLM) objective so the encoder must reconstruct randomly\nhidden symbols in every sequence.  By forcing the network to model local\nsymbolic regularities as well as the global class, we expect richer rule\nabstractions and higher Rule-Generalization-Accuracy (RGA).  The new \u201cAuxCL\u201d\nmodel shares the encoder with two heads: (i) a CLS-token classifier, and (ii) a\ntied-weights decoder that predicts the original vocabulary at masked positions.\nTotal loss = CE_class + 0.5 \u00d7 CE_MLM.  We train both Baseline and AuxCL for a\nfew epochs and record losses, macro-F1 and RGA each epoch; everything is saved\nto ./working/experiment_data.npy.  All tensors, models and batches are moved to\nGPU when available, respecting the required boiler-plate.  The script runs\nimmediately, needs no entry point, and finishes in <30 min on a single GPU.", "We extend the baseline by adding an auxiliary masked-language-model (MLM)\nobjective: while classifying the rule that generated a sequence, the model\nsimultaneously tries to recover 15 % randomly-masked tokens. This multi-task\nsetup encourages the encoder to internalise the generative logic rather than\nmemorising surface statistics, and so should improve Rule Generalisation\nAccuracy. Concretely, we (1) add a shared embedding/soft-max layer for MLM, (2)\ncombine classification loss with MLM loss (weight 0.5), and (3) apply dynamic\non-the-fly masking inside each training batch. We keep the architecture\nlightweight so the script trains in minutes on a single GPU/CPU and logs per-\nepoch losses, macro-F1, and RGA. Baseline and multi-task variants are both\ntrained for quick comparison, and all metrics are stored in\n`experiment_data.npy`.", "We extend the symbolic branch by making it a first-class \u201creasoning token\u2019\u2019 that\nthe Transformer can attend to throughout all layers.  Concretely, for each\nsequence we form a bag-of-symbols count vector, project it to the model\ndimension, and prepend this embedding in front of the usual [CLS] token and word\ntokens.  This allows bidirectional information flow between symbolic statistics\nand contextual pattern learning instead of the late fusion used previously.  A\nsmall gating MLP on the [CLS] output further modulates the contribution of the\nreasoning token.  We train this SymbolicTokenTransformer together with the\nearlier baseline for three epochs, track loss, macro-F1 and our proxy Rule-\nGeneralization-Accuracy (dev accuracy), and save everything for later analysis.\nThe whole script is self-contained, GPU-aware, and finishes within the time\nbudget.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, random, math, json, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------------\n# 0) Reproducibility & device\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------------\n# 1) Working dir & experiment data dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"d_model_tuning\": {}}  # each key will be a d_model value as string\n\n\n# -------------------------------------------------------------------------------\n# 2) Load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------------\n# 3) Build vocab (whitespace)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# -------------------------------------------------------------------------------\n# 4) Dataset & dataloaders\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n\n# -------------------------------------------------------------------------------\n# 5) Transformer model\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# -------------------------------------------------------------------------------\n# 6) Train / eval helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------------------------------------------------------------------\n# 7) Hyperparameter grid search over d_model\nDMODEL_GRID = [64, 128, 256, 384]\n\n\ndef pick_head(d):\n    if d % 8 == 0:\n        return 8\n    if d % 4 == 0:\n        return 4\n    return 1\n\n\nEPOCHS = 5\n\nfor d_model in DMODEL_GRID:\n    run_key = str(d_model)\n    experiment_data[\"d_model_tuning\"][run_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    print(f\"\\n=== Training with d_model={d_model} ===\")\n    model = SimpleTransformer(\n        vocab_size,\n        num_labels,\n        d_model=d_model,\n        nhead=pick_head(d_model),\n        nlayers=2,\n        dim_ff=d_model * 2,\n    ).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader)\n\n        exp_dict = experiment_data[\"d_model_tuning\"][run_key][\"SPR_BENCH\"]\n        exp_dict[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_dict[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        exp_dict[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_dict[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"d_model={d_model} | Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n\n    # store last predictions\n    exp_dict[\"predictions\"] = val_pred\n    exp_dict[\"ground_truth\"] = val_gt\n\n# -------------------------------------------------------------------------------\n# 8) Save all results\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data, allow_pickle=True)\nprint(\"Saved experiment data to\", save_path)\n", "import os, pathlib, time, random, math, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------------------------------------------------------\n# reproducibility\nseed = 7\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# ------------------------------------------------------------------\n# experiment data structure\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"symbolic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------------------------------------------\n# load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------------\n# simple whitespace vocab\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ------------------------------------------------------------------\n# dataset / dataloader\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\nBATCH = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), BATCH, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), BATCH, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    SPRDataset(spr[\"test\"]), BATCH, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ------------------------------------------------------------------\n# models\nclass SimpleTransformer(nn.Module):\n    def __init__(self, d_model=128, nhead=4, nlayers=2, d_ff=256):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_lay = nn.TransformerEncoderLayer(d_model, nhead, d_ff, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_lay, nlayers)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.cls(x[:, 0, :])\n        return logits\n\n\nclass SymbolicFeatureExtractor(nn.Module):\n    def __init__(self, out_dim=32):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(3, 32), nn.ReLU(), nn.Linear(32, out_dim), nn.ReLU()\n        )\n\n    def forward(self, input_ids, attention_mask):\n        inp = input_ids.detach().cpu()\n        att = attention_mask.detach().cpu()\n        feats = []\n        for ids, mask in zip(inp, att):\n            toks = ids[mask.bool()]\n            seq_len = toks.size(0)\n            uniq = len(torch.unique(toks))\n            uniq_ratio = uniq / seq_len if seq_len > 0 else 0.0\n            feats.append([seq_len / MAX_LEN, uniq / MAX_LEN, uniq_ratio])\n        feats = torch.tensor(feats, dtype=torch.float, device=input_ids.device)\n        return self.mlp(feats)\n\n\nclass SymbolicAugTransformer(nn.Module):\n    def __init__(self, d_model=128, nhead=4, nlayers=2, d_ff=256, sym_dim=32):\n        super().__init__()\n        self.trans = SimpleTransformer(d_model, nhead, nlayers, d_ff)\n        self.sym = SymbolicFeatureExtractor(sym_dim)\n        self.cls = nn.Linear(d_model + sym_dim, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        # get transformer hidden\n        x = self.trans.embed(input_ids) + self.trans.pos[:, : input_ids.size(1), :]\n        x = self.trans.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        cls_rep = x[:, 0, :]\n        sym_rep = self.sym(input_ids, attention_mask)\n        logits = self.cls(torch.cat([cls_rep, sym_rep], dim=-1))\n        return logits\n\n\n# ------------------------------------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if is_train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)\n    return tot_loss / len(loader.dataset), f1, acc, preds, gts\n\n\n# ------------------------------------------------------------------\ndef train_model(model, tag, epochs=3, lr=3e-4):\n    model = model.to(device)\n    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n    for epoch in range(1, epochs + 1):\n        s = time.time()\n        tr_loss, tr_f1, tr_acc, _, _ = run_epoch(model, train_loader, optim)\n        val_loss, val_f1, val_acc, val_pred, val_gt = run_epoch(model, dev_loader)\n        d = experiment_data[\"SPR_BENCH\"][tag]\n        d[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1, \"acc\": tr_acc})\n        d[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1, \"acc\": val_acc})\n        d[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        d[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n        print(\n            f\"{tag} | Ep {epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_F1={tr_f1:.4f} val_F1={val_f1:.4f} ({time.time()-s:.1f}s)\"\n        )\n    # store final predictions\n    d[\"predictions\"] = val_pred\n    d[\"ground_truth\"] = val_gt\n    return model\n\n\n# ------------------------------------------------------------------\nprint(\"\\n=== Baseline ===\")\nbaseline_model = train_model(\n    SimpleTransformer(d_model=128, nhead=4), \"baseline\", epochs=3\n)\n\nprint(\"\\n=== Symbolic Augmented ===\")\nsym_model = train_model(\n    SymbolicAugTransformer(d_model=128, nhead=4), \"symbolic\", epochs=3\n)\n\n# ------------------------------------------------------------------\n# final evaluation on test set for best model (symbolic)\ntest_loss, test_f1, test_acc, test_pred, test_gt = run_epoch(sym_model, test_loader)\nprint(\n    f\"\\nSymbolic model TEST: loss={test_loss:.4f}  F1={test_f1:.4f}  ACC={test_acc:.4f}\"\n)\nexperiment_data[\"SPR_BENCH\"][\"symbolic\"][\"test_metrics\"] = {\n    \"loss\": test_loss,\n    \"macro_f1\": test_f1,\n    \"acc\": test_acc,\n}\n\n# save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, random, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------\n# reproducibility & device\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------\n# experiment data container\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n        },\n        \"symbolic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n        },\n    }\n}\n\n\n# -----------------------------------------------------------\n# 1) load dataset ---------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(os.getenv(\"SPR_DATA_PATH\", \"./SPR_BENCH\"))\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -----------------------------------------------------------\n# 2) build vocab ---------------------------------------------------------------\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# -----------------------------------------------------------\n# 3) dataset classes ------------------------------------------------------------\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=64, shuffle=False, collate_fn=collate)\n\n\n# -----------------------------------------------------------\n# 4) model definitions ----------------------------------------------------------\nclass SimpleTransformer(nn.Module):\n    def __init__(self, use_bow=False, d_model=128, nhead=4, nlayers=2, dim_ff=256):\n        super().__init__()\n        self.use_bow = use_bow\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        if use_bow:\n            self.fuse = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.ReLU())\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        mask = ~attention_mask.bool()\n        x = self.encoder(x, src_key_padding_mask=mask)\n        cls = x[:, 0, :]\n        if self.use_bow:\n            bow = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n                1, keepdim=True\n            )\n            cls = self.fuse(torch.cat([cls, bow], dim=-1))\n        return self.classifier(cls)\n\n\n# -----------------------------------------------------------\n# 5) training / evaluation helpers ---------------------------------------------\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = (np.array(preds) == np.array(gts)).mean()\n    return tot_loss / len(loader.dataset), macro_f1, acc\n\n\ndef train_model(model_name, use_bow):\n    model = SimpleTransformer(use_bow=use_bow).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    for epoch in range(1, 4):  # 3 epochs to stay quick\n        t0 = time.time()\n        tr_loss, tr_f1, tr_acc = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_acc = run_epoch(model, dev_loader)\n        experiment_data[\"SPR_BENCH\"][model_name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": tr_loss}\n        )\n        experiment_data[\"SPR_BENCH\"][model_name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        experiment_data[\"SPR_BENCH\"][model_name][\"metrics\"][\"train\"].append(\n            {\"epoch\": epoch, \"macro_f1\": tr_f1, \"RGA\": tr_acc}\n        )\n        experiment_data[\"SPR_BENCH\"][model_name][\"metrics\"][\"val\"].append(\n            {\"epoch\": epoch, \"macro_f1\": val_f1, \"RGA\": val_acc}\n        )\n        print(\n            f\"{model_name} | epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.3f}, val_F1={val_f1:.3f}, \"\n            f\"val_RGA={val_acc:.3f} ({time.time()-t0:.1f}s)\"\n        )\n    return model\n\n\n# -----------------------------------------------------------\n# 6) run both models ------------------------------------------------------------\nbaseline_model = train_model(\"baseline\", use_bow=False)\nsymbolic_model = train_model(\"symbolic\", use_bow=True)\n\n# -----------------------------------------------------------\n# 7) save experiment data -------------------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved all metrics to working/experiment_data.npy\")\n", "import os, pathlib, random, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------------------- boilerplate ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"SymbolicAug\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ---------------------------- data & vocabulary --------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_file):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_file),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add(tok)\nvocab_size = len(vocab)\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"Vocab:{vocab_size}  Labels:{num_labels}\")\n\nMAX_LEN = 128\n\n\ndef encode(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seqs, self.labels = hf[\"sequence\"], hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(SPRDataset(spr[\"train\"]), BATCH, True, collate_fn=collate)\ndev_loader = DataLoader(SPRDataset(spr[\"dev\"]), BATCH, False, collate_fn=collate)\n\n\n# ---------------------------- model definitions --------------------------------\nclass BaselineTransformer(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_l = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_l, n_layer)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~mask.bool())\n        return self.cls(x[:, 0])\n\n\nclass SymbolicAugTransformer(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256, sym_dim=64):\n        super().__init__()\n        self.baseline = BaselineTransformer(d_model, n_head, n_layer, ff)\n        # symbolic path\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(vocab_size, sym_dim), nn.ReLU(), nn.LayerNorm(sym_dim)\n        )\n        self.final = nn.Linear(d_model + sym_dim, num_labels)\n\n    def forward(self, ids, mask):\n        # transformer branch\n        x = self.baseline.emb(ids) + self.baseline.pos[:, : ids.size(1)]\n        x = self.baseline.enc(x, src_key_padding_mask=~mask.bool())\n        h_token = x[:, 0]  # (B,d_model)\n        # symbolic count branch\n        counts = []\n        for seq in ids:  # loop over batch\n            c = torch.bincount(seq, minlength=vocab_size)\n            counts.append(c)\n        counts = torch.stack(counts).float().to(device)  # (B,V)\n        h_sym = self.sym_mlp(counts)\n        h = torch.cat([h_token, h_sym], dim=-1)\n        return self.final(h)\n\n\n# ------------------------- training / evaluation utils -------------------------\ncrit = nn.CrossEntropyLoss()\n\n\ndef loop(model, loader, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = crit(logits, batch[\"labels\"])\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)  # used as RGA proxy\n    return tot_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\n# --------------------------------- run exp -------------------------------------\ndef train_model(name, model, epochs=3, lr=3e-4):\n    model.to(device)\n    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n    for epoch in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _, _ = loop(model, train_loader, optim)\n        val_loss, val_f1, val_acc, pred, gt = loop(model, dev_loader)\n        ed = experiment_data[\"SPR_BENCH\"][name]\n        ed[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1, \"RGA\": None})\n        ed[\"metrics\"][\"val\"].append(\n            {\"epoch\": epoch, \"macro_f1\": val_f1, \"RGA\": val_acc}\n        )\n        ed[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        ed[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n        print(\n            f\"{name} Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"trainF1={tr_f1:.3f} valF1={val_f1:.3f} RGA={val_acc:.3f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n    ed[\"predictions\"], ed[\"ground_truth\"] = pred, gt\n\n\ntrain_model(\"Baseline\", BaselineTransformer())\ntrain_model(\"SymbolicAug\", SymbolicAugTransformer())\n\n# --------------------------------- save results --------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data.\")\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------\n# working dir & device ----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------\n# experiment data dict ----------------------------------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -----------------------------------------------------------\n# load SPR ---------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\ndset = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in dset.items()})\n\n# -----------------------------------------------------------\n# vocab ------------------------------------------------------\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\nfor seq in dset[\"train\"][\"sequence\"]:\n    for tok in seq.split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(dset[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\n\n# -----------------------------------------------------------\n# dataset ----------------------------------------------------\nMAX_LEN = 128\nNGRAM_V = 256  # symbolic hash vector size\n\n\ndef encode_seq(seq):\n    toks = [CLS] + seq.split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\ndef ngram_hash(seq, V=NGRAM_V):\n    toks = seq.split()\n    vec = np.zeros(V, dtype=np.float32)\n    for i in range(len(toks)):\n        unigram = toks[i]\n        h1 = hash(unigram) % V\n        vec[h1] += 1.0\n        if i + 1 < len(toks):\n            bigram = unigram + \"|\" + toks[i + 1]\n            h2 = hash(bigram) % V\n            vec[h2] += 1.0\n    return vec / (len(toks) + 1e-6)\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split, take=None):\n        idxs = list(range(len(hf_split)))\n        if take:\n            idxs = random.sample(idxs, take)\n        self.seqs = [hf_split[i][\"sequence\"] for i in idxs]\n        self.labels = [hf_split[i][\"label\"] for i in idxs]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_seq(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"symb_feat\": torch.tensor(ngram_hash(self.seqs[idx]), dtype=torch.float),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds = SPRDataset(dset[\"train\"], take=8000)  # small subset for speed\ndev_ds = SPRDataset(dset[\"dev\"])\nBATCH = 64\n\n\ndef collate(b):\n    out = {}\n    for k in b[0]:\n        out[k] = torch.stack([x[k] for x in b])\n    return out\n\n\ntrain_loader = DataLoader(train_ds, BATCH, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, BATCH, shuffle=False, collate_fn=collate)\n\n\n# -----------------------------------------------------------\n# model ------------------------------------------------------\nclass SymbolicReasoningModule(nn.Module):\n    def __init__(self, in_dim=NGRAM_V, hid=128, out_dim=64):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, hid), nn.ReLU(), nn.Linear(hid, out_dim), nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass HybridModel(nn.Module):\n    def __init__(\n        self, vocab_sz, num_labels, d_model=128, symb_dim=NGRAM_V, symb_out=64\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead=4, dim_feedforward=2 * d_model, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=2)\n        self.symb = SymbolicReasoningModule(symb_dim, hid=128, out_dim=symb_out)\n        self.classifier = nn.Linear(d_model + symb_out, num_labels)\n\n    def forward(self, ids, mask, symb_feat):\n        x = self.embed(ids) + self.pos[:, : ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~mask.bool())\n        cls_tok = x[:, 0, :]\n        symb_vec = self.symb(symb_feat)\n        logits = self.classifier(torch.cat([cls_tok, symb_vec], dim=-1))\n        return logits\n\n\nmodel = HybridModel(vocab_size, num_labels).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\ncriterion = nn.CrossEntropyLoss()\n\n\n# -----------------------------------------------------------\n# train / eval ----------------------------------------------\ndef run_epoch(model, loader, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"symb_feat\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)\n    return avg_loss, f1, acc, preds, gts\n\n\nEPOCHS = 5\nfor ep in range(1, EPOCHS + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, tr_acc, _, _ = run_epoch(model, train_loader, optimizer)\n    val_loss, val_f1, val_acc, val_pred, val_gt = run_epoch(model, dev_loader)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"epoch\": ep, \"macro_f1\": tr_f1, \"RGA\": tr_acc}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": ep, \"macro_f1\": val_f1, \"RGA\": val_acc}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        {\"epoch\": ep, \"loss\": tr_loss}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        {\"epoch\": ep, \"loss\": val_loss}\n    )\n    print(\n        f\"Epoch {ep}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_F1={tr_f1:.4f} val_F1={val_f1:.4f} RGA={val_acc:.4f} \"\n        f\"({time.time()-t0:.1f}s)\"\n    )\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = val_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_gt\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, random, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# ------------------- boilerplate + reproducibility -----------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"MultiTask\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------- data loading ----------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\nDATA_DIR = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(DATA_DIR)\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------- vocabulary ------------------------------------\nCLS, PAD, UNK, MASK = \"[CLS]\", \"[PAD]\", \"[UNK]\", \"[MASK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2, MASK: 3}\n\n\ndef add_tok(t):\n    if t not in vocab:\n        vocab[t] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for t in seq.strip().split():\n        add_tok(t)\nvocab_size = len(vocab)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(labels)\nprint(\"vocab\", vocab_size, \"labels\", num_labels)\n\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    pad = MAX_LEN - len(ids)\n    if pad > 0:\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, mask = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(mask),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------- models ----------------------------------------\nclass BaseEncoder(nn.Module):\n    def __init__(self, d_model=128, nhead=4, nlayer=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, nlayer)\n\n    def forward(self, ids, attn):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~attn.bool())\n        return x[:, 0]  # CLS repr\n\n\nclass BaselineClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = BaseEncoder()\n        self.cls_head = nn.Linear(128, num_labels)\n\n    def forward(self, ids, attn, mlm_labels=None):\n        h = self.encoder(ids, attn)\n        return self.cls_head(h), None  # second output kept for compat\n\n\nclass MultiTaskTransformer(nn.Module):\n    def __init__(self, mlm_lambda=0.5):\n        super().__init__()\n        self.lmb = mlm_lambda\n        self.encoder = BaseEncoder()\n        self.cls_head = nn.Linear(128, num_labels)\n        self.mlm_decoder = nn.Linear(128, vocab_size)\n        # tie weights with embedding\n        self.mlm_decoder.weight = self.encoder.emb.weight\n\n    def _mask(self, ids):\n        ids = ids.clone()\n        rand = torch.rand(ids.shape)\n        mask_arr = (rand < 0.15) & (ids != vocab[PAD]) & (ids != vocab[CLS])\n        mlm_labels = ids.clone()\n        mlm_labels[~mask_arr] = -100\n        # 80% mask token\n        ids[mask_arr & (torch.rand(ids.shape) < 0.8)] = vocab[MASK]\n        # 10% random token\n        random_tok = torch.randint(4, vocab_size, ids.shape, device=ids.device)\n        ids[mask_arr & (torch.rand(ids.shape) < 0.1)] = random_tok[\n            mask_arr & (torch.rand(ids.shape) < 0.1)\n        ]\n        # 10% keep original\n        return ids, mlm_labels\n\n    def forward(self, ids, attn, mlm_labels=None, training=False):\n        if training:\n            ids, mlm_labels = self._mask(ids)\n        h_seq = self.encoder.emb(ids) + self.encoder.pos[:, : ids.size(1)]\n        h_seq = self.encoder.enc(h_seq, src_key_padding_mask=~attn.bool())\n        h_cls = h_seq[:, 0]\n        logits_cls = self.cls_head(h_cls)\n        logits_mlm = None\n        if training:\n            logits_mlm = self.mlm_decoder(h_seq)\n        return logits_cls, (logits_mlm, mlm_labels)\n\n\n# --------------------------- training utilities --------------------------------\nce_loss = nn.CrossEntropyLoss()\nce_loss_ignore = nn.CrossEntropyLoss(ignore_index=-100)\n\n\ndef run_epoch(model, loader, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        if train:\n            logits_cls, (logits_mlm, mlm_labels) = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], training=True\n            )\n            loss_cls = ce_loss(logits_cls, batch[\"labels\"])\n            loss_mlm = ce_loss_ignore(\n                logits_mlm.view(-1, vocab_size), mlm_labels.view(-1)\n            )\n            loss = loss_cls + model.lmb * loss_mlm\n        else:\n            logits_cls, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = ce_loss(logits_cls, batch[\"labels\"])\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits_cls, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)  # proxy for RGA\n    return total_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\n# ----------------------------- training loops ----------------------------------\ndef train_model(name, model, epochs=4, lr=3e-4):\n    model.to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t = time.time()\n        tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, opt)\n        val_loss, val_f1, val_acc, pred, gt = run_epoch(model, dev_loader)\n        ed = experiment_data[\"SPR_BENCH\"][name]\n        ed[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        ed[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        ed[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_F1\": tr_f1, \"RGA\": None})\n        ed[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_F1\": val_f1, \"RGA\": val_acc})\n        print(\n            f\"{name} Ep{ep}: train_loss={tr_loss:.3f} val_loss={val_loss:.3f} \"\n            f\"valF1={val_f1:.3f} RGA={val_acc:.3f} ({time.time()-t:.1f}s)\"\n        )\n    ed[\"predictions\"] = pred\n    ed[\"ground_truth\"] = gt\n\n\n# ----------------------------- run experiments ---------------------------------\ntrain_model(\"Baseline\", BaselineClassifier(), epochs=4)\ntrain_model(\"MultiTask\", MultiTaskTransformer(), epochs=4)\n\n# ----------------------------- save everything ---------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- mandatory boiler-plate ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- reproducibility ----------\nseed = 1234\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# ---------- experiment tracking ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n        },\n        \"AuxCL\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n        },\n    }\n}\n\n\n# ---------- dataset ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv):\n        return load_dataset(\n            \"csv\", data_files=str(root / csv), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------- vocabulary ----------\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\nfor s in spr[\"train\"][\"sequence\"]:\n    for tok in s.split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nnum_labels = len(label2id)\nMAX_LEN = 128\n\n\ndef encode(seq: str):\n    toks = [CLS] + seq.split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seqs, self.labels = hf[\"sequence\"], hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_dl = DataLoader(SPRDataset(spr[\"train\"]), BATCH, shuffle=True, collate_fn=collate)\ndev_dl = DataLoader(SPRDataset(spr[\"dev\"]), BATCH, shuffle=False, collate_fn=collate)\n\n\n# ---------- models ----------\nclass Encoder(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n\n    def forward(self, ids, mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~mask.bool())\n        return x  # (B,T,d)\n\n\nclass Baseline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.enc = Encoder()\n        self.cls = nn.Linear(128, num_labels)\n\n    def forward(self, ids, mask):\n        h = self.enc(ids, mask)\n        return self.cls(h[:, 0])\n\n\nclass AuxCL(nn.Module):\n    def __init__(self, mlm_coef=0.5):\n        super().__init__()\n        self.mlm_coef = mlm_coef\n        self.enc = Encoder()\n        self.cls_head = nn.Linear(128, num_labels)\n        self.tied_decoder = nn.Linear(128, vocab_size, bias=False)  # weight tying\n        self.tied_decoder.weight = self.enc.emb.weight\n\n    def forward(self, ids, mask, mlm_labels=None):\n        h = self.enc(ids, mask)\n        logits_cls = self.cls_head(h[:, 0])\n        if mlm_labels is None:\n            return logits_cls, None\n        token_logits = self.tied_decoder(h)  # (B,T,V)\n        return logits_cls, token_logits\n\n\n# ---------- MLM helper ----------\ndef create_mlm_inputs(batch_ids, mask_prob=0.15):\n    input_ids = batch_ids.clone()\n    mlm_labels = torch.full_like(input_ids, -100)  # ignore index\n    rand = torch.rand(input_ids.shape)\n    mask_arr = (\n        (rand < mask_prob) & (input_ids != vocab[PAD]) & (input_ids != vocab[CLS])\n    )\n    mlm_labels[mask_arr] = input_ids[mask_arr]\n    input_ids[mask_arr] = vocab[UNK]\n    return input_ids, mlm_labels\n\n\n# ---------- train / eval ----------\nce_cls = nn.CrossEntropyLoss()\nce_mlm = nn.CrossEntropyLoss(ignore_index=-100)\n\n\ndef run_epoch(model, dl, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in dl:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        ids, attn, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n        if isinstance(model, AuxCL):\n            if train:\n                ids_masked, mlm_labs = create_mlm_inputs(ids)\n                ids_masked, mlm_labs = ids_masked.to(device), mlm_labs.to(device)\n                logits_cls, token_logits = model(ids_masked, attn, mlm_labs)\n                loss_cls = ce_cls(logits_cls, labels)\n                loss_mlm = ce_mlm(token_logits.view(-1, vocab_size), mlm_labs.view(-1))\n                loss = loss_cls + model.mlm_coef * loss_mlm\n            else:\n                logits_cls, _ = model(ids, attn, None)\n                loss = ce_cls(logits_cls, labels)\n        else:\n            logits_cls = model(ids, attn)\n            loss = ce_cls(logits_cls, labels)\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        tot_loss += loss.item() * labels.size(0)\n        preds.extend(torch.argmax(logits_cls, 1).cpu().tolist())\n        gts.extend(labels.cpu().tolist())\n    macroF1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)  # proxy for RGA\n    return tot_loss / len(dl.dataset), macroF1, acc\n\n\ndef train_model(name, model, epochs=3, lr=3e-4):\n    model.to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _ = run_epoch(model, train_dl, opt)\n        val_loss, val_f1, val_rga = run_epoch(model, dev_dl, None)\n        experiment_data[\"SPR_BENCH\"][name][\"losses\"][\"train\"].append(\n            {\"epoch\": ep, \"loss\": tr_loss}\n        )\n        experiment_data[\"SPR_BENCH\"][name][\"losses\"][\"val\"].append(\n            {\"epoch\": ep, \"loss\": val_loss}\n        )\n        experiment_data[\"SPR_BENCH\"][name][\"metrics\"][\"train\"].append(\n            {\"epoch\": ep, \"macro_f1\": tr_f1, \"RGA\": None}\n        )\n        experiment_data[\"SPR_BENCH\"][name][\"metrics\"][\"val\"].append(\n            {\"epoch\": ep, \"macro_f1\": val_f1, \"RGA\": val_rga}\n        )\n        print(\n            f\"{name} Epoch {ep}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"trainF1={tr_f1:.3f} valF1={val_f1:.3f} RGA={val_rga:.3f} ({time.time()-t0:.1f}s)\"\n        )\n\n\n# ---------- run ----------\ntrain_model(\"Baseline\", Baseline(), epochs=2)\ntrain_model(\"AuxCL\", AuxCL(), epochs=2)\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data at\", working_dir)\n", "import os, pathlib, random, time, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------ working dir & device ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ------------------ reproducibility -----------------------\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n\n# ------------------ data loading --------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------ vocab construction -------------------\nCLS, PAD, UNK, MASK = \"[CLS]\", \"[PAD]\", \"[UNK]\", \"[MASK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2, MASK: 3}\n\n\ndef add(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for t in seq.split():\n        add(t)\nvocab_size = len(vocab)\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"Vocab={vocab_size}, Labels={num_labels}\")\n\n# ------------------ encode utils -------------------------\nMAX_LEN = 128\n\n\ndef encode(seq: str):\n    toks = [CLS] + seq.split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seq = hf[\"sequence\"]\n        self.lab = hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        ids, att = encode(self.seq[idx])\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(att),\n            \"labels\": torch.tensor(label2id[self.lab[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------ model defs ---------------------------\nclass Encoder(nn.Module):\n    def __init__(self, d_model=128, heads=4, layers=2, ff=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        layer = nn.TransformerEncoderLayer(d_model, heads, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(layer, layers)\n\n    def forward(self, ids, mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~mask.bool())\n        return x\n\n\nclass Baseline(nn.Module):\n    def __init__(self, d_model=128):\n        super().__init__()\n        self.encoder = Encoder(d_model)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, mask):\n        h = self.encoder(ids, mask)\n        return self.cls(h[:, 0])\n\n\nclass MultiTask(nn.Module):\n    def __init__(self, d_model=128):\n        super().__init__()\n        self.encoder = Encoder(d_model)\n        self.cls_head = nn.Linear(d_model, num_labels)\n        # MLM head ties weights with embedding\n        self.lm_decoder = nn.Linear(d_model, vocab_size, bias=False)\n        self.lm_decoder.weight = self.encoder.emb.weight\n\n    def forward(self, ids, mask, mlm_labels=None):\n        h = self.encoder(ids, mask)\n        logits_cls = self.cls_head(h[:, 0])\n        if mlm_labels is None:\n            return logits_cls, None\n        logits_mlm = self.lm_decoder(h)  # (B,L,V)\n        return logits_cls, logits_mlm\n\n\n# ------------------ masking util -------------------------\ndef create_mlm_inputs(input_ids, prob=0.15):\n    ids = input_ids.clone()\n    labels = torch.full_like(ids, -100)\n    # skip CLS token at pos0\n    rand = torch.rand_like(ids.float())\n    mask_sel = (rand < prob) & (ids != vocab[PAD]) & (ids != vocab[CLS])\n    labels[mask_sel] = ids[mask_sel]\n    ids[mask_sel] = vocab[MASK]\n    return ids, labels\n\n\n# ------------------ training / evaluation ---------------\nce = nn.CrossEntropyLoss(ignore_index=-100)\n\n\ndef run_epoch(model, loader, opt=None):\n    train = opt is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        if isinstance(model, MultiTask):\n            ids_mlm, mlm_lbl = create_mlm_inputs(batch[\"input_ids\"])\n            ids_mlm = ids_mlm.to(device)\n            mlm_lbl = mlm_lbl.to(device)\n            out_cls, out_mlm = model(ids_mlm, batch[\"attention_mask\"], mlm_lbl)\n            loss_cls = ce(out_cls, batch[\"labels\"])\n            loss_mlm = ce(out_mlm.view(-1, vocab_size), mlm_lbl.view(-1))\n            loss = loss_cls + 0.5 * loss_mlm\n        else:\n            out_cls = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = ce(out_cls, batch[\"labels\"])\n            loss_cls, loss_mlm = loss, torch.tensor(0.0, device=device)\n        if train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(out_cls, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)  # proxy for RGA\n    return tot_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\n# ------------------ experiment dict ----------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n        },\n        \"MultiTask\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n        },\n    }\n}\n\n\ndef train_model(name, model, epochs=3, lr=3e-4):\n    model.to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, opt)\n        val_loss, val_f1, val_acc, pred, gt = run_epoch(model, dev_loader)\n        ed = experiment_data[\"SPR_BENCH\"][name]\n        ed[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_F1\": tr_f1, \"RGA\": None})\n        ed[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_F1\": val_f1, \"RGA\": val_acc})\n        ed[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        ed[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        print(\n            f\"{name} Epoch{ep}: val_loss={val_loss:.4f} val_F1={val_f1:.3f} RGA={val_acc:.3f} ({time.time()-t0:.1f}s)\"\n        )\n    ed[\"predictions\"] = pred\n    ed[\"ground_truth\"] = gt\n\n\n# ------------------ run both models ----------------------\ntrain_model(\"Baseline\", Baseline())\ntrain_model(\"MultiTask\", MultiTask())\n\n# ------------------ save & exit --------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "import os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------ paths / utils\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"SymToken\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------------------------------------ dataset / encoding\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\ndsets = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\n\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = seq.strip().split()\n    ids = [vocab[CLS]] + [vocab.get(t, vocab[UNK]) for t in toks]\n    ids = ids[:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seqs, self.labels = hf[\"sequence\"], hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        token_ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(token_ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------- models\nclass BaselineTransformer(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.randn(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~attn_mask.bool())\n        return self.cls(x[:, 0])\n\n\nclass SymbolicTokenTransformer(nn.Module):\n    \"\"\"\n    Prepends a 'reasoning' token derived from bag-of-symbols counts,\n    allowing full cross-attention with sequence tokens.\n    \"\"\"\n\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.sym_proj = nn.Linear(vocab_size, d_model)\n        self.pos = nn.Parameter(\n            torch.randn(1, MAX_LEN + 2, d_model)\n        )  # +2 for CLS + sym\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.gate = nn.Sequential(nn.Linear(d_model, d_model), nn.Sigmoid())\n        self.head = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        B, L = ids.shape\n        # bag-of-symbols counts\n        counts = torch.zeros(B, vocab_size, device=ids.device)\n        ones = torch.ones_like(ids, dtype=torch.float)\n        counts.scatter_add_(1, ids, ones)\n        sym_tok = self.sym_proj(counts).unsqueeze(1)  # (B,1,d)\n        cls_tok = self.token_emb(torch.full((B, 1), vocab[CLS], device=ids.device))\n        tok_emb = self.token_emb(ids)  # (B,L,d)\n        x = torch.cat([cls_tok, sym_tok, tok_emb], dim=1) + self.pos[:, : L + 2]\n        # build new attention mask (1 for real, 0 for pad)\n        new_mask = torch.cat([torch.ones(B, 2, device=ids.device), attn_mask], dim=1)\n        x = self.enc(x, src_key_padding_mask=~new_mask.bool())\n        cls_out = x[:, 0]\n        gated = cls_out * self.gate(cls_out)\n        return self.head(gated)\n\n\n# ---------------------------------------------------------------- training helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optim=None):\n    train = optim is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)\n    return tot_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\ndef train(model, name, epochs=3, lr=3e-4):\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_acc, preds, gts = run_epoch(model, dev_loader)\n        exp = experiment_data[\"SPR_BENCH\"][name]\n        exp[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        exp[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        exp[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_f1\": tr_f1, \"RGA\": None})\n        exp[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_f1\": val_f1, \"RGA\": val_acc})\n        print(\n            f\"{name} Epoch {ep}: tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"trF1={tr_f1:.3f} valF1={val_f1:.3f} RGA={val_acc:.3f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n    exp[\"predictions\"], exp[\"ground_truth\"] = preds, gts\n\n\n# ------------------------------------------------------------------- run training\ntrain(BaselineTransformer(), \"Baseline\")\ntrain(SymbolicTokenTransformer(), \"SymToken\")\n\n# ------------------------------------------------------------ save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------ paths / utils\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"SymToken\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------------------------------------ dataset / encoding\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\ndsets = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\n\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = seq.strip().split()\n    ids = [vocab[CLS]] + [vocab.get(t, vocab[UNK]) for t in toks]\n    ids = ids[:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seqs, self.labels = hf[\"sequence\"], hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        token_ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(token_ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------- models\nclass BaselineTransformer(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.randn(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~attn_mask.bool())\n        return self.cls(x[:, 0])\n\n\nclass SymbolicTokenTransformer(nn.Module):\n    \"\"\"\n    Prepends a 'reasoning' token derived from bag-of-symbols counts,\n    allowing full cross-attention with sequence tokens.\n    \"\"\"\n\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.sym_proj = nn.Linear(vocab_size, d_model)\n        self.pos = nn.Parameter(\n            torch.randn(1, MAX_LEN + 2, d_model)\n        )  # +2 for CLS + sym\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.gate = nn.Sequential(nn.Linear(d_model, d_model), nn.Sigmoid())\n        self.head = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        B, L = ids.shape\n        # bag-of-symbols counts\n        counts = torch.zeros(B, vocab_size, device=ids.device)\n        ones = torch.ones_like(ids, dtype=torch.float)\n        counts.scatter_add_(1, ids, ones)\n        sym_tok = self.sym_proj(counts).unsqueeze(1)  # (B,1,d)\n        cls_tok = self.token_emb(torch.full((B, 1), vocab[CLS], device=ids.device))\n        tok_emb = self.token_emb(ids)  # (B,L,d)\n        x = torch.cat([cls_tok, sym_tok, tok_emb], dim=1) + self.pos[:, : L + 2]\n        # build new attention mask (1 for real, 0 for pad)\n        new_mask = torch.cat([torch.ones(B, 2, device=ids.device), attn_mask], dim=1)\n        x = self.enc(x, src_key_padding_mask=~new_mask.bool())\n        cls_out = x[:, 0]\n        gated = cls_out * self.gate(cls_out)\n        return self.head(gated)\n\n\n# ---------------------------------------------------------------- training helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optim=None):\n    train = optim is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)\n    return tot_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\ndef train(model, name, epochs=3, lr=3e-4):\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_acc, preds, gts = run_epoch(model, dev_loader)\n        exp = experiment_data[\"SPR_BENCH\"][name]\n        exp[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        exp[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        exp[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_f1\": tr_f1, \"RGA\": None})\n        exp[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_f1\": val_f1, \"RGA\": val_acc})\n        print(\n            f\"{name} Epoch {ep}: tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"trF1={tr_f1:.3f} valF1={val_f1:.3f} RGA={val_acc:.3f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n    exp[\"predictions\"], exp[\"ground_truth\"] = preds, gts\n\n\n# ------------------------------------------------------------------- run training\ntrain(BaselineTransformer(), \"Baseline\")\ntrain(SymbolicTokenTransformer(), \"SymToken\")\n\n# ------------------------------------------------------------ save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------ paths / utils\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"SymToken\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------------------------------------ dataset / encoding\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\ndsets = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\n\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = seq.strip().split()\n    ids = [vocab[CLS]] + [vocab.get(t, vocab[UNK]) for t in toks]\n    ids = ids[:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seqs, self.labels = hf[\"sequence\"], hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        token_ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(token_ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------- models\nclass BaselineTransformer(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.randn(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~attn_mask.bool())\n        return self.cls(x[:, 0])\n\n\nclass SymbolicTokenTransformer(nn.Module):\n    \"\"\"\n    Prepends a 'reasoning' token derived from bag-of-symbols counts,\n    allowing full cross-attention with sequence tokens.\n    \"\"\"\n\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.sym_proj = nn.Linear(vocab_size, d_model)\n        self.pos = nn.Parameter(\n            torch.randn(1, MAX_LEN + 2, d_model)\n        )  # +2 for CLS + sym\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.gate = nn.Sequential(nn.Linear(d_model, d_model), nn.Sigmoid())\n        self.head = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        B, L = ids.shape\n        # bag-of-symbols counts\n        counts = torch.zeros(B, vocab_size, device=ids.device)\n        ones = torch.ones_like(ids, dtype=torch.float)\n        counts.scatter_add_(1, ids, ones)\n        sym_tok = self.sym_proj(counts).unsqueeze(1)  # (B,1,d)\n        cls_tok = self.token_emb(torch.full((B, 1), vocab[CLS], device=ids.device))\n        tok_emb = self.token_emb(ids)  # (B,L,d)\n        x = torch.cat([cls_tok, sym_tok, tok_emb], dim=1) + self.pos[:, : L + 2]\n        # build new attention mask (1 for real, 0 for pad)\n        new_mask = torch.cat([torch.ones(B, 2, device=ids.device), attn_mask], dim=1)\n        x = self.enc(x, src_key_padding_mask=~new_mask.bool())\n        cls_out = x[:, 0]\n        gated = cls_out * self.gate(cls_out)\n        return self.head(gated)\n\n\n# ---------------------------------------------------------------- training helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optim=None):\n    train = optim is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)\n    return tot_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\ndef train(model, name, epochs=3, lr=3e-4):\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_acc, preds, gts = run_epoch(model, dev_loader)\n        exp = experiment_data[\"SPR_BENCH\"][name]\n        exp[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        exp[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        exp[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_f1\": tr_f1, \"RGA\": None})\n        exp[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_f1\": val_f1, \"RGA\": val_acc})\n        print(\n            f\"{name} Epoch {ep}: tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"trF1={tr_f1:.3f} valF1={val_f1:.3f} RGA={val_acc:.3f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n    exp[\"predictions\"], exp[\"ground_truth\"] = preds, gts\n\n\n# ------------------------------------------------------------------- run training\ntrain(BaselineTransformer(), \"Baseline\")\ntrain(SymbolicTokenTransformer(), \"SymToken\")\n\n# ------------------------------------------------------------ save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------ paths / utils\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"Baseline\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"SymToken\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\n\n# ------------------------------------------------------------ dataset / encoding\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\ndsets = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in dsets.items()})\n\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\nfor seq in dsets[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\nvocab_size = len(vocab)\n\nlabels = sorted(set(dsets[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(labels)}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\n\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = seq.strip().split()\n    ids = [vocab[CLS]] + [vocab.get(t, vocab[UNK]) for t in toks]\n    ids = ids[:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf):\n        self.seqs, self.labels = hf[\"sequence\"], hf[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        token_ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(token_ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH = 64\ntrain_loader = DataLoader(\n    SPRDataset(dsets[\"train\"]), batch_size=BATCH, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRDataset(dsets[\"dev\"]), batch_size=BATCH, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------- models\nclass BaselineTransformer(nn.Module):\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.randn(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        x = self.emb(ids) + self.pos[:, : ids.size(1)]\n        x = self.enc(x, src_key_padding_mask=~attn_mask.bool())\n        return self.cls(x[:, 0])\n\n\nclass SymbolicTokenTransformer(nn.Module):\n    \"\"\"\n    Prepends a 'reasoning' token derived from bag-of-symbols counts,\n    allowing full cross-attention with sequence tokens.\n    \"\"\"\n\n    def __init__(self, d_model=128, n_head=4, n_layer=2, ff=256):\n        super().__init__()\n        self.d_model = d_model\n        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.sym_proj = nn.Linear(vocab_size, d_model)\n        self.pos = nn.Parameter(\n            torch.randn(1, MAX_LEN + 2, d_model)\n        )  # +2 for CLS + sym\n        enc_layer = nn.TransformerEncoderLayer(d_model, n_head, ff, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, n_layer)\n        self.gate = nn.Sequential(nn.Linear(d_model, d_model), nn.Sigmoid())\n        self.head = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        B, L = ids.shape\n        # bag-of-symbols counts\n        counts = torch.zeros(B, vocab_size, device=ids.device)\n        ones = torch.ones_like(ids, dtype=torch.float)\n        counts.scatter_add_(1, ids, ones)\n        sym_tok = self.sym_proj(counts).unsqueeze(1)  # (B,1,d)\n        cls_tok = self.token_emb(torch.full((B, 1), vocab[CLS], device=ids.device))\n        tok_emb = self.token_emb(ids)  # (B,L,d)\n        x = torch.cat([cls_tok, sym_tok, tok_emb], dim=1) + self.pos[:, : L + 2]\n        # build new attention mask (1 for real, 0 for pad)\n        new_mask = torch.cat([torch.ones(B, 2, device=ids.device), attn_mask], dim=1)\n        x = self.enc(x, src_key_padding_mask=~new_mask.bool())\n        cls_out = x[:, 0]\n        gated = cls_out * self.gate(cls_out)\n        return self.head(gated)\n\n\n# ---------------------------------------------------------------- training helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optim=None):\n    train = optim is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    acc = accuracy_score(gts, preds)\n    return tot_loss / len(loader.dataset), macro_f1, acc, preds, gts\n\n\ndef train(model, name, epochs=3, lr=3e-4):\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_acc, preds, gts = run_epoch(model, dev_loader)\n        exp = experiment_data[\"SPR_BENCH\"][name]\n        exp[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        exp[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        exp[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_f1\": tr_f1, \"RGA\": None})\n        exp[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_f1\": val_f1, \"RGA\": val_acc})\n        print(\n            f\"{name} Epoch {ep}: tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"trF1={tr_f1:.3f} valF1={val_f1:.3f} RGA={val_acc:.3f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n    exp[\"predictions\"], exp[\"ground_truth\"] = preds, gts\n\n\n# ------------------------------------------------------------------- run training\ntrain(BaselineTransformer(), \"Baseline\")\ntrain(SymbolicTokenTransformer(), \"SymToken\")\n\n# ------------------------------------------------------------ save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 114767.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 143365.60\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 234502.07\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training with\nd_model=64 ===', '\\n', 'd_model=64 | Epoch 1: train_loss=0.6256,\nval_loss=0.6493, train_F1=0.6890, val_F1=0.6506 (1.0s)', '\\n', 'd_model=64 |\nEpoch 2: train_loss=0.2232, val_loss=1.2567, train_F1=0.9555, val_F1=0.6799\n(0.7s)', '\\n', 'd_model=64 | Epoch 3: train_loss=0.0717, val_loss=1.5965,\ntrain_F1=0.9805, val_F1=0.6920 (0.7s)', '\\n', 'd_model=64 | Epoch 4:\ntrain_loss=0.0555, val_loss=1.7007, train_F1=0.9840, val_F1=0.6920 (0.7s)',\n'\\n', 'd_model=64 | Epoch 5: train_loss=0.0472, val_loss=1.8795,\ntrain_F1=0.9875, val_F1=0.6879 (0.7s)', '\\n', '\\n=== Training with d_model=128\n===', '\\n', 'd_model=128 | Epoch 1: train_loss=0.2858, val_loss=1.4873,\ntrain_F1=0.8915, val_F1=0.6839 (0.7s)', '\\n', 'd_model=128 | Epoch 2:\ntrain_loss=0.0649, val_loss=1.9645, train_F1=0.9805, val_F1=0.6879 (0.7s)',\n'\\n', 'd_model=128 | Epoch 3: train_loss=0.0470, val_loss=2.2001,\ntrain_F1=0.9885, val_F1=0.6879 (0.7s)', '\\n', 'd_model=128 | Epoch 4:\ntrain_loss=0.0435, val_loss=2.1868, train_F1=0.9910, val_F1=0.6879 (0.7s)',\n'\\n', 'd_model=128 | Epoch 5: train_loss=0.0447, val_loss=2.0346,\ntrain_F1=0.9895, val_F1=0.6900 (0.7s)', '\\n', '\\n=== Training with d_model=256\n===', '\\n', 'd_model=256 | Epoch 1: train_loss=0.2619, val_loss=1.9211,\ntrain_F1=0.8819, val_F1=0.6879 (1.0s)', '\\n', 'd_model=256 | Epoch 2:\ntrain_loss=0.0716, val_loss=2.3180, train_F1=0.9800, val_F1=0.6859 (1.0s)',\n'\\n', 'd_model=256 | Epoch 3: train_loss=0.0544, val_loss=2.4372,\ntrain_F1=0.9860, val_F1=0.6940 (1.0s)', '\\n', 'd_model=256 | Epoch 4:\ntrain_loss=0.0423, val_loss=2.5196, train_F1=0.9880, val_F1=0.6899 (1.0s)',\n'\\n', 'd_model=256 | Epoch 5: train_loss=0.0516, val_loss=2.3775,\ntrain_F1=0.9870, val_F1=0.6859 (1.0s)', '\\n', '\\n=== Training with d_model=384\n===', '\\n', 'd_model=384 | Epoch 1: train_loss=0.2699, val_loss=2.1986,\ntrain_F1=0.8764, val_F1=0.6820 (1.3s)', '\\n', 'd_model=384 | Epoch 2:\ntrain_loss=0.0652, val_loss=2.2939, train_F1=0.9830, val_F1=0.6940 (1.3s)',\n'\\n', 'd_model=384 | Epoch 3: train_loss=0.0590, val_loss=2.4175,\ntrain_F1=0.9840, val_F1=0.6900 (1.3s)', '\\n', 'd_model=384 | Epoch 4:\ntrain_loss=0.0514, val_loss=2.6437, train_F1=0.9895, val_F1=0.6818 (1.3s)',\n'\\n', 'd_model=384 | Epoch 5: train_loss=0.0594, val_loss=2.5905,\ntrain_F1=0.9840, val_F1=0.6960 (1.3s)', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 22 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 137266.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 100447.94\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 231358.82\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Baseline ===',\n'\\n', 'baseline | Ep 1: train_loss=0.3424 val_loss=1.3783 train_F1=0.8705\nval_F1=0.6920 (1.3s)', '\\n', 'baseline | Ep 2: train_loss=0.0620 val_loss=1.9681\ntrain_F1=0.9845 val_F1=0.6940 (1.0s)', '\\n', 'baseline | Ep 3: train_loss=0.0528\nval_loss=2.0515 train_F1=0.9865 val_F1=0.6920 (1.0s)', '\\n', '\\n=== Symbolic\nAugmented ===', '\\n', 'symbolic | Ep 1: train_loss=0.3962 val_loss=1.3270\ntrain_F1=0.8279 val_F1=0.6776 (1.2s)', '\\n', 'symbolic | Ep 2: train_loss=0.0849\nval_loss=1.7700 train_F1=0.9755 val_F1=0.6940 (1.2s)', '\\n', 'symbolic | Ep 3:\ntrain_loss=0.0569 val_loss=1.8913 train_F1=0.9845 val_F1=0.6899 (1.1s)', '\\n',\n'\\nSymbolic model TEST: loss=1.9074  F1=0.6959  ACC=0.6960', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_23-44-22_conceptual_generalization_poly_rule_attempt_0/0-\nrun/process_ForkProcess-11/working/experiment_data.npy', '\\n', 'Execution time:\n10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 57, in <module>\\n    spr = load_spr_bench(SPR_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 50, in load_spr_bench\\n\ndd[\"train\"] = _load(\"train.csv\")\\n                  ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 42, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n12/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 164321.41\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 121201.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 188448.76\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab:19\nLabels:2', '\\n', 'Baseline Epoch 1: train_loss=0.3178 val_loss=1.5119\ntrainF1=0.875 valF1=0.676 RGA=0.676 (0.7s)', '\\n', 'Baseline Epoch 2:\ntrain_loss=0.0650 val_loss=1.7365 trainF1=0.980 valF1=0.690 RGA=0.690 (0.4s)',\n'\\n', 'Baseline Epoch 3: train_loss=0.0554 val_loss=1.7193 trainF1=0.986\nvalF1=0.684 RGA=0.684 (0.3s)', '\\n', 'SymbolicAug Epoch 1: train_loss=0.3582\nval_loss=1.3668 trainF1=0.823 valF1=0.688 RGA=0.688 (1.0s)', '\\n', 'SymbolicAug\nEpoch 2: train_loss=0.0910 val_loss=1.5761 trainF1=0.971 valF1=0.698 RGA=0.698\n(1.0s)', '\\n', 'SymbolicAug Epoch 3: train_loss=0.0726 val_loss=1.7434\ntrainF1=0.977 valF1=0.696 RGA=0.696 (1.1s)', '\\n', 'Saved experiment data.',\n'\\n', 'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 115821.56\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 93447.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 143561.88\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 120, in <module>\\n    train_ds = SPRDataset(dset[\"train\"],\ntake=8000)  # small subset for speed\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 103, in\n__init__\\n    idxs = random.sample(idxs, take)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/random.py\", line\n456, in sample\\n    raise ValueError(\"Sample larger than population or is\nnegative\")\\nValueError: Sample larger than population or is negative\\n',\n'Execution time: a second seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'vocab', ' ', '20', ' ', 'labels', ' ', '2', '\\n', 'Traceback (most recent call\nlast):\\n  File \"runfile.py\", line 240, in <module>\\n    train_model(\"Baseline\",\nBaselineClassifier(), epochs=4)\\n  File \"runfile.py\", line 224, in train_model\\n\ntr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, opt)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 195, in\nrun_epoch\\n    logits_cls, (logits_mlm, mlm_labels) = model(\\n\n^^^^^^\\n  File \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: BaselineClassifier.forward() got an\nunexpected keyword argument \\'training\\'\\n', 'Execution time: 2 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 124895.53\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 116852.51\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 184081.81\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Baseline\nEpoch 1: train_loss=0.3610 val_loss=1.4333 trainF1=0.845 valF1=0.690 RGA=0.690\n(0.5s)', '\\n', 'Baseline Epoch 2: train_loss=0.0571 val_loss=2.0580\ntrainF1=0.986 valF1=0.688 RGA=0.688 (0.3s)', '\\n', 'Traceback (most recent call\nlast):\\n  File \"runfile.py\", line 226, in <module>\\n    train_model(\"AuxCL\",\nAuxCL(), epochs=2)\\n  File \"runfile.py\", line 204, in train_model\\n    tr_loss,\ntr_f1, _ = run_epoch(model, train_dl, opt)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 175, in run_epoch\\n\nids_masked, mlm_labs = create_mlm_inputs(ids)\\n\n^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 154, in create_mlm_inputs\\n\n(rand < mask_prob) & (input_ids != vocab[PAD]) & (input_ids != vocab[CLS])\\n\n~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\\nRuntimeError: Expected all\ntensors to be on the same device, but found at least two devices, cuda:0 and\ncpu!\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab=20, Labels=2', '\\n', 'Baseline Epoch1: val_loss=1.4615 val_F1=0.686\nRGA=0.686 (0.5s)', '\\n', 'Baseline Epoch2: val_loss=1.6866 val_F1=0.694\nRGA=0.694 (0.3s)', '\\n', 'Baseline Epoch3: val_loss=1.8061 val_F1=0.690\nRGA=0.690 (0.3s)', '\\n', 'MultiTask Epoch1: val_loss=3.2669 val_F1=0.631\nRGA=0.632 (0.3s)', '\\n', 'MultiTask Epoch2: val_loss=2.1305 val_F1=0.630\nRGA=0.636 (0.3s)', '\\n', 'MultiTask Epoch3: val_loss=2.1497 val_F1=0.600\nRGA=0.602 (0.3s)', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Baseline Epoch 1: tr_loss=0.4121 val_loss=1.3240 trF1=0.815 valF1=0.688\nRGA=0.688 (1.1s)', '\\n', 'Baseline Epoch 2: tr_loss=0.0751 val_loss=1.8213\ntrF1=0.977 valF1=0.692 RGA=0.692 (0.8s)', '\\n', 'Baseline Epoch 3:\ntr_loss=0.0405 val_loss=1.9424 trF1=0.988 valF1=0.698 RGA=0.698 (0.8s)', '\\n',\n'SymToken Epoch 1: tr_loss=0.6287 val_loss=0.6744 trF1=0.646 valF1=0.682\nRGA=0.682 (0.8s)', '\\n', 'SymToken Epoch 2: tr_loss=0.1318 val_loss=1.4574\ntrF1=0.967 valF1=0.696 RGA=0.696 (0.8s)', '\\n', 'SymToken Epoch 3:\ntr_loss=0.0489 val_loss=1.5619 trF1=0.985 valF1=0.696 RGA=0.696 (0.8s)', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Baseline Epoch 1: tr_loss=0.4121 val_loss=1.3240 trF1=0.815 valF1=0.688\nRGA=0.688 (1.2s)', '\\n', 'Baseline Epoch 2: tr_loss=0.0751 val_loss=1.8213\ntrF1=0.977 valF1=0.692 RGA=0.692 (0.8s)', '\\n', 'Baseline Epoch 3:\ntr_loss=0.0405 val_loss=1.9424 trF1=0.988 valF1=0.698 RGA=0.698 (0.8s)', '\\n',\n'SymToken Epoch 1: tr_loss=0.6287 val_loss=0.6744 trF1=0.646 valF1=0.682\nRGA=0.682 (0.9s)', '\\n', 'SymToken Epoch 2: tr_loss=0.1318 val_loss=1.4574\ntrF1=0.967 valF1=0.696 RGA=0.696 (0.8s)', '\\n', 'SymToken Epoch 3:\ntr_loss=0.0489 val_loss=1.5619 trF1=0.985 valF1=0.696 RGA=0.696 (0.9s)', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n12/working/experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Baseline Epoch 1: tr_loss=0.4121 val_loss=1.3240 trF1=0.815 valF1=0.688\nRGA=0.688 (0.6s)', '\\n', 'Baseline Epoch 2: tr_loss=0.0751 val_loss=1.8213\ntrF1=0.977 valF1=0.692 RGA=0.692 (0.3s)', '\\n', 'Baseline Epoch 3:\ntr_loss=0.0405 val_loss=1.9424 trF1=0.988 valF1=0.698 RGA=0.698 (0.3s)', '\\n',\n'SymToken Epoch 1: tr_loss=0.6287 val_loss=0.6744 trF1=0.646 valF1=0.682\nRGA=0.682 (0.3s)', '\\n', 'SymToken Epoch 2: tr_loss=0.1318 val_loss=1.4574\ntrF1=0.967 valF1=0.696 RGA=0.696 (0.4s)', '\\n', 'SymToken Epoch 3:\ntr_loss=0.0489 val_loss=1.5619 trF1=0.985 valF1=0.696 RGA=0.696 (0.4s)', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Baseline Epoch 1: tr_loss=0.4121 val_loss=1.3240 trF1=0.815 valF1=0.688\nRGA=0.688 (0.6s)', '\\n', 'Baseline Epoch 2: tr_loss=0.0751 val_loss=1.8213\ntrF1=0.977 valF1=0.692 RGA=0.692 (0.3s)', '\\n', 'Baseline Epoch 3:\ntr_loss=0.0405 val_loss=1.9424 trF1=0.988 valF1=0.698 RGA=0.698 (0.3s)', '\\n',\n'SymToken Epoch 1: tr_loss=0.6287 val_loss=0.6744 trF1=0.646 valF1=0.682\nRGA=0.682 (0.3s)', '\\n', 'SymToken Epoch 2: tr_loss=0.1318 val_loss=1.4574\ntrF1=0.967 valF1=0.696 RGA=0.696 (0.3s)', '\\n', 'SymToken Epoch 3:\ntr_loss=0.0489 val_loss=1.5619 trF1=0.985 valF1=0.696 RGA=0.696 (0.3s)', '\\n',\n'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["The code executed successfully without any errors or bugs. The training process\nfor different d_model values was completed, and the results, including losses\nand F1 scores for both training and validation datasets, were logged. The\nexperiment data was saved successfully. No issues were observed in the output.", "The execution of the training script was successful without any bugs. The\nbaseline and symbolic augmented models were trained and evaluated on the\nSPR_BENCH dataset. The symbolic augmented model achieved a test F1 score of\n0.6959 and accuracy of 0.6960, which is close to the benchmark accuracy of\n70.0%. However, neither model surpassed the state-of-the-art (SOTA) performance.\nFurther experimentation and optimization are required to improve the model's\nperformance.", "The script failed to execute because it could not find the required dataset file\n'train.csv' in the specified directory '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n12/SPR_BENCH/'. This indicates that the dataset is either missing or the path is\nincorrect. To fix this issue, ensure that the SPR_BENCH directory contains the\nrequired 'train.csv', 'dev.csv', and 'test.csv' files at the specified path.\nAlternatively, update the path to point to the correct location of the dataset.", "", "The execution failed due to a ValueError in the SPRDataset class initialization.\nSpecifically, the random.sample function was asked to sample 8000 items from a\npopulation of only 2000 items in the 'train' dataset. To fix this, ensure that\nthe 'take' parameter does not exceed the size of the dataset. Modify the code to\nuse 'take=min(take, len(hf_split))' or adjust the 'take' value to a smaller\nnumber that aligns with the dataset size.", "The error occurs because the `BaselineClassifier` model's `forward` method does\nnot accept a `training` keyword argument. However, the `run_epoch` function\ncalls the model's `forward` method with this argument. To fix this issue, update\nthe `forward` method of the `BaselineClassifier` class to accept a `training`\nkeyword argument and handle it appropriately, even if it's not used in the\ncomputation. Alternatively, modify the `run_epoch` function to avoid passing the\n`training` argument when using the `BaselineClassifier`.", "The execution failed due to a device mismatch error during the training of the\n\"AuxCL\" model. Specifically, the error 'Expected all tensors to be on the same\ndevice, but found at least two devices, cuda:0 and cpu!' occurred when creating\nmasked language model (MLM) inputs. This issue arises because the random tensor\n`rand` is on the CPU, while the other tensors are on the GPU (cuda:0). To fix\nthis, ensure that the `rand` tensor is moved to the same device as the input\ntensors (`input_ids`). Modify the `create_mlm_inputs` function to include `rand\n= torch.rand(input_ids.shape, device=input_ids.device)`.", "", "", "", "", "The training script executed successfully without any errors or bugs. The\nBaseline Transformer and Symbolic Token Transformer models were trained and\nevaluated on the SPR_BENCH dataset. The results were saved for further analysis.\nBoth models achieved validation accuracies close to the state-of-the-art\nperformance (70.0%), with the Baseline model slightly outperforming the SymToken\nmodel in terms of accuracy. Overall, the experiment appears to have been\nconducted as intended and provides valuable insights for further analysis.", ""], "exc_type": [null, null, "FileNotFoundError", null, "ValueError", "TypeError", "RuntimeError", null, null, null, null, null, null], "exc_info": [null, null, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-12/SPR_BENCH/train.csv'"]}, null, {"args": ["Sample larger than population or is negative"]}, {"args": ["BaselineClassifier.forward() got an unexpected keyword argument 'training'"]}, {"args": ["Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"]}, null, null, null, null, null, null], "exc_stack": [null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 57, "<module>", "spr = load_spr_bench(SPR_PATH)"], ["runfile.py", 50, "load_spr_bench", "dd[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 42, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 120, "<module>", "train_ds = SPRDataset(dset[\"train\"], take=8000)  # small subset for speed"], ["runfile.py", 103, "__init__", "idxs = random.sample(idxs, take)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/random.py", 456, "sample", "raise ValueError(\"Sample larger than population or is negative\")"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 240, "<module>", "train_model(\"Baseline\", BaselineClassifier(), epochs=4)"], ["runfile.py", 224, "train_model", "tr_loss, tr_f1, _, _, _ = run_epoch(model, train_loader, opt)"], ["runfile.py", 195, "run_epoch", "logits_cls, (logits_mlm, mlm_labels) = model("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"]], [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 226, "<module>", "train_model(\"AuxCL\", AuxCL(), epochs=2)"], ["runfile.py", 204, "train_model", "tr_loss, tr_f1, _ = run_epoch(model, train_dl, opt)"], ["runfile.py", 175, "run_epoch", "ids_masked, mlm_labs = create_mlm_inputs(ids)"], ["runfile.py", 154, "create_mlm_inputs", "(rand < mask_prob) & (input_ids != vocab[PAD]) & (input_ids != vocab[CLS])"]], null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training, higher is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.9875, "best_value": 0.9875}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.991, "best_value": 0.991}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.9895, "best_value": 0.9895}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation, higher is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training, lower is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.0472, "best_value": 0.0472}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.0435, "best_value": 0.0435}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.0423, "best_value": 0.0423}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.0514, "best_value": 0.0514}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation, lower is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.6493, "best_value": 0.6493}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 1.4873, "best_value": 1.4873}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 1.9211, "best_value": 1.9211}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 2.1986, "best_value": 2.1986}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro F1 score measures the harmonic mean of precision and recall across all classes, treating all classes equally.", "data": [{"dataset_name": "baseline", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "symbolic", "final_value": 0.6959, "best_value": 0.6959}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Accuracy measures the proportion of correctly predicted instances out of the total instances.", "data": [{"dataset_name": "baseline", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "symbolic", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss indicates the error or discrepancy between the predicted and actual values. Lower values are better.", "data": [{"dataset_name": "baseline", "final_value": 2.0515, "best_value": 2.0515}, {"dataset_name": "symbolic", "final_value": 1.9074, "best_value": 1.9074}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score, which measures the balance between precision and recall across all classes.", "data": [{"dataset_name": "Baseline - train", "final_value": 0.986, "best_value": 0.986}, {"dataset_name": "Baseline - validation", "final_value": 0.684, "best_value": 0.684}, {"dataset_name": "SymbolicAug - train", "final_value": 0.977, "best_value": 0.977}, {"dataset_name": "SymbolicAug - validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The loss value, which measures the error during training or validation. Lower values indicate better performance.", "data": [{"dataset_name": "Baseline - train", "final_value": 0.0554, "best_value": 0.0554}, {"dataset_name": "Baseline - validation", "final_value": 1.7193, "best_value": 1.7193}, {"dataset_name": "SymbolicAug - train", "final_value": 0.0726, "best_value": 0.0726}, {"dataset_name": "SymbolicAug - validation", "final_value": 1.7434, "best_value": 1.7434}]}, {"metric_name": "RGA accuracy", "lower_is_better": false, "description": "The accuracy of the RGA model during validation.", "data": [{"dataset_name": "Baseline - validation", "final_value": 0.684, "best_value": 0.684}, {"dataset_name": "SymbolicAug - validation", "final_value": 0.696, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train macro F1", "lower_is_better": false, "description": "Macro F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0493, "best_value": 0.0493}]}, {"metric_name": "validation macro F1", "lower_is_better": false, "description": "Macro F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.694, "best_value": 0.694}]}, {"metric_name": "validation RGA", "lower_is_better": false, "description": "RGA score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.694, "best_value": 0.694}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.6866, "best_value": 1.6866}]}]}, {"metric_names": [{"metric_name": "macro_f1", "lower_is_better": false, "description": "Macro-averaged F1 score for classification tasks.", "data": [{"dataset_name": "Baseline - train", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "Baseline - validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken - train", "final_value": 0.985, "best_value": 0.985}, {"dataset_name": "SymToken - validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "Baseline - validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken - validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss value indicating model performance, lower is better.", "data": [{"dataset_name": "Baseline - training", "final_value": 0.0405, "best_value": 0.0405}, {"dataset_name": "Baseline - validation", "final_value": 1.9424, "best_value": 1.9424}, {"dataset_name": "SymToken - training", "final_value": 0.0489, "best_value": 0.0489}, {"dataset_name": "SymToken - validation", "final_value": 1.5619, "best_value": 1.5619}]}]}, {"metric_names": [{"metric_name": "macro_f1", "lower_is_better": false, "description": "Macro-averaged F1 score, a measure of a model's accuracy in classifying datasets, averaged across all classes.", "data": [{"dataset_name": "Baseline train", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "Baseline validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken train", "final_value": 0.985, "best_value": 0.985}, {"dataset_name": "SymToken validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Classification accuracy, a measure of the percentage of correct predictions.", "data": [{"dataset_name": "Baseline validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss function value, a measure of how well the model is performing. Lower values indicate better performance.", "data": [{"dataset_name": "Baseline training", "final_value": 0.0405, "best_value": 0.0405}, {"dataset_name": "Baseline validation", "final_value": 1.9424, "best_value": 1.9424}, {"dataset_name": "SymToken training", "final_value": 0.0489, "best_value": 0.0489}, {"dataset_name": "SymToken validation", "final_value": 1.5619, "best_value": 1.5619}]}]}, {"metric_names": [{"metric_name": "macro_f1", "lower_is_better": false, "description": "Macro-averaged F1 score", "data": [{"dataset_name": "Baseline train", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "Baseline validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken train", "final_value": 0.985, "best_value": 0.985}, {"dataset_name": "SymToken validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "Baseline validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss during training and validation", "data": [{"dataset_name": "Baseline training", "final_value": 0.0405, "best_value": 0.0405}, {"dataset_name": "Baseline validation", "final_value": 1.9424, "best_value": 1.9424}, {"dataset_name": "SymToken training", "final_value": 0.0489, "best_value": 0.0489}, {"dataset_name": "SymToken validation", "final_value": 1.5619, "best_value": 1.5619}]}]}, {"metric_names": [{"metric_name": "macro_f1", "lower_is_better": false, "description": "Macro-averaged F1 score, which is the harmonic mean of precision and recall, averaged over classes.", "data": [{"dataset_name": "Baseline train", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "Baseline validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken train", "final_value": 0.985, "best_value": 0.985}, {"dataset_name": "SymToken validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Proportion of correctly classified instances over the total instances.", "data": [{"dataset_name": "Baseline validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "SymToken validation", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The objective function value that the model is trying to minimize during training.", "data": [{"dataset_name": "Baseline train", "final_value": 0.0405, "best_value": 0.0405}, {"dataset_name": "Baseline validation", "final_value": 1.9424, "best_value": 1.9424}, {"dataset_name": "SymToken train", "final_value": 0.0489, "best_value": 0.0489}, {"dataset_name": "SymToken validation", "final_value": 1.5619, "best_value": 1.5619}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_baseline_vs_symbolic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_baseline_vs_symbolic_f1_curves.png", "../../logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_final_val_f1_comparison.png", "../../logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_symbolic_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_final_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_confusion_matrix_Baseline.png", "../../logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_confusion_matrix_SymbolicAug.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_final_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_Baseline_conf_matrix.png"], ["../../logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_final_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_confusion_Baseline.png"], ["../../logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_final_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_confusion_Baseline.png"], ["../../logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_final_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_confusion_Baseline.png"], ["../../logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_final_val_f1_bar.png", "../../logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_confusion_Baseline.png"], ["../../logs/0-run/experiment_results/seed_aggregation_7c47c1e6c59247dea01ead701c0ab7fa/SPR_BENCH_f1_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_7c47c1e6c59247dea01ead701c0ab7fa/SPR_BENCH_final_val_f1_mean_se_bar.png"]], "plot_paths": [["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_baseline_vs_symbolic_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_baseline_vs_symbolic_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_final_val_f1_comparison.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_symbolic_confusion_matrix.png"], [], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_final_val_f1_bar.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_confusion_matrix_Baseline.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_confusion_matrix_SymbolicAug.png"], [], [], [], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_final_val_f1_bar.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_Baseline_conf_matrix.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_final_val_f1_bar.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_confusion_Baseline.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_final_val_f1_bar.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_confusion_Baseline.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_final_val_f1_bar.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_confusion_Baseline.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_final_val_f1_bar.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_confusion_Baseline.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_7c47c1e6c59247dea01ead701c0ab7fa/SPR_BENCH_f1_mean_se.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_7c47c1e6c59247dea01ead701c0ab7fa/SPR_BENCH_final_val_f1_mean_se_bar.png"]], "plot_analyses": [[{"analysis": "The first plot shows training and validation loss trends across different model configurations (d_model sizes of 64, 128, 256, and 384). For smaller d_model sizes (64 and 128), training loss decreases steadily, indicating good learning, but validation loss diverges after the first few epochs, suggesting overfitting. For larger d_model sizes (256 and 384), both training and validation losses remain lower, with reduced divergence, indicating better generalization. However, validation loss for larger models stabilizes at higher values, implying room for optimization.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot illustrates the Macro-F1 score trends for training and validation across epochs for various d_model sizes. Smaller models (64 and 128) achieve high training Macro-F1 scores quickly but exhibit poor validation performance, confirming overfitting. Larger models (256 and 384) maintain consistent training and validation Macro-F1 scores, indicating better generalization. Notably, larger models achieve near-perfect training scores by epoch 2, with validation scores stabilizing around 0.7, suggesting a trade-off between capacity and overfitting.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_f1_curves.png"}, {"analysis": "The third plot presents the final validation Macro-F1 scores for different d_model sizes. All configurations achieve similar final validation scores (~0.7), indicating that increasing model size beyond a certain point does not yield significant improvements in this metric. This suggests that factors other than model size, such as regularization or data augmentation, may be critical for further performance gains.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_final_val_f1_bar.png"}], [{"analysis": "The plot shows the training and validation loss for the baseline and symbolic models over three epochs. The symbolic model exhibits lower training and validation loss compared to the baseline, indicating better optimization and generalization. However, the validation loss for both models does not decrease significantly after the first epoch, suggesting potential overfitting or insufficient learning beyond this point.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_baseline_vs_symbolic_loss_curves.png"}, {"analysis": "This plot compares the macro-F1 scores for training and validation across the baseline and symbolic models. The symbolic model achieves slightly lower macro-F1 scores on the validation set compared to the baseline, despite having higher training scores. This indicates that the symbolic model may not generalize as effectively as expected, potentially due to overfitting or insufficient capacity to handle the validation set complexities.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_baseline_vs_symbolic_f1_curves.png"}, {"analysis": "The bar chart compares the final validation macro-F1 scores of the baseline and symbolic models. Both models achieve nearly identical performance, with the baseline marginally outperforming the symbolic model. This suggests that the symbolic reasoning module did not provide a significant advantage in this experimental setup.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_final_val_f1_comparison.png"}, {"analysis": "The confusion matrix for the symbolic model reveals its performance on the test set. The diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. The matrix shows a reasonable balance between classes but highlights areas where the model struggles with specific labels. This suggests room for improvement in the symbolic reasoning module's ability to generalize across diverse rule complexities.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_dc4c3d4e7b3541c595d28d6f23e58a3a_proc_3469037/SPR_BENCH_symbolic_confusion_matrix.png"}], [], [{"analysis": "The plot compares training and validation loss for the baseline and SymbolicAug models. The baseline model exhibits consistently lower loss on both training and validation data compared to the SymbolicAug model. The SymbolicAug model shows a higher validation loss, suggesting potential overfitting or difficulty in generalizing. Both models stabilize after epoch 2, but the baseline model achieves better convergence.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates macro-F1 scores across epochs for both training and validation sets. The baseline model achieves higher macro-F1 scores on both training and validation, indicating better classification performance. SymbolicAug's validation macro-F1 scores remain slightly lower, reflecting its struggle to generalize as effectively as the baseline model. Both models stabilize after epoch 2.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart compares the final validation macro-F1 scores of the baseline and SymbolicAug models. Both models achieve similar macro-F1 scores, with only a negligible difference favoring the baseline model. This indicates that while the baseline model performs slightly better, the SymbolicAug model is competitive.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_final_val_f1_bar.png"}, {"analysis": "The confusion matrix for the baseline model shows moderately balanced predictions, with a slight preference for one class. The baseline model achieves good performance but has some misclassifications, as seen in the non-diagonal entries. The true positives for both classes are relatively high, indicating reasonable classification accuracy.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_confusion_matrix_Baseline.png"}, {"analysis": "The confusion matrix for the SymbolicAug model reveals a similar pattern to the baseline model, with slightly improved true positive counts in both classes. However, the overall misclassifications are comparable to the baseline model. This suggests that the SymbolicAug model is competitive but does not provide a significant improvement over the baseline.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9006213182bd43c5aa61521077aefd88_proc_3469039/SPR_BENCH_confusion_matrix_SymbolicAug.png"}], [], [], [], [{"analysis": "The loss curves indicate that the baseline model achieves a significantly lower cross-entropy loss compared to the multitask model on both training and validation sets. The multitask model starts with a high training loss, which decreases but does not reach the same low levels as the baseline. This suggests that the multitask model struggles to converge effectively, potentially due to increased complexity or insufficient tuning of the symbolic reasoning components.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_loss_curves.png"}, {"analysis": "The macro-F1 curves show that the baseline model consistently outperforms the multitask model in both training and validation sets. The baseline model achieves near-perfect training performance and maintains superior validation performance, while the multitask model's validation macro-F1 score declines slightly after an initial increase. This indicates that the multitask model may be overfitting or not generalizing well to the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_f1_curves.png"}, {"analysis": "The final validation macro-F1 comparison reveals that the baseline model outperforms the multitask model, achieving a higher macro-F1 score. This reinforces the observation that the multitask model is less effective in generalizing to the validation set, possibly due to the challenges of integrating symbolic reasoning modules.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_final_val_f1_bar.png"}, {"analysis": "The confusion matrix for the baseline model on the validation set shows a reasonable balance between true positives and true negatives, with slightly higher false positives and false negatives. This suggests that while the baseline model performs well overall, there is still room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_990fc896d73e47e4b5e2c0737bed2a10_proc_3469039/SPR_BENCH_Baseline_conf_matrix.png"}], [{"analysis": "This plot shows the training and validation loss trends for the baseline and symbolic token (SymToken) models. The baseline model achieves consistently lower training and validation loss compared to the SymToken model. While the baseline model converges quickly, the SymToken model struggles with higher loss values, indicating challenges in learning or optimization for the symbolic reasoning modules. The gap between training and validation losses for the SymToken model suggests potential overfitting or a need for better hyperparameter tuning.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares the macro-F1 scores of the baseline and SymToken models during training and validation. The baseline model achieves significantly higher macro-F1 scores, indicating better performance on the classification task. The SymToken model shows minimal improvement over epochs, suggesting difficulty in leveraging symbolic reasoning for enhanced performance. The lack of significant improvement in validation scores for the SymToken model points to possible generalization issues.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart compares the final validation macro-F1 scores of the baseline and SymToken models. Both models achieve similar final scores, with the baseline slightly outperforming the SymToken model. This result suggests that the symbolic reasoning modules in the SymToken model do not provide a clear advantage in terms of overall performance on the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_final_val_f1_bar.png"}, {"analysis": "This confusion matrix visualizes the performance of the baseline model on the test set. The diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. The matrix shows a reasonable balance in the classification performance across classes, with relatively low misclassification rates. This supports the baseline model's robustness and effectiveness in handling the SPR_BENCH dataset.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b7eac33c2d341b4867b07e86ce4f259_proc_3469037/SPR_BENCH_confusion_Baseline.png"}], [{"analysis": "The plot shows the training and validation cross-entropy loss for the baseline and SymToken models. The baseline model exhibits a steady decrease in training loss, with validation loss remaining consistently higher, indicating potential overfitting. Conversely, the SymToken model's training loss decreases more slowly, and its validation loss aligns more closely with the training loss, suggesting better generalization. However, the overall loss values for SymToken are higher than the baseline, indicating it struggles to optimize as effectively.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares the macro-F1 scores for training and validation phases of the baseline and SymToken models. The baseline model achieves a near-perfect training macro-F1 score, but its validation macro-F1 score is capped at 0.7, suggesting overfitting. The SymToken model shows a more gradual improvement in both training and validation macro-F1 scores, with training and validation performances closely aligned, indicating better generalization but limited improvement over the baseline.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart compares the final validation macro-F1 scores of the baseline and SymToken models. Both models achieve nearly identical final validation macro-F1 scores of approximately 0.7, suggesting that the symbolic reasoning augmentation in the SymToken model does not provide a significant performance advantage over the baseline in terms of validation macro-F1.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_final_val_f1_bar.png"}, {"analysis": "The confusion matrix for the baseline model shows a relatively balanced distribution of true positives and true negatives, but it also indicates some misclassifications. The detailed breakdown of true and false predictions could provide insights into specific areas where the model struggles, such as particular classes or patterns in the data.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/SPR_BENCH_confusion_Baseline.png"}], [{"analysis": "This plot compares the training and validation loss for the baseline and symbolic token (SymToken) models over three epochs. The baseline model shows significantly lower training and validation loss compared to the SymToken model. The validation loss for the SymToken model is consistently higher than its training loss, indicating potential overfitting or insufficient training. The baseline model, on the other hand, achieves convergence more effectively, with both training and validation loss decreasing steadily and closely aligned.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation Macro-F1 scores for the baseline and SymToken models. The baseline model achieves near-perfect Macro-F1 for training and validation, indicating high accuracy and generalization on the SPR_BENCH dataset. In contrast, the SymToken model shows much lower scores, with minimal improvement over epochs, suggesting that the symbolic reasoning module is not effectively contributing to performance on this task.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart shows the final validation Macro-F1 scores for both models. Both the baseline and SymToken models achieve nearly identical scores (~0.70), demonstrating that the symbolic reasoning augmentation did not lead to a noticeable improvement in overall performance. This result aligns with the earlier observation that the SymToken model struggles to match the baseline's performance during training.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_final_val_f1_bar.png"}, {"analysis": "This confusion matrix visualizes the classification performance of the baseline model. The diagonal dominance indicates that the baseline model performs well in predicting the correct classes. However, there are still some misclassifications, as shown by the off-diagonal elements. This suggests that while the baseline is effective, there is room for improvement in handling certain cases.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/SPR_BENCH_confusion_Baseline.png"}], [{"analysis": "The plot shows the training and validation loss over three epochs for both the baseline and the symbolic reasoning-augmented (SymToken) models. The baseline model achieves a lower validation loss compared to the SymToken model. The SymToken model's validation loss remains higher than its training loss, indicating a potential overfitting issue or a lack of generalization. The baseline model demonstrates better convergence and generalization, as evidenced by the narrower gap between its training and validation loss.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot compares the training and validation macro-F1 scores for both models. The baseline model achieves near-perfect training and validation macro-F1 scores after the first epoch, suggesting that it fits the dataset well. In contrast, the SymToken model shows slower improvement and ends with a validation macro-F1 score of approximately 0.7, indicating its difficulty in generalizing to unseen data.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart compares the final validation macro-F1 scores of the two models. Both models achieve similar validation macro-F1 scores, with the baseline model slightly outperforming the SymToken model. This further supports the observation that the symbolic reasoning module did not significantly enhance performance on the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_final_val_f1_bar.png"}, {"analysis": "The confusion matrix for the baseline model demonstrates its performance in classifying the SPR_BENCH dataset. The diagonal elements indicate correct classifications, while the off-diagonal elements represent misclassifications. The model appears to perform well, with a high number of correct predictions and relatively few misclassifications. This aligns with its high validation macro-F1 score.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/SPR_BENCH_confusion_Baseline.png"}], []], "vlm_feedback_summary": ["The plots provide insights into the effects of model size (d_model) on training\nand validation performance. Larger models generalize better and avoid\noverfitting but do not significantly improve final validation performance. The\nresults suggest focusing on techniques to improve generalization and reduce\noverfitting for smaller models.", "The symbolic model demonstrates promising optimization behavior with lower loss\nvalues compared to the baseline. However, its generalization performance, as\nmeasured by macro-F1 scores, is slightly inferior to the baseline. The confusion\nmatrix indicates balanced but imperfect classification, suggesting areas for\nfurther enhancement in symbolic reasoning capabilities.", "[]", "The plots indicate that the baseline model outperforms the SymbolicAug model in\nterms of loss and macro-F1 scores, particularly on validation data. Both models\nachieve similar final validation macro-F1 scores, suggesting comparable overall\nperformance. Confusion matrices show that both models perform reasonably well,\nwith no significant advantage for SymbolicAug.", "[]", "[]", "[]", "The baseline transformer model outperforms the multitask model with symbolic\nreasoning capabilities across all metrics and analyses. The multitask model\nstruggles with convergence and generalization, indicating potential issues with\nthe integration or training of symbolic reasoning components. Further refinement\nof the multitask architecture and training approach is needed to realize the\nhypothesized benefits of symbolic reasoning.", "The analysis reveals that the baseline model outperforms the SymToken model in\nterms of training/validation loss, macro-F1 scores, and overall classification\nperformance. The symbolic reasoning modules in the SymToken model do not provide\na clear performance advantage, and further optimization or architectural\nadjustments may be needed to harness their potential.", "The experimental results indicate that while the SymToken model demonstrates\nbetter generalization characteristics compared to the baseline, it does not\nsignificantly outperform the baseline in terms of validation macro-F1 score. The\nbaseline model shows overfitting tendencies, achieving high training scores but\nplateauing in validation performance. The symbolic reasoning augmentation in the\nSymToken model does not yield a substantial improvement in overall performance,\nsuggesting the need for further refinement or alternative approaches to enhance\nits effectiveness.", "The baseline model demonstrates superior performance compared to the symbolic\ntoken (SymToken) model across all metrics and analyses. While the baseline\nachieves near-perfect training and validation Macro-F1 scores, the SymToken\nmodel struggles to generalize and shows no improvement over the baseline in\nfinal validation performance. The symbolic reasoning augmentation does not\nappear to contribute effectively to the task, and further investigation is\nneeded to refine the approach or identify alternative methods for integrating\nsymbolic reasoning.", "The experimental results suggest that the baseline model outperforms the\nsymbolic reasoning-augmented model (SymToken) in terms of validation loss and\nmacro-F1 score. While the SymToken model introduces a novel approach, its\nperformance does not surpass the baseline, possibly due to overfitting or\ninsufficient integration of symbolic reasoning capabilities. Further refinement\nof the symbolic reasoning modules is recommended to improve generalization and\noverall performance.", "[]"], "exec_time": [22.244227647781372, 10.499045610427856, 1.1857819557189941, 7.423694610595703, 1.2655270099639893, 2.6964426040649414, 3.1776225566864014, 4.508517742156982, 8.033915042877197, 8.705890893936157, 5.120001792907715, 5.2419092655181885, null], "exec_time_feedback": ["", "", "", "", "Implementation works but runs too quickly (0.12 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["[]"], [], ["['SPR_BENCH']"], [], [], [], ["\"\""], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndct = experiment_data.get(\"d_model_tuning\", {})\ndataset_name = \"SPR_BENCH\"\n\n# gather epochs and metrics\nd_models = sorted(dct.keys(), key=int)\nepochs = {}\nloss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}\nfor dm in d_models:\n    run = dct[dm][dataset_name]\n    loss_tr[dm] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[dm] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[dm] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[dm] = f1_val[dm][-1] if f1_val[dm] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], loss_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], loss_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], f1_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], f1_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val F1\ntry:\n    plt.figure()\n    xs = np.arange(len(d_models))\n    vals = [final_val_f1[dm] for dm in d_models]\n    plt.bar(xs, vals, tick_label=d_models)\n    plt.xlabel(\"d_model\")\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 per d_model\\nDataset: SPR_BENCH\")\n    save_name = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print best d_model\nif final_val_f1:\n    best_dm = max(final_val_f1, key=final_val_f1.get)\n    print(\n        f\"Best d_model by final validation Macro-F1: {best_dm} \"\n        f\"({final_val_f1[best_dm]:.4f})\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset = \"SPR_BENCH\"\nbase_data = experiment_data.get(dataset, {}).get(\"baseline\", {})\nsymb_data = experiment_data.get(dataset, {}).get(\"symbolic\", {})\n\n\ndef _collect(d, key):\n    return [x[key] for x in d] if d else []\n\n\nepochs_b = _collect(base_data.get(\"losses\", {}).get(\"train\", []), \"epoch\")\nepochs_s = _collect(symb_data.get(\"losses\", {}).get(\"train\", []), \"epoch\")\n\nloss_tr_b = _collect(base_data.get(\"losses\", {}).get(\"train\", []), \"loss\")\nloss_val_b = _collect(base_data.get(\"losses\", {}).get(\"val\", []), \"loss\")\nloss_tr_s = _collect(symb_data.get(\"losses\", {}).get(\"train\", []), \"loss\")\nloss_val_s = _collect(symb_data.get(\"losses\", {}).get(\"val\", []), \"loss\")\n\nf1_tr_b = _collect(base_data.get(\"metrics\", {}).get(\"train\", []), \"macro_f1\")\nf1_val_b = _collect(base_data.get(\"metrics\", {}).get(\"val\", []), \"macro_f1\")\nf1_tr_s = _collect(symb_data.get(\"metrics\", {}).get(\"train\", []), \"macro_f1\")\nf1_val_s = _collect(symb_data.get(\"metrics\", {}).get(\"val\", []), \"macro_f1\")\n\n# --------------------------------- 1) Loss curves\ntry:\n    plt.figure()\n    if epochs_b:\n        plt.plot(epochs_b, loss_tr_b, \"--\", label=\"Baseline-Train\")\n        plt.plot(epochs_b, loss_val_b, label=\"Baseline-Val\")\n    if epochs_s:\n        plt.plot(epochs_s, loss_tr_s, \"--\", label=\"Symbolic-Train\")\n        plt.plot(epochs_s, loss_val_s, label=\"Symbolic-Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"SPR_BENCH_baseline_vs_symbolic_loss_curves.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting loss curves: {e}\")\n    plt.close()\n\n# --------------------------------- 2) Macro-F1 curves\ntry:\n    plt.figure()\n    if epochs_b:\n        plt.plot(epochs_b, f1_tr_b, \"--\", label=\"Baseline-Train\")\n        plt.plot(epochs_b, f1_val_b, label=\"Baseline-Val\")\n    if epochs_s:\n        plt.plot(epochs_s, f1_tr_s, \"--\", label=\"Symbolic-Train\")\n        plt.plot(epochs_s, f1_val_s, label=\"Symbolic-Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"SPR_BENCH_baseline_vs_symbolic_f1_curves.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting F1 curves: {e}\")\n    plt.close()\n\n# --------------------------------- 3) Final validation Macro-F1 bar chart\ntry:\n    plt.figure()\n    labels, vals = [], []\n    if f1_val_b:\n        labels.append(\"Baseline\")\n        vals.append(f1_val_b[-1])\n    if f1_val_s:\n        labels.append(\"Symbolic\")\n        vals.append(f1_val_s[-1])\n    xs = np.arange(len(labels))\n    plt.bar(xs, vals, tick_label=labels)\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 Comparison\\nDataset: SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_f1_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error plotting bar chart: {e}\")\n    plt.close()\n\n# --------------------------------- 4) Confusion matrix for Symbolic model\ntry:\n    preds = symb_data.get(\"predictions\", [])\n    gts = symb_data.get(\"ground_truth\", [])\n    if preds and gts:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.title(\"Confusion Matrix \u2013 Symbolic Model\\nDataset: SPR_BENCH\")\n        plt.savefig(\n            os.path.join(working_dir, \"SPR_BENCH_symbolic_confusion_matrix.png\")\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting confusion matrix: {e}\")\n    plt.close()\n\n# --------------------------------- Print final validation scores\nif f1_val_b:\n    print(f\"Baseline final val Macro-F1: {f1_val_b[-1]:.4f}\")\nif f1_val_s:\n    print(f\"Symbolic final val Macro-F1: {f1_val_s[-1]:.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, f1_score\n\n# ------------------- setup & data loading -------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset_name = \"SPR_BENCH\"\nruns = experiment_data.get(dataset_name, {})\nmodels = [\"Baseline\", \"SymbolicAug\"]\nepochs = {}\nloss_tr = {}\nloss_val = {}\nf1_tr = {}\nf1_val = {}\nfinal_val_f1 = {}\n\nfor m in models:\n    run = runs.get(m, {})\n    epochs[m] = [x[\"epoch\"] for x in run.get(\"metrics\", {}).get(\"train\", [])]\n    loss_tr[m] = [x[\"loss\"] for x in run.get(\"losses\", {}).get(\"train\", [])]\n    loss_val[m] = [x[\"loss\"] for x in run.get(\"losses\", {}).get(\"val\", [])]\n    f1_tr[m] = [x[\"macro_f1\"] for x in run.get(\"metrics\", {}).get(\"train\", [])]\n    f1_val[m] = [x[\"macro_f1\"] for x in run.get(\"metrics\", {}).get(\"val\", [])]\n    final_val_f1[m] = f1_val[m][-1] if f1_val[m] else 0.0\n\n# ------------------- 1) Loss curves -------------------\ntry:\n    plt.figure()\n    for m in models:\n        if epochs[m]:\n            plt.plot(epochs[m], loss_tr[m], \"--\", label=f\"{m}-train\")\n            plt.plot(epochs[m], loss_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ------------------- 2) Macro-F1 curves -------------------\ntry:\n    plt.figure()\n    for m in models:\n        if epochs[m]:\n            plt.plot(epochs[m], f1_tr[m], \"--\", label=f\"{m}-train\")\n            plt.plot(epochs[m], f1_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# ------------------- 3) Bar chart of final val F1 -------------------\ntry:\n    plt.figure()\n    xs = np.arange(len(models))\n    vals = [final_val_f1[m] for m in models]\n    plt.bar(xs, vals, tick_label=models)\n    plt.xlabel(\"Model\")\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 per Model\\nDataset: SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------- 4-5) Confusion matrices (one per model) -------------------\nfor m in models:\n    try:\n        preds = runs[m].get(\"predictions\", [])\n        gts = runs[m].get(\"ground_truth\", [])\n        if preds and gts:\n            cm = confusion_matrix(gts, preds, labels=sorted(set(gts)))\n            plt.figure()\n            plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar()\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"Confusion Matrix\\nModel: {m} | Dataset: SPR_BENCH\")\n            # add counts\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n            plt.tight_layout()\n            fname = f\"SPR_BENCH_confusion_matrix_{m}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {m}: {e}\")\n        plt.close()\n\n# ------------------- print best model -------------------\nif final_val_f1:\n    best_model = max(final_val_f1, key=final_val_f1.get)\n    print(\n        f\"Best model by final validation Macro-F1: {best_model} \"\n        f\"({final_val_f1[best_model]:.4f})\"\n    )\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor dataset_name, runs in experiment_data.items():\n    models = list(runs.keys())\n    epochs, loss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}, {}\n    for m in models:\n        loss_tr[m] = [x[\"loss\"] for x in runs[m][\"losses\"][\"train\"]]\n        loss_val[m] = [x[\"loss\"] for x in runs[m][\"losses\"][\"val\"]]\n        f1_tr[m] = [x[\"macro_F1\"] for x in runs[m][\"metrics\"][\"train\"]]\n        f1_val[m] = [x[\"macro_F1\"] for x in runs[m][\"metrics\"][\"val\"]]\n        epochs[m] = [x[\"epoch\"] for x in runs[m][\"metrics\"][\"train\"]]\n        final_val_f1[m] = f1_val[m][-1] if f1_val[m] else 0.0\n\n    # 1) Loss curves ------------------------------------------------\n    try:\n        plt.figure()\n        for m in models:\n            plt.plot(epochs[m], loss_tr[m], \"--\", label=f\"{m}-train\")\n            plt.plot(epochs[m], loss_val[m], label=f\"{m}-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CE Loss\")\n        plt.title(f\"Loss Curves\\nDataset: {dataset_name} (Train vs Val)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset_name}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dataset_name}: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curves -------------------------------------------\n    try:\n        plt.figure()\n        for m in models:\n            plt.plot(epochs[m], f1_tr[m], \"--\", label=f\"{m}-train\")\n            plt.plot(epochs[m], f1_val[m], label=f\"{m}-val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"Macro-F1 Curves\\nDataset: {dataset_name} (Train vs Val)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset_name}_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curves for {dataset_name}: {e}\")\n        plt.close()\n\n    # 3) Final validation F1 bar chart ------------------------------\n    try:\n        plt.figure()\n        xs = np.arange(len(models))\n        vals = [final_val_f1[m] for m in models]\n        plt.bar(xs, vals, tick_label=models)\n        plt.xlabel(\"Model\")\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(f\"Final Validation Macro-F1 Comparison\\nDataset: {dataset_name}\")\n        plt.savefig(os.path.join(working_dir, f\"{dataset_name}_final_val_f1_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar chart for {dataset_name}: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix for best model ---------------------------\n    try:\n        best_model = max(final_val_f1, key=final_val_f1.get)\n        preds = runs[best_model].get(\"predictions\", [])\n        gts = runs[best_model].get(\"ground_truth\", [])\n        if preds and gts:\n            cm = confusion_matrix(gts, preds, labels=np.unique(gts))\n            disp = ConfusionMatrixDisplay(cm)\n            plt.figure()\n            disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n            plt.title(\n                f\"Confusion Matrix (Val)\\nDataset: {dataset_name} \u2013 Best: {best_model}\"\n            )\n            plt.savefig(\n                os.path.join(\n                    working_dir, f\"{dataset_name}_{best_model}_conf_matrix.png\"\n                )\n            )\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dataset_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset_name = \"SPR_BENCH\"\nruns = experiment_data.get(dataset_name, {})\n\n# gather per-model statistics\nepochs, loss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}, {}\nfor model_name, run in runs.items():\n    loss_tr[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[model_name] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[model_name] = f1_val[model_name][-1] if f1_val[model_name] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], loss_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], loss_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], f1_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], f1_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val Macro-F1\ntry:\n    plt.figure()\n    models = list(runs.keys())\n    vals = [final_val_f1[m] for m in models]\n    plt.bar(np.arange(len(models)), vals, tick_label=models)\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.xlabel(\"Model\")\n    plt.title(\"Final Validation Macro-F1 per Model\\nDataset: SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best model\nbest_model = max(final_val_f1, key=final_val_f1.get) if final_val_f1 else None\nif best_model:\n    try:\n        preds = runs[best_model][\"predictions\"]\n        gts = runs[best_model][\"ground_truth\"]\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f\"Confusion Matrix\\nDataset: SPR_BENCH; Model: {best_model}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_model}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n# ------------------------------------------------------------------\nif final_val_f1:\n    print(f\"Best model: {best_model} ({final_val_f1[best_model]:.4f} Macro-F1)\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset_name = \"SPR_BENCH\"\nruns = experiment_data.get(dataset_name, {})\n\n# gather per-model statistics\nepochs, loss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}, {}\nfor model_name, run in runs.items():\n    loss_tr[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[model_name] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[model_name] = f1_val[model_name][-1] if f1_val[model_name] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], loss_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], loss_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], f1_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], f1_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val Macro-F1\ntry:\n    plt.figure()\n    models = list(runs.keys())\n    vals = [final_val_f1[m] for m in models]\n    plt.bar(np.arange(len(models)), vals, tick_label=models)\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.xlabel(\"Model\")\n    plt.title(\"Final Validation Macro-F1 per Model\\nDataset: SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best model\nbest_model = max(final_val_f1, key=final_val_f1.get) if final_val_f1 else None\nif best_model:\n    try:\n        preds = runs[best_model][\"predictions\"]\n        gts = runs[best_model][\"ground_truth\"]\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f\"Confusion Matrix\\nDataset: SPR_BENCH; Model: {best_model}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_model}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n# ------------------------------------------------------------------\nif final_val_f1:\n    print(f\"Best model: {best_model} ({final_val_f1[best_model]:.4f} Macro-F1)\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset_name = \"SPR_BENCH\"\nruns = experiment_data.get(dataset_name, {})\n\n# gather per-model statistics\nepochs, loss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}, {}\nfor model_name, run in runs.items():\n    loss_tr[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[model_name] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[model_name] = f1_val[model_name][-1] if f1_val[model_name] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], loss_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], loss_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], f1_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], f1_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val Macro-F1\ntry:\n    plt.figure()\n    models = list(runs.keys())\n    vals = [final_val_f1[m] for m in models]\n    plt.bar(np.arange(len(models)), vals, tick_label=models)\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.xlabel(\"Model\")\n    plt.title(\"Final Validation Macro-F1 per Model\\nDataset: SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best model\nbest_model = max(final_val_f1, key=final_val_f1.get) if final_val_f1 else None\nif best_model:\n    try:\n        preds = runs[best_model][\"predictions\"]\n        gts = runs[best_model][\"ground_truth\"]\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f\"Confusion Matrix\\nDataset: SPR_BENCH; Model: {best_model}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_model}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n# ------------------------------------------------------------------\nif final_val_f1:\n    print(f\"Best model: {best_model} ({final_val_f1[best_model]:.4f} Macro-F1)\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndataset_name = \"SPR_BENCH\"\nruns = experiment_data.get(dataset_name, {})\n\n# gather per-model statistics\nepochs, loss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}, {}\nfor model_name, run in runs.items():\n    loss_tr[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[model_name] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[model_name] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[model_name] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[model_name] = f1_val[model_name][-1] if f1_val[model_name] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], loss_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], loss_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for m in runs:\n        plt.plot(epochs[m], f1_tr[m], \"--\", label=f\"{m}-train\")\n        plt.plot(epochs[m], f1_val[m], label=f\"{m}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val Macro-F1\ntry:\n    plt.figure()\n    models = list(runs.keys())\n    vals = [final_val_f1[m] for m in models]\n    plt.bar(np.arange(len(models)), vals, tick_label=models)\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.xlabel(\"Model\")\n    plt.title(\"Final Validation Macro-F1 per Model\\nDataset: SPR_BENCH\")\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# 4) Confusion matrix for best model\nbest_model = max(final_val_f1, key=final_val_f1.get) if final_val_f1 else None\nif best_model:\n    try:\n        preds = runs[best_model][\"predictions\"]\n        gts = runs[best_model][\"ground_truth\"]\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f\"Confusion Matrix\\nDataset: SPR_BENCH; Model: {best_model}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_model}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n# ------------------------------------------------------------------\nif final_val_f1:\n    print(f\"Best model: {best_model} ({final_val_f1[best_model]:.4f} Macro-F1)\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------\n# 1. Load every experiment_data.npy that the orchestrator produced\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3e2bb0f24be44712890754709df90091_proc_3469038/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_10adfd972499486080b7d3a2eb3036c8_proc_3469040/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_404b3ac67c35477aac307b3a698caf87_proc_3469037/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p), allow_pickle=True\n        ).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"Error loading experiment data from {p}: {e}\")\n\n# --------------------------------------------------------------\n# 2. Aggregate runs -> {dataset: {model: [run_dict, ...]}}\naggregated = {}\nfor exp in all_experiment_data:\n    for dset, runs in exp.items():\n        aggregated.setdefault(dset, {})\n        for model, run_dict in runs.items():\n            aggregated[dset].setdefault(model, []).append(run_dict)\n\n\n# --------------------------------------------------------------\ndef stack_metric(run_list, split, metric_key):\n    \"\"\"\n    Build (epochs, stacked_values) where\n    - epochs is a list of common epochs across runs ordered by appearance\n    - stacked_values is a dict epoch -> list(values)\n    \"\"\"\n    common_epochs = None\n    epoch_to_vals = {}\n    # First, find epochs common to all runs\n    for r in run_list:\n        e_list = [e[\"epoch\"] for e in r[\"metrics\"][split]]\n        common_epochs = (\n            set(e_list) if common_epochs is None else common_epochs.intersection(e_list)\n        )\n    common_epochs = sorted(list(common_epochs))\n    # Collect metric values\n    for e in common_epochs:\n        epoch_to_vals[e] = []\n        for r in run_list:\n            # find metric for epoch e\n            for m in r[\"metrics\"][split]:\n                if m[\"epoch\"] == e:\n                    epoch_to_vals[e].append(m[metric_key])\n                    break\n    return common_epochs, epoch_to_vals\n\n\n# helper to convert dict epoch->list(values) to sorted arrays\ndef mean_se_arrays(epoch_to_vals):\n    epochs = sorted(epoch_to_vals.keys())\n    means = np.array([np.mean(epoch_to_vals[e]) for e in epochs])\n    ses = np.array(\n        [\n            (\n                np.std(epoch_to_vals[e], ddof=1) / np.sqrt(len(epoch_to_vals[e]))\n                if len(epoch_to_vals[e]) > 1\n                else 0.0\n            )\n            for e in epochs\n        ]\n    )\n    return epochs, means, ses\n\n\n# --------------------------------------------------------------\nfor dataset, model_runs in aggregated.items():\n    # We will keep numerical summary for console print\n    summary = {}\n    # ------------- LOSS CURVES ----------------------\n    try:\n        plt.figure()\n        for model, runs in model_runs.items():\n            # Train\n            ep_train, d_train = stack_metric(runs, \"train\", \"loss\")\n            ep_val, d_val = stack_metric(runs, \"val\", \"loss\")\n            if not ep_train or not ep_val:\n                continue\n            ep_t, m_t, se_t = mean_se_arrays(d_train)\n            ep_v, m_v, se_v = mean_se_arrays(d_val)\n            plt.plot(ep_t, m_t, \"--\", label=f\"{model}-train \u03bc\")\n            plt.fill_between(ep_t, m_t - se_t, m_t + se_t, alpha=0.2)\n            plt.plot(ep_v, m_v, label=f\"{model}-val \u03bc\")\n            plt.fill_between(ep_v, m_v - se_v, m_v + se_v, alpha=0.2)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"Mean \u00b1 SE Training vs Validation Loss\\nDataset: {dataset}\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_mean_se.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss curves for {dataset}: {e}\")\n        plt.close()\n\n    # ------------- F1 CURVES ------------------------\n    try:\n        plt.figure()\n        for model, runs in model_runs.items():\n            ep_train, d_train = stack_metric(runs, \"train\", \"macro_f1\")\n            ep_val, d_val = stack_metric(runs, \"val\", \"macro_f1\")\n            if not ep_train or not ep_val:\n                continue\n            ep_t, m_t, se_t = mean_se_arrays(d_train)\n            ep_v, m_v, se_v = mean_se_arrays(d_val)\n            plt.plot(ep_t, m_t, \"--\", label=f\"{model}-train \u03bc\")\n            plt.fill_between(ep_t, m_t - se_t, m_t + se_t, alpha=0.2)\n            plt.plot(ep_v, m_v, label=f\"{model}-val \u03bc\")\n            plt.fill_between(ep_v, m_v - se_v, m_v + se_v, alpha=0.2)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"Mean \u00b1 SE Training vs Validation Macro-F1\\nDataset: {dataset}\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset}_f1_mean_se.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated F1 curves for {dataset}: {e}\")\n        plt.close()\n\n    # ------------- FINAL VAL F1 BAR -----------------\n    try:\n        plt.figure()\n        models = []\n        means = []\n        ses = []\n        for model, runs in model_runs.items():\n            vals = []\n            for r in runs:\n                if r[\"metrics\"][\"val\"]:\n                    vals.append(r[\"metrics\"][\"val\"][-1][\"macro_f1\"])\n            if not vals:\n                continue\n            models.append(model)\n            means.append(np.mean(vals))\n            ses.append(\n                np.std(vals, ddof=1) / np.sqrt(len(vals)) if len(vals) > 1 else 0.0\n            )\n            summary[model] = (np.mean(vals), ses[-1])\n        x = np.arange(len(models))\n        plt.bar(x, means, yerr=ses, capsize=4)\n        plt.xticks(x, models, rotation=45, ha=\"right\")\n        plt.ylabel(\"Final Validation Macro-F1\")\n        plt.xlabel(\"Model\")\n        plt.title(f\"Final Val Macro-F1 (Mean \u00b1 SE)\\nDataset: {dataset}\")\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"{dataset}_final_val_f1_mean_se_bar.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated final F1 bar chart for {dataset}: {e}\")\n        plt.close()\n\n    # Console summary\n    if summary:\n        print(f\"=== {dataset} Final Validation Macro-F1 ===\")\n        for m, (mu, se) in summary.items():\n            print(f\"{m:20s}: {mu:.4f} \u00b1 {se:.4f}\")\n"], "plot_plan": ["We will load the saved experiment_data.npy, extract the metrics recorded for each d_model run, and assemble three concise visualizations.  First, we will draw one figure with the training- and validation-loss curves for every d_model, so users can see convergence behaviour.  Second, we will do the same for macro-F1, giving insight into generalisation.  Third, we will build a bar chart of the final\u2010epoch validation macro-F1 for each d_model, making hyper-parameter comparison easy.  Optionally, if predictions/labels exist, we compute the macro-F1 again to double-check and print the best d_model at the end (no extra plot, so we keep \u22645 figures).  Each figure is created inside its own try-except, titled clearly, saved to working_dir with descriptive names, and the figure is always closed.  No data are fabricated; everything comes directly from experiment_data.npy.  After plotting, we print the d_model that achieved the highest validation macro-F1.", "We will load the stored experiment_data.npy, focus on the SPR_BENCH dataset and extract training/validation losses and macro-F1 for both the baseline and symbolic models across epochs. Four plots will be generated: (1) combined loss curves, (2) combined macro-F1 curves, (3) a bar chart comparing the final validation macro-F1 of the two models, and (4) a confusion matrix for the symbolic model on the validation set. Each plot is wrapped in an independent try-except block, saved with descriptive names inside the working directory, and the figures are always closed. Only data that exist in the numpy file are visualised, and no more than the required number of figures are created. Finally, the script prints out the final validation macro-F1 values so they are visible in logs. Everything is done with plain matplotlib, adhering strictly to the given plotting guidelines.", null, "The script loads `experiment_data.npy`, identifies the two runs (Baseline and SymbolicAug) stored under the SPR_BENCH dataset, and extracts epoch-wise training/validation loss and macro-F1. Three global comparison plots are produced: (1) loss curves, (2) macro-F1 curves and (3) a bar chart of the final validation macro-F1, all with clear titles and labels. When predictions and ground-truth are present for a run, an additional confusion-matrix heat-map is generated for each model, but only one matrix per model is plotted to keep the total number of figures \u2264 5. Every plot is wrapped in its own try-except block, saved with descriptive names into `working_dir`, and closed afterward to free memory. The code follows the required imports and directory handling, never invents data, and respects the given stylistic constraints.", null, null, null, "The script will load experiment_data.npy from the working folder, loop over every dataset it contains (currently only \u201cSPR_BENCH\u201d), and for each dataset extract the list of models (Baseline, MultiTask). For every model we gather epoch-wise training/validation losses and macro-F1 scores, plus final\u2010epoch validation F1 so we can rank models. We then create up to four plots per dataset\u2014(1) loss curves, (2) macro-F1 curves, (3) a bar chart comparing final validation F1 across models, and (4) a confusion-matrix plot for the single best model\u2014each wrapped in its own try/except block and saved to working_dir with descriptive filenames. All figures are closed after saving, and titles include clear subtitles that show which split or comparison is being visualised. If the required data (e.g. predictions) are missing a plot is skipped gracefully. The code adheres to the base template, uses only matplotlib and numpy plus sklearn.metrics for the confusion matrix, and limits the total number of figures to four (\u22645 as required).", "We load the saved experiment_data.npy, extract per-model train/val losses and Macro-F1 across epochs, and identify the best model by final val Macro-F1. Four separate try-except blocks then generate (1) loss curves, (2) Macro-F1 curves, (3) a bar chart comparing final val Macro-F1 of Baseline vs SymToken, and (4) a confusion matrix for the best model\u2019s predictions vs ground truth. Every figure is titled, saved under a descriptive name in working_dir, and properly closed to conserve resources. We rely solely on data present in experiment_data.npy, plotting at most one figure per category to stay within the 5-figure cap. Finally, we print the best model and its score to stdout.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["d_model (Transformer hidden/embedding size)", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved numpy dictionary from the working directory,\niterate over every d_model run inside the \u201cd_model_tuning\u201d branch, and for each\nrun access the single dataset entry \u201cSPR_BENCH.\u201d   For every run it scans the\nstored metric/loss history, selects the best (highest) macro-F1 scores and the\nbest (lowest) cross-entropy losses, and prints them with fully qualified names\nsuch as \u201cbest training macro F1 score\u201d or \u201clowest validation loss.\u201d   All code\nsits at global scope so that it executes immediately when the file is run, with\nno reliance on an `if __name__ == \"__main__\":` guard.", "The script first locates the working directory and loads the saved NumPy\ndictionary containing the experimental results. It then iterates through each\nmodel run stored under the SPR _BENCH entry, extracts the final (i.e., last-\nepoch) train/validation metrics and losses, and, when available, the test-set\nmetrics. For clarity, each printed line specifies both the dataset/model name\nand an explicit metric label such as \u201cvalidation accuracy\u201d or \u201ctest macro F1\nscore.\u201d No plots are produced and the code runs immediately upon execution.", "", "We load the experiment dictionary from the working directory, iterate over every\ntop-level dataset (here \u201cSPR_BENCH\u201d) and then over every run stored inside that\ndataset (\u201cBaseline\u201d, \u201cSymbolicAug\u201d). For each run we fetch the last entry (i.e.\nthe final epoch) in the lists that hold training/validation metrics and losses,\nand print these values using explicit metric names such as \u201ctrain macro F1\nscore\u201d or \u201cvalidation loss\u201d. Metrics that are missing or None are skipped to\nkeep the output clean.", "", "", "", "The script will locate the NumPy file in the working directory, load it into a\nPython dict, and iterate over each dataset and model. For every model it will\nreport (1) the final train macro-F1 and train loss and (2) the best validation\nmacro-F1, its corresponding validation RGA, and validation loss (chosen from the\nepoch that maximises validation macro-F1). Each value is printed with a clear,\nexplicit label so the output is unambiguous.", "The solution loads the saved NumPy file, walks through the nested dictionary to\nreach the two experimental runs (Baseline and SymToken), and then prints the\nfinal-epoch values for every recorded metric and loss in a clear, labelled way.\nAll logic is in the global scope so the script runs immediately.", "The solution loads the saved NumPy file, walks through the nested dictionary to\nreach the two experimental runs (Baseline and SymToken), and then prints the\nfinal-epoch values for every recorded metric and loss in a clear, labelled way.\nAll logic is in the global scope so the script runs immediately.", "The solution loads the saved NumPy file, walks through the nested dictionary to\nreach the two experimental runs (Baseline and SymToken), and then prints the\nfinal-epoch values for every recorded metric and loss in a clear, labelled way.\nAll logic is in the global scope so the script runs immediately.", "The solution loads the saved NumPy file, walks through the nested dictionary to\nreach the two experimental runs (Baseline and SymToken), and then prints the\nfinal-epoch values for every recorded metric and loss in a clear, labelled way.\nAll logic is in the global scope so the script runs immediately.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Traverse every hyper-parameter run and print the requested stats\ndmodel_runs = experiment_data.get(\"d_model_tuning\", {})\nfor d_model_str, run_dict in dmodel_runs.items():\n    for dataset_name, ds in run_dict.items():  # here only \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name} (d_model={d_model_str})\")\n\n        # --- F1 scores -------------------------------------------------------\n        train_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"train\"]]\n        val_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"val\"]]\n\n        best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n        best_val_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n\n        # --- Losses ----------------------------------------------------------\n        train_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"train\"]]\n        val_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"val\"]]\n\n        best_train_loss = (\n            min(train_loss_history) if train_loss_history else float(\"nan\")\n        )\n        best_val_loss = min(val_loss_history) if val_loss_history else float(\"nan\")\n\n        # --- Print nicely ----------------------------------------------------\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n        print(f\"lowest training loss: {best_train_loss:.4f}\")\n        print(f\"lowest validation loss: {best_val_loss:.4f}\")\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate the experiment file and load it\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef print_final_metrics(exp_dict):\n    \"\"\"\n    exp_dict is the dictionary for one model run\n    (e.g., experiment_data['SPR_BENCH']['baseline'])\n    \"\"\"\n    # final values correspond to the last epoch stored\n    final_train_metrics = exp_dict[\"metrics\"][\"train\"][-1]\n    final_val_metrics = exp_dict[\"metrics\"][\"val\"][-1]\n    final_train_loss = exp_dict[\"losses\"][\"train\"][-1]\n    final_val_loss = exp_dict[\"losses\"][\"val\"][-1]\n\n    # Training metrics\n    print(f\"  train macro F1 score: {final_train_metrics['macro_f1']:.4f}\")\n    print(f\"  train accuracy: {final_train_metrics['acc']:.4f}\")\n    print(f\"  train loss: {final_train_loss['loss']:.4f}\")\n\n    # Validation metrics\n    print(f\"  validation macro F1 score: {final_val_metrics['macro_f1']:.4f}\")\n    print(f\"  validation accuracy: {final_val_metrics['acc']:.4f}\")\n    print(f\"  validation loss: {final_val_loss['loss']:.4f}\")\n\n    # Test metrics (may be absent, e.g. for baseline)\n    if \"test_metrics\" in exp_dict:\n        tm = exp_dict[\"test_metrics\"]\n        print(f\"  test macro F1 score: {tm['macro_f1']:.4f}\")\n        print(f\"  test accuracy: {tm['acc']:.4f}\")\n        print(f\"  test loss: {tm['loss']:.4f}\")\n\n\n# ------------------------------------------------------------------\n# iterate through each stored model/dataset and print results\nfor model_name, data in experiment_data.get(\"SPR_BENCH\", {}).items():\n    print(f\"\\nDataset: {model_name}\")\n    print_final_metrics(data)\n", "", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------\n# 1. Load experiment data\n# --------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------------------------\n# 2. Helper to safely fetch the final (or best) value from a list\n# --------------------------------------------------------------------\ndef final_value(lst, key):\n    \"\"\"Return the value associated with `key` from the last dict in `lst`.\"\"\"\n    if not lst:\n        return None\n    return lst[-1].get(key, None)\n\n\n# --------------------------------------------------------------------\n# 3. Traverse the dictionary and print metrics\n# --------------------------------------------------------------------\nfor dataset_name, runs in experiment_data.items():\n    print(f\"{dataset_name}:\")  # dataset header\n\n    for run_name, contents in runs.items():\n        print(f\"  {run_name}:\")  # model / run header\n\n        # Final macro-F1 scores\n        train_f1 = final_value(contents[\"metrics\"][\"train\"], \"macro_f1\")\n        val_f1 = final_value(contents[\"metrics\"][\"val\"], \"macro_f1\")\n\n        # Final RGA (validation accuracy proxy)\n        val_rga = final_value(contents[\"metrics\"][\"val\"], \"RGA\")\n\n        # Final losses\n        train_loss = final_value(contents[\"losses\"][\"train\"], \"loss\")\n        val_loss = final_value(contents[\"losses\"][\"val\"], \"loss\")\n\n        # Print, skipping None values\n        if train_f1 is not None:\n            print(f\"    train macro F1 score: {train_f1:.4f}\")\n        if val_f1 is not None:\n            print(f\"    validation macro F1 score: {val_f1:.4f}\")\n        if train_loss is not None:\n            print(f\"    train loss: {train_loss:.4f}\")\n        if val_loss is not None:\n            print(f\"    validation loss: {val_loss:.4f}\")\n        if val_rga is not None:\n            print(f\"    validation RGA accuracy: {val_rga:.4f}\")\n\n        print()  # blank line for readability\n", "", "", "", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate working directory and load the saved experiment results\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1. Iterate over datasets and models, extract and print key metrics\n# ------------------------------------------------------------------\nfor dataset_name, models in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    for model_name, content in models.items():\n        # ------------------ TRAIN ------------------\n        final_train_metrics = content[\"metrics\"][\"train\"][-1]\n        final_train_loss = content[\"losses\"][\"train\"][-1][\"loss\"]\n        train_macro_f1 = final_train_metrics[\"macro_F1\"]\n\n        # ------------------ VALIDATION (best by F1) ------------------\n        val_metrics = content[\"metrics\"][\"val\"]\n        val_losses = content[\"losses\"][\"val\"]\n\n        # Identify the epoch with the best (highest) validation macro-F1\n        best_val_entry = max(val_metrics, key=lambda x: x[\"macro_F1\"])\n        best_epoch = best_val_entry[\"epoch\"]\n\n        # Fetch the matching validation loss\n        best_val_loss = next(l for l in val_losses if l[\"epoch\"] == best_epoch)[\"loss\"]\n\n        val_macro_f1 = best_val_entry[\"macro_F1\"]\n        val_rga = best_val_entry[\"RGA\"]\n\n        # ------------------ PRINT ------------------\n        print(f\"  Model: {model_name}\")\n        print(f\"    train macro F1: {train_macro_f1:.4f}\")\n        print(f\"    train loss: {final_train_loss:.4f}\")\n        print(f\"    validation macro F1: {val_macro_f1:.4f}\")\n        print(f\"    validation RGA: {val_rga:.4f}\")\n        print(f\"    validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------- load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------- print metrics\n# The outermost key is the benchmark; inside are the individual experiment runs.\nbenchmark_name, benchmark_dict = next(iter(experiment_data.items()))\n\nfor run_name, run_dict in benchmark_dict.items():\n    print(run_name)  # dataset / run name first\n\n    # ----- losses\n    final_train_loss = run_dict[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = run_dict[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # ----- metrics\n    final_train_metrics = run_dict[\"metrics\"][\"train\"][-1]\n    final_val_metrics = run_dict[\"metrics\"][\"val\"][-1]\n\n    # training metrics\n    print(f\"train macro_f1: {final_train_metrics['macro_f1']:.4f}\")\n    # validation metrics\n    print(f\"validation macro_f1: {final_val_metrics['macro_f1']:.4f}\")\n    print(f\"validation accuracy: {final_val_metrics['RGA']:.4f}\")\n    # losses\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print()  # blank line between runs\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------- load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------- print metrics\n# The outermost key is the benchmark; inside are the individual experiment runs.\nbenchmark_name, benchmark_dict = next(iter(experiment_data.items()))\n\nfor run_name, run_dict in benchmark_dict.items():\n    print(run_name)  # dataset / run name first\n\n    # ----- losses\n    final_train_loss = run_dict[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = run_dict[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # ----- metrics\n    final_train_metrics = run_dict[\"metrics\"][\"train\"][-1]\n    final_val_metrics = run_dict[\"metrics\"][\"val\"][-1]\n\n    # training metrics\n    print(f\"train macro_f1: {final_train_metrics['macro_f1']:.4f}\")\n    # validation metrics\n    print(f\"validation macro_f1: {final_val_metrics['macro_f1']:.4f}\")\n    print(f\"validation accuracy: {final_val_metrics['RGA']:.4f}\")\n    # losses\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print()  # blank line between runs\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------- load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------- print metrics\n# The outermost key is the benchmark; inside are the individual experiment runs.\nbenchmark_name, benchmark_dict = next(iter(experiment_data.items()))\n\nfor run_name, run_dict in benchmark_dict.items():\n    print(run_name)  # dataset / run name first\n\n    # ----- losses\n    final_train_loss = run_dict[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = run_dict[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # ----- metrics\n    final_train_metrics = run_dict[\"metrics\"][\"train\"][-1]\n    final_val_metrics = run_dict[\"metrics\"][\"val\"][-1]\n\n    # training metrics\n    print(f\"train macro_f1: {final_train_metrics['macro_f1']:.4f}\")\n    # validation metrics\n    print(f\"validation macro_f1: {final_val_metrics['macro_f1']:.4f}\")\n    print(f\"validation accuracy: {final_val_metrics['RGA']:.4f}\")\n    # losses\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print()  # blank line between runs\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------- load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------- print metrics\n# The outermost key is the benchmark; inside are the individual experiment runs.\nbenchmark_name, benchmark_dict = next(iter(experiment_data.items()))\n\nfor run_name, run_dict in benchmark_dict.items():\n    print(run_name)  # dataset / run name first\n\n    # ----- losses\n    final_train_loss = run_dict[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = run_dict[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # ----- metrics\n    final_train_metrics = run_dict[\"metrics\"][\"train\"][-1]\n    final_val_metrics = run_dict[\"metrics\"][\"val\"][-1]\n\n    # training metrics\n    print(f\"train macro_f1: {final_train_metrics['macro_f1']:.4f}\")\n    # validation metrics\n    print(f\"validation macro_f1: {final_val_metrics['macro_f1']:.4f}\")\n    print(f\"validation accuracy: {final_val_metrics['RGA']:.4f}\")\n    # losses\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print()  # blank line between runs\n", ""], "parse_term_out": ["['Dataset: SPR_BENCH (d_model=64)', '\\n', 'best training macro F1 score:\n0.9875', '\\n', 'best validation macro F1 score: 0.6920', '\\n', 'lowest training\nloss: 0.0472', '\\n', 'lowest validation loss: 0.6493', '\\n', '\\n', 'Dataset:\nSPR_BENCH (d_model=128)', '\\n', 'best training macro F1 score: 0.9910', '\\n',\n'best validation macro F1 score: 0.6900', '\\n', 'lowest training loss: 0.0435',\n'\\n', 'lowest validation loss: 1.4873', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=256)', '\\n', 'best training macro F1 score: 0.9880', '\\n', 'best\nvalidation macro F1 score: 0.6940', '\\n', 'lowest training loss: 0.0423', '\\n',\n'lowest validation loss: 1.9211', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=384)', '\\n', 'best training macro F1 score: 0.9895', '\\n', 'best\nvalidation macro F1 score: 0.6960', '\\n', 'lowest training loss: 0.0514', '\\n',\n'lowest validation loss: 2.1986', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nDataset: baseline', '\\n', '  train macro F1 score: 0.9865', '\\n', '  train\naccuracy: 0.9865', '\\n', '  train loss: 0.0528', '\\n', '  validation macro F1\nscore: 0.6920', '\\n', '  validation accuracy: 0.6920', '\\n', '  validation loss:\n2.0515', '\\n', '\\nDataset: symbolic', '\\n', '  train macro F1 score: 0.9845',\n'\\n', '  train accuracy: 0.9845', '\\n', '  train loss: 0.0569', '\\n', '\nvalidation macro F1 score: 0.6899', '\\n', '  validation accuracy: 0.6900', '\\n',\n'  validation loss: 1.8913', '\\n', '  test macro F1 score: 0.6959', '\\n', '\ntest accuracy: 0.6960', '\\n', '  test loss: 1.9074', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "", "['SPR_BENCH:', '\\n', '  Baseline:', '\\n', '    train macro F1 score: 0.9860',\n'\\n', '    validation macro F1 score: 0.6840', '\\n', '    train loss: 0.0554',\n'\\n', '    validation loss: 1.7193', '\\n', '    validation RGA accuracy:\n0.6840', '\\n', '\\n', '  SymbolicAug:', '\\n', '    train macro F1 score: 0.9770',\n'\\n', '    validation macro F1 score: 0.6960', '\\n', '    train loss: 0.0726',\n'\\n', '    validation loss: 1.7434', '\\n', '    validation RGA accuracy:\n0.6960', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "", "", "", "['\\nDataset: SPR_BENCH', '\\n', '  Model: Baseline', '\\n', '    train macro F1:\n0.9885', '\\n', '    train loss: 0.0493', '\\n', '    validation macro F1:\n0.6940', '\\n', '    validation RGA: 0.6940', '\\n', '    validation loss:\n1.6866', '\\n', '  Model: MultiTask', '\\n', '    train macro F1: 0.7602', '\\n', '\ntrain loss: 2.9468', '\\n', '    validation macro F1: 0.6313', '\\n', '\nvalidation RGA: 0.6320', '\\n', '    validation loss: 3.2669', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Baseline', '\\n', 'train macro_f1: 0.9885', '\\n', 'validation macro_f1:\n0.6980', '\\n', 'validation accuracy: 0.6980', '\\n', 'training loss: 0.0405',\n'\\n', 'validation loss: 1.9424', '\\n', '\\n', 'SymToken', '\\n', 'train macro_f1:\n0.9850', '\\n', 'validation macro_f1: 0.6960', '\\n', 'validation accuracy:\n0.6960', '\\n', 'training loss: 0.0489', '\\n', 'validation loss: 1.5619', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Baseline', '\\n', 'train macro_f1: 0.9885', '\\n', 'validation macro_f1:\n0.6980', '\\n', 'validation accuracy: 0.6980', '\\n', 'training loss: 0.0405',\n'\\n', 'validation loss: 1.9424', '\\n', '\\n', 'SymToken', '\\n', 'train macro_f1:\n0.9850', '\\n', 'validation macro_f1: 0.6960', '\\n', 'validation accuracy:\n0.6960', '\\n', 'training loss: 0.0489', '\\n', 'validation loss: 1.5619', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Baseline', '\\n', 'train macro_f1: 0.9885', '\\n', 'validation macro_f1:\n0.6980', '\\n', 'validation accuracy: 0.6980', '\\n', 'training loss: 0.0405',\n'\\n', 'validation loss: 1.9424', '\\n', '\\n', 'SymToken', '\\n', 'train macro_f1:\n0.9850', '\\n', 'validation macro_f1: 0.6960', '\\n', 'validation accuracy:\n0.6960', '\\n', 'training loss: 0.0489', '\\n', 'validation loss: 1.5619', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Baseline', '\\n', 'train macro_f1: 0.9885', '\\n', 'validation macro_f1:\n0.6980', '\\n', 'validation accuracy: 0.6980', '\\n', 'training loss: 0.0405',\n'\\n', 'validation loss: 1.9424', '\\n', '\\n', 'SymToken', '\\n', 'train macro_f1:\n0.9850', '\\n', 'validation macro_f1: 0.6960', '\\n', 'validation accuracy:\n0.6960', '\\n', 'training loss: 0.0489', '\\n', 'validation loss: 1.5619', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
