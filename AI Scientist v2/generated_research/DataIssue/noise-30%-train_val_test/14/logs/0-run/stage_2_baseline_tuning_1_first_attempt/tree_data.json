{"edges": [[0, 7], [0, 1], [0, 8], [0, 5], [0, 6], [0, 2], [0, 4], [0, 3], [4, 9], [4, 10], [4, 11], [4, 12]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.14285714285714285, 0.5], [0.2857142857142857, 0.5], [0.42857142857142855, 0.5], [0.5714285714285714, 0.5], [0.7142857142857143, 0.5], [0.8571428571428571, 0.5], [1.0, 0.5], [0.21428571428571427, 1.0], [0.35714285714285715, 1.0], [0.5, 1.0], [0.6428571428571429, 1.0]], "plan": ["We start by loading the SPR_BENCH splits, build a simple whitespace-based\nvocabulary from the training set, and map every sequence to integer IDs with\npadding and a special CLS token.  A compact Transformer encoder (2 layers, 4\nheads, 128-dimensional embeddings) is used for sequence classification; the\nrepresentation of the CLS token is passed through a linear layer to predict\nlabels.  We train for a few epochs with cross-entropy, evaluate after each epoch\non the dev split, and report Macro-F1 together with loss curves.  All tensors\nand the model are moved to GPU when available, metrics/losses are tracked in an\nexperiment_data dictionary, and everything is saved under ./working for later\nanalysis.  The implementation is fully self-contained, executable as-is, and\nfollows all required coding conventions for device handling, metric tracking,\nand data saving.", "Hyperparam tuning name: EPOCHS. We keep all existing logic but wrap the training\nloop inside a function so we can easily re-initialise and train a fresh model\nfor several epoch budgets (e.g. 5, 10, 15 and 20).   For every epoch setting we\ncollect per-epoch losses/F1 on train and validation splits, plus the final dev\npredictions/labels, and store them in a nested experiment_data dictionary under\nthe key \u201cepochs_tuning\u201d.   After the sweep finishes we persist the complete\ndictionary to working/experiment_data.npy so it can be plotted later.   Only the\nepoch count changes; optimiser, architecture, batch-size, etc. remain identical\nto the baseline.", "Hyperparam tuning name: learning_rate. We sweep four candidate learning rates\n(1e-3, 5e-4, 1e-4, 5e-5).   For every rate we re-initialise the model, train for\nfive epochs with all other settings unchanged, log losses/F1 on train & dev\nsplits, and keep the predictions from the last epoch.   All results are\ncollected inside experiment_data under the key 'learning_rate', best-dev\nmacro-F1 is tracked and the best learning rate is reported.   Finally, the full\nexperiment_data structure is saved to working/experiment_data.npy.", "Hyperparam tuning name: BATCH_SIZE. Below is a concise extension of the baseline\nthat performs mini-batch hyperparameter tuning.  It trains exactly the same\nmodel/optimizer for three candidate batch sizes (32, 64, 128), records losses/F1\nper epoch, keeps the final dev predictions, and stores everything in the\nrequired experiment_data.npy file.", "Hyperparam tuning name: d_model (Transformer hidden/embedding size). We load the\nSPR benchmark, build a whitespace vocabulary, and create PyTorch\ndatasets/dataloaders exactly as in the baseline.   A grid search is then\nperformed over d_model \u2208 {64, 128, 256, 384}.   For every d_model we pick a\ncompatible nhead, build a fresh SimpleTransformer, train for five epochs,\nevaluate after each epoch, and log losses/F1.   All results are stored in the\nnested experiment_data structure under the key 'd_model_tuning' and finally\nsaved to working/experiment_data.npy.", "Hyperparam tuning name: nlayers (number_of_transformer_encoder_layers). We sweep\nthe number of encoder layers over {1, 2, 3, 4, 6}.   For each nlayers value the\nscript (1) re-initialises a SimpleTransformer with that depth, (2) trains for\nthe same number of epochs, (3) records per-epoch train / validation loss and\nmacro-F1 plus final predictions, and (4) stores all results in the required\nexperiment_data structure under the key \u201cnlayers_tuning \u2192 SPR_BENCH \u2192\nnlayers_X\u201d.   After finishing all runs the full experiment_data dictionary is\nsaved to working/experiment_data.npy so it can be plotted later.", "Hyperparam tuning name: weight_decay. We load the SPR benchmark, build the same\nwhitespace vocabulary and DataLoaders, then sweep AdamW\u2019s weight-decay over the\nvalues [0, 1e-5, 1e-4, 1e-3, 1e-2].   For every setting we re-instantiate the\nSimpleTransformer, train for five epochs, evaluate on the dev split, and store\nper-epoch macro-F1/loss together with final predictions and gold labels.   All\nresults are collected in a nested experiment_data dictionary under the key\n\"weight_decay\" with one sub-entry per coefficient and finally saved to\nworking/experiment_data.npy for later plotting or analysis.", "Hyperparam tuning name: dropout_probability. We introduce a loop over dropout\nrates [0.0, 0.1, 0.2, 0.3].   For every value we (1) build a fresh\nSimpleTransformer whose TransformerEncoderLayer receives that dropout\nprobability, (2) train five epochs, (3) log train / validation macro-F1 and\nlosses, and (4) keep the final validation predictions and ground truth.   All\nresults are stored in the nested experiment_data structure under the key\n'dropout_probability'\u2192'SPR_BENCH'\u2192<rate>.   Finally, the whole dictionary is\nwritten to working/experiment_data.npy.", "Hyperparam tuning name: nhead. We loop over the candidate values nhead \u2208 {2, 4,\n8}.   For every value we rebuild a fresh SimpleTransformer that uses that number\nof attention heads, train it for five epochs, evaluate on the dev split and\nrecord losses, macro-F1 scores, predictions and ground-truth labels.   All\nresults are collected in the nested dictionary\nexperiment_data['nhead_tuning']['SPR_BENCH'][str(nhead)] and finally saved to\nworking/experiment_data.npy.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, math, re, time, json, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------------------\n# mandatory working dir & device declarations\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------------\n# small util to load benchmark (copy of provided helper)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\n# ---------------------------------------------------------------------------------\n# 1) Load dataset\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------------\n# 2) Build vocabulary (whitespace tokenisation)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n# label mapping\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ---------------------------------------------------------------------------------\n# 3) Dataset wrapper\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    tok_ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(tok_ids)\n    if len(tok_ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(tok_ids)\n        tok_ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return tok_ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds = SPRDataset(spr[\"train\"])\ndev_ds = SPRDataset(spr[\"dev\"])\n\n# ---------------------------------------------------------------------------------\n# 4) DataLoaders\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    batch_tensors = {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n    return batch_tensors\n\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ---------------------------------------------------------------------------------\n# 5) Model definition\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        cls_rep = x[:, 0, :]\n        logits = self.classifier(cls_rep)\n        return logits\n\n\nmodel = SimpleTransformer(vocab_size, num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n\n# ---------------------------------------------------------------------------------\n# 6) Training & evaluation helpers\ndef run_epoch(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ---------------------------------------------------------------------------------\n# 7) Experiment loop\nEPOCHS = 5\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    train_loss, train_f1, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_f1, val_pred, val_gt = run_epoch(dev_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n        {\"epoch\": epoch, \"macro_f1\": train_f1}\n    )\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch, \"macro_f1\": val_f1}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        {\"epoch\": epoch, \"loss\": train_loss}\n    )\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        {\"epoch\": epoch, \"loss\": val_loss}\n    )\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n        f\"train_F1={train_f1:.4f}, val_F1={val_f1:.4f}  (time {time.time()-t0:.1f}s)\"\n    )\n\n# store last predictions / gts\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = val_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_gt\n\n# ---------------------------------------------------------------------------------\n# 8) Save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, random, math, re, json, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------\n# basic setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------------------------------------------------------------\n# small util to load benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dd[split] = _load(f\"{split}.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------\n# vocabulary\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(t):\n    if t not in vocab:\n        vocab[t] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ---------------------------------------------------------------------\n# dataset & dataloader helpers\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\nBATCH_SIZE = 64\ntrain_loader_base = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader_base = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ---------------------------------------------------------------------\n# model definition\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn):\n        x = self.embed(ids) + self.pos[:, : ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attn.bool())\n        cls = x[:, 0]\n        return self.classifier(cls)\n\n\n# ---------------------------------------------------------------------\n# training helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\n# hyper-parameter tuning over different epoch budgets\nepoch_options = [5, 10, 15, 20]\nexperiment_data = {\"epochs_tuning\": {}}\n\nfor EPOCHS in epoch_options:\n    print(f\"\\n===== Training for {EPOCHS} epochs =====\")\n    # reproducibility\n    torch.manual_seed(42)\n    random.seed(42)\n    np.random.seed(42)\n    # fresh model & optimiser\n    model = SimpleTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    # copy loaders (they are small wrappers, reuse OK)\n    train_loader, dev_loader = train_loader_base, dev_loader_base\n\n    run_record = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for ep in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader)\n        run_record[\"metrics\"][\"train\"].append({\"epoch\": ep, \"macro_f1\": tr_f1})\n        run_record[\"metrics\"][\"val\"].append({\"epoch\": ep, \"macro_f1\": val_f1})\n        run_record[\"losses\"][\"train\"].append({\"epoch\": ep, \"loss\": tr_loss})\n        run_record[\"losses\"][\"val\"].append({\"epoch\": ep, \"loss\": val_loss})\n        print(\n            f\"Ep {ep}/{EPOCHS}  tr_loss:{tr_loss:.4f} val_loss:{val_loss:.4f}  tr_F1:{tr_f1:.4f} val_F1:{val_f1:.4f}  ({time.time()-t0:.1f}s)\"\n        )\n    run_record[\"predictions\"] = val_pred\n    run_record[\"ground_truth\"] = val_gt\n    # store under experiment_data\n    experiment_data[\"epochs_tuning\"][f\"E{EPOCHS}\"] = {\"SPR_BENCH\": run_record}\n\n# ---------------------------------------------------------------------\n# save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy to\", working_dir)\n", "import os, pathlib, math, re, time, json, random, numpy as np, torch, gc\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------------------\n# mandatory working dir & device declarations\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------------\n# helper to load benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dd[split] = _load(f\"{split}.csv\")\n    return dd\n\n\n# ---------------------------------------------------------------------------------\n# 1) Load dataset\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------------\n# 2) Build vocabulary (whitespace tokenisation)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n# label mapping\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ---------------------------------------------------------------------------------\n# 3) Dataset wrapper\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    tok_ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(tok_ids)\n    if len(tok_ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(tok_ids)\n        tok_ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return tok_ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds = SPRDataset(spr[\"train\"])\ndev_ds = SPRDataset(spr[\"dev\"])\n\n# ---------------------------------------------------------------------------------\n# 4) DataLoaders\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ---------------------------------------------------------------------------------\n# 5) Model definition\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        cls_rep = x[:, 0, :]\n        return self.classifier(cls_rep)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\n# ---------------------------------------------------------------------------------\n# 6) Train / eval helpers\n@torch.no_grad()\ndef evaluate(loader, model):\n    model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\ndef train_one_epoch(loader, model, optimizer):\n    model.train()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).detach().cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1\n\n\n# ---------------------------------------------------------------------------------\n# 7) Hyperparameter tuning: learning rate sweep\nLR_SWEEP = [1e-3, 5e-4, 1e-4, 5e-5]\nEPOCHS = 5\nexperiment_data = {\n    \"learning_rate\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"lr_used\": [],\n            \"best_lr\": None,\n            \"best_val_f1\": -1.0,\n        }\n    }\n}\n\nfor lr in LR_SWEEP:\n    print(f\"\\n==== Training with learning rate {lr} ====\")\n    model = SimpleTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1 = train_one_epoch(train_loader, model, optimizer)\n        val_loss, val_f1, val_pred, val_gt = evaluate(dev_loader, model)\n\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(\n            {\"epoch\": epoch, \"lr\": lr, \"macro_f1\": tr_f1}\n        )\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(\n            {\"epoch\": epoch, \"lr\": lr, \"macro_f1\": val_f1}\n        )\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"lr\": lr, \"loss\": tr_loss}\n        )\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"lr\": lr, \"loss\": val_loss}\n        )\n\n        print(\n            f\"LR {lr} | Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} \"\n            f\"(time {time.time()-t0:.1f}s)\"\n        )\n\n    # store last epoch predictions / gts for this LR\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"predictions\"].append(\n        {\"lr\": lr, \"values\": val_pred}\n    )\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\n        \"ground_truth\"\n    ] = val_gt  # same across LRs, overwrite OK\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"lr_used\"].append(lr)\n\n    # update best lr\n    if val_f1 > experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_val_f1\"]:\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_val_f1\"] = val_f1\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_lr\"] = lr\n\n    # memory cleanup\n    del model, optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\nprint(\n    \"\\nBest LR:\",\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_lr\"],\n    \"with dev macro-F1:\",\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"best_val_f1\"],\n)\n\n# ---------------------------------------------------------------------------------\n# 8) Save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, random, math, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------------------\n# mandatory working dir & device declarations\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------------\n# util to load benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\n# ---------------------------------------------------------------------------------\n# 1) Load dataset\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------------\n# 2) Build vocabulary\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n# label mapping\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ---------------------------------------------------------------------------------\n# 3) Dataset wrapper\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    tok_ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(tok_ids)\n    if len(tok_ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(tok_ids)\n        tok_ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return tok_ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\n# ---------------------------------------------------------------------------------\n# 5) Model definition\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# ---------------------------------------------------------------------------------\n# helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, gts, preds = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        total_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------------------\n# 7) Hyper-parameter tuning loop (batch size)\nEPOCHS = 5\nBATCH_SIZES = [32, 64, 128]  # candidate batch sizes\n\nexperiment_data = {\"batch_size_tuning\": {\"SPR_BENCH\": {}}}\n\nfor bs in BATCH_SIZES:\n    print(f\"\\n====== Training with batch size {bs} ======\")\n    train_loader = DataLoader(\n        train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn\n    )\n    dev_loader = DataLoader(dev_ds, batch_size=bs, shuffle=False, collate_fn=collate_fn)\n\n    model = SimpleTransformer(vocab_size, num_labels).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    # prepare storage\n    experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"][bs] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        dv_loss, dv_f1, dv_pred, dv_gt = run_epoch(model, dev_loader)\n\n        exp_entry = experiment_data[\"batch_size_tuning\"][\"SPR_BENCH\"][bs]\n        exp_entry[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_entry[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": dv_f1})\n        exp_entry[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_entry[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": dv_loss})\n\n        print(\n            f\"Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={dv_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={dv_f1:.4f}  \"\n            f\"(time {time.time()-t0:.1f}s)\"\n        )\n\n    # store final preds/gts\n    exp_entry[\"predictions\"], exp_entry[\"ground_truth\"] = dv_pred, dv_gt\n\n# ---------------------------------------------------------------------------------\n# 8) Save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, random, math, json, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------------\n# 0) Reproducibility & device\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------------\n# 1) Working dir & experiment data dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"d_model_tuning\": {}}  # each key will be a d_model value as string\n\n\n# -------------------------------------------------------------------------------\n# 2) Load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------------\n# 3) Build vocab (whitespace)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# -------------------------------------------------------------------------------\n# 4) Dataset & dataloaders\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n\n# -------------------------------------------------------------------------------\n# 5) Transformer model\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# -------------------------------------------------------------------------------\n# 6) Train / eval helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------------------------------------------------------------------\n# 7) Hyperparameter grid search over d_model\nDMODEL_GRID = [64, 128, 256, 384]\n\n\ndef pick_head(d):\n    if d % 8 == 0:\n        return 8\n    if d % 4 == 0:\n        return 4\n    return 1\n\n\nEPOCHS = 5\n\nfor d_model in DMODEL_GRID:\n    run_key = str(d_model)\n    experiment_data[\"d_model_tuning\"][run_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    print(f\"\\n=== Training with d_model={d_model} ===\")\n    model = SimpleTransformer(\n        vocab_size,\n        num_labels,\n        d_model=d_model,\n        nhead=pick_head(d_model),\n        nlayers=2,\n        dim_ff=d_model * 2,\n    ).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader)\n\n        exp_dict = experiment_data[\"d_model_tuning\"][run_key][\"SPR_BENCH\"]\n        exp_dict[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_dict[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        exp_dict[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_dict[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"d_model={d_model} | Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n\n    # store last predictions\n    exp_dict[\"predictions\"] = val_pred\n    exp_dict[\"ground_truth\"] = val_gt\n\n# -------------------------------------------------------------------------------\n# 8) Save all results\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data, allow_pickle=True)\nprint(\"Saved experiment data to\", save_path)\n", "import os, pathlib, time, random, json, numpy as np, torch, math, re\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------\n# compulsory dirs / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -----------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"),\n        dev=_load(\"dev.csv\"),\n        test=_load(\"test.csv\"),\n    )\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -----------------------------------------------------------\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(t):\n    if t not in vocab:\n        vocab[t] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Vocab size =\", vocab_size, \" Num labels =\", num_labels)\n\nMAX_LEN, BATCH_SIZE, EPOCHS = 128, 64, 5\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, split):\n        self.seqs = split[\"sequence\"]\n        self.labels = split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"labels\": torch.tensor(label2id[self.labels[idx]]),\n        }\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    SPRDataset(spr[\"train\"]), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    SPRDataset(spr[\"dev\"]), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# -----------------------------------------------------------\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn):\n        x = self.embed(ids) + self.pos[:, : ids.size(1)]\n        x = self.encoder(x, src_key_padding_mask=~attn.bool())\n        return self.cls(x[:, 0])\n\n\n# -----------------------------------------------------------\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -----------------------------------------------------------\nNLAYERS_LIST = [1, 2, 3, 4, 6]\nexperiment_data = {\"nlayers_tuning\": {\"SPR_BENCH\": {}}}\n\nfor nl in NLAYERS_LIST:\n    print(f\"\\n===== Running experiment with nlayers={nl} =====\")\n    model = SimpleTransformer(vocab_size, num_labels, nlayers=nl).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    run_dict = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, criterion, optimizer)\n        vl_loss, vl_f1, pred, gt = run_epoch(model, dev_loader, criterion)\n        run_dict[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        run_dict[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": vl_f1})\n        run_dict[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        run_dict[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": vl_loss})\n        print(\n            f\"Epoch {epoch}: tr_loss={tr_loss:.4f} vl_loss={vl_loss:.4f} tr_F1={tr_f1:.4f} vl_F1={vl_f1:.4f} ({time.time()-t0:.1f}s)\"\n        )\n    run_dict[\"predictions\"] = pred\n    run_dict[\"ground_truth\"] = gt\n    experiment_data[\"nlayers_tuning\"][\"SPR_BENCH\"][f\"nlayers_{nl}\"] = run_dict\n    del model\n    torch.cuda.empty_cache()\n\n# -----------------------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, numpy as np, torch, random, json, math, re\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------------------\n# mandatory working dir & device declarations\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------------\n# util to load benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# ---------------------------------------------------------------------------------\n# 1) Load dataset\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------------\n# 2) Build vocabulary (whitespace tokenisation)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n# label mapping\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ---------------------------------------------------------------------------------\n# 3) Dataset wrapper\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    tok_ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(tok_ids)\n    if len(tok_ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(tok_ids)\n        tok_ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return tok_ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds = SPRDataset(spr[\"train\"])\ndev_ds = SPRDataset(spr[\"dev\"])\n\n# ---------------------------------------------------------------------------------\n# 4) DataLoaders\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ---------------------------------------------------------------------------------\n# 5) Model definition\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# ---------------------------------------------------------------------------------\n# 6) Training & evaluation helpers\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ---------------------------------------------------------------------------------\n# 7) Hyperparameter sweep over weight_decay\nEPOCHS = 5\nweight_decay_values = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\nexperiment_data = {\"weight_decay\": {}}\n\nfor wd in weight_decay_values:\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    model = SimpleTransformer(vocab_size, num_labels).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=wd)\n\n    run_record = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader, criterion)\n\n        run_record[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        run_record[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        run_record[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        run_record[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"Epoch {epoch}: wd={wd}  train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f}  (time {time.time()-t0:.1f}s)\"\n        )\n\n    # store final predictions / gts of last epoch\n    run_record[\"predictions\"] = val_pred\n    run_record[\"ground_truth\"] = val_gt\n    experiment_data[\"weight_decay\"][str(wd)] = run_record\n\n# ---------------------------------------------------------------------------------\n# 8) Save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, math, re, time, json, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------\n# mandatory working dir & device declarations\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# helper to load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ------------------------------------------------------------\n# build vocabulary\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\n# labels\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ------------------------------------------------------------\n# dataset wrapper\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    tok_ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(tok_ids)\n    if len(tok_ids) < MAX_LEN:\n        pad = MAX_LEN - len(tok_ids)\n        tok_ids += [vocab[PAD]] * pad\n        attn += [0] * pad\n    return tok_ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds = SPRDataset(spr[\"train\"])\ndev_ds = SPRDataset(spr[\"dev\"])\n\n# ------------------------------------------------------------\n# dataloaders\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ------------------------------------------------------------\n# model definition factory\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        dropout_prob,\n        d_model=128,\n        nhead=4,\n        nlayers=2,\n        dim_ff=256,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout=dropout_prob, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# ------------------------------------------------------------\n# training / evaluation helpers\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = tot_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ------------------------------------------------------------\n# hyperparameter tuning loop\nEPOCHS = 5\nDROP_RATES = [0.0, 0.1, 0.2, 0.3]\n\nexperiment_data = {\"dropout_probability\": {\"SPR_BENCH\": {}}}\n\nfor dr in DROP_RATES:\n    print(f\"\\n=== Training with dropout={dr} ===\")\n    model = SimpleTransformer(vocab_size, num_labels, dr).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    exp_rec = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader, criterion)\n\n        exp_rec[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_rec[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        exp_rec[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_rec[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"Epoch {epoch}: train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} (time {time.time()-t0:.1f}s)\"\n        )\n\n    # store final predictions / ground truth\n    exp_rec[\"predictions\"] = val_pred\n    exp_rec[\"ground_truth\"] = val_gt\n    experiment_data[\"dropout_probability\"][\"SPR_BENCH\"][str(dr)] = exp_rec\n\n    # free gpu memory\n    del model\n    torch.cuda.empty_cache()\n\n# ------------------------------------------------------------\n# save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os, pathlib, time, json, random, math, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------------------------------------------------------------------\n# mandatory working dir & device declarations\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------------------\n# util to load benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# ---------------------------------------------------------------------------------\n# 1) Load dataset\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------------------------------------------------------------------------\n# 2) Build vocabulary (whitespace tokenisation)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_token(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_token(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# ---------------------------------------------------------------------------------\n# 3) Dataset wrapper\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    tokens = [CLS] + seq.strip().split()\n    tok_ids = [vocab.get(t, vocab[UNK]) for t in tokens][:MAX_LEN]\n    attn = [1] * len(tok_ids)\n    if len(tok_ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(tok_ids)\n        tok_ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return tok_ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\n\n# ---------------------------------------------------------------------------------\n# 4) DataLoaders\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n)\n\n\n# ---------------------------------------------------------------------------------\n# 5) Model definition\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        return self.classifier(x[:, 0, :])\n\n\n# ---------------------------------------------------------------------------------\n# 6) Training & evaluation helpers\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------------------\n# 7) Hyperparameter tuning loop\nEPOCHS = 5\nNHEAD_OPTIONS = [2, 4, 8]\nexperiment_data = {\"nhead_tuning\": {\"SPR_BENCH\": {}}}\n\nfor nhead in NHEAD_OPTIONS:\n    print(f\"\\n===== Training with nhead={nhead} =====\")\n    model = SimpleTransformer(vocab_size, num_labels, nhead=nhead).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    run_record = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader, criterion)\n        run_record[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        run_record[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        run_record[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        run_record[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n        print(\n            f\"nhead={nhead} Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_F1={tr_f1:.4f} val_F1={val_f1:.4f} \"\n            f\"time={time.time()-t0:.1f}s\"\n        )\n    run_record[\"predictions\"] = val_pred\n    run_record[\"ground_truth\"] = val_gt\n    experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][str(nhead)] = run_record\n    # free memory\n    del model\n    torch.cuda.empty_cache()\n\n# ---------------------------------------------------------------------------------\n# 8) Save experiment data\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment data to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, random, math, json, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------------\n# 0) Reproducibility & device\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------------\n# 1) Working dir & experiment data dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"d_model_tuning\": {}}  # each key will be a d_model value as string\n\n\n# -------------------------------------------------------------------------------\n# 2) Load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------------\n# 3) Build vocab (whitespace)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# -------------------------------------------------------------------------------\n# 4) Dataset & dataloaders\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n\n# -------------------------------------------------------------------------------\n# 5) Transformer model\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# -------------------------------------------------------------------------------\n# 6) Train / eval helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------------------------------------------------------------------\n# 7) Hyperparameter grid search over d_model\nDMODEL_GRID = [64, 128, 256, 384]\n\n\ndef pick_head(d):\n    if d % 8 == 0:\n        return 8\n    if d % 4 == 0:\n        return 4\n    return 1\n\n\nEPOCHS = 5\n\nfor d_model in DMODEL_GRID:\n    run_key = str(d_model)\n    experiment_data[\"d_model_tuning\"][run_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    print(f\"\\n=== Training with d_model={d_model} ===\")\n    model = SimpleTransformer(\n        vocab_size,\n        num_labels,\n        d_model=d_model,\n        nhead=pick_head(d_model),\n        nlayers=2,\n        dim_ff=d_model * 2,\n    ).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader)\n\n        exp_dict = experiment_data[\"d_model_tuning\"][run_key][\"SPR_BENCH\"]\n        exp_dict[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_dict[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        exp_dict[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_dict[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"d_model={d_model} | Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n\n    # store last predictions\n    exp_dict[\"predictions\"] = val_pred\n    exp_dict[\"ground_truth\"] = val_gt\n\n# -------------------------------------------------------------------------------\n# 8) Save all results\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data, allow_pickle=True)\nprint(\"Saved experiment data to\", save_path)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, random, math, json, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------------\n# 0) Reproducibility & device\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------------\n# 1) Working dir & experiment data dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"d_model_tuning\": {}}  # each key will be a d_model value as string\n\n\n# -------------------------------------------------------------------------------\n# 2) Load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------------\n# 3) Build vocab (whitespace)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# -------------------------------------------------------------------------------\n# 4) Dataset & dataloaders\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n\n# -------------------------------------------------------------------------------\n# 5) Transformer model\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# -------------------------------------------------------------------------------\n# 6) Train / eval helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------------------------------------------------------------------\n# 7) Hyperparameter grid search over d_model\nDMODEL_GRID = [64, 128, 256, 384]\n\n\ndef pick_head(d):\n    if d % 8 == 0:\n        return 8\n    if d % 4 == 0:\n        return 4\n    return 1\n\n\nEPOCHS = 5\n\nfor d_model in DMODEL_GRID:\n    run_key = str(d_model)\n    experiment_data[\"d_model_tuning\"][run_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    print(f\"\\n=== Training with d_model={d_model} ===\")\n    model = SimpleTransformer(\n        vocab_size,\n        num_labels,\n        d_model=d_model,\n        nhead=pick_head(d_model),\n        nlayers=2,\n        dim_ff=d_model * 2,\n    ).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader)\n\n        exp_dict = experiment_data[\"d_model_tuning\"][run_key][\"SPR_BENCH\"]\n        exp_dict[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_dict[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        exp_dict[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_dict[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"d_model={d_model} | Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n\n    # store last predictions\n    exp_dict[\"predictions\"] = val_pred\n    exp_dict[\"ground_truth\"] = val_gt\n\n# -------------------------------------------------------------------------------\n# 8) Save all results\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data, allow_pickle=True)\nprint(\"Saved experiment data to\", save_path)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, time, random, math, json, re, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------------------------------------------------------------------\n# 0) Reproducibility & device\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------------------------------------\n# 1) Working dir & experiment data dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\"d_model_tuning\": {}}  # each key will be a d_model value as string\n\n\n# -------------------------------------------------------------------------------\n# 2) Load SPR benchmark\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    dd = DatasetDict()\n    dd[\"train\"] = _load(\"train.csv\")\n    dd[\"dev\"] = _load(\"dev.csv\")\n    dd[\"test\"] = _load(\"test.csv\")\n    return dd\n\n\nSPR_PATH = pathlib.Path(\n    os.getenv(\"SPR_DATA_PATH\", \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\")\n)\nspr = load_spr_bench(SPR_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------------------\n# 3) Build vocab (whitespace)\nCLS, PAD, UNK = \"[CLS]\", \"[PAD]\", \"[UNK]\"\nvocab = {PAD: 0, CLS: 1, UNK: 2}\n\n\ndef add_tok(tok):\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\n\nfor seq in spr[\"train\"][\"sequence\"]:\n    for tok in seq.strip().split():\n        add_tok(tok)\nvocab_size = len(vocab)\nprint(\"Vocab size:\", vocab_size)\n\nlabel2id = {lab: i for i, lab in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: lab for lab, i in label2id.items()}\nnum_labels = len(label2id)\nprint(\"Num labels:\", num_labels)\n\n# -------------------------------------------------------------------------------\n# 4) Dataset & dataloaders\nMAX_LEN = 128\n\n\ndef encode_sequence(seq: str):\n    toks = [CLS] + seq.strip().split()\n    ids = [vocab.get(t, vocab[UNK]) for t in toks][:MAX_LEN]\n    attn = [1] * len(ids)\n    if len(ids) < MAX_LEN:\n        pad_len = MAX_LEN - len(ids)\n        ids += [vocab[PAD]] * pad_len\n        attn += [0] * pad_len\n    return ids, attn\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids, attn = encode_sequence(self.seqs[idx])\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\ntrain_ds, dev_ds = SPRDataset(spr[\"train\"]), SPRDataset(spr[\"dev\"])\nBATCH_SIZE = 64\n\n\ndef collate_fn(batch):\n    return {k: torch.stack([b[k] for b in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n\n# -------------------------------------------------------------------------------\n# 5) Transformer model\nclass SimpleTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=128, nhead=4, nlayers=2, dim_ff=256\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=vocab[PAD])\n        self.pos = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, nlayers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos[:, : input_ids.size(1), :]\n        x = self.transformer(x, src_key_padding_mask=~attention_mask.bool())\n        logits = self.classifier(x[:, 0, :])\n        return logits\n\n\n# -------------------------------------------------------------------------------\n# 6) Train / eval helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef run_epoch(model, loader, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, 1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# -------------------------------------------------------------------------------\n# 7) Hyperparameter grid search over d_model\nDMODEL_GRID = [64, 128, 256, 384]\n\n\ndef pick_head(d):\n    if d % 8 == 0:\n        return 8\n    if d % 4 == 0:\n        return 4\n    return 1\n\n\nEPOCHS = 5\n\nfor d_model in DMODEL_GRID:\n    run_key = str(d_model)\n    experiment_data[\"d_model_tuning\"][run_key] = {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    print(f\"\\n=== Training with d_model={d_model} ===\")\n    model = SimpleTransformer(\n        vocab_size,\n        num_labels,\n        d_model=d_model,\n        nhead=pick_head(d_model),\n        nlayers=2,\n        dim_ff=d_model * 2,\n    ).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, val_pred, val_gt = run_epoch(model, dev_loader)\n\n        exp_dict = experiment_data[\"d_model_tuning\"][run_key][\"SPR_BENCH\"]\n        exp_dict[\"metrics\"][\"train\"].append({\"epoch\": epoch, \"macro_f1\": tr_f1})\n        exp_dict[\"metrics\"][\"val\"].append({\"epoch\": epoch, \"macro_f1\": val_f1})\n        exp_dict[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": tr_loss})\n        exp_dict[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n\n        print(\n            f\"d_model={d_model} | Epoch {epoch}: \"\n            f\"train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, \"\n            f\"train_F1={tr_f1:.4f}, val_F1={val_f1:.4f} \"\n            f\"({time.time()-t0:.1f}s)\"\n        )\n\n    # store last predictions\n    exp_dict[\"predictions\"] = val_pred\n    exp_dict[\"ground_truth\"] = val_gt\n\n# -------------------------------------------------------------------------------\n# 8) Save all results\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data, allow_pickle=True)\nprint(\"Saved experiment data to\", save_path)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 161645.78\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 110650.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 188457.23\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', 'Epoch 1:\ntrain_loss=0.3344, val_loss=1.6811, train_F1=0.8878, val_F1=0.6839  (time\n1.1s)', '\\n', 'Epoch 2: train_loss=0.0536, val_loss=2.0087, train_F1=0.9855,\nval_F1=0.6940  (time 0.8s)', '\\n', 'Epoch 3: train_loss=0.0521, val_loss=2.0947,\ntrain_F1=0.9870, val_F1=0.6899  (time 0.8s)', '\\n', 'Epoch 4: train_loss=0.0476,\nval_loss=2.0644, train_F1=0.9875, val_F1=0.6920  (time 0.8s)', '\\n', 'Epoch 5:\ntrain_loss=0.0395, val_loss=2.1822, train_F1=0.9920, val_F1=0.6899  (time\n0.8s)', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n1/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n151670.79 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 117916.90\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 198424.83\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n===== Training for 5\nepochs =====', '\\n', 'Ep 1/5  tr_loss:0.3178 val_loss:1.5119  tr_F1:0.8755\nval_F1:0.6759  (1.1s)', '\\n', 'Ep 2/5  tr_loss:0.0650 val_loss:1.7365\ntr_F1:0.9805 val_F1:0.6899  (0.8s)', '\\n', 'Ep 3/5  tr_loss:0.0554\nval_loss:1.7193  tr_F1:0.9860 val_F1:0.6840  (0.8s)', '\\n', 'Ep 4/5\ntr_loss:0.0470 val_loss:1.9492  tr_F1:0.9875 val_F1:0.6860  (0.8s)', '\\n', 'Ep\n5/5  tr_loss:0.0415 val_loss:1.9193  tr_F1:0.9900 val_F1:0.6920  (0.8s)', '\\n',\n'\\n===== Training for 10 epochs =====', '\\n', 'Ep 1/10  tr_loss:0.3178\nval_loss:1.5119  tr_F1:0.8755 val_F1:0.6759  (0.8s)', '\\n', 'Ep 2/10\ntr_loss:0.0650 val_loss:1.7365  tr_F1:0.9805 val_F1:0.6899  (0.8s)', '\\n', 'Ep\n3/10  tr_loss:0.0554 val_loss:1.7193  tr_F1:0.9860 val_F1:0.6840  (0.8s)', '\\n',\n'Ep 4/10  tr_loss:0.0470 val_loss:1.9492  tr_F1:0.9875 val_F1:0.6860  (0.8s)',\n'\\n', 'Ep 5/10  tr_loss:0.0415 val_loss:1.9193  tr_F1:0.9900 val_F1:0.6920\n(0.8s)', '\\n', 'Ep 6/10  tr_loss:0.0439 val_loss:1.9092  tr_F1:0.9870\nval_F1:0.6551  (0.8s)', '\\n', 'Ep 7/10  tr_loss:0.0465 val_loss:2.0995\ntr_F1:0.9885 val_F1:0.6839  (0.8s)', '\\n', 'Ep 8/10  tr_loss:0.0470\nval_loss:1.9116  tr_F1:0.9875 val_F1:0.6940  (0.8s)', '\\n', 'Ep 9/10\ntr_loss:0.0336 val_loss:2.0151  tr_F1:0.9930 val_F1:0.6960  (0.8s)', '\\n', 'Ep\n10/10  tr_loss:0.0356 val_loss:1.9195  tr_F1:0.9915 val_F1:0.6960  (0.8s)',\n'\\n', '\\n===== Training for 15 epochs =====', '\\n', 'Ep 1/15  tr_loss:0.3178\nval_loss:1.5119  tr_F1:0.8755 val_F1:0.6759  (0.8s)', '\\n', 'Ep 2/15\ntr_loss:0.0650 val_loss:1.7365  tr_F1:0.9805 val_F1:0.6899  (0.8s)', '\\n', 'Ep\n3/15  tr_loss:0.0554 val_loss:1.7193  tr_F1:0.9860 val_F1:0.6840  (0.8s)', '\\n',\n'Ep 4/15  tr_loss:0.0470 val_loss:1.9492  tr_F1:0.9875 val_F1:0.6860  (0.8s)',\n'\\n', 'Ep 5/15  tr_loss:0.0415 val_loss:1.9193  tr_F1:0.9900 val_F1:0.6920\n(0.8s)', '\\n', 'Ep 6/15  tr_loss:0.0439 val_loss:1.9092  tr_F1:0.9870\nval_F1:0.6551  (0.8s)', '\\n', 'Ep 7/15  tr_loss:0.0465 val_loss:2.0995\ntr_F1:0.9885 val_F1:0.6839  (0.8s)', '\\n', 'Ep 8/15  tr_loss:0.0470\nval_loss:1.9116  tr_F1:0.9875 val_F1:0.6940  (0.8s)', '\\n', 'Ep 9/15\ntr_loss:0.0336 val_loss:2.0151  tr_F1:0.9930 val_F1:0.6960  (0.8s)', '\\n', 'Ep\n10/15  tr_loss:0.0356 val_loss:1.9195  tr_F1:0.9915 val_F1:0.6960  (0.8s)',\n'\\n', 'Ep 11/15  tr_loss:0.0445 val_loss:1.8879  tr_F1:0.9900 val_F1:0.6920\n(0.9s)', '\\n', 'Ep 12/15  tr_loss:0.0320 val_loss:2.0541  tr_F1:0.9940\nval_F1:0.6920  (0.8s)', '\\n', 'Ep 13/15  tr_loss:0.0268 val_loss:2.0744\ntr_F1:0.9955 val_F1:0.6940  (0.8s)', '\\n', 'Ep 14/15  tr_loss:0.0316\nval_loss:2.0820  tr_F1:0.9930 val_F1:0.6940  (0.8s)', '\\n', 'Ep 15/15\ntr_loss:0.0246 val_loss:2.1175  tr_F1:0.9945 val_F1:0.6940  (0.8s)', '\\n',\n'\\n===== Training for 20 epochs =====', '\\n', 'Ep 1/20  tr_loss:0.3178\nval_loss:1.5119  tr_F1:0.8755 val_F1:0.6759  (0.8s)', '\\n', 'Ep 2/20\ntr_loss:0.0650 val_loss:1.7365  tr_F1:0.9805 val_F1:0.6899  (0.8s)', '\\n', 'Ep\n3/20  tr_loss:0.0554 val_loss:1.7193  tr_F1:0.9860 val_F1:0.6840  (0.8s)', '\\n',\n'Ep 4/20  tr_loss:0.0470 val_loss:1.9492  tr_F1:0.9875 val_F1:0.6860  (0.8s)',\n'\\n', 'Ep 5/20  tr_loss:0.0415 val_loss:1.9193  tr_F1:0.9900 val_F1:0.6920\n(0.8s)', '\\n', 'Ep 6/20  tr_loss:0.0439 val_loss:1.9092  tr_F1:0.9870\nval_F1:0.6551  (0.8s)', '\\n', 'Ep 7/20  tr_loss:0.0465 val_loss:2.0995\ntr_F1:0.9885 val_F1:0.6839  (0.8s)', '\\n', 'Ep 8/20  tr_loss:0.0470\nval_loss:1.9116  tr_F1:0.9875 val_F1:0.6940  (0.8s)', '\\n', 'Ep 9/20\ntr_loss:0.0336 val_loss:2.0151  tr_F1:0.9930 val_F1:0.6960  (0.8s)', '\\n', 'Ep\n10/20  tr_loss:0.0356 val_loss:1.9195  tr_F1:0.9915 val_F1:0.6960  (0.8s)',\n'\\n', 'Ep 11/20  tr_loss:0.0445 val_loss:1.8879  tr_F1:0.9900 val_F1:0.6920\n(0.8s)', '\\n', 'Ep 12/20  tr_loss:0.0320 val_loss:2.0541  tr_F1:0.9940\nval_F1:0.6920  (0.8s)', '\\n', 'Ep 13/20  tr_loss:0.0268 val_loss:2.0744\ntr_F1:0.9955 val_F1:0.6940  (0.8s)', '\\n', 'Ep 14/20  tr_loss:0.0316\nval_loss:2.0820  tr_F1:0.9930 val_F1:0.6940  (0.8s)', '\\n', 'Ep 15/20\ntr_loss:0.0246 val_loss:2.1175  tr_F1:0.9945 val_F1:0.6940  (0.8s)', '\\n', 'Ep\n16/20  tr_loss:0.0499 val_loss:1.9494  tr_F1:0.9865 val_F1:0.6940  (0.8s)',\n'\\n', 'Ep 17/20  tr_loss:0.0288 val_loss:2.0260  tr_F1:0.9950 val_F1:0.6920\n(0.8s)', '\\n', 'Ep 18/20  tr_loss:0.0241 val_loss:2.1458  tr_F1:0.9950\nval_F1:0.6920  (0.8s)', '\\n', 'Ep 19/20  tr_loss:0.0244 val_loss:2.0961\ntr_F1:0.9940 val_F1:0.6920  (0.8s)', '\\n', 'Ep 20/20  tr_loss:0.0360\nval_loss:2.0532  tr_F1:0.9905 val_F1:0.6839  (0.8s)', '\\n', 'Saved\nexperiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/0-\nrun/process_ForkProcess-6/working', '\\n', 'Execution time: 43 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 98129.59\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 76920.19\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 129318.12\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n==== Training with\nlearning rate 0.001 ====', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'LR 0.001 | Epoch 1:\ntrain_loss=0.2860, val_loss=1.9981, train_F1=0.8505, val_F1=0.6737 (time 0.5s)',\n'\\n', 'LR 0.001 | Epoch 2: train_loss=0.0594, val_loss=2.1965, train_F1=0.9875,\nval_F1=0.6818 (time 0.2s)', '\\n', 'LR 0.001 | Epoch 3: train_loss=0.0505,\nval_loss=1.8347, train_F1=0.9875, val_F1=0.6799 (time 0.2s)', '\\n', 'LR 0.001 |\nEpoch 4: train_loss=0.0642, val_loss=1.9449, train_F1=0.9820, val_F1=0.6879\n(time 0.2s)', '\\n', 'LR 0.001 | Epoch 5: train_loss=0.0390, val_loss=2.1673,\ntrain_F1=0.9925, val_F1=0.6899 (time 0.2s)', '\\n', '\\n==== Training with\nlearning rate 0.0005 ====', '\\n', 'LR 0.0005 | Epoch 1: train_loss=0.2649,\nval_loss=2.0033, train_F1=0.9079, val_F1=0.6780 (time 0.3s)', '\\n', 'LR 0.0005 |\nEpoch 2: train_loss=0.0736, val_loss=1.8419, train_F1=0.9815, val_F1=0.6899\n(time 0.2s)', '\\n', 'LR 0.0005 | Epoch 3: train_loss=0.0699, val_loss=1.9580,\ntrain_F1=0.9815, val_F1=0.6879 (time 0.2s)', '\\n', 'LR 0.0005 | Epoch 4:\ntrain_loss=0.0468, val_loss=2.0519, train_F1=0.9885, val_F1=0.6920 (time 0.2s)',\n'\\n', 'LR 0.0005 | Epoch 5: train_loss=0.0486, val_loss=2.2359, train_F1=0.9905,\nval_F1=0.6818 (time 0.2s)', '\\n', '\\n==== Training with learning rate 0.0001\n====', '\\n', 'LR 0.0001 | Epoch 1: train_loss=0.5565, val_loss=0.6557,\ntrain_F1=0.8149, val_F1=0.6734 (time 0.2s)', '\\n', 'LR 0.0001 | Epoch 2:\ntrain_loss=0.1850, val_loss=1.1809, train_F1=0.9700, val_F1=0.6859 (time 0.2s)',\n'\\n', 'LR 0.0001 | Epoch 3: train_loss=0.0789, val_loss=1.5229, train_F1=0.9815,\nval_F1=0.6920 (time 0.3s)', '\\n', 'LR 0.0001 | Epoch 4: train_loss=0.0668,\nval_loss=1.6945, train_F1=0.9840, val_F1=0.6940 (time 0.4s)', '\\n', 'LR 0.0001 |\nEpoch 5: train_loss=0.0554, val_loss=1.8411, train_F1=0.9855, val_F1=0.6940\n(time 0.4s)', '\\n', '\\n==== Training with learning rate 5e-05 ====', '\\n', 'LR\n5e-05 | Epoch 1: train_loss=0.6539, val_loss=0.6612, train_F1=0.6663,\nval_F1=0.6525 (time 0.3s)', '\\n', 'LR 5e-05 | Epoch 2: train_loss=0.4738,\nval_loss=0.6655, train_F1=0.9115, val_F1=0.6797 (time 0.3s)', '\\n', 'LR 5e-05 |\nEpoch 3: train_loss=0.2102, val_loss=0.9938, train_F1=0.9680, val_F1=0.6737\n(time 0.3s)', '\\n', 'LR 5e-05 | Epoch 4: train_loss=0.1076, val_loss=1.3026,\ntrain_F1=0.9755, val_F1=0.6879 (time 0.3s)', '\\n', 'LR 5e-05 | Epoch 5:\ntrain_loss=0.0809, val_loss=1.5287, train_F1=0.9815, val_F1=0.6859 (time 0.3s)',\n'\\n', '\\nBest LR:', ' ', '0.0001', ' ', 'with dev macro-F1:', ' ',\n'0.693969396939694', '\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-\nScientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 149343.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 147499.79\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 115676.22\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n====== Training with\nbatch size 32 ======', '\\n', 'Epoch 1: train_loss=0.2233, val_loss=2.0082,\ntrain_F1=0.9175, val_F1=0.6879  (time 0.7s)', '\\n', 'Epoch 2: train_loss=0.0480,\nval_loss=2.2381, train_F1=0.9870, val_F1=0.6899  (time 0.4s)', '\\n', 'Epoch 3:\ntrain_loss=0.0484, val_loss=2.1365, train_F1=0.9890, val_F1=0.6839  (time\n0.4s)', '\\n', 'Epoch 4: train_loss=0.0395, val_loss=2.0795, train_F1=0.9920,\nval_F1=0.6940  (time 0.4s)', '\\n', 'Epoch 5: train_loss=0.0745, val_loss=1.9672,\ntrain_F1=0.9815, val_F1=0.6818  (time 0.4s)', '\\n', '\\n====== Training with\nbatch size 64 ======', '\\n', 'Epoch 1: train_loss=0.3429, val_loss=1.6091,\ntrain_F1=0.8842, val_F1=0.6879  (time 0.2s)', '\\n', 'Epoch 2: train_loss=0.0531,\nval_loss=1.9250, train_F1=0.9870, val_F1=0.6940  (time 0.2s)', '\\n', 'Epoch 3:\ntrain_loss=0.0707, val_loss=2.1045, train_F1=0.9800, val_F1=0.6798  (time\n0.2s)', '\\n', 'Epoch 4: train_loss=0.0529, val_loss=1.9370, train_F1=0.9855,\nval_F1=0.6940  (time 0.2s)', '\\n', 'Epoch 5: train_loss=0.0418, val_loss=2.0657,\ntrain_F1=0.9890, val_F1=0.6940  (time 0.2s)', '\\n', '\\n====== Training with\nbatch size 128 ======', '\\n', 'Epoch 1: train_loss=0.4953, val_loss=0.7774,\ntrain_F1=0.8210, val_F1=0.6756  (time 0.2s)', '\\n', 'Epoch 2: train_loss=0.1163,\nval_loss=1.5296, train_F1=0.9705, val_F1=0.6900  (time 0.2s)', '\\n', 'Epoch 3:\ntrain_loss=0.0700, val_loss=1.8523, train_F1=0.9790, val_F1=0.6840  (time\n0.2s)', '\\n', 'Epoch 4: train_loss=0.0661, val_loss=1.9146, train_F1=0.9825,\nval_F1=0.6920  (time 0.2s)', '\\n', 'Epoch 5: train_loss=0.0473, val_loss=2.0659,\ntrain_F1=0.9875, val_F1=0.6899  (time 0.2s)', '\\n', 'Saved experiment data to',\n' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 114767.80\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 143365.60\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 234502.07\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training with\nd_model=64 ===', '\\n', 'd_model=64 | Epoch 1: train_loss=0.6256,\nval_loss=0.6493, train_F1=0.6890, val_F1=0.6506 (1.0s)', '\\n', 'd_model=64 |\nEpoch 2: train_loss=0.2232, val_loss=1.2567, train_F1=0.9555, val_F1=0.6799\n(0.7s)', '\\n', 'd_model=64 | Epoch 3: train_loss=0.0717, val_loss=1.5965,\ntrain_F1=0.9805, val_F1=0.6920 (0.7s)', '\\n', 'd_model=64 | Epoch 4:\ntrain_loss=0.0555, val_loss=1.7007, train_F1=0.9840, val_F1=0.6920 (0.7s)',\n'\\n', 'd_model=64 | Epoch 5: train_loss=0.0472, val_loss=1.8795,\ntrain_F1=0.9875, val_F1=0.6879 (0.7s)', '\\n', '\\n=== Training with d_model=128\n===', '\\n', 'd_model=128 | Epoch 1: train_loss=0.2858, val_loss=1.4873,\ntrain_F1=0.8915, val_F1=0.6839 (0.7s)', '\\n', 'd_model=128 | Epoch 2:\ntrain_loss=0.0649, val_loss=1.9645, train_F1=0.9805, val_F1=0.6879 (0.7s)',\n'\\n', 'd_model=128 | Epoch 3: train_loss=0.0470, val_loss=2.2001,\ntrain_F1=0.9885, val_F1=0.6879 (0.7s)', '\\n', 'd_model=128 | Epoch 4:\ntrain_loss=0.0435, val_loss=2.1868, train_F1=0.9910, val_F1=0.6879 (0.7s)',\n'\\n', 'd_model=128 | Epoch 5: train_loss=0.0447, val_loss=2.0346,\ntrain_F1=0.9895, val_F1=0.6900 (0.7s)', '\\n', '\\n=== Training with d_model=256\n===', '\\n', 'd_model=256 | Epoch 1: train_loss=0.2619, val_loss=1.9211,\ntrain_F1=0.8819, val_F1=0.6879 (1.0s)', '\\n', 'd_model=256 | Epoch 2:\ntrain_loss=0.0716, val_loss=2.3180, train_F1=0.9800, val_F1=0.6859 (1.0s)',\n'\\n', 'd_model=256 | Epoch 3: train_loss=0.0544, val_loss=2.4372,\ntrain_F1=0.9860, val_F1=0.6940 (1.0s)', '\\n', 'd_model=256 | Epoch 4:\ntrain_loss=0.0423, val_loss=2.5196, train_F1=0.9880, val_F1=0.6899 (1.0s)',\n'\\n', 'd_model=256 | Epoch 5: train_loss=0.0516, val_loss=2.3775,\ntrain_F1=0.9870, val_F1=0.6859 (1.0s)', '\\n', '\\n=== Training with d_model=384\n===', '\\n', 'd_model=384 | Epoch 1: train_loss=0.2699, val_loss=2.1986,\ntrain_F1=0.8764, val_F1=0.6820 (1.3s)', '\\n', 'd_model=384 | Epoch 2:\ntrain_loss=0.0652, val_loss=2.2939, train_F1=0.9830, val_F1=0.6940 (1.3s)',\n'\\n', 'd_model=384 | Epoch 3: train_loss=0.0590, val_loss=2.4175,\ntrain_F1=0.9840, val_F1=0.6900 (1.3s)', '\\n', 'd_model=384 | Epoch 4:\ntrain_loss=0.0514, val_loss=2.6437, train_F1=0.9895, val_F1=0.6818 (1.3s)',\n'\\n', 'd_model=384 | Epoch 5: train_loss=0.0594, val_loss=2.5905,\ntrain_F1=0.9840, val_F1=0.6960 (1.3s)', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 22 seconds seconds (time\nlimit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab size =', ' ', '19', ' ', ' Num labels =', ' ', '2', '\\n',\n'\\n===== Running experiment with nlayers=1 =====', '\\n', 'Epoch 1:\ntr_loss=0.4559 vl_loss=1.0003 tr_F1=0.8683 vl_F1=0.6695 (1.1s)', '\\n', 'Epoch 2:\ntr_loss=0.0823 vl_loss=1.8920 tr_F1=0.9795 vl_F1=0.6899 (0.7s)', '\\n', 'Epoch 3:\ntr_loss=0.0543 vl_loss=2.0036 tr_F1=0.9875 vl_F1=0.6940 (0.7s)', '\\n', 'Epoch 4:\ntr_loss=0.0491 vl_loss=2.1266 tr_F1=0.9890 vl_F1=0.6920 (0.8s)', '\\n', 'Epoch 5:\ntr_loss=0.0539 vl_loss=2.1434 tr_F1=0.9875 vl_F1=0.6879 (0.7s)', '\\n', '\\n=====\nRunning experiment with nlayers=2 =====', '\\n', 'Epoch 1: tr_loss=0.3430\nvl_loss=1.5872 tr_F1=0.8782 vl_F1=0.6879 (0.9s)', '\\n', 'Epoch 2: tr_loss=0.0595\nvl_loss=1.8870 tr_F1=0.9865 vl_F1=0.6960 (0.9s)', '\\n', 'Epoch 3: tr_loss=0.0628\nvl_loss=2.1118 tr_F1=0.9830 vl_F1=0.6859 (0.9s)', '\\n', 'Epoch 4: tr_loss=0.0503\nvl_loss=1.9938 tr_F1=0.9875 vl_F1=0.6960 (0.9s)', '\\n', 'Epoch 5: tr_loss=0.0397\nvl_loss=2.1567 tr_F1=0.9905 vl_F1=0.6899 (0.9s)', '\\n', '\\n===== Running\nexperiment with nlayers=3 =====', '\\n', 'Epoch 1: tr_loss=0.2861 vl_loss=1.6516\ntr_F1=0.8870 vl_F1=0.6920 (1.0s)', '\\n', 'Epoch 2: tr_loss=0.0848 vl_loss=1.8392\ntr_F1=0.9785 vl_F1=0.6879 (1.0s)', '\\n', 'Epoch 3: tr_loss=0.0544 vl_loss=1.8447\ntr_F1=0.9860 vl_F1=0.6940 (1.0s)', '\\n', 'Epoch 4: tr_loss=0.0464 vl_loss=1.8605\ntr_F1=0.9895 vl_F1=0.6859 (1.0s)', '\\n', 'Epoch 5: tr_loss=0.0507 vl_loss=1.9731\ntr_F1=0.9880 vl_F1=0.6940 (1.0s)', '\\n', '\\n===== Running experiment with\nnlayers=4 =====', '\\n', 'Epoch 1: tr_loss=0.2751 vl_loss=1.9048 tr_F1=0.8870\nvl_F1=0.6859 (1.2s)', '\\n', 'Epoch 2: tr_loss=0.0564 vl_loss=2.1115 tr_F1=0.9855\nvl_F1=0.6879 (1.1s)', '\\n', 'Epoch 3: tr_loss=0.0513 vl_loss=2.0750 tr_F1=0.9885\nvl_F1=0.6920 (1.1s)', '\\n', 'Epoch 4: tr_loss=0.0451 vl_loss=2.0260 tr_F1=0.9895\nvl_F1=0.6920 (1.2s)', '\\n', 'Epoch 5: tr_loss=0.0422 vl_loss=2.2320 tr_F1=0.9880\nvl_F1=0.6920 (1.1s)', '\\n', '\\n===== Running experiment with nlayers=6 =====',\n'\\n', 'Epoch 1: tr_loss=0.2076 vl_loss=1.8380 tr_F1=0.9130 vl_F1=0.6920 (1.4s)',\n'\\n', 'Epoch 2: tr_loss=0.0576 vl_loss=2.0979 tr_F1=0.9875 vl_F1=0.6778 (1.4s)',\n'\\n', 'Epoch 3: tr_loss=0.0856 vl_loss=1.7008 tr_F1=0.9755 vl_F1=0.6940 (1.4s)',\n'\\n', 'Epoch 4: tr_loss=0.0446 vl_loss=2.0018 tr_F1=0.9905 vl_F1=0.6899 (1.4s)',\n'\\n', 'Epoch 5: tr_loss=0.0305 vl_loss=2.0762 tr_F1=0.9945 vl_F1=0.6960 (1.6s)',\n'\\n', 'Saved experiment data to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 30 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training\nwith weight_decay=0.0 ===', '\\n', 'Epoch 1: wd=0.0  train_loss=0.3345,\nval_loss=1.7072, train_F1=0.8893, val_F1=0.6798  (time 0.6s)', '\\n', 'Epoch 2:\nwd=0.0  train_loss=0.0581, val_loss=1.9225, train_F1=0.9860, val_F1=0.6940\n(time 0.3s)', '\\n', 'Epoch 3: wd=0.0  train_loss=0.0484, val_loss=2.1642,\ntrain_F1=0.9885, val_F1=0.6899  (time 0.2s)', '\\n', 'Epoch 4: wd=0.0\ntrain_loss=0.0565, val_loss=1.9911, train_F1=0.9840, val_F1=0.6879  (time\n0.2s)', '\\n', 'Epoch 5: wd=0.0  train_loss=0.0451, val_loss=2.0650,\ntrain_F1=0.9905, val_F1=0.6920  (time 0.2s)', '\\n', '\\n=== Training with\nweight_decay=1e-05 ===', '\\n', 'Epoch 1: wd=1e-05  train_loss=0.3499,\nval_loss=1.6278, train_F1=0.8757, val_F1=0.6818  (time 0.3s)', '\\n', 'Epoch 2:\nwd=1e-05  train_loss=0.0552, val_loss=1.8688, train_F1=0.9855, val_F1=0.6960\n(time 0.3s)', '\\n', 'Epoch 3: wd=1e-05  train_loss=0.0514, val_loss=2.0707,\ntrain_F1=0.9865, val_F1=0.6839  (time 0.2s)', '\\n', 'Epoch 4: wd=1e-05\ntrain_loss=0.0628, val_loss=2.0117, train_F1=0.9810, val_F1=0.6879  (time\n0.2s)', '\\n', 'Epoch 5: wd=1e-05  train_loss=0.0399, val_loss=2.1230,\ntrain_F1=0.9915, val_F1=0.6920  (time 0.2s)', '\\n', '\\n=== Training with\nweight_decay=0.0001 ===', '\\n', 'Epoch 1: wd=0.0001  train_loss=0.3327,\nval_loss=1.5038, train_F1=0.8945, val_F1=0.6899  (time 0.3s)', '\\n', 'Epoch 2:\nwd=0.0001  train_loss=0.0720, val_loss=1.8786, train_F1=0.9800, val_F1=0.6920\n(time 0.3s)', '\\n', 'Epoch 3: wd=0.0001  train_loss=0.0526, val_loss=2.0417,\ntrain_F1=0.9840, val_F1=0.6920  (time 0.3s)', '\\n', 'Epoch 4: wd=0.0001\ntrain_loss=0.0486, val_loss=1.9721, train_F1=0.9885, val_F1=0.6900  (time\n0.3s)', '\\n', 'Epoch 5: wd=0.0001  train_loss=0.0536, val_loss=2.0169,\ntrain_F1=0.9875, val_F1=0.6940  (time 0.3s)', '\\n', '\\n=== Training with\nweight_decay=0.001 ===', '\\n', 'Epoch 1: wd=0.001  train_loss=0.3442,\nval_loss=1.6562, train_F1=0.8765, val_F1=0.6819  (time 0.3s)', '\\n', 'Epoch 2:\nwd=0.001  train_loss=0.0705, val_loss=1.9480, train_F1=0.9825, val_F1=0.6879\n(time 0.2s)', '\\n', 'Epoch 3: wd=0.001  train_loss=0.0508, val_loss=2.1224,\ntrain_F1=0.9890, val_F1=0.6920  (time 0.2s)', '\\n', 'Epoch 4: wd=0.001\ntrain_loss=0.0612, val_loss=2.1816, train_F1=0.9850, val_F1=0.6778  (time\n0.3s)', '\\n', 'Epoch 5: wd=0.001  train_loss=0.0593, val_loss=2.1437,\ntrain_F1=0.9835, val_F1=0.6859  (time 0.2s)', '\\n', '\\n=== Training with\nweight_decay=0.01 ===', '\\n', 'Epoch 1: wd=0.01  train_loss=0.3757,\nval_loss=1.3757, train_F1=0.8220, val_F1=0.6777  (time 0.2s)', '\\n', 'Epoch 2:\nwd=0.01  train_loss=0.0659, val_loss=2.0017, train_F1=0.9840, val_F1=0.6879\n(time 0.2s)', '\\n', 'Epoch 3: wd=0.01  train_loss=0.0614, val_loss=2.0543,\ntrain_F1=0.9860, val_F1=0.6879  (time 0.2s)', '\\n', 'Epoch 4: wd=0.01\ntrain_loss=0.0473, val_loss=2.1102, train_F1=0.9895, val_F1=0.6899  (time\n0.3s)', '\\n', 'Epoch 5: wd=0.01  train_loss=0.0785, val_loss=1.7828,\ntrain_F1=0.9755, val_F1=0.6960  (time 0.3s)', '\\n', 'Saved experiment data to',\n' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training\nwith dropout=0.0 ===', '\\n', 'Epoch 1: train_loss=0.3206, val_loss=1.7165,\ntrain_F1=0.8954, val_F1=0.6839 (time 0.5s)', '\\n', 'Epoch 2: train_loss=0.0497,\nval_loss=1.9024, train_F1=0.9900, val_F1=0.6940 (time 0.2s)', '\\n', 'Epoch 3:\ntrain_loss=0.0385, val_loss=2.0850, train_F1=0.9935, val_F1=0.6920 (time 0.2s)',\n'\\n', 'Epoch 4: train_loss=0.0433, val_loss=2.1391, train_F1=0.9880,\nval_F1=0.6899 (time 0.2s)', '\\n', 'Epoch 5: train_loss=0.0327, val_loss=2.2009,\ntrain_F1=0.9940, val_F1=0.6920 (time 0.2s)', '\\n', '\\n=== Training with\ndropout=0.1 ===', '\\n', 'Epoch 1: train_loss=0.3468, val_loss=1.6110,\ntrain_F1=0.8812, val_F1=0.6839 (time 0.3s)', '\\n', 'Epoch 2: train_loss=0.0560,\nval_loss=1.8587, train_F1=0.9845, val_F1=0.6960 (time 0.2s)', '\\n', 'Epoch 3:\ntrain_loss=0.0739, val_loss=1.9462, train_F1=0.9810, val_F1=0.6899 (time 0.3s)',\n'\\n', 'Epoch 4: train_loss=0.0461, val_loss=2.0070, train_F1=0.9880,\nval_F1=0.6940 (time 0.2s)', '\\n', 'Epoch 5: train_loss=0.0361, val_loss=2.0791,\ntrain_F1=0.9930, val_F1=0.6940 (time 0.2s)', '\\n', '\\n=== Training with\ndropout=0.2 ===', '\\n', 'Epoch 1: train_loss=0.3486, val_loss=1.4364,\ntrain_F1=0.8775, val_F1=0.6880 (time 0.2s)', '\\n', 'Epoch 2: train_loss=0.0942,\nval_loss=1.8294, train_F1=0.9705, val_F1=0.6920 (time 0.2s)', '\\n', 'Epoch 3:\ntrain_loss=0.0619, val_loss=1.9132, train_F1=0.9820, val_F1=0.6920 (time 0.2s)',\n'\\n', 'Epoch 4: train_loss=0.0700, val_loss=1.8591, train_F1=0.9805,\nval_F1=0.6940 (time 0.4s)', '\\n', 'Epoch 5: train_loss=0.0555, val_loss=1.9217,\ntrain_F1=0.9855, val_F1=0.6960 (time 0.4s)', '\\n', '\\n=== Training with\ndropout=0.3 ===', '\\n', 'Epoch 1: train_loss=0.3854, val_loss=1.4680,\ntrain_F1=0.8465, val_F1=0.6859 (time 0.3s)', '\\n', 'Epoch 2: train_loss=0.0980,\nval_loss=1.8440, train_F1=0.9710, val_F1=0.6899 (time 0.3s)', '\\n', 'Epoch 3:\ntrain_loss=0.0716, val_loss=2.0641, train_F1=0.9805, val_F1=0.6879 (time 0.4s)',\n'\\n', 'Epoch 4: train_loss=0.0721, val_loss=2.0358, train_F1=0.9785,\nval_F1=0.6899 (time 0.3s)', '\\n', 'Epoch 5: train_loss=0.0534, val_loss=2.1508,\ntrain_F1=0.9865, val_F1=0.6879 (time 0.3s)', '\\n', 'Saved experiment data to', '\n', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n===== Training\nwith nhead=2 =====', '\\n', 'nhead=2 Epoch 1: train_loss=0.3303 val_loss=1.6039\ntrain_F1=0.8834 val_F1=0.6840 time=1.2s', '\\n', 'nhead=2 Epoch 2:\ntrain_loss=0.0696 val_loss=1.8494 train_F1=0.9810 val_F1=0.6899 time=0.8s',\n'\\n', 'nhead=2 Epoch 3: train_loss=0.0455 val_loss=2.1042 train_F1=0.9910\nval_F1=0.6899 time=0.8s', '\\n', 'nhead=2 Epoch 4: train_loss=0.0592\nval_loss=1.9809 train_F1=0.9845 val_F1=0.6879 time=0.8s', '\\n', 'nhead=2 Epoch\n5: train_loss=0.0501 val_loss=2.0408 train_F1=0.9870 val_F1=0.6920 time=0.8s',\n'\\n', '\\n===== Training with nhead=4 =====', '\\n', 'nhead=4 Epoch 1:\ntrain_loss=0.3495 val_loss=1.5955 train_F1=0.8843 val_F1=0.6879 time=0.8s',\n'\\n', 'nhead=4 Epoch 2: train_loss=0.0551 val_loss=1.9660 train_F1=0.9845\nval_F1=0.6920 time=0.7s', '\\n', 'nhead=4 Epoch 3: train_loss=0.0455\nval_loss=2.0201 train_F1=0.9915 val_F1=0.6920 time=0.7s', '\\n', 'nhead=4 Epoch\n4: train_loss=0.0486 val_loss=1.9220 train_F1=0.9865 val_F1=0.6960 time=0.7s',\n'\\n', 'nhead=4 Epoch 5: train_loss=0.0517 val_loss=2.0923 train_F1=0.9855\nval_F1=0.6879 time=0.7s', '\\n', '\\n===== Training with nhead=8 =====', '\\n',\n'nhead=8 Epoch 1: train_loss=0.3440 val_loss=1.4584 train_F1=0.8809\nval_F1=0.6840 time=0.8s', '\\n', 'nhead=8 Epoch 2: train_loss=0.0777\nval_loss=1.9136 train_F1=0.9785 val_F1=0.6879 time=0.8s', '\\n', 'nhead=8 Epoch\n3: train_loss=0.0494 val_loss=1.9952 train_F1=0.9880 val_F1=0.6940 time=0.8s',\n'\\n', 'nhead=8 Epoch 4: train_loss=0.0476 val_loss=2.0001 train_F1=0.9895\nval_F1=0.6940 time=0.8s', '\\n', 'nhead=8 Epoch 5: train_loss=0.0522\nval_loss=1.9577 train_F1=0.9870 val_F1=0.6960 time=0.8s', '\\n', 'Saved\nexperiment data to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-\n17_23-44-22_conceptual_generalization_poly_rule_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time:\n14 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training\nwith d_model=64 ===', '\\n', 'd_model=64 | Epoch 1: train_loss=0.6256,\nval_loss=0.6493, train_F1=0.6890, val_F1=0.6506 (1.1s)', '\\n', 'd_model=64 |\nEpoch 2: train_loss=0.2232, val_loss=1.2567, train_F1=0.9555, val_F1=0.6799\n(0.7s)', '\\n', 'd_model=64 | Epoch 3: train_loss=0.0717, val_loss=1.5965,\ntrain_F1=0.9805, val_F1=0.6920 (0.7s)', '\\n', 'd_model=64 | Epoch 4:\ntrain_loss=0.0555, val_loss=1.7007, train_F1=0.9840, val_F1=0.6920 (0.7s)',\n'\\n', 'd_model=64 | Epoch 5: train_loss=0.0472, val_loss=1.8795,\ntrain_F1=0.9875, val_F1=0.6879 (0.7s)', '\\n', '\\n=== Training with d_model=128\n===', '\\n', 'd_model=128 | Epoch 1: train_loss=0.2858, val_loss=1.4873,\ntrain_F1=0.8915, val_F1=0.6839 (0.8s)', '\\n', 'd_model=128 | Epoch 2:\ntrain_loss=0.0649, val_loss=1.9645, train_F1=0.9805, val_F1=0.6879 (0.8s)',\n'\\n', 'd_model=128 | Epoch 3: train_loss=0.0470, val_loss=2.2001,\ntrain_F1=0.9885, val_F1=0.6879 (0.8s)', '\\n', 'd_model=128 | Epoch 4:\ntrain_loss=0.0435, val_loss=2.1868, train_F1=0.9910, val_F1=0.6879 (0.8s)',\n'\\n', 'd_model=128 | Epoch 5: train_loss=0.0447, val_loss=2.0346,\ntrain_F1=0.9895, val_F1=0.6900 (0.8s)', '\\n', '\\n=== Training with d_model=256\n===', '\\n', 'd_model=256 | Epoch 1: train_loss=0.2619, val_loss=1.9211,\ntrain_F1=0.8819, val_F1=0.6879 (1.1s)', '\\n', 'd_model=256 | Epoch 2:\ntrain_loss=0.0716, val_loss=2.3180, train_F1=0.9800, val_F1=0.6859 (1.1s)',\n'\\n', 'd_model=256 | Epoch 3: train_loss=0.0544, val_loss=2.4372,\ntrain_F1=0.9860, val_F1=0.6940 (1.1s)', '\\n', 'd_model=256 | Epoch 4:\ntrain_loss=0.0423, val_loss=2.5196, train_F1=0.9880, val_F1=0.6899 (1.1s)',\n'\\n', 'd_model=256 | Epoch 5: train_loss=0.0516, val_loss=2.3775,\ntrain_F1=0.9870, val_F1=0.6859 (1.1s)', '\\n', '\\n=== Training with d_model=384\n===', '\\n', 'd_model=384 | Epoch 1: train_loss=0.2699, val_loss=2.1986,\ntrain_F1=0.8764, val_F1=0.6820 (1.4s)', '\\n', 'd_model=384 | Epoch 2:\ntrain_loss=0.0652, val_loss=2.2939, train_F1=0.9830, val_F1=0.6940 (1.4s)',\n'\\n', 'd_model=384 | Epoch 3: train_loss=0.0590, val_loss=2.4175,\ntrain_F1=0.9840, val_F1=0.6900 (1.4s)', '\\n', 'd_model=384 | Epoch 4:\ntrain_loss=0.0514, val_loss=2.6437, train_F1=0.9895, val_F1=0.6818 (1.4s)',\n'\\n', 'd_model=384 | Epoch 5: train_loss=0.0594, val_loss=2.5905,\ntrain_F1=0.9840, val_F1=0.6960 (1.4s)', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 24 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training\nwith d_model=64 ===', '\\n', 'd_model=64 | Epoch 1: train_loss=0.6256,\nval_loss=0.6493, train_F1=0.6890, val_F1=0.6506 (0.5s)', '\\n', 'd_model=64 |\nEpoch 2: train_loss=0.2232, val_loss=1.2567, train_F1=0.9555, val_F1=0.6799\n(0.2s)', '\\n', 'd_model=64 | Epoch 3: train_loss=0.0717, val_loss=1.5965,\ntrain_F1=0.9805, val_F1=0.6920 (0.2s)', '\\n', 'd_model=64 | Epoch 4:\ntrain_loss=0.0555, val_loss=1.7007, train_F1=0.9840, val_F1=0.6920 (0.2s)',\n'\\n', 'd_model=64 | Epoch 5: train_loss=0.0472, val_loss=1.8795,\ntrain_F1=0.9875, val_F1=0.6879 (0.2s)', '\\n', '\\n=== Training with d_model=128\n===', '\\n', 'd_model=128 | Epoch 1: train_loss=0.2858, val_loss=1.4873,\ntrain_F1=0.8915, val_F1=0.6839 (0.2s)', '\\n', 'd_model=128 | Epoch 2:\ntrain_loss=0.0649, val_loss=1.9645, train_F1=0.9805, val_F1=0.6879 (0.2s)',\n'\\n', 'd_model=128 | Epoch 3: train_loss=0.0470, val_loss=2.2001,\ntrain_F1=0.9885, val_F1=0.6879 (0.2s)', '\\n', 'd_model=128 | Epoch 4:\ntrain_loss=0.0435, val_loss=2.1868, train_F1=0.9910, val_F1=0.6879 (0.2s)',\n'\\n', 'd_model=128 | Epoch 5: train_loss=0.0447, val_loss=2.0346,\ntrain_F1=0.9895, val_F1=0.6900 (0.2s)', '\\n', '\\n=== Training with d_model=256\n===', '\\n', 'd_model=256 | Epoch 1: train_loss=0.2619, val_loss=1.9211,\ntrain_F1=0.8819, val_F1=0.6879 (0.3s)', '\\n', 'd_model=256 | Epoch 2:\ntrain_loss=0.0716, val_loss=2.3180, train_F1=0.9800, val_F1=0.6859 (0.3s)',\n'\\n', 'd_model=256 | Epoch 3: train_loss=0.0544, val_loss=2.4372,\ntrain_F1=0.9860, val_F1=0.6940 (0.3s)', '\\n', 'd_model=256 | Epoch 4:\ntrain_loss=0.0423, val_loss=2.5196, train_F1=0.9880, val_F1=0.6899 (0.3s)',\n'\\n', 'd_model=256 | Epoch 5: train_loss=0.0516, val_loss=2.3775,\ntrain_F1=0.9870, val_F1=0.6859 (0.3s)', '\\n', '\\n=== Training with d_model=384\n===', '\\n', 'd_model=384 | Epoch 1: train_loss=0.2699, val_loss=2.1986,\ntrain_F1=0.8764, val_F1=0.6820 (0.5s)', '\\n', 'd_model=384 | Epoch 2:\ntrain_loss=0.0652, val_loss=2.2939, train_F1=0.9830, val_F1=0.6940 (0.5s)',\n'\\n', 'd_model=384 | Epoch 3: train_loss=0.0590, val_loss=2.4175,\ntrain_F1=0.9840, val_F1=0.6900 (0.5s)', '\\n', 'd_model=384 | Epoch 4:\ntrain_loss=0.0514, val_loss=2.6437, train_F1=0.9895, val_F1=0.6818 (0.5s)',\n'\\n', 'd_model=384 | Epoch 5: train_loss=0.0594, val_loss=2.5905,\ntrain_F1=0.9840, val_F1=0.6960 (0.5s)', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 9 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size:', ' ', '19', '\\n', 'Num labels:', ' ', '2', '\\n', '\\n=== Training\nwith d_model=64 ===', '\\n', 'd_model=64 | Epoch 1: train_loss=0.6256,\nval_loss=0.6493, train_F1=0.6890, val_F1=0.6506 (0.5s)', '\\n', 'd_model=64 |\nEpoch 2: train_loss=0.2232, val_loss=1.2567, train_F1=0.9555, val_F1=0.6799\n(0.2s)', '\\n', 'd_model=64 | Epoch 3: train_loss=0.0717, val_loss=1.5965,\ntrain_F1=0.9805, val_F1=0.6920 (0.2s)', '\\n', 'd_model=64 | Epoch 4:\ntrain_loss=0.0555, val_loss=1.7007, train_F1=0.9840, val_F1=0.6920 (0.2s)',\n'\\n', 'd_model=64 | Epoch 5: train_loss=0.0472, val_loss=1.8795,\ntrain_F1=0.9875, val_F1=0.6879 (0.2s)', '\\n', '\\n=== Training with d_model=128\n===', '\\n', 'd_model=128 | Epoch 1: train_loss=0.2858, val_loss=1.4873,\ntrain_F1=0.8915, val_F1=0.6839 (0.2s)', '\\n', 'd_model=128 | Epoch 2:\ntrain_loss=0.0649, val_loss=1.9645, train_F1=0.9805, val_F1=0.6879 (0.2s)',\n'\\n', 'd_model=128 | Epoch 3: train_loss=0.0470, val_loss=2.2001,\ntrain_F1=0.9885, val_F1=0.6879 (0.2s)', '\\n', 'd_model=128 | Epoch 4:\ntrain_loss=0.0435, val_loss=2.1868, train_F1=0.9910, val_F1=0.6879 (0.2s)',\n'\\n', 'd_model=128 | Epoch 5: train_loss=0.0447, val_loss=2.0346,\ntrain_F1=0.9895, val_F1=0.6900 (0.3s)', '\\n', '\\n=== Training with d_model=256\n===', '\\n', 'd_model=256 | Epoch 1: train_loss=0.2619, val_loss=1.9211,\ntrain_F1=0.8819, val_F1=0.6879 (0.3s)', '\\n', 'd_model=256 | Epoch 2:\ntrain_loss=0.0716, val_loss=2.3180, train_F1=0.9800, val_F1=0.6859 (0.3s)',\n'\\n', 'd_model=256 | Epoch 3: train_loss=0.0544, val_loss=2.4372,\ntrain_F1=0.9860, val_F1=0.6940 (0.3s)', '\\n', 'd_model=256 | Epoch 4:\ntrain_loss=0.0423, val_loss=2.5196, train_F1=0.9880, val_F1=0.6899 (0.4s)',\n'\\n', 'd_model=256 | Epoch 5: train_loss=0.0516, val_loss=2.3775,\ntrain_F1=0.9870, val_F1=0.6859 (0.3s)', '\\n', '\\n=== Training with d_model=384\n===', '\\n', 'd_model=384 | Epoch 1: train_loss=0.2699, val_loss=2.1986,\ntrain_F1=0.8764, val_F1=0.6820 (0.5s)', '\\n', 'd_model=384 | Epoch 2:\ntrain_loss=0.0652, val_loss=2.2939, train_F1=0.9830, val_F1=0.6940 (0.5s)',\n'\\n', 'd_model=384 | Epoch 3: train_loss=0.0590, val_loss=2.4175,\ntrain_F1=0.9840, val_F1=0.6900 (0.5s)', '\\n', 'd_model=384 | Epoch 4:\ntrain_loss=0.0514, val_loss=2.6437, train_F1=0.9895, val_F1=0.6818 (0.5s)',\n'\\n', 'd_model=384 | Epoch 5: train_loss=0.0594, val_loss=2.5905,\ntrain_F1=0.9840, val_F1=0.6960 (0.5s)', '\\n', 'Saved experiment data to', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n22_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is 30 minutes).']", ""], "analysis": ["", "", "", "", "The code executed successfully without any errors or bugs. The training process\nfor different d_model values was completed, and the results, including losses\nand F1 scores for both training and validation datasets, were logged. The\nexperiment data was saved successfully. No issues were observed in the output.", "The script executed successfully without any errors or bugs. It loaded the\ndatasets, created a vocabulary, and executed the training and validation process\nfor different numbers of transformer layers. The results were saved to a file\nfor further analysis. The training process showed consistent improvements in\ntraining F1 scores, but the validation F1 scores plateaued, indicating potential\noverfitting. No immediate bugs were detected in the implementation or execution.", "", "The training script executed successfully without any errors or bugs. The model\nwas trained with different dropout rates (0.0, 0.1, 0.2, 0.3), and the results\nfor each configuration were logged. The training and validation F1 scores, as\nwell as losses, were recorded for each epoch. The experiment data was saved\nsuccessfully, and the execution completed within the time limit. No issues were\nobserved in the script or the output.", "The execution output shows that the script ran successfully, training a\ntransformer model with varying numbers of attention heads (2, 4, 8) and saving\nthe experiment data. The model's performance was evaluated using macro F1 score\non the validation set, with results showing consistent but moderate performance\nimprovement across epochs. There are no bugs in the implementation or execution.", "The training script executed successfully without any errors or bugs. The model\nwas trained on the SPR_BENCH dataset with different d_model values, and\ntraining/validation metrics were logged for each configuration. The results were\nsaved successfully, and the execution completed within the time limit. No issues\nwere identified in the execution output.", "The training script executed successfully without any errors or bugs. The model\nwas trained with four different values of d_model (64, 128, 256, 384), and\nmetrics such as training loss, validation loss, and F1 scores were logged for\neach epoch. The results were saved to a file for further analysis. While the\nvalidation F1 scores showed some variability across hyperparameter settings, the\ntraining process behaved as expected.", "The execution of the training script completed successfully without any bugs.\nThe training was conducted for different values of d_model (64, 128, 256, 384),\nand the results were saved in the specified file. The model showed improvement\nin training metrics over the epochs, but the validation F1 scores remained\nrelatively stable, indicating possible overfitting. Further analysis and tuning\nof hyperparameters may be required to improve validation performance.", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score, used to evaluate classification performance.", "data": [{"dataset_name": "training", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "validation", "final_value": 0.694, "best_value": 0.694}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss function value, used to evaluate the error in predictions.", "data": [{"dataset_name": "training", "final_value": 0.0395, "best_value": 0.0395}, {"dataset_name": "validation", "final_value": 1.6811, "best_value": 1.6811}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "The harmonic mean of precision and recall, calculated for each label and averaged.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The error value that the model aims to minimize during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.5119, "best_value": 1.5119}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9855, "best_value": 0.9855}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.694, "best_value": 0.694}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0554, "best_value": 0.0554}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.8411, "best_value": 1.8411}]}]}, {"metric_names": [{"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score for the training dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9815, "best_value": 0.992}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score for the validation dataset, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6818, "best_value": 0.694}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value for the training dataset, indicating the error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0745, "best_value": 0.0395}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset, indicating the error during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9672, "best_value": 1.6091}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training, higher is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.9875, "best_value": 0.9875}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.991, "best_value": 0.991}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.9895, "best_value": 0.9895}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation, higher is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training, lower is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.0472, "best_value": 0.0472}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.0435, "best_value": 0.0435}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.0423, "best_value": 0.0423}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.0514, "best_value": 0.0514}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation, lower is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.6493, "best_value": 0.6493}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 1.4873, "best_value": 1.4873}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 1.9211, "best_value": 1.9211}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 2.1986, "best_value": 2.1986}]}]}, {"metric_names": [{"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score on the validation set, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "train F1 score", "lower_is_better": false, "description": "F1 score on the training set, measuring the balance between precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9865, "best_value": 0.9865}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set, measuring the error of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.887, "best_value": 1.887}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set, measuring the error of the model.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0595, "best_value": 0.0595}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The best macro F1 score achieved on the training set.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 0.9905, "best_value": 0.9905}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.9915, "best_value": 0.9915}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "weight_decay=0.001", "final_value": 0.989, "best_value": 0.989}, {"dataset_name": "weight_decay=0.01", "final_value": 0.9895, "best_value": 0.9895}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The best macro F1 score achieved on the validation set.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.696, "best_value": 0.696}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "weight_decay=0.001", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "weight_decay=0.01", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value on the training set at the final epoch.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 0.0451, "best_value": 0.0451}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.0399, "best_value": 0.0399}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.0536, "best_value": 0.0536}, {"dataset_name": "weight_decay=0.001", "final_value": 0.0593, "best_value": 0.0593}, {"dataset_name": "weight_decay=0.01", "final_value": 0.0785, "best_value": 0.0785}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value on the validation set at the final epoch.", "data": [{"dataset_name": "weight_decay=0.0", "final_value": 2.065, "best_value": 2.065}, {"dataset_name": "weight_decay=1e-05", "final_value": 2.123, "best_value": 2.123}, {"dataset_name": "weight_decay=0.0001", "final_value": 2.0169, "best_value": 2.0169}, {"dataset_name": "weight_decay=0.001", "final_value": 2.1437, "best_value": 2.1437}, {"dataset_name": "weight_decay=0.01", "final_value": 1.7828, "best_value": 1.7828}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0327, "best_value": 0.0327}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.2009, "best_value": 2.2009}]}, {"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the training dataset during the final epoch.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.994, "best_value": 0.994}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the validation dataset. 'Final value' refers to the score in the final epoch, while 'best value' is the highest score achieved.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6879, "best_value": 0.696}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9865, "best_value": 0.9865}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0486, "best_value": 0.0486}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.922, "best_value": 1.922}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during training. Higher is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.9875, "best_value": 0.9875}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.991, "best_value": 0.991}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.9895, "best_value": 0.9895}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during validation. Higher is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss achieved during training. Lower is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.0472, "best_value": 0.0472}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.0435, "best_value": 0.0435}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.0423, "best_value": 0.0423}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.0514, "best_value": 0.0514}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss achieved during validation. Lower is better.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.6493, "best_value": 0.6493}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 1.4873, "best_value": 1.4873}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 1.9211, "best_value": 1.9211}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 2.1986, "best_value": 2.1986}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during training, indicating the harmonic mean of precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.9875, "best_value": 0.9875}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.991, "best_value": 0.991}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.9895, "best_value": 0.9895}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved during validation, indicating the harmonic mean of precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value achieved during training, indicating how well the model fits the training data.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.0472, "best_value": 0.0472}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.0435, "best_value": 0.0435}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.0423, "best_value": 0.0423}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.0514, "best_value": 0.0514}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved during validation, indicating how well the model generalizes to unseen data.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.6493, "best_value": 0.6493}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 1.4873, "best_value": 1.4873}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 1.9211, "best_value": 1.9211}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 2.1986, "best_value": 2.1986}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.9875, "best_value": 0.9875}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.991, "best_value": 0.991}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.9895, "best_value": 0.9895}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.692, "best_value": 0.692}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value for the training dataset.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.0472, "best_value": 0.0472}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 0.0435, "best_value": 0.0435}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 0.0423, "best_value": 0.0423}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 0.0514, "best_value": 0.0514}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH (d_model=64)", "final_value": 0.6493, "best_value": 0.6493}, {"dataset_name": "SPR_BENCH (d_model=128)", "final_value": 1.4873, "best_value": 1.4873}, {"dataset_name": "SPR_BENCH (d_model=256)", "final_value": 1.9211, "best_value": 1.9211}, {"dataset_name": "SPR_BENCH (d_model=384)", "final_value": 2.1986, "best_value": 2.1986}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, true, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E5_loss_f1_curves.png", "../../logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E10_loss_f1_curves.png", "../../logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E15_loss_f1_curves.png", "../../logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E20_loss_f1_curves.png"], ["../../logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_valF1_vs_LR.png", "../../logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_confusion_matrix_bestLR.png"], ["../../logs/0-run/experiment_results/experiment_87c7372af7c9491b959c94fa7290dc0f_proc_3462555/SPR_BENCH_macro_f1_batch_size_tuning.png", "../../logs/0-run/experiment_results/experiment_87c7372af7c9491b959c94fa7290dc0f_proc_3462555/SPR_BENCH_loss_batch_size_tuning.png"], ["../../logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_loss_curves_nlayers.png", "../../logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_f1_curves_nlayers.png", "../../logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p0_curves.png", "../../logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_1e-05_curves.png", "../../logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p0001_curves.png", "../../logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p001_curves.png", "../../logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p01_curves.png"], ["../../logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_train_loss.png", "../../logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_val_loss.png", "../../logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_train_f1.png", "../../logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_val_f1.png", "../../logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_loss_curves_nhead.png", "../../logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_macroF1_curves_nhead.png", "../../logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_final_valF1_bar.png"], ["../../logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_final_val_f1_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_a46a93146d3a4d2883c7a85b6c481d6d/SPR_BENCH_val_f1_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_a46a93146d3a4d2883c7a85b6c481d6d/SPR_BENCH_val_loss_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_a46a93146d3a4d2883c7a85b6c481d6d/SPR_BENCH_final_val_f1_mean_se_bar.png"]], "plot_paths": [["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E5_loss_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E10_loss_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E15_loss_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E20_loss_f1_curves.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_valF1_vs_LR.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_confusion_matrix_bestLR.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_87c7372af7c9491b959c94fa7290dc0f_proc_3462555/SPR_BENCH_macro_f1_batch_size_tuning.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_87c7372af7c9491b959c94fa7290dc0f_proc_3462555/SPR_BENCH_loss_batch_size_tuning.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_loss_curves_nlayers.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_f1_curves_nlayers.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p0_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_1e-05_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p0001_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p001_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p01_curves.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_train_loss.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_val_loss.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_train_f1.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_val_f1.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_loss_curves_nhead.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_macroF1_curves_nhead.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_final_valF1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_final_val_f1_bar.png"], ["experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_a46a93146d3a4d2883c7a85b6c481d6d/SPR_BENCH_val_f1_mean_se.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_a46a93146d3a4d2883c7a85b6c481d6d/SPR_BENCH_val_loss_mean_se.png", "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_a46a93146d3a4d2883c7a85b6c481d6d/SPR_BENCH_final_val_f1_mean_se_bar.png"]], "plot_analyses": [[{"analysis": "The loss curves indicate that the training loss quickly decreases to near zero within the first two epochs, suggesting the model is able to effectively minimize the training error. However, the validation loss increases steadily over the epochs, exhibiting a clear overfitting trend. This implies the model is not generalizing well to unseen data and might be memorizing the training data instead of learning the underlying rules.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show a similar trend to the loss curves. The training Macro-F1 score quickly reaches 1, indicating perfect performance on the training set. However, the validation Macro-F1 score remains relatively low and stable, suggesting that the model struggles to generalize to the validation set. This further corroborates the overfitting issue observed in the loss curves.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well on the positive class (label 1) with 176 correct predictions and 76 errors, but struggles more with the negative class (label 0), where it makes 79 errors compared to 169 correct predictions. This imbalance in performance between the classes could indicate a bias in the model or the data distribution.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_14c0415593b342efb228ebc3472279dc_proc_3458556/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases rapidly and stabilizes at a very low value, indicating that the model learns the task well on the training set. However, the validation loss increases after the first epoch, suggesting overfitting. The train F1 score reaches near-perfect values quickly, while the validation F1 score remains significantly lower, with minimal improvement over epochs. This indicates poor generalization to the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E5_loss_f1_curves.png"}, {"analysis": "The training loss continues to decrease and stabilizes at near-zero values, showing consistent learning on the training data. The validation loss fluctuates but does not show significant improvement, indicating persistent overfitting. The train F1 score remains near-perfect, while the validation F1 score shows minor fluctuations and slight improvement, but the gap between training and validation performance remains substantial, highlighting generalization issues.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E10_loss_f1_curves.png"}, {"analysis": "The training loss remains low and stable, while the validation loss continues to fluctuate without significant improvement. This pattern suggests that the model is not learning to generalize well to the validation set. The train F1 score stays near-perfect throughout, while the validation F1 score shows minor fluctuations and remains considerably lower than the training F1 score, further emphasizing the overfitting problem.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E15_loss_f1_curves.png"}, {"analysis": "The training loss remains very low and stable, while the validation loss continues to fluctuate without improvement, reinforcing the observation of overfitting. The train F1 score stays near-perfect, but the validation F1 score shows no significant improvement and even a slight decline in later epochs. This indicates that the model's ability to generalize to unseen data does not improve with more training epochs, and overfitting persists.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_9262c5d304894c378602130ee3452406_proc_3462553/SPR_BENCH_E20_loss_f1_curves.png"}], [{"analysis": "This plot illustrates the relationship between training and validation loss across different learning rates over five epochs. At higher learning rates (e.g., 0.001), the validation loss increases significantly, indicating overfitting or instability in learning. Lower learning rates (e.g., 5e-05 and 0.0001) show a smoother convergence for both training and validation loss. However, the validation loss for these rates still remains higher than the training loss, suggesting a potential underfitting issue or the need for better regularization techniques.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot shows the Macro-F1 score for training and validation datasets across different learning rates. While the training Macro-F1 scores reach high values (close to 1.0) for most learning rates, the validation Macro-F1 scores remain consistently lower, indicating overfitting. The learning rate of 5e-05 exhibits the most stable improvement in validation Macro-F1, suggesting it might be the optimal choice for this setup.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_macroF1_curves.png"}, {"analysis": "This bar chart compares the final validation Macro-F1 scores across different learning rates. The scores are relatively close, with the learning rate of 0.0001 slightly outperforming others. This reinforces the observation that lower learning rates tend to generalize better, though the differences are not substantial.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_valF1_vs_LR.png"}, {"analysis": "The confusion matrix for the best learning rate (0.0001) reveals that the model has a moderate balance between true positives and true negatives. However, there is a noticeable number of false positives and false negatives, indicating room for improvement in classification performance. Adjusting the model's threshold or incorporating additional features might help reduce these errors.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_edaf734feb8f478bb49916bb51fc6b71_proc_3462554/SPR_BENCH_confusion_matrix_bestLR.png"}], [{"analysis": "The Macro-F1 scores for batch sizes 32 and 64 show a strong performance, with both training and validation curves converging to high values by epoch 2. Batch size 128, however, exhibits a slower convergence and overall lower Macro-F1 scores, indicating that smaller batch sizes are more effective for this task. The validation curves remain close to the training curves for batch sizes 32 and 64, suggesting good generalization. However, for batch size 128, the gap between training and validation scores is slightly larger, which could indicate underfitting or a lack of sufficient updates per epoch due to the large batch size.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_87c7372af7c9491b959c94fa7290dc0f_proc_3462555/SPR_BENCH_macro_f1_batch_size_tuning.png"}, {"analysis": "The loss curves indicate that batch sizes 32 and 64 achieve faster convergence to lower cross-entropy loss values compared to batch size 128. The validation loss for batch size 128 is consistently higher than that of the smaller batch sizes, further confirming its inferior performance. Additionally, the training and validation loss curves for batch sizes 32 and 64 remain close to each other, reflecting good generalization. In contrast, for batch size 128, the larger gap between training and validation loss suggests suboptimal learning dynamics.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_87c7372af7c9491b959c94fa7290dc0f_proc_3462555/SPR_BENCH_loss_batch_size_tuning.png"}], [{"analysis": "The first plot shows training and validation loss trends across different model configurations (d_model sizes of 64, 128, 256, and 384). For smaller d_model sizes (64 and 128), training loss decreases steadily, indicating good learning, but validation loss diverges after the first few epochs, suggesting overfitting. For larger d_model sizes (256 and 384), both training and validation losses remain lower, with reduced divergence, indicating better generalization. However, validation loss for larger models stabilizes at higher values, implying room for optimization.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot illustrates the Macro-F1 score trends for training and validation across epochs for various d_model sizes. Smaller models (64 and 128) achieve high training Macro-F1 scores quickly but exhibit poor validation performance, confirming overfitting. Larger models (256 and 384) maintain consistent training and validation Macro-F1 scores, indicating better generalization. Notably, larger models achieve near-perfect training scores by epoch 2, with validation scores stabilizing around 0.7, suggesting a trade-off between capacity and overfitting.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_f1_curves.png"}, {"analysis": "The third plot presents the final validation Macro-F1 scores for different d_model sizes. All configurations achieve similar final validation scores (~0.7), indicating that increasing model size beyond a certain point does not yield significant improvements in this metric. This suggests that factors other than model size, such as regularization or data augmentation, may be critical for further performance gains.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fd25c9ee7e7e49f89867a83b3163a8b2_proc_3462556/SPR_BENCH_final_val_f1_bar.png"}], [{"analysis": "The first plot shows the training and validation loss across different transformer depths (nl=1 to nl=6) over epochs. Training loss decreases consistently for all depths, converging to near-zero values by epoch 3, indicating that the model is successfully learning the training data. However, validation loss exhibits a diverging pattern for depths greater than 2, suggesting overfitting as the model complexity increases. Shallower models (nl=1 and nl=2) demonstrate better generalization with lower validation losses.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_loss_curves_nlayers.png"}, {"analysis": "The second plot displays the training and validation Macro-F1 scores across transformer depths. Training Macro-F1 scores approach 1.0 for all depths, confirming strong performance on the training data. However, validation Macro-F1 scores remain stagnant around 0.7 for all depths, indicating that the model struggles to generalize to unseen data. The lack of improvement in validation scores with increasing depth suggests diminishing returns in performance with deeper architectures.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_f1_curves_nlayers.png"}, {"analysis": "The third plot summarizes the final validation Macro-F1 scores across transformer depths. The scores remain largely constant at approximately 0.69, with a slight improvement for nl=6 (0.70). This reinforces the observation that increasing transformer depth does not significantly enhance generalization performance, and the slight improvement at nl=6 may not justify the added computational cost.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_01ddbe6b25da4e9f9be158f79d8e1282_proc_3462555/SPR_BENCH_final_val_f1_bar.png"}], [{"analysis": "The loss for the training set decreases steadily across epochs, indicating that the model is learning effectively. However, the validation loss increases, suggesting that the model overfits to the training data. The Macro F1 score for the training set reaches near-perfect levels, while the validation Macro F1 score remains low and almost flat, further confirming overfitting.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p0_curves.png"}, {"analysis": "Similar to the previous plot, the training loss decreases steadily, while the validation loss increases, indicating overfitting. The training Macro F1 score reaches near-perfect values, but the validation Macro F1 score shows minimal improvement and remains low, suggesting that the model's generalization capabilities are inadequate.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_1e-05_curves.png"}, {"analysis": "The training loss continues to decrease, while the validation loss increases, showing a persistent overfitting issue. The training Macro F1 score is near-perfect, whereas the validation Macro F1 score remains flat with negligible improvement, signaling poor generalization.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p0001_curves.png"}, {"analysis": "The pattern of decreasing training loss and increasing validation loss continues, highlighting overfitting. The training Macro F1 score is near-perfect, but the validation Macro F1 score shows minimal improvement, staying low and flat. This indicates that the model fails to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p001_curves.png"}, {"analysis": "The training loss decreases steadily, but the validation loss decreases slightly in the later epochs, which might suggest a slight reduction in overfitting. The training Macro F1 score remains near-perfect, while the validation Macro F1 score shows a small improvement, but it is still significantly lower than the training score, indicating limited generalization.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e878df6fb5cd401c980c43a85a44d3d4_proc_3462553/spr_wd_0p01_curves.png"}], [{"analysis": "This plot shows the training loss curves for different dropout probabilities over five epochs. All dropout configurations demonstrate a sharp decrease in loss within the first two epochs, indicating rapid learning during the initial phase of training. As training progresses, the loss values converge to very low levels, with dropout=0.0 achieving the lowest loss. This suggests that higher dropout rates slightly hinder the model's ability to minimize training loss, as expected due to the regularization effect of dropout.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_train_loss.png"}, {"analysis": "This plot displays the validation loss curves for different dropout probabilities. Unlike the training loss, the validation loss exhibits an increasing trend for all dropout configurations after the first epoch, indicating potential overfitting. The dropout=0.2 setting achieves the lowest validation loss overall, suggesting a balance between underfitting and overfitting. Dropout=0.0 leads to the highest validation loss, likely due to overfitting on the training data.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_val_loss.png"}, {"analysis": "This plot illustrates the training Macro-F1 scores for different dropout probabilities. All configurations show a rapid improvement in Macro-F1 scores during the first two epochs, reaching near-maximum scores by the third epoch. Dropout=0.0 achieves the highest training Macro-F1 score, consistent with its lowest training loss. However, the differences between configurations are relatively small, indicating that dropout has a limited impact on training performance in terms of Macro-F1.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_train_f1.png"}, {"analysis": "This plot shows the validation Macro-F1 scores for different dropout probabilities. The scores fluctuate more compared to the training Macro-F1, reflecting the model's varying generalization performance. Dropout=0.2 achieves the highest and most stable validation Macro-F1 score, while dropout=0.3 tends to underperform, likely due to excessive regularization. This highlights the importance of tuning dropout rates to optimize validation performance.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_val_f1.png"}, {"analysis": "This bar chart summarizes the final validation Macro-F1 scores for each dropout probability. Dropout=0.2 achieves the highest Macro-F1 score, followed closely by dropout=0.1 and dropout=0.0. Dropout=0.3 results in the lowest score, reinforcing the observation that excessive regularization can hinder generalization. Overall, dropout=0.2 appears to be the most effective setting for balancing regularization and performance.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_da6da122950d40d0ad6e0a0aada8dc35_proc_3462554/spr_bench_final_val_f1_bar.png"}], [{"analysis": "This plot shows the cross-entropy loss for both training and validation sets across different numbers of attention heads (nhead = 2, 4, 8). The training loss consistently decreases and stabilizes across epochs for all configurations, indicating effective learning. However, the validation loss increases with epochs for all nhead values, suggesting overfitting. This trend is more pronounced for higher nhead values, where the validation loss remains significantly higher than the training loss.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_loss_curves_nhead.png"}, {"analysis": "This plot illustrates the macro-F1 scores for training and validation sets for different nhead values. The training macro-F1 scores quickly converge to nearly 1.0, indicating excellent performance on the training set. However, the validation macro-F1 scores remain consistently around 0.7 for all nhead values, showing a significant gap between training and validation performance. This suggests that the model is overfitting to the training data and failing to generalize well to the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_macroF1_curves_nhead.png"}, {"analysis": "This bar chart presents the best validation macro-F1 scores achieved for different nhead values. All configurations (nhead = 2, 4, 8) achieve a similar maximum validation macro-F1 score of approximately 0.7. This indicates that increasing the number of attention heads does not improve the model's generalization performance on the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_574e9d63e5714758b49e4b9b9a878f7c_proc_3462556/SPR_BENCH_final_valF1_bar.png"}], [{"analysis": "This plot shows the training and validation loss trends for different model sizes (d_model = 64, 128, 256, 384). The training loss decreases significantly across all model sizes, indicating that the models are learning effectively. However, the validation loss initially decreases but starts increasing after a few epochs, particularly for larger models (256 and 384). This suggests potential overfitting, especially for larger models, as they may be memorizing the training data rather than generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the training and validation Macro-F1 scores for different model sizes. The training Macro-F1 scores quickly approach 1.0 for all model sizes, indicating excellent performance on the training set. However, the validation Macro-F1 scores remain relatively low and stable across epochs, with minimal improvement. This highlights a significant generalization gap, suggesting that the models are not effectively learning patterns that generalize well to the validation set.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart compares the final validation Macro-F1 scores across different model sizes. The scores are nearly identical across all d_model values, indicating that increasing the model size does not lead to significant improvements in validation performance. This suggests that the model's capacity is not the limiting factor for generalization in this task, and other factors, such as the training process or data representation, may need to be addressed.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/SPR_BENCH_final_val_f1_bar.png"}], [{"analysis": "The plot shows the training and validation loss trends across different model dimensions (d_model: 64, 128, 256, 384) over 5 epochs. Smaller models (e.g., d_model=64) achieve lower loss values for both training and validation compared to larger models. However, all models exhibit a consistent trend where validation loss increases after a certain epoch, suggesting possible overfitting. The training losses continue to decrease across all configurations, indicating that the models are learning effectively but may not generalize well to unseen data. The choice of cross-entropy loss as the evaluation metric aligns with the classification nature of the SPR task.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot highlights the training and validation macro-F1 scores for various model dimensions (d_model: 64, 128, 256, 384). Smaller models (e.g., d_model=64) achieve near-perfect training macro-F1 after the first epoch, but their validation macro-F1 scores remain significantly lower, indicating overfitting. Larger models show more stable validation macro-F1 scores but generally lower performance compared to their training counterparts. The macro-F1 score is an appropriate metric for this task as it ensures balanced evaluation across all classes in the SPR dataset.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_f1_curves.png"}, {"analysis": "This bar chart presents the final validation macro-F1 scores for different model dimensions (d_model: 64, 128, 256, 384). All models achieve similar final validation macro-F1 scores, approximately 0.7, despite differences in model sizes. This suggests that increasing model capacity does not necessarily improve generalization performance on the SPR task. The results highlight that model complexity might not be the limiting factor and point towards other factors, such as the need for better regularization or data augmentation, to improve performance.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/SPR_BENCH_final_val_f1_bar.png"}], [{"analysis": "The first plot shows the training and validation cross-entropy loss for different model sizes (d_model = 64, 128, 256, 384). The training loss decreases consistently for all model sizes, indicating that the models are learning from the data. However, the validation loss initially decreases and then increases for larger models (128, 256, 384), suggesting overfitting. The smallest model (d_model = 64) exhibits the most stable validation loss, indicating better generalization. This suggests that larger models may require additional regularization or adjustments to the learning rate to prevent overfitting.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_loss_curves.png"}, {"analysis": "The second plot displays the training and validation Macro-F1 scores for varying model sizes. Training Macro-F1 scores quickly reach near-perfect values for all models, but validation scores remain relatively low, particularly for the larger models. This discrepancy highlights overfitting, as the models perform well on the training data but fail to generalize to the validation set. Interestingly, the smallest model (d_model = 64) achieves slightly better validation Macro-F1 scores compared to larger models, further supporting the observation that smaller models generalize better in this setup.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_f1_curves.png"}, {"analysis": "The third plot summarizes the final validation Macro-F1 scores across different model sizes. All models achieve similar final validation Macro-F1 scores, clustering around 0.7. This uniformity suggests that increasing the model size does not significantly improve performance on the validation set, reinforcing the earlier observation that larger models are prone to overfitting without contributing to better generalization.", "plot_path": "experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/SPR_BENCH_final_val_f1_bar.png"}], []], "vlm_feedback_summary": ["The experimental results indicate significant overfitting, as evidenced by the\ndivergence between training and validation performance metrics. While the model\nachieves near-perfect training scores, its validation performance is suboptimal,\npointing to poor generalization. The confusion matrix highlights a class\nimbalance in prediction accuracy, which might need to be addressed to improve\noverall performance.", "The plots reveal a consistent pattern of overfitting, with the model achieving\nnear-perfect training performance but failing to generalize to the validation\nset. The validation loss fluctuates and does not improve, while the validation\nF1 score remains significantly lower than the training F1 score. This suggests\nthat the current approach is insufficient for achieving generalization and might\nrequire adjustments such as regularization, data augmentation, or changes to the\ntraining process.", "The analysis highlights that lower learning rates (e.g., 0.0001 and 5e-05)\nprovide better generalization, as evidenced by lower validation loss and higher\nMacro-F1 scores. However, there is a consistent gap between training and\nvalidation metrics, suggesting overfitting. The confusion matrix for the best\nlearning rate shows moderate performance but indicates opportunities for\nreducing misclassifications.", "The analysis highlights that smaller batch sizes (32 and 64) perform better in\nterms of both Macro-F1 and cross-entropy loss, with faster convergence and\nbetter generalization compared to batch size 128. Batch size 128 exhibits slower\nconvergence and signs of suboptimal learning, making it less effective for this\ntask.", "The plots provide insights into the effects of model size (d_model) on training\nand validation performance. Larger models generalize better and avoid\noverfitting but do not significantly improve final validation performance. The\nresults suggest focusing on techniques to improve generalization and reduce\noverfitting for smaller models.", "The plots indicate that while the model learns the training data effectively, it\nstruggles to generalize to validation data. Increasing transformer depth does\nnot significantly improve validation performance, and overfitting becomes\nevident for deeper models. Shallower architectures seem to generalize better,\nbut overall, the validation Macro-F1 scores plateau around 0.69, highlighting a\nneed for further optimization or architectural modifications to improve\ngeneralization.", "The plots consistently show that the model suffers from overfitting, with\ntraining loss decreasing and training Macro F1 scores reaching near-perfect\nlevels, while validation loss increases and validation Macro F1 scores remain\nlow. Adjustments to reduce overfitting, such as regularization techniques or\nearly stopping, may be necessary.", "The plots provide a comprehensive view of the model's performance across\ndifferent dropout probabilities. Dropout=0.2 emerges as the most effective\nconfiguration, achieving the best balance between training and validation\nperformance. The training loss and Macro-F1 curves highlight rapid convergence\nand strong learning, while the validation metrics emphasize the importance of\nregularization in preventing overfitting. Excessive dropout (0.3) negatively\nimpacts generalization, indicating the need for careful hyperparameter tuning.", "The results indicate that while the model achieves excellent training\nperformance, it struggles with generalization as evidenced by the consistently\nlower validation metrics. The increasing validation loss and the plateaued\nvalidation macro-F1 scores suggest overfitting. Adjustments to regularization\ntechniques or data augmentation strategies could be explored to address this\nissue.", "The analysis reveals that while the models learn effectively on the training\ndata, they struggle to generalize to the validation set, as evidenced by the\nincreasing validation loss and the stagnant validation Macro-F1 scores. Larger\nmodels appear prone to overfitting, and increasing model size does not improve\nvalidation performance. Efforts should focus on addressing the generalization\ngap through strategies like regularization, data augmentation, or modifications\nto the training process.", "The plots provide valuable insights into the performance of transformer models\non the SPR task. Smaller models show signs of overfitting, with lower validation\nperformance despite high training performance. Larger models demonstrate more\nstable validation performance but do not significantly outperform smaller\nmodels. The macro-F1 score remains consistent across model sizes, indicating\nthat increasing model capacity alone does not improve generalization. Further\nexperimentation with regularization techniques or data augmentation may be\nnecessary to enhance performance.", "The plots indicate that while all models learn effectively during training,\nlarger models tend to overfit, as evidenced by their higher validation loss and\nlower validation Macro-F1 scores. The smallest model (d_model = 64) demonstrates\nbetter generalization, achieving relatively stable validation performance.\nAdjustments such as regularization or learning rate tuning may be necessary for\nlarger models to improve their generalization capabilities.", "[]"], "exec_time": [7.024276971817017, 43.04655408859253, 9.329076766967773, 7.05710506439209, 22.244227647781372, 30.06272315979004, 9.46900749206543, 8.310306310653687, 14.894572973251343, 24.012538194656372, 9.646777391433716, 10.065891027450562, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[]"], ["[]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], ["[]"], ["[]"], ["[\"SPR_BENCH\"]"], ["[]"], ["[]"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# mandatory working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load experiment data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# -------------------- iterate over datasets -------------------\nfor dset_name, dset_data in experiment_data.items():\n    # ---------- collect per-epoch stats ----------\n    train_losses = [d[\"loss\"] for d in dset_data[\"losses\"][\"train\"]]\n    val_losses = [d[\"loss\"] for d in dset_data[\"losses\"][\"val\"]]\n    train_f1s = [d[\"macro_f1\"] for d in dset_data[\"metrics\"][\"train\"]]\n    val_f1s = [d[\"macro_f1\"] for d in dset_data[\"metrics\"][\"val\"]]\n    epochs = [d[\"epoch\"] for d in dset_data[\"metrics\"][\"train\"]]\n\n    # ---------- plot: loss curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_losses, label=\"Train Loss\")\n        plt.plot(epochs, val_losses, label=\"Validation Loss\")\n        plt.title(f\"{dset_name} Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dset_name}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset_name}: {e}\")\n        plt.close()\n\n    # ---------- plot: macro-F1 curves ----------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1s, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1s, label=\"Validation Macro-F1\")\n        plt.title(f\"{dset_name} Macro-F1 Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dset_name}_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for {dset_name}: {e}\")\n        plt.close()\n\n    # ---------- plot: confusion matrix (dev set) ----------\n    try:\n        y_true = dset_data[\"ground_truth\"]\n        y_pred = dset_data[\"predictions\"]\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure()\n        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        plt.title(\n            f\"{dset_name} Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.colorbar()\n        tick_marks = np.arange(cm.shape[0])\n        plt.xticks(tick_marks, tick_marks)\n        plt.yticks(tick_marks, tick_marks)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                f\"{cm[i, j]}\",\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"{dset_name}_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset_name}: {e}\")\n        plt.close()\n\n    # ---------- print final val macro-F1 ----------\n    if val_f1s:\n        print(f\"{dset_name} final validation Macro-F1: {val_f1s[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nsummary = {}\nif experiment_data is not None:\n    for exp_key, exp_val in experiment_data.get(\"epochs_tuning\", {}).items():\n        run = exp_val.get(\"SPR_BENCH\", {})\n        losses_tr = [x[\"loss\"] for x in run.get(\"losses\", {}).get(\"train\", [])]\n        losses_val = [x[\"loss\"] for x in run.get(\"losses\", {}).get(\"val\", [])]\n        f1_tr = [x[\"macro_f1\"] for x in run.get(\"metrics\", {}).get(\"train\", [])]\n        f1_val = [x[\"macro_f1\"] for x in run.get(\"metrics\", {}).get(\"val\", [])]\n        epochs = list(range(1, len(losses_tr) + 1))\n        # store last val F1 for summary\n        if f1_val:\n            summary[exp_key] = f1_val[-1]\n\n        try:\n            plt.figure(figsize=(10, 4))\n            # Left subplot: Loss\n            plt.subplot(1, 2, 1)\n            plt.plot(epochs, losses_tr, label=\"Train Loss\")\n            plt.plot(epochs, losses_val, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.title(\"Left: Loss\")\n\n            # Right subplot: F1\n            plt.subplot(1, 2, 2)\n            plt.plot(epochs, f1_tr, label=\"Train F1\")\n            plt.plot(epochs, f1_val, label=\"Val F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro F1\")\n            plt.legend()\n            plt.title(\"Right: Macro F1\")\n\n            plt.suptitle(\n                f\"SPR_BENCH Results for {exp_key}\\nLeft: Loss Curves, Right: F1 Curves\"\n            )\n            save_name = f\"SPR_BENCH_{exp_key}_loss_f1_curves.png\"\n            plt.savefig(os.path.join(working_dir, save_name))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating plot for {exp_key}: {e}\")\n            plt.close()\n\n# print summary table\nif summary:\n    print(\"Final Validation Macro F1 per Epoch Budget:\")\n    for k, v in sorted(\n        summary.items(), key=lambda x: int(x[0][1:])\n    ):  # sort by numeric epochs\n        print(f\"  {k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------- load data ---------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    spr_data = experiment_data[\"learning_rate\"][\"SPR_BENCH\"]\n    train_losses = spr_data[\"losses\"][\"train\"]\n    val_losses = spr_data[\"losses\"][\"val\"]\n    train_metrics = spr_data[\"metrics\"][\"train\"]\n    val_metrics = spr_data[\"metrics\"][\"val\"]\n    lrs = spr_data[\"lr_used\"]\n\n    # regroup by lr\n    def regroup(records, key):\n        out = {}\n        for d in records:\n            lr = d[\"lr\"]\n            out.setdefault(lr, {\"epoch\": [], key: []})\n            out[lr][\"epoch\"].append(d[\"epoch\"])\n            out[lr][key].append(d[key])\n        return out\n\n    tr_loss_by_lr = regroup(train_losses, \"loss\")\n    va_loss_by_lr = regroup(val_losses, \"loss\")\n    tr_f1_by_lr = regroup(train_metrics, \"macro_f1\")\n    va_f1_by_lr = regroup(val_metrics, \"macro_f1\")\n\n    # -------------------- 1) loss curves --------------------------\n    try:\n        plt.figure()\n        for lr in lrs:\n            plt.plot(\n                tr_loss_by_lr[lr][\"epoch\"],\n                tr_loss_by_lr[lr][\"loss\"],\n                label=f\"Train LR={lr}\",\n            )\n            plt.plot(\n                va_loss_by_lr[lr][\"epoch\"],\n                va_loss_by_lr[lr][\"loss\"],\n                linestyle=\"--\",\n                label=f\"Val LR={lr}\",\n            )\n        plt.title(\"SPR_BENCH: Train vs Val Loss across Learning Rates\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------------------- 2) macro-F1 curves ----------------------\n    try:\n        plt.figure()\n        for lr in lrs:\n            plt.plot(\n                tr_f1_by_lr[lr][\"epoch\"],\n                tr_f1_by_lr[lr][\"macro_f1\"],\n                label=f\"Train LR={lr}\",\n            )\n            plt.plot(\n                va_f1_by_lr[lr][\"epoch\"],\n                va_f1_by_lr[lr][\"macro_f1\"],\n                linestyle=\"--\",\n                label=f\"Val LR={lr}\",\n            )\n        plt.title(\"SPR_BENCH: Train vs Val Macro-F1 across Learning Rates\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot: {e}\")\n        plt.close()\n\n    # -------- 3) final validation macro-F1 vs LR (bar chart) -----\n    try:\n        final_f1s = []\n        for lr in lrs:\n            # last entry corresponding to this lr is at index -1 of each 5-epoch block\n            f1_vals = va_f1_by_lr[lr][\"macro_f1\"]\n            final_f1s.append(f1_vals[-1])\n        plt.figure()\n        plt.bar([str(lr) for lr in lrs], final_f1s, color=\"skyblue\")\n        plt.title(\"SPR_BENCH: Final Validation Macro-F1 vs Learning Rate\")\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Macro-F1\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_valF1_vs_LR.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar chart: {e}\")\n        plt.close()\n\n    # ---------------- 4) confusion matrix for best LR -------------\n    try:\n        best_lr = spr_data[\"best_lr\"]\n        preds_entry = next(d for d in spr_data[\"predictions\"] if d[\"lr\"] == best_lr)\n        preds = np.array(preds_entry[\"values\"])\n        gts = np.array(spr_data[\"ground_truth\"])\n        labels = np.unique(gts)\n        cm = np.zeros((labels.size, labels.size), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f\"SPR_BENCH Confusion Matrix (Best LR={best_lr})\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.xticks(labels)\n        plt.yticks(labels)\n        for i in range(len(labels)):\n            for j in range(len(labels)):\n                plt.text(\n                    j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=7\n                )\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix_bestLR.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ------------------- print evaluation metric -----------------\n    print(\n        f\"Best LR = {spr_data['best_lr']}  |  Dev Macro-F1 = {spr_data['best_val_f1']:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up paths\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(experiment_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# helper to fetch nested dict\nbench_key = (\"batch_size_tuning\", \"SPR_BENCH\")\nexp = experiment_data.get(bench_key[0], {}).get(bench_key[1], {})\n\n# print best val F1 for every batch size\nfor bs, d in exp.items():\n    val_f1s = [e[\"macro_f1\"] for e in d[\"metrics\"][\"val\"]]\n    if val_f1s:\n        print(f\"Batch size {bs}: best val macro-F1 = {max(val_f1s):.4f}\")\n\n# COLORS for consistency\nCOLORS = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n\n# ------------- Plot 1: Macro-F1 curves -------------\ntry:\n    plt.figure()\n    for idx, (bs, d) in enumerate(sorted(exp.items())):\n        epochs = [e[\"epoch\"] for e in d[\"metrics\"][\"train\"]]\n        tr_f1 = [e[\"macro_f1\"] for e in d[\"metrics\"][\"train\"]]\n        val_f1 = [e[\"macro_f1\"] for e in d[\"metrics\"][\"val\"]]\n        c = COLORS[idx % len(COLORS)]\n        plt.plot(epochs, tr_f1, color=c, label=f\"train_bs{bs}\")\n        plt.plot(epochs, val_f1, color=c, linestyle=\"--\", label=f\"val_bs{bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 vs Epoch (Batch-Size Tuning)\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_macro_f1_batch_size_tuning.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 plot: {e}\")\n    plt.close()\n\n# ------------- Plot 2: Loss curves -------------\ntry:\n    plt.figure()\n    for idx, (bs, d) in enumerate(sorted(exp.items())):\n        epochs = [e[\"epoch\"] for e in d[\"losses\"][\"train\"]]\n        tr_loss = [e[\"loss\"] for e in d[\"losses\"][\"train\"]]\n        val_loss = [e[\"loss\"] for e in d[\"losses\"][\"val\"]]\n        c = COLORS[idx % len(COLORS)]\n        plt.plot(epochs, tr_loss, color=c, label=f\"train_bs{bs}\")\n        plt.plot(epochs, val_loss, color=c, linestyle=\"--\", label=f\"val_bs{bs}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss vs Epoch (Batch-Size Tuning)\")\n    plt.legend()\n    save_path = os.path.join(working_dir, \"SPR_BENCH_loss_batch_size_tuning.png\")\n    plt.savefig(save_path)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndct = experiment_data.get(\"d_model_tuning\", {})\ndataset_name = \"SPR_BENCH\"\n\n# gather epochs and metrics\nd_models = sorted(dct.keys(), key=int)\nepochs = {}\nloss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}\nfor dm in d_models:\n    run = dct[dm][dataset_name]\n    loss_tr[dm] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[dm] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[dm] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[dm] = f1_val[dm][-1] if f1_val[dm] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], loss_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], loss_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], f1_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], f1_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val F1\ntry:\n    plt.figure()\n    xs = np.arange(len(d_models))\n    vals = [final_val_f1[dm] for dm in d_models]\n    plt.bar(xs, vals, tick_label=d_models)\n    plt.xlabel(\"d_model\")\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 per d_model\\nDataset: SPR_BENCH\")\n    save_name = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print best d_model\nif final_val_f1:\n    best_dm = max(final_val_f1, key=final_val_f1.get)\n    print(\n        f\"Best d_model by final validation Macro-F1: {best_dm} \"\n        f\"({final_val_f1[best_dm]:.4f})\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------- load experiment data -----------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# shortcut handles\nbench_key = \"nlayers_tuning\"\ndataset_key = \"SPR_BENCH\"\nruns = experiment_data.get(bench_key, {}).get(dataset_key, {})\n\n# aggregate per-run metrics\nlayer_ids, train_losses, val_losses, train_f1s, val_f1s, final_val_f1 = (\n    [],\n    [],\n    [],\n    [],\n    [],\n    [],\n)\nfor cfg_name, run_dict in runs.items():  # e.g. cfg_name = 'nlayers_3'\n    nlayers = int(cfg_name.split(\"_\")[-1])\n    layer_ids.append(nlayers)\n    tr_loss = [e[\"loss\"] for e in run_dict[\"losses\"][\"train\"]]\n    vl_loss = [e[\"loss\"] for e in run_dict[\"losses\"][\"val\"]]\n    tr_f1 = [e[\"macro_f1\"] for e in run_dict[\"metrics\"][\"train\"]]\n    vl_f1 = [e[\"macro_f1\"] for e in run_dict[\"metrics\"][\"val\"]]\n    train_losses.append(tr_loss)\n    val_losses.append(vl_loss)\n    train_f1s.append(tr_f1)\n    val_f1s.append(vl_f1)\n    final_val_f1.append(vl_f1[-1] if vl_f1 else np.nan)\n\n# sort by nlayers so plots look orderly\norder = np.argsort(layer_ids)\nlayer_ids = np.array(layer_ids)[order]\ntrain_losses = np.array(train_losses, dtype=object)[order]\nval_losses = np.array(val_losses, dtype=object)[order]\ntrain_f1s = np.array(train_f1s, dtype=object)[order]\nval_f1s = np.array(val_f1s, dtype=object)[order]\nfinal_val_f1 = np.array(final_val_f1)[order]\n\n# ----------------- plotting helpers -----------------\nepochs = np.arange(1, len(train_losses[0]) + 1) if len(train_losses) else []\n\n# 1) Loss curves\ntry:\n    plt.figure(figsize=(7, 4))\n    for nl, tr, vl in zip(layer_ids, train_losses, val_losses):\n        plt.plot(epochs, tr, \"--\", label=f\"Train nl={nl}\")\n        plt.plot(epochs, vl, \"-\", label=f\"Val nl={nl}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss across Transformer Depths\")\n    plt.legend(fontsize=7, ncol=2)\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_nlayers.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure(figsize=(7, 4))\n    for nl, tr, vl in zip(layer_ids, train_f1s, val_f1s):\n        plt.plot(epochs, tr, \"--\", label=f\"Train nl={nl}\")\n        plt.plot(epochs, vl, \"-\", label=f\"Val nl={nl}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1 across Transformer Depths\")\n    plt.legend(fontsize=7, ncol=2)\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves_nlayers.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# 3) Final validation F1 bar chart\ntry:\n    plt.figure(figsize=(6, 4))\n    plt.bar(layer_ids, final_val_f1, color=\"skyblue\")\n    plt.xlabel(\"Number of Transformer Layers\")\n    plt.ylabel(\"Final Epoch Validation Macro-F1\")\n    plt.title(\"SPR_BENCH: Final Val Macro-F1 vs Transformer Depth\")\n    for x, y in zip(layer_ids, final_val_f1):\n        plt.text(x, y + 0.01, f\"{y:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\nprint(\"Plots saved to:\", working_dir)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    wd_dict = experiment_data.get(\"weight_decay\", {})\n    best_f1, best_wd = -1.0, None\n\n    # Iterate through weight_decay runs (max 5)\n    for idx, (wd, run) in enumerate(sorted(wd_dict.items(), key=lambda x: float(x[0]))):\n        tr_loss = [pt[\"loss\"] for pt in run[\"losses\"][\"train\"]]\n        val_loss = [pt[\"loss\"] for pt in run[\"losses\"][\"val\"]]\n        tr_f1 = [pt[\"macro_f1\"] for pt in run[\"metrics\"][\"train\"]]\n        val_f1 = [pt[\"macro_f1\"] for pt in run[\"metrics\"][\"val\"]]\n        epochs = list(range(1, len(tr_loss) + 1))\n\n        # Track best\n        if val_f1[-1] > best_f1:\n            best_f1, best_wd = val_f1[-1], wd\n\n        # ----------------------------------------------------------\n        try:\n            plt.figure(figsize=(10, 4))\n\n            # Left subplot: Loss\n            plt.subplot(1, 2, 1)\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss\")\n\n            # Right subplot: Macro F1\n            plt.subplot(1, 2, 2)\n            plt.plot(epochs, tr_f1, label=\"Train F1\")\n            plt.plot(epochs, val_f1, label=\"Val F1\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro F1\")\n            plt.title(\"Macro F1\")\n\n            plt.suptitle(f\"SPR \u2013 weight_decay={wd}\\nLeft: Loss, Right: Macro F1\")\n            plt.legend()\n\n            fname = f\"spr_wd_{wd.replace('.', 'p')}_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating plot for wd={wd}: {e}\")\n            plt.close()\n\n    # --------------------------------------------------------------\n    print(\"\\nFinal validation Macro-F1 by weight_decay:\")\n    for wd, run in sorted(wd_dict.items(), key=lambda x: float(x[0])):\n        final_f1 = run[\"metrics\"][\"val\"][-1][\"macro_f1\"]\n        print(f\"  wd={wd}: {final_f1:.4f}\")\n    print(f\"\\nBest weight_decay = {best_wd}  (Val Macro-F1 = {best_f1:.4f})\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# mandatory working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    data = experiment_data[\"dropout_probability\"][\"SPR_BENCH\"]\n    dropouts = sorted(data.keys(), key=float)\n\n    # pre-collect stats\n    epochs = None\n    train_loss, val_loss = {}, {}\n    train_f1, val_f1 = {}, {}\n    final_val_f1 = {}\n\n    for dr in dropouts:\n        rec = data[dr]\n        tl = [d[\"loss\"] for d in rec[\"losses\"][\"train\"]]\n        vl = [d[\"loss\"] for d in rec[\"losses\"][\"val\"]]\n        tf = [d[\"macro_f1\"] for d in rec[\"metrics\"][\"train\"]]\n        vf = [d[\"macro_f1\"] for d in rec[\"metrics\"][\"val\"]]\n\n        train_loss[dr], val_loss[dr] = tl, vl\n        train_f1[dr], val_f1[dr] = tf, vf\n        final_val_f1[dr] = vf[-1]\n        if epochs is None:\n            epochs = [d[\"epoch\"] for d in rec[\"metrics\"][\"train\"]]\n\n    # helper to plot multiple curves\n    def multi_curve_plot(curves, ylabel, title, filename):\n        try:\n            plt.figure()\n            for dr in dropouts:\n                plt.plot(epochs, curves[dr], marker=\"o\", label=f\"dropout={dr}\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(ylabel)\n            plt.title(f\"SPR_BENCH {title}\\nLines: different dropout probabilities\")\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            save_path = os.path.join(working_dir, filename)\n            plt.savefig(save_path)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {filename}: {e}\")\n            plt.close()\n\n    # 1-4: curve plots\n    multi_curve_plot(\n        train_loss, \"Loss\", \"Training Loss Curves\", \"spr_bench_train_loss.png\"\n    )\n    multi_curve_plot(\n        val_loss, \"Loss\", \"Validation Loss Curves\", \"spr_bench_val_loss.png\"\n    )\n    multi_curve_plot(\n        train_f1, \"Macro F1\", \"Training Macro-F1 Curves\", \"spr_bench_train_f1.png\"\n    )\n    multi_curve_plot(\n        val_f1, \"Macro F1\", \"Validation Macro-F1 Curves\", \"spr_bench_val_f1.png\"\n    )\n\n    # 5: bar chart of final val F1\n    try:\n        plt.figure()\n        bars = [final_val_f1[dr] for dr in dropouts]\n        plt.bar(dropouts, bars, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.xlabel(\"Dropout Probability\")\n        plt.ylabel(\"Final Validation Macro F1\")\n        plt.title(\"SPR_BENCH Final Validation Macro-F1\\nOne bar per dropout setting\")\n        for i, v in enumerate(bars):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n        save_path = os.path.join(working_dir, \"spr_bench_final_val_f1_bar.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final F1 bar plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # print summary metrics\n    print(\"Final Validation Macro-F1 by Dropout:\")\n    for dr in dropouts:\n        print(f\"  dropout={dr}: {final_val_f1[dr]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------ load data ------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata_root = experiment_data.get(\"nhead_tuning\", {}).get(\"SPR_BENCH\", {})\n\n\n# ------------------ helper ------------------\ndef extract_curve(nhead_str, key_outer, key_inner, val_key):\n    recs = data_root[nhead_str][key_outer][key_inner]\n    return [d[\"epoch\"] for d in recs], [d[val_key] for d in recs]\n\n\n# ------------------ figure 1: loss curves ------------------\ntry:\n    plt.figure()\n    for nhead_str in sorted(data_root.keys(), key=lambda x: int(x)):\n        ep_tr, tr_loss = extract_curve(nhead_str, \"losses\", \"train\", \"loss\")\n        _, val_loss = extract_curve(nhead_str, \"losses\", \"val\", \"loss\")\n        plt.plot(ep_tr, tr_loss, label=f\"train nhead={nhead_str}\")\n        plt.plot(ep_tr, val_loss, linestyle=\"--\", label=f\"val nhead={nhead_str}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nTrain vs Validation for different nhead values\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_nhead.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves figure: {e}\")\n    plt.close()\n\n# ------------------ figure 2: macro-F1 curves ------------------\ntry:\n    plt.figure()\n    for nhead_str in sorted(data_root.keys(), key=lambda x: int(x)):\n        ep_tr, tr_f1 = extract_curve(nhead_str, \"metrics\", \"train\", \"macro_f1\")\n        _, val_f1 = extract_curve(nhead_str, \"metrics\", \"val\", \"macro_f1\")\n        plt.plot(ep_tr, tr_f1, label=f\"train nhead={nhead_str}\")\n        plt.plot(ep_tr, val_f1, linestyle=\"--\", label=f\"val nhead={nhead_str}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\n        \"SPR_BENCH Macro-F1 Curves\\nTrain vs Validation for different nhead values\"\n    )\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves_nhead.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 curves figure: {e}\")\n    plt.close()\n\n# ------------------ figure 3: final val F1 bar chart ------------------\nbest_scores = {}\ntry:\n    for nhead_str in data_root.keys():\n        _, val_f1 = extract_curve(nhead_str, \"metrics\", \"val\", \"macro_f1\")\n        best_scores[int(nhead_str)] = max(val_f1)\n    plt.figure()\n    heads = sorted(best_scores.keys())\n    scores = [best_scores[h] for h in heads]\n    plt.bar(heads, scores, color=\"skyblue\")\n    plt.xlabel(\"nhead\")\n    plt.ylabel(\"Best Val Macro-F1\")\n    plt.title(\"SPR_BENCH Final Validation Macro-F1 vs nhead\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_valF1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val F1 bar chart: {e}\")\n    plt.close()\n\n# ------------------ print evaluation summary ------------------\nif best_scores:\n    print(\"Best Validation Macro-F1 per nhead:\")\n    for h, sc in sorted(best_scores.items()):\n        print(f\"  nhead={h}: {sc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndct = experiment_data.get(\"d_model_tuning\", {})\ndataset_name = \"SPR_BENCH\"\n\n# gather epochs and metrics\nd_models = sorted(dct.keys(), key=int)\nepochs = {}\nloss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}\nfor dm in d_models:\n    run = dct[dm][dataset_name]\n    loss_tr[dm] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[dm] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[dm] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[dm] = f1_val[dm][-1] if f1_val[dm] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], loss_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], loss_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], f1_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], f1_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val F1\ntry:\n    plt.figure()\n    xs = np.arange(len(d_models))\n    vals = [final_val_f1[dm] for dm in d_models]\n    plt.bar(xs, vals, tick_label=d_models)\n    plt.xlabel(\"d_model\")\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 per d_model\\nDataset: SPR_BENCH\")\n    save_name = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print best d_model\nif final_val_f1:\n    best_dm = max(final_val_f1, key=final_val_f1.get)\n    print(\n        f\"Best d_model by final validation Macro-F1: {best_dm} \"\n        f\"({final_val_f1[best_dm]:.4f})\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndct = experiment_data.get(\"d_model_tuning\", {})\ndataset_name = \"SPR_BENCH\"\n\n# gather epochs and metrics\nd_models = sorted(dct.keys(), key=int)\nepochs = {}\nloss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}\nfor dm in d_models:\n    run = dct[dm][dataset_name]\n    loss_tr[dm] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[dm] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[dm] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[dm] = f1_val[dm][-1] if f1_val[dm] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], loss_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], loss_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], f1_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], f1_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val F1\ntry:\n    plt.figure()\n    xs = np.arange(len(d_models))\n    vals = [final_val_f1[dm] for dm in d_models]\n    plt.bar(xs, vals, tick_label=d_models)\n    plt.xlabel(\"d_model\")\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 per d_model\\nDataset: SPR_BENCH\")\n    save_name = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print best d_model\nif final_val_f1:\n    best_dm = max(final_val_f1, key=final_val_f1.get)\n    print(\n        f\"Best d_model by final validation Macro-F1: {best_dm} \"\n        f\"({final_val_f1[best_dm]:.4f})\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndct = experiment_data.get(\"d_model_tuning\", {})\ndataset_name = \"SPR_BENCH\"\n\n# gather epochs and metrics\nd_models = sorted(dct.keys(), key=int)\nepochs = {}\nloss_tr, loss_val, f1_tr, f1_val, final_val_f1 = {}, {}, {}, {}, {}\nfor dm in d_models:\n    run = dct[dm][dataset_name]\n    loss_tr[dm] = [x[\"loss\"] for x in run[\"losses\"][\"train\"]]\n    loss_val[dm] = [x[\"loss\"] for x in run[\"losses\"][\"val\"]]\n    f1_tr[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"train\"]]\n    f1_val[dm] = [x[\"macro_f1\"] for x in run[\"metrics\"][\"val\"]]\n    epochs[dm] = [x[\"epoch\"] for x in run[\"metrics\"][\"train\"]]\n    final_val_f1[dm] = f1_val[dm][-1] if f1_val[dm] else 0.0\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], loss_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], loss_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training vs Validation Loss\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    for dm in d_models:\n        plt.plot(epochs[dm], f1_tr[dm], label=f\"{dm}-train\", linestyle=\"--\")\n        plt.plot(epochs[dm], f1_val[dm], label=f\"{dm}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Training vs Validation Macro-F1\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curves: {e}\")\n    plt.close()\n\n# 3) Bar chart of final val F1\ntry:\n    plt.figure()\n    xs = np.arange(len(d_models))\n    vals = [final_val_f1[dm] for dm in d_models]\n    plt.bar(xs, vals, tick_label=d_models)\n    plt.xlabel(\"d_model\")\n    plt.ylabel(\"Final Val Macro-F1\")\n    plt.title(\"Final Validation Macro-F1 per d_model\\nDataset: SPR_BENCH\")\n    save_name = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_bar.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# Print best d_model\nif final_val_f1:\n    best_dm = max(final_val_f1, key=final_val_f1.get)\n    print(\n        f\"Best d_model by final validation Macro-F1: {best_dm} \"\n        f\"({final_val_f1[best_dm]:.4f})\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom collections import defaultdict\n\n# ------------------------------------------------------------\nimport matplotlib\n\nmatplotlib.use(\"Agg\")  # in case we are on a head-less machine\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# 1) Load every experiment_data.npy that is listed\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fde43d6cf81461e9b957d61b8627901_proc_3462555/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ec6b0cc0444648c0972a6bf53c1379fb_proc_3462553/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-22_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d84eec3083be415caa30b7232c125db5_proc_3462554/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor rel_path in experiment_data_path_list:\n    try:\n        abs_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), rel_path)\n        data = np.load(abs_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"[WARN] Could not load {rel_path}: {e}\")\n\nif not all_experiment_data:\n    print(\"No experiment data could be loaded; aborting plotting.\")\n    quit()\n\n# ------------------------------------------------------------\ndataset_name = \"SPR_BENCH\"\nsection_name = \"d_model_tuning\"\n\n# Aggregation containers: dict[d_model] -> dict[epoch] -> list(values_across_runs)\nmetric_collectors = {\n    \"val_f1\": defaultdict(lambda: defaultdict(list)),\n    \"val_loss\": defaultdict(lambda: defaultdict(list)),\n}\n\nfinal_val_f1 = defaultdict(list)  # d_model -> list of final f1 for each run\n\nfor exp in all_experiment_data:\n    section = exp.get(section_name, {})\n    for d_model, ds_dict in section.items():\n        if dataset_name not in ds_dict:\n            continue\n        run = ds_dict[dataset_name]\n        val_metrics = run[\"metrics\"][\"val\"]\n        val_losses = run[\"losses\"][\"val\"]\n        # assume len(val_metrics) == len(val_losses)\n        for m, l in zip(val_metrics, val_losses):\n            epoch = m[\"epoch\"]\n            metric_collectors[\"val_f1\"][d_model][epoch].append(m[\"macro_f1\"])\n            metric_collectors[\"val_loss\"][d_model][epoch].append(l[\"loss\"])\n        # final value for bar chart\n        if val_metrics:\n            final_val_f1[d_model].append(val_metrics[-1][\"macro_f1\"])\n\n\n# helper to compute mean and stderr arrays per d_model\ndef build_curve(data_dict):\n    epochs_sorted = sorted({e for d in data_dict.values() for e in d.keys()})\n    curve = {}\n    for d_model, epoch_dict in data_dict.items():\n        xs, means, errs = [], [], []\n        for e in epochs_sorted:\n            vals = epoch_dict.get(e, [])\n            if not vals:\n                continue\n            xs.append(e)\n            arr = np.asarray(vals, dtype=float)\n            means.append(arr.mean())\n            errs.append(arr.std(ddof=1) / np.sqrt(len(arr)) if len(arr) > 1 else 0.0)\n        curve[d_model] = (np.asarray(xs), np.asarray(means), np.asarray(errs))\n    return curve\n\n\ncurves_f1 = build_curve(metric_collectors[\"val_f1\"])\ncurves_loss = build_curve(metric_collectors[\"val_loss\"])\n\n# ------------------------------------------------------------\n# 2) Plotting\n# NOTE: each figure in its own try/except\n\n# (a) Validation Macro-F1 curves (mean \u00b1 SE)\ntry:\n    plt.figure()\n    for d_model, (xs, m, se) in curves_f1.items():\n        if len(xs) > 200:  # subsample for readability\n            idx = np.linspace(0, len(xs) - 1, 200, dtype=int)\n            xs, m, se = xs[idx], m[idx], se[idx]\n        plt.plot(xs, m, label=f\"{d_model} mean\")\n        plt.fill_between(xs, m - se, m + se, alpha=0.3, label=f\"{d_model} \u00b1SE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Macro-F1\")\n    plt.title(\"Validation Macro-F1 (mean \u00b1 SE)\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_val_f1_mean_se.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated F1 curves: {e}\")\n    plt.close()\n\n# (b) Validation Loss curves (mean \u00b1 SE)\ntry:\n    plt.figure()\n    for d_model, (xs, m, se) in curves_loss.items():\n        if len(xs) > 200:\n            idx = np.linspace(0, len(xs) - 1, 200, dtype=int)\n            xs, m, se = xs[idx], m[idx], se[idx]\n        plt.plot(xs, m, label=f\"{d_model} mean\")\n        plt.fill_between(xs, m - se, m + se, alpha=0.3, label=f\"{d_model} \u00b1SE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Loss\")\n    plt.title(\"Validation Loss (mean \u00b1 SE)\\nDataset: SPR_BENCH\")\n    plt.legend()\n    save_name = os.path.join(working_dir, \"SPR_BENCH_val_loss_mean_se.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curves: {e}\")\n    plt.close()\n\n# (c) Bar chart of final validation Macro-F1\ntry:\n    plt.figure()\n    d_models = sorted(final_val_f1.keys(), key=int)\n    means = [np.mean(final_val_f1[d]) for d in d_models]\n    ses = [\n        (\n            np.std(final_val_f1[d], ddof=1) / np.sqrt(len(final_val_f1[d]))\n            if len(final_val_f1[d]) > 1\n            else 0.0\n        )\n        for d in d_models\n    ]\n    xs = np.arange(len(d_models))\n    plt.bar(xs, means, yerr=ses, capsize=4)\n    plt.xticks(xs, d_models)\n    plt.xlabel(\"d_model\")\n    plt.ylabel(\"Final Validation Macro-F1 (mean \u00b1 SE)\")\n    plt.title(\"Final Val Macro-F1 per d_model\\nDataset: SPR_BENCH\")\n    save_name = os.path.join(working_dir, \"SPR_BENCH_final_val_f1_mean_se_bar.png\")\n    plt.savefig(save_name)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# 3) Print numeric table\nprint(\"\\nAggregated Final Validation Macro-F1 (mean \u00b1 SE) for SPR_BENCH\")\nfor d_model in sorted(final_val_f1.keys(), key=int):\n    arr = np.asarray(final_val_f1[d_model], dtype=float)\n    mean = arr.mean()\n    se = arr.std(ddof=1) / np.sqrt(len(arr)) if len(arr) > 1 else 0.0\n    print(f\"d_model {d_model:>4}: {mean:.4f} \u00b1 {se:.4f}\")\n"], "plot_plan": ["We will load the saved experiment_data.npy, extract per-epoch losses and macro-F1 scores, and create two time-series plots: one for training/validation loss and another for training/validation macro-F1. Using the final epoch\u2019s predictions and ground-truth labels, we will also plot a confusion matrix to visualise class-wise performance. Each figure is generated inside its own try-except block, saved to the working directory with descriptive filenames that contain the dataset name and plot type, and the figure is always closed afterward to free resources. We limit ourselves to these three plots to keep the output concise and under the five-figure cap. The code begins with the required imports, ensures the working directory exists, and never invents data\u2014everything comes directly from experiment_data.npy. After plotting, the script prints the final validation macro-F1 so users can quickly see the headline metric. All matplotlib functions use default styles as requested, and titles/subtitles clearly indicate the content and dataset. Finally, every plot is saved as a PNG file in working_dir for easy retrieval.", "We will load the saved experiment_data.npy, loop over the four epoch-budget experiments, and for each extract the per-epoch training/validation loss and macro-F1. For every budget we create one figure with two subplots\u2014left: loss curves, right: F1 curves\u2014so we generate four figures total (\u22645 as required). Each figure is protected by its own try-except, includes a descriptive title plus a subtitle clarifying which curve is on which side, and is saved in working_dir with the dataset name and epoch count in the filename. After plotting, we compute and print a small summary table of the final validation macro-F1 achieved by each budget so the user can quickly see which setting worked best. All figures are closed in finally/except blocks to avoid memory leaks, and the script uses only data actually present in experiment_data.npy with basic matplotlib defaults.", "We load experiment_data.npy, isolate the \u201clearning_rate \u2192 SPR_BENCH\u201d section, and regroup the flat lists of dictionaries into per-LR arrays for losses and macro-F1. Two line plots are produced: one for train/val losses and one for train/val macro-F1, each coloured by learning rate and sharing the epoch axis. We then compute the final validation macro-F1 for every LR (last epoch of each sweep) and show these in a bar chart. Using the stored best_lr field we gather its prediction vector and the common ground-truth vector, build a confusion matrix, and visualise it with imshow. Each figure is wrapped in a try-except block, titled, saved in working_dir with descriptive names, and closed immediately. Only data already present in experiment_data.npy are accessed\u2014no synthetic values are introduced\u2014and at most four figures are generated. Finally, the script prints the best learning rate together with its corresponding validation macro-F1 so that the main evaluation metric is visible in the console.", "We will first load experiment_data.npy from the working directory, then iterate over the three batch sizes used during tuning. For each batch size we will extract the list of epochs, training macro-F1, validation macro-F1, training loss, and validation loss that were recorded. We will compute and print the best validation macro-F1 obtained for every batch size so the user can immediately see which setting performed best. Next, we will create two summary figures: (1) \u201cSPR_BENCH Macro-F1 vs Epoch (Batch-Size Tuning)\u201d showing both train and validation curves for all batch sizes, and (2) \u201cSPR_BENCH Loss vs Epoch (Batch-Size Tuning)\u201d showing the corresponding loss curves. Each plot is wrapped in its own try-except block, contains clear legends and titles, and is saved to working_dir with a descriptive filename; figures are always closed afterward. No more than these two figures are produced, satisfying the \u22645-figure requirement while covering both key metrics. Finally, any I/O or plotting error is caught and reported without stopping the script.", "We will load the saved experiment_data.npy, extract the metrics recorded for each d_model run, and assemble three concise visualizations.  First, we will draw one figure with the training- and validation-loss curves for every d_model, so users can see convergence behaviour.  Second, we will do the same for macro-F1, giving insight into generalisation.  Third, we will build a bar chart of the final\u2010epoch validation macro-F1 for each d_model, making hyper-parameter comparison easy.  Optionally, if predictions/labels exist, we compute the macro-F1 again to double-check and print the best d_model at the end (no extra plot, so we keep \u22645 figures).  Each figure is created inside its own try-except, titled clearly, saved to working_dir with descriptive names, and the figure is always closed.  No data are fabricated; everything comes directly from experiment_data.npy.  After plotting, we print the d_model that achieved the highest validation macro-F1.", "We will first load experiment_data.npy to retrieve, for each Transformer layer count, the per-epoch train/validation loss and macro-F1 that were stored during the sweep.  \nNext, we will build three concise visualizations (\u22645 total as required):  \n1) a single figure with all train/val loss curves across epochs and across nlayers;  \n2) a similar figure with all macro-F1 curves;  \n3) a bar chart that summarizes the final-epoch validation macro-F1 for every layer count, so we can quickly spot the best model depth.  \nEach figure is wrapped in its own try-except block, saved to working_dir with descriptive names, titled clearly, and closed afterwards.  \nWe rely only on the information present in experiment_data.npy\u2014no synthetic data.  \nPlots are generated at the natural epoch granularity (\u22645 epochs), keeping visual clutter minimal.  \nFinally, the script reports the location of saved plots so users know where to look.", "We will load the saved experiment_data.npy, iterate over the five weight-decay settings, and extract epoch-wise training/validation loss and macro-F1. For each weight-decay value (at most five, so we stay within the limit) we create one figure with two sub-plots: left shows training vs. validation loss; right shows training vs. validation macro-F1. Each figure is wrapped in a try-except block, titled clearly (e.g. \u201cSPR \u2013 wd=1e-4   Left: Loss, Right: Macro F1\u201d) and saved to the working directory with descriptive names. After plotting we close every figure to avoid memory leaks. Finally, we compute the final validation macro-F1 for every run, print them, and highlight the best weight-decay choice. All code follows the required import order and directory handling rules.", "We will load the stored experiment_data.npy, extract the per-epoch training/validation loss and macro-F1 entries for every tested dropout probability on the SPR_BENCH dataset, and aggregate them into simple Python lists. Five figures will be produced: (1) training loss curves, (2) validation loss curves, (3) training macro-F1 curves, (4) validation macro-F1 curves, and (5) a bar chart comparing the final validation macro-F1 achieved by each dropout setting. Each figure is created inside an independent try-except block to guard against missing data, given a clear title and subtitle, saved to the working directory with descriptive names, and then closed. No more than five plots are generated, satisfying the guideline. After plotting, we print the final validation macro-F1 for every dropout to the console so the user can immediately see the key evaluation metric. All plotting uses standard matplotlib without extra styles, and every figure uses legends and axis labels for clarity. The entire procedure is self-contained, starts with the required imports, and will gracefully handle the absence of the experiment file by emitting an informative message.", "The solution loads experiment_data.npy, navigates to the \u201cnhead_tuning \u2192 SPR_BENCH\u201d dictionary, and for every recorded n-head value extracts epoch-wise training/validation loss and macro-F1.  Three separate figures are produced: (1) combined training/validation loss curves, (2) combined training/validation macro-F1 curves, and (3) a bar chart summarising the final validation macro-F1 for each n-head.  Each figure is wrapped in its own try-except block to prevent a single failure from halting the rest of the plotting routine, and every figure is saved into the required working directory with descriptive names that include the dataset and plot type.  Only the data present in the NumPy file are plotted\u2014no synthetic values are introduced\u2014and at most three plots are generated, meeting the five-figure cap.  Titles explicitly mention \u201cSPR_BENCH\u201d while subtitles clarify what each curve represents (e.g., \u201cTrain vs Validation\u201d).  After plotting, figures are always closed to free memory.  Finally, the script prints a concise table of the best validation macro-F1 obtained for each n-head configuration, serving as an additional evaluation metric.  The code is compact, self-contained, and adheres strictly to basic matplotlib usage without custom styling.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "EPOCHS", "learning_rate", "BATCH_SIZE", "d_model (Transformer hidden/embedding size)", "nlayers (number_of_transformer_encoder_layers)", "weight_decay", "dropout_probability", "nhead", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["We will load the saved NumPy file from the working directory, convert the object\narray back to a Python dict, and iterate over each dataset contained in it. For\nevery dataset we collect the lists of recorded losses and macro-F1 scores,\ndetermine the best (highest) F1 and the lowest loss for both training and\nvalidation phases, and then print these four key values with explicit metric\nnames. The script runs immediately when executed and respects the required code-\nlayout rules.", "The script will load the saved NumPy file from the working directory, walk\nthrough every hyper-parameter run contained in the \u201cepochs_tuning\u201d dictionary,\nand aggregate all metric values for each dataset (here, \u201cSPR_BENCH\u201d). For every\nmetric (train macro-F1, validation macro-F1, train loss, validation loss) it\nwill keep the best value across all epochs and runs (max for F1 scores, min for\nlosses) and then print them with explicit names. No plots are produced and\neverything executes at the global level.", "The script loads the saved NumPy file, retrieves the learning-rate sweep\nresults, and focuses on the dataset(s) inside the dictionary (here: SPR_BENCH).\nFor each dataset it first identifies the best learning rate recorded during\ntraining, then searches the stored lists to fetch the final epoch\u2019s\ntrain/validation loss and train/validation macro-F1 that correspond to this best\nlearning rate.   Finally, it prints the dataset name followed by clearly\nlabelled metric values (best learning rate, train macro-F1, validation macro-F1,\ntrain loss, validation loss).", "Below is a small script that immediately loads the saved NumPy file, walks\nthrough its structure, and prints the best (highest macro-F1) and best (lowest)\nloss obtained for every batch-size setting inside each recorded dataset. It\nhonours the required naming convention for every metric printed.", "The script will load the saved numpy dictionary from the working directory,\niterate over every d_model run inside the \u201cd_model_tuning\u201d branch, and for each\nrun access the single dataset entry \u201cSPR_BENCH.\u201d   For every run it scans the\nstored metric/loss history, selects the best (highest) macro-F1 scores and the\nbest (lowest) cross-entropy losses, and prints them with fully qualified names\nsuch as \u201cbest training macro F1 score\u201d or \u201clowest validation loss.\u201d   All code\nsits at global scope so that it executes immediately when the file is run, with\nno reliance on an `if __name__ == \"__main__\":` guard.", "The script will load the saved numpy file from the working directory, walk\nthrough the nested dictionary to find every dataset (here only \u201cSPR_BENCH\u201d),\nand, for that dataset, scan all saved runs with different numbers of encoder\nlayers.   For every epoch in every run it will track the highest validation\nmacro-F1 score, remembering the corresponding training macro-F1, training loss,\nvalidation loss, and the nlayers setting that achieved it.   Finally, it prints\nthe dataset name followed by clearly-labelled metric values for that best\ncheckpoint.", "Below is a concise script that loads the saved NumPy archive, iterates over\nevery weight-decay experiment, and prints the best (highest) macro-F1 as well as\nthe final-epoch loss for both the training and validation splits. Each block of\nresults is clearly prefixed with the corresponding \u201cdataset\u201d name (here: the\nspecific weight-decay value).", "The script will load the saved numpy file from the working directory, walk\nthrough the nested structure (hyper-parameter \u2192 dataset \u2192 record), and for every\ndataset print its name once followed by the most relevant metrics.  For each\ndropout setting it reports the final (last-epoch) train loss, validation loss,\ntrain macro-F1, and validation macro-F1; additionally, it highlights the best\nvalidation macro-F1 obtained across epochs because that is often the metric of\ninterest.  All code executes at the global scope so the file runs immediately\nwhen executed, with no plots or special entry point.", "The script loads the saved experiment results, drills down to the `nhead_tuning`\nsection, and for every dataset (only \u201cSPR_BENCH\u201d in this case) finds the single\nrun/epoch that achieved the highest validation macro-F1.  It then prints that\ndataset\u2019s name once, followed by the best-run hyper-parameter value (`nhead`)\nand the corresponding final metrics: training macro-F1 score, validation\nmacro-F1 score, training loss, and validation loss.  All code executes\nimmediately at import time and nothing is hidden behind a special entry point.", "The script will load the saved numpy dictionary from the working directory,\niterate over every d_model run inside the \u201cd_model_tuning\u201d branch, and for each\nrun access the single dataset entry \u201cSPR_BENCH.\u201d   For every run it scans the\nstored metric/loss history, selects the best (highest) macro-F1 scores and the\nbest (lowest) cross-entropy losses, and prints them with fully qualified names\nsuch as \u201cbest training macro F1 score\u201d or \u201clowest validation loss.\u201d   All code\nsits at global scope so that it executes immediately when the file is run, with\nno reliance on an `if __name__ == \"__main__\":` guard.", "The script will load the saved numpy dictionary from the working directory,\niterate over every d_model run inside the \u201cd_model_tuning\u201d branch, and for each\nrun access the single dataset entry \u201cSPR_BENCH.\u201d   For every run it scans the\nstored metric/loss history, selects the best (highest) macro-F1 scores and the\nbest (lowest) cross-entropy losses, and prints them with fully qualified names\nsuch as \u201cbest training macro F1 score\u201d or \u201clowest validation loss.\u201d   All code\nsits at global scope so that it executes immediately when the file is run, with\nno reliance on an `if __name__ == \"__main__\":` guard.", "The script will load the saved numpy dictionary from the working directory,\niterate over every d_model run inside the \u201cd_model_tuning\u201d branch, and for each\nrun access the single dataset entry \u201cSPR_BENCH.\u201d   For every run it scans the\nstored metric/loss history, selects the best (highest) macro-F1 scores and the\nbest (lowest) cross-entropy losses, and prints them with fully qualified names\nsuch as \u201cbest training macro F1 score\u201d or \u201clowest validation loss.\u201d   All code\nsits at global scope so that it executes immediately when the file is run, with\nno reliance on an `if __name__ == \"__main__\":` guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------------------------------\n# 0. Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -------------------------------------------------------------------------\n# 1. Extract and report metrics\nfor dataset_name, details in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # --- F1 scores -------------------------------------------------------\n    train_f1_scores = [entry[\"macro_f1\"] for entry in details[\"metrics\"][\"train\"]]\n    val_f1_scores = [entry[\"macro_f1\"] for entry in details[\"metrics\"][\"val\"]]\n\n    if train_f1_scores:  # safety check\n        best_train_f1 = max(train_f1_scores)\n        print(f\"Best training macro F1 score: {best_train_f1:.4f}\")\n\n    if val_f1_scores:\n        best_val_f1 = max(val_f1_scores)\n        print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # --- Losses ----------------------------------------------------------\n    train_losses = [entry[\"loss\"] for entry in details[\"losses\"][\"train\"]]\n    val_losses = [entry[\"loss\"] for entry in details[\"losses\"][\"val\"]]\n\n    if train_losses:\n        lowest_train_loss = min(train_losses)\n        print(f\"Lowest training loss: {lowest_train_loss:.4f}\")\n\n    if val_losses:\n        lowest_val_loss = min(val_losses)\n        print(f\"Lowest validation loss: {lowest_val_loss:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# locate and load the saved experiment results\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# container to hold best values per dataset\nbest_metrics = {}\n\n# iterate over the different epoch-budget experiments\nfor run_key, run_val in experiment_data.get(\"epochs_tuning\", {}).items():\n    for dataset_name, result_dict in run_val.items():  # e.g. \"SPR_BENCH\"\n        # ensure an entry exists\n        if dataset_name not in best_metrics:\n            best_metrics[dataset_name] = {\n                \"best train macro F1 score\": -1.0,\n                \"best validation macro F1 score\": -1.0,\n                \"best train loss\": np.inf,\n                \"best validation loss\": np.inf,\n            }\n\n        # pull out per-epoch lists\n        train_metrics = result_dict[\"metrics\"][\"train\"]  # list of dicts\n        val_metrics = result_dict[\"metrics\"][\"val\"]\n        train_losses = result_dict[\"losses\"][\"train\"]\n        val_losses = result_dict[\"losses\"][\"val\"]\n\n        # update best F1 scores\n        for m in train_metrics:\n            best_metrics[dataset_name][\"best train macro F1 score\"] = max(\n                best_metrics[dataset_name][\"best train macro F1 score\"], m[\"macro_f1\"]\n            )\n        for m in val_metrics:\n            best_metrics[dataset_name][\"best validation macro F1 score\"] = max(\n                best_metrics[dataset_name][\"best validation macro F1 score\"],\n                m[\"macro_f1\"],\n            )\n\n        # update best (minimum) losses\n        for l in train_losses:\n            best_metrics[dataset_name][\"best train loss\"] = min(\n                best_metrics[dataset_name][\"best train loss\"], l[\"loss\"]\n            )\n        for l in val_losses:\n            best_metrics[dataset_name][\"best validation loss\"] = min(\n                best_metrics[dataset_name][\"best validation loss\"], l[\"loss\"]\n            )\n\n# ---------------------------------------------------------------------\n# pretty-print the collected best metrics\nfor dset, metrics in best_metrics.items():\n    print(dset)\n    for k, v in metrics.items():\n        print(f\"{k}: {v:.4f}\")\n    print(\"\")  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Iterate over datasets contained in the sweep\nsweep_key = \"learning_rate\"\nfor dataset_name, ds_data in experiment_data[sweep_key].items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # best learning rate and best validation F1 (already stored)\n    best_lr = ds_data[\"best_lr\"]\n    best_val_f1 = ds_data[\"best_val_f1\"]\n\n    # helper: get last epoch recorded for this lr\n    max_epoch = max(item[\"epoch\"] for item in ds_data[\"metrics\"][\"train\"])\n\n    # fetch final-epoch train macro-F1 for best_lr\n    train_macro_f1 = next(\n        item[\"macro_f1\"]\n        for item in ds_data[\"metrics\"][\"train\"]\n        if item[\"lr\"] == best_lr and item[\"epoch\"] == max_epoch\n    )\n\n    # fetch final-epoch validation macro-F1 for best_lr\n    val_macro_f1 = next(\n        item[\"macro_f1\"]\n        for item in ds_data[\"metrics\"][\"val\"]\n        if item[\"lr\"] == best_lr and item[\"epoch\"] == max_epoch\n    )\n\n    # fetch final-epoch train loss for best_lr\n    train_loss = next(\n        item[\"loss\"]\n        for item in ds_data[\"losses\"][\"train\"]\n        if item[\"lr\"] == best_lr and item[\"epoch\"] == max_epoch\n    )\n\n    # fetch final-epoch validation loss for best_lr\n    val_loss = next(\n        item[\"loss\"]\n        for item in ds_data[\"losses\"][\"val\"]\n        if item[\"lr\"] == best_lr and item[\"epoch\"] == max_epoch\n    )\n\n    # ------------------------------------------------------------------\n    # 2) Print metrics with explicit labels\n    print(f\"best learning rate: {best_lr}\")\n    print(f\"train macro F1 score: {train_macro_f1:.4f}\")\n    print(\n        f\"validation macro F1 score: {val_macro_f1:.4f}  (best recorded: {best_val_f1:.4f})\"\n    )\n    print(f\"train loss: {train_loss:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Traverse the stored results and report the best/final metrics\nroot_key = \"batch_size_tuning\"  # top-level key used in original script\n\nfor dataset_name, bs_dict in experiment_data[root_key].items():\n    print(f\"\\nDataset: {dataset_name}\")  # always print dataset name first\n\n    for batch_size, run_dict in bs_dict.items():\n        print(f\"  Batch size {batch_size}:\")\n\n        # ----- F1 scores -----\n        train_f1_values = [e[\"macro_f1\"] for e in run_dict[\"metrics\"][\"train\"]]\n        val_f1_values = [e[\"macro_f1\"] for e in run_dict[\"metrics\"][\"val\"]]\n\n        best_train_f1 = max(train_f1_values)\n        best_val_f1 = max(val_f1_values)\n\n        # ----- Losses -----\n        train_loss_values = [e[\"loss\"] for e in run_dict[\"losses\"][\"train\"]]\n        val_loss_values = [e[\"loss\"] for e in run_dict[\"losses\"][\"val\"]]\n\n        lowest_train_loss = min(train_loss_values)\n        lowest_val_loss = min(val_loss_values)\n\n        # final epoch values (sometimes useful to display as \u201cfinal\u201d)\n        final_train_f1 = train_f1_values[-1]\n        final_val_f1 = val_f1_values[-1]\n        final_train_loss = train_loss_values[-1]\n        final_val_loss = val_loss_values[-1]\n\n        # ----- Printing (explicit metric names) -----\n        print(f\"    best train F1 score: {best_train_f1:.4f}\")\n        print(f\"    best validation F1 score: {best_val_f1:.4f}\")\n        print(f\"    lowest train loss: {lowest_train_loss:.4f}\")\n        print(f\"    lowest validation loss: {lowest_val_loss:.4f}\")\n        print(f\"    final train F1 score: {final_train_f1:.4f}\")\n        print(f\"    final validation F1 score: {final_val_f1:.4f}\")\n        print(f\"    final train loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Traverse every hyper-parameter run and print the requested stats\ndmodel_runs = experiment_data.get(\"d_model_tuning\", {})\nfor d_model_str, run_dict in dmodel_runs.items():\n    for dataset_name, ds in run_dict.items():  # here only \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name} (d_model={d_model_str})\")\n\n        # --- F1 scores -------------------------------------------------------\n        train_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"train\"]]\n        val_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"val\"]]\n\n        best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n        best_val_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n\n        # --- Losses ----------------------------------------------------------\n        train_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"train\"]]\n        val_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"val\"]]\n\n        best_train_loss = (\n            min(train_loss_history) if train_loss_history else float(\"nan\")\n        )\n        best_val_loss = min(val_loss_history) if val_loss_history else float(\"nan\")\n\n        # --- Print nicely ----------------------------------------------------\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n        print(f\"lowest training loss: {best_train_loss:.4f}\")\n        print(f\"lowest validation loss: {best_val_loss:.4f}\")\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------\n# 0. Locate the file in the prescribed working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# 1. Load the experiment data (stored with allow_pickle=True)\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------\ndef print_best_metrics(exp_dict):\n    \"\"\"\n    exp_dict structure:\n    {\n        'nlayers_tuning': {\n            '<DATASET_NAME>': {\n                 'nlayers_<n>': run_dict, ...\n            }\n        }\n    }\n    \"\"\"\n    for dataset_name, dataset_runs in exp_dict.get(\"nlayers_tuning\", {}).items():\n\n        # variables to track the best validation F1 score across all runs/epochs\n        best_val_f1 = -1.0\n        best_train_f1 = None\n        best_val_loss = None\n        best_train_loss = None\n        best_nlayers_conf = None\n        best_epoch_idx = None\n\n        for conf_name, run_dict in dataset_runs.items():\n\n            train_metrics = run_dict[\"metrics\"][\"train\"]  # list of dicts per epoch\n            val_metrics = run_dict[\"metrics\"][\"val\"]\n            train_losses = run_dict[\"losses\"][\"train\"]\n            val_losses = run_dict[\"losses\"][\"val\"]\n\n            for idx, val_epoch in enumerate(val_metrics):\n                val_f1 = val_epoch[\"macro_f1\"]\n\n                if val_f1 > best_val_f1:\n                    best_val_f1 = val_f1\n                    best_train_f1 = train_metrics[idx][\"macro_f1\"]\n                    best_val_loss = val_losses[idx][\"loss\"]\n                    best_train_loss = train_losses[idx][\"loss\"]\n                    best_nlayers_conf = conf_name\n                    best_epoch_idx = val_epoch[\"epoch\"]\n\n        # 3. Print dataset name\n        print(f\"Dataset: {dataset_name}\")\n\n        # 4 & 5. Print clearly-labelled metrics (best values only)\n        print(f\"  best validation F1 score: {best_val_f1:.4f}\")\n        print(f\"  corresponding train F1 score: {best_train_f1:.4f}\")\n        print(f\"  corresponding validation loss: {best_val_loss:.4f}\")\n        print(f\"  corresponding train loss: {best_train_loss:.4f}\")\n        print(f\"  achieved with setting: {best_nlayers_conf} at epoch {best_epoch_idx}\")\n        print()  # blank line for readability\n\n\n# -----------------------------------------------------------\nprint_best_metrics(experiment_data)\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0) Locate working directory and load the saved experiment dictionary\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ---------------------------------------------------------------------\n# 1) Iterate through every weight-decay run and report metrics\n# ---------------------------------------------------------------------\nfor wd_key, run_record in experiment_data[\"weight_decay\"].items():\n    print(f\"Dataset: weight_decay={wd_key}\")\n\n    # -------- best / final metric extraction --------\n    # Best (highest) macro-F1\n    best_train_f1 = max(item[\"macro_f1\"] for item in run_record[\"metrics\"][\"train\"])\n    best_val_f1 = max(item[\"macro_f1\"] for item in run_record[\"metrics\"][\"val\"])\n\n    # Final-epoch loss (last entry)\n    final_train_loss = run_record[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = run_record[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # -------- print with explicit labels --------\n    print(f\"train macro F1 score (best): {best_train_f1:.4f}\")\n    print(f\"validation macro F1 score (best): {best_val_f1:.4f}\")\n    print(f\"train loss (final epoch): {final_train_loss:.4f}\")\n    print(f\"validation loss (final epoch): {final_val_loss:.4f}\")\n    print()  # blank line for readability\n", "import os\nimport numpy as np\n\n\n# ------------------------------------------------------------------\n# Helper functions\ndef get_final(metric_list, key):\n    \"\"\"\n    Return the value of `key` from the last element of a list of\n    dicts like [{'epoch': 1, key: val}, ...]\n    \"\"\"\n    if not metric_list:\n        return None\n    return metric_list[-1][key]\n\n\ndef get_best(metric_list, key, higher_is_better=True):\n    \"\"\"\n    Return the best value of `key` across the list according\n    to whether higher or lower is better.\n    \"\"\"\n    if not metric_list:\n        return None\n    best_fn = max if higher_is_better else min\n    return best_fn(item[key] for item in metric_list)\n\n\n# ------------------------------------------------------------------\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate through the data hierarchy and print metrics\nfor hp_group, datasets in experiment_data.items():  # 'dropout_probability'\n    for ds_name, hp_records in datasets.items():  # 'SPR_BENCH'\n        print(f\"\\nDataset: {ds_name}\")\n        for hp_value, record in hp_records.items():  # e.g. dropout '0.1'\n            print(f\"  {hp_group.replace('_', ' ').capitalize()}: {hp_value}\")\n\n            # Final metrics\n            final_train_loss = get_final(record[\"losses\"][\"train\"], \"loss\")\n            final_val_loss = get_final(record[\"losses\"][\"val\"], \"loss\")\n            final_train_f1 = get_final(record[\"metrics\"][\"train\"], \"macro_f1\")\n            final_val_f1 = get_final(record[\"metrics\"][\"val\"], \"macro_f1\")\n\n            # Best validation macro-F1 across epochs\n            best_val_f1 = get_best(\n                record[\"metrics\"][\"val\"], \"macro_f1\", higher_is_better=True\n            )\n\n            # Print with explicit metric names\n            print(f\"    train loss (final epoch): {final_train_loss:.4f}\")\n            print(f\"    validation loss (final epoch): {final_val_loss:.4f}\")\n            print(f\"    train macro F1 score (final epoch): {final_train_f1:.4f}\")\n            print(f\"    validation macro F1 score (final epoch): {final_val_f1:.4f}\")\n            print(f\"    validation macro F1 score (best): {best_val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 1) Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 2) Traverse data, find best epoch/run for each dataset\ntuning_block = experiment_data.get(\"nhead_tuning\", {})\n\nfor dataset_name, runs in tuning_block.items():\n    # Track best result across all hyper-parameter settings\n    best_record = None\n    best_val_f1 = -1.0\n    best_nhead = None\n    best_epoch_idx = None\n\n    for nhead_value, run_record in runs.items():\n        # Iterate over epochs to pick the epoch with highest val F1 for this nhead\n        for idx, ep_entry in enumerate(run_record[\"metrics\"][\"val\"]):\n            val_f1 = ep_entry[\"macro_f1\"]\n            if val_f1 > best_val_f1:\n                best_val_f1 = val_f1\n                best_record = run_record\n                best_nhead = nhead_value\n                best_epoch_idx = idx  # zero-based\n\n    # Fetch the metrics/losses for the chosen epoch\n    train_f1 = best_record[\"metrics\"][\"train\"][best_epoch_idx][\"macro_f1\"]\n    val_f1 = best_val_f1\n    train_loss = best_record[\"losses\"][\"train\"][best_epoch_idx][\"loss\"]\n    val_loss = best_record[\"losses\"][\"val\"][best_epoch_idx][\"loss\"]\n\n    # ------------------------------------------------------------------\n    # 3) Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"best nhead value: {best_nhead}\")\n    print(f\"training macro F1 score: {train_f1:.4f}\")\n    print(f\"validation macro F1 score: {val_f1:.4f}\")\n    print(f\"training loss: {train_loss:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Traverse every hyper-parameter run and print the requested stats\ndmodel_runs = experiment_data.get(\"d_model_tuning\", {})\nfor d_model_str, run_dict in dmodel_runs.items():\n    for dataset_name, ds in run_dict.items():  # here only \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name} (d_model={d_model_str})\")\n\n        # --- F1 scores -------------------------------------------------------\n        train_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"train\"]]\n        val_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"val\"]]\n\n        best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n        best_val_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n\n        # --- Losses ----------------------------------------------------------\n        train_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"train\"]]\n        val_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"val\"]]\n\n        best_train_loss = (\n            min(train_loss_history) if train_loss_history else float(\"nan\")\n        )\n        best_val_loss = min(val_loss_history) if val_loss_history else float(\"nan\")\n\n        # --- Print nicely ----------------------------------------------------\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n        print(f\"lowest training loss: {best_train_loss:.4f}\")\n        print(f\"lowest validation loss: {best_val_loss:.4f}\")\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Traverse every hyper-parameter run and print the requested stats\ndmodel_runs = experiment_data.get(\"d_model_tuning\", {})\nfor d_model_str, run_dict in dmodel_runs.items():\n    for dataset_name, ds in run_dict.items():  # here only \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name} (d_model={d_model_str})\")\n\n        # --- F1 scores -------------------------------------------------------\n        train_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"train\"]]\n        val_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"val\"]]\n\n        best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n        best_val_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n\n        # --- Losses ----------------------------------------------------------\n        train_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"train\"]]\n        val_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"val\"]]\n\n        best_train_loss = (\n            min(train_loss_history) if train_loss_history else float(\"nan\")\n        )\n        best_val_loss = min(val_loss_history) if val_loss_history else float(\"nan\")\n\n        # --- Print nicely ----------------------------------------------------\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n        print(f\"lowest training loss: {best_train_loss:.4f}\")\n        print(f\"lowest validation loss: {best_val_loss:.4f}\")\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0) Locate and load the experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 1) Traverse every hyper-parameter run and print the requested stats\ndmodel_runs = experiment_data.get(\"d_model_tuning\", {})\nfor d_model_str, run_dict in dmodel_runs.items():\n    for dataset_name, ds in run_dict.items():  # here only \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name} (d_model={d_model_str})\")\n\n        # --- F1 scores -------------------------------------------------------\n        train_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"train\"]]\n        val_f1_history = [entry[\"macro_f1\"] for entry in ds[\"metrics\"][\"val\"]]\n\n        best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n        best_val_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n\n        # --- Losses ----------------------------------------------------------\n        train_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"train\"]]\n        val_loss_history = [entry[\"loss\"] for entry in ds[\"losses\"][\"val\"]]\n\n        best_train_loss = (\n            min(train_loss_history) if train_loss_history else float(\"nan\")\n        )\n        best_val_loss = min(val_loss_history) if val_loss_history else float(\"nan\")\n\n        # --- Print nicely ----------------------------------------------------\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n        print(f\"lowest training loss: {best_train_loss:.4f}\")\n        print(f\"lowest validation loss: {best_val_loss:.4f}\")\n        print()  # blank line for readability\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', 'Best training macro F1 score: 0.9920', '\\n', 'Best\nvalidation macro F1 score: 0.6940', '\\n', 'Lowest training loss: 0.0395', '\\n',\n'Lowest validation loss: 1.6811', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best train macro F1 score: 0.9955', '\\n', 'best validation\nmacro F1 score: 0.6960', '\\n', 'best train loss: 0.0241', '\\n', 'best validation\nloss: 1.5119', '\\n', '', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'best learning rate: 0.0001', '\\n', 'train macro\nF1 score: 0.9855', '\\n', 'validation macro F1 score: 0.6940  (best recorded:\n0.6940)', '\\n', 'train loss: 0.0554', '\\n', 'validation loss: 1.8411', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Batch size 32:', '\\n', '    best train F1\nscore: 0.9920', '\\n', '    best validation F1 score: 0.6940', '\\n', '    lowest\ntrain loss: 0.0395', '\\n', '    lowest validation loss: 1.9672', '\\n', '\nfinal train F1 score: 0.9815', '\\n', '    final validation F1 score: 0.6818',\n'\\n', '    final train loss: 0.0745', '\\n', '    final validation loss: 1.9672',\n'\\n', '  Batch size 64:', '\\n', '    best train F1 score: 0.9890', '\\n', '\nbest validation F1 score: 0.6940', '\\n', '    lowest train loss: 0.0418', '\\n',\n'    lowest validation loss: 1.6091', '\\n', '    final train F1 score: 0.9890',\n'\\n', '    final validation F1 score: 0.6940', '\\n', '    final train loss:\n0.0418', '\\n', '    final validation loss: 2.0657', '\\n', '  Batch size 128:',\n'\\n', '    best train F1 score: 0.9875', '\\n', '    best validation F1 score:\n0.6920', '\\n', '    lowest train loss: 0.0473', '\\n', '    lowest validation\nloss: 0.7774', '\\n', '    final train F1 score: 0.9875', '\\n', '    final\nvalidation F1 score: 0.6899', '\\n', '    final train loss: 0.0473', '\\n', '\nfinal validation loss: 2.0659', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH (d_model=64)', '\\n', 'best training macro F1 score:\n0.9875', '\\n', 'best validation macro F1 score: 0.6920', '\\n', 'lowest training\nloss: 0.0472', '\\n', 'lowest validation loss: 0.6493', '\\n', '\\n', 'Dataset:\nSPR_BENCH (d_model=128)', '\\n', 'best training macro F1 score: 0.9910', '\\n',\n'best validation macro F1 score: 0.6900', '\\n', 'lowest training loss: 0.0435',\n'\\n', 'lowest validation loss: 1.4873', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=256)', '\\n', 'best training macro F1 score: 0.9880', '\\n', 'best\nvalidation macro F1 score: 0.6940', '\\n', 'lowest training loss: 0.0423', '\\n',\n'lowest validation loss: 1.9211', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=384)', '\\n', 'best training macro F1 score: 0.9895', '\\n', 'best\nvalidation macro F1 score: 0.6960', '\\n', 'lowest training loss: 0.0514', '\\n',\n'lowest validation loss: 2.1986', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  best validation F1 score: 0.6960', '\\n', '\ncorresponding train F1 score: 0.9865', '\\n', '  corresponding validation loss:\n1.8870', '\\n', '  corresponding train loss: 0.0595', '\\n', '  achieved with\nsetting: nlayers_2 at epoch 2', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: weight_decay=0.0', '\\n', 'train macro F1 score (best): 0.9905', '\\n',\n'validation macro F1 score (best): 0.6940', '\\n', 'train loss (final epoch):\n0.0451', '\\n', 'validation loss (final epoch): 2.0650', '\\n', '\\n', 'Dataset:\nweight_decay=1e-05', '\\n', 'train macro F1 score (best): 0.9915', '\\n',\n'validation macro F1 score (best): 0.6960', '\\n', 'train loss (final epoch):\n0.0399', '\\n', 'validation loss (final epoch): 2.1230', '\\n', '\\n', 'Dataset:\nweight_decay=0.0001', '\\n', 'train macro F1 score (best): 0.9885', '\\n',\n'validation macro F1 score (best): 0.6940', '\\n', 'train loss (final epoch):\n0.0536', '\\n', 'validation loss (final epoch): 2.0169', '\\n', '\\n', 'Dataset:\nweight_decay=0.001', '\\n', 'train macro F1 score (best): 0.9890', '\\n',\n'validation macro F1 score (best): 0.6920', '\\n', 'train loss (final epoch):\n0.0593', '\\n', 'validation loss (final epoch): 2.1437', '\\n', '\\n', 'Dataset:\nweight_decay=0.01', '\\n', 'train macro F1 score (best): 0.9895', '\\n',\n'validation macro F1 score (best): 0.6960', '\\n', 'train loss (final epoch):\n0.0785', '\\n', 'validation loss (final epoch): 1.7828', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Dropout probability: 0.0', '\\n', '    train\nloss (final epoch): 0.0327', '\\n', '    validation loss (final epoch): 2.2009',\n'\\n', '    train macro F1 score (final epoch): 0.9940', '\\n', '    validation\nmacro F1 score (final epoch): 0.6920', '\\n', '    validation macro F1 score\n(best): 0.6940', '\\n', '  Dropout probability: 0.1', '\\n', '    train loss\n(final epoch): 0.0361', '\\n', '    validation loss (final epoch): 2.0791', '\\n',\n'    train macro F1 score (final epoch): 0.9930', '\\n', '    validation macro F1\nscore (final epoch): 0.6940', '\\n', '    validation macro F1 score (best):\n0.6960', '\\n', '  Dropout probability: 0.2', '\\n', '    train loss (final\nepoch): 0.0555', '\\n', '    validation loss (final epoch): 1.9217', '\\n', '\ntrain macro F1 score (final epoch): 0.9855', '\\n', '    validation macro F1\nscore (final epoch): 0.6960', '\\n', '    validation macro F1 score (best):\n0.6960', '\\n', '  Dropout probability: 0.3', '\\n', '    train loss (final\nepoch): 0.0534', '\\n', '    validation loss (final epoch): 2.1508', '\\n', '\ntrain macro F1 score (final epoch): 0.9865', '\\n', '    validation macro F1\nscore (final epoch): 0.6879', '\\n', '    validation macro F1 score (best):\n0.6899', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'best nhead value: 4', '\\n', 'training macro F1\nscore: 0.9865', '\\n', 'validation macro F1 score: 0.6960', '\\n', 'training loss:\n0.0486', '\\n', 'validation loss: 1.9220', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH (d_model=64)', '\\n', 'best training macro F1 score:\n0.9875', '\\n', 'best validation macro F1 score: 0.6920', '\\n', 'lowest training\nloss: 0.0472', '\\n', 'lowest validation loss: 0.6493', '\\n', '\\n', 'Dataset:\nSPR_BENCH (d_model=128)', '\\n', 'best training macro F1 score: 0.9910', '\\n',\n'best validation macro F1 score: 0.6900', '\\n', 'lowest training loss: 0.0435',\n'\\n', 'lowest validation loss: 1.4873', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=256)', '\\n', 'best training macro F1 score: 0.9880', '\\n', 'best\nvalidation macro F1 score: 0.6940', '\\n', 'lowest training loss: 0.0423', '\\n',\n'lowest validation loss: 1.9211', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=384)', '\\n', 'best training macro F1 score: 0.9895', '\\n', 'best\nvalidation macro F1 score: 0.6960', '\\n', 'lowest training loss: 0.0514', '\\n',\n'lowest validation loss: 2.1986', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH (d_model=64)', '\\n', 'best training macro F1 score:\n0.9875', '\\n', 'best validation macro F1 score: 0.6920', '\\n', 'lowest training\nloss: 0.0472', '\\n', 'lowest validation loss: 0.6493', '\\n', '\\n', 'Dataset:\nSPR_BENCH (d_model=128)', '\\n', 'best training macro F1 score: 0.9910', '\\n',\n'best validation macro F1 score: 0.6900', '\\n', 'lowest training loss: 0.0435',\n'\\n', 'lowest validation loss: 1.4873', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=256)', '\\n', 'best training macro F1 score: 0.9880', '\\n', 'best\nvalidation macro F1 score: 0.6940', '\\n', 'lowest training loss: 0.0423', '\\n',\n'lowest validation loss: 1.9211', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=384)', '\\n', 'best training macro F1 score: 0.9895', '\\n', 'best\nvalidation macro F1 score: 0.6960', '\\n', 'lowest training loss: 0.0514', '\\n',\n'lowest validation loss: 2.1986', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['Dataset: SPR_BENCH (d_model=64)', '\\n', 'best training macro F1 score:\n0.9875', '\\n', 'best validation macro F1 score: 0.6920', '\\n', 'lowest training\nloss: 0.0472', '\\n', 'lowest validation loss: 0.6493', '\\n', '\\n', 'Dataset:\nSPR_BENCH (d_model=128)', '\\n', 'best training macro F1 score: 0.9910', '\\n',\n'best validation macro F1 score: 0.6900', '\\n', 'lowest training loss: 0.0435',\n'\\n', 'lowest validation loss: 1.4873', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=256)', '\\n', 'best training macro F1 score: 0.9880', '\\n', 'best\nvalidation macro F1 score: 0.6940', '\\n', 'lowest training loss: 0.0423', '\\n',\n'lowest validation loss: 1.9211', '\\n', '\\n', 'Dataset: SPR_BENCH\n(d_model=384)', '\\n', 'best training macro F1 score: 0.9895', '\\n', 'best\nvalidation macro F1 score: 0.6960', '\\n', 'lowest training loss: 0.0514', '\\n',\n'lowest validation loss: 2.1986', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}