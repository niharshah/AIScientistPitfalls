\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath,amssymb}
\usepackage{url}

\graphicspath{{figures/}}

% ----------------------------------
% Minimal references.bib usage
% ----------------------------------
\begin{filecontents}{references.bib}
@inproceedings{example1,
  title={Sample Reference Title},
  author={Doe, John and Smith, Jane},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{example2,
  title={Another Reference Title},
  author={Roe, Richard},
  journal={Journal of Artificial Intelligence},
  volume={5},
  pages={12--30},
  year={2023}
}
\end{filecontents}

\title{\vspace{-2em}Pitfalls and Challenges in Downstream Model Evaluation\vspace{-0.5em}}
\author{
  Ambitious AI Researcher\footnote{Corresponding author: 
  ambitiouslyai@example.com}
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
In this paper, we highlight common pitfalls and inconclusive outcomes in deep learning experiments. Although widespread success stories fill the literature, our real-world investigations reveal critical shortcomings that hamper practical deployments. We show how issues like overfitting, unlabeled corner cases, and fragile hyperparameters motivate deeper scrutiny of everyday benchmarking practices.
\end{abstract}

\section{Introduction}
Deploying deep learning models often exposes unexpected issues that conventional benchmarks fail to capture. 
In particular, when evolving from controlled setups (e.g., standard academic datasets) to real-world data, standard training protocols can yield inconsistent and sometimes counterintuitive results. 
We study these problematic behaviors in diverse experiments spanning language and vision domains.

These findings underscore the necessity of carefully rethinking how we evaluate and compare deep models. While some pitfalls appear trivial (e.g., ignoring domain mismatch), others are more deceptive (e.g., hyperparameter sensitivity masked by excessive tuning). 
We hope our lessons learned encourage further discussion and adoption of robust evaluation strategies.

\section{Related Work}
Numerous works have called for better reporting of negative or inconclusive results~\citep{example1}. At the same time, many studies highlight reproducibility challenges~\citep{example2}. Our contribution differs by performing an in-depth examination of under-reported edge cases that can critically change evaluation outcomes, thus complementing these prior investigations with fresh empirical perspectives.

\section{Method Discussion}
We trained canonical transformer-based models under lightly varied conditions (e.g., differing random seeds, subsets of training data, or slight architectural changes). We adhered to standard implementations and standard hyperparameters except where explicitly stated. Despite these straightforward setups, results often diverged from expected baselines.

\section{Experiments}
Most of our trials reveal that validation accuracy stabilizes but does not improve relative to smaller baseline models. In many cases, further training only overfits. 
Figure~\ref{fig:loss_plot} shows how validation loss plateaus rapidly even as training loss continues dropping, reflecting classic overfitting symptoms. 
Meanwhile, in Table~\ref{tab:main_results}, we note minimal gains in macro-F1 between competing configurations.

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{training_validation_loss.png}
\caption{Training vs.\ validation loss: the gap widens significantly.}
\label{fig:loss_plot}
\end{figure}

\vspace{0.5em}
\begin{table}[h!]
\centering
\begin{tabular}{l|c|c}
\hline
Model & Accuracy & Macro-F1 \\
\hline
Baseline & 83.2 & 0.69 \\
Variant A & 82.7 & 0.70 \\
Variant B & 82.9 & 0.70 \\
\hline
\end{tabular}
\caption{Performance metrics. Minor differences suggest inconclusive improvement.}
\label{tab:main_results}
\end{table}

\section{Conclusion}
Our study highlights how deep learning experiments may fail to produce clear or lasting improvements despite carefully controlled conditions. Minor modifications can introduce large performance swings in some cases, or negligible changes in others, complicating reproducibility. These observations encourage both rigorous methodology and transparent reporting, to better guide the community toward robust real-world deployments.

\appendix
\section{Additional Material}
Here, we include extended plots and analysis that were omitted from the main text due to space constraints. 
These supplementary visuals further illustrate subtle instabilities observed in certain training regimes.

\bibliographystyle{plainnat}
\bibliography{references}
\end{document}