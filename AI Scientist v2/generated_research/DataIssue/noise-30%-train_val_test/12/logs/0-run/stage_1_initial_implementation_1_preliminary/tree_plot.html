<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[2, 4], [2, 7], [2, 6], [2, 5]], "layout": [[0.0, 0.0], [0.2857142857142857, 0.0], [0.5714285714285714, 0.0], [0.8571428571428571, 0.0], [0.14285714285714285, 1.0], [0.42857142857142855, 1.0], [0.7142857142857143, 1.0], [1.0, 1.0]], "plan": ["We start with a lightweight character-level baseline: each symbol sequence is\nsplit into individual characters, mapped to a learned embedding, enriched with\nsinusoidal positional encodings and passed through a two-layer Transformer\nencoder. The mean of the encoder hidden states is fed to a linear layer for\nlabel prediction. Vocabulary, padding and batching are handled in a custom\ncollate_fn so everything can be pushed to GPU efficiently. We train for a\nhandful of epochs with cross-entropy on the training split, monitor loss and\nMacro-F1 on the dev split each epoch, and finally evaluate on the held-out test\nset. All losses, metrics, predictions and ground-truth labels are stored in the\nrequired experiment_data structure and saved to ./working for later analysis.\nThe whole script is single-file, self-contained, honours the specified device\nhandling rules and finishes within minutes on a modern GPU.", "We start with a minimal but end-to-end baseline: load SPR_BENCH via the provided\nutility, build a character-level vocabulary from the training split (plus\nPAD/UNK), and map each distinct label to an index.  Each sequence is encoded to\nintegers and dynamically padded inside a DataLoader.  The model is a lightweight\nTransformer encoder (2 layers, 4 heads) with learned positional embeddings\nfollowed by mean pooling and a linear classifier; this already captures long-\nrange symbolic dependencies.  We train for a few epochs with Adam, monitoring\nvalidation loss and Macro-F1 after every epoch.  All tensors and the model are\nmoved to GPU when available, and optimizer creation happens after the model is\non the correct device.  During training we collect losses and metrics into the\nprescribed experiment_data dict, then save it as a .npy file; we also save a\nsimple loss/metric plot for later inspection.  This script gives a solid,\nreproducible baseline that can later be augmented with explicit symbolic\nreasoning modules.  It complies with all execution, saving, and evaluation\nrequirements.", "We start with a very lightweight character-level baseline.  After loading the\nSPR_BENCH splits with the provided helper, we build a vocabulary from every\ncharacter that appears in the training \u201csequence\u201d field and reserve PAD/UNK ids.\nEach sequence is converted to an integer tensor, truncated/padded to a fixed\nlength (max 128), and an attention-mask is created.  A tiny Transformer encoder\n(2 layers, 4 heads, d_model = 128) with mean-pooling feeds a linear classifier\nwhose output dimension equals the number of labels discovered in the data.  We\ntrain for a few epochs with cross-entropy, track validation loss and Macro-F1\nafter every epoch, store all metrics/predictions in the mandatory\nexperiment_data structure, and plot loss/F1 curves.  All tensors, the model, and\nthe optimizer are moved to GPU when available, respecting the critical device\nguidelines.  Finally, metrics and plots are saved to the \u201cworking\u201d directory and\nthe final Macro-F1 on the dev split is printed.  This code forms a solid,\nreproducible baseline which we can later extend with symbolic-reasoning\ncomponents.", "A simple but solid baseline is to treat each SPR sequence as a sequence of\ndiscrete symbols, build a character-level vocabulary from the training split,\nand train a small Transformer encoder for multi-class classification. The data\npipeline loads the benchmark (or fabricates a synthetic set if the files are\nabsent), tokenises each sequence into characters, encodes them to integer ids,\nand feeds batches with dynamic padding. A 2-layer TransformerEncoder (64-d\nembedding, 4 heads) processes the padded batches; its mean-pooled output is\npassed through a linear layer to predict the hidden rule label. We train with\ncross-entropy for a handful of epochs, monitor validation loss and macro-F1 each\nepoch, and finally report test macro-F1. All metrics, losses, predictions and\nground-truths are stored in experiment_data and saved to disk; a learning-curve\nplot is also produced. The script follows the mandatory GPU handling, DataLoader\ndevice transfer, and file-saving conventions so it can run immediately on either\nCPU or GPU.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, math, time, random\nimport numpy as np\nfrom typing import Dict, List\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ------------------------------------------------------------------\n# compulsory working dir and experiment data dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n# ------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------- Dataset loading helper --------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",  # treat csv as one split\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# ---------------- Char-level vocabulary ---------------------------------------\nclass CharVocab:\n    def __init__(self, sequences: List[str]):\n        chars = set(\"\".join(sequences))\n        self.pad_token = \"<PAD>\"\n        self.unk_token = \"<UNK>\"\n        self.idx2c = [self.pad_token, self.unk_token] + sorted(chars)\n        self.c2idx = {c: i for i, c in enumerate(self.idx2c)}\n\n    def encode(self, s: str) -> List[int]:\n        return [self.c2idx.get(c, self.c2idx[self.unk_token]) for c in s]\n\n    def __len__(self):\n        return len(self.idx2c)\n\n\n# ---------------- PyTorch dataset wrapper -------------------------------------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_split, vocab: CharVocab, label2id: Dict[str, int]):\n        self.split = hf_split\n        self.vocab = vocab\n        self.label2id = label2id\n\n    def __len__(self):\n        return len(self.split)\n\n    def __getitem__(self, idx):\n        row = self.split[int(idx)]\n        seq_encoded = torch.tensor(self.vocab.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(self.label2id[row[\"label\"]], dtype=torch.long)\n        return {\"input_ids\": seq_encoded, \"label\": label}\n\n\ndef collate_fn(batch):\n    # pad to max length in batch\n    lens = [len(item[\"input_ids\"]) for item in batch]\n    max_len = max(lens)\n    padded = []\n    for item in batch:\n        ids = item[\"input_ids\"]\n        pad_len = max_len - len(ids)\n        padded_ids = torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)])\n        padded.append(padded_ids)\n    input_ids = torch.stack(padded)\n    labels = torch.stack([item[\"label\"] for item in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels, \"lengths\": torch.tensor(lens)}\n\n\n# ---------------- Model -------------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1), :]\n        return x\n\n\nclass CharTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        num_layers=2,\n        dim_feedforward=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.positional = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, lengths):\n        x = self.embedding(input_ids)\n        x = self.positional(x)\n        mask = input_ids == 0  # padding mask (batch, seq_len)\n        h = self.transformer(x, src_key_padding_mask=mask)\n        # mean pooling excluding pads\n        lengths = lengths.unsqueeze(1).to(h.dtype)\n        sum_h = torch.sum(h * (~mask).unsqueeze(-1), dim=1)\n        mean_h = sum_h / lengths\n        return self.classifier(mean_h)\n\n\n# ---------------- Training & evaluation utilities -----------------------------\ndef run_epoch(model, loader, optimizer=None):\n    is_train = optimizer is not None\n    if is_train:\n        model.train()\n    else:\n        model.eval()\n\n    total_loss, preds, gts = 0.0, [], []\n    criterion = nn.CrossEntropyLoss()\n\n    for batch in loader:\n        # move tensors\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"lengths\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if is_train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(torch.argmax(logits, dim=1).cpu().tolist())\n        gts.extend(batch[\"labels\"].cpu().tolist())\n\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ---------------- Main experiment pipeline ------------------------------------\ndef main():\n    # 1. Load dataset\n    DATA_PATH = pathlib.Path(\"./SPR_BENCH\")\n    dset = load_spr_bench(DATA_PATH)\n    # 2. Build vocab and label mapping\n    train_seqs = dset[\"train\"][\"sequence\"]\n    vocab = CharVocab(train_seqs)\n\n    labels = sorted(set(dset[\"train\"][\"label\"]))\n    label2id = {l: i for i, l in enumerate(labels)}\n\n    # 3. Torch datasets & loaders\n    train_ds = SPRTorchDataset(dset[\"train\"], vocab, label2id)\n    dev_ds = SPRTorchDataset(dset[\"dev\"], vocab, label2id)\n    test_ds = SPRTorchDataset(dset[\"test\"], vocab, label2id)\n\n    train_loader = DataLoader(\n        train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn\n    )\n    dev_loader = DataLoader(\n        dev_ds, batch_size=256, shuffle=False, collate_fn=collate_fn\n    )\n    test_loader = DataLoader(\n        test_ds, batch_size=256, shuffle=False, collate_fn=collate_fn\n    )\n\n    # 4. Model\n    model = CharTransformerClassifier(len(vocab), len(labels)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # 5. Training loop\n    num_epochs = 5\n    for epoch in range(1, num_epochs + 1):\n        train_loss, train_f1, _, _ = run_epoch(model, train_loader, optimizer)\n        val_loss, val_f1, _, _ = run_epoch(model, dev_loader, optimizer=None)\n\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_macroF1 = {val_f1:.4f}\"\n        )\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(train_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n\n    # 6. Test evaluation\n    test_loss, test_f1, test_preds, test_gts = run_epoch(\n        model, test_loader, optimizer=None\n    )\n    print(f\"Test Macro_F1 = {test_f1:.4f}\")\n\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\n\n    # 7. Save experiment data\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# ------------------------------------------------------------------\n# Execute immediately\nmain()\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport pathlib\nfrom typing import List, Dict\nfrom datasets import DatasetDict\n\n\n# -------- data loading util (from prompt) ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# ---------------------------------------------------\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ----------------- load dataset --------------------\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint(\"Data loaded:\", {k: len(v) for k, v in spr.items()})\n\n\n# ---------------- token + label vocab --------------\ndef build_vocab(seqs: List[str]) -> Dict[str, int]:\n    chars = set()\n    for s in seqs:\n        chars.update(list(s))\n    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n    for c in sorted(chars):\n        if c not in vocab:\n            vocab[c] = len(vocab)\n    return vocab\n\n\ntoken2id = build_vocab(spr[\"train\"][\"sequence\"])\nid2token = {i: t for t, i in token2id.items()}\n\n\ndef build_label_map(labels: List[str]) -> Dict[str, int]:\n    uniq = sorted(set(labels))\n    return {l: i for i, l in enumerate(uniq)}\n\n\nlabel2id = build_label_map(spr[\"train\"][\"label\"])\nid2label = {i: l for l, i in label2id.items()}\nnum_labels = len(label2id)\nprint(f\"Vocab size={len(token2id)}, num_labels={num_labels}\")\n\n\n# ------------ dataset / dataloader -----------------\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split, token2id, label2id):\n        self.seqs = hf_split[\"sequence\"]\n        self.labels = hf_split[\"label\"]\n        self.token2id = token2id\n        self.label2id = label2id\n\n    def encode(self, seq: str) -> List[int]:\n        return [self.token2id.get(ch, 1) for ch in seq]  # 1=UNK\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.encode(self.seqs[idx])\n        label = self.label2id[self.labels[idx]]\n        return {\n            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n            \"label\": torch.tensor(label, dtype=torch.long),\n        }\n\n\ndef collate_fn(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    max_len = max(lengths)\n    pad_id = 0\n    input_ids = []\n    for x in batch:\n        padded = torch.cat(\n            [\n                x[\"input_ids\"],\n                torch.full((max_len - len(x[\"input_ids\"]),), pad_id, dtype=torch.long),\n            ]\n        )\n        input_ids.append(padded)\n    input_ids = torch.stack(input_ids)\n    labels = torch.stack([x[\"label\"] for x in batch])\n    attn_mask = (input_ids != pad_id).long()\n    return {\"input_ids\": input_ids, \"attention_mask\": attn_mask, \"labels\": labels}\n\n\ntrain_ds = SPRDataset(spr[\"train\"], token2id, label2id)\ndev_ds = SPRDataset(spr[\"dev\"], token2id, label2id)\ntest_ds = SPRDataset(spr[\"test\"], token2id, label2id)\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n)\ndev_loader = DataLoader(\n    dev_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n)\ntest_loader = DataLoader(\n    test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n)\n\n\n# --------------- model -----------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # shape (1,max_len,d_model)\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=4,\n        num_layers=2,\n        dim_ff=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.encoder(x, src_key_padding_mask=(attention_mask == 0))\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )  # masked mean\n        return self.classifier(x)\n\n\nmodel = SPRTransformer(len(token2id), num_labels).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# --------- experiment data storage -----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# --------------- training loop ---------------------\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    model.train()\n    total_loss, all_pred, all_true = 0.0, [], []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        all_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n        all_true.extend(batch[\"labels\"].cpu().numpy())\n    train_loss = total_loss / len(train_ds)\n    train_f1 = f1_score(all_true, all_pred, average=\"macro\")\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(train_f1)\n\n    # validation\n    model.eval()\n    val_loss, val_pred, val_true = 0.0, [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            val_loss += loss.item() * batch[\"labels\"].size(0)\n            val_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n            val_true.extend(batch[\"labels\"].cpu().numpy())\n    val_loss /= len(dev_ds)\n    val_f1 = f1_score(val_true, val_pred, average=\"macro\")\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val Macro_F1 = {val_f1:.4f}\"\n    )\n\n# ------------- test evaluation ---------------------\nmodel.eval()\ntest_pred, test_true = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        test_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n        test_true.extend(batch[\"labels\"].cpu().numpy())\ntest_f1 = f1_score(test_true, test_pred, average=\"macro\")\nprint(f\"Test Macro_F1: {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_pred\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_true\n\n# ------------- save metrics and plot ---------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\nepochs_axis = np.arange(1, epochs + 1)\nplt.figure()\nplt.plot(\n    epochs_axis, experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"], label=\"train_loss\"\n)\nplt.plot(epochs_axis, experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"], label=\"val_loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"loss_curve_spr.png\"))\n\nplt.figure()\nplt.plot(\n    epochs_axis,\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"],\n    label=\"train_F1\",\n)\nplt.plot(\n    epochs_axis, experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"], label=\"val_F1\"\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Macro F1\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"f1_curve_spr.png\"))\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport pathlib\nfrom typing import List, Dict\n\n# Device handling -----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------------------- DATA LOAD\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------- VOCAB\nPAD, UNK = \"<pad>\", \"<unk>\"\nchar_set = set()\nfor ex in spr[\"train\"]:\n    char_set.update(list(ex[\"sequence\"]))\nitos = [PAD, UNK] + sorted(list(char_set))\nstoi = {ch: i for i, ch in enumerate(itos)}\n\n\ndef encode(seq: str, max_len: int = 128) -> List[int]:\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nmax_len = 128\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# -------------------------------------------------------------------- DATASET WRAPPER\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset, max_len=128):\n        self.data = hf_dataset\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        input_ids = torch.tensor(\n            encode(row[\"sequence\"], self.max_len), dtype=torch.long\n        )\n        attention_mask = (input_ids != stoi[PAD]).long()\n        label = torch.tensor(row[\"label\"], dtype=torch.long)\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label,\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], max_len), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], max_len), batch_size=batch_size)\n\n\n# -------------------------------------------------------------------- MODEL\nclass TinyTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        num_classes: int,\n        d_model: int = 128,\n        n_heads: int = 4,\n        n_layers: int = 2,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=n_heads, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos_embed[:, : input_ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )  # mean pool\n        return self.fc(x)\n\n\nmodel = TinyTransformer(len(itos), num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# -------------------------------------------------------------------- EXPERIMENT DATA\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------- TRAIN / EVAL FUNCTIONS\ndef run_loader(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss = 0\n    preds, gts = [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(outputs.argmax(-1).detach().cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------------------- TRAIN LOOP\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    train_loss, train_f1, _, _ = run_loader(train_loader, train=True)\n    val_loss, val_f1, val_preds, val_gts = run_loader(dev_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if epoch == epochs:\n        experiment_data[\"SPR_BENCH\"][\"predictions\"] = val_preds\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_macroF1={val_f1:.4f}\"\n    )\n\n# -------------------------------------------------------------------- SAVE METRICS & PLOTS\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"],\n    label=\"train_loss\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"],\n    label=\"val_loss\",\n)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nplt.close()\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"],\n    label=\"val_macro_f1\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Macro F1\")\nplt.title(\"Validation Macro F1\")\nplt.savefig(os.path.join(working_dir, \"f1_curve.png\"))\nplt.close()\n\nprint(\n    f'Final Dev Macro_F1: {experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"][-1]:.4f}'\n)\n", "import os, random, string, time\nfrom collections import Counter\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import Dataset as HFDataset, DatasetDict, load_dataset\nimport matplotlib.pyplot as plt\nimport pathlib\n\n# ------------------------------------------------------------------\n# working dir + experiment store\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\n    \"SPR\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n# ------------------------------------------------------------------\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------------\n# 1. DATA --------------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\ndef make_synthetic(split_size):\n    sequences = []\n    labels = []\n    vocab = list(string.ascii_uppercase[:6])  # 6 symbols\n    for _ in range(split_size):\n        length = random.randint(5, 20)\n        seq = \"\".join(random.choices(vocab, k=length))\n        lbl = random.randint(0, 4)  # 5 classes\n        sequences.append(seq)\n        labels.append(lbl)\n    return {\"id\": list(range(split_size)), \"sequence\": sequences, \"label\": labels}\n\n\n# try loading real data, else fabricate synthetic\ndata_path = pathlib.Path(os.environ.get(\"SPR_DATA_PATH\", \"SPR_BENCH\"))\nif data_path.exists() and (data_path / \"train.csv\").exists():\n    print(\"Loading SPR_BENCH from disk \u2026\")\n    dsets = load_spr_bench(data_path)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic data \u2026\")\n    dsets = DatasetDict()\n    dsets[\"train\"] = HFDataset.from_dict(make_synthetic(4000))\n    dsets[\"dev\"] = HFDataset.from_dict(make_synthetic(800))\n    dsets[\"test\"] = HFDataset.from_dict(make_synthetic(800))\n\n# ------------------------------------------------------------------\n# 2. TOKENISER + ENCODER ------------------------------------------------------\nPAD, UNK = \"<pad>\", \"<unk>\"\n\n\ndef build_vocab(dataset):\n    counter = Counter()\n    for seq in dataset[\"sequence\"]:\n        counter.update(list(seq.strip()))\n    vocab = [PAD, UNK] + sorted(counter.keys())\n    stoi = {s: i for i, s in enumerate(vocab)}\n    return vocab, stoi\n\n\nvocab, stoi = build_vocab(dsets[\"train\"])\nvocab_size = len(vocab)\nprint(f\"Vocab size = {vocab_size}\")\n\n\ndef encode(seq):\n    return [stoi.get(ch, stoi[UNK]) for ch in list(seq.strip())]\n\n\nfor split in [\"train\", \"dev\", \"test\"]:\n    dsets[split] = dsets[split].map(lambda ex: {\"input_ids\": encode(ex[\"sequence\"])})\n\n\n# ------------------------------------------------------------------\n# 3. PYTORCH DATASET ----------------------------------------------------------\nclass SPRTorchSet(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n            \"labels\": torch.tensor(item[\"label\"], dtype=torch.long),\n        }\n\n\ndef collate(batch):\n    lengths = [len(x[\"input_ids\"]) for x in batch]\n    max_len = max(lengths)\n    input_ids = []\n    attention = []\n    labels = []\n    for item in batch:\n        ids = item[\"input_ids\"]\n        pad_len = max_len - len(ids)\n        input_ids.append(torch.cat([ids, torch.full((pad_len,), stoi[PAD])]))\n        attention.append(torch.cat([torch.ones(len(ids)), torch.zeros(pad_len)]))\n        labels.append(item[\"labels\"])\n    return {\n        \"input_ids\": torch.stack(input_ids),\n        \"attention_mask\": torch.stack(attention),\n        \"labels\": torch.stack(labels),\n    }\n\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    SPRTorchSet(dsets[\"train\"]), batch_size=batch_size, shuffle=True, collate_fn=collate\n)\nval_loader = DataLoader(\n    SPRTorchSet(dsets[\"dev\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRTorchSet(dsets[\"test\"]), batch_size=batch_size, shuffle=False, collate_fn=collate\n)\n\n\n# ------------------------------------------------------------------\n# 4. MODEL --------------------------------------------------------------------\nclass TinyTransformer(nn.Module):\n    def __init__(self, vocab_size, num_classes, emb_dim=64, nhead=4, nlayers=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=stoi[PAD])\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=nhead,\n            dim_feedforward=emb_dim * 2,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.fc = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, ids, attn_mask):\n        x = self.embed(ids)\n        key_padding_mask = ~(attn_mask.bool())\n        x = self.transformer(x, src_key_padding_mask=key_padding_mask)\n        # mean pooling over non-pad tokens\n        lengths = attn_mask.sum(1).unsqueeze(-1)\n        pooled = (x * attn_mask.unsqueeze(-1)).sum(1) / lengths\n        return self.fc(pooled)\n\n\nnum_classes = len(set(dsets[\"train\"][\"label\"]))\nmodel = TinyTransformer(vocab_size, num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------------------------------------------------------\n# 5. TRAIN / EVAL LOOP --------------------------------------------------------\ndef run_epoch(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss, all_preds, all_labels = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        labels = batch[\"labels\"].cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels)\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    return avg_loss, macro_f1, np.array(all_preds), np.array(all_labels)\n\n\nepochs = 6\nfor epoch in range(1, epochs + 1):\n    t0 = time.time()\n    train_loss, train_f1, _, _ = run_epoch(train_loader, train=True)\n    val_loss, val_f1, _, _ = run_epoch(val_loader, train=False)\n\n    experiment_data[\"SPR\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR\"][\"metrics\"][\"train_f1\"].append(train_f1)\n    experiment_data[\"SPR\"][\"metrics\"][\"val_f1\"].append(val_f1)\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n        f\"train_F1={train_f1:.4f} val_F1={val_f1:.4f}  ({time.time()-t0:.1f}s)\"\n    )\n\n# ------------------------------------------------------------------\n# 6. FINAL TEST EVALUATION -----------------------------------------\ntest_loss, test_f1, test_preds, test_labels = run_epoch(test_loader, train=False)\nprint(f\"\\nTest   : loss={test_loss:.4f} Macro_F1={test_f1:.4f}\")\n\nexperiment_data[\"SPR\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR\"][\"ground_truth\"] = test_labels\nexperiment_data[\"SPR\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR\"][\"test_macro_f1\"] = test_f1\n\n# ------------------------------------------------------------------\n# 7. SAVE METRICS & PLOTS ------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\nplt.figure(figsize=(6, 4))\nplt.plot(\n    experiment_data[\"SPR\"][\"epochs\"],\n    experiment_data[\"SPR\"][\"metrics\"][\"train_f1\"],\n    label=\"train\",\n)\nplt.plot(\n    experiment_data[\"SPR\"][\"epochs\"],\n    experiment_data[\"SPR\"][\"metrics\"][\"val_f1\"],\n    label=\"val\",\n)\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Macro F1\")\nplt.title(\"SPR Macro-F1\")\nplt.legend()\nplt.tight_layout()\nplot_path = os.path.join(working_dir, \"SPR_f1_curve.png\")\nplt.savefig(plot_path)\nprint(f\"Learning-curve figure saved to {plot_path}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport pathlib\nfrom typing import List, Dict\n\n# Device handling -----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------------------- DATA LOAD\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------- VOCAB\nPAD, UNK = \"<pad>\", \"<unk>\"\nchar_set = set()\nfor ex in spr[\"train\"]:\n    char_set.update(list(ex[\"sequence\"]))\nitos = [PAD, UNK] + sorted(list(char_set))\nstoi = {ch: i for i, ch in enumerate(itos)}\n\n\ndef encode(seq: str, max_len: int = 128) -> List[int]:\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nmax_len = 128\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# -------------------------------------------------------------------- DATASET WRAPPER\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset, max_len=128):\n        self.data = hf_dataset\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        input_ids = torch.tensor(\n            encode(row[\"sequence\"], self.max_len), dtype=torch.long\n        )\n        attention_mask = (input_ids != stoi[PAD]).long()\n        label = torch.tensor(row[\"label\"], dtype=torch.long)\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label,\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], max_len), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], max_len), batch_size=batch_size)\n\n\n# -------------------------------------------------------------------- MODEL\nclass TinyTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        num_classes: int,\n        d_model: int = 128,\n        n_heads: int = 4,\n        n_layers: int = 2,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=n_heads, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos_embed[:, : input_ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )  # mean pool\n        return self.fc(x)\n\n\nmodel = TinyTransformer(len(itos), num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# -------------------------------------------------------------------- EXPERIMENT DATA\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------- TRAIN / EVAL FUNCTIONS\ndef run_loader(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss = 0\n    preds, gts = [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(outputs.argmax(-1).detach().cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------------------- TRAIN LOOP\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    train_loss, train_f1, _, _ = run_loader(train_loader, train=True)\n    val_loss, val_f1, val_preds, val_gts = run_loader(dev_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if epoch == epochs:\n        experiment_data[\"SPR_BENCH\"][\"predictions\"] = val_preds\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_macroF1={val_f1:.4f}\"\n    )\n\n# -------------------------------------------------------------------- SAVE METRICS & PLOTS\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"],\n    label=\"train_loss\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"],\n    label=\"val_loss\",\n)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nplt.close()\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"],\n    label=\"val_macro_f1\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Macro F1\")\nplt.title(\"Validation Macro F1\")\nplt.savefig(os.path.join(working_dir, \"f1_curve.png\"))\nplt.close()\n\nprint(\n    f'Final Dev Macro_F1: {experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"][-1]:.4f}'\n)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport pathlib\nfrom typing import List, Dict\n\n# Device handling -----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------------------- DATA LOAD\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------- VOCAB\nPAD, UNK = \"<pad>\", \"<unk>\"\nchar_set = set()\nfor ex in spr[\"train\"]:\n    char_set.update(list(ex[\"sequence\"]))\nitos = [PAD, UNK] + sorted(list(char_set))\nstoi = {ch: i for i, ch in enumerate(itos)}\n\n\ndef encode(seq: str, max_len: int = 128) -> List[int]:\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nmax_len = 128\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# -------------------------------------------------------------------- DATASET WRAPPER\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset, max_len=128):\n        self.data = hf_dataset\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        input_ids = torch.tensor(\n            encode(row[\"sequence\"], self.max_len), dtype=torch.long\n        )\n        attention_mask = (input_ids != stoi[PAD]).long()\n        label = torch.tensor(row[\"label\"], dtype=torch.long)\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label,\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], max_len), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], max_len), batch_size=batch_size)\n\n\n# -------------------------------------------------------------------- MODEL\nclass TinyTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        num_classes: int,\n        d_model: int = 128,\n        n_heads: int = 4,\n        n_layers: int = 2,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=n_heads, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos_embed[:, : input_ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )  # mean pool\n        return self.fc(x)\n\n\nmodel = TinyTransformer(len(itos), num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# -------------------------------------------------------------------- EXPERIMENT DATA\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------- TRAIN / EVAL FUNCTIONS\ndef run_loader(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss = 0\n    preds, gts = [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(outputs.argmax(-1).detach().cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------------------- TRAIN LOOP\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    train_loss, train_f1, _, _ = run_loader(train_loader, train=True)\n    val_loss, val_f1, val_preds, val_gts = run_loader(dev_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if epoch == epochs:\n        experiment_data[\"SPR_BENCH\"][\"predictions\"] = val_preds\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_macroF1={val_f1:.4f}\"\n    )\n\n# -------------------------------------------------------------------- SAVE METRICS & PLOTS\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"],\n    label=\"train_loss\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"],\n    label=\"val_loss\",\n)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nplt.close()\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"],\n    label=\"val_macro_f1\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Macro F1\")\nplt.title(\"Validation Macro F1\")\nplt.savefig(os.path.join(working_dir, \"f1_curve.png\"))\nplt.close()\n\nprint(\n    f'Final Dev Macro_F1: {experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"][-1]:.4f}'\n)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import DatasetDict\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport pathlib\nfrom typing import List, Dict\n\n# Device handling -----------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------------------------------------------------------- DATA LOAD\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    from datasets import load_dataset\n\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\nDATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nspr = load_spr_bench(DATA_PATH)\nprint(\"Dataset loaded:\", {k: len(v) for k, v in spr.items()})\n\n# -------------------------------------------------------------------- VOCAB\nPAD, UNK = \"<pad>\", \"<unk>\"\nchar_set = set()\nfor ex in spr[\"train\"]:\n    char_set.update(list(ex[\"sequence\"]))\nitos = [PAD, UNK] + sorted(list(char_set))\nstoi = {ch: i for i, ch in enumerate(itos)}\n\n\ndef encode(seq: str, max_len: int = 128) -> List[int]:\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:max_len]]\n    if len(ids) < max_len:\n        ids += [stoi[PAD]] * (max_len - len(ids))\n    return ids\n\n\nmax_len = 128\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\n\n# -------------------------------------------------------------------- DATASET WRAPPER\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_dataset, max_len=128):\n        self.data = hf_dataset\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        input_ids = torch.tensor(\n            encode(row[\"sequence\"], self.max_len), dtype=torch.long\n        )\n        attention_mask = (input_ids != stoi[PAD]).long()\n        label = torch.tensor(row[\"label\"], dtype=torch.long)\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label,\n        }\n\n\nbatch_size = 128\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"], max_len), batch_size=batch_size, shuffle=True\n)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], max_len), batch_size=batch_size)\n\n\n# -------------------------------------------------------------------- MODEL\nclass TinyTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        num_classes: int,\n        d_model: int = 128,\n        n_heads: int = 4,\n        n_layers: int = 2,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=n_heads, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids) + self.pos_embed[:, : input_ids.size(1), :]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )  # mean pool\n        return self.fc(x)\n\n\nmodel = TinyTransformer(len(itos), num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# -------------------------------------------------------------------- EXPERIMENT DATA\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------------------------------------------------------- TRAIN / EVAL FUNCTIONS\ndef run_loader(loader, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    total_loss = 0\n    preds, gts = [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(outputs.argmax(-1).detach().cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# -------------------------------------------------------------------- TRAIN LOOP\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    train_loss, train_f1, _, _ = run_loader(train_loader, train=True)\n    val_loss, val_f1, val_preds, val_gts = run_loader(dev_loader, train=False)\n\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(train_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    if epoch == epochs:\n        experiment_data[\"SPR_BENCH\"][\"predictions\"] = val_preds\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = val_gts\n\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_macroF1={val_f1:.4f}\"\n    )\n\n# -------------------------------------------------------------------- SAVE METRICS & PLOTS\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"],\n    label=\"train_loss\",\n)\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"],\n    label=\"val_loss\",\n)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Curve\")\nplt.savefig(os.path.join(working_dir, \"loss_curve.png\"))\nplt.close()\n\nplt.figure()\nplt.plot(\n    experiment_data[\"SPR_BENCH\"][\"epochs\"],\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"],\n    label=\"val_macro_f1\",\n)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Macro F1\")\nplt.title(\"Validation Macro F1\")\nplt.savefig(os.path.join(working_dir, \"f1_curve.png\"))\nplt.close()\n\nprint(\n    f'Final Dev Macro_F1: {experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"][-1]:.4f}'\n)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 238, in <module>\\n    main()\\n  File \"runfile.py\", line 181,\nin main\\n    dset = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 40, in load_spr_bench\\n\ndset[\"train\"] = _load(\"train.csv\")\\n                    ^^^^^^^^^^^^^^^^^^\\n\nFile \"runfile.py\", line 32, in _load\\n    return load_dataset(\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n14_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 113932.31\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 84007.05\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 170659.72\nexamples/s]', '\\n', 'Data loaded:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', 'Vocab size=11, num_labels=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.9889\n| val Macro_F1 = 0.6879', '\\n', 'Epoch 2: validation_loss = 2.0040 | val\nMacro_F1 = 0.6980', '\\n', 'Epoch 3: validation_loss = 2.2477 | val Macro_F1 =\n0.6960', '\\n', 'Epoch 4: validation_loss = 2.0798 | val Macro_F1 = 0.6980',\n'\\n', 'Epoch 5: validation_loss = 2.1197 | val Macro_F1 = 0.6960', '\\n', 'Test\nMacro_F1: 0.6959', '\\n', 'Execution time: 3 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 122549.09\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 82656.16\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 142179.80\nexamples/s]', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6793\nval_loss=0.6439 val_macroF1=0.6694', '\\n', 'Epoch 2: train_loss=0.3093\nval_loss=1.1379 val_macroF1=0.6839', '\\n', 'Epoch 3: train_loss=0.0550\nval_loss=1.8871 val_macroF1=0.6960', '\\n', 'Epoch 4: train_loss=0.0371\nval_loss=2.1939 val_macroF1=0.6920', '\\n', 'Epoch 5: train_loss=0.0305\nval_loss=2.2094 val_macroF1=0.6920', '\\n', 'Final Dev Macro_F1: 0.6920', '\\n',\n'Execution time: 5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'SPR_BENCH not found \u2013 generating synthetic data\n\u2026', '\\n', 'Vocab size = 8', '\\n', '\\rMap:   0%|          | 0/4000 [00:00<?, ?\nexamples/s]', '\\rMap:  85%|########5 | 3415/4000 [00:00<00:00, 33993.77\nexamples/s]', '', '\\rMap: 100%|##########| 4000/4000 [00:00<00:00, 32993.28\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/800 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 800/800 [00:00<00:00, 33800.50 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/800 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 800/800 [00:00<00:00, 33956.13 examples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=1.6237\nval_loss=1.6156 train_F1=0.2043 val_F1=0.1516  (2.2s)', '\\n', 'Epoch 2:\ntrain_loss=1.6151 val_loss=1.6232 train_F1=0.1892 val_F1=0.1314  (1.9s)', '\\n',\n'Epoch 3: train_loss=1.6131 val_loss=1.6196 train_F1=0.1921 val_F1=0.1757\n(1.7s)', '\\n', 'Epoch 4: train_loss=1.6123 val_loss=1.6243 train_F1=0.1995\nval_F1=0.0865  (1.7s)', '\\n', 'Epoch 5: train_loss=1.6102 val_loss=1.6173\ntrain_F1=0.1963 val_F1=0.1433  (1.7s)', '\\n', 'Epoch 6: train_loss=1.6078\nval_loss=1.6115 train_F1=0.2088 val_F1=0.1590  (1.7s)', '\\n', '\\nTest   :\nloss=1.6170 Macro_F1=0.1525', '\\n', 'Learning-curve figure saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n14_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n4/working/SPR_f1_curve.png', '\\n', 'Execution time: 14 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 128293.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 108615.70\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 222415.10\nexamples/s]', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev': 500, 'test':\n1000}\", '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.7735\nval_loss=0.7216 val_macroF1=0.3351', '\\n', 'Epoch 2: train_loss=0.5088\nval_loss=0.6946 val_macroF1=0.6839', '\\n', 'Epoch 3: train_loss=0.1348\nval_loss=1.5233 val_macroF1=0.6900', '\\n', 'Epoch 4: train_loss=0.0470\nval_loss=1.8491 val_macroF1=0.6900', '\\n', 'Epoch 5: train_loss=0.0328\nval_loss=1.8374 val_macroF1=0.6880', '\\n', 'Final Dev Macro_F1: 0.6880', '\\n',\n'Execution time: 10 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6524\nval_loss=0.6488 val_macroF1=0.6819', '\\n', 'Epoch 2: train_loss=0.2790\nval_loss=1.3438 val_macroF1=0.6960', '\\n', 'Epoch 3: train_loss=0.0445\nval_loss=1.8586 val_macroF1=0.6980', '\\n', 'Epoch 4: train_loss=0.0203\nval_loss=2.1762 val_macroF1=0.6960', '\\n', 'Epoch 5: train_loss=0.0144\nval_loss=2.4570 val_macroF1=0.7000', '\\n', 'Final Dev Macro_F1: 0.7000', '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Dataset loaded:', ' ', \"{'train': 2000, 'dev':\n500, 'test': 1000}\", '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_loss=0.6958\nval_loss=0.6797 val_macroF1=0.3316', '\\n', 'Epoch 2: train_loss=0.4100\nval_loss=0.8624 val_macroF1=0.6920', '\\n', 'Epoch 3: train_loss=0.0811\nval_loss=1.5859 val_macroF1=0.6960', '\\n', 'Epoch 4: train_loss=0.0376\nval_loss=1.9382 val_macroF1=0.6940', '\\n', 'Epoch 5: train_loss=0.0206\nval_loss=2.0907 val_macroF1=0.6960', '\\n', 'Final Dev Macro_F1: 0.6960', '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution failed due to a FileNotFoundError. The script attempted to load\nthe dataset from '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n14_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n1/SPR_BENCH/train.csv', but the file or directory does not exist. To fix this,\nensure that the dataset files (train.csv, dev.csv, test.csv) are correctly\nplaced in the specified directory path or update the 'DATA_PATH' variable in the\nscript to point to the correct location of the dataset.", "The training and validation loss do not decrease as the epochs progress, and the\nvalidation Macro F1 score remains stagnant around 0.69. This indicates that the\nmodel is not learning effectively. Possible reasons could include an overly\nsimplistic model architecture, insufficient training epochs, or an inappropriate\nlearning rate. To fix this, consider: 1. Experimenting with a larger or more\ncomplex model architecture. 2. Increasing the number of training epochs. 3.\nAdjusting the learning rate or using a learning rate scheduler. 4. Checking for\ndata preprocessing issues or class imbalance in the dataset.", "", "", "The execution output shows that the implementation successfully loads the\ndataset, trains a simple transformer model, and evaluates it on the validation\nset. The training and validation losses and F1 scores are logged for each epoch,\nand the final validation macro F1 score is 0.6880. There are no errors or bugs\nin the implementation, and the output is consistent with the intended\nfunctionality of the code. The plots for loss and F1 curves were also saved\nsuccessfully. Overall, the implementation is functionally correct.", "The execution of the training script was successful. The model trained without\nerrors, and the final validation Macro F1 score achieved was 0.7000. There are\nno issues or bugs in the implementation based on the provided execution logs.", "The execution output indicates that the training script successfully ran without\nany errors, and the model was trained for 5 epochs. The validation macro F1\nscore improved significantly after the first epoch and stabilized around 0.6960\nby the end of training. While the validation loss increased in later epochs, the\nF1 score remained stable, suggesting that the model is overfitting slightly to\nthe training data. However, this is not a bug but rather an observation about\nmodel behavior. The implementation appears functionally correct for this stage\nof experimentation.", ""], "exc_type": ["FileNotFoundError", null, null, null, null, null, null, null], "exc_info": [{"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-1/SPR_BENCH/train.csv'"]}, null, null, null, null, null, null, null], "exc_stack": [[["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 238, "<module>", "main()"], ["runfile.py", 181, "main", "dset = load_spr_bench(DATA_PATH)"], ["runfile.py", 40, "load_spr_bench", "dset[\"train\"] = _load(\"train.csv\")"], ["runfile.py", 32, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0433, "best_value": 0.0433}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.1197, "best_value": 2.1197}]}, {"metric_name": "train macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9905, "best_value": 0.9905}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6959, "best_value": 0.6959}]}]}, {"metric_names": [{"metric_name": "training F1", "lower_is_better": false, "description": "F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9935, "best_value": 0.9935}]}, {"metric_name": "validation F1", "lower_is_better": false, "description": "F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0305, "best_value": 0.0305}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6439, "best_value": 0.6439}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro F1 score measures the balance between precision and recall for each class, averaged without considering class imbalance.", "data": [{"dataset_name": "SPR", "final_value": 0.1525, "best_value": 0.1757}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error or deviation of the model's predictions from the actual values.", "data": [{"dataset_name": "SPR", "final_value": 1.617, "best_value": 1.6115}]}]}, {"metric_names": [{"metric_name": "training f1", "lower_is_better": false, "description": "F1 score during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9915, "best_value": 0.9915}]}, {"metric_name": "validation f1", "lower_is_better": false, "description": "F1 score during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.69, "best_value": 0.69}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0328, "best_value": 0.0328}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6946, "best_value": 0.6946}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9965, "best_value": 0.9965}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0144, "best_value": 0.0144}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6488, "best_value": 0.6488}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0206, "best_value": 0.0206}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6797, "best_value": 0.6797}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false, false], "plots": [[], [], ["../../logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/loss_curve.png", "../../logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/f1_curve.png", "../../logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_f1_curve.png", "../../logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_loss_curve.png", "../../logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/loss_curve.png", "../../logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/f1_curve.png", "../../logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/loss_curve_spr.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/f1_curve_spr.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/loss_curve.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/f1_curve.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/loss_curve.png", "../../logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/f1_curve.png", "../../logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/seed_aggregation_4f82023476e144a9874280644d8f964f/SPR_BENCH_agg_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_4f82023476e144a9874280644d8f964f/SPR_BENCH_agg_f1_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_4f82023476e144a9874280644d8f964f/SPR_BENCH_final_val_f1_scatter.png"]], "plot_paths": [[], [], ["experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_label_distribution.png"], ["experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_confusion_matrix.png"], ["experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_label_distribution.png"], ["experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/loss_curve_spr.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/f1_curve_spr.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_label_distribution.png"], ["experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_label_distribution.png"], ["experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_4f82023476e144a9874280644d8f964f/SPR_BENCH_agg_loss_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_4f82023476e144a9874280644d8f964f/SPR_BENCH_agg_f1_curve.png", "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_4f82023476e144a9874280644d8f964f/SPR_BENCH_final_val_f1_scatter.png"]], "plot_analyses": [[], [], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning effectively on the training data. However, the validation loss increases after an initial drop, suggesting overfitting. This implies that the model generalizes poorly to unseen data despite learning well on the training set.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/loss_curve.png"}, {"analysis": "The validation macro F1 score improves in the initial epochs, peaking at epoch 3, but then starts to decline slightly. This trend aligns with the overfitting observed in the loss curve, as the model's performance on unseen data deteriorates after a certain point.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/f1_curve.png"}, {"analysis": "The training loss decreases to near zero, showing that the model fits the training data very well. However, the validation loss increases significantly after epoch 3, confirming overfitting and poor generalization.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_loss_curve.png"}, {"analysis": "The training macro F1 score rapidly approaches 1.0, indicating perfect classification on the training set. In contrast, the validation macro F1 score shows marginal improvement and saturates early, highlighting a gap in generalization performance.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_f1_curve.png"}, {"analysis": "The confusion matrix shows a strong diagonal pattern, indicating that the model is making correct predictions for many samples. However, there may still be some misclassification in the off-diagonal elements, which could be analyzed further for specific patterns.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The label distribution comparison shows that the generated samples have a balanced class distribution, similar to the ground truth. This suggests that the model is not biased toward any particular class during generation.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_a815aafbf5b4457da328d6b2ba83083e_proc_3458408/SPR_BENCH_label_distribution.png"}], [{"analysis": "The Macro-F1 curve shows a noticeable gap between training and validation performance. The training Macro-F1 improves gradually over the epochs, indicating the model is learning from the training data. However, the validation Macro-F1 fluctuates significantly, with a sharp drop at epoch 4 and a partial recovery afterward. This suggests a potential issue with overfitting or instability in the model's generalization capability.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_f1_curve.png"}, {"analysis": "The loss curve indicates that the training loss decreases steadily, showing that the model is optimizing its parameters effectively on the training data. However, the validation loss exhibits fluctuations, especially around epochs 2 and 4, before decreasing again. This pattern aligns with the fluctuations observed in the validation Macro-F1 curve and might indicate overfitting or sensitivity to certain aspects of the validation data.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_loss_curve.png"}, {"analysis": "The confusion matrix highlights that the model struggles with classifying certain labels correctly. There is a significant degree of misclassification across all classes, with some classes (e.g., class 1 and class 2) being particularly prone to confusion. This indicates that the model might not have learned the underlying symbolic reasoning rules effectively, or the dataset might have inherent challenges such as imbalanced class distributions or noisy labels.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_324d35fcbfad4077bc44a34a16edfcf4_proc_3458409/SPR_confusion_matrix.png"}], [{"analysis": "The loss curve shows a significant divergence between training loss and validation loss after epoch 2. While the training loss decreases steadily and reaches near zero, the validation loss increases sharply after epoch 2. This indicates overfitting, as the model performs well on the training set but fails to generalize to the validation set beyond a certain point.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/loss_curve.png"}, {"analysis": "The validation Macro F1 score improves significantly in the first two epochs but plateaus afterward. This suggests that the model quickly learns the basic patterns in the data but struggles to improve further. The lack of improvement after epoch 2 aligns with the overfitting observed in the loss curve.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/f1_curve.png"}, {"analysis": "This loss curve again highlights the overfitting issue. The training loss decreases consistently, but the validation loss increases after epoch 2, indicating the model's inability to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_loss_curve.png"}, {"analysis": "The Macro-F1 curve demonstrates a similar trend to the previous validation F1 curve. The training Macro-F1 score continues to improve, reaching nearly 1.0, while the validation Macro-F1 score plateaus and slightly decreases after epoch 4. This reinforces the observation of overfitting.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_f1_curve.png"}, {"analysis": "The confusion matrix suggests a moderate level of class imbalance in the predictions. The darker blue cells along the diagonal indicate that the model performs better for some classes than others, but there is room for improvement in overall classification performance.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The label distribution comparison shows a good balance between ground truth labels and generated samples. Both distributions are nearly identical, indicating that the model maintains the original class proportions in its predictions. This is a positive result, as it suggests no bias in class representation.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/SPR_BENCH_label_distribution.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning from the training data. However, the validation loss increases after the first epoch, suggesting overfitting. The model performs well on the training set but struggles to generalize to the validation set.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/loss_curve_spr.png"}, {"analysis": "The training macro F1 score improves rapidly and reaches near-perfect performance, while the validation macro F1 score remains relatively constant with only slight improvement. This further indicates overfitting, as the model is not generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/f1_curve_spr.png"}, {"analysis": "The loss curves show a clear divergence between training and validation loss. While the training loss approaches zero, the validation loss increases steadily, reinforcing the observation that the model is overfitting to the training data.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/loss_curve.png"}, {"analysis": "The validation macro F1 score shows a slight upward trend over the epochs but remains relatively low, indicating limited improvement in the model's generalization capability on the validation set.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/f1_curve.png"}, {"analysis": "The loss curves for the SPR_BENCH dataset show a similar pattern of overfitting. The training loss decreases to near zero, while the validation loss increases consistently, indicating poor generalization.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_loss_curve.png"}, {"analysis": "The macro F1 curves for the SPR_BENCH dataset confirm the overfitting trend. The training macro F1 score reaches near-perfect levels, while the validation macro F1 score remains stagnant and significantly lower, further indicating poor generalization.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_f1_curve.png"}, {"analysis": "The confusion matrix shows that the model performs well on the majority class but struggles with the minority class. This imbalance could contribute to the overfitting observed in the loss and F1 score curves.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The label distribution comparison shows a balanced dataset in terms of ground truth and generated samples, suggesting that the dataset is not inherently biased. However, the model's performance issues are likely due to overfitting rather than data imbalance.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/SPR_BENCH_label_distribution.png"}], [{"analysis": "The training loss decreases steadily, showing that the model is learning the training data effectively. However, the validation loss increases significantly after the first epoch, indicating overfitting. This suggests that the model is not generalizing well to unseen data, and adjustments such as regularization, early stopping, or data augmentation may be necessary.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/loss_curve.png"}, {"analysis": "The validation Macro F1 score improves rapidly in the first two epochs and then plateaus. This indicates that the model achieves a reasonable level of performance early in training but does not improve further. This plateau might be due to insufficient model capacity, suboptimal hyperparameters, or the inherent difficulty of the task.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/f1_curve.png"}, {"analysis": "This plot mirrors the earlier loss trends, with the training loss decreasing consistently and the validation loss increasing. The gap between the two losses confirms overfitting, and interventions to improve generalization are required.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro F1 score improves steadily and reaches near-perfect performance, while the validation Macro F1 score stabilizes at a lower level. This discrepancy further confirms overfitting, as the model performs well on the training data but struggles to generalize to the validation set.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_f1_curve.png"}, {"analysis": "The confusion matrix shows clear separation between the two classes, suggesting that the model is learning meaningful patterns. However, the imbalance in the intensity of the diagonal elements indicates that the model may be biased towards one class, which could be addressed by balancing the training data or using class weights.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The label distribution comparison shows that the generated samples have a distribution identical to the ground truth. This indicates that the data generation process is consistent and unbiased, which is a positive aspect of the experimental setup.", "plot_path": "experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/SPR_BENCH_label_distribution.png"}], []], "vlm_feedback_summary": ["[]", "[]", "The plots reveal significant overfitting in the model, as evidenced by the\ndivergence between training and validation loss curves and the stagnation of\nvalidation macro F1 scores. While the model performs perfectly on the training\nset, its generalization to unseen data is limited. The balanced label\ndistribution in generated samples is a positive outcome, indicating no class\nbias.", "The plots indicate that while the model is learning effectively on the training\ndata, its generalization to the validation set is unstable, as evidenced by the\nfluctuating validation Macro-F1 and loss curves. The confusion matrix further\nreveals significant misclassification issues, suggesting that the model\nstruggles with the symbolic reasoning task and might require architectural\nimprovements or better regularization strategies.", "The plots reveal significant overfitting in the model, as evidenced by the\ndivergence between training and validation loss and the plateauing of validation\nMacro F1 scores. While the label distribution comparison indicates that class\nproportions are preserved, the confusion matrix and performance curves suggest\nthat the model struggles with generalization and classification accuracy.", "The plots highlight significant overfitting in the model. While the training\nperformance (loss and F1 score) improves significantly, the validation\nperformance remains stagnant or worsens. This indicates poor generalization\ncapabilities, which is a critical issue for the SPR task.", "The plots indicate that while the model learns well on the training data, it\noverfits and struggles to generalize to the validation set. The Macro F1 scores\nhighlight this gap, and the confusion matrix suggests potential class imbalance\nissues. The label distribution comparison confirms that the data generation\nprocess is unbiased.", "[]"], "exec_time": [1.3090028762817383, 3.8070666790008545, 5.025304555892944, 14.823029041290283, 10.869011640548706, 7.460189342498779, 7.587592124938965, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], [], ["[]"], ["[]"], ["[]"], ["[]"], ["[]"], []], "plot_code": [null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------- LOAD DATA -----------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Assume single dataset key\nif experiment_data:\n    dname = list(experiment_data.keys())[0]\n    data = experiment_data[dname]\n    epochs = data[\"epochs\"]\n    train_loss, val_loss = data[\"losses\"][\"train\"], data[\"losses\"][\"val\"]\n    train_f1, val_f1 = data[\"metrics\"][\"train_f1\"], data[\"metrics\"][\"val_f1\"]\n    preds, gts = np.array(data[\"predictions\"]), np.array(data[\"ground_truth\"])\n\n    # ----------------------- PLOT 1: LOSS CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname} Loss Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 2: F1 CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro F1\")\n        plt.plot(epochs, val_f1, label=\"Val Macro F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(f\"{dname} Macro-F1 Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_f1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 3: CONFUSION MATRIX -----------------------\n    try:\n        num_classes = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"{dname} Confusion Matrix\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 4: LABEL DISTRIBUTION -----------------------\n    try:\n        labels = np.arange(int(max(gts.max(), preds.max()) + 1))\n        gt_counts = np.bincount(gts, minlength=len(labels))\n        pred_counts = np.bincount(preds, minlength=len(labels))\n\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n        axes[0].bar(labels, gt_counts)\n        axes[0].set_title(\"Left: Ground Truth\")\n        axes[0].set_xlabel(\"Class\")\n        axes[0].set_ylabel(\"Count\")\n\n        axes[1].bar(labels, pred_counts, color=\"orange\")\n        axes[1].set_title(\"Right: Generated Samples\")\n        axes[1].set_xlabel(\"Class\")\n\n        fig.suptitle(f\"{dname} Label Distribution Comparison\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_label_distribution.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n\n    # ----------------------- PRINT METRIC -----------------------\n    print(f\"Final validation Macro-F1 (from file): {val_f1[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\n# basic setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# load stored experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------\nfor dset_name, d in experiment_data.items():\n    epochs = d.get(\"epochs\", [])\n    # 1) Loss curve -------------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, d[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, d[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{dset_name} Loss Curve\")\n        plt.legend()\n        save_path = os.path.join(working_dir, f\"{dset_name}_loss_curve.png\")\n        plt.tight_layout()\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {dset_name}: {e}\")\n        plt.close()\n\n    # 2) Macro-F1 curve --------------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, d[\"metrics\"][\"train_f1\"], label=\"Train\")\n        plt.plot(epochs, d[\"metrics\"][\"val_f1\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{dset_name} Macro-F1 Curve\")\n        plt.legend()\n        save_path = os.path.join(working_dir, f\"{dset_name}_f1_curve.png\")\n        plt.tight_layout()\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve for {dset_name}: {e}\")\n        plt.close()\n\n    # 3) Confusion matrix ------------------------------------------\n    try:\n        y_true = d[\"ground_truth\"]\n        y_pred = d[\"predictions\"]\n        if len(y_true) and len(y_true) == len(y_pred):\n            cm = confusion_matrix(y_true, y_pred)\n            plt.figure(figsize=(5, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            plt.title(f\"{dset_name} Test Confusion Matrix\")\n            # annotate cells\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                        fontsize=8,\n                    )\n            plt.tight_layout()\n            save_path = os.path.join(working_dir, f\"{dset_name}_confusion_matrix.png\")\n            plt.savefig(save_path)\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------- LOAD DATA -----------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Assume single dataset key\nif experiment_data:\n    dname = list(experiment_data.keys())[0]\n    data = experiment_data[dname]\n    epochs = data[\"epochs\"]\n    train_loss, val_loss = data[\"losses\"][\"train\"], data[\"losses\"][\"val\"]\n    train_f1, val_f1 = data[\"metrics\"][\"train_f1\"], data[\"metrics\"][\"val_f1\"]\n    preds, gts = np.array(data[\"predictions\"]), np.array(data[\"ground_truth\"])\n\n    # ----------------------- PLOT 1: LOSS CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname} Loss Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 2: F1 CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro F1\")\n        plt.plot(epochs, val_f1, label=\"Val Macro F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(f\"{dname} Macro-F1 Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_f1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 3: CONFUSION MATRIX -----------------------\n    try:\n        num_classes = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"{dname} Confusion Matrix\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 4: LABEL DISTRIBUTION -----------------------\n    try:\n        labels = np.arange(int(max(gts.max(), preds.max()) + 1))\n        gt_counts = np.bincount(gts, minlength=len(labels))\n        pred_counts = np.bincount(preds, minlength=len(labels))\n\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n        axes[0].bar(labels, gt_counts)\n        axes[0].set_title(\"Left: Ground Truth\")\n        axes[0].set_xlabel(\"Class\")\n        axes[0].set_ylabel(\"Count\")\n\n        axes[1].bar(labels, pred_counts, color=\"orange\")\n        axes[1].set_title(\"Right: Generated Samples\")\n        axes[1].set_xlabel(\"Class\")\n\n        fig.suptitle(f\"{dname} Label Distribution Comparison\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_label_distribution.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n\n    # ----------------------- PRINT METRIC -----------------------\n    print(f\"Final validation Macro-F1 (from file): {val_f1[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------- LOAD DATA -----------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Assume single dataset key\nif experiment_data:\n    dname = list(experiment_data.keys())[0]\n    data = experiment_data[dname]\n    epochs = data[\"epochs\"]\n    train_loss, val_loss = data[\"losses\"][\"train\"], data[\"losses\"][\"val\"]\n    train_f1, val_f1 = data[\"metrics\"][\"train_f1\"], data[\"metrics\"][\"val_f1\"]\n    preds, gts = np.array(data[\"predictions\"]), np.array(data[\"ground_truth\"])\n\n    # ----------------------- PLOT 1: LOSS CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname} Loss Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 2: F1 CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro F1\")\n        plt.plot(epochs, val_f1, label=\"Val Macro F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(f\"{dname} Macro-F1 Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_f1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 3: CONFUSION MATRIX -----------------------\n    try:\n        num_classes = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"{dname} Confusion Matrix\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 4: LABEL DISTRIBUTION -----------------------\n    try:\n        labels = np.arange(int(max(gts.max(), preds.max()) + 1))\n        gt_counts = np.bincount(gts, minlength=len(labels))\n        pred_counts = np.bincount(preds, minlength=len(labels))\n\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n        axes[0].bar(labels, gt_counts)\n        axes[0].set_title(\"Left: Ground Truth\")\n        axes[0].set_xlabel(\"Class\")\n        axes[0].set_ylabel(\"Count\")\n\n        axes[1].bar(labels, pred_counts, color=\"orange\")\n        axes[1].set_title(\"Right: Generated Samples\")\n        axes[1].set_xlabel(\"Class\")\n\n        fig.suptitle(f\"{dname} Label Distribution Comparison\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_label_distribution.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n\n    # ----------------------- PRINT METRIC -----------------------\n    print(f\"Final validation Macro-F1 (from file): {val_f1[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ----------------------- LOAD DATA -----------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Assume single dataset key\nif experiment_data:\n    dname = list(experiment_data.keys())[0]\n    data = experiment_data[dname]\n    epochs = data[\"epochs\"]\n    train_loss, val_loss = data[\"losses\"][\"train\"], data[\"losses\"][\"val\"]\n    train_f1, val_f1 = data[\"metrics\"][\"train_f1\"], data[\"metrics\"][\"val_f1\"]\n    preds, gts = np.array(data[\"predictions\"]), np.array(data[\"ground_truth\"])\n\n    # ----------------------- PLOT 1: LOSS CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dname} Loss Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 2: F1 CURVE -----------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train Macro F1\")\n        plt.plot(epochs, val_f1, label=\"Val Macro F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(f\"{dname} Macro-F1 Curves\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_f1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 3: CONFUSION MATRIX -----------------------\n    try:\n        num_classes = int(max(gts.max(), preds.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"{dname} Confusion Matrix\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT 4: LABEL DISTRIBUTION -----------------------\n    try:\n        labels = np.arange(int(max(gts.max(), preds.max()) + 1))\n        gt_counts = np.bincount(gts, minlength=len(labels))\n        pred_counts = np.bincount(preds, minlength=len(labels))\n\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n        axes[0].bar(labels, gt_counts)\n        axes[0].set_title(\"Left: Ground Truth\")\n        axes[0].set_xlabel(\"Class\")\n        axes[0].set_ylabel(\"Count\")\n\n        axes[1].bar(labels, pred_counts, color=\"orange\")\n        axes[1].set_title(\"Right: Generated Samples\")\n        axes[1].set_xlabel(\"Class\")\n\n        fig.suptitle(f\"{dname} Label Distribution Comparison\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{dname}_label_distribution.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n\n    # ----------------------- PRINT METRIC -----------------------\n    print(f\"Final validation Macro-F1 (from file): {val_f1[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom math import sqrt\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# Load every experiment_data.npy that was listed in the instructions\n# ------------------------------------------------------------------\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3fb72856d0ef4cf499f47544df4cf1c7_proc_3458402/experiment_data.npy\",\n        \"experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_03523b99bca44b79a63d807c9573d529_proc_3458404/experiment_data.npy\",\n        \"experiments/2025-08-17_23-44-14_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_fda6956bb09d4da5b6e1132ec9817237_proc_3458408/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for p in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp_dict = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dict)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# ---------------------------------------------------------------\n# Aggregate curves (mean and SEM) and create the requested plots\n# ---------------------------------------------------------------\nif all_experiment_data:\n    # assume every run contains exactly one dataset key and that\n    # they are identical across runs\n    dataset_name = list(all_experiment_data[0].keys())[0]\n\n    # containers for per-run arrays\n    train_loss_runs, val_loss_runs = [], []\n    train_f1_runs, val_f1_runs = [], []\n    epochs = None\n\n    for run in all_experiment_data:\n        d = run[dataset_name]\n        if epochs is None:\n            epochs = np.asarray(d[\"epochs\"])\n        train_loss_runs.append(np.asarray(d[\"losses\"][\"train\"]))\n        val_loss_runs.append(np.asarray(d[\"losses\"][\"val\"]))\n        train_f1_runs.append(np.asarray(d[\"metrics\"][\"train_f1\"]))\n        val_f1_runs.append(np.asarray(d[\"metrics\"][\"val_f1\"]))\n\n    # stack and compute mean / SEM\n    train_loss_stack = np.vstack(train_loss_runs)\n    val_loss_stack = np.vstack(val_loss_runs)\n    train_f1_stack = np.vstack(train_f1_runs)\n    val_f1_stack = np.vstack(val_f1_runs)\n\n    n_runs = train_loss_stack.shape[0]\n\n    def mean_sem(stack):\n        mean = stack.mean(axis=0)\n        sem = stack.std(axis=0, ddof=1) / sqrt(n_runs)\n        return mean, sem\n\n    train_loss_mean, train_loss_sem = mean_sem(train_loss_stack)\n    val_loss_mean, val_loss_sem = mean_sem(val_loss_stack)\n    train_f1_mean, train_f1_sem = mean_sem(train_f1_stack)\n    val_f1_mean, val_f1_sem = mean_sem(val_f1_stack)\n\n    # ----------------------- PLOT A: LOSS w/ SEM -----------------------\n    try:\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_loss_mean, label=\"Train Loss (mean)\")\n        plt.fill_between(\n            epochs,\n            train_loss_mean - train_loss_sem,\n            train_loss_mean + train_loss_sem,\n            alpha=0.3,\n        )\n        plt.plot(epochs, val_loss_mean, label=\"Val Loss (mean)\")\n        plt.fill_between(\n            epochs,\n            val_loss_mean - val_loss_sem,\n            val_loss_mean + val_loss_sem,\n            alpha=0.3,\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{dataset_name} Loss Curves\\nMean \u00b1 SEM across {n_runs} runs\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"{dataset_name}_agg_loss_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT B: F1 w/ SEM -----------------------\n    try:\n        plt.figure(figsize=(6, 4))\n        plt.plot(epochs, train_f1_mean, label=\"Train Macro-F1 (mean)\")\n        plt.fill_between(\n            epochs,\n            train_f1_mean - train_f1_sem,\n            train_f1_mean + train_f1_sem,\n            alpha=0.3,\n        )\n        plt.plot(epochs, val_f1_mean, label=\"Val Macro-F1 (mean)\")\n        plt.fill_between(\n            epochs, val_f1_mean - val_f1_sem, val_f1_mean + val_f1_sem, alpha=0.3\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(f\"{dataset_name} Macro-F1 Curves\\nMean \u00b1 SEM across {n_runs} runs\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"{dataset_name}_agg_f1_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------- PLOT C: Final Val F1 Distribution -----------------------\n    try:\n        final_val_f1 = val_f1_stack[:, -1]\n        mean_final = final_val_f1.mean()\n        sem_final = final_val_f1.std(ddof=1) / sqrt(n_runs)\n\n        plt.figure(figsize=(5, 4))\n        x = np.arange(n_runs)\n        plt.scatter(x, final_val_f1, label=\"Individual runs\")\n        plt.hlines(\n            mean_final,\n            -0.5,\n            n_runs - 0.5,\n            colors=\"red\",\n            label=f\"Mean = {mean_final:.3f}\",\n        )\n        plt.fill_between(\n            [-0.5, n_runs - 0.5],\n            mean_final - sem_final,\n            mean_final + sem_final,\n            color=\"red\",\n            alpha=0.2,\n            label=\"Mean \u00b1 SEM\",\n        )\n        plt.xticks(x)\n        plt.ylabel(\"Final Val Macro-F1\")\n        plt.title(f\"{dataset_name} Final Val Macro-F1 across runs\")\n        plt.legend()\n        plt.tight_layout()\n        fname = f\"{dataset_name}_final_val_f1_scatter.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final-F1 scatter: {e}\")\n        plt.close()\n\n    # ----------------------- PRINT METRIC -----------------------\n    print(f\"Mean final validation Macro-F1 \u00b1 SEM: {mean_final:.4f} \u00b1 {sem_final:.4f}\")\nelse:\n    print(\"No experiment data could be aggregated.\")\n"], "plot_plan": [null, null, "We first load experiment_data.npy, extract the single dataset key (SPR_BENCH), and retrieve epochs, loss lists, F1 lists, predictions, and ground-truth labels.  \nFour plots will be produced, each enclosed in its own try-except block: (1) train/val loss curves, (2) train/val macro-F1 curves, (3) a confusion-matrix heat-map for the final validation predictions, and (4) a bar chart comparing label frequency for ground-truth versus predictions (left/right subplot) to satisfy the \u201cLeft: Ground Truth, Right: Generated Samples\u201d guideline.  \nAll figures are saved inside working_dir with descriptive names that include the dataset and plot type, then immediately closed.  \nWe cap the number of generated figures at four (<5) and use imshow for the heat-map with a colorbar.  \nNo synthetic data is created\u2014everything is read directly from experiment_data.npy.  \nAfter plotting, the script prints the final validation macro-F1 stored in the file so users can verify consistency with the plots.  \nStandard matplotlib and numpy are the only libraries used, and every figure is given an informative title.  \nThis concise script can be run independently inside the current environment and will gracefully handle missing or corrupted data thanks to the try-except blocks.", "We will first load the experiment_data.npy file from the working directory and pull out the single dataset (\u201cSPR\u201d). From the saved arrays we can derive the x-axis (epochs) and the y-values for training/validation loss and Macro-F1, allowing us to create two standard learning-curve plots.  We then take the saved test predictions and ground-truth labels and compute a confusion matrix; this will be rendered as an image so users can quickly spot systematic errors.  Each figure is wrapped in a try-except to avoid crashing the script if something is missing, and every figure is explicitly closed after saving.  Figure filenames always contain the dataset name plus an informative suffix so they stay readable when many experiments accumulate.  All plots are saved to the existing working directory created by the training script.  The code sticks to basic matplotlib, avoids custom styles, and uses clear titles/subtitles that mention the dataset.  Only stored values are plotted, never any fabricated data, and at most three figures are produced so we remain well under the five-figure limit.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["", "The script will load the stored NumPy dictionary from the working directory,\niterate through each dataset (here only \u201cSPR_BENCH\u201d), and print the final\nepoch\u2019s training/validation loss and macro-F1 values. It will also recompute and\nreport the test macro-F1 score from the saved predictions and ground-truth\nlabels. No plots are generated; the code runs immediately at import time.", "The script first locates the working directory, loads the saved NumPy\ndictionary, and then iterates through each dataset it contains. For every\ndataset, it prints the dataset\u2019s name, followed by the best (max for F1-like\nmetrics, min for losses) value found in each recorded list. Metric names are\nexpanded into clear labels such as \u201ctraining macro F1\u201d or \u201cvalidation loss\u201d\nbefore printing. No plots are generated, and everything runs immediately on\nexecution without requiring a special entry point.", "The script loads the saved NumPy dictionary from the \u201cworking\u201d directory,\nretrieves the metrics for every dataset present (in this example, just \u201cSPR\u201d),\nand then prints the final values recorded for training, validation, and test\nperformance. It follows the exact key structure used when the data were stored,\nensuring that each metric is clearly labeled and associated with its dataset.", "The script first locates the working directory, loads the saved NumPy\ndictionary, and then iterates through each dataset it contains. For every\ndataset, it prints the dataset\u2019s name, followed by the best (max for F1-like\nmetrics, min for losses) value found in each recorded list. Metric names are\nexpanded into clear labels such as \u201ctraining macro F1\u201d or \u201cvalidation loss\u201d\nbefore printing. No plots are generated, and everything runs immediately on\nexecution without requiring a special entry point.", "The script first locates the working directory, loads the saved NumPy\ndictionary, and then iterates through each dataset it contains. For every\ndataset, it prints the dataset\u2019s name, followed by the best (max for F1-like\nmetrics, min for losses) value found in each recorded list. Metric names are\nexpanded into clear labels such as \u201ctraining macro F1\u201d or \u201cvalidation loss\u201d\nbefore printing. No plots are generated, and everything runs immediately on\nexecution without requiring a special entry point.", "The script first locates the working directory, loads the saved NumPy\ndictionary, and then iterates through each dataset it contains. For every\ndataset, it prints the dataset\u2019s name, followed by the best (max for F1-like\nmetrics, min for losses) value found in each recorded list. Metric names are\nexpanded into clear labels such as \u201ctraining macro F1\u201d or \u201cvalidation loss\u201d\nbefore printing. No plots are generated, and everything runs immediately on\nexecution without requiring a special entry point.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ------------------------------------------------------------------\n# Locate and load stored experiment results\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"Could not find {data_file}\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# Iterate over datasets and print final metrics\n# ------------------------------------------------------------------\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # Retrieve metric/loss lists\n    train_losses = dataset_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset_dict.get(\"losses\", {}).get(\"val\", [])\n    train_f1s = dataset_dict.get(\"metrics\", {}).get(\"train_macro_f1\", [])\n    val_f1s = dataset_dict.get(\"metrics\", {}).get(\"val_macro_f1\", [])\n\n    # Print final (last epoch) values if available\n    if train_losses:\n        print(f\"train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"validation loss: {val_losses[-1]:.4f}\")\n    if train_f1s:\n        print(f\"train macro F1 score: {train_f1s[-1]:.4f}\")\n    if val_f1s:\n        print(f\"validation macro F1 score: {val_f1s[-1]:.4f}\")\n\n    # Compute and print test-set macro F1 score\n    preds = dataset_dict.get(\"predictions\", [])\n    gts = dataset_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n        print(f\"test macro F1 score: {test_macro_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ LOAD DATA\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------ FORMAT HELPERS\ndef prettify(name: str, suffix: str = \"\") -> str:\n    \"\"\"\n    Convert keys like 'train_f1' or 'val' into readable labels.\n    Examples:\n        'train_f1' -> 'training macro F1'\n        'val'      -> 'validation'\n    \"\"\"\n    # map short prefixes to full words\n    mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"test\"}\n    parts = name.split(\"_\")\n    # first part (train / val / test) if present\n    if parts[0] in mapping:\n        prefix = mapping[parts[0]]\n        rest = parts[1:]  # could be ['f1'] or []\n    else:\n        prefix = parts[0]\n        rest = parts[1:]\n    # build remainder\n    remainder = \" \".join(rest)\n    if remainder:\n        label = f\"{prefix} {remainder}\"\n    else:\n        label = prefix\n    # attach optional suffix\n    if suffix:\n        label = f\"{label} {suffix}\"\n    return label\n\n\n# ------------------------------------------------------------------ PRINT METRICS\nfor dataset_name, details in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Handle standard scalar metrics\n    metrics_dict = details.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if not values:\n            continue\n        # Assume larger is better for F1 / accuracy style metrics\n        best_val = max(values)\n        label = prettify(metric_key)\n        print(f\"{label}: {best_val:.4f}\")\n\n    # Handle losses\n    losses_dict = details.get(\"losses\", {})\n    for loss_key, values in losses_dict.items():\n        if not values:\n            continue\n        # Smaller is better for loss\n        best_val = min(values)\n        # Explicitly call it loss in the label\n        label = prettify(loss_key, \"loss\")\n        print(f\"{label}: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 1. Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# 2. Iterate through datasets and print final / best metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Training / validation history\n    train_f1_history = data[\"metrics\"][\"train_f1\"]\n    val_f1_history = data[\"metrics\"][\"val_f1\"]\n    train_loss_history = data[\"losses\"][\"train\"]\n    val_loss_history = data[\"losses\"][\"val\"]\n\n    # Final values (last epoch)\n    final_train_f1 = train_f1_history[-1] if train_f1_history else None\n    final_val_f1 = val_f1_history[-1] if val_f1_history else None\n    final_train_loss = train_loss_history[-1] if train_loss_history else None\n    final_val_loss = val_loss_history[-1] if val_loss_history else None\n\n    # Best validation metrics\n    best_val_f1 = max(val_f1_history) if val_f1_history else None\n    best_val_loss = min(val_loss_history) if val_loss_history else None\n\n    # Test metrics (single values)\n    test_macro_f1 = data.get(\"test_macro_f1\")\n    test_loss = data.get(\"test_loss\")\n\n    # ------------------------------------------------------------------\n    # 3. Print metrics with explicit labels\n    if final_train_f1 is not None:\n        print(f\"final training macro F1 score: {final_train_f1:.4f}\")\n    if final_val_f1 is not None:\n        print(f\"final validation macro F1 score: {final_val_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"best validation macro F1 score:  {best_val_f1:.4f}\")\n\n    if final_train_loss is not None:\n        print(f\"final training loss:            {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"final validation loss:           {final_val_loss:.4f}\")\n    if best_val_loss is not None:\n        print(f\"best validation loss:            {best_val_loss:.4f}\")\n\n    if test_macro_f1 is not None:\n        print(f\"test macro F1 score:             {test_macro_f1:.4f}\")\n    if test_loss is not None:\n        print(f\"test loss:                       {test_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ LOAD DATA\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------ FORMAT HELPERS\ndef prettify(name: str, suffix: str = \"\") -> str:\n    \"\"\"\n    Convert keys like 'train_f1' or 'val' into readable labels.\n    Examples:\n        'train_f1' -> 'training macro F1'\n        'val'      -> 'validation'\n    \"\"\"\n    # map short prefixes to full words\n    mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"test\"}\n    parts = name.split(\"_\")\n    # first part (train / val / test) if present\n    if parts[0] in mapping:\n        prefix = mapping[parts[0]]\n        rest = parts[1:]  # could be ['f1'] or []\n    else:\n        prefix = parts[0]\n        rest = parts[1:]\n    # build remainder\n    remainder = \" \".join(rest)\n    if remainder:\n        label = f\"{prefix} {remainder}\"\n    else:\n        label = prefix\n    # attach optional suffix\n    if suffix:\n        label = f\"{label} {suffix}\"\n    return label\n\n\n# ------------------------------------------------------------------ PRINT METRICS\nfor dataset_name, details in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Handle standard scalar metrics\n    metrics_dict = details.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if not values:\n            continue\n        # Assume larger is better for F1 / accuracy style metrics\n        best_val = max(values)\n        label = prettify(metric_key)\n        print(f\"{label}: {best_val:.4f}\")\n\n    # Handle losses\n    losses_dict = details.get(\"losses\", {})\n    for loss_key, values in losses_dict.items():\n        if not values:\n            continue\n        # Smaller is better for loss\n        best_val = min(values)\n        # Explicitly call it loss in the label\n        label = prettify(loss_key, \"loss\")\n        print(f\"{label}: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ LOAD DATA\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------ FORMAT HELPERS\ndef prettify(name: str, suffix: str = \"\") -> str:\n    \"\"\"\n    Convert keys like 'train_f1' or 'val' into readable labels.\n    Examples:\n        'train_f1' -> 'training macro F1'\n        'val'      -> 'validation'\n    \"\"\"\n    # map short prefixes to full words\n    mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"test\"}\n    parts = name.split(\"_\")\n    # first part (train / val / test) if present\n    if parts[0] in mapping:\n        prefix = mapping[parts[0]]\n        rest = parts[1:]  # could be ['f1'] or []\n    else:\n        prefix = parts[0]\n        rest = parts[1:]\n    # build remainder\n    remainder = \" \".join(rest)\n    if remainder:\n        label = f\"{prefix} {remainder}\"\n    else:\n        label = prefix\n    # attach optional suffix\n    if suffix:\n        label = f\"{label} {suffix}\"\n    return label\n\n\n# ------------------------------------------------------------------ PRINT METRICS\nfor dataset_name, details in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Handle standard scalar metrics\n    metrics_dict = details.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if not values:\n            continue\n        # Assume larger is better for F1 / accuracy style metrics\n        best_val = max(values)\n        label = prettify(metric_key)\n        print(f\"{label}: {best_val:.4f}\")\n\n    # Handle losses\n    losses_dict = details.get(\"losses\", {})\n    for loss_key, values in losses_dict.items():\n        if not values:\n            continue\n        # Smaller is better for loss\n        best_val = min(values)\n        # Explicitly call it loss in the label\n        label = prettify(loss_key, \"loss\")\n        print(f\"{label}: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------ LOAD DATA\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------ FORMAT HELPERS\ndef prettify(name: str, suffix: str = \"\") -> str:\n    \"\"\"\n    Convert keys like 'train_f1' or 'val' into readable labels.\n    Examples:\n        'train_f1' -> 'training macro F1'\n        'val'      -> 'validation'\n    \"\"\"\n    # map short prefixes to full words\n    mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"test\"}\n    parts = name.split(\"_\")\n    # first part (train / val / test) if present\n    if parts[0] in mapping:\n        prefix = mapping[parts[0]]\n        rest = parts[1:]  # could be ['f1'] or []\n    else:\n        prefix = parts[0]\n        rest = parts[1:]\n    # build remainder\n    remainder = \" \".join(rest)\n    if remainder:\n        label = f\"{prefix} {remainder}\"\n    else:\n        label = prefix\n    # attach optional suffix\n    if suffix:\n        label = f\"{label} {suffix}\"\n    return label\n\n\n# ------------------------------------------------------------------ PRINT METRICS\nfor dataset_name, details in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Handle standard scalar metrics\n    metrics_dict = details.get(\"metrics\", {})\n    for metric_key, values in metrics_dict.items():\n        if not values:\n            continue\n        # Assume larger is better for F1 / accuracy style metrics\n        best_val = max(values)\n        label = prettify(metric_key)\n        print(f\"{label}: {best_val:.4f}\")\n\n    # Handle losses\n    losses_dict = details.get(\"losses\", {})\n    for loss_key, values in losses_dict.items():\n        if not values:\n            continue\n        # Smaller is better for loss\n        best_val = min(values)\n        # Explicitly call it loss in the label\n        label = prettify(loss_key, \"loss\")\n        print(f\"{label}: {best_val:.4f}\")\n", ""], "parse_term_out": ["", "['SPR_BENCH', '\\n', 'train loss: 0.0433', '\\n', 'validation loss: 2.1197', '\\n',\n'train macro F1 score: 0.9905', '\\n', 'validation macro F1 score: 0.6960', '\\n',\n'test macro F1 score: 0.6959', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'training f1: 0.9935', '\\n', 'validation f1:\n0.6960', '\\n', 'training loss: 0.0305', '\\n', 'validation loss: 0.6439', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR', '\\n', 'final training macro F1 score: 0.2088', '\\n', 'final\nvalidation macro F1 score: 0.1590', '\\n', 'best validation macro F1 score:\n0.1757', '\\n', 'final training loss:            1.6078', '\\n', 'final validation\nloss:           1.6115', '\\n', 'best validation loss:            1.6115', '\\n',\n'test macro F1 score:             0.1525', '\\n', 'test loss:\n1.6170', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'training f1: 0.9915', '\\n', 'validation f1:\n0.6900', '\\n', 'training loss: 0.0328', '\\n', 'validation loss: 0.6946', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'training f1: 0.9965', '\\n', 'validation f1:\n0.7000', '\\n', 'training loss: 0.0144', '\\n', 'validation loss: 0.6488', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'training f1: 0.9950', '\\n', 'validation f1:\n0.6960', '\\n', 'training loss: 0.0206', '\\n', 'validation loss: 0.6797', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
