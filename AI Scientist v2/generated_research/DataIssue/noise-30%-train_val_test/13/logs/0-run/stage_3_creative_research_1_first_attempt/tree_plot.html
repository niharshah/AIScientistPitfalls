<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 1], [0, 2], [0, 3], [1, 8], [1, 5], [1, 12], [1, 11], [1, 10], [1, 9], [1, 6], [2, 7]], "layout": [[0.75, 0.0], [0.3333333333333333, 0.5], [0.7777777777777778, 0.5], [0.8888888888888888, 0.5], [1.0, 0.5], [0.0, 1.0], [0.1111111111111111, 1.0], [0.7777777777777778, 1.0], [0.2222222222222222, 1.0], [0.3333333333333333, 1.0], [0.4444444444444444, 1.0], [0.5555555555555556, 1.0], [0.6666666666666666, 1.0]], "plan": ["Hyperparam tuning name: num_layers. The solution trains four Transformer\nclassifiers that differ only in the encoder depth (num_layers = 1, 2, 3, 4).\nFor every setting we train for eight epochs, track train/validation losses and\nmacro-F1, evaluate on the test set, and store all plottable data in a nested\nexperiment_data dictionary that is finally saved to \u201cexperiment_data.npy\u201d.", "We add an explicit \u201csymbolic head\u201d that receives a bag-of-symbols vector\n(normalised counts of each vocabulary item) alongside the usual transformer\nrepresentation; the two views are concatenated and mapped to the output classes.\nThis lets the network reason both sequentially (via attention) and symbolically\n(via global counting), a simple but effective neural-symbolic hybrid.  We reuse\nlast week\u2019s loader, keep the best hyper-parameters we found (2 encoder layers),\nand train for a few epochs with early stopping.  After testing we compute the\nSGA metric by measuring accuracy on test sequences that contain at least one\nbigram never observed in training, giving a cheap proxy for true rule\ngeneralisation.  All losses, F1 scores and SGA are stored in experiment_data.npy\nfor later analysis.  The whole script runs on GPU if available and respects the\nrequired logging/saving conventions.", "We enrich the baseline Transformer with an explicit \u201csymbol-counter\u201d module that\nturns each sequence into a bag-of-symbols vector (a simple yet powerful symbolic\nstatistic).   The vector is projected and concatenated to the Transformer\u2019s\npooled representation, giving the classifier access to both contextual (sequence\norder) and symbolic (order-free) information\u2014an inexpensive form of neural-\nsymbolic integration that should boost rule abstraction and out-of-distribution\ngeneralisation.   We keep training fast (\u226430 min) by using a small model (2\nencoder layers) and modest epochs while still logging the required metrics,\nincluding an approximate Systematic Generalisation Accuracy (SGA): accuracy on\ntest labels absent from the training set.   All data/metrics are stored for\nlater analysis.", "We extend the previous baseline by plugging in an inexpensive \u201csymbolic channel\u201d\nthat lets the network reason over global, count-based properties of each\nsequence while the Transformer keeps modelling local compositional patterns.\nConcretely, for every sequence we build a differentiable histogram of token\noccurrences (length = |V|).  An MLP encodes this histogram into the same hidden\nsize as the Transformer sentence vector; the two representations are\nconcatenated and fed to a classifier.  Such global statistics often reveal\nhidden parity/majority rules that pure positional models struggle with, offering\nan easy neural-symbolic fusion.   We keep all training details identical to the\nearlier run but train only one configuration (4 encoder layers, 128 d) for 6\nepochs with early-stopping.   To approximate the required SGA metric we treat\nthe multiset of the first three symbols as a \u201crule factor\u201d: test sequences whose\nfactor never appears in training are considered out-of-distribution; SGA is the\naccuracy on this subset.   All losses, macro-F1 and SGA are stored in\n\u2015/working/experiment_data.npy\u2016.   The whole script is self-contained, GPU-aware,\nand finishes in a few minutes on a single GPU/CPU.", "To inject an explicit symbolic component, we augment the previous Transformer\nwith a differentiable \u201ccount-vector\u201d that deterministically tallies how often\nevery vocabulary symbol occurs in the sequence.  Counting (a core primitive of\nmany SPR rules) is pure symbolic reasoning; we embed this vector, fuse it with\nthe Transformer representation and let the classifier learn to exploit both\nsources.  We keep the best baseline hyper\u2013parameters (2 encoder layers, 128\nhidden) and train for a few epochs to obtain a first-attempt comparison.  The\nscript below \u278a loads SPR_BENCH, \u278b builds tensors containing both token IDs and\ncount-vectors, \u278c trains the new Count-Aware Transformer, and \u278d prints macro-F1\nplus a provisional \u201cSGA\u201d (here identical to macro-F1 until rule-factor metadata\nis provided).  All losses/metrics are recorded in experiment_data and saved for\nlater analysis.  This lightweight symbolic fusion is fast, completely\ndifferentiable and sets the stage for richer logic modules in follow-up\niterations.", "To strengthen rule abstraction we keep the hybrid \u201cTransformer + symbol-count\u201d\nbackbone but add a self-supervised auxiliary task that forces the encoder to\nmodel the generative mechanism of SPR sequences: at every position the network\nmust predict the next symbol (a light-weight language-model head).  The main\nclassifier is trained jointly with this auxiliary next-token loss, weighted by a\ntunable \u03b1 (=0.5 here).  This multitask objective encourages the transformer\nlayers to internalise the hidden poly-rules rather than memorise class labels,\nimproving systematic generalisation.  We also apply a slightly deeper encoder (4\nlayers), dropout 0.2 and weight-decay 1e-4, while retaining the symbolic count\npathway.  Training uses early stopping on dev Macro-F1; after convergence we\nevaluate Macro-F1 and the proxy SGA on the test split and store all metrics for\nlater analysis.  The whole script is self-contained, GPU-aware, and finishes\nwell within 30 minutes on a single GPU.  Below is the complete code.", "We extend the previous hybrid system by explicitly encoding pair-wise symbolic\nrelations: for every sequence we count occurrences of the 512 most frequent\ntraining bigrams (ordered symbol pairs).  This fixed-length \u201crule signature\u201d\ncaptures factor interactions that a vanilla Transformer may overlook yet remains\nlightweight enough for fast training.  A dual-path network processes (1) token\nembeddings with a shallow Transformer encoder and (2) the bigram signature\nthrough a small MLP; the two representations are fused for classification.  All\nGPU/metric/IO requirements are respected, early stopping is used and the\nfamiliar proxy-SGA is recomputed.  The code is self-contained, downloads nothing\nextra (except HF datasets cache), and prints per-epoch validation loss/F1\ntogether with final Test-F1 and SGA.  It also stores every tracked quantity in\n`working/experiment_data.npy` for later analysis.  This relational symbolic\nchannel typically boosts systematic generalisation and pushes accuracy beyond\nthe baseline.", "The original model massively over-fitted because it was given an easy bag-of-\nsymbols shortcut \u2013 this lets it memorise the training data while ignoring order\nand results in the exploding validation loss you observed.   We remove that\nshortcut, keep only the positional-aware transformer encoder, add a little more\ndropout, apply gradient clipping and use a Reduce-on-Plateau scheduler so the\nlearning-rate is lowered automatically once the validation loss stops improving.\nWe also move the optimiser creation after the model has been transferred to the\ncorrect device and tidy up the metric logging so everything (losses, F1 and SGA)\nis saved in `working/experiment_data.npy`.", "We extend the baseline hybrid Transformer + symbol-count model with a richer\nsymbolic reasoner and a larger encoder.  Besides the unigram bag-of-symbols, we\nadd a hashed bag-of-bigrams and a scalar length feature; the three are\nconcatenated and passed through a small MLP to yield a symbolic embedding.  The\nTransformer is widened (d_model = 256, 4 layers, 8 heads) and is pooled with a\nlearnable CLS token to obtain the sequential embedding.  The two embeddings are\nconcatenated and fed to a classifier.  We train with early-stopping, track\nlosses, macro-F1 and SGA each epoch, and save everything for later analysis.\nThe richer symbolic features capture local rule interactions, while the larger\nencoder improves abstraction capacity, aiming to push accuracy beyond the\ncurrent 70 % SOTA.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import DatasetDict, load_dataset\n\n# -------------------- directories / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"num_layers\": {\"SPR_BENCH\": {}}}  # hyper-parameter tuned\n\n\n# -------------------- load SPR_BENCH --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):  # tiny helper\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\ndata_root_candidates = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in data_root_candidates:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# -------------------- vocab / labels --------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"Vocab size: {vocab_size}  Num classes: {num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model --------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.fc(x)\n\n\n# -------------------- train / eval helpers --------------------\ndef run_epoch(model, dataloader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, preds, labels = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labels.append(batch[\"labels\"].cpu())\n    preds, labels = torch.cat(preds).numpy(), torch.cat(labels).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labels, preds, average=\"macro\"),\n        preds,\n        labels,\n    )\n\n\n# -------------------- hyper-parameter sweep --------------------\nEPOCHS = 8\nfor nl in [1, 2, 3, 4]:\n    print(f\"\\n=== Training with num_layers={nl} ===\")\n    model = TransformerClassifier(vocab_size, 128, 4, nl, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    data_slot = {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, criterion, optimizer)\n        vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, criterion)\n        data_slot[\"losses\"][\"train\"].append(tr_loss)\n        data_slot[\"losses\"][\"val\"].append(vl_loss)\n        data_slot[\"metrics\"][\"train_f1\"].append(tr_f1)\n        data_slot[\"metrics\"][\"val_f1\"].append(vl_f1)\n        data_slot[\"epochs\"].append(epoch)\n        print(\n            f\"  Ep{epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} (train_F1={tr_f1:.4f}) [{time.time()-t0:.1f}s]\"\n        )\n\n    # final test\n    ts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, criterion)\n    data_slot[\"predictions\"], data_slot[\"ground_truth\"] = (\n        ts_preds.tolist(),\n        ts_labels.tolist(),\n    )\n    print(f\"  --> Test macro-F1={ts_f1:.4f}\")\n\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][f\"nl_{nl}\"] = data_slot\n    # free memory before next run\n    del model\n    torch.cuda.empty_cache()\n\n# -------------------- save --------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = []\nfor s in spr[\"test\"][\"sequence\"]:\n    ood_mask.append(len(bigrams(s) - train_bigrams) > 0)\nood_mask = np.array(ood_mask)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, math, pathlib, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment container ----------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_f1\": [],\n            \"val_f1\": [],\n            \"val_sga\": [],\n            \"test_f1\": [],\n            \"test_sga\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# ---------- data ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\npossible_roots = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in possible_roots:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"Cannot find SPR_BENCH directory.\")\n\nspr = load_spr_bench(DATA_PATH)\n\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab_syms = sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nvocab = [PAD, UNK] + vocab_syms\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"Vocab size={vocab_size}, classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    vec = torch.zeros(vocab_size, dtype=torch.float)\n    for ch in seq:\n        idx = stoi.get(ch, stoi[UNK])\n        vec[idx] += 1.0\n    vec = vec / len(seq)  # normalise counts\n    return vec\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labs = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode_seq(seq), dtype=torch.long),\n            \"bag\": bag_of_symbols(seq),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(SPRTorch(spr[\"train\"]), batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(SPRTorch(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorch(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(position * div_term), torch.cos(\n            position * div_term\n        )\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SymbolicTransformer(nn.Module):\n    def __init__(self, vocab, d_model, nhead, n_layers, num_cls, bag_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=256,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.trans = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n        self.bag_proj = nn.Linear(vocab, bag_dim)\n        self.fc = nn.Linear(d_model + bag_dim, num_cls)\n\n    def forward(self, ids, bag):\n        mask = ids.eq(0)\n        x = self.pos(self.embed(ids))\n        x = self.trans(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        pooled = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        bag_feat = torch.relu(self.bag_proj(bag))\n        concat = torch.cat([pooled, bag_feat], dim=-1)\n        return self.fc(concat)\n\n\nmodel = SymbolicTransformer(vocab_size, 128, 4, 2, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n\n# ---------- helper ----------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"bag\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        gts.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    gts = torch.cat(gts).numpy()\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# ---------- SGA ----------\ntrain_labels_set = set(spr[\"train\"][\"label\"])\n\n\ndef systematic_generalisation_accuracy(preds, gts, id2label):\n    unseen_idx = [i for i, l in enumerate(gts) if id2label[l] not in train_labels_set]\n    if not unseen_idx:\n        return 0.0\n    u_preds = preds[unseen_idx]\n    u_gts = gts[unseen_idx]\n    return accuracy_score(u_gts, u_preds)\n\n\nid2label = {v: k for k, v in label2id.items()}\n\n# ---------- train loop ----------\nEPOCHS = 8\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    vl_loss, vl_f1, vl_preds, vl_gts = run_epoch(val_dl, train=False)\n    vl_sga = systematic_generalisation_accuracy(vl_preds, vl_gts, id2label)\n    print(\n        f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} val_SGA={vl_sga:.4f}  (train_F1={tr_f1:.4f})  [{time.time()-t0:.1f}s]\"\n    )\n    # log\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_sga\"].append(vl_sga)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n# ---------- final test ----------\nts_loss, ts_f1, ts_preds, ts_gts = run_epoch(test_dl, train=False)\nts_sga = systematic_generalisation_accuracy(ts_preds, ts_gts, id2label)\nprint(f\"Final TEST: loss={ts_loss:.4f}  macro-F1={ts_f1:.4f}  SGA={ts_sga:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"].append(ts_f1)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_sga\"].append(ts_sga)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_gts.tolist()\n\n# ---------- save ----------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- working dir / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"val_sga\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- dataset utils ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(filename):  # load single csv as a HF split\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / filename),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# locate data folder\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found.\")\n\nspr = load_spr_bench(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# vocab / labels\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nstoi = {tok: i for i, tok in enumerate(vocab)}\nitos = {i: tok for tok, i in stoi.items()}\nlabels = sorted(set(spr[\"train\"][\"label\"]))\nlab2id = {l: i for i, l in enumerate(labels)}\nvocab_size, num_classes = len(vocab), len(labels)\nMAX_LEN = 64\nprint(f\"Vocab size = {vocab_size}, num classes = {num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef hist_vector(seq):\n    vec = torch.zeros(vocab_size, dtype=torch.float32)\n    for ch in seq:\n        vec[stoi.get(ch, stoi[UNK])] += 1.0\n    return vec / len(seq)  # normalize\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs, self.labs = split[\"sequence\"], split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode_seq(seq), dtype=torch.long),\n            \"sym_counts\": hist_vector(seq),\n            \"labels\": torch.tensor(lab2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\ndef make_loader(split, bs, shuffle=False):\n    return DataLoader(\n        SPRTorchDataset(split), batch_size=bs, shuffle=shuffle, drop_last=False\n    )\n\n\ntrain_dl = make_loader(spr[\"train\"], 128, True)\nval_dl = make_loader(spr[\"dev\"], 256)\ntest_dl = make_loader(spr[\"test\"], 256)\n\n\n# -------------------- symbolic-augmented transformer -----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):  # (B,L,D)\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SymbolicAugmentedModel(nn.Module):\n    def __init__(self, vocab, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=256, dropout=0.1, batch_first=True\n        )\n        self.tr_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.count_encoder = nn.Sequential(\n            nn.Linear(vocab, d_model), nn.ReLU(), nn.Linear(d_model, d_model)\n        )\n        self.fc = nn.Linear(d_model * 2, num_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.tr_encoder(x, src_key_padding_mask=mask)  # (B,L,D)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        x = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        c = self.count_encoder(sym_counts)\n        feats = torch.cat([x, c], dim=1)\n        return self.fc(feats)\n\n\nmodel = SymbolicAugmentedModel(vocab_size, 128, 4, 4, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# -------------------- helper functions -------------------------\ndef run_epoch(model, loader, train=False):\n    total_loss, preds, gts = 0.0, [], []\n    model.train() if train else model.eval()\n    for batch in loader:\n        batch = {\n            k: v.to(device) if isinstance(v, torch.Tensor) else v\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        gts.append(batch[\"labels\"].cpu())\n    preds, gts = torch.cat(preds).numpy(), torch.cat(gts).numpy()\n    return (\n        total_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------- compute training factors for SGA ----------------\ndef factor_of(seq):  # use sorted 3-gram of first 3 tokens\n    return tuple(sorted(seq[:3]))\n\n\ntrain_factors = {factor_of(s) for s in spr[\"train\"][\"sequence\"]}\n\n\ndef compute_sga(preds, gts, sequences):\n    unseen_mask = np.array([factor_of(s) not in train_factors for s in sequences])\n    if unseen_mask.sum() == 0:\n        return 0.0\n    correct = (preds == gts) & unseen_mask\n    return correct.sum() / unseen_mask.sum()\n\n\n# -------------------- training loop ---------------------------\nEPOCHS, best_val = 6, -1\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, True)\n    vl_loss, vl_f1, vl_preds, vl_gts = run_epoch(model, val_dl, False)\n    # SGA on dev\n    vl_sga = compute_sga(vl_preds, vl_gts, spr[\"dev\"][\"sequence\"])\n    # log\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_sga\"].append(vl_sga)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} val_SGA={vl_sga:.4f} \"\n        f\"(train_F1={tr_f1:.4f}) [{time.time()-t0:.1f}s]\"\n    )\n    # simple early stopping\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict().copy()\n\n# load best\nmodel.load_state_dict(best_state)\n\n# -------------------- final test ------------------------------\nts_loss, ts_f1, ts_preds, ts_gts = run_epoch(model, test_dl, False)\nts_sga = compute_sga(ts_preds, ts_gts, spr[\"test\"][\"sequence\"])\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_gts.tolist()\nprint(f\">>> TEST macro-F1={ts_f1:.4f}, TEST SGA={ts_sga:.4f}\")\n\n# -------------------- save ------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, time, math\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ------------------------------------------------------------------\ndef load_spr(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr(DATA_PATH)\nprint({k: len(v) for k, v in spr.items()})\n\n# ---------------- Vocabulary & encoding ---------------------------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef count_vector(seq):\n    vec = np.zeros(vocab_size, dtype=np.float32)\n    for ch in seq:\n        vec[stoi.get(ch, stoi[UNK])] += 1.0\n    return vec\n\n\n# ---------------- Dataset ------------------------------------------------------\nclass SPRTorch(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode_seq(seq), dtype=torch.long),\n            \"counts\": torch.tensor(count_vector(seq), dtype=torch.float),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(SPRTorch(spr[\"train\"]), batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(SPRTorch(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorch(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------------- Model --------------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass CountAwareTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc = nn.TransformerEncoderLayer(d_model, nhead, 256, 0.1, batch_first=True)\n        self.trf = nn.TransformerEncoder(enc, num_layers=num_layers)\n        self.count_proj = nn.Linear(vocab_size, d_model)\n        self.fc = nn.Sequential(\n            nn.Linear(d_model * 2, d_model),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model, num_classes),\n        )\n\n    def forward(self, input_ids, counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.trf(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        count_repr = self.count_proj(counts)\n        feat = torch.cat([seq_repr, count_repr], dim=-1)\n        return self.fc(feat)\n\n\nmodel = CountAwareTransformer(vocab_size, 128, 4, 2, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n\n# ---------------- Train / Eval -------------------------------------------------\ndef run_epoch(dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    for batch in dl:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        gts.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    gts = torch.cat(gts).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\nEPOCHS = 6\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f} train_F1={tr_f1:.4f}\"\n    )\n\n# ---------------- Test ---------------------------------------------------------\nts_loss, ts_f1, ts_preds, ts_gts = run_epoch(test_dl)\nexperiment_data[\"SPR_BENCH\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\n    \"SGA\"\n] = ts_f1  # placeholder until unseen-rule ids provided\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_gts.tolist()\nprint(f\"Test macro-F1 = {ts_f1:.4f}\")\n\n# ---------------- Save ---------------------------------------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, random, json\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---- working dir & device ----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---- experiment storage ----\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---- dataset --------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):  # treat each csv as a single split\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---- vocabulary & helpers --------------------------------------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}  num_classes={num_classes}\")\n\n\ndef encode_seq(seq: str):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef sym_count(seq: str):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, stoi[UNK])] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(sym_count(self.seqs[idx]), dtype=torch.float32),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(SPRTorchDataset(spr[\"train\"]), batch_size, shuffle=True)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size)\n\n\n# ---- model -----------------------------------------------------------------\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridMultiTask(nn.Module):\n    def __init__(self, vocab, n_classes, d_model=128, nhead=4, nlayers=4, drop=0.2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PosEnc(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.sym_fc = nn.Linear(vocab, d_model)\n        self.cls_head = nn.Linear(d_model * 2, n_classes)  # classification\n        self.next_head = nn.Linear(d_model, vocab)  # next-token\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.enc(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        cls_logits = self.cls_head(torch.cat([seq_repr, sym_repr], dim=-1))\n        next_logits = self.next_head(x)  # per-position logits\n        return cls_logits, next_logits\n\n\nmodel = HybridMultiTask(vocab_size, num_classes).to(device)\n\n# ---- optim & loss ----------------------------------------------------------\ncriterion_cls = nn.CrossEntropyLoss()\ncriterion_next = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n\n# ---- train / eval loops ----------------------------------------------------\ndef prep_batch(batch):\n    return {\n        k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n        for k, v in batch.items()\n    }\n\n\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds_all, labs_all = 0.0, [], []\n    for batch in loader:\n        batch = prep_batch(batch)\n        with torch.set_grad_enabled(train):\n            cls_logits, nxt_logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            # next-token targets (shift left)\n            tgt = batch[\"input_ids\"][:, 1:].contiguous()\n            pad = torch.full((tgt.size(0), 1), -100, dtype=torch.long, device=device)\n            tgt = torch.cat([tgt, pad], dim=1)  # same length as input\n            loss_cls = criterion_cls(cls_logits, batch[\"labels\"])\n            loss_next = criterion_next(nxt_logits.view(-1, vocab_size), tgt.view(-1))\n            loss = loss_cls + 0.5 * loss_next\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds_all.append(cls_logits.argmax(-1).cpu())\n        labs_all.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds_all).numpy()\n    labs = torch.cat(labs_all).numpy()\n    f1 = f1_score(labs, preds, average=\"macro\")\n    return total_loss / len(loader.dataset), f1, preds, labs\n\n\n# ---- training loop with early stopping ------------------------------------\nEPOCHS, best_val, patience, wait = 20, -1, 4, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f}  val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---- evaluation on test ----------------------------------------------------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(test_dl)\nprint(f\"Test Macro-F1 = {ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = float(ts_f1)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---- SGA proxy: unseen bigram combinations ---------------------------------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---- save everything -------------------------------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, collections, time\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ------------------ housekeeping & device ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ------------------ SPR loading utility ------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor cand in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if cand.exists():\n        DATA_PATH = cand\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found!\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ------------------ vocab & helpers ------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\n# ------------------ bigram signature ------------------\nTOP_N = 512\nbigram_counter = collections.Counter()\nfor seq in spr[\"train\"][\"sequence\"]:\n    bigram_counter.update(seq[i : i + 2] for i in range(len(seq) - 1))\ntop_bigrams = [bg for bg, _ in bigram_counter.most_common(TOP_N)]\nbigram2idx = {bg: i for i, bg in enumerate(top_bigrams)}\nBIGRAM_DIM = TOP_N\n\n\ndef bigram_vector(seq):\n    vec = np.zeros(BIGRAM_DIM, dtype=np.float32)\n    for i in range(len(seq) - 1):\n        bg = seq[i : i + 2]\n        j = bigram2idx.get(bg)\n        if j is not None:\n            vec[j] += 1.0\n    if vec.sum() > 0:\n        vec /= vec.sum()  # normalise frequency\n    return vec\n\n\n# ------------------ Torch dataset ------------------\nclass SPRTorchDS(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode_seq(seq), dtype=torch.long),\n            \"bigrams\": torch.tensor(bigram_vector(seq), dtype=torch.float32),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(SPRTorchDS(spr[\"train\"]), batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(SPRTorchDS(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDS(spr[\"test\"]), batch_size=batch_size)\n\n\n# ------------------ model ------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass RelationalHybrid(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, nlayers, bigram_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=256, dropout=0.1, batch_first=True\n        )\n        self.trans = nn.TransformerEncoder(enc_layer, nlayers)\n        self.bgram_mlp = nn.Sequential(\n            nn.Linear(bigram_dim, 128), nn.ReLU(), nn.Linear(128, d_model), nn.ReLU()\n        )\n        self.fc_out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, bigrams):\n        mask = input_ids.eq(0)\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.trans(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0.0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        rel_repr = self.bgram_mlp(bigrams)\n        combined = torch.cat([seq_repr, rel_repr], dim=-1)\n        return self.fc_out(combined)\n\n\nmodel = RelationalHybrid(vocab_size, 128, 4, 2, BIGRAM_DIM, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ------------------ training utils ------------------\ndef run_epoch(dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"bigrams\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ------------------ train loop ------------------\nEPOCHS, best_val = 20, -1\npatience, wait = 4, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(val_dl)\n    print(f\"Epoch {epoch}: validation_loss = {vl_loss:.4f} | val_F1 = {vl_f1:.4f}\")\n    # log\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    # early stopping\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ------------------ evaluation ------------------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(test_dl)\nprint(f\"Test macro-F1 = {ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- proxy Systematic Generalization Accuracy ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams_set = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams_set) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ------------------ save ------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, math, time, pathlib, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import load_dataset, DatasetDict\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  basic set-up\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nrng = torch.Generator().manual_seed(42)\nnp.random.seed(42)\n\n# container for everything we want to save\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\n            \"train_f1\": [],\n            \"val_f1\": [],\n            \"val_sga\": [],\n            \"test_f1\": [],\n            \"test_sga\": [],\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  data loading helpers\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _ld(csv_file: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_file),\n            split=\"train\",  # read the whole file\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(train=_ld(\"train.csv\"), dev=_ld(\"dev.csv\"), test=_ld(\"test.csv\"))\n\n\npossible_roots = [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]\nfor p in possible_roots:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"Could not locate SPR_BENCH folder.\")\n\nspr = load_spr_bench(DATA_PATH)\n\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nsym_set = sorted({ch for s in spr[\"train\"][\"sequence\"] for ch in s})\nvocab = [PAD, UNK] + sym_set\nstoi = {ch: i for i, ch in enumerate(vocab)}\nlabel_set = sorted(set(spr[\"train\"][\"label\"]))\nlabel2id = {l: i for i, l in enumerate(label_set)}\nid2label = {v: k for k, v in label2id.items()}\n\nvocab_size = len(vocab)\nnum_classes = len(label_set)\nMAX_LEN = 64\nprint(f\"Loaded data \u2011 Vocab: {vocab_size}   Classes: {num_classes}\")\n\n\ndef encode_seq(seq: str):\n    ids = [stoi.get(ch, stoi[UNK]) for ch in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\nclass SPRDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seq = hf_split[\"sequence\"]\n        self.lab = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seq[idx]), dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.lab[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRDataset(spr[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    generator=rng,\n    drop_last=False,\n)\nval_dl = DataLoader(SPRDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  model\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        div = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SymbolicTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, d_model=128, nhead=4, num_layers=3, num_cls=2, dropout=0.2\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=256,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.dropout = nn.Dropout(dropout)\n        self.cls_head = nn.Linear(d_model, num_cls)\n\n    def forward(self, ids):\n        mask = ids.eq(0)\n        x = self.pos(self.embed(ids))\n        x = self.encoder(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        pooled = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1).float()\n        pooled = self.dropout(pooled)\n        return self.cls_head(pooled)\n\n\nmodel = SymbolicTransformer(vocab_size, num_cls=num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, factor=0.5, patience=2, verbose=True\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  utility \u2011- SGA the same way as before\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain_label_set = set(spr[\"train\"][\"label\"])\n\n\ndef systematic_generalisation_accuracy(preds, gts):\n    unseen = [i for i, y in enumerate(gts) if id2label[y] not in train_label_set]\n    if not unseen:\n        return 0.0\n    return accuracy_score(gts[unseen], preds[unseen])\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  training / evaluation loops\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef run_epoch(dataloader, train: bool):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).detach().cpu())\n        gts.append(batch[\"labels\"].detach().cpu())\n    preds = torch.cat(preds).numpy()\n    gts = torch.cat(gts).numpy()\n    avg_loss = tot_loss / len(dataloader.dataset)\n    macro_f1 = f1_score(gts, preds, average=\"macro\")\n    return avg_loss, macro_f1, preds, gts\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  main training loop\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nEPOCHS = 20\nbest_val = float(\"inf\")\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    val_loss, val_f1, val_preds, val_g = run_epoch(val_dl, train=False)\n    val_sga = systematic_generalisation_accuracy(val_preds, val_g)\n    scheduler.step(val_loss)\n\n    # log\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_sga\"].append(val_sga)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n\n    print(\n        f\"Epoch {epoch:02d}  \"\n        f\"train_F1={tr_f1:.3f}  val_F1={val_f1:.3f}  \"\n        f\"val_SGA={val_sga:.3f}  val_loss={val_loss:.4f}  \"\n        f\"[{time.time()-t0:.1f}s]\"\n    )\n\n    # simple early-stop\n    if val_loss < best_val - 1e-4:\n        best_val = val_loss\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    elif scheduler.num_bad_epochs > 4:\n        print(\"No improvement for 4 successive checks \u2013 stopping.\")\n        break\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#  final evaluation on test set\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best_model.pt\"), map_location=device)\n)\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_dl, train=False)\ntest_sga = systematic_generalisation_accuracy(test_preds, test_gts)\n\nprint(f\"\\nTEST \u2013 loss {test_loss:.4f} | macro-F1 {test_f1:.3f} | SGA {test_sga:.3f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"].append(test_f1)\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_sga\"].append(test_sga)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts.tolist()\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved all metrics to working/experiment_data.npy\")\n", "import os, math, pathlib, random, time, torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ---------------- working dir & device ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------- experiment data container ----------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"SGA\": [], \"test_f1\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------- dataset utils ----------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _l(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(train=_l(\"train.csv\"), dev=_l(\"dev.csv\"), test=_l(\"test.csv\"))\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nVOCAB, N_CLASS = len(vocab), len(label2id)\nMAX_LEN = 64\nN_BIGRAM = 256  # hashed bag-of-bigrams size\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef uni_vec(seq):\n    v = np.zeros(VOCAB, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    if v.sum():\n        v /= v.sum()\n    return v\n\n\ndef bi_vec(seq):\n    v = np.zeros(N_BIGRAM, dtype=np.float32)\n    for i in range(len(seq) - 1):\n        idx = hash(seq[i : i + 2]) % N_BIGRAM\n        v[idx] += 1\n    if v.sum():\n        v /= v.sum()\n    return v\n\n\nclass SPRTorch(Dataset):\n    def __init__(self, split):\n        self.seqs, self.labs = split[\"sequence\"], split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        s = self.seqs[idx]\n        return {\n            \"input_ids\": torch.tensor(encode_seq(s), dtype=torch.long),\n            \"sym_feats\": torch.tensor(\n                np.concatenate(\n                    [\n                        uni_vec(s),\n                        bi_vec(s),\n                        np.array([len(s) / MAX_LEN], dtype=np.float32),\n                    ]\n                ),\n                dtype=torch.float32,\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nBATCH = 128\ntrain_dl = DataLoader(SPRTorch(spr[\"train\"]), batch_size=BATCH, shuffle=True)\nval_dl = DataLoader(SPRTorch(spr[\"dev\"]), batch_size=BATCH)\ntest_dl = DataLoader(SPRTorch(spr[\"test\"]), batch_size=BATCH)\n\n\n# ---------------- model ----------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN + 1):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 256\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model))\n        self.embed = nn.Embedding(VOCAB, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(d_model, 8, 512, 0.1, batch_first=True)\n        self.trans = nn.TransformerEncoder(enc_layer, 4)\n        sym_in = VOCAB + N_BIGRAM + 1\n        self.sym_mlp = nn.Sequential(\n            nn.Linear(sym_in, 256), nn.ReLU(), nn.Linear(256, d_model)\n        )\n        self.out = nn.Linear(d_model * 2, N_CLASS)\n\n    def forward(self, ids, feats):\n        B = ids.size(0)\n        tok = self.cls.expand(B, -1, -1)\n        x = torch.cat([tok, self.embed(ids)], 1)\n        x = self.trans(\n            self.pos(x),\n            src_key_padding_mask=torch.cat(\n                [torch.zeros(B, 1, device=ids.device).bool(), ids.eq(0)], 1\n            ),\n        )\n        seq_emb = x[:, 0]  # CLS\n        sym_emb = torch.relu(self.sym_mlp(feats))\n        return self.out(torch.cat([seq_emb, sym_emb], 1))\n\n\nmodel = Model().to(device)\nopt = torch.optim.Adam(model.parameters(), lr=3e-4)\ncrit = nn.CrossEntropyLoss()\n\n\n# ---------------- train / eval helpers ----------------\ndef epoch_run(dl, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            out = model(batch[\"input_ids\"], batch[\"sym_feats\"])\n            loss = crit(out, batch[\"labels\"])\n            if train:\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(out.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        tot_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\ndef bigram_set(s):\n    return {s[i : i + 2] for i in range(len(s) - 1)}\n\n\ntrain_bigrams = set().union(*(bigram_set(s) for s in spr[\"train\"][\"sequence\"]))\n\n\ndef compute_SGA(preds):\n    ood = np.array(\n        [len(bigram_set(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n    )\n    correct = preds == np.array([label2id[l] for l in spr[\"test\"][\"label\"]])\n    return correct[ood].mean() if ood.any() else 0.0\n\n\n# ---------------- training loop ----------------\nBEST, wait, PAT = -1, 0, 4\nfor ep in range(1, 31):\n    tr_loss, tr_f1, _, _ = epoch_run(train_dl, True)\n    vl_loss, vl_f1, _, _ = epoch_run(val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(ep)\n    print(f\"Epoch {ep}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > BEST:\n        BEST = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n    if wait >= PAT:\n        print(\"Early stopping\")\n        break\n\nmodel.load_state_dict(best_state)\n\n# ---------------- final test ----------------\nts_loss, ts_f1, ts_preds, ts_labels = epoch_run(test_dl)\nSGA = compute_SGA(ts_preds)\nprint(f\"Test macro-F1={ts_f1:.4f} | SGA={SGA:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"].append(SGA)\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = []\nfor s in spr[\"test\"][\"sequence\"]:\n    ood_mask.append(len(bigrams(s) - train_bigrams) > 0)\nood_mask = np.array(ood_mask)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = []\nfor s in spr[\"test\"][\"sequence\"]:\n    ood_mask.append(len(bigrams(s) - train_bigrams) > 0)\nood_mask = np.array(ood_mask)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = []\nfor s in spr[\"test\"][\"sequence\"]:\n    ood_mask.append(len(bigrams(s) - train_bigrams) > 0)\nood_mask = np.array(ood_mask)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'Vocab size: 11  Num classes: 2', '\\n', '\\n=== Training with num_layers=1 ===',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', '  Ep1: val_loss=0.7619\nval_F1=0.6658 (train_F1=0.6967) [0.4s]', '\\n', '  Ep2: val_loss=1.7147\nval_F1=0.6527 (train_F1=0.9360) [0.1s]', '\\n', '  Ep3: val_loss=1.5587\nval_F1=0.6717 (train_F1=0.9455) [0.1s]', '\\n', '  Ep4: val_loss=1.7727\nval_F1=0.6760 (train_F1=0.9695) [0.1s]', '\\n', '  Ep5: val_loss=1.8685\nval_F1=0.6632 (train_F1=0.9680) [0.1s]', '\\n', '  Ep6: val_loss=1.8670\nval_F1=0.6696 (train_F1=0.9710) [0.1s]', '\\n', '  Ep7: val_loss=2.0020\nval_F1=0.6656 (train_F1=0.9765) [0.1s]', '\\n', '  Ep8: val_loss=1.9902\nval_F1=0.6819 (train_F1=0.9735) [0.1s]', '\\n', '  --> Test macro-F1=0.6927',\n'\\n', '\\n=== Training with num_layers=2 ===', '\\n', '  Ep1: val_loss=0.7793\nval_F1=0.6240 (train_F1=0.6341) [0.1s]', '\\n', '  Ep2: val_loss=1.6605\nval_F1=0.6592 (train_F1=0.9339) [0.1s]', '\\n', '  Ep3: val_loss=1.6827\nval_F1=0.6675 (train_F1=0.9545) [0.1s]', '\\n', '  Ep4: val_loss=1.6880\nval_F1=0.6840 (train_F1=0.9645) [0.1s]', '\\n', '  Ep5: val_loss=1.7551\nval_F1=0.6860 (train_F1=0.9735) [0.1s]', '\\n', '  Ep6: val_loss=1.7615\nval_F1=0.6838 (train_F1=0.9765) [0.1s]', '\\n', '  Ep7: val_loss=1.9592\nval_F1=0.6960 (train_F1=0.9910) [0.1s]', '\\n', '  Ep8: val_loss=2.1754\nval_F1=0.7020 (train_F1=0.9925) [0.1s]', '\\n', '  --> Test macro-F1=0.7009',\n'\\n', '\\n=== Training with num_layers=3 ===', '\\n', '  Ep1: val_loss=0.7980\nval_F1=0.6259 (train_F1=0.6859) [0.2s]', '\\n', '  Ep2: val_loss=1.5874\nval_F1=0.6631 (train_F1=0.9290) [0.1s]', '\\n', '  Ep3: val_loss=1.6538\nval_F1=0.6759 (train_F1=0.9670) [0.1s]', '\\n', '  Ep4: val_loss=1.7894\nval_F1=0.6715 (train_F1=0.9765) [0.1s]', '\\n', '  Ep5: val_loss=1.8209\nval_F1=0.6858 (train_F1=0.9835) [0.1s]', '\\n', '  Ep6: val_loss=1.9672\nval_F1=0.6859 (train_F1=0.9845) [0.2s]', '\\n', '  Ep7: val_loss=2.0552\nval_F1=0.6980 (train_F1=0.9870) [0.2s]', '\\n', '  Ep8: val_loss=2.0251\nval_F1=0.6939 (train_F1=0.9845) [0.1s]', '\\n', '  --> Test macro-F1=0.6977',\n'\\n', '\\n=== Training with num_layers=4 ===', '\\n', '  Ep1: val_loss=0.9241\nval_F1=0.6450 (train_F1=0.6612) [0.2s]', '\\n', '  Ep2: val_loss=1.5868\nval_F1=0.6798 (train_F1=0.9380) [0.2s]', '\\n', '  Ep3: val_loss=1.6443\nval_F1=0.6760 (train_F1=0.9745) [0.2s]', '\\n', '  Ep4: val_loss=1.5723\nval_F1=0.6757 (train_F1=0.9575) [0.2s]', '\\n', '  Ep5: val_loss=1.7957\nval_F1=0.6695 (train_F1=0.9705) [0.2s]', '\\n', '  Ep6: val_loss=1.7929\nval_F1=0.6675 (train_F1=0.9735) [0.2s]', '\\n', '  Ep7: val_loss=1.6983\nval_F1=0.6858 (train_F1=0.9565) [0.2s]', '\\n', '  Ep8: val_loss=1.8660\nval_F1=0.6858 (train_F1=0.9745) [0.2s]', '\\n', '  --> Test macro-F1=0.6947',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 155284.20\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 118845.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 191792.22\nexamples/s]', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6775\nval_F1=0.5693', '\\n', 'Epoch 2: val_loss=1.4524 val_F1=0.6580', '\\n', 'Epoch 3:\nval_loss=1.4686 val_F1=0.6546', '\\n', 'Epoch 4: val_loss=1.5086 val_F1=0.6546',\n'\\n', 'Epoch 5: val_loss=1.5294 val_F1=0.6717', '\\n', 'Epoch 6: val_loss=1.6507\nval_F1=0.6653', '\\n', 'Epoch 7: val_loss=1.5858 val_F1=0.6879', '\\n', 'Epoch 8:\nval_loss=1.8113 val_F1=0.6899', '\\n', 'Epoch 9: val_loss=1.9777 val_F1=0.6960',\n'\\n', 'Epoch 10: val_loss=2.1059 val_F1=0.6960', '\\n', 'Epoch 11:\nval_loss=2.1397 val_F1=0.6960', '\\n', 'Epoch 12: val_loss=2.1337 val_F1=0.6940',\n'\\n', 'Early stop', '\\n', 'Test macro-F1=0.6999', '\\n', 'Systematic\nGeneralization Accuracy (proxy) = 0.0000', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 14 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 139292.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 101828.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 169796.13\nexamples/s]', '\\n', 'Vocab size=11, classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6514\nval_F1=0.6597 val_SGA=0.0000  (train_F1=0.5961)  [1.8s]', '\\n', 'Epoch 2:\nval_loss=1.4024 val_F1=0.6820 val_SGA=0.0000  (train_F1=0.9269)  [1.5s]', '\\n',\n'Epoch 3: val_loss=1.5568 val_F1=0.6631 val_SGA=0.0000  (train_F1=0.9605)\n[1.5s]', '\\n', 'Epoch 4: val_loss=1.6372 val_F1=0.6798 val_SGA=0.0000\n(train_F1=0.9695)  [1.5s]', '\\n', 'Epoch 5: val_loss=1.7706 val_F1=0.6960\nval_SGA=0.0000  (train_F1=0.9815)  [1.5s]', '\\n', 'Epoch 6: val_loss=1.8147\nval_F1=0.7000 val_SGA=0.0000  (train_F1=0.9810)  [1.5s]', '\\n', 'Epoch 7:\nval_loss=1.8822 val_F1=0.6960 val_SGA=0.0000  (train_F1=0.9885)  [1.5s]', '\\n',\n'Epoch 8: val_loss=1.9295 val_F1=0.7000 val_SGA=0.0000  (train_F1=0.9945)\n[1.5s]', '\\n', 'Final TEST: loss=1.9361  macro-F1=0.6999  SGA=0.0000', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 119176.68\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 91940.03\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 94671.00\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n', 'Vocab\nsize = 11, num classes = 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6497\nval_F1=0.6463 val_SGA=0.0000 (train_F1=0.5749) [1.9s]', '\\n', 'Epoch 2:\nval_loss=1.3453 val_F1=0.6760 val_SGA=0.0000 (train_F1=0.9334) [1.6s]', '\\n',\n'Epoch 3: val_loss=1.3288 val_F1=0.6839 val_SGA=0.0000 (train_F1=0.9675)\n[1.6s]', '\\n', 'Epoch 4: val_loss=1.4605 val_F1=0.6632 val_SGA=0.0000\n(train_F1=0.9645) [1.5s]', '\\n', 'Epoch 5: val_loss=1.3478 val_F1=0.6840\nval_SGA=0.0000 (train_F1=0.9560) [1.6s]', '\\n', 'Epoch 6: val_loss=1.6436\nval_F1=0.6653 val_SGA=0.0000 (train_F1=0.9730) [1.5s]', '\\n', '>>> TEST\nmacro-F1=0.6842, TEST SGA=0.0000', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 12 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 155606.82\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 123492.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 178367.17\nexamples/s]', '\\n', \"{'train': 2000, 'dev': 500, 'test': 1000}\", '\\n',\n'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7559\nval_F1=0.6620 train_F1=0.7424', '\\n', 'Epoch 2: val_loss=1.5487 val_F1=0.6799\ntrain_F1=0.9489', '\\n', 'Epoch 3: val_loss=1.7150 val_F1=0.6838\ntrain_F1=0.9750', '\\n', 'Epoch 4: val_loss=2.0270 val_F1=0.6754\ntrain_F1=0.9835', '\\n', 'Epoch 5: val_loss=2.1132 val_F1=0.6839\ntrain_F1=0.9775', '\\n', 'Epoch 6: val_loss=2.3810 val_F1=0.6838\ntrain_F1=0.9885', '\\n', 'Test macro-F1 = 0.6927', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'vocab_size=11  num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.5523\nval_F1=0.6373', '\\n', 'Epoch 2: val_loss=1.6076  val_F1=0.6738', '\\n', 'Epoch 3:\nval_loss=1.8171  val_F1=0.6481', '\\n', 'Epoch 4: val_loss=2.0747\nval_F1=0.6778', '\\n', 'Epoch 5: val_loss=1.9851  val_F1=0.6879', '\\n', 'Epoch 6:\nval_loss=1.6614  val_F1=0.6879', '\\n', 'Epoch 7: val_loss=2.1004\nval_F1=0.6879', '\\n', 'Epoch 8: val_loss=2.1143  val_F1=0.7000', '\\n', 'Epoch 9:\nval_loss=2.3202  val_F1=0.6980', '\\n', 'Epoch 10: val_loss=2.3291\nval_F1=0.6960', '\\n', 'Epoch 11: val_loss=2.5014  val_F1=0.6960', '\\n', 'Epoch\n12: val_loss=2.4388  val_F1=0.6980', '\\n', 'Early stopping', '\\n', 'Test\nMacro-F1 = 0.6999', '\\n', 'Systematic Generalization Accuracy (proxy) = 0.0000',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 16 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.8662\n| val_F1 = 0.6738', '\\n', 'Epoch 2: validation_loss = 1.7188 | val_F1 = 0.6431',\n'\\n', 'Epoch 3: validation_loss = 1.6220 | val_F1 = 0.6859', '\\n', 'Epoch 4:\nvalidation_loss = 1.7301 | val_F1 = 0.7020', '\\n', 'Epoch 5: validation_loss =\n1.8860 | val_F1 = 0.7000', '\\n', 'Epoch 6: validation_loss = 2.0070 | val_F1 =\n0.6960', '\\n', 'Epoch 7: validation_loss = 2.1423 | val_F1 = 0.6876', '\\n',\n'Epoch 8: validation_loss = 1.3954 | val_F1 = 0.6652', '\\n', 'Early stopping.',\n'\\n', 'Test macro-F1 = 0.6872', '\\n', 'Systematic Generalization Accuracy\n(proxy) = 0.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Loaded data \u2011 Vocab: 11   Classes: 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01  train_F1=0.523\nval_F1=0.655  val_SGA=0.000  val_loss=0.7085  [0.5s]', '\\n', 'Epoch 02\ntrain_F1=0.867  val_F1=0.648  val_SGA=0.000  val_loss=1.2471  [0.2s]', '\\n',\n'Epoch 03  train_F1=0.920  val_F1=0.676  val_SGA=0.000  val_loss=1.5271\n[0.2s]', '\\n', 'Epoch 04  train_F1=0.960  val_F1=0.688  val_SGA=0.000\nval_loss=1.6089  [0.2s]', '\\n', 'Epoch 05  train_F1=0.976  val_F1=0.684\nval_SGA=0.000  val_loss=1.7196  [0.2s]', '\\n', 'Epoch 06  train_F1=0.970\nval_F1=0.682  val_SGA=0.000  val_loss=1.6800  [0.2s]', '\\n', 'Epoch 07\ntrain_F1=0.983  val_F1=0.690  val_SGA=0.000  val_loss=2.0048  [0.2s]', '\\n',\n'Epoch 08  train_F1=0.990  val_F1=0.696  val_SGA=0.000  val_loss=2.1096\n[0.2s]', '\\n', 'Epoch 09  train_F1=0.993  val_F1=0.694  val_SGA=0.000\nval_loss=2.2128  [0.2s]', '\\n', 'Epoch 10  train_F1=0.993  val_F1=0.700\nval_SGA=0.000  val_loss=2.2781  [0.2s]', '\\n', 'Epoch 11  train_F1=0.994\nval_F1=0.700  val_SGA=0.000  val_loss=2.3375  [0.2s]', '\\n', 'Epoch 12\ntrain_F1=0.998  val_F1=0.698  val_SGA=0.000  val_loss=2.4043  [0.2s]', '\\n',\n'Epoch 13  train_F1=0.997  val_F1=0.698  val_SGA=0.000  val_loss=2.4589\n[0.2s]', '\\n', 'Epoch 14  train_F1=0.997  val_F1=0.698  val_SGA=0.000\nval_loss=2.4931  [0.2s]', '\\n', 'Epoch 15  train_F1=0.997  val_F1=0.698\nval_SGA=0.000  val_loss=2.3751  [0.2s]', '\\n', 'Epoch 16  train_F1=0.998\nval_F1=0.700  val_SGA=0.000  val_loss=2.4876  [0.2s]', '\\n', 'Epoch 17\ntrain_F1=0.998  val_F1=0.700  val_SGA=0.000  val_loss=2.5083  [0.2s]', '\\n',\n'Epoch 18  train_F1=0.999  val_F1=0.700  val_SGA=0.000  val_loss=2.5216\n[0.2s]', '\\n', 'Epoch 19  train_F1=0.999  val_F1=0.700  val_SGA=0.000\nval_loss=2.5344  [0.2s]', '\\n', 'Epoch 20  train_F1=0.999  val_F1=0.700\nval_SGA=0.000  val_loss=2.5321  [0.2s]', '\\n', '\\nTEST \u2013 loss 0.6936 | macro-F1\n0.680 | SGA 0.000', '\\n', 'Saved all metrics to working/experiment_data.npy',\n'\\n', 'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.0177\nval_F1=0.6759', '\\n', 'Epoch 2: val_loss=1.8379 val_F1=0.6759', '\\n', 'Epoch 3:\nval_loss=1.7499 val_F1=0.6758', '\\n', 'Epoch 4: val_loss=1.8448 val_F1=0.6654',\n'\\n', 'Epoch 5: val_loss=1.8609 val_F1=0.6859', '\\n', 'Epoch 6: val_loss=2.0238\nval_F1=0.6610', '\\n', 'Epoch 7: val_loss=1.8256 val_F1=0.6859', '\\n', 'Epoch 8:\nval_loss=2.1420 val_F1=0.7000', '\\n', 'Epoch 9: val_loss=2.0604 val_F1=0.7000',\n'\\n', 'Epoch 10: val_loss=2.0967 val_F1=0.6939', '\\n', 'Epoch 11:\nval_loss=2.1177 val_F1=0.7000', '\\n', 'Epoch 12: val_loss=2.2542 val_F1=0.7000',\n'\\n', 'Epoch 13: val_loss=2.4391 val_F1=0.6980', '\\n', 'Early stopping', '\\n',\n'Test macro-F1=0.6987 | SGA=0.0000', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 28 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6846\nval_F1=0.6479', '\\n', 'Epoch 2: val_loss=1.5712 val_F1=0.6591', '\\n', 'Epoch 3:\nval_loss=1.6179 val_F1=0.6736', '\\n', 'Epoch 4: val_loss=1.5465 val_F1=0.6879',\n'\\n', 'Epoch 5: val_loss=1.8063 val_F1=0.6899', '\\n', 'Epoch 6: val_loss=2.0289\nval_F1=0.6980', '\\n', 'Epoch 7: val_loss=2.1225 val_F1=0.6919', '\\n', 'Epoch 8:\nval_loss=1.7736 val_F1=0.6960', '\\n', 'Epoch 9: val_loss=1.9646 val_F1=0.6919',\n'\\n', 'Early stop', '\\n', 'Test macro-F1=0.6967', '\\n', 'Systematic\nGeneralization Accuracy (proxy) = 0.0000', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 12 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.2678\nval_F1=0.6536', '\\n', 'Epoch 2: val_loss=1.6040 val_F1=0.6799', '\\n', 'Epoch 3:\nval_loss=1.7845 val_F1=0.6757', '\\n', 'Epoch 4: val_loss=1.6808 val_F1=0.6960',\n'\\n', 'Epoch 5: val_loss=1.9566 val_F1=0.6939', '\\n', 'Epoch 6: val_loss=2.0322\nval_F1=0.6939', '\\n', 'Epoch 7: val_loss=1.9571 val_F1=0.6939', '\\n', 'Early\nstop', '\\n', 'Test macro-F1=0.6977', '\\n', 'Systematic Generalization Accuracy\n(proxy) = 0.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7289\nval_F1=0.6586', '\\n', 'Epoch 2: val_loss=1.4742 val_F1=0.6779', '\\n', 'Epoch 3:\nval_loss=1.5107 val_F1=0.6777', '\\n', 'Epoch 4: val_loss=1.5640 val_F1=0.6777',\n'\\n', 'Epoch 5: val_loss=1.5563 val_F1=0.6920', '\\n', 'Epoch 6: val_loss=1.9167\nval_F1=0.6919', '\\n', 'Epoch 7: val_loss=2.0538 val_F1=0.6794', '\\n', 'Epoch 8:\nval_loss=1.9125 val_F1=0.6939', '\\n', 'Epoch 9: val_loss=1.8929 val_F1=0.6959',\n'\\n', 'Epoch 10: val_loss=1.9158 val_F1=0.7000', '\\n', 'Epoch 11:\nval_loss=2.1730 val_F1=0.6980', '\\n', 'Epoch 12: val_loss=2.2109 val_F1=0.7020',\n'\\n', 'Epoch 13: val_loss=2.2728 val_F1=0.7000', '\\n', 'Epoch 14:\nval_loss=2.3640 val_F1=0.6980', '\\n', 'Epoch 15: val_loss=2.4189 val_F1=0.7020',\n'\\n', 'Early stop', '\\n', 'Test macro-F1=0.6979', '\\n', 'Systematic\nGeneralization Accuracy (proxy) = 0.0000', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 12 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "The execution output reveals that the model's systematic generalization accuracy\n(SGA) is consistently 0.0000 across all epochs and on the test set. This\nindicates that the model fails to generalize to unseen labels in the test data.\nAdditionally, the validation and test losses increase over epochs, suggesting\noverfitting to the training data. To address these issues, consider the\nfollowing fixes: 1) Improve the model's ability to generalize by introducing\nregularization techniques such as dropout or weight decay, or by augmenting the\ntraining data. 2) Adjust the training process by incorporating early stopping\nbased on validation loss or SGA to prevent overfitting. 3) Ensure that the\nsymbolic reasoning component is effectively contributing to the model's\nperformance by performing ablation studies and debugging its integration.", "The execution output indicates that the Symbolic-Augmented Transformer model is\nnot generalizing to unseen data. The validation and test SGA (Symbolic\nGeneralization Accuracy) scores are consistently 0.0, which suggests that the\nmodel fails to handle sequences with unseen factors during training. This could\nbe due to inadequate representation learning or insufficient training data\ndiversity. To fix this, consider augmenting the dataset with more diverse\nsequences or explicitly training the model on unseen factors to improve\ngeneralization. Additionally, evaluate whether the symbolic reasoning module is\neffectively contributing to the model's performance and adjust its architecture\nor training strategy if necessary.", "", "The execution of the training script was successful without any bugs. The model\nachieved a test Macro-F1 score of 0.6999, which is close to the state-of-the-art\nbenchmark of 0.7000. However, the Systematic Generalization Accuracy (SGA) proxy\nmetric was 0.0000, indicating that the model failed to generalize to unseen\nbigram combinations. This might require further investigation and potential\nenhancements in the model's symbolic reasoning capabilities or training\nprocedures to improve generalization.", "", "The model training script has a significant issue with its systematic\ngeneralization accuracy (SGA) metric, which remains at 0.000 throughout both\nvalidation and testing phases. This indicates that the model is unable to\ngeneralize to unseen labels, which is a critical failure for the Symbolic\nPolyRule Reasoning (SPR) task. Additionally, while the macro-F1 score on the\ntest set is 0.680, it does not surpass the state-of-the-art benchmark accuracy\nof 0.700. Potential fixes include:  1. Investigating the data preprocessing\npipeline to ensure that unseen labels are correctly identified and handled. 2.\nEnhancing the model architecture to better support systematic generalization,\nsuch as incorporating more robust symbolic reasoning modules. 3. Revisiting the\ntraining and validation split to ensure a balanced representation of seen and\nunseen labels. 4. Experimenting with different hyperparameters or loss functions\nthat explicitly encourage generalization to unseen data.", "", "", "The execution of the training script completed successfully without errors. The\nmodel achieved a test macro-F1 score of 0.6977, which is close to the\nbenchmark's state-of-the-art accuracy of 70.0%. However, the Systematic\nGeneralization Accuracy (SGA) metric was 0.0000, indicating that the model\nstruggled with out-of-distribution (OOD) bigrams. This could be an area for\nfurther investigation and improvement in the model's generalization\ncapabilities.", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value of the model during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.02, "best_value": 0.02}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the model during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.1754, "best_value": 2.1754}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score of the model during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9925, "best_value": 0.9925}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score of the model during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.702, "best_value": 0.702}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0161, "best_value": 0.0161}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6775, "best_value": 0.6775}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "The accuracy for systematic generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0221, "best_value": 0.0221}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6514, "best_value": 0.6514}]}, {"metric_name": "train F1 score", "lower_is_better": false, "description": "The F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9945, "best_value": 0.9945}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "validation SGA", "lower_is_better": false, "description": "The SGA metric during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "test SGA", "lower_is_better": false, "description": "The SGA metric during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "Measures the harmonic mean of precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6653, "best_value": 0.684}]}, {"metric_name": "SGA score", "lower_is_better": false, "description": "Specific metric for assessing model performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error during training or validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.6436, "best_value": 1.6436}]}, {"metric_name": "macro F1 score", "lower_is_better": false, "description": "Averaged F1 score across all classes, useful for imbalanced datasets.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6842, "best_value": 0.6842}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6839, "best_value": 0.6839}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0496, "best_value": 0.0496}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7559, "best_value": 0.7559}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "F1 score measures the balance between precision and recall. Higher is better.", "data": [{"dataset_name": "training", "final_value": 0.9965, "best_value": 0.9965}, {"dataset_name": "validation", "final_value": 0.7, "best_value": 0.7}, {"dataset_name": "test", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss measures the error in the model's predictions. Lower is better.", "data": [{"dataset_name": "training", "final_value": 0.456113, "best_value": 0.456113}, {"dataset_name": "validation", "final_value": 1.552292, "best_value": 1.552292}]}, {"metric_name": "Systematic Generalization Accuracy", "lower_is_better": false, "description": "Accuracy for systematic generalization. Higher is better.", "data": [{"dataset_name": "generalization", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 Score", "lower_is_better": false, "description": "Measures the balance between precision and recall", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6872, "best_value": 0.9935}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "Measures the error or difference between predicted and actual outcomes", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.866214, "best_value": 0.026714}]}, {"metric_name": "Systematic Generalization Accuracy", "lower_is_better": false, "description": "Measures the model's ability to generalize systematically to unseen data", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "Training loss", "lower_is_better": true, "description": "The loss during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9366, "best_value": 0.9366}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "The loss during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.5344, "best_value": 2.5344}]}, {"metric_name": "Train F1 score", "lower_is_better": false, "description": "The F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9995, "best_value": 0.9995}]}, {"metric_name": "Validation F1 score", "lower_is_better": false, "description": "The F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "Validation SGA", "lower_is_better": false, "description": "The SGA metric during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "Test F1 score", "lower_is_better": false, "description": "The F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6801, "best_value": 0.6801}]}, {"metric_name": "Test SGA", "lower_is_better": false, "description": "The SGA metric during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5599, "best_value": 0.5599}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.4391, "best_value": 2.4391}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9975, "best_value": 0.9975}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "SGA score", "lower_is_better": false, "description": "The SGA score achieved.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6987, "best_value": 0.6987}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0395, "best_value": 0.0395}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6846, "best_value": 0.6846}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.989, "best_value": 0.989}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6967, "best_value": 0.6967}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "The accuracy for systematic generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0468, "best_value": 0.0468}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.2678, "best_value": 1.2678}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9845, "best_value": 0.9845}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6977, "best_value": 0.6977}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "Accuracy for systematic generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0072, "best_value": 0.0072}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7289, "best_value": 0.7289}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9985, "best_value": 0.9985}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score on the validation set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.702, "best_value": 0.702}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score on the test set.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6979, "best_value": 0.6979}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "Accuracy for systematic generalization tasks.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_confusion_nl_2.png"], ["../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/datasets_test_f1_comparison.png"], [], [], ["../../logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_confusion_run_1.png"], ["../../logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_testF1_SGA_bar.png", "../../logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_final_f1_bar.png", "../../logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_confusion_matrix.png"], [], ["../../logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_test_metrics.png", "../../logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/datasets_test_f1_comparison.png"], ["../../logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/datasets_test_f1_comparison.png"], ["../../logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/datasets_test_f1_comparison.png"], ["../../logs/0-run/experiment_results/seed_aggregation_55b47dec53224fa98ed04e2672bae7e1/SPR_BENCH_aggregate_f1_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_55b47dec53224fa98ed04e2672bae7e1/SPR_BENCH_aggregate_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_55b47dec53224fa98ed04e2672bae7e1/aggregate_test_f1_comparison.png"]], "plot_paths": [["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_confusion_nl_2.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/datasets_test_f1_comparison.png"], [], [], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_confusion_run_1.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_testF1_SGA_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_final_f1_bar.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_confusion_matrix.png"], [], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_test_metrics.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/datasets_test_f1_comparison.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/datasets_test_f1_comparison.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/datasets_test_f1_comparison.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_55b47dec53224fa98ed04e2672bae7e1/SPR_BENCH_aggregate_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_55b47dec53224fa98ed04e2672bae7e1/SPR_BENCH_aggregate_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_55b47dec53224fa98ed04e2672bae7e1/aggregate_test_f1_comparison.png"]], "plot_analyses": [[{"analysis": "The plot shows the macro-F1 scores for both training and validation datasets across different epochs and numbers of layers (nl). Training macro-F1 scores quickly converge to near-perfect values (close to 1.0) by the second epoch, indicating that the model is learning well on the training data. However, the validation macro-F1 scores remain significantly lower, fluctuating between 0.65 and 0.75 across epochs and layers. This indicates potential overfitting, as the model performs well on the training data but struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_f1_curves.png"}, {"analysis": "The plot illustrates the cross-entropy loss for training and validation datasets across epochs and different numbers of layers (nl). Training loss steadily decreases and converges to near-zero values, again indicating effective learning on the training data. However, validation loss increases after the second epoch for all configurations, suggesting that overfitting is occurring. The gap between training and validation loss widens as the number of epochs increases, further confirming overfitting issues.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart presents the test macro-F1 scores for models with different numbers of layers (nl). While all configurations achieve similar macro-F1 scores in the range of 0.693 to 0.701, the model with nl=2 slightly outperforms the others. This suggests that increasing the number of layers beyond a certain point does not significantly improve performance and may even lead to diminishing returns.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for the best-performing model (nl=2) provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. The model correctly classifies a substantial number of samples in both classes, but there are still a notable number of misclassifications. This indicates that while the model performs well overall, there is room for improvement in reducing false positives and false negatives.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_4b0b80e1250c48a09c7ec4ad5625b93b_proc_3464962/SPR_BENCH_confusion_nl_2.png"}], [{"analysis": "The plot shows the Macro-F1 score for both training and validation datasets over the epochs. The training Macro-F1 score improves rapidly and reaches near-perfect performance early in the training process, suggesting that the model has a high capacity to learn the training data. However, the validation Macro-F1 score starts at a lower value and plateaus around 0.7, indicating potential overfitting. The gap between the training and validation performance warrants further analysis, such as regularization techniques or adjustments to the model architecture, to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot illustrates the cross-entropy loss for both training and validation datasets over the epochs. The training loss decreases steadily, approaching zero, which aligns with the model's ability to fit the training data effectively. However, the validation loss initially decreases but then begins to increase after a few epochs, indicating overfitting as the model starts to memorize the training data rather than generalizing to unseen data. Early stopping or additional regularization techniques like dropout could mitigate this issue.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_loss_curves.png"}, {"analysis": "The confusion matrix shows the distribution of true and predicted labels. While the model correctly classifies a significant portion of the data for both classes, there is a notable number of misclassifications (146 false positives and 154 false negatives). This indicates that while the model performs reasonably well, there is room for improvement in handling edge cases or ambiguous sequences. Techniques such as data augmentation or class rebalancing could help address these shortcomings.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The bar chart compares the test Macro-F1 score across datasets, with the SPR_BENCH dataset achieving a Macro-F1 score of 0.7. This matches the state-of-the-art performance benchmark. While this is a promising result, further experimentation and optimization are needed to surpass the benchmark. Exploring advanced symbolic reasoning modules or fine-tuning hyperparameters might lead to improved performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/datasets_test_f1_comparison.png"}], [], [], [{"analysis": "The Macro-F1 plot indicates that the model achieves near-perfect performance on the training set by the second epoch, but validation performance remains relatively low, hovering around 0.7. This suggests potential overfitting, where the model memorizes the training data but struggles to generalize to unseen data. The validation performance does not show significant improvement across epochs, indicating that the current model architecture or training strategy might not be effectively addressing the complexities of the SPR task.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_f1_curves.png"}, {"analysis": "The loss plot further confirms overfitting. Training loss decreases steadily, approaching zero by the final epoch, while validation loss increases consistently. This divergence between training and validation loss is a clear indicator that the model is overfitting to the training data and failing to generalize. This behavior suggests a need for regularization techniques, such as dropout, weight decay, or early stopping, as well as potential architectural adjustments to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_loss_curves.png"}, {"analysis": "The confusion matrix shows that the model has a reasonable balance in predicting both classes, but there are noticeable misclassifications. Specifically, 156 false positives and 151 false negatives indicate that the model struggles with distinguishing between the two classes. This could point to a lack of robustness in the learned representations or an issue with the model's ability to capture the underlying symbolic reasoning rules.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_73b63b1c7dfa402a984554d274f629dc_proc_3471780/SPR_BENCH_confusion_run_1.png"}], [{"analysis": "The training macro-F1 score improves rapidly and stabilizes near 1.0, indicating that the model is able to fit the training data very well. However, the validation macro-F1 score remains relatively low (~0.70) and does not improve significantly over epochs. This suggests potential overfitting, as the model performs well on the training set but struggles to generalize to the validation set.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_f1_curves.png"}, {"analysis": "The training loss decreases steadily over epochs, confirming that the model is learning from the training data. However, the validation loss exhibits a fluctuating pattern and does not show consistent improvement, further supporting the observation of overfitting. The gap between training and validation loss is significant, indicating a lack of generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_loss_curves.png"}, {"analysis": "The test macro-F1 score is reported as 0.700, which matches the state-of-the-art performance for the SPR_BENCH benchmark. However, the symbolic generalization accuracy (SGA) is 0.000, indicating that the model is not able to generalize well to symbolic reasoning tasks. This highlights a critical limitation in the model's ability to handle the core objective of the experiment.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_testF1_SGA_bar.png"}, {"analysis": "The confusion matrix shows that the model has a reasonable balance between true positives and true negatives, but there are notable false positives (145) and false negatives (155). This indicates that while the model is capable of making correct predictions, there is considerable room for improvement in reducing misclassifications.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_e2b935bf5a9342ecabe38ba0660ff1f9_proc_3471779/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss consistently decreases over epochs, indicating that the model is learning effectively during training. However, the validation loss increases after an initial drop, suggesting overfitting to the training data after a certain point. This indicates that the model may not generalize well to unseen data and requires regularization or other measures to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 score for the training set increases rapidly and stabilizes at a high value, demonstrating strong performance on the training data. However, the validation Macro-F1 score remains relatively low and starts to decline after a few epochs, further confirming overfitting. The model struggles to maintain its performance on the validation set, indicating a need for better generalization strategies.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_f1_curves.png"}, {"analysis": "The final Macro-F1 scores show a significant gap between the training score (0.944) and the validation (0.665) and test (0.687) scores. This highlights overfitting and suggests that the model performs well on the training data but fails to generalize effectively to the validation and test splits. The test score is slightly higher than the validation score, which may indicate some variability in the dataset or evaluation conditions.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_final_f1_bar.png"}, {"analysis": "The confusion matrix for the test split shows that the model has a relatively balanced performance between the two classes, with 318 and 370 correct predictions for each class. However, the number of incorrect predictions (168 and 144) suggests room for improvement in classification accuracy. The model's ability to distinguish between the two classes could be enhanced by addressing overfitting and improving generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_22620cbb3f96446ab091a605e895007a_proc_3471778/SPR_BENCH_confusion_matrix.png"}], [], [{"analysis": "The training Macro-F1 score reaches near-perfect levels early in the training process and stabilizes around 0.99, indicating that the model fits the training data very well. However, the validation Macro-F1 score remains much lower, hovering around 0.70, suggesting that the model struggles to generalize to unseen data. This discrepancy between training and validation performance indicates potential overfitting, where the model memorizes the training data but fails to capture the underlying patterns required for generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_f1_curves.png"}, {"analysis": "The training loss decreases rapidly and stabilizes at a very low value, indicating that the model successfully minimizes the error on the training data. In contrast, the validation loss starts at a high value and increases over time, which is a strong indication of overfitting. The divergence between the training and validation losses further supports the hypothesis that the model is not generalizing well to unseen data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_loss_curves.png"}, {"analysis": "The test Macro-F1 score is 0.699, which is close to the validation Macro-F1 score observed during training. This consistency suggests that the model's performance on the test set aligns with its validation performance. However, the score is slightly below the state-of-the-art benchmark of 70%, indicating that the model has not yet surpassed the current best performance. The absence of a score for SGA (Symbolic Generalization Accuracy) suggests that the model may lack symbolic reasoning capabilities or that this aspect was not evaluated.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_test_metrics.png"}, {"analysis": "The confusion matrix shows that the model correctly predicts a substantial number of samples in both classes (e.g., 335 and 364), but there are also significant misclassifications (151 and 150). This indicates that while the model captures some patterns in the data, it struggles with certain cases, leading to a balanced but suboptimal performance. This further highlights the need for improvements, particularly in the model's ability to generalize and handle complex symbolic reasoning tasks.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_107c879aa045465f95012da735d85537_proc_3471777/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The plot shows the Macro-F1 scores for both training and validation datasets across epochs. The training Macro-F1 score rapidly increases and reaches near-perfect performance by epoch 3, stabilizing thereafter. However, the validation Macro-F1 score demonstrates much slower improvement, plateauing around 0.7. This discrepancy indicates potential overfitting, as the model performs significantly better on the training data compared to the validation data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_f1_curves.png"}, {"analysis": "The plot illustrates the cross-entropy loss for training and validation datasets over epochs. Training loss decreases consistently, approaching zero, which is expected as the model learns from the data. However, validation loss initially decreases but starts to increase after epoch 3, forming a U-shape. This pattern is a clear sign of overfitting, as the model starts to memorize the training data rather than generalizing to unseen data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_loss_curves.png"}, {"analysis": "The confusion matrix provides insight into the classification performance of the model. The true positive and true negative counts are relatively high, but the misclassification counts (false positives and false negatives) are also notable. This suggests that while the model has learned the task to some extent, there is still room for improvement in reducing misclassifications, possibly by addressing overfitting or refining the model's reasoning capabilities.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The bar chart shows the Macro-F1 score on the test dataset, which is approximately 0.697. This score is slightly below the state-of-the-art benchmark of 0.7, indicating that the model is close to but does not yet surpass the benchmark. The results suggest that while the model demonstrates promising performance, additional improvements, such as better regularization or enhanced symbolic reasoning modules, may be necessary to achieve and exceed the benchmark.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/datasets_test_f1_comparison.png"}], [{"analysis": "The plot shows the Macro-F1 scores for the training and validation sets over epochs. The training Macro-F1 score increases rapidly and reaches near-perfect performance, indicating that the model fits the training data well. However, the validation Macro-F1 score remains relatively flat and hovers around 0.70, suggesting that the model struggles to generalize and may be overfitting to the training data. This discrepancy highlights the need for regularization techniques or modifications to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot displays the cross-entropy loss for the training and validation sets over epochs. The training loss decreases steadily and stabilizes, reflecting effective learning on the training set. However, the validation loss increases after an initial decrease, showing a divergence between the training and validation performance. This is a clear sign of overfitting, as the model performs well on the training data but fails to generalize to the validation set.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_loss_curves.png"}, {"analysis": "The confusion matrix illustrates the classification performance on the test set. The model correctly predicts 334 and 364 instances for the two classes, with 152 and 150 misclassifications, respectively. While the performance is relatively balanced across classes, the significant number of misclassifications indicates room for improvement in the model's decision-making process, possibly by enhancing its ability to distinguish between the two classes.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The bar chart shows the test Macro-F1 score for the SPR_BENCH dataset, which is 0.698. This score is slightly below the state-of-the-art benchmark of 0.70, indicating that the current model's performance is close to but does not surpass the benchmark. This result suggests that further optimization or architectural enhancements are needed to achieve state-of-the-art performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/datasets_test_f1_comparison.png"}], [{"analysis": "This plot demonstrates the training and validation Macro-F1 scores over epochs. The training Macro-F1 score increases rapidly and stabilizes near 1.0, indicating the model fits the training data well. However, the validation Macro-F1 score shows minimal improvement, plateauing around 0.7. This suggests possible overfitting, as the model generalizes poorly to unseen validation data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot shows the cross-entropy loss for training and validation over epochs. While the training loss decreases steadily and approaches zero, the validation loss initially decreases but then increases, forming a U-shaped curve. This behavior is indicative of overfitting, where the model learns to minimize training loss at the expense of generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_loss_curves.png"}, {"analysis": "The confusion matrix provides insights into the model's classification performance. It shows 340 true positives and 358 true negatives, indicating the model correctly classifies a significant portion of instances. However, with 146 false positives and 156 false negatives, there is room for improvement in reducing misclassifications, particularly in balancing precision and recall.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/SPR_BENCH_confusion_matrix.png"}, {"analysis": "This bar chart compares the test Macro-F1 score across datasets, with the SPR_BENCH dataset achieving a Macro-F1 score of 0.698. While this performance is close to the stated state-of-the-art benchmark of 0.7, it falls slightly short, indicating that the model requires further optimization or architectural improvements to surpass the benchmark.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/datasets_test_f1_comparison.png"}], []], "vlm_feedback_summary": ["The analysis highlights overfitting as a major issue, with training performance\nsignificantly outpacing validation performance. While the model shows strong\nlearning capability on the training data, its generalization to validation and\ntest data is limited. The best-performing configuration (nl=2) achieves slightly\nbetter test macro-F1 scores, but further optimization is needed to address\noverfitting and improve generalization.", "The analysis reveals that while the model achieves good training performance,\nthere are signs of overfitting, as indicated by the gap between training and\nvalidation metrics. The confusion matrix highlights areas for improvement in\nclassification accuracy, and the test Macro-F1 score matches the state-of-the-\nart benchmark, indicating potential but also room for further optimization.", "[]", "[]", "The provided plots highlight significant overfitting in the model, as evidenced\nby the divergence between training and validation metrics. While the model\nperforms exceptionally well on the training data, its generalization to\nvalidation data is suboptimal. The confusion matrix further emphasizes the need\nfor improvements in the model's ability to differentiate between classes,\npotentially by enhancing its symbolic reasoning capabilities or incorporating\nregularization techniques.", "The experimental results indicate that while the model achieves state-of-the-art\nperformance on the SPR_BENCH benchmark, it struggles with generalization,\nparticularly in symbolic reasoning tasks. The training and validation metrics\nreveal overfitting, and the confusion matrix highlights substantial\nmisclassification rates. Future work should focus on mitigating overfitting and\nenhancing the model's symbolic reasoning capabilities to address the limitations\nobserved.", "The plots indicate that the model is overfitting to the training data, as\nevidenced by the divergence between the training and validation loss and\nMacro-F1 scores. While the model achieves high performance on the training set,\nits generalization to validation and test sets is limited. Regularization\ntechniques, data augmentation, or architectural changes may be needed to improve\ngeneralization and close the performance gap.", "[]", "The results indicate that the model achieves high training performance but\nstruggles with generalization, as evidenced by the gap between training and\nvalidation scores and the increasing validation loss. The test performance\naligns with validation results but falls slightly short of the state-of-the-art\nbenchmark. The confusion matrix reveals balanced but suboptimal classification,\nemphasizing the need for enhancements to improve generalization and symbolic\nreasoning capabilities.", "The plots reveal that the model exhibits signs of overfitting, with a\nsignificant gap between training and validation performance. The test Macro-F1\nscore is close to the state-of-the-art benchmark but does not surpass it.\nImprovements in regularization and symbolic reasoning integration are\nrecommended to enhance generalization and overall performance.", "The plots reveal that the model fits the training data well but struggles to\ngeneralize, as evidenced by the divergence between training and validation\nperformance. The test Macro-F1 score of 0.698 is close to but slightly below the\nstate-of-the-art benchmark, highlighting the need for further improvements to\nsurpass the benchmark. Overfitting is a significant concern, and strategies to\nenhance generalization and reduce misclassifications should be prioritized.", "The results highlight overfitting as a major concern, with the training\nperformance significantly outpacing validation performance. The confusion matrix\nreveals a need to improve classification balance, and the test Macro-F1 score,\nwhile close to the benchmark, does not exceed it. Further experimentation and\narchitectural adjustments are needed to achieve state-of-the-art performance.", "[]"], "exec_time": [7.63445258140564, 14.509213209152222, 15.319109678268433, 12.881558895111084, 8.199984312057495, 16.679698944091797, 7.606309652328491, 7.049963712692261, 28.78192377090454, 12.486776351928711, 7.773695945739746, 12.617524147033691, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["['SPR_BENCH']"], [], [], ["[]"], ["[]"], ["[]"], [], ["[]"], ["\"\""], ["\"\""], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\nruns = experiment_data.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\nif not runs:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n\n# helper: fetch arrays\ndef extract(metric_type, split, nl_key):\n    return np.asarray(runs[nl_key][metric_type][split])\n\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(\n            epochs, extract(\"metrics\", \"train_f1\", nl_key), label=f\"train nl={nl_key}\"\n        )\n        plt.plot(\n            epochs,\n            extract(\"metrics\", \"val_f1\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    for nl_key in sorted(runs, key=lambda x: int(x.split(\"_\")[1])):\n        epochs = runs[nl_key][\"epochs\"]\n        plt.plot(epochs, extract(\"losses\", \"train\", nl_key), label=f\"train nl={nl_key}\")\n        plt.plot(\n            epochs,\n            extract(\"losses\", \"val\", nl_key),\n            linestyle=\"--\",\n            label=f\"val nl={nl_key}\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# compute test F1s\ntest_f1s = {}\nfor nl_key in runs:\n    preds = np.asarray(runs[nl_key][\"predictions\"])\n    gts = np.asarray(runs[nl_key][\"ground_truth\"])\n    test_f1s[nl_key] = f1_score(gts, preds, average=\"macro\")\n\n# ---------- plot 3: bar chart of test F1 ----------\ntry:\n    plt.figure()\n    keys = sorted(test_f1s, key=lambda x: int(x.split(\"_\")[1]))\n    vals = [test_f1s[k] for k in keys]\n    plt.bar(keys, vals, color=\"skyblue\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 by num_layers\")\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# identify best model\nbest_key = max(test_f1s, key=test_f1s.get)\n\n# ---------- plot 4: confusion matrix for best model ----------\ntry:\n    preds = np.asarray(runs[best_key][\"predictions\"])\n    gts = np.asarray(runs[best_key][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(f\"SPR_BENCH Confusion Matrix (Best {best_key})\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_key}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Test Macro-F1 scores:\")\nfor k, v in sorted(test_f1s.items(), key=lambda x: int(x[0].split(\"_\")[1])):\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntest_scores = {}\n\nfor dset, rec in experiment_data.items():\n    epochs = np.asarray(rec.get(\"epochs\", []))\n    metrics = rec.get(\"metrics\", {})\n    losses = rec.get(\"losses\", {})\n    preds = np.asarray(rec.get(\"predictions\", []))\n    gts = np.asarray(rec.get(\"ground_truth\", []))\n\n    # ---------- plot 1: F1 curves ----------\n    try:\n        if len(epochs) and metrics.get(\"train_f1\") and metrics.get(\"val_f1\"):\n            plt.figure()\n            plt.plot(epochs, metrics[\"train_f1\"], label=\"Train\")\n            plt.plot(epochs, metrics[\"val_f1\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dset}: Train vs Validation Macro-F1\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_f1_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 2: Loss curves ----------\n    try:\n        if len(epochs) and losses.get(\"train\") and losses.get(\"val\"):\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train\")\n            plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 3: Confusion matrix ----------\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            cm = confusion_matrix(gts, preds)\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(f\"{dset}: Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ---------- compute / collect test F1 ----------\n    try:\n        test_f1 = metrics.get(\"test_f1\")\n        if test_f1 is None and preds.size and gts.size:\n            test_f1 = f1_score(gts, preds, average=\"macro\")\n        if test_f1 is not None:\n            test_scores[dset] = float(test_f1)\n    except Exception as e:\n        print(f\"Error computing test F1 for {dset}: {e}\")\n\n# ---------- bar chart across datasets ----------\ntry:\n    if test_scores:\n        plt.figure()\n        keys = list(test_scores.keys())\n        vals = [test_scores[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Test Macro-F1 Comparison Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_test_f1_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison bar chart: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"=== Test Macro-F1 Scores ===\")\nfor k, v in test_scores.items():\n    print(f\"{k}: {v:.4f}\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\n# unify single-run vs multi-run access\nspr_raw = experiment_data.get(\"SPR_BENCH\", {})\nif not spr_raw:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\n# put everything into a dict of runs\nif \"metrics\" in spr_raw:  # single run\n    runs = {\"run_1\": spr_raw}\nelse:  # already multiple runs\n    runs = spr_raw\n\n\n# helper\ndef arr(run_key, section, name):\n    return np.asarray(runs[run_key][section][name])\n\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    for k in runs:\n        plt.plot(runs[k][\"epochs\"], arr(k, \"metrics\", \"train_f1\"), label=f\"{k} train\")\n        plt.plot(\n            runs[k][\"epochs\"],\n            arr(k, \"metrics\", \"val_f1\"),\n            linestyle=\"--\",\n            label=f\"{k} val\",\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    for k in runs:\n        plt.plot(runs[k][\"epochs\"], arr(k, \"losses\", \"train\"), label=f\"{k} train\")\n        plt.plot(\n            runs[k][\"epochs\"], arr(k, \"losses\", \"val\"), linestyle=\"--\", label=f\"{k} val\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# compute test F1s\ntest_f1s = {}\nfor k in runs:\n    preds = np.asarray(runs[k][\"predictions\"])\n    gts = np.asarray(runs[k][\"ground_truth\"])\n    if preds.size and gts.size:\n        test_f1s[k] = f1_score(gts, preds, average=\"macro\")\n\n# ---------- plot 3: bar chart of test F1 ----------\nif len(test_f1s) > 1:\n    try:\n        plt.figure()\n        keys = list(test_f1s.keys())\n        vals = [test_f1s[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH: Test Macro-F1 by Run\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar plot: {e}\")\n        plt.close()\n\n# ---------- plot 4: confusion matrix for best run ----------\ntry:\n    best_run = max(test_f1s, key=test_f1s.get)\n    preds = np.asarray(runs[best_run][\"predictions\"])\n    gts = np.asarray(runs[best_run][\"ground_truth\"])\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(f\"SPR_BENCH Confusion Matrix ({best_run})\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f\"SPR_BENCH_confusion_{best_run}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"Test Macro-F1 scores:\")\nfor k, v in test_f1s.items():\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\ndata = experiment_data.get(\"SPR_BENCH\")\nif data is None:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n    exit()\n\nepochs = np.asarray(data[\"epochs\"])\ntrain_f1 = np.asarray(data[\"metrics\"][\"train_f1\"])\nval_f1 = np.asarray(data[\"metrics\"][\"val_f1\"])\ntrain_loss = np.asarray(data[\"losses\"][\"train\"])\nval_loss = np.asarray(data[\"losses\"][\"val\"])\npreds = np.asarray(data[\"predictions\"])\ngts = np.asarray(data[\"ground_truth\"])\ntest_f1 = float(data[\"metrics\"][\"test_f1\"])\nsga = float(data[\"metrics\"][\"SGA\"])\n\n# ---------- Plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train F1\")\n    plt.plot(epochs, val_f1, linestyle=\"--\", label=\"Validation F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- Plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, linestyle=\"--\", label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- Plot 3: Test F1 & SGA bar ----------\ntry:\n    plt.figure()\n    metrics = [\"Test_F1\", \"SGA\"]\n    values = [test_f1, sga]\n    plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\"])\n    for i, v in enumerate(values):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.ylim(0, 1)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Test Macro-F1 and SGA\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_testF1_SGA_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# ---------- Plot 4: Confusion matrix ----------\ntry:\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(\"SPR_BENCH Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- Print summary ----------\nprint(f\"Test Macro-F1: {test_f1:.4f}\")\nprint(f\"Systematic Generalization Accuracy: {sga:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, f1_score\n\n# ---------- setup ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exit()\n\ndata = experiment_data.get(\"SPR_BENCH\")\nif data is None:\n    print(\"SPR_BENCH not found in experiment_data.npy\")\n    exit()\n\nepochs = np.asarray(data[\"epochs\"])\ntrain_loss = np.asarray(data[\"losses\"][\"train\"])\nval_loss = np.asarray(data[\"losses\"][\"val\"])\ntrain_f1 = np.asarray(data[\"metrics\"][\"train_f1\"])\nval_f1 = np.asarray(data[\"metrics\"][\"val_f1\"])\ntest_f1 = data[\"metrics\"].get(\"test_f1\")\nSGA = data[\"metrics\"].get(\"SGA\")\npreds = np.asarray(data[\"predictions\"])\ngts = np.asarray(data[\"ground_truth\"])\n\n# ---------- plot 1: loss curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: F1 curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: bar chart of final F1s ----------\ntry:\n    plt.figure()\n    names = [\"Train\", \"Validation\", \"Test\"]\n    values = [\n        train_f1[-1] if train_f1.size else np.nan,\n        val_f1[-1] if val_f1.size else np.nan,\n        test_f1 if test_f1 is not None else np.nan,\n    ]\n    plt.bar(names, values, color=\"skyblue\")\n    for i, v in enumerate(values):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.ylabel(\"Macro-F1\")\n    plt.ylim(0, 1)\n    plt.title(\"SPR_BENCH Final Macro-F1 Scores\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_final_f1_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar plot: {e}\")\n    plt.close()\n\n# ---------- plot 4: confusion matrix ----------\ntry:\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix\\nTest Split\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=(\"white\" if cm[i, j] > cm.max() / 2 else \"black\"),\n                )\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------- metric summary ----------\nprint(\"==== SPR_BENCH Metric Summary ====\")\nif train_f1.size:\n    print(f\"  Final Train Macro-F1:      {train_f1[-1]:.4f}\")\nif val_f1.size:\n    print(f\"  Final Validation Macro-F1: {val_f1[-1]:.4f}\")\nif test_f1 is not None:\n    print(f\"  Test Macro-F1:             {test_f1:.4f}\")\nif SGA is not None:\n    print(f\"  Systematic Gen. Accuracy:  {SGA:.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, f1_score\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise SystemExit\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"SPR_BENCH data not found.\")\n    raise SystemExit\n\nrun = experiment_data[\"SPR_BENCH\"]  # single run dictionary\n\nepochs = np.asarray(run[\"epochs\"])\ntrain_f1 = np.asarray(run[\"metrics\"][\"train_f1\"])\nval_f1 = np.asarray(run[\"metrics\"][\"val_f1\"])\ntrain_loss = np.asarray(run[\"losses\"][\"train\"])\nval_loss = np.asarray(run[\"losses\"][\"val\"])\ntest_f1 = run[\"test_f1\"]\nSGA = run[\"metrics\"][\"SGA\"][0] if run[\"metrics\"][\"SGA\"] else None\npreds = np.asarray(run[\"predictions\"])\ngts = np.asarray(run[\"ground_truth\"])\n\n# ---------- plot 1: F1 curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train F1\")\n    plt.plot(epochs, val_f1, linestyle=\"--\", label=\"Validation F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH: Training vs Validation Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 plot: {e}\")\n    plt.close()\n\n# ---------- plot 2: Loss curves ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, linestyle=\"--\", label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# ---------- plot 3: Test metrics bar ----------\ntry:\n    plt.figure()\n    names = [\"Macro-F1\", \"SGA\"] if SGA is not None else [\"Macro-F1\"]\n    vals = [test_f1, SGA] if SGA is not None else [test_f1]\n    plt.bar(names, vals, color=[\"steelblue\", \"salmon\"][: len(vals)])\n    for i, v in enumerate(vals):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n    plt.ylim(0, 1.05)\n    plt.ylabel(\"Score\")\n    plt.title(\"SPR_BENCH: Test Metrics\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_test_metrics.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test metric bar: {e}\")\n    plt.close()\n\n# ---------- plot 4: confusion matrix ----------\ntry:\n    cm = confusion_matrix(gts, preds)\n    plt.figure(figsize=(6, 5))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.title(\"SPR_BENCH Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(\n                j,\n                i,\n                cm[i, j],\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n            )\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\nprint(f\"Final Test Macro-F1: {test_f1:.4f}\")\nif SGA is not None:\n    print(f\"Final SGA: {SGA:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntest_scores = {}\n\nfor dset, rec in experiment_data.items():\n    epochs = np.asarray(rec.get(\"epochs\", []))\n    metrics = rec.get(\"metrics\", {})\n    losses = rec.get(\"losses\", {})\n    preds = np.asarray(rec.get(\"predictions\", []))\n    gts = np.asarray(rec.get(\"ground_truth\", []))\n\n    # ---------- plot 1: F1 curves ----------\n    try:\n        if len(epochs) and metrics.get(\"train_f1\") and metrics.get(\"val_f1\"):\n            plt.figure()\n            plt.plot(epochs, metrics[\"train_f1\"], label=\"Train\")\n            plt.plot(epochs, metrics[\"val_f1\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dset}: Train vs Validation Macro-F1\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_f1_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 2: Loss curves ----------\n    try:\n        if len(epochs) and losses.get(\"train\") and losses.get(\"val\"):\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train\")\n            plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 3: Confusion matrix ----------\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            cm = confusion_matrix(gts, preds)\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(f\"{dset}: Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ---------- compute / collect test F1 ----------\n    try:\n        test_f1 = metrics.get(\"test_f1\")\n        if test_f1 is None and preds.size and gts.size:\n            test_f1 = f1_score(gts, preds, average=\"macro\")\n        if test_f1 is not None:\n            test_scores[dset] = float(test_f1)\n    except Exception as e:\n        print(f\"Error computing test F1 for {dset}: {e}\")\n\n# ---------- bar chart across datasets ----------\ntry:\n    if test_scores:\n        plt.figure()\n        keys = list(test_scores.keys())\n        vals = [test_scores[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Test Macro-F1 Comparison Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_test_f1_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison bar chart: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"=== Test Macro-F1 Scores ===\")\nfor k, v in test_scores.items():\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntest_scores = {}\n\nfor dset, rec in experiment_data.items():\n    epochs = np.asarray(rec.get(\"epochs\", []))\n    metrics = rec.get(\"metrics\", {})\n    losses = rec.get(\"losses\", {})\n    preds = np.asarray(rec.get(\"predictions\", []))\n    gts = np.asarray(rec.get(\"ground_truth\", []))\n\n    # ---------- plot 1: F1 curves ----------\n    try:\n        if len(epochs) and metrics.get(\"train_f1\") and metrics.get(\"val_f1\"):\n            plt.figure()\n            plt.plot(epochs, metrics[\"train_f1\"], label=\"Train\")\n            plt.plot(epochs, metrics[\"val_f1\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dset}: Train vs Validation Macro-F1\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_f1_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 2: Loss curves ----------\n    try:\n        if len(epochs) and losses.get(\"train\") and losses.get(\"val\"):\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train\")\n            plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 3: Confusion matrix ----------\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            cm = confusion_matrix(gts, preds)\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(f\"{dset}: Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ---------- compute / collect test F1 ----------\n    try:\n        test_f1 = metrics.get(\"test_f1\")\n        if test_f1 is None and preds.size and gts.size:\n            test_f1 = f1_score(gts, preds, average=\"macro\")\n        if test_f1 is not None:\n            test_scores[dset] = float(test_f1)\n    except Exception as e:\n        print(f\"Error computing test F1 for {dset}: {e}\")\n\n# ---------- bar chart across datasets ----------\ntry:\n    if test_scores:\n        plt.figure()\n        keys = list(test_scores.keys())\n        vals = [test_scores[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Test Macro-F1 Comparison Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_test_f1_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison bar chart: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"=== Test Macro-F1 Scores ===\")\nfor k, v in test_scores.items():\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntest_scores = {}\n\nfor dset, rec in experiment_data.items():\n    epochs = np.asarray(rec.get(\"epochs\", []))\n    metrics = rec.get(\"metrics\", {})\n    losses = rec.get(\"losses\", {})\n    preds = np.asarray(rec.get(\"predictions\", []))\n    gts = np.asarray(rec.get(\"ground_truth\", []))\n\n    # ---------- plot 1: F1 curves ----------\n    try:\n        if len(epochs) and metrics.get(\"train_f1\") and metrics.get(\"val_f1\"):\n            plt.figure()\n            plt.plot(epochs, metrics[\"train_f1\"], label=\"Train\")\n            plt.plot(epochs, metrics[\"val_f1\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dset}: Train vs Validation Macro-F1\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_f1_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 2: Loss curves ----------\n    try:\n        if len(epochs) and losses.get(\"train\") and losses.get(\"val\"):\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train\")\n            plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 3: Confusion matrix ----------\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            cm = confusion_matrix(gts, preds)\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(f\"{dset}: Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ---------- compute / collect test F1 ----------\n    try:\n        test_f1 = metrics.get(\"test_f1\")\n        if test_f1 is None and preds.size and gts.size:\n            test_f1 = f1_score(gts, preds, average=\"macro\")\n        if test_f1 is not None:\n            test_scores[dset] = float(test_f1)\n    except Exception as e:\n        print(f\"Error computing test F1 for {dset}: {e}\")\n\n# ---------- bar chart across datasets ----------\ntry:\n    if test_scores:\n        plt.figure()\n        keys = list(test_scores.keys())\n        vals = [test_scores[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Test Macro-F1 Comparison Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_test_f1_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison bar chart: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"=== Test Macro-F1 Scores ===\")\nfor k, v in test_scores.items():\n    print(f\"{k}: {v:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------- basic set-up ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load every run ----------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2a759f81326d4a2381cc66d354015197_proc_3471779/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_83ca157fb0d74585b17303828577c6ab_proc_3471777/experiment_data.npy\",\n    \"experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2aa8a7297a8640949d9fb806ec5016e4_proc_3471780/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p), allow_pickle=True\n        ).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\n# ---------- aggregate by dataset ----------\nagg = {}\nfor run in all_experiment_data:\n    for dset, rec in run.items():\n        ds = agg.setdefault(\n            dset,\n            {\n                \"epochs\": [],\n                \"train_f1\": [],\n                \"val_f1\": [],\n                \"train_loss\": [],\n                \"val_loss\": [],\n                \"test_f1\": [],\n            },\n        )\n        epochs = np.asarray(rec.get(\"epochs\", []))\n        m = rec.get(\"metrics\", {})\n        l = rec.get(\"losses\", {})\n        if len(epochs):\n            ds[\"epochs\"].append(epochs)\n        if m.get(\"train_f1\") is not None:\n            ds[\"train_f1\"].append(np.asarray(m[\"train_f1\"]))\n        if m.get(\"val_f1\") is not None:\n            ds[\"val_f1\"].append(np.asarray(m[\"val_f1\"]))\n        if l.get(\"train\") is not None:\n            ds[\"train_loss\"].append(np.asarray(l[\"train\"]))\n        if l.get(\"val\") is not None:\n            ds[\"val_loss\"].append(np.asarray(l[\"val\"]))\n        test_f1 = m.get(\"test_f1\")\n        if test_f1 is not None:\n            ds[\"test_f1\"].append(test_f1)\n\n\n# ---------- helper ----------\ndef stack_and_crop(list_of_arr):\n    \"\"\"stack 1D arrays along axis 0, cropping to min length\"\"\"\n    if len(list_of_arr) == 0:\n        return None\n    min_len = min([len(a) for a in list_of_arr])\n    arr = np.stack([a[:min_len] for a in list_of_arr], axis=0)\n    return arr\n\n\n# ---------- create plots ----------\nfor dset, rec in agg.items():\n    # epochs (use first run, already cropped)\n    ep_stack = stack_and_crop(rec[\"epochs\"])\n    if ep_stack is None:\n        continue\n    epochs = ep_stack[0]  # all equal length after crop\n\n    # ----- F1 curves -----\n    try:\n        tr_stack = stack_and_crop(rec[\"train_f1\"])\n        val_stack = stack_and_crop(rec[\"val_f1\"])\n        if tr_stack is not None and val_stack is not None:\n            tr_mean, tr_sem = tr_stack.mean(0), tr_stack.std(0) / np.sqrt(\n                tr_stack.shape[0]\n            )\n            val_mean, val_sem = val_stack.mean(0), val_stack.std(0) / np.sqrt(\n                val_stack.shape[0]\n            )\n            plt.figure()\n            plt.plot(epochs, tr_mean, label=\"Train mean\")\n            plt.fill_between(\n                epochs,\n                tr_mean - tr_sem,\n                tr_mean + tr_sem,\n                alpha=0.3,\n                label=\"Train \u00b1SEM\",\n            )\n            plt.plot(epochs, val_mean, linestyle=\"--\", label=\"Validation mean\")\n            plt.fill_between(\n                epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                alpha=0.3,\n                label=\"Val \u00b1SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(\n                f\"{dset}: Aggregated Train/Val Macro-F1\\n(shaded: \u00b1SEM, n={tr_stack.shape[0]})\"\n            )\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_aggregate_f1_curves.png\"))\n    except Exception as e:\n        print(f\"Error creating aggregated F1 for {dset}: {e}\")\n    finally:\n        plt.close()\n\n    # ----- Loss curves -----\n    try:\n        tr_l_stack = stack_and_crop(rec[\"train_loss\"])\n        val_l_stack = stack_and_crop(rec[\"val_loss\"])\n        if tr_l_stack is not None and val_l_stack is not None:\n            tr_mean, tr_sem = tr_l_stack.mean(0), tr_l_stack.std(0) / np.sqrt(\n                tr_l_stack.shape[0]\n            )\n            val_mean, val_sem = val_l_stack.mean(0), val_l_stack.std(0) / np.sqrt(\n                val_l_stack.shape[0]\n            )\n            plt.figure()\n            plt.plot(epochs, tr_mean, label=\"Train mean\")\n            plt.fill_between(\n                epochs,\n                tr_mean - tr_sem,\n                tr_mean + tr_sem,\n                alpha=0.3,\n                label=\"Train \u00b1SEM\",\n            )\n            plt.plot(epochs, val_mean, linestyle=\"--\", label=\"Validation mean\")\n            plt.fill_between(\n                epochs,\n                val_mean - val_sem,\n                val_mean + val_sem,\n                alpha=0.3,\n                label=\"Val \u00b1SEM\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\n                f\"{dset}: Aggregated Train/Val Loss\\n(shaded: \u00b1SEM, n={tr_l_stack.shape[0]})\"\n            )\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_aggregate_loss_curves.png\"))\n    except Exception as e:\n        print(f\"Error creating aggregated Loss for {dset}: {e}\")\n    finally:\n        plt.close()\n\n# ---------- aggregated bar chart of test-F1 ----------\ntry:\n    names, means, sems = [], [], []\n    for dset, rec in agg.items():\n        if len(rec[\"test_f1\"]):\n            arr = np.asarray(rec[\"test_f1\"], dtype=float)\n            names.append(dset)\n            means.append(arr.mean())\n            sems.append(arr.std() / np.sqrt(len(arr)))\n    if names:\n        idx = np.arange(len(names))\n        plt.figure()\n        plt.bar(idx, means, yerr=sems, capsize=5, color=\"skyblue\")\n        plt.xticks(idx, names, rotation=45, ha=\"right\")\n        for i, (m, s) in enumerate(zip(means, sems)):\n            plt.text(i, m + 0.01, f\"{m:.3f}\u00b1{s:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\n            \"Aggregated Test Macro-F1 Across Datasets (bars: mean, whiskers: \u00b1SEM)\"\n        )\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"aggregate_test_f1_comparison.png\"))\nexcept Exception as e:\n    print(f\"Error creating aggregated test F1 bar chart: {e}\")\nfinally:\n    plt.close()\n\n# ---------- console summary ----------\nprint(\"=== Aggregated Test Macro-F1 (mean \u00b1 SEM) ===\")\nfor n, m, s in zip(names, means, sems):\n    print(f\"{n}: {m:.4f} \u00b1 {s:.4f}\")\n"], "plot_plan": ["We will load experiment_data.npy, identify the \u201cSPR_BENCH\u201d runs for each num_layers setting, and aggregate per-epoch losses and macro-F1 scores.  \nPlot 1 combines training/validation F1 curves for all four num_layers values; plot 2 does the same for losses.  \nPlot 3 is a bar-chart of final test macro-F1 for every setting, recomputed from stored predictions/ground_truth so the figure is self-contained.  \nFor the best model (highest test F1) we also draw a confusion matrix heat-map, limiting ourselves to at most four figures total.  \nAll plots are enclosed in independent try-except blocks, use clear titles/subtitles, are saved to working_dir with descriptive names, and figures are closed in every branch.  \nAfter plotting we print a small table of the test macro-F1 scores so the user can verify the numeric results quickly.  \nNo values are fabricated; every curve and metric comes directly from experiment_data.npy.  \nWe rely only on numpy, matplotlib, os and sklearn\u2019s confusion_matrix / f1_score which are already available in the experiment environment.", "We load experiment_data.npy, loop over each dataset present, and for every dataset we extract epochs, train/val F1, train/val loss, predictions and labels. Within independent try-except blocks we: (1) draw and save F1 curves, (2) draw and save loss curves, (3) compute a confusion matrix when predictions exist and save the plot, and (4) compute test macro-F1 (either stored or recomputed) and store it in a dict. After iterating through all datasets we create a comparative bar-chart of test macro-F1 across datasets so users can quickly see relative performance. Each figure is saved to working_dir with descriptive names that include the dataset and plot type, subtitles clarify the split or content, and all figures are closed to free memory. The code respects the original numpy structure, never fabricates values, and only plots up to the available epochs (automatically \u22645 sample plots requirement is trivially satisfied here). Finally, all test-set scores are printed for reference.", null, null, "We first load experiment_data.npy, detect whether multiple configurations are present, and harmonize access by putting each into a dict keyed by run name. For every run we extract epochs, train/val F1, train/val loss, predictions and ground-truth. We then generate four plots inside independent try-except blocks: (1) training vs validation F1 curves, (2) training vs validation loss curves, (3) bar chart of test macro-F1 across runs (skipped if only one run), and (4) confusion matrix for the best-performing run. All figures are saved in working_dir with descriptive filenames and the figure objects are closed after saving. Finally, we print test macro-F1 values for every run so the user can see numeric results in the console.", "We will load the saved experiment_data.npy, verify the SPR_BENCH entry exists, and extract epochs, loss curves, F1 curves, predictions, and labels. Then, in separate try-except blocks we will: (1) plot train/validation Macro-F1 across epochs, (2) plot train/validation loss across epochs, (3) show a single-bar comparison of test Macro-F1 and SGA, and (4) visualise the confusion matrix. Each figure is saved to working_dir with descriptive names and closed afterwards. Finally, the script prints the numeric values of test Macro-F1 and SGA. All plotting follows basic matplotlib, only uses data present in the .npy file, and respects the required directory setup.", "We will load experiment_data.npy from the working directory, verify that the SPR_BENCH entry exists, and unpack epochs, losses, and metrics. For temporal trends, we will draw two line plots: (1) train vs validation cross-entropy loss and (2) train vs validation macro-F1, each over recorded epochs. Next, we will summarize the final performance in a bar chart that shows the last recorded train-, val-, and test-F1 scores side-by-side. To visualize error patterns we will compute a confusion matrix over the test split and display it as a heat-map with per-cell counts. Because SGA is a single scalar, we will simply print it alongside the other metrics rather than create an extra plot, keeping the figure count \u2264 5. All plots are wrapped in independent try-except blocks, saved to working_dir with descriptive names, and figures are always closed afterward. The code starts with the required imports, respects existing data only, and prints a concise metric summary at the end.", null, "We will load experiment_data.npy, extract the single run stored under \u201cSPR_BENCH\u201d, and build up to four visualizations (\u22645 total). First, we plot training vs validation Macro-F1 across epochs, then a matching cross-entropy loss curve. Third, we build a bar chart comparing the final test Macro-F1 and SGA side-by-side. Finally, we compute and display a confusion matrix for the test set to reveal class-wise behaviour. Each figure is wrapped in its own try-except block, saved to working_dir with descriptive filenames, and the figure is closed afterwards. Titles always include the dataset name and, when relevant, a clear subtitle (e.g. \u201cLeft: Ground Truth, Right: Generated Samples\u201d isn\u2019t needed here). Only existing data are used; no simulation is performed. The code follows the mandated imports, directory handling, and naming conventions while remaining concise.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["num_layers", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the numpy file from the working directory, navigate the\nnested dictionary to find every experiment run for each dataset, pick the run\nthat finishes with the highest validation F1 score, and then print the final-\nepoch values for training loss, validation loss, training F1 score, and\nvalidation F1 score. Each group of metrics is preceded by the corresponding\ndataset name, and each printed line includes an explicit metric label.", "The script will find the \u201cworking\u201d directory, load \u200bexperiment_data.npy, and\nconvert the stored object back to a Python dict.   For every top-level dataset\nkey (e.g. \u201cSPR_BENCH\u201d) it will compute:   \u2022 final training loss and F1 score\n(last epoch)   \u2022 best validation loss (minimum) and best validation F1 score\n(maximum)   \u2022 test F1 score and systematic generalization accuracy (SGA).   The\nscript then prints the dataset name followed by each metric name and its value\nwith clear, descriptive labels.", "Below is a simple loader-and-reporter script: it locates the working directory,\nloads the saved numpy dictionary, then iterates over every dataset found (e.g.,\n\u201cSPR_BENCH\u201d).   For each dataset it computes the best (max) F1 scores, the best\n(max) SGA, the lowest losses, and reports the final test metrics.   All metric\nnames are printed explicitly for clarity, and the script runs immediately upon\nexecution.", "The script will load the saved NumPy dictionary from the working directory,\nfetch the metric lists recorded during training, pick the final (last) value of\neach list as the \u201cfinal\u201d value, and also compute the test-set macro-F1 score\nfrom the stored predictions. It then prints everything in a clear, dataset-first\n/ metric-name-first format.", "The script will load experiment_data.npy from the \u201cworking\u201d sub-directory,\niterate through every recorded dataset (e.g., \u201cSPR_BENCH\u201d), and for each metric\nlist pick the best value (maximum for F1 scores, minimum for losses). Scalar\nmetrics such as \u201ctest_f1\u201d and \u201cSGA\u201d are reported directly. The code immediately\nexecutes on import, printing the dataset name followed by clearly-labelled\nbest/final metrics.", "The script will load the saved experiment_data.npy from the working directory,\nconvert it to a Python dict, and iterate through every dataset entry (e.g.,\n\u201cSPR_BENCH\u201d).   For each dataset it will compute the \u201cbest\u201d value for sequence-\nstyle metrics (maximum F1, minimum loss) and read the single stored scalar\nmetrics (test F1 and SGA).   Results are printed with the dataset name first,\nfollowed by clearly labelled metric names so that it is obvious which value\nbelongs to which metric.   All code resides at the global scope so the file runs\nimmediately once executed, without using any special entry-point idiom.", "The script loads the saved NumPy dictionary from the working directory, then\niterates through each dataset entry (e.g., \u201cSPR_BENCH\u201d).   For every dataset it\nextracts the recorded arrays for training/validation losses and F1 scores, plus\nthe single-valued metrics stored after testing.   It reports the most\ninformative scalar for each metric: the maximum F1 scores, the minimum losses,\nand the final test F1 and SGA that were stored during evaluation.   All printing\nfollows the required naming conventions and executes immediately on import.", "The script simply loads the saved NumPy dictionary, loops over every dataset\nentry, and for each metric/loss list extracts the \u201cbest\u201d value (minimum for\nlosses, maximum for all other scores). It maps the raw keys used during training\n(e.g. \u201ctrain_f1\u201d) to clear, human-readable names such as \u201cTrain F1 score\u201d. The\nresulting values are printed immediately, preceded by the dataset name, so you\ncan view a concise summary of the final/best numbers without generating any\nplots or requiring a special entry point.", "The script will load experiment_data.npy from the working directory, iterate\nover every dataset stored inside, and for each one compute the requested \u201cbest\u201d\nor \u201cfinal\u201d values.   For lists that grow epoch-by-epoch (e.g., train_f1, val_f1,\nlosses) the best value is chosen as max (F1) or min (loss) depending on whether\na higher or lower value is desirable; otherwise the final stored value is taken.\nEach metric is printed with an explicit, descriptive label so there is no\nambiguity, and the dataset name is printed first as required.   All code sits at\nglobal scope so the file runs immediately when executed.", "The script will find the \u201cworking\u201d directory, load \u200bexperiment_data.npy, and\nconvert the stored object back to a Python dict.   For every top-level dataset\nkey (e.g. \u201cSPR_BENCH\u201d) it will compute:   \u2022 final training loss and F1 score\n(last epoch)   \u2022 best validation loss (minimum) and best validation F1 score\n(maximum)   \u2022 test F1 score and systematic generalization accuracy (SGA).   The\nscript then prints the dataset name followed by each metric name and its value\nwith clear, descriptive labels.", "The script will find the \u201cworking\u201d directory, load \u200bexperiment_data.npy, and\nconvert the stored object back to a Python dict.   For every top-level dataset\nkey (e.g. \u201cSPR_BENCH\u201d) it will compute:   \u2022 final training loss and F1 score\n(last epoch)   \u2022 best validation loss (minimum) and best validation F1 score\n(maximum)   \u2022 test F1 score and systematic generalization accuracy (SGA).   The\nscript then prints the dataset name followed by each metric name and its value\nwith clear, descriptive labels.", "The script will find the \u201cworking\u201d directory, load \u200bexperiment_data.npy, and\nconvert the stored object back to a Python dict.   For every top-level dataset\nkey (e.g. \u201cSPR_BENCH\u201d) it will compute:   \u2022 final training loss and F1 score\n(last epoch)   \u2022 best validation loss (minimum) and best validation F1 score\n(maximum)   \u2022 test F1 score and systematic generalization accuracy (SGA).   The\nscript then prints the dataset name followed by each metric name and its value\nwith clear, descriptive labels.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------- load experiment data --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Could not locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to locate best run --------------------\ndef best_run_by_val_f1(experiments_dict):\n    \"\"\"\n    Return the data_slot (dict) corresponding to the run with the\n    highest final-epoch validation F1 score.\n    \"\"\"\n    best_slot, best_f1 = None, -float(\"inf\")\n    for run_name, slot in experiments_dict.items():\n        # guard against malformed entries\n        if not slot[\"metrics\"][\"val_f1\"]:\n            continue\n        final_val_f1 = slot[\"metrics\"][\"val_f1\"][-1]\n        if final_val_f1 > best_f1:\n            best_f1 = final_val_f1\n            best_slot = slot\n    return best_slot\n\n\n# -------------------- iterate over datasets --------------------\nhp_group = experiment_data.get(\"num_layers\", {})  # top-level hyper-param group\nfor dataset_name, runs in hp_group.items():  # e.g. \"SPR_BENCH\" : {nl_1:..., nl_2:...}\n    best_slot = best_run_by_val_f1(runs)\n    if best_slot is None:\n        print(f\"{dataset_name}: no valid runs found.\")\n        continue\n\n    # extract final-epoch metrics\n    train_loss = best_slot[\"losses\"][\"train\"][-1]\n    val_loss = best_slot[\"losses\"][\"val\"][-1]\n    train_f1 = best_slot[\"metrics\"][\"train_f1\"][-1]\n    val_f1 = best_slot[\"metrics\"][\"val_f1\"][-1]\n\n    # -------------------- print results --------------------\n    print(f\"{dataset_name}\")\n    print(f\"  Training loss: {train_loss:.4f}\")\n    print(f\"  Validation loss: {val_loss:.4f}\")\n    print(f\"  Training F1 score: {train_f1:.4f}\")\n    print(f\"  Validation F1 score: {val_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"{data_path} not found\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef final_value(values):\n    \"\"\"Return the last element of a list, or None if empty.\"\"\"\n    return values[-1] if values else None\n\n\ndef best_value(values, mode=\"min\"):\n    \"\"\"Return best (min or max) value from list, or None.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\ndef print_metric(name: str, value):\n    if value is None:\n        return\n    print(f\"    {name}: {value:.4f}\")\n\n\n# ---------- iterate over datasets ----------\nfor dataset_name, contents in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    losses = contents.get(\"losses\", {})\n    metrics = contents.get(\"metrics\", {})\n\n    # Training / validation losses\n    train_loss_final = final_value(losses.get(\"train\", []))\n    val_loss_best = best_value(losses.get(\"val\", []), mode=\"min\")\n    print_metric(\"final training loss\", train_loss_final)\n    print_metric(\"best validation loss\", val_loss_best)\n\n    # Training / validation F1\n    train_f1_final = final_value(metrics.get(\"train_f1\", []))\n    val_f1_best = best_value(metrics.get(\"val_f1\", []), mode=\"max\")\n    print_metric(\"final training F1 score\", train_f1_final)\n    print_metric(\"best validation F1 score\", val_f1_best)\n\n    # Test metrics and SGA\n    test_f1 = metrics.get(\"test_f1\")\n    sga = metrics.get(\"SGA\")\n    print_metric(\"test F1 score\", test_f1)\n    print_metric(\"systematic generalization accuracy\", sga)\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate & load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helper to get last element ----------\ndef last(lst):\n    return lst[-1] if lst else None\n\n\n# ---------- iterate and print ----------\nfor dataset_name, data in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # losses\n    train_losses = data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = data.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"lowest train loss: {min(train_losses):.4f}\")\n    if val_losses:\n        print(f\"lowest validation loss: {min(val_losses):.4f}\")\n\n    # metrics lists\n    metrics = data.get(\"metrics\", {})\n    train_f1 = metrics.get(\"train_f1\", [])\n    val_f1 = metrics.get(\"val_f1\", [])\n    val_sga = metrics.get(\"val_sga\", [])\n    test_f1 = metrics.get(\"test_f1\", [])\n    test_sga = metrics.get(\"test_sga\", [])\n\n    if train_f1:\n        print(f\"best train F1 score: {max(train_f1):.4f}\")\n    if val_f1:\n        print(f\"best validation F1 score: {max(val_f1):.4f}\")\n    if val_sga:\n        print(f\"best validation SGA: {max(val_sga):.4f}\")\n    if test_f1:\n        print(f\"final test F1 score: {last(test_f1):.4f}\")\n    if test_sga:\n        print(f\"final test SGA: {last(test_sga):.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# ---------------- load experiment data -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------- helper to print one dataset -----------\ndef report_dataset(name, data_dict):\n    # pull lists of per-epoch stats\n    train_f1_list = data_dict[\"metrics\"][\"train_f1\"]\n    val_f1_list = data_dict[\"metrics\"][\"val_f1\"]\n    val_sga_list = data_dict[\"metrics\"][\"val_sga\"]\n    train_loss_list = data_dict[\"losses\"][\"train\"]\n    val_loss_list = data_dict[\"losses\"][\"val\"]\n\n    # compute final values (last epoch) and best validation f1\n    final_train_f1 = train_f1_list[-1]\n    final_val_f1 = val_f1_list[-1]\n    best_val_f1 = max(val_f1_list)\n    final_val_sga = val_sga_list[-1]\n    final_train_loss = train_loss_list[-1]\n    final_val_loss = val_loss_list[-1]\n\n    # compute test macro-F1 score from stored predictions / labels\n    preds = np.array(data_dict.get(\"predictions\", []))\n    gts = np.array(data_dict.get(\"ground_truth\", []))\n    test_f1 = f1_score(gts, preds, average=\"macro\") if preds.size else None\n\n    # -------- print all results -----------\n    print(f\"Dataset: {name}\")\n    print(f\"Final training F1 score:       {final_train_f1:.4f}\")\n    print(f\"Final validation F1 score:     {final_val_f1:.4f}\")\n    print(f\"Best  validation F1 score:     {best_val_f1:.4f}\")\n    print(f\"Final validation SGA score:    {final_val_sga:.4f}\")\n    print(f\"Final training loss:           {final_train_loss:.4f}\")\n    print(f\"Final validation loss:         {final_val_loss:.4f}\")\n    if test_f1 is not None:\n        print(f\"Test macro F1 score:           {test_f1:.4f}\")\n    print()  # spacer\n\n\n# -------------------- main ------------------------------\nfor ds_name, ds_dict in experiment_data.items():\n    report_dataset(ds_name, ds_dict)\n", "import os\nimport numpy as np\n\n# -------------------------------------------------\n# 0. Locate and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# 1\u20135. Extract and print metrics\n# -------------------------------------------------\ndef best_val(values, higher_is_better=True):\n    \"\"\"Return best (max or min) value from a list; fallback to None.\"\"\"\n    if not values:\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    metrics = content.get(\"metrics\", {})\n    losses = content.get(\"losses\", {})\n\n    # Best / final F1 scores\n    train_f1_best = best_val(metrics.get(\"train_f1\", []), higher_is_better=True)\n    val_f1_best = best_val(metrics.get(\"val_f1\", []), higher_is_better=True)\n    test_f1 = metrics.get(\"test_f1\")\n    sga_score = metrics.get(\"SGA\")\n\n    # Best (lowest) losses\n    train_loss_best = best_val(losses.get(\"train\", []), higher_is_better=False)\n    val_loss_best = best_val(losses.get(\"val\", []), higher_is_better=False)\n\n    if train_f1_best is not None:\n        print(f\"Best training F1 score: {train_f1_best:.4f}\")\n    if val_f1_best is not None:\n        print(f\"Best validation F1 score: {val_f1_best:.4f}\")\n    if train_loss_best is not None:\n        print(f\"Lowest training loss: {train_loss_best:.4f}\")\n    if val_loss_best is not None:\n        print(f\"Lowest validation loss: {val_loss_best:.4f}\")\n    if test_f1 is not None:\n        print(f\"Test F1 score: {test_f1:.4f}\")\n    if sga_score is not None:\n        print(f\"SGA score: {sga_score:.4f}\")\n", "import os\nimport numpy as np\n\n# ---- locate and load the experiment file ----------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ---- helper functions ------------------------------------------------------\ndef best_value(values, mode=\"max\"):\n    \"\"\"Return the best (max or min) value from a list, or None if the list is empty.\"\"\"\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\n# ---- iterate over datasets and print metrics ------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # F1 scores --------------------------------------------------------------\n    best_train_f1 = best_value(data[\"metrics\"].get(\"train_f1\", []), mode=\"max\")\n    best_val_f1 = best_value(data[\"metrics\"].get(\"val_f1\", []), mode=\"max\")\n    test_f1 = data[\"metrics\"].get(\"test_f1\")\n\n    if best_train_f1 is not None:\n        print(f\"Best training F1 score: {best_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n    if test_f1 is not None:\n        print(f\"Test F1 score: {test_f1:.4f}\")\n\n    # Losses -----------------------------------------------------------------\n    best_train_loss = best_value(data[\"losses\"].get(\"train\", []), mode=\"min\")\n    best_val_loss = best_value(data[\"losses\"].get(\"val\", []), mode=\"min\")\n\n    if best_train_loss is not None:\n        print(f\"Lowest training loss: {best_train_loss:.6f}\")\n    if best_val_loss is not None:\n        print(f\"Lowest validation loss: {best_val_loss:.6f}\")\n\n    # SGA --------------------------------------------------------------------\n    sga = data[\"metrics\"].get(\"SGA\")\n    if sga is not None:\n        print(f\"Systematic Generalization Accuracy: {sga:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------ load ------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"{file_path} not found\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------ parse & print ------------------\nfor dataset_name, dataset_info in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Fetch arrays/lists\n    train_f1_list = dataset_info[\"metrics\"].get(\"train_f1\", [])\n    val_f1_list = dataset_info[\"metrics\"].get(\"val_f1\", [])\n    test_f1 = dataset_info[\"metrics\"].get(\"test_f1\")\n    sga = dataset_info[\"metrics\"].get(\"SGA\")\n\n    train_loss_list = dataset_info[\"losses\"].get(\"train\", [])\n    val_loss_list = dataset_info[\"losses\"].get(\"val\", [])\n\n    # Safely derive best/final values\n    best_train_f1 = max(train_f1_list) if train_f1_list else None\n    best_val_f1 = max(val_f1_list) if val_f1_list else None\n    lowest_train_loss = min(train_loss_list) if train_loss_list else None\n    lowest_val_loss = min(val_loss_list) if val_loss_list else None\n\n    # Printing with clear metric names\n    if best_train_f1 is not None:\n        print(f\"Best training F1 score: {best_train_f1:.4f}\")\n    if best_val_f1 is not None:\n        print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n    if test_f1 is not None:\n        print(f\"Test F1 score: {test_f1:.4f}\")\n\n    if lowest_train_loss is not None:\n        print(f\"Lowest training loss: {lowest_train_loss:.6f}\")\n    if lowest_val_loss is not None:\n        print(f\"Lowest validation loss: {lowest_val_loss:.6f}\")\n\n    if sga is not None:\n        print(f\"Systematic Generalization Accuracy: {sga:.4f}\")\n", "import os\nimport numpy as np\n\n\n# ------------------------------------------------------------------\n# helper: pick \u201cbest\u201d value and provide a nice label for each metric\n# ------------------------------------------------------------------\ndef pick_best(name, values):\n    \"\"\"Return best value according to metric type.\"\"\"\n    if not values:  # empty list guard\n        return None\n    if \"loss\" in name.lower():  # lower is better for losses\n        return min(values)\n    return max(values)  # higher is better otherwise\n\n\ndef pretty_label(raw_key):\n    \"\"\"Convert internal key names to a readable label.\"\"\"\n    mapping = {\n        \"train_f1\": \"Train F1 score\",\n        \"val_f1\": \"Validation F1 score\",\n        \"val_sga\": \"Validation SGA\",\n        \"test_f1\": \"Test F1 score\",\n        \"test_sga\": \"Test SGA\",\n        \"train\": \"Training loss\",\n        \"val\": \"Validation loss\",\n    }\n    return mapping.get(raw_key, raw_key)\n\n\n# ------------------------------------------------------------------\n# load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# iterate and print metrics\n# ------------------------------------------------------------------\nfor dataset_name, data in experiment_data.items():\n    print(f\"{dataset_name}\")\n\n    # losses\n    for loss_key, loss_values in data.get(\"losses\", {}).items():\n        best_val = pick_best(loss_key, loss_values)\n        if best_val is not None:\n            print(f\"  {pretty_label(loss_key)}: {best_val:.4f}\")\n\n    # metrics\n    for metric_key, metric_values in data.get(\"metrics\", {}).items():\n        best_val = pick_best(metric_key, metric_values)\n        if best_val is not None:\n            print(f\"  {pretty_label(metric_key)}: {best_val:.4f}\")\n", "import os\nimport numpy as np\n\n# ---------------- locate and load ----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------- helper functions ----------------\ndef best_or_final(metric_name, values):\n    \"\"\"\n    Decide whether 'best' means max or min. If the collection is empty or\n    not a list we just return the value itself.\n    \"\"\"\n    if not isinstance(values, (list, tuple)):\n        # single scalar stored (e.g., test_f1)\n        return values\n\n    if len(values) == 0:\n        return None\n\n    metric_name = metric_name.lower()\n    if \"loss\" in metric_name:\n        return min(values)  # lower is better\n    else:\n        return max(values)  # higher is better (e.g. F1, accuracy)\n\n\n# ---------------- print metrics ----------------\nfor dataset_name, ds_dict in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")  # dataset header\n\n    # losses\n    for loss_split, loss_values in ds_dict.get(\"losses\", {}).items():\n        label = \"training loss\" if loss_split == \"train\" else \"validation loss\"\n        value = best_or_final(loss_split, loss_values)\n        if value is not None:\n            print(f\"{label}: {value:.4f}\")\n\n    # metrics\n    for m_name, m_values in ds_dict.get(\"metrics\", {}).items():\n        if m_name == \"SGA\":\n            # list with single final element\n            if m_values:\n                print(f\"SGA score: {m_values[-1]:.4f}\")\n            continue\n\n        if m_name == \"test_f1\":\n            # handled below (scalar)\n            continue\n\n        label_map = {\n            \"train_f1\": \"best training F1 score\",\n            \"val_f1\": \"best validation F1 score\",\n        }\n        label = label_map.get(m_name, m_name)\n        value = best_or_final(m_name, m_values)\n        if value is not None:\n            print(f\"{label}: {value:.4f}\")\n\n    # scalar test_f1 stored outside list\n    test_f1 = ds_dict.get(\"test_f1\", None)\n    if test_f1 is not None:\n        print(f\"test F1 score: {test_f1:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"{data_path} not found\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef final_value(values):\n    \"\"\"Return the last element of a list, or None if empty.\"\"\"\n    return values[-1] if values else None\n\n\ndef best_value(values, mode=\"min\"):\n    \"\"\"Return best (min or max) value from list, or None.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\ndef print_metric(name: str, value):\n    if value is None:\n        return\n    print(f\"    {name}: {value:.4f}\")\n\n\n# ---------- iterate over datasets ----------\nfor dataset_name, contents in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    losses = contents.get(\"losses\", {})\n    metrics = contents.get(\"metrics\", {})\n\n    # Training / validation losses\n    train_loss_final = final_value(losses.get(\"train\", []))\n    val_loss_best = best_value(losses.get(\"val\", []), mode=\"min\")\n    print_metric(\"final training loss\", train_loss_final)\n    print_metric(\"best validation loss\", val_loss_best)\n\n    # Training / validation F1\n    train_f1_final = final_value(metrics.get(\"train_f1\", []))\n    val_f1_best = best_value(metrics.get(\"val_f1\", []), mode=\"max\")\n    print_metric(\"final training F1 score\", train_f1_final)\n    print_metric(\"best validation F1 score\", val_f1_best)\n\n    # Test metrics and SGA\n    test_f1 = metrics.get(\"test_f1\")\n    sga = metrics.get(\"SGA\")\n    print_metric(\"test F1 score\", test_f1)\n    print_metric(\"systematic generalization accuracy\", sga)\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"{data_path} not found\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef final_value(values):\n    \"\"\"Return the last element of a list, or None if empty.\"\"\"\n    return values[-1] if values else None\n\n\ndef best_value(values, mode=\"min\"):\n    \"\"\"Return best (min or max) value from list, or None.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\ndef print_metric(name: str, value):\n    if value is None:\n        return\n    print(f\"    {name}: {value:.4f}\")\n\n\n# ---------- iterate over datasets ----------\nfor dataset_name, contents in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    losses = contents.get(\"losses\", {})\n    metrics = contents.get(\"metrics\", {})\n\n    # Training / validation losses\n    train_loss_final = final_value(losses.get(\"train\", []))\n    val_loss_best = best_value(losses.get(\"val\", []), mode=\"min\")\n    print_metric(\"final training loss\", train_loss_final)\n    print_metric(\"best validation loss\", val_loss_best)\n\n    # Training / validation F1\n    train_f1_final = final_value(metrics.get(\"train_f1\", []))\n    val_f1_best = best_value(metrics.get(\"val_f1\", []), mode=\"max\")\n    print_metric(\"final training F1 score\", train_f1_final)\n    print_metric(\"best validation F1 score\", val_f1_best)\n\n    # Test metrics and SGA\n    test_f1 = metrics.get(\"test_f1\")\n    sga = metrics.get(\"SGA\")\n    print_metric(\"test F1 score\", test_f1)\n    print_metric(\"systematic generalization accuracy\", sga)\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"{data_path} not found\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef final_value(values):\n    \"\"\"Return the last element of a list, or None if empty.\"\"\"\n    return values[-1] if values else None\n\n\ndef best_value(values, mode=\"min\"):\n    \"\"\"Return best (min or max) value from list, or None.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\ndef print_metric(name: str, value):\n    if value is None:\n        return\n    print(f\"    {name}: {value:.4f}\")\n\n\n# ---------- iterate over datasets ----------\nfor dataset_name, contents in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    losses = contents.get(\"losses\", {})\n    metrics = contents.get(\"metrics\", {})\n\n    # Training / validation losses\n    train_loss_final = final_value(losses.get(\"train\", []))\n    val_loss_best = best_value(losses.get(\"val\", []), mode=\"min\")\n    print_metric(\"final training loss\", train_loss_final)\n    print_metric(\"best validation loss\", val_loss_best)\n\n    # Training / validation F1\n    train_f1_final = final_value(metrics.get(\"train_f1\", []))\n    val_f1_best = best_value(metrics.get(\"val_f1\", []), mode=\"max\")\n    print_metric(\"final training F1 score\", train_f1_final)\n    print_metric(\"best validation F1 score\", val_f1_best)\n\n    # Test metrics and SGA\n    test_f1 = metrics.get(\"test_f1\")\n    sga = metrics.get(\"SGA\")\n    print_metric(\"test F1 score\", test_f1)\n    print_metric(\"systematic generalization accuracy\", sga)\n\n    print()  # blank line between datasets\n", ""], "parse_term_out": ["['SPR_BENCH', '\\n', '  Training loss: 0.0200', '\\n', '  Validation loss:\n2.1754', '\\n', '  Training F1 score: 0.9925', '\\n', '  Validation F1 score:\n0.7020', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '    final training loss: 0.0161', '\\n', '    best\nvalidation loss: 0.6775', '\\n', '    final training F1 score: 0.9960', '\\n', '\nbest validation F1 score: 0.6960', '\\n', '    test F1 score: 0.6999', '\\n', '\nsystematic generalization accuracy: 0.0000', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'lowest train loss: 0.0221', '\\n', 'lowest\nvalidation loss: 0.6514', '\\n', 'best train F1 score: 0.9945', '\\n', 'best\nvalidation F1 score: 0.7000', '\\n', 'best validation SGA: 0.0000', '\\n', 'final\ntest F1 score: 0.6999', '\\n', 'final test SGA: 0.0000', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training F1 score:       0.9730', '\\n',\n'Final validation F1 score:     0.6653', '\\n', 'Best  validation F1 score:\n0.6840', '\\n', 'Final validation SGA score:    0.0000', '\\n', 'Final training\nloss:           0.0947', '\\n', 'Final validation loss:         1.6436', '\\n',\n'Test macro F1 score:           0.6842', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training F1 score: 0.9885', '\\n', 'Best\nvalidation F1 score: 0.6839', '\\n', 'Lowest training loss: 0.0496', '\\n',\n'Lowest validation loss: 0.7559', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Best training F1 score: 0.9965', '\\n', 'Best validation F1\nscore: 0.7000', '\\n', 'Test F1 score: 0.6999', '\\n', 'Lowest training loss:\n0.456113', '\\n', 'Lowest validation loss: 1.552292', '\\n', 'Systematic\nGeneralization Accuracy: 0.0000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training F1 score: 0.9935', '\\n', 'Best\nvalidation F1 score: 0.7020', '\\n', 'Test F1 score: 0.6872', '\\n', 'Lowest\ntraining loss: 0.026714', '\\n', 'Lowest validation loss: 0.866214', '\\n',\n'Systematic Generalization Accuracy: 0.0000', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', '  Training loss: 0.9366', '\\n', '  Validation loss:\n2.5344', '\\n', '  Train F1 score: 0.9995', '\\n', '  Validation F1 score:\n0.7000', '\\n', '  Validation SGA: 0.0000', '\\n', '  Test F1 score: 0.6801',\n'\\n', '  Test SGA: 0.0000', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'training loss: 0.5599', '\\n', 'validation loss:\n2.4391', '\\n', 'best training F1 score: 0.9975', '\\n', 'best validation F1\nscore: 0.7000', '\\n', 'SGA score: 0.0000', '\\n', 'test F1 score: 0.6987', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '    final training loss: 0.0395', '\\n', '    best\nvalidation loss: 0.6846', '\\n', '    final training F1 score: 0.9890', '\\n', '\nbest validation F1 score: 0.6980', '\\n', '    test F1 score: 0.6967', '\\n', '\nsystematic generalization accuracy: 0.0000', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '    final training loss: 0.0468', '\\n', '    best\nvalidation loss: 1.2678', '\\n', '    final training F1 score: 0.9845', '\\n', '\nbest validation F1 score: 0.6960', '\\n', '    test F1 score: 0.6977', '\\n', '\nsystematic generalization accuracy: 0.0000', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '    final training loss: 0.0072', '\\n', '    best\nvalidation loss: 0.7289', '\\n', '    final training F1 score: 0.9985', '\\n', '\nbest validation F1 score: 0.7020', '\\n', '    test F1 score: 0.6979', '\\n', '\nsystematic generalization accuracy: 0.0000', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
