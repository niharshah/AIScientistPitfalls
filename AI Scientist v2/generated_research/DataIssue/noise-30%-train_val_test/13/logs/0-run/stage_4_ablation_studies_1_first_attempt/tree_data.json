{"edges": [[0, 3], [0, 4], [0, 6], [0, 2], [0, 8], [0, 1], [0, 5], [1, 7]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.16666666666666666, 0.5], [0.3333333333333333, 0.5], [0.5, 0.5], [0.6666666666666666, 0.5], [0.8333333333333334, 0.5], [0.0, 1.0], [1.0, 0.5]], "plan": ["We add an explicit \u201csymbolic head\u201d that receives a bag-of-symbols vector\n(normalised counts of each vocabulary item) alongside the usual transformer\nrepresentation; the two views are concatenated and mapped to the output classes.\nThis lets the network reason both sequentially (via attention) and symbolically\n(via global counting), a simple but effective neural-symbolic hybrid.  We reuse\nlast week\u2019s loader, keep the best hyper-parameters we found (2 encoder layers),\nand train for a few epochs with early stopping.  After testing we compute the\nSGA metric by measuring accuracy on test sequences that contain at least one\nbigram never observed in training, giving a cheap proxy for true rule\ngeneralisation.  All losses, F1 scores and SGA are stored in experiment_data.npy\nfor later analysis.  The whole script runs on GPU if available and respects the\nrequired logging/saving conventions.", "Ablation name: Remove Symbol-Count Branch (Transformer-Only). This ablation\nstudy removes the auxiliary symbol-count pathway to evaluate a pure Transformer\nencoder.  The sym_fc layer is deleted, the sequence representation alone is\npassed to a new output layer of size (d_model \u2192 n_classes), and sym_counts are\nignored in the forward pass.  Training, evaluation, metric logging, and file-\nsaving remain unchanged so results can be directly compared against the hybrid\nbaseline.", "Ablation name: Symbols-Only Classifier (Remove Transformer Branch). We eliminate\nthe entire sequence-transformer pathway and keep only the order-agnostic bag-of-\nsymbols features: sym_counts \u2192 Linear \u2192 ReLU \u2192 out.  The script below loads SPR-\nBENCH, builds a SymbolsOnlyClassifier, trains with early stopping, evaluates\nmacro-F1 and the SGA proxy, logs everything into a properly-structured\nexperiment_data dict, and finally stores it as experiment_data.npy.  The rest of\nthe pipeline (data prep, dataloaders, metrics, saving) mirrors the baseline to\nenable direct comparison.", "Ablation name: No Positional Encoding. Below is a single-file script that re-\nruns the baseline training pipeline after ablating the sinusoidal positional\nencoding layer (the model now receives only raw token embeddings). It logs the\nsame statistics and saves everything to \u201cexperiment_data.npy\u201d.", "Ablation name: Unnormalized-Symbol-Counts. The ablation simply removes the\nL2-length normalisation in the Bag-of-Symbols branch so that the transformer\nstill receives a frequency-invariant sequence representation, while the symbol-\ncount branch now feeds raw occurrence counts into the sym_fc layer.  All other\nhyper-parameters, model structure and training loop remain unchanged, allowing\nus to isolate the effect of the missing normalisation.  Results (loss curves, F1\nscores, predictions, etc.) are collected under the key 'Unnormalized-Symbol-\nCounts' and saved to experiment_data.npy for later comparison with the baseline.", "Ablation name: Frozen-Character-Embeddings. The ablation simply prevents the\ncharacter-level embedding matrix from learning: after creating the model we set\n`model.embed.weight.requires_grad = False` and build the optimizer with the\nremaining trainable parameters. The rest of the training / evaluation loop,\nmetrics collection and saving logic are identical to the baseline. Results are\nstored under the ablation key \u201cFrozen-Character-Embeddings\u201d and written to\n`experiment_data.npy`.", "Ablation name: CLS-Token Pooling (Replace Mean Sequence Pooling with Dedicated\n[CLS] Token). We replace the mean-pooling step with a dedicated, trainable [CLS]\ntoken: a learnable vector is prepended to every embedded sequence, positional\nencodings are added, and the Transformer\u2019s output at position 0 becomes the\nsequence representation that is concatenated with the symbol-count branch. No\nother training details are changed, so any performance difference isolates the\neffect of the pooling strategy.", "The previous script silently truncated all sequences to a hard-coded length of\n64, so any SPR example longer than 64 symbols lost information before it ever\nreached the transformer.  This not only hurt accuracy but also produced\nmisleading \u201csystematic generalisation\u201d scores.  The fix is to (i) compute the\ntrue maximum sequence length once, (ii) keep every symbol by padding batches\ndynamically, and (iii) rebuild positional encodings to this maximum length.\nWhile touching the data pipeline we also drop the unused symbol-count branch and\nclean up device handling and metric logging.  The rest of the training loop is\nunchanged, so running the file reproduces earlier results but without the\ntruncation bug.", "Ablation name: BiGRU-Sequence-Encoder. This ablation keeps the data pipeline,\ncharacter embedding table, symbol-count branch, classifier head, training loop\nand evaluation unchanged, but swaps the Transformer encoder for a two-layer\nbidirectional GRU (hidden size = d_model // 2 per direction, giving capacity-\nmatched d_model outputs). No positional encoding is added because the recurrent\nnetwork carries its own inductive bias for order. Training, early-stopping,\nmetric logging and file saving follow the original schedule so performance and\nSGA can be compared directly."], "code": ["import os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_f1\"] = ts_f1\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = ts_preds.tolist()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = []\nfor s in spr[\"test\"][\"sequence\"]:\n    ood_mask.append(len(bigrams(s) - train_bigrams) > 0)\nood_mask = np.array(ood_mask)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# REMOVE SYMBOL-COUNT BRANCH (Transformer-Only) \u2013 single-file experiment\nimport os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# -------------------- dirs / device --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data dict -------------\nexperiment_data = {\n    \"REMOVE_SYM_BRANCH\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_rec = experiment_data[\"REMOVE_SYM_BRANCH\"][\"SPR_BENCH\"]\n\n\n# -------------------- load SPR_BENCH -------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# -------------------- vocab / labels -------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq: str):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq: str):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),  # kept for API consistency, ignored by model\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# -------------------- model (Transformer-Only) ---------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    \"\"\"Pure Transformer encoder, no symbol-count path.\"\"\"\n\n    def __init__(self, vocab_sz, d_model, nhead, nlayers, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.out = nn.Linear(d_model, n_classes)\n\n    def forward(self, input_ids, sym_counts=None):  # sym_counts ignored\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.out(seq_repr)\n\n\nmodel = TransformerClassifier(vocab_size, 128, 4, 2, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# -------------------- helper ---------------------------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        tot_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# -------------------- training -------------------------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    exp_rec[\"losses\"][\"train\"].append(tr_loss)\n    exp_rec[\"losses\"][\"val\"].append(vl_loss)\n    exp_rec[\"metrics\"][\"train_f1\"].append(tr_f1)\n    exp_rec[\"metrics\"][\"val_f1\"].append(vl_f1)\n    exp_rec[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop triggered.\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# -------------------- test -----------------------------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexp_rec[\"metrics\"][\"test_f1\"] = float(ts_f1)\nexp_rec[\"predictions\"] = ts_preds.tolist()\nexp_rec[\"ground_truth\"] = ts_labels.tolist()\n\n\n# -------------------- SGA metric (OOD bigram proxy) ----\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexp_rec[\"metrics\"][\"SGA\"] = float(SGA)\n\n# -------------------- save -----------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, json, random, time\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment bookkeeping ----------\nexperiment_data = {\n    \"symbols_only\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(fname):\n        return load_dataset(\n            \"csv\", data_files=str(root / fname), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass SymbolsOnlyClassifier(nn.Module):\n    def __init__(self, sym_dim, hidden_dim, n_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(sym_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, n_classes)\n\n    def forward(self, sym_counts):\n        x = torch.relu(self.fc1(sym_counts))\n        return self.out(x)\n\n\nhidden_dim = 128\nmodel = SymbolsOnlyClassifier(vocab_size, hidden_dim, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    ed = experiment_data[\"symbols_only\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(vl_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(vl_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop triggered.\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1 = {ts_f1:.4f}\")\ned = experiment_data[\"symbols_only\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test_f1\"] = ts_f1\ned[\"predictions\"] = ts_preds.tolist()\ned[\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\ned[\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, json, random\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ---------- reproducibility ----------\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment dict ----------\nexperiment_data = {\n    \"no_positional_encoding\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": [], \"test\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nvocab_size = len(vocab)\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nnum_classes = len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    return ids + [stoi[PAD]] * (MAX_LEN - len(ids))\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, split):\n        self.seqs, self.labs = split[\"sequence\"], split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model (NO positional encoding) ----------\nclass HybridClassifierNoPos(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.embed(input_ids)  # NO positional encoding added\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        return self.out(torch.cat([seq_repr, sym_repr], -1))\n\n\nmodel = HybridClassifierNoPos(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val, patience, wait = 15, -1, 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(val_dl)\n    ed = experiment_data[\"no_positional_encoding\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(vl_loss)\n    ed[\"metrics\"][\"train\"].append(tr_f1)\n    ed[\"metrics\"][\"val\"].append(vl_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val, best_state, wait = vl_f1, model.state_dict(), 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping\")\n            break\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\ned = experiment_data[\"no_positional_encoding\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test\"] = ts_f1\ned[\"predictions\"] = ts_preds.tolist()\ned[\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\ned[\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# Unnormalized-Symbol-Counts Ablation\nimport os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data ----------\nexperiment_data = {\n    \"Unnormalized-Symbol-Counts\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_key = \"Unnormalized-Symbol-Counts\"\nbench_key = \"SPR_BENCH\"\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# locate dataset\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\n# ---------- encoding helpers ----------\ndef encode_seq(seq: str):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq: str):\n    \"\"\"UNNORMALIZED raw occurrence counts.\"\"\"\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v  # NOTE: no length normalisation\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.pos(self.embed(input_ids))\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    # log\n    experiment_data[exp_key][bench_key][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[exp_key][bench_key][\"losses\"][\"val\"].append(vl_loss)\n    experiment_data[exp_key][bench_key][\"metrics\"][\"train_f1\"].append(tr_f1)\n    experiment_data[exp_key][bench_key][\"metrics\"][\"val_f1\"].append(vl_f1)\n    experiment_data[exp_key][bench_key][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    # early stopping\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\nexperiment_data[exp_key][bench_key][\"metrics\"][\"test_f1\"] = float(ts_f1)\nexperiment_data[exp_key][bench_key][\"predictions\"] = ts_preds.tolist()\nexperiment_data[exp_key][bench_key][\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexperiment_data[exp_key][bench_key][\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# ---------------- Frozen-Character-Embeddings ablation ----------------\nimport os, pathlib, math, time, json, random, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# experiment log --------------------------------------------------------\nexperiment_data = {\n    \"Frozen-Character-Embeddings\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found on disk!\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq: str):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq: str):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, vocab, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward=256, dropout=0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        mask = input_ids.eq(0)\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifier(vocab_size, 128, 4, 2, vocab_size, num_classes).to(device)\n\n# ---------- FREEZE CHARACTER EMBEDDINGS ----------\nmodel.embed.weight.requires_grad = False\n\n# only trainable params (embedding excluded)\ntrainable_params = filter(lambda p: p.requires_grad, model.parameters())\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(trainable_params, lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    ed = experiment_data[\"Frozen-Character-Embeddings\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(vl_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(vl_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping!\")\n            break\n\n# restore best\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1 = {ts_f1:.4f}\")\ned = experiment_data[\"Frozen-Character-Embeddings\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test_f1\"] = ts_f1\ned[\"predictions\"] = ts_preds.tolist()\ned[\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- Systematic Generalization Accuracy (bigram OOD proxy) ----------\ndef bigrams(seq: str):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = float((correct[ood_mask].mean() if ood_mask.any() else 0.0))\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\ned[\"metrics\"][\"SGA\"] = SGA\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, json, random, time\nimport numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# --------------- misc / dirs / device -----------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------- experiment log -----------------------\nexperiment_data = {\n    \"CLS_TOKEN_POOL\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_rec = experiment_data[\"CLS_TOKEN_POOL\"][\"SPR_BENCH\"]\n\n\n# --------------- load SPR_BENCH -----------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(fname):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / fname),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# --------------- vocab & helpers ----------------------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab_chars = sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nvocab = [PAD, UNK] + vocab_chars\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\n\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\n\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64  # length WITHOUT the CLS token\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq: str):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq: str):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labels = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labels[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# --------------- model -------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN + 1):  # +1 for CLS\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass HybridClassifierCLS(nn.Module):\n    \"\"\"\n    Transformer branch uses a dedicated [CLS] token.\n    Sequence representation = hidden state at that position.\n    \"\"\"\n\n    def __init__(self, vocab_sz, d_model, nhead, nlayers, sym_dim, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model, padding_idx=0)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=256,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        # input_ids: (B, L)  where L = MAX_LEN\n        pad_mask = input_ids.eq(0)  # (B, L)\n        x = self.embed(input_ids)  # (B, L, d)\n        cls = self.cls_token.expand(input_ids.size(0), -1, -1)  # (B,1,d)\n        x = torch.cat([cls, x], dim=1)  # (B, L+1, d)\n        mask = torch.cat([torch.zeros_like(pad_mask[:, :1]), pad_mask], dim=1)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        cls_repr = x[:, 0]  # (B, d)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))  # (B, d)\n        combined = torch.cat([cls_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nmodel = HybridClassifierCLS(\n    vocab_size,\n    d_model=128,\n    nhead=4,\n    nlayers=2,\n    sym_dim=vocab_size,\n    n_classes=num_classes,\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# --------------- train / eval helpers ----------------\ndef run_epoch(model, dataloader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dataloader:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(dim=-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dataloader.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# --------------- training loop -----------------------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl, train=False)\n    exp_rec[\"losses\"][\"train\"].append(tr_loss)\n    exp_rec[\"losses\"][\"val\"].append(vl_loss)\n    exp_rec[\"metrics\"][\"train_f1\"].append(tr_f1)\n    exp_rec[\"metrics\"][\"val_f1\"].append(vl_f1)\n    exp_rec[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: train_F1={tr_f1:.4f}  val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = model.state_dict()\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# --------------- test -------------------------------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl, train=False)\nprint(f\"Test macro-F1 = {ts_f1:.4f}\")\nexp_rec[\"metrics\"][\"test_f1\"] = float(ts_f1)\nexp_rec[\"predictions\"] = ts_preds.tolist()\nexp_rec[\"ground_truth\"] = ts_labels.tolist()\n\n\n# --------------- SGA proxy --------------------------\ndef bigrams(seq: str):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexp_rec[\"metrics\"][\"SGA\"] = float(SGA)\n\n# --------------- save -------------------------------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "import os, pathlib, math, time, random, json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# -------------------------------------------------- paths / dirs\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------- device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------------------------------------- experiment log skeleton\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\nexp_rec = experiment_data[\"SPR_BENCH\"]\n\n\n# -------------------------------------------------- dataset helpers\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\n# find dataset folder\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH folder not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# -------------------------------------------------- vocab & labels\nPAD_TOKEN, UNK_TOKEN = \"<PAD>\", \"<UNK>\"\nvocab = [PAD_TOKEN, UNK_TOKEN] + sorted(\n    {c for s in spr[\"train\"][\"sequence\"] for c in s}\n)\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\n\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\n\nvocab_size = len(vocab)\nnum_classes = len(label2id)\nMAX_LEN = max(len(seq) for split in spr for seq in spr[split][\"sequence\"])  # dynamic!\nprint(f\"vocab_size={vocab_size} | num_classes={num_classes} | MAX_LEN={MAX_LEN}\")\n\n\n# -------------------------------------------------- dataset class\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs = hf_split[\"sequence\"]\n        self.labs = hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        token_ids = [stoi.get(c, stoi[UNK_TOKEN]) for c in seq]  # no truncation\n        return {\n            \"input_ids\": torch.tensor(token_ids, dtype=torch.long),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\ndef collate_batch(batch):\n    ids = [b[\"input_ids\"] for b in batch]\n    labs = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(\n        ids, batch_first=True, padding_value=stoi[PAD_TOKEN]\n    )\n    return {\"input_ids\": padded.to(device), \"labels\": labs.to(device)}\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]),\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_batch,\n)\nval_dl = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_batch,\n)\ntest_dl = DataLoader(\n    SPRTorchDataset(spr[\"test\"]),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_batch,\n)\n\n\n# -------------------------------------------------- model\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_sz, d_model, nhead, nlayers, n_classes):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model, padding_idx=stoi[PAD_TOKEN])\n        self.pos = PositionalEncoding(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, 256, 0.1, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.out = nn.Linear(d_model, n_classes)\n\n    def forward(self, input_ids):\n        mask = input_ids.eq(stoi[PAD_TOKEN])\n        x = self.pos(self.embed(input_ids))\n        x = self.enc(x, src_key_padding_mask=mask)\n        x.masked_fill_(mask.unsqueeze(-1), 0)\n        seq_repr = x.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        return self.out(seq_repr)\n\n\nmodel = TransformerClassifier(vocab_size, 128, 4, 2, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# -------------------------------------------------- training / evaluation helpers\ndef run_epoch(dataloader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, labs = 0.0, [], []\n    for batch in dataloader:\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).detach().cpu())\n        labs.append(batch[\"labels\"].detach().cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        tot_loss / len(dataloader.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# -------------------------------------------------- training loop\nEPOCHS, best_val, patience, wait = 15, -1, 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(val_dl, train=False)\n\n    exp_rec[\"losses\"][\"train\"].append(tr_loss)\n    exp_rec[\"losses\"][\"val\"].append(vl_loss)\n    exp_rec[\"metrics\"][\"train_f1\"].append(tr_f1)\n    exp_rec[\"metrics\"][\"val_f1\"].append(vl_f1)\n    exp_rec[\"epochs\"].append(epoch)\n\n    print(f\"Epoch {epoch}: val_loss = {vl_loss:.4f} | val_macroF1 = {vl_f1:.4f}\")\n\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# -------------------------------------------------- test evaluation\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(test_dl)\nprint(f\"Test macro-F1 = {ts_f1:.4f}\")\nexp_rec[\"metrics\"][\"test_f1\"] = float(ts_f1)\nexp_rec[\"predictions\"] = ts_preds.tolist()\nexp_rec[\"ground_truth\"] = ts_labels.tolist()\n\n\n# -------------------------------------------------- SGA proxy\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = np.array(\n    [len(bigrams(s) - train_bigrams) > 0 for s in spr[\"test\"][\"sequence\"]]\n)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\nexp_rec[\"metrics\"][\"SGA\"] = float(SGA)\n\n# -------------------------------------------------- save everything\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n", "# BiGRU-Sequence-Encoder ablation for SPR-BENCH\nimport os, pathlib, math, time, json, random\nimport torch, numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.metrics import f1_score\n\n# ---------- dirs / device ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------- experiment data dict ----------\nexperiment_data = {\n    \"BiGRU\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": [], \"test_f1\": None, \"SGA\": None},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ---------- load SPR_BENCH ----------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(f):\n        return load_dataset(\n            \"csv\", data_files=str(root / f), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nfor p in [\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n]:\n    if p.exists():\n        DATA_PATH = p\n        break\nelse:\n    raise FileNotFoundError(\"SPR_BENCH not found\")\n\nspr = load_spr_bench(DATA_PATH)\n\n# ---------- vocab / labels ----------\nPAD, UNK = \"<PAD>\", \"<UNK>\"\nvocab = [PAD, UNK] + sorted({c for s in spr[\"train\"][\"sequence\"] for c in s})\nstoi = {c: i for i, c in enumerate(vocab)}\nitos = {i: c for c, i in stoi.items()}\nlabel2id = {l: i for i, l in enumerate(sorted(set(spr[\"train\"][\"label\"])))}\nid2label = {i: l for l, i in label2id.items()}\nvocab_size, num_classes = len(vocab), len(label2id)\nMAX_LEN = 64\nprint(f\"vocab_size={vocab_size}, num_classes={num_classes}\")\n\n\ndef encode_seq(seq):\n    ids = [stoi.get(c, stoi[UNK]) for c in seq[:MAX_LEN]]\n    ids += [stoi[PAD]] * (MAX_LEN - len(ids))\n    return ids\n\n\ndef bag_of_symbols(seq):\n    v = np.zeros(vocab_size, dtype=np.float32)\n    for c in seq:\n        v[stoi.get(c, 1)] += 1\n    return v / max(1, len(seq))\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_split):\n        self.seqs, self.labs = hf_split[\"sequence\"], hf_split[\"label\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(encode_seq(self.seqs[idx]), dtype=torch.long),\n            \"sym_counts\": torch.tensor(\n                bag_of_symbols(self.seqs[idx]), dtype=torch.float32\n            ),\n            \"labels\": torch.tensor(label2id[self.labs[idx]], dtype=torch.long),\n        }\n\n\nbatch_size = 128\ntrain_dl = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=batch_size, shuffle=True\n)\nval_dl = DataLoader(SPRTorchDataset(spr[\"dev\"]), batch_size=batch_size)\ntest_dl = DataLoader(SPRTorchDataset(spr[\"test\"]), batch_size=batch_size)\n\n\n# ---------- model ----------\nclass BiGRUEncoder(nn.Module):\n    \"\"\"Two-layer bidirectional GRU returning sequence outputs of dim d_model.\"\"\"\n\n    def __init__(self, vocab, d_model):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d_model, padding_idx=0)\n        self.bigru = nn.GRU(\n            d_model,\n            d_model // 2,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True,\n            dropout=0.1,\n        )\n\n    def forward(self, input_ids):\n        # input_ids: [B, L]\n        mask = input_ids.eq(0)  # PAD mask\n        x = self.embed(input_ids)  # [B, L, d_model]\n        lengths = (~mask).sum(1).cpu()\n        # pack to avoid useless computation\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lengths, batch_first=True, enforce_sorted=False\n        )\n        out_packed, _ = self.bigru(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(\n            out_packed, batch_first=True, total_length=MAX_LEN\n        )\n        out.masked_fill_(mask.unsqueeze(-1), 0)\n        return out, mask\n\n\nclass HybridGRUClassifier(nn.Module):\n    def __init__(self, vocab, d_model, sym_dim, n_classes):\n        super().__init__()\n        self.encoder = BiGRUEncoder(vocab, d_model)\n        self.sym_fc = nn.Linear(sym_dim, d_model)\n        self.out = nn.Linear(d_model * 2, n_classes)\n\n    def forward(self, input_ids, sym_counts):\n        seq_out, mask = self.encoder(input_ids)  # [B,L,d_model]\n        seq_repr = seq_out.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1)\n        sym_repr = torch.relu(self.sym_fc(sym_counts))\n        combined = torch.cat([seq_repr, sym_repr], dim=-1)\n        return self.out(combined)\n\n\nd_model = 128\nmodel = HybridGRUClassifier(vocab_size, d_model, vocab_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# ---------- helper ----------\ndef run_epoch(model, dl, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, labs = 0.0, [], []\n    for batch in dl:\n        batch = {\n            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n            for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            logits = model(batch[\"input_ids\"], batch[\"sym_counts\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.append(logits.argmax(-1).cpu())\n        labs.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labs = torch.cat(labs).numpy()\n    return (\n        total_loss / len(dl.dataset),\n        f1_score(labs, preds, average=\"macro\"),\n        preds,\n        labs,\n    )\n\n\n# ---------- training ----------\nEPOCHS, best_val = 15, -1\npatience, wait = 3, 0\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(model, train_dl, train=True)\n    vl_loss, vl_f1, _, _ = run_epoch(model, val_dl)\n    ed = experiment_data[\"BiGRU\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(vl_loss)\n    ed[\"metrics\"][\"train_f1\"].append(tr_f1)\n    ed[\"metrics\"][\"val_f1\"].append(vl_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={vl_loss:.4f} val_F1={vl_f1:.4f}\")\n    if vl_f1 > best_val:\n        best_val = vl_f1\n        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stop\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# ---------- test ----------\nts_loss, ts_f1, ts_preds, ts_labels = run_epoch(model, test_dl)\nprint(f\"Test macro-F1={ts_f1:.4f}\")\ned = experiment_data[\"BiGRU\"][\"SPR_BENCH\"]\ned[\"metrics\"][\"test_f1\"] = ts_f1\ned[\"predictions\"] = ts_preds.tolist()\ned[\"ground_truth\"] = ts_labels.tolist()\n\n\n# ---------- SGA metric (OOD bigram proxy) ----------\ndef bigrams(seq):\n    return {seq[i : i + 2] for i in range(len(seq) - 1)}\n\n\ntrain_bigrams = set().union(*(bigrams(s) for s in spr[\"train\"][\"sequence\"]))\nood_mask = []\nfor s in spr[\"test\"][\"sequence\"]:\n    ood_mask.append(len(bigrams(s) - train_bigrams) > 0)\nood_mask = np.array(ood_mask)\ncorrect = ts_preds == ts_labels\nSGA = correct[ood_mask].mean() if ood_mask.any() else 0.0\nprint(f\"Systematic Generalization Accuracy (proxy) = {SGA:.4f}\")\ned[\"metrics\"][\"SGA\"] = float(SGA)\n\n# ---------- save ----------\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy\")\n"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 155284.20\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 118845.74\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 191792.22\nexamples/s]', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6775\nval_F1=0.5693', '\\n', 'Epoch 2: val_loss=1.4524 val_F1=0.6580', '\\n', 'Epoch 3:\nval_loss=1.4686 val_F1=0.6546', '\\n', 'Epoch 4: val_loss=1.5086 val_F1=0.6546',\n'\\n', 'Epoch 5: val_loss=1.5294 val_F1=0.6717', '\\n', 'Epoch 6: val_loss=1.6507\nval_F1=0.6653', '\\n', 'Epoch 7: val_loss=1.5858 val_F1=0.6879', '\\n', 'Epoch 8:\nval_loss=1.8113 val_F1=0.6899', '\\n', 'Epoch 9: val_loss=1.9777 val_F1=0.6960',\n'\\n', 'Epoch 10: val_loss=2.1059 val_F1=0.6960', '\\n', 'Epoch 11:\nval_loss=2.1397 val_F1=0.6960', '\\n', 'Epoch 12: val_loss=2.1337 val_F1=0.6940',\n'\\n', 'Early stop', '\\n', 'Test macro-F1=0.6999', '\\n', 'Systematic\nGeneralization Accuracy (proxy) = 0.0000', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 14 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 138204.66\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 120373.78\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 178777.72\nexamples/s]', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7426\nval_F1=0.6658', '\\n', 'Epoch 2: val_loss=1.5910 val_F1=0.6640', '\\n', 'Epoch 3:\nval_loss=1.5772 val_F1=0.6738', '\\n', 'Epoch 4: val_loss=1.7726 val_F1=0.6820',\n'\\n', 'Epoch 5: val_loss=1.8073 val_F1=0.6635', '\\n', 'Epoch 6: val_loss=1.8521\nval_F1=0.6653', '\\n', 'Epoch 7: val_loss=1.9407 val_F1=0.6719', '\\n', 'Early\nstop triggered.', '\\n', 'Test macro-F1=0.6926', '\\n', 'Systematic Generalization\nAccuracy (proxy) = 0.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution\ntime: 9 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 143390.11\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 88903.81\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 162714.98\nexamples/s]', '\\n', 'vocab_size=11, num_classes=2', '\\n', 'Epoch 1:\nval_loss=0.6919 val_F1=0.3316', '\\n', 'Epoch 2: val_loss=0.6896 val_F1=0.5035',\n'\\n', 'Epoch 3: val_loss=0.6875 val_F1=0.6819', '\\n', 'Epoch 4: val_loss=0.6847\nval_F1=0.6699', '\\n', 'Epoch 5: val_loss=0.6818 val_F1=0.6699', '\\n', 'Epoch 6:\nval_loss=0.6785 val_F1=0.6712', '\\n', 'Early stop triggered.', '\\n', 'Test\nmacro-F1 = 0.6777', '\\n', 'Systematic Generalization Accuracy (proxy) = 0.0000',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 129180.71\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 103783.44\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 166460.45\nexamples/s]', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7518\nval_F1=0.6614', '\\n', 'Epoch 2: val_loss=1.6166 val_F1=0.6655', '\\n', 'Epoch 3:\nval_loss=1.7062 val_F1=0.6675', '\\n', 'Epoch 4: val_loss=1.4928 val_F1=0.6675',\n'\\n', 'Epoch 5: val_loss=1.6177 val_F1=0.6633', '\\n', 'Epoch 6: val_loss=1.6216\nval_F1=0.6675', '\\n', 'Early stopping', '\\n', 'Test macro-F1=0.6884', '\\n',\n'Systematic Generalization Accuracy (proxy) = 0.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 143106.35\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 116327.49\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 171070.40\nexamples/s]', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7147\nval_F1=0.5120', '\\n', 'Epoch 2: val_loss=1.3671 val_F1=0.6697', '\\n', 'Epoch 3:\nval_loss=1.5901 val_F1=0.6838', '\\n', 'Epoch 4: val_loss=1.7577 val_F1=0.6859',\n'\\n', 'Epoch 5: val_loss=1.8468 val_F1=0.6960', '\\n', 'Epoch 6: val_loss=2.0999\nval_F1=0.6918', '\\n', 'Epoch 7: val_loss=2.1463 val_F1=0.6980', '\\n', 'Epoch 8:\nval_loss=2.2175 val_F1=0.6960', '\\n', 'Epoch 9: val_loss=2.2782 val_F1=0.6940',\n'\\n', 'Epoch 10: val_loss=2.3589 val_F1=0.6980', '\\n', 'Early stop', '\\n', 'Test\nmacro-F1=0.6978', '\\n', 'Systematic Generalization Accuracy (proxy) = 0.0000',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds\n(time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6775\nval_F1=0.5874', '\\n', 'Epoch 2: val_loss=1.4503 val_F1=0.6600', '\\n', 'Epoch 3:\nval_loss=1.4541 val_F1=0.6500', '\\n', 'Epoch 4: val_loss=1.5057 val_F1=0.6588',\n'\\n', 'Epoch 5: val_loss=1.5309 val_F1=0.6717', '\\n', 'Epoch 6: val_loss=1.6710\nval_F1=0.6695', '\\n', 'Epoch 7: val_loss=1.6345 val_F1=0.6858', '\\n', 'Epoch 8:\nval_loss=1.8263 val_F1=0.6939', '\\n', 'Epoch 9: val_loss=2.0137 val_F1=0.6919',\n'\\n', 'Epoch 10: val_loss=1.8665 val_F1=0.6896', '\\n', 'Epoch 11:\nval_loss=1.7504 val_F1=0.6939', '\\n', 'Early stopping!', '\\n', 'Test macro-F1 =\n0.6958', '\\n', 'Systematic Generalization Accuracy (proxy) = 0.0000', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 14 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: train_F1=0.5825\nval_F1=0.6105', '\\n', 'Epoch 2: train_F1=0.9045  val_F1=0.5966', '\\n', 'Epoch 3:\ntrain_F1=0.9104  val_F1=0.6700', '\\n', 'Epoch 4: train_F1=0.9560\nval_F1=0.6780', '\\n', 'Epoch 5: train_F1=0.9605  val_F1=0.6717', '\\n', 'Epoch 6:\ntrain_F1=0.9705  val_F1=0.6697', '\\n', 'Epoch 7: train_F1=0.9695\nval_F1=0.6736', '\\n', 'Early stopping', '\\n', 'Test macro-F1 = 0.6895', '\\n',\n'Systematic Generalization Accuracy (proxy) = 0.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'vocab_size=11 | num_classes=2 | MAX_LEN=95', '\\n',\n'Epoch 1: val_loss = 0.7634 | val_macroF1 = 0.6777', '\\n', 'Epoch 2: val_loss =\n2.0479 | val_macroF1 = 0.6920', '\\n', 'Epoch 3: val_loss = 2.0110 | val_macroF1\n= 0.6960', '\\n', 'Epoch 4: val_loss = 2.1917 | val_macroF1 = 0.6940', '\\n',\n'Epoch 5: val_loss = 2.2643 | val_macroF1 = 0.6940', '\\n', 'Epoch 6: val_loss =\n2.1777 | val_macroF1 = 0.6960', '\\n', 'Early stopping.', '\\n', 'Test macro-F1 =\n0.6910', '\\n', 'Systematic Generalization Accuracy (proxy) = 0.0000', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'vocab_size=11, num_classes=2', '\\n', 'Epoch 1:\nval_loss=0.6633 val_F1=0.6633', '\\n', 'Epoch 2: val_loss=1.3426 val_F1=0.6563',\n'\\n', 'Epoch 3: val_loss=1.7352 val_F1=0.6609', '\\n', 'Epoch 4: val_loss=1.8743\nval_F1=0.6716', '\\n', 'Epoch 5: val_loss=1.9102 val_F1=0.6797', '\\n', 'Epoch 6:\nval_loss=2.0086 val_F1=0.6736', '\\n', 'Epoch 7: val_loss=1.9807 val_F1=0.6838',\n'\\n', 'Epoch 8: val_loss=2.1398 val_F1=0.6838', '\\n', 'Epoch 9: val_loss=2.2950\nval_F1=0.6899', '\\n', 'Epoch 10: val_loss=2.3954 val_F1=0.6980', '\\n', 'Epoch\n11: val_loss=2.5882 val_F1=0.6960', '\\n', 'Epoch 12: val_loss=2.7353\nval_F1=0.6980', '\\n', 'Epoch 13: val_loss=2.8531 val_F1=0.7000', '\\n', 'Epoch\n14: val_loss=2.9277 val_F1=0.7020', '\\n', 'Epoch 15: val_loss=3.0430\nval_F1=0.7020', '\\n', 'Test macro-F1=0.6979', '\\n', 'Systematic Generalization\nAccuracy (proxy) = 0.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution\ntime: 17 seconds seconds (time limit is 30 minutes).']"], "analysis": ["", "The training script executed successfully, but there are several issues worth\naddressing. Firstly, the validation F1 scores fluctuate and do not consistently\nimprove, which might suggest a suboptimal learning rate or other hyperparameter\ntuning issues. The early stopping mechanism was triggered, indicating that the\nmodel might not have been improving sufficiently. Secondly, the Systematic\nGeneralization Accuracy (SGA) metric is 0.0000, which indicates the model failed\nto generalize to out-of-distribution bigrams. This is a significant concern as\nit suggests the model is not capturing the desired generalization capabilities.\nTo address these issues, consider fine-tuning the learning rate, experimenting\nwith different model architectures, or incorporating additional regularization\ntechniques to enhance generalization. Additionally, analyze the training and\nvalidation datasets to ensure they are representative and free from data\nleakage.", "The training script executed successfully without any errors or bugs. The model\nwas trained on the SPR_BENCH dataset, and an early stopping mechanism was\ntriggered after 6 epochs due to no improvement in the validation F1 score. The\nmacro-F1 score on the test set was 0.6777, and the SGA (Systematic\nGeneralization Accuracy) was calculated as 0.0000, indicating no systematic\ngeneralization to out-of-distribution bigrams. The results were saved\nsuccessfully in 'experiment_data.npy'. No issues were identified in the\nimplementation or execution.", "The execution of the script was successful without any bugs. The model was\ntrained on the SPR_BENCH dataset, and validation and test performances were\nreported. Early stopping was applied based on validation F1 scores. The final\ntest macro-F1 score was 0.6884, and the Systematic Generalization Accuracy (SGA)\nwas 0.0000. The experiment data was saved successfully.", "", "The code executed successfully without any runtime errors or bugs. The model\ntrained and evaluated as expected, and the outputs were saved appropriately. The\nresults showed a Test macro-F1 score of 0.6958 and a Systematic Generalization\nAccuracy (SGA) of 0.0000. No issues were identified in the execution.", "The training script executed successfully without any errors or bugs. The model\nwas trained and validated over multiple epochs, and early stopping was triggered\nbased on validation F1 scores. The test macro-F1 score was 0.6895, and the\nSystematic Generalization Accuracy (SGA) proxy was calculated as 0.0000. The\nexperiment data was saved successfully. No issues were observed in the output.", "The execution output indicates that the code ran successfully without any errors\nor bugs. The model was trained and evaluated on the SPR_BENCH dataset, achieving\na test macro-F1 score of 0.6910. Early stopping was applied during training\nbased on validation performance. However, the Systematic Generalization Accuracy\n(SGA) proxy metric was 0.0000, indicating that the model failed to generalize to\nsystematic variations in the test set. This could be an area for further\ninvestigation and improvement in the model design or training process. The\nexperiment data was successfully saved as 'experiment_data.npy'.", "The training script has a significant issue in achieving systematic\ngeneralization. The Systematic Generalization Accuracy (SGA) is reported as\n0.0000, indicating the model fails to generalize to out-of-distribution (OOD)\nbigrams in the test set. This suggests that the model is overfitting to the\ntraining data and is unable to effectively handle unseen patterns.   Proposed\nFix: 1. Introduce data augmentation techniques to expose the model to a wider\nvariety of bigrams during training. 2. Modify the loss function to include a\nregularization term that penalizes overfitting. 3. Experiment with alternative\narchitectures or symbolic reasoning modules that are better suited for handling\nOOD generalization. 4. Perform hyperparameter tuning, especially with dropout\nrates or learning rates to improve generalization. 5. Increase the diversity of\ntraining data by including more sequences with varied bigrams from the start."], "exc_type": [null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0161, "best_value": 0.0161}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6775, "best_value": 0.6775}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "The accuracy for systematic generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "A measure of a model's accuracy, considering both precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6926, "best_value": 0.6926}]}, {"metric_name": "Systematic Generalization Accuracy (SGA)", "lower_is_better": false, "description": "Accuracy measure for systematic generalization tasks.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "A measure of error or difference between the predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9407, "best_value": 0.0864}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures how well the model is performing on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6567, "best_value": 0.6567}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures how well the model is performing on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6785, "best_value": 0.6785}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8985, "best_value": 0.8985}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6819, "best_value": 0.6819}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6777, "best_value": 0.6777}]}, {"metric_name": "Systematic Generalization Accuracy", "lower_is_better": false, "description": "Accuracy for systematic generalization tasks.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "F1 score measures the balance between precision and recall. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6884, "best_value": 0.6884}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Training loss indicates how well the model is performing on the training dataset. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1114, "best_value": 0.1114}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss measures how well the model generalizes to unseen data. Lower values are better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7518, "best_value": 0.7518}]}, {"metric_name": "Systematic generalization accuracy", "lower_is_better": false, "description": "Measures the model's ability to generalize systematically. Higher values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "F1 score is the harmonic mean of precision and recall. It ranges from 0 to 1, where higher values are better.", "data": [{"dataset_name": "train", "final_value": 0.997, "best_value": 0.997}, {"dataset_name": "validation", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "test", "final_value": 0.6978, "best_value": 0.6978}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss is a measure of how well the model is performing; lower values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.0146, "best_value": 0.0146}, {"dataset_name": "validation", "final_value": 0.7147, "best_value": 0.7147}]}, {"metric_name": "Systematic Generalization Accuracy", "lower_is_better": false, "description": "Accuracy metric for systematic generalization, indicating the ability of the model to generalize to unseen systematic patterns.", "data": [{"dataset_name": "systematic generalization", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 Score", "lower_is_better": false, "description": "F1 Score is the harmonic mean of precision and recall, used to measure model accuracy.", "data": [{"dataset_name": "Training", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "Validation", "final_value": 0.6939, "best_value": 0.6939}, {"dataset_name": "Test", "final_value": 0.6958, "best_value": 0.6958}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "Loss measures the error in a model's predictions.", "data": [{"dataset_name": "Training", "final_value": 0.0244, "best_value": 0.0244}, {"dataset_name": "Validation", "final_value": 0.6775, "best_value": 0.6775}]}, {"metric_name": "Systematic Generalization Accuracy", "lower_is_better": false, "description": "Accuracy for systematic generalization, measuring the model's ability to generalize to new systematic patterns.", "data": [{"dataset_name": "Systematic Generalization", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "A measure of a model's accuracy considering both precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6895, "best_value": 0.678}]}, {"metric_name": "Systematic Generalization Accuracy", "lower_is_better": false, "description": "Measures the model's ability to generalize systematically.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "Loss", "lower_is_better": true, "description": "A measure of the error in the model's predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.1045, "best_value": 0.6652}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.028892, "best_value": 0.028892}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 2.177748, "best_value": 2.177748}]}, {"metric_name": "training macro-F1 score", "lower_is_better": false, "description": "Macro-F1 score during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.992, "best_value": 0.992}]}, {"metric_name": "validation macro-F1 score", "lower_is_better": false, "description": "Macro-F1 score during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.695995, "best_value": 0.695995}]}, {"metric_name": "test macro-F1 score", "lower_is_better": false, "description": "Macro-F1 score during the testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.690985, "best_value": 0.690985}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "Accuracy for systematic generalization.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "macro-F1 score", "lower_is_better": false, "description": "Measures the macro-averaged F1 score, which is the harmonic mean of precision and recall, across all classes.", "data": [{"dataset_name": "train", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "validation", "final_value": 0.702, "best_value": 0.702}, {"dataset_name": "test", "final_value": 0.6979, "best_value": 0.6979}]}, {"metric_name": "systematic generalization accuracy", "lower_is_better": false, "description": "Accuracy measuring systematic generalization capability.", "data": [{"dataset_name": "systematic generalization", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the loss or error value during training or validation.", "data": [{"dataset_name": "train", "final_value": 0.0025, "best_value": 0.0025}, {"dataset_name": "validation", "final_value": 0.6633, "best_value": 0.6633}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/datasets_test_f1_comparison.png"], [], ["../../logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_F1_curve.png", "../../logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_metric_summary.png", "../../logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_loss_curve_FrozenCharEmb.png", "../../logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_f1_curve_FrozenCharEmb.png", "../../logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_confusion_matrix_FrozenCharEmb.png"], ["../../logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_f1_curve.png", "../../logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_final_metrics.png", "../../logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_confusion_matrix.png"], []], "plot_paths": [["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/datasets_test_f1_comparison.png"], [], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_F1_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_metric_summary.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_loss_curve_FrozenCharEmb.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_f1_curve_FrozenCharEmb.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_confusion_matrix_FrozenCharEmb.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_loss_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_f1_curve.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_final_metrics.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_confusion_matrix.png"], ["experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_confusion_matrix.png"], []], "plot_analyses": [[{"analysis": "The plot shows the Macro-F1 score for both training and validation datasets over the epochs. The training Macro-F1 score improves rapidly and reaches near-perfect performance early in the training process, suggesting that the model has a high capacity to learn the training data. However, the validation Macro-F1 score starts at a lower value and plateaus around 0.7, indicating potential overfitting. The gap between the training and validation performance warrants further analysis, such as regularization techniques or adjustments to the model architecture, to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_f1_curves.png"}, {"analysis": "This plot illustrates the cross-entropy loss for both training and validation datasets over the epochs. The training loss decreases steadily, approaching zero, which aligns with the model's ability to fit the training data effectively. However, the validation loss initially decreases but then begins to increase after a few epochs, indicating overfitting as the model starts to memorize the training data rather than generalizing to unseen data. Early stopping or additional regularization techniques like dropout could mitigate this issue.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_loss_curves.png"}, {"analysis": "The confusion matrix shows the distribution of true and predicted labels. While the model correctly classifies a significant portion of the data for both classes, there is a notable number of misclassifications (146 false positives and 154 false negatives). This indicates that while the model performs reasonably well, there is room for improvement in handling edge cases or ambiguous sequences. Techniques such as data augmentation or class rebalancing could help address these shortcomings.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The bar chart compares the test Macro-F1 score across datasets, with the SPR_BENCH dataset achieving a Macro-F1 score of 0.7. This matches the state-of-the-art performance benchmark. While this is a promising result, further experimentation and optimization are needed to surpass the benchmark. Exploring advanced symbolic reasoning modules or fine-tuning hyperparameters might lead to improved performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_3eb8bea5b5774f3991a7842748719097_proc_3471777/datasets_test_f1_comparison.png"}], [], [{"analysis": "The loss curve indicates a consistent reduction in both training and validation loss over epochs, demonstrating that the model is learning effectively. The gap between training and validation loss is relatively small, suggesting that the model is not overfitting at this stage.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_loss_curve.png"}, {"analysis": "The F1 curve shows a rapid increase in training F1, reaching a plateau around the third epoch. Validation F1 also improves but at a slower rate, stabilizing at a lower value compared to training. This indicates that while the model is learning effectively on the training data, its generalization to validation data could be improved, possibly through regularization or hyperparameter tuning.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_f1_curve.png"}, {"analysis": "The confusion matrix reveals that the model performs well on classifying one of the classes but struggles with the other. This class imbalance in performance could indicate a need for better handling of class distributions, potentially through techniques like class weighting or oversampling the underrepresented class.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_09cbbe63ee7446d0a30d89b8b8466a35_proc_3477729/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curve shows a clear divergence between training and validation losses after the first epoch. Training loss decreases consistently, indicating that the model is learning the training data well. However, validation loss increases after the second epoch, suggesting potential overfitting. The model's ability to generalize to unseen data may be limited, and regularization techniques or early stopping could be considered to address this issue.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_loss_curve.png"}, {"analysis": "The Macro-F1 curve reveals a stark contrast between training and validation performance. While the training Macro-F1 score rapidly increases and stabilizes near 0.95, the validation Macro-F1 score remains relatively flat and low, hovering around 0.65-0.70. This further supports the observation of overfitting from the loss curve. The model is failing to generalize well to the validation set, which could be due to the complexity of the task or insufficient diversity in the training data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_F1_curve.png"}, {"analysis": "The confusion matrix for the test set shows a relatively balanced distribution of true and false predictions. However, there is a noticeable number of false positives and false negatives, indicating that the model struggles with certain patterns or classes in the SPR task. This aligns with the limited generalization ability observed in the earlier plots. Analyzing class-specific performance or incorporating data augmentation may help address these issues.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_2b5634daf14a4962a279807a9d422af9_proc_3477730/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily and converges to near zero, indicating that the model is effectively learning from the training data. However, the validation loss increases after the second epoch, suggesting overfitting. This implies that the model is memorizing the training data but struggling to generalize to unseen validation data. Regularization techniques or early stopping might mitigate this issue.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score improves rapidly and reaches near-perfect performance by the third epoch, while the validation Macro-F1 score shows only slight improvement and plateaus around 0.7. This further supports the observation of overfitting, as the model performs exceptionally well on the training data but less so on the validation data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_f1_curve.png"}, {"analysis": "The summary metrics reveal a significant gap between the training F1 score (close to 1.0) and the validation and test F1 scores (around 0.7). This discrepancy highlights the overfitting issue identified in the loss and Macro-F1 plots. The validation F1 scores (both last and best) and test F1 scores are consistent, indicating stable but suboptimal generalization performance.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_metric_summary.png"}, {"analysis": "The confusion matrix shows a relatively balanced performance across both classes, with no significant bias towards either class. However, the model's overall performance is limited by the generalization issues observed in the other metrics.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_362477ca0afc40a8ae6e1a471365673d_proc_3477731/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curve indicates that the training loss decreases steadily and converges to near zero by the end of training, suggesting that the model is fitting the training data well. However, the validation loss initially decreases but then increases after a few epochs, forming a U-shape. This pattern is indicative of overfitting, where the model starts to memorize the training data and loses its ability to generalize to unseen data. The use of frozen character embeddings might limit the model's ability to adapt to the validation set, contributing to this behavior.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_loss_curve_FrozenCharEmb.png"}, {"analysis": "The macro-F1 curve shows that the training F1 score increases rapidly and stabilizes close to 1.0, indicating excellent performance on the training set. In contrast, the validation F1 score improves initially but plateaus at a lower value, suggesting that the model's generalization capability is limited. The gap between the training and validation F1 scores is consistent with the overfitting observed in the loss curve. This result highlights the need for regularization techniques or architectural adjustments to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_f1_curve_FrozenCharEmb.png"}, {"analysis": "The confusion matrix shows a clear diagonal dominance, indicating that the model performs well in classifying the majority of samples correctly. However, the off-diagonal elements suggest that there are still some misclassifications. The distribution of errors across classes should be analyzed further to identify specific weaknesses in the model's performance. This could help in diagnosing whether certain symbolic rules or patterns are more challenging for the model to learn.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_118d864256fd4c1ea53e3598c8c6d404_proc_3477728/SPR_BENCH_confusion_matrix_FrozenCharEmb.png"}], [{"analysis": "This plot shows the loss curves for both training and validation datasets over seven epochs. The training loss decreases steadily and stabilizes after epoch 4, indicating effective learning. However, the validation loss initially decreases but then increases slightly after epoch 4, suggesting potential overfitting. The gap between training and validation losses widens over epochs, reinforcing the overfitting concern.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_loss_curve.png"}, {"analysis": "This plot illustrates the macro-F1 scores for training and validation datasets over seven epochs. The training macro-F1 score improves significantly, reaching near-perfect performance. However, the validation macro-F1 score shows a smaller improvement and stabilizes with a noticeable gap compared to the training score. This discrepancy further supports the overfitting hypothesis, as the model performs better on the training data than on the validation data.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_f1_curve.png"}, {"analysis": "This bar chart compares the final F1 scores for the training, validation, and test datasets. The training F1 score is very high, consistent with the earlier observations of overfitting. The validation and test F1 scores are lower and closer to each other, suggesting that the model generalizes moderately well but still underperforms compared to the training set.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_final_metrics.png"}, {"analysis": "The confusion matrix provides a detailed view of the model's prediction performance. The diagonal entries represent correct predictions, while off-diagonal entries indicate misclassifications. The matrix shows that the model performs well overall, but there are still some misclassifications, particularly in one of the classes. This indicates room for improvement in handling specific cases.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ad3ef1d6d44c44cbbe3c0458a75b8919_proc_3477729/SPR_BENCH_CLS_TOKEN_POOL_confusion_matrix.png"}], [{"analysis": "The cross-entropy loss for the training set decreases rapidly and stabilizes close to zero, indicating that the model is effectively learning the training data. However, the validation loss starts to increase after the second epoch, suggesting overfitting. The divergence between the training and validation loss curves highlights the need for regularization techniques or adjustments to the model to improve generalization.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_loss_curves.png"}, {"analysis": "The F1 score for the training set increases rapidly and reaches near-perfect levels, which aligns with the low training loss observed. However, the validation F1 score remains relatively constant and significantly lower than the training score. This indicates that while the model performs well on the training data, its ability to generalize to unseen data is limited. This further supports the observation of overfitting from the loss curves.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix indicates imbalanced performance across classes. The model performs well on some classes but struggles with others, as evidenced by the off-diagonal values. This suggests that the model may not have learned the underlying rules equally well for all classes, possibly due to class imbalance or insufficient representation of certain patterns in the training data. Further investigation into the data distribution and model behavior is recommended.", "plot_path": "experiments/2025-08-17_23-44-17_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_ea0d0b1198c64f54a7cd837268837510_proc_3477730/SPR_BENCH_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The analysis reveals that while the model achieves good training performance,\nthere are signs of overfitting, as indicated by the gap between training and\nvalidation metrics. The confusion matrix highlights areas for improvement in\nclassification accuracy, and the test Macro-F1 score matches the state-of-the-\nart benchmark, indicating potential but also room for further optimization.", "[]", "The plots demonstrate effective learning by the model, with consistent loss\nreduction and improving F1 scores. However, there is a noticeable gap in\nvalidation performance, and the confusion matrix highlights class imbalance\nissues that need to be addressed.", "The results indicate that while the model learns the training data effectively,\nits generalization to validation and test sets is limited. Overfitting is a\nsignificant issue, as evident from the divergence in loss and Macro-F1 scores.\nThe confusion matrix highlights the presence of misclassifications, suggesting\nroom for improvement in handling complex patterns or classes in the SPR task.\nStrategies like regularization, data augmentation, or architectural\nmodifications could be explored to enhance performance.", "The experimental results indicate that the model is overfitting to the training\ndata, as evidenced by the divergence between training and validation loss, as\nwell as the gap in F1 scores. Although the model achieves high performance on\ntraining data, its generalization to validation and test data is suboptimal.\nRegularization or architectural modifications may be necessary to address this\nlimitation.", "The plots reveal that the model fits the training data well but struggles to\ngeneralize to the validation set, as evidenced by increasing validation loss and\na plateau in validation F1 score. The confusion matrix indicates good overall\nclassification performance but highlights areas for improvement in handling\nspecific classes.", "The results indicate that the model learns effectively during training but\nsuffers from overfitting, as evidenced by the widening gap between training and\nvalidation performance. The final metrics reveal moderate generalization, but\nthere is a need for further optimization to close the gap between training and\nvalidation/test results. The confusion matrix highlights areas where the model\nstruggles with certain classes, suggesting potential directions for improvement.", "The plots highlight significant overfitting of the model to the training data,\nas evidenced by the divergence in loss and F1 score between training and\nvalidation datasets. The confusion matrix reveals imbalanced performance across\nclasses, suggesting potential issues with class representation or model bias.\nRegularization, data augmentation, or rebalancing strategies may be necessary to\nimprove generalization and address these issues.", "[]"], "exec_time": [14.509213209152222, 9.768418788909912, 5.386713743209839, 6.1136393547058105, 11.572536706924438, 14.817127227783203, 7.121495723724365, 4.211983680725098, 17.504175901412964], "exec_time_feedback": ["", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], [], ["[]"], ["[]"], ["[]"], ["[]"], ["[]"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# ---------- load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntest_scores = {}\n\nfor dset, rec in experiment_data.items():\n    epochs = np.asarray(rec.get(\"epochs\", []))\n    metrics = rec.get(\"metrics\", {})\n    losses = rec.get(\"losses\", {})\n    preds = np.asarray(rec.get(\"predictions\", []))\n    gts = np.asarray(rec.get(\"ground_truth\", []))\n\n    # ---------- plot 1: F1 curves ----------\n    try:\n        if len(epochs) and metrics.get(\"train_f1\") and metrics.get(\"val_f1\"):\n            plt.figure()\n            plt.plot(epochs, metrics[\"train_f1\"], label=\"Train\")\n            plt.plot(epochs, metrics[\"val_f1\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{dset}: Train vs Validation Macro-F1\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_f1_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 2: Loss curves ----------\n    try:\n        if len(epochs) and losses.get(\"train\") and losses.get(\"val\"):\n            plt.figure()\n            plt.plot(epochs, losses[\"train\"], label=\"Train\")\n            plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.title(f\"{dset}: Train vs Validation Loss\\nLeft: Train, Right: Val\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dset}: {e}\")\n        plt.close()\n\n    # ---------- plot 3: Confusion matrix ----------\n    try:\n        if preds.size and gts.size and preds.shape == gts.shape:\n            cm = confusion_matrix(gts, preds)\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(f\"{dset}: Confusion Matrix\\nLeft: Ground Truth, Right: Predicted\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            for i in range(cm.shape[0]):\n                for j in range(cm.shape[1]):\n                    plt.text(\n                        j,\n                        i,\n                        cm[i, j],\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{dset}_confusion_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix for {dset}: {e}\")\n        plt.close()\n\n    # ---------- compute / collect test F1 ----------\n    try:\n        test_f1 = metrics.get(\"test_f1\")\n        if test_f1 is None and preds.size and gts.size:\n            test_f1 = f1_score(gts, preds, average=\"macro\")\n        if test_f1 is not None:\n            test_scores[dset] = float(test_f1)\n    except Exception as e:\n        print(f\"Error computing test F1 for {dset}: {e}\")\n\n# ---------- bar chart across datasets ----------\ntry:\n    if test_scores:\n        plt.figure()\n        keys = list(test_scores.keys())\n        vals = [test_scores[k] for k in keys]\n        plt.bar(keys, vals, color=\"skyblue\")\n        for i, v in enumerate(vals):\n            plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Test Macro-F1 Comparison Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"datasets_test_f1_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison bar chart: {e}\")\n    plt.close()\n\n# ---------- print summary ----------\nprint(\"=== Test Macro-F1 Scores ===\")\nfor k, v in test_scores.items():\n    print(f\"{k}: {v:.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment data --------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# -------- iterate over all stored runs --------\nfor model_name, datasets in experiment_data.items():\n    for ds_name, data in datasets.items():\n        epochs = data.get(\"epochs\", [])\n        losses_tr = data.get(\"losses\", {}).get(\"train\", [])\n        losses_val = data.get(\"losses\", {}).get(\"val\", [])\n        f1_tr = data.get(\"metrics\", {}).get(\"train_f1\", [])\n        f1_val = data.get(\"metrics\", {}).get(\"val_f1\", [])\n        preds = np.array(data.get(\"predictions\", []))\n        gts = np.array(data.get(\"ground_truth\", []))\n\n        # ---- 1) loss curves ----\n        try:\n            if epochs and losses_tr and losses_val:\n                plt.figure()\n                plt.plot(epochs, losses_tr, label=\"Train Loss\")\n                plt.plot(epochs, losses_val, label=\"Val Loss\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Cross-Entropy Loss\")\n                plt.title(f\"{ds_name} \u2013 Loss Curve\\nModel: {model_name} (Train vs Val)\")\n                plt.legend()\n                fname = f\"{ds_name}_loss_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve for {ds_name}: {e}\")\n            plt.close()\n\n        # ---- 2) F1 curves ----\n        try:\n            if epochs and f1_tr and f1_val:\n                plt.figure()\n                plt.plot(epochs, f1_tr, label=\"Train F1\")\n                plt.plot(epochs, f1_val, label=\"Val F1\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Macro-F1\")\n                plt.title(f\"{ds_name} \u2013 F1 Curve\\nModel: {model_name} (Train vs Val)\")\n                plt.legend()\n                fname = f\"{ds_name}_f1_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating F1 curve for {ds_name}: {e}\")\n            plt.close()\n\n        # ---- 3) confusion matrix ----\n        try:\n            if preds.size and gts.size and preds.shape == gts.shape:\n                labels = np.unique(np.concatenate([preds, gts]))\n                cm = np.zeros((len(labels), len(labels)), dtype=int)\n                for p, t in zip(preds, gts):\n                    cm[t, p] += 1\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im, fraction=0.046, pad=0.04)\n                plt.xticks(range(len(labels)), labels, rotation=45)\n                plt.yticks(range(len(labels)), labels)\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.title(f\"{ds_name} \u2013 Confusion Matrix\\nModel: {model_name}\")\n                fname = f\"{ds_name}_confusion_matrix.png\"\n                plt.savefig(os.path.join(working_dir, fname), bbox_inches=\"tight\")\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating confusion matrix for {ds_name}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ---------- paths ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ned = experiment_data.get(\"no_positional_encoding\", {}).get(\"SPR_BENCH\", {})\n\nepochs = np.array(ed.get(\"epochs\", []))\ntrain_loss = np.array(ed.get(\"losses\", {}).get(\"train\", []))\nval_loss = np.array(ed.get(\"losses\", {}).get(\"val\", []))\ntrain_f1 = np.array(ed.get(\"metrics\", {}).get(\"train\", []))\nval_f1 = np.array(ed.get(\"metrics\", {}).get(\"val\", []))\ntest_f1 = ed.get(\"metrics\", {}).get(\"test\", None)\nSGA = ed.get(\"metrics\", {}).get(\"SGA\", None)\npreds = np.array(ed.get(\"predictions\", []))\ngts = np.array(ed.get(\"ground_truth\", []))\n\n# ---------- 1. Loss curve ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curve\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ---------- 2. F1 curve ----------\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train Macro-F1\")\n    plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curve\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"SPR_BENCH_F1_curve.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# ---------- 3. Confusion matrix ----------\ntry:\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds)\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        tick_marks = np.arange(cm.shape[0])\n        plt.xticks(tick_marks, tick_marks)\n        plt.yticks(tick_marks, tick_marks)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH Confusion Matrix (Test)\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ---------- print metrics ----------\nif len(val_f1):\n    print(f\"Best validation macro-F1: {max(val_f1):.4f}\")\nif test_f1 is not None:\n    print(f\"Test macro-F1: {test_f1:.4f}\")\nif SGA is not None:\n    print(f\"Systematic Generalization Accuracy: {SGA:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# quick sanity and key extraction\nexp_key = next(iter(experiment_data.keys())) if experiment_data else None\nbench_key = next(iter(experiment_data[exp_key].keys())) if exp_key else None\n\nif exp_key and bench_key:\n    data = experiment_data[exp_key][bench_key]\n    epochs = np.array(data[\"epochs\"])\n    tr_loss = np.array(data[\"losses\"][\"train\"])\n    val_loss = np.array(data[\"losses\"][\"val\"])\n    tr_f1 = np.array(data[\"metrics\"][\"train_f1\"])\n    val_f1 = np.array(data[\"metrics\"][\"val_f1\"])\n    test_f1 = data[\"metrics\"][\"test_f1\"]\n    sga = data[\"metrics\"][\"SGA\"]\n    preds = np.array(data[\"predictions\"])\n    gts = np.array(data[\"ground_truth\"])\n\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(f\"{bench_key}: Train vs Val Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{bench_key}_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # 2) F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{bench_key}: Train vs Val Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"{bench_key}_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # 3) Bar chart of final metrics\n    try:\n        plt.figure()\n        bars = [\"Train_F1_last\", \"Val_F1_last\", \"Val_F1_best\", \"Test_F1\", \"SGA\"]\n        vals = [tr_f1[-1], val_f1[-1], val_f1.max(), test_f1, sga]\n        plt.bar(bars, vals)\n        plt.ylim(0, 1)\n        plt.ylabel(\"Score\")\n        plt.title(f\"{bench_key}: Summary Metrics\")\n        fname = os.path.join(working_dir, f\"{bench_key}_metric_summary.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating metric bar chart: {e}\")\n        plt.close()\n\n    # 4) Confusion matrix (truncate to first 15 classes to stay readable)\n    try:\n        num_classes = int(max(gts.max(), preds.max()) + 1)\n        if num_classes <= 15:  # only plot if small enough\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for t, p in zip(gts, preds):\n                cm[t, p] += 1\n            plt.figure(figsize=(6, 5))\n            im = plt.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(f\"{bench_key}: Confusion Matrix\")\n            plt.xticks(np.arange(num_classes))\n            plt.yticks(np.arange(num_classes))\n            fname = os.path.join(working_dir, f\"{bench_key}_confusion_matrix.png\")\n            plt.savefig(fname, bbox_inches=\"tight\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ----- print key metrics -----\n    print(f\"Test macro-F1: {test_f1:.4f}, SGA: {sga:.4f}\")\nelse:\n    print(\"No experiment data found to plot.\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load data -------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nexp_key, ds_key = \"Frozen-Character-Embeddings\", \"SPR_BENCH\"\ned = experiment_data.get(exp_key, {}).get(ds_key, {})\n\nmetrics = ed.get(\"metrics\", {})\nlosses = ed.get(\"losses\", {})\nepochs = ed.get(\"epochs\", [])\n\n# print stored evaluation metrics --------------------------------------\nif metrics:\n    print(f\"Test macro-F1: {metrics.get('test_f1'):.4f}\")\n    print(f\"Systematic Generalization Accuracy: {metrics.get('SGA'):.4f}\")\n\n# 1. loss curve ---------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, losses.get(\"train\", []), label=\"Train\")\n    plt.plot(epochs, losses.get(\"val\", []), label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curve (Frozen-Character-Embeddings)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve_FrozenCharEmb.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. F1 curve -----------------------------------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, metrics.get(\"train_f1\", []), label=\"Train F1\")\n    plt.plot(epochs, metrics.get(\"val_f1\", []), label=\"Validation F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curve (Frozen-Character-Embeddings)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curve_FrozenCharEmb.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 3. confusion matrix ---------------------------------------------------\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    preds = np.array(ed.get(\"predictions\", []))\n    gts = np.array(ed.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        cm = confusion_matrix(gts, preds, labels=sorted(set(gts)))\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Frozen-Character-Embeddings)\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_confusion_matrix_FrozenCharEmb.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# -------- basic setup ---------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# -------- iterate & plot ------\nfor model_name, model_dict in experiment_data.items():\n    for dataset_name, rec in model_dict.items():\n        epochs = np.array(rec.get(\"epochs\", []))\n        tr_loss = np.array(rec.get(\"losses\", {}).get(\"train\", []))\n        vl_loss = np.array(rec.get(\"losses\", {}).get(\"val\", []))\n        tr_f1 = np.array(rec.get(\"metrics\", {}).get(\"train_f1\", []))\n        vl_f1 = np.array(rec.get(\"metrics\", {}).get(\"val_f1\", []))\n        test_f1 = rec.get(\"metrics\", {}).get(\"test_f1\", None)\n        sga = rec.get(\"metrics\", {}).get(\"SGA\", None)\n        preds = np.array(rec.get(\"predictions\", []))\n        gts = np.array(rec.get(\"ground_truth\", []))\n        num_classes = len(set(gts)) if gts.size else 0\n\n        # 1) Loss curve\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, vl_loss, label=\"Validation\")\n            plt.title(f\"{dataset_name} Loss vs Epochs ({model_name})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy\")\n            plt.legend()\n            fname = f\"{dataset_name}_{model_name}_loss_curve.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve: {e}\")\n            plt.close()\n\n        # 2) F1 curve\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train\")\n            plt.plot(epochs, vl_f1, label=\"Validation\")\n            plt.title(f\"{dataset_name} Macro-F1 vs Epochs ({model_name})\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.legend()\n            fname = f\"{dataset_name}_{model_name}_f1_curve.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating F1 curve: {e}\")\n            plt.close()\n\n        # 3) Bar chart of final metrics\n        try:\n            plt.figure()\n            bars = [\"Train_F1\", \"Val_F1\", \"Test_F1\", \"SGA\"]\n            vals = [\n                tr_f1[-1] if tr_f1.size else 0,\n                vl_f1[-1] if vl_f1.size else 0,\n                test_f1 if test_f1 is not None else 0,\n                sga if sga is not None else 0,\n            ]\n            plt.bar(bars, vals, color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"])\n            plt.ylim(0, 1)\n            plt.title(f\"{dataset_name} Final Metrics ({model_name})\")\n            fname = f\"{dataset_name}_{model_name}_final_metrics.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating metrics bar chart: {e}\")\n            plt.close()\n\n        # 4) Confusion matrix\n        if preds.size and gts.size and num_classes <= 50:  # sensible display cap\n            try:\n                cm = np.zeros((num_classes, num_classes), dtype=int)\n                for p, t in zip(preds, gts):\n                    cm[t, p] += 1\n                plt.figure(figsize=(6, 5))\n                plt.imshow(cm, cmap=\"Blues\")\n                plt.title(f\"{dataset_name} Confusion Matrix ({model_name})\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"True\")\n                plt.colorbar()\n                plt.tight_layout()\n                fname = f\"{dataset_name}_{model_name}_confusion_matrix.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating confusion matrix: {e}\")\n                plt.close()\n\n        # -------- console summary ----------\n        print(f\"[{model_name} | {dataset_name}] Test_F1={test_f1:.4f}  SGA={sga:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------- load data -------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp:\n    epochs = exp[\"epochs\"]\n    train_loss = exp[\"losses\"][\"train\"]\n    val_loss = exp[\"losses\"][\"val\"]\n    train_f1 = exp[\"metrics\"][\"train_f1\"]\n    val_f1 = exp[\"metrics\"][\"val_f1\"]\n    preds = np.array(exp[\"predictions\"])\n    gts = np.array(exp[\"ground_truth\"])\n    test_f1 = exp[\"metrics\"][\"test_f1\"]\n    sga = exp[\"metrics\"][\"SGA\"]\n\n    # --------------------- loss curves ------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH Loss Curves\\nTrain vs Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # ---------------------- F1 curves -------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(\"SPR_BENCH F1 Curves\\nTrain vs Validation\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # ------------------- confusion matrix --------------------\n    try:\n        if preds.size and gts.size:\n            cm = confusion_matrix(gts, preds)\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.title(\"SPR_BENCH Confusion Matrix\\nTest Set\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            # annotate cells\n            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n                plt.text(\n                    j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n                )\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # ---------------------- print metrics --------------------\n    print(f\"Test Macro-F1: {test_f1:.4f} | SGA: {sga:.4f}\")\n", null], "plot_plan": ["We load experiment_data.npy, loop over each dataset present, and for every dataset we extract epochs, train/val F1, train/val loss, predictions and labels. Within independent try-except blocks we: (1) draw and save F1 curves, (2) draw and save loss curves, (3) compute a confusion matrix when predictions exist and save the plot, and (4) compute test macro-F1 (either stored or recomputed) and store it in a dict. After iterating through all datasets we create a comparative bar-chart of test macro-F1 across datasets so users can quickly see relative performance. Each figure is saved to working_dir with descriptive names that include the dataset and plot type, subtitles clarify the split or content, and all figures are closed to free memory. The code respects the original numpy structure, never fabricates values, and only plots up to the available epochs (automatically \u22645 sample plots requirement is trivially satisfied here). Finally, all test-set scores are printed for reference.", null, "First, we load experiment_data.npy from the working directory, inspect the nested dict, and pull out losses, F1 scores, predictions, ground-truth labels, and epoch indices.  \nFor every (model_name, dataset_name) pair present, we produce up to three plots: (1) training/validation loss curves, (2) training/validation macro-F1 curves, and (3) a confusion-matrix heat-map for test predictions vs. ground truth.  \nEach figure is generated inside its own try-except block to prevent one failure from stopping subsequent plots; on error we print a short message and still close the figure.  \nAll figures are saved to working_dir with descriptive file names like \u201cSPR_BENCH_loss_curve.png\u201d.  \nTitles include dataset name, model name, and a short subtitle describing the content, following the requested style.  \nWe restrict ourselves to the data that actually exists in the .npy file and never fabricate values; if a particular field is missing or empty we skip that plot gracefully.  \nAfter saving we always call plt.close() to free resources, and we cap the total number of plot types at three (<5 as required).  \nFinally the script can be run as-is to create PNGs inside the working directory.", "We first load experiment_data.npy, fetch the SPR_BENCH entry, and pull out arrays for epochs, training/validation losses, training/validation macro-F1, the final test predictions, ground-truth labels, and SGA.  Three plots are produced: (1) a loss curve showing train vs. validation loss over epochs; (2) a macro-F1 curve for train vs. validation; and (3) a confusion-matrix heat-map on the held-out test set.  Each plot is wrapped in its own try-except so a single failure will not break the rest, and every figure is closed after saving.  Figure filenames include the dataset name and plot type for clarity and are written into the working directory.  All titles explicitly state the dataset plus a short subtitle clarifying content (e.g., \u201cLeft: Train, Right: Val\u201d).  We refrain from generating any extra or simulated data; everything plotted comes directly from experiment_data.npy.  Finally we print the key evaluation metrics (best validation macro-F1, test macro-F1, and SGA) so users can see quantitative results alongside the graphics.  No more than three figures are produced, satisfying the five-figure upper bound.", "We will first load experiment_data.npy from the working directory, extract the single experiment (\u201cUnnormalized-Symbol-Counts\u201d on \u201cSPR_BENCH\u201d), and pull out epochs, per-epoch train/val losses and F1 scores, plus the final test-set metrics and predictions.  \nThree main plots will be produced: (1) Training vs validation loss over epochs, (2) Training vs validation macro-F1 over epochs, and (3) a bar chart comparing the best/last train-F1, best/last val-F1, test-F1, and SGA.  \nBecause the predictions and ground-truth labels are also stored, we additionally create a confusion-matrix heat-map (limited to at most the first 15 classes to avoid unreadable figures).  \nEach plot sits in an independent try-except block; all figures are saved into working_dir with descriptive filenames that include the dataset and plot type, then immediately closed.  \nWe respect the five-figure limit (four figures total) and only use data already present in experiment_data.npy.  \nAfter plotting we print out the key evaluation metrics (test macro-F1 and SGA) so that running the script shows performance in the console as well.  \nThe code below follows all formatting, naming, and error-handling conventions specified, beginning with the required imports and working_dir setup.", "We will load experiment_data.npy, extract metrics for the \u201cFrozen-Character-Embeddings / SPR_BENCH\u201d run, and create up to three standard plots: (1) training vs. validation loss over epochs, (2) training vs. validation macro-F1 over epochs, and (3) a confusion-matrix heat-map of test predictions vs. ground-truth labels. Each plot is enclosed in its own try-except block, saved to working_dir with descriptive filenames, and the figure is always closed. After loading the data we also print the stored test macro-F1 and SGA values so users see key evaluation metrics in the console. Only data contained in experiment_data.npy are used\u2014no synthetic values are generated. The code obeys all directory, naming, and matplotlib guidelines while remaining concise.", "The script first loads the saved experiment_data.npy, then iterates over every model\u2013dataset pair it finds. For each pair it extracts epochs, train/val losses, train/val F1, final test F1, SGA, predictions and ground-truth. It then produces at most four standard plots, each in its own try-except block: (1) train vs. validation loss over epochs, (2) train vs. validation macro-F1 over epochs, (3) a bar chart summarising the final train/val/test F1 and SGA, and (4) a confusion-matrix heat-map if predictions are present. All figures are saved to the existing working directory with descriptive file names that include the dataset and plot type; every figure is explicitly closed. Finally the script prints a compact summary of the main evaluation metrics so users see results directly in the console.", "We load the saved experiment_data.npy, extract the SPR_BENCH record, and fetch arrays for epochs, train/val loss, and train/val macro-F1. Three figures are produced: (1) loss curves, (2) F1 curves, and (3) a confusion matrix of test predictions vs. ground-truth. Each figure is wrapped in its own try-except block to guarantee cleanup. Titles include the dataset name and a brief subtitle clarifying the content, and files are saved under working_dir with descriptive names. We cap plotting to these three essential visuals, well below the 5-figure limit. After plotting, we print the final test macro-F1 and SGA values to give immediate feedback in the notebook/console.", null], "ablation_name": [null, "Remove Symbol-Count Branch (Transformer-Only)", "Symbols-Only Classifier (Remove Transformer Branch)", "No Positional Encoding", "Unnormalized-Symbol-Counts", "Frozen-Character-Embeddings", "CLS-Token Pooling (Replace Mean Sequence Pooling with Dedicated [CLS] Token)", null, "BiGRU-Sequence-Encoder"], "hyperparam_name": [null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false], "parse_metrics_plan": ["The script will find the \u201cworking\u201d directory, load \u200bexperiment_data.npy, and\nconvert the stored object back to a Python dict.   For every top-level dataset\nkey (e.g. \u201cSPR_BENCH\u201d) it will compute:   \u2022 final training loss and F1 score\n(last epoch)   \u2022 best validation loss (minimum) and best validation F1 score\n(maximum)   \u2022 test F1 score and systematic generalization accuracy (SGA).   The\nscript then prints the dataset name followed by each metric name and its value\nwith clear, descriptive labels.", "We load experiment_data.npy from the working directory, convert it back to a\nnative Python dict, and iterate through each experiment and its contained\ndataset(s).   For every dataset we extract the stored metric/loss arrays and\nscalars, derive \u201cbest\u201d values where appropriate (max validation/train F1, final\nepoch losses), and then print them with explicit, unambiguous names.   The\nscript contains no special entry-point guard, so it executes immediately when\nrun.", "The script will locate the working directory, load the saved numpy dictionary,\nand iterate through every stored experiment.   For each dataset it prints the\ndataset name first, then clearly labelled metrics: final training loss, final\nvalidation loss, final training macro-F1, best validation macro-F1, test\nmacro-F1, and systematic generalization accuracy.   Arrays that track values\nacross epochs (e.g., losses, F1 scores) use the last value for \u201cfinal\u201d and the\nmaximum (or minimum where appropriate) for \u201cbest.\u201d   No plots are generated and\nall code runs immediately at import time, satisfying the structural constraints.", "The solution loads the `experiment_data.npy` file from the working directory,\nwalks through every stored dataset, derives the best or final values for each\nrecorded metric, and prints them with explicit, self-describing names. For F1\nscores the highest value is reported, for validation loss the lowest, while\ntraining loss is taken from the last epoch. All printing happens immediately\nwhen the script is run.", "The script loads the saved NumPy file from the working directory, unpacks the\nnested dictionaries, and for each dataset prints the most informative single\nvalue for every tracked metric\u2014choosing the maximum for F1 scores, the minimum\nfor losses, and the stored single value for test-time metrics and SGA.\nEverything is executed at import time without relying on a special entry point.", "The script loads the saved NumPy dictionary, walks through every dataset\ncontained in it, and prints the best (max\u2010F1 / min\u2010loss) or single stored value\nfor each metric in a clear, labeled format. Nothing is hidden behind a special\nentry-point, so running the file immediately shows the results.", "The script will locate the saved NumPy file inside the automatically-created\n\u201cworking\u201d directory, load the nested dictionary it contains, and iterate through\nevery pooling strategy and dataset it holds. For each dataset it extracts the\narrays or scalars that correspond to the recorded metrics and losses, computes\nthe final (last epoch) training values, the best validation values (max F1, min\nloss), and simply reports the stored test F1 score and SGA value. Each dataset\nname is printed first, followed by clearly labelled metric/value pairs\u2014no\nfigures or extra output are generated.", "The script simply loads the saved NumPy file from the working directory,\niterates over every dataset entry it contains, and prints a concise report. For\neach dataset it fetches the last (final) training loss, the last validation\nloss, the last training macro-F1 score, the best (maximum) validation macro-F1\nscore, the test macro-F1 score, and the systematic generalization accuracy that\nwere recorded during the run. Each printed line begins with an explicit metric\nname so the output is self-explanatory. Everything is executed immediately at\nimport time\u2014no special entry points or plotting are used.", "The script will locate the saved experiment_data.npy file inside the \u201cworking\u201d\ndirectory, load it with NumPy, and traverse the nested dictionary.   For each\ndataset it will compute:   \u2022 the best (maximum) training macro-F1 score,   \u2022 the\nbest (maximum) validation macro-F1 score,   \u2022 the reported test macro-F1 score,\n\u2022 the systematic generalization accuracy,   \u2022 the lowest training loss, and   \u2022\nthe lowest validation loss.   It then prints the dataset name followed by each\nmetric label and its value with four-decimal precision."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"{data_path} not found\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef final_value(values):\n    \"\"\"Return the last element of a list, or None if empty.\"\"\"\n    return values[-1] if values else None\n\n\ndef best_value(values, mode=\"min\"):\n    \"\"\"Return best (min or max) value from list, or None.\"\"\"\n    if not values:\n        return None\n    return min(values) if mode == \"min\" else max(values)\n\n\ndef print_metric(name: str, value):\n    if value is None:\n        return\n    print(f\"    {name}: {value:.4f}\")\n\n\n# ---------- iterate over datasets ----------\nfor dataset_name, contents in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    losses = contents.get(\"losses\", {})\n    metrics = contents.get(\"metrics\", {})\n\n    # Training / validation losses\n    train_loss_final = final_value(losses.get(\"train\", []))\n    val_loss_best = best_value(losses.get(\"val\", []), mode=\"min\")\n    print_metric(\"final training loss\", train_loss_final)\n    print_metric(\"best validation loss\", val_loss_best)\n\n    # Training / validation F1\n    train_f1_final = final_value(metrics.get(\"train_f1\", []))\n    val_f1_best = best_value(metrics.get(\"val_f1\", []), mode=\"max\")\n    print_metric(\"final training F1 score\", train_f1_final)\n    print_metric(\"best validation F1 score\", val_f1_best)\n\n    # Test metrics and SGA\n    test_f1 = metrics.get(\"test_f1\")\n    sga = metrics.get(\"SGA\")\n    print_metric(\"test F1 score\", test_f1)\n    print_metric(\"systematic generalization accuracy\", sga)\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# -------------------- locate and load --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -------------------- helper to fetch best/final values ---\ndef _safe_best(arr, bigger_is_better=True):\n    if not arr:  # empty list\n        return None\n    return max(arr) if bigger_is_better else min(arr)\n\n\ndef _safe_last(arr):\n    if not arr:\n        return None\n    return arr[-1]\n\n\n# -------------------- iterate and report ------------------\nfor exp_name, datasets in experiment_data.items():\n    for ds_name, rec in datasets.items():\n        print(f\"\\nDataset: {ds_name}\")\n        # F1 scores\n        best_train_f1 = _safe_best(rec[\"metrics\"].get(\"train_f1\", []))\n        best_val_f1 = _safe_best(rec[\"metrics\"].get(\"val_f1\", []))\n        test_f1 = rec[\"metrics\"].get(\"test_f1\")\n        sga = rec[\"metrics\"].get(\"SGA\")\n\n        # Losses\n        final_train_loss = _safe_last(rec[\"losses\"].get(\"train\", []))\n        final_val_loss = _safe_last(rec[\"losses\"].get(\"val\", []))\n\n        # ---------- printing ----------\n        if best_train_f1 is not None:\n            print(f\"Best training F1 score:          {best_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Best validation F1 score:        {best_val_f1:.4f}\")\n        if test_f1 is not None:\n            print(f\"Test F1 score:                   {test_f1:.4f}\")\n        if sga is not None:\n            print(f\"Systematic Generalization Accuracy (SGA): {sga:.4f}\")\n        if final_train_loss is not None:\n            print(f\"Final training loss:             {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"Final validation loss:           {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Locate working directory and load the file ---------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"experiment_data.npy not found in {working_dir}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# 1. Iterate over all stored experiments ----------------------------------------------------------\nfor model_name, datasets in experiment_data.items():\n    for ds_name, ds_dict in datasets.items():\n        # Retrieve metric/loss containers\n        metrics = ds_dict.get(\"metrics\", {})\n        losses = ds_dict.get(\"losses\", {})\n\n        # Compute best/final statistics where applicable\n        train_losses = losses.get(\"train\", [])\n        val_losses = losses.get(\"val\", [])\n        train_f1s = metrics.get(\"train_f1\", [])\n        val_f1s = metrics.get(\"val_f1\", [])\n\n        final_train_loss = train_losses[-1] if train_losses else None\n        final_val_loss = val_losses[-1] if val_losses else None\n        final_train_f1 = train_f1s[-1] if train_f1s else None\n        best_val_f1 = max(val_f1s) if val_f1s else None\n\n        test_f1 = metrics.get(\"test_f1\", None)\n        sga = metrics.get(\"SGA\", None)\n\n        # 2. Print results in the required format -------------------------------------------------\n        print(f\"\\nDataset: {ds_name}\")\n        if final_train_loss is not None:\n            print(f\"Final training loss: {final_train_loss:.4f}\")\n        if final_val_loss is not None:\n            print(f\"Final validation loss: {final_val_loss:.4f}\")\n        if final_train_f1 is not None:\n            print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n        if test_f1 is not None:\n            print(f\"Test macro F1 score: {test_f1:.4f}\")\n        if sga is not None:\n            print(f\"Systematic Generalization Accuracy: {sga:.4f}\")\n", "import os\nimport numpy as np\n\n# ----------- locate and load ------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"{file_path} not found\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ----------- helpers --------------------\ndef summarize_dataset(edict):\n    \"\"\"Return a dict containing best/final metrics given an experiment sub-dictionary.\"\"\"\n    metrics = edict[\"metrics\"]\n    losses = edict[\"losses\"]\n\n    summary = {}\n    # F1 scores --------------------------------------------------------------\n    if metrics[\"train\"]:\n        summary[\"Best training F1 score\"] = max(metrics[\"train\"])\n    if metrics[\"val\"]:\n        summary[\"Best validation F1 score\"] = max(metrics[\"val\"])\n    if metrics[\"test\"] is not None:\n        summary[\"Test F1 score\"] = metrics[\"test\"]\n\n    # Losses -----------------------------------------------------------------\n    if losses[\"train\"]:\n        summary[\"Final training loss\"] = losses[\"train\"][-1]\n    if losses[\"val\"]:\n        summary[\"Best validation loss\"] = min(losses[\"val\"])\n\n    # Systematic Generalization ---------------------------------------------\n    if metrics.get(\"SGA\") is not None:\n        summary[\"Systematic generalization accuracy\"] = metrics[\"SGA\"]\n\n    return summary\n\n\n# ----------- iterate and print ----------\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, exp_dict in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        best_vals = summarize_dataset(exp_dict)\n        for metric_name, value in best_vals.items():\n            print(f\"{metric_name}: {value:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ---------- locate and load experiment data ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ---------- utility helpers ----------\ndef safe_best(values, better=max, default=None):\n    \"\"\"Return best value from list or default if list is empty / None.\"\"\"\n    if isinstance(values, (list, tuple)) and values:\n        return better(values)\n    return default\n\n\ndef print_metric(name: str, value):\n    if value is not None:\n        if isinstance(value, float):\n            print(f\"{name}: {value:.4f}\")\n        else:\n            print(f\"{name}: {value}\")\n\n\n# ---------- iterate through experiments / datasets ----------\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, record in datasets.items():\n        print(dataset_name)  # dataset heading\n\n        metrics = record.get(\"metrics\", {})\n        losses = record.get(\"losses\", {})\n\n        # training metrics\n        print_metric(\"Best train F1 score\", safe_best(metrics.get(\"train_f1\")))\n        print_metric(\"Lowest train loss\", safe_best(losses.get(\"train\"), better=min))\n\n        # validation metrics\n        print_metric(\"Best validation F1 score\", safe_best(metrics.get(\"val_f1\")))\n        print_metric(\"Lowest validation loss\", safe_best(losses.get(\"val\"), better=min))\n\n        # test / OOD metrics\n        print_metric(\"Test F1 score\", metrics.get(\"test_f1\"))\n        print_metric(\"Systematic Generalization Accuracy\", metrics.get(\"SGA\"))\n\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# ----- locate and load experiment results -----\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_file):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_file}\")\n\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# ----- helper functions --------------------------------------------------------\ndef best_or_none(values, mode=\"max\"):\n    \"\"\"Return best value from list based on mode or None if list is empty.\"\"\"\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\ndef print_metric(name, value, fmt=\".4f\"):\n    \"\"\"Print metric name and value if value is not None.\"\"\"\n    if value is not None:\n        print(f\"{name}: {value:{fmt}}\")\n\n\n# ----- iterate and report -------------------------------------------------------\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(dataset_name)  # dataset heading\n\n        metrics = data.get(\"metrics\", {})\n        losses = data.get(\"losses\", {})\n\n        # F1 scores\n        print_metric(\n            \"Training F1 score\", best_or_none(metrics.get(\"train_f1\", []), mode=\"max\")\n        )\n        print_metric(\n            \"Validation F1 score\", best_or_none(metrics.get(\"val_f1\", []), mode=\"max\")\n        )\n        print_metric(\"Test F1 score\", metrics.get(\"test_f1\"))\n\n        # Losses\n        print_metric(\"Training loss\", best_or_none(losses.get(\"train\", []), mode=\"min\"))\n        print_metric(\"Validation loss\", best_or_none(losses.get(\"val\", []), mode=\"min\"))\n\n        # SGA\n        print_metric(\"Systematic Generalization Accuracy\", metrics.get(\"SGA\"))\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# helper functions for picking best/final metrics\n# ------------------------------------------------------------------\ndef final_value(seq):\n    \"\"\"Return the last element of a non-empty list, else None.\"\"\"\n    return seq[-1] if seq else None\n\n\ndef best_validation_f1(seq):\n    \"\"\"Return the maximum validation F1 score.\"\"\"\n    return max(seq) if seq else None\n\n\ndef best_validation_loss(seq):\n    \"\"\"Return the minimum validation loss.\"\"\"\n    return min(seq) if seq else None\n\n\n# ------------------------------------------------------------------\n# iterate and print\n# ------------------------------------------------------------------\nfor pooling_strategy, datasets in experiment_data.items():  # e.g. \"CLS_TOKEN_POOL\"\n    for dataset_name, record in datasets.items():  # e.g. \"SPR_BENCH\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # losses\n        train_losses = record.get(\"losses\", {}).get(\"train\", [])\n        val_losses = record.get(\"losses\", {}).get(\"val\", [])\n\n        # f1 metrics\n        train_f1s = record.get(\"metrics\", {}).get(\"train_f1\", [])\n        val_f1s = record.get(\"metrics\", {}).get(\"val_f1\", [])\n        test_f1 = record.get(\"metrics\", {}).get(\"test_f1\")\n        sga_val = record.get(\"metrics\", {}).get(\"SGA\")\n\n        # print metrics with explicit labels\n        fv = final_value(train_f1s)\n        if fv is not None:\n            print(f\"Final training F1 score: {fv:.4f}\")\n\n        bv = best_validation_f1(val_f1s)\n        if bv is not None:\n            print(f\"Best validation F1 score: {bv:.4f}\")\n\n        if test_f1 is not None:\n            print(f\"Test F1 score: {test_f1:.4f}\")\n\n        if sga_val is not None:\n            print(f\"Systematic Generalization Accuracy: {sga_val:.4f}\")\n\n        fl = final_value(train_losses)\n        if fl is not None:\n            print(f\"Final training loss: {fl:.4f}\")\n\n        bl = best_validation_loss(val_losses)\n        if bl is not None:\n            print(f\"Best validation loss: {bl:.4f}\")\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------\n# iterate through datasets and report metrics\nfor dataset_name, record in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # ---- losses ----\n    train_losses = record.get(\"losses\", {}).get(\"train\", [])\n    val_losses = record.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]:.6f}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n\n    # ---- F1 scores ----\n    metrics = record.get(\"metrics\", {})\n    train_f1 = metrics.get(\"train_f1\", [])\n    val_f1 = metrics.get(\"val_f1\", [])\n    if train_f1:\n        print(f\"Final training macro-F1 score: {train_f1[-1]:.6f}\")\n    if val_f1:\n        print(f\"Best validation macro-F1 score: {max(val_f1):.6f}\")\n\n    test_f1 = metrics.get(\"test_f1\")\n    if test_f1 is not None:\n        print(f\"Test macro-F1 score: {test_f1:.6f}\")\n\n    sga = metrics.get(\"SGA\")\n    if sga is not None:\n        print(f\"Systematic generalization accuracy: {sga:.6f}\")\n", "import os\nimport numpy as np\n\n# ---------- locate and load ----------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------- helpers ----------\ndef format_float(x):\n    return f\"{x:.4f}\" if isinstance(x, (int, float, np.floating)) else str(x)\n\n\ndef best_value(values, mode=\"max\"):\n    \"\"\"Return best value from a list according to mode ('max' or 'min').\"\"\"\n    if not values:  # safety check for empty list\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\n# ---------- iterate and print ----------\nfor _, datasets in experiment_data.items():  # e.g., \"BiGRU\"\n    for dataset_name, details in datasets.items():  # e.g., \"SPR_BENCH\"\n        print(dataset_name)  # dataset header\n\n        # ----- F1 scores -----\n        train_f1_best = best_value(details[\"metrics\"][\"train_f1\"], mode=\"max\")\n        val_f1_best = best_value(details[\"metrics\"][\"val_f1\"], mode=\"max\")\n        test_f1 = details[\"metrics\"][\"test_f1\"]\n        sga_score = details[\"metrics\"][\"SGA\"]\n\n        print(f\"train macro-F1 score (best): {format_float(train_f1_best)}\")\n        print(f\"validation macro-F1 score (best): {format_float(val_f1_best)}\")\n        print(f\"test macro-F1 score: {format_float(test_f1)}\")\n        print(f\"systematic generalization accuracy: {format_float(sga_score)}\")\n\n        # ----- Losses -----\n        train_loss_best = best_value(details[\"losses\"][\"train\"], mode=\"min\")\n        val_loss_best = best_value(details[\"losses\"][\"val\"], mode=\"min\")\n\n        print(f\"train loss (lowest): {format_float(train_loss_best)}\")\n        print(f\"validation loss (lowest): {format_float(val_loss_best)}\\n\")\n"], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', '    final training loss: 0.0161', '\\n', '    best\nvalidation loss: 0.6775', '\\n', '    final training F1 score: 0.9960', '\\n', '\nbest validation F1 score: 0.6960', '\\n', '    test F1 score: 0.6999', '\\n', '\nsystematic generalization accuracy: 0.0000', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training F1 score:          0.9775', '\\n',\n'Best validation F1 score:        0.6820', '\\n', 'Test F1 score:\n0.6926', '\\n', 'Systematic Generalization Accuracy (SGA): 0.0000', '\\n', 'Final\ntraining loss:             0.0864', '\\n', 'Final validation loss:\n1.9407', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.6567', '\\n', 'Final\nvalidation loss: 0.6785', '\\n', 'Final training macro F1 score: 0.8985', '\\n',\n'Best validation macro F1 score: 0.6819', '\\n', 'Test macro F1 score: 0.6777',\n'\\n', 'Systematic Generalization Accuracy: 0.0000', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best training F1 score: 0.9730', '\\n', 'Best\nvalidation F1 score: 0.6675', '\\n', 'Test F1 score: 0.6884', '\\n', 'Final\ntraining loss: 0.1114', '\\n', 'Best validation loss: 0.7518', '\\n', 'Systematic\ngeneralization accuracy: 0.0000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'Best train F1 score: 0.9970', '\\n', 'Lowest train loss:\n0.0146', '\\n', 'Best validation F1 score: 0.6980', '\\n', 'Lowest validation\nloss: 0.7147', '\\n', 'Test F1 score: 0.6978', '\\n', 'Systematic Generalization\nAccuracy: 0.0000', '\\n', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['SPR_BENCH', '\\n', 'Training F1 score: 0.9940', '\\n', 'Validation F1 score:\n0.6939', '\\n', 'Test F1 score: 0.6958', '\\n', 'Training loss: 0.0244', '\\n',\n'Validation loss: 0.6775', '\\n', 'Systematic Generalization Accuracy: 0.0000',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Final training F1 score: 0.9695', '\\n', 'Best\nvalidation F1 score: 0.6780', '\\n', 'Test F1 score: 0.6895', '\\n', 'Systematic\nGeneralization Accuracy: 0.0000', '\\n', 'Final training loss: 0.1045', '\\n',\n'Best validation loss: 0.6652', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.028892', '\\n', 'Final\nvalidation loss: 2.177748', '\\n', 'Final training macro-F1 score: 0.992000',\n'\\n', 'Best validation macro-F1 score: 0.695995', '\\n', 'Test macro-F1 score:\n0.690985', '\\n', 'Systematic generalization accuracy: 0.000000', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train macro-F1 score (best): 0.9995', '\\n', 'validation\nmacro-F1 score (best): 0.7020', '\\n', 'test macro-F1 score: 0.6979', '\\n',\n'systematic generalization accuracy: 0.0000', '\\n', 'train loss (lowest):\n0.0025', '\\n', 'validation loss (lowest): 0.6633\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is 30 minutes).']"], "parse_exc_type": [null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}