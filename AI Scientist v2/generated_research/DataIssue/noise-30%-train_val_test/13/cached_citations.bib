
% This paper introduces the Transformer architecture, which serves as the foundational basis for the models used in this research. It should be cited in the methodology section when describing the baseline Transformer model used in the experiments.
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}

% This paper provides a comprehensive overview of neural-symbolic learning and reasoning frameworks, addressing their contributions and challenges. It is relevant for discussing the integration of symbolic reasoning capabilities into transformers and should be cited in the related work and methodology sections of the research.
@article{garcez2015neuralsymbolicla,
 author = {A. Garcez and Tarek R. Besold and L. D. Raedt and Peter Földiák and P. Hitzler and Thomas F. Icard and Kai-Uwe Kühnberger and L. Lamb and R. Miikkulainen and Daniel L. Silver},
 booktitle = {AAAI Spring Symposia},
 title = {Neural-Symbolic Learning and Reasoning: Contributions and Challenges},
 year = {2015}
}

% The paper 'Systematic Generalization with Edge Transformers' discusses systematic generalization in neural models, particularly Transformers, and introduces innovative architectural changes inspired by symbolic AI. This is highly relevant for discussing systematic generalization challenges and the integration of symbolic reasoning modules in the proposed model. It should be cited in the methodology and related work sections where systematic generalization in symbolic reasoning tasks is discussed.
@article{bergen2021systematicgw,
 author = {Leon Bergen and T. O’Donnell and Dzmitry Bahdanau},
 booktitle = {Neural Information Processing Systems},
 pages = {1390-1402},
 title = {Systematic Generalization with Edge Transformers},
 year = {2021}
}

% Paper 1 discusses the Multi-LogiEval benchmark, which evaluates multi-step logical reasoning with various inference rules, highlighting challenges in systematic generalization. Paper 2 introduces the Symbolic Chain-of-Thought (SymbCoT) framework, combining symbolic logic with reasoning steps to enhance explainability and logical reasoning capabilities. Both papers are relevant for supporting the methodology and research gap discussions in the proposed work, particularly regarding systematic generalization and the integration of symbolic reasoning modules in the SPR task.
@article{patel2024multilogievalte,
 author = {Nisarg Patel and Mohith Kulkarni and Mihir Parmar and Aashna Budhiraja and Mutsumi Nakamura and Neeraj Varshney and Chitta Baral},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 journal = {ArXiv},
 title = {Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models},
 volume = {abs/2406.17169},
 year = {2024}
}

@article{xu2024faithfullr,
 author = {Jundong Xu and Hao Fei and Liangming Pan and Qian Liu and M. Lee and W. Hsu},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 pages = {13326-13365},
 title = {Faithful Logical Reasoning via Symbolic Chain-of-Thought},
 year = {2024}
}

% The ORCHARD benchmark is a diagnostic dataset for evaluating systematic generalization in neural models, particularly in multi-hierarchical reasoning tasks. This is relevant for discussing systematic generalization challenges and supporting the methodology section, as it aligns with the objectives of the SPR_BENCH dataset used in the research.
@article{pung2021orchardab,
 author = {B. Pung and Alvin Chan},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {ORCHARD: A Benchmark For Measuring Systematic Generalization of Multi-Hierarchical Reasoning},
 volume = {abs/2111.14034},
 year = {2021}
}
