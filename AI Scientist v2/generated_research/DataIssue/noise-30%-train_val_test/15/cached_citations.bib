
% The paper 'Neural Turing Machines' by Graves et al. (2014) introduces a model that couples neural networks with external memory resources, enabling differentiable end-to-end training and the inference of algorithms. This foundational work is directly relevant to the neural-symbolic integration discussed in the research and will be cited to support the incorporation of symbolic reasoning modules into transformer models.
@article{graves2014neuraltm,
 author = {Alex Graves and Greg Wayne and Ivo Danihelka},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Neural Turing Machines},
 volume = {abs/1410.5401},
 year = {2014}
}

% The paper 'A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task' by Brinkmann et al. (2024) provides a detailed analysis of how transformers perform symbolic multi-step reasoning tasks, identifying interpretable mechanisms used by the model. This work is highly relevant as it supports the hypothesis that transformers are capable of symbolic reasoning and provides foundational insights into the internal mechanisms of transformers in such tasks. This citation will be used to back the claim that transformers can effectively perform symbolic reasoning and to contextualize the use of symbolic reasoning tasks in the proposed research.
@article{brinkmann2024ama,
 author = {Jannik Brinkmann and A. Sheshadri and Victor Levoso and Paul Swoboda and Christian Bartelt},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 pages = {4082-4102},
 title = {A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task},
 year = {2024}
}

% Paper 0 provides a comprehensive overview of neural-symbolic computing, focusing on the integration of neural learning with symbolic reasoning to create interpretable AI systems. This is directly relevant for supporting the symbolic reasoning modules in the proposed transformer model. Paper 4 maps neuro-symbolic techniques into architectural families, linking specific strengths of frameworks to their architectures. It is relevant for discussing how symbolic reasoning can augment neural networks and fits well into the discussion of neural-symbolic integration in the paper's methodology section.
@article{garcez2019neuralsymbolicca,
 author = {A. Garcez and M. Gori and L. Lamb and L. Serafini and Michael Spranger and S. Tran},
 booktitle = {FLAP},
 journal = {FLAP},
 pages = {611-632},
 title = {Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning},
 volume = {6},
 year = {2019}
}

@article{feldstein2024mappingtn,
 author = {Jonathan Feldstein and Paulius Dilkas and Vaishak Belle and Efthymia Tsamoura},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Mapping the Neuro-Symbolic AI Landscape by Architectures: A Handbook on Augmenting Deep Learning Through Symbolic Reasoning},
 volume = {abs/2410.22077},
 year = {2024}
}

% Paper 1 introduces FinChain, a symbolic benchmark for multi-step financial reasoning, emphasizing the importance of intermediate reasoning steps and verifiable chain-of-thought processes. It is relevant for contextualizing the SPR_BENCH dataset and symbolic reasoning tasks in the proposed research. Paper 4 introduces GNS-260K, a large dataset for plane geometry problems that integrates symbolic reasoning and annotations, which supports the importance of datasets incorporating symbolic reasoning. These should be cited in the related work section to highlight the relevance of SPR_BENCH and the importance of symbolic reasoning benchmarks.
@article{xie2025finchainas,
 author = {Zhuohan Xie and Dhruv Sahnan and Debopriyo Banerjee and Georgi N. Georgiev and Rushil Thareja and Hachem Madmoun and Jinyan Su and Aaryamonvikram Singh and Yuxia Wang and Rui Xing and Fajri Koto and Haonan Li and Ivan Koychev and Tanmoy Chakraborty and Salem Lahlou and Veselin Stoyanov and Preslav Nakov},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning},
 volume = {abs/2506.02515},
 year = {2025}
}

@article{ning2025gnssp,
 author = {Maizhen Ning and Zihao Zhou and Qiufeng Wang and Xiaowei Huang and Kaizhu Huang},
 booktitle = {AAAI Conference on Artificial Intelligence},
 pages = {24957-24965},
 title = {GNS: Solving Plane Geometry Problems by Neural-Symbolic Reasoning with Multi-Modal LLMs},
 year = {2025}
}

% Paper 3 discusses a Transformer-based hybrid neural network model for reasoning tasks, emphasizing enhanced inference capabilities through positional information and attention mechanisms. It is relevant for supporting the methodology of augmenting transformers with symbolic reasoning. Paper 4 introduces natural language embedded programs (NLEP) as a framework for symbolic and numeric reasoning, showcasing how interpretable reasoning can be achieved by combining symbolic computation with language models. These citations will be used in the related work and methodology sections to strengthen the discussion on hybrid approaches and symbolic reasoning integration.
@conference{jiao2024transformerhn,
 author = {Bingyu Jiao and Yongzhong Huang},
 booktitle = {IEEE Advanced Information Technology, Electronic and Automation Control Conference},
 journal = {2024 IEEE 7th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
 pages = {901-906},
 title = {Transformer Hybrid Neural Network Model for Natural Language Reasoning},
 volume = {7},
 year = {2024}
}

@article{zhang2023naturalle,
 author = {Tianhua Zhang and Jiaxin Ge and Hongyin Luo and Yung-Sung Chuang and Mingye Gao and Yuan Gong and Xixin Wu and Yoon Kim and Helen Meng and James R. Glass},
 booktitle = {NAACL-HLT},
 pages = {4131-4155},
 title = {Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning},
 year = {2023}
}

% Paper 0 introduces the concept of a buffer mechanism to enhance generalization in symbolic reasoning tasks using transformers, which aligns with the focus on improving generalization capabilities in the proposed research. It will be cited to support the discussion on methodologies for enhancing reasoning and generalization in transformer models. Paper 2 studies the emergence of generalization in symbolic reasoning tasks within transformers, offering insights into interpretability and representational structure. It is relevant for contextualizing the generalization analysis in the experiments and providing a theoretical basis for understanding generalization behaviors in the proposed model.
@inproceedings{wang2024thebm,
 author = {Zhiwei Wang and Yunji Wang and Zhongwang Zhang and Zhangchen Zhou and Hui Jin and Tianyang Hu and Jiacheng Sun and Zhenguo Li and Yaoyu Zhang and Z. Xu},
 title = {The Buffer Mechanism for Multi-Step Information Reasoning in Language Models},
 year = {2024}
}

@article{ye2025howdt,
 author = {Jiaran Ye and Zijun Yao and Zhidian Huang and Liangming Pan and Jinxin Liu and Yushi Bai and Amy Xin and Weichuan Liu and Xiaoyin Che and Lei Hou and Juanzi Li},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {How does Transformer Learn Implicit Reasoning?},
 volume = {abs/2505.23653},
 year = {2025}
}

% The paper 'A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts' introduces rsbench, a benchmark suite for evaluating reasoning shortcuts and concept quality in neural-symbolic models. This is directly relevant to the SPR_BENCH dataset used in the proposed research, as both focus on symbolic reasoning and generalization capabilities. It should be cited in the related work section to contextualize the challenges in reasoning tasks and the role of benchmarks in advancing model evaluation.
@article{bortolotti2024anb,
 author = {Samuele Bortolotti and Emanuele Marconato and Tommaso Carraro and Paolo Morettin and Emile van Krieken and Antonio Vergari and Stefano Teso and Andrea Passerini},
 booktitle = {Neural Information Processing Systems},
 title = {A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts},
 year = {2024}
}

% The paper 'SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating Mechanisms' introduces an innovative approach to enhancing memory and information flow in neural networks by leveraging path signatures in gating mechanisms. This is relevant to the proposed research as it provides theoretical and practical insights into the design of gating mechanisms, a key component of the symbolic reasoning modules in the transformer architecture. It will be cited in the methodology section to support the discussion on gating mechanisms and their role in improving symbolic reasoning capabilities.
@article{genet2025siggateer,
 author = {R'emi Genet and Hugo Inzirillo},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating Mechanisms},
 volume = {abs/2502.09318},
 year = {2025}
}
