{"edges": [[0, 9], [0, 12], [0, 3], [0, 8], [0, 5], [0, 1], [0, 2], [0, 10], [0, 6], [0, 7], [0, 4], [0, 11]], "layout": [[0.5, 0.0], [0.0, 1.0], [0.09090909090909091, 1.0], [0.18181818181818182, 1.0], [0.2727272727272727, 1.0], [0.36363636363636365, 1.0], [0.45454545454545453, 1.0], [0.5454545454545454, 1.0], [0.6363636363636364, 1.0], [0.7272727272727273, 1.0], [0.8181818181818182, 1.0], [0.9090909090909091, 1.0], [1.0, 1.0]], "plan": ["To probe the impact of richer symbolic features and tighter neural-symbolic\nfusion, I will (1) extend the symbolic branch with a 64-dimensional hashed\nbigram histogram and a normalised length feature, (2) learn a gating vector from\nthe symbolic embedding that multiplicatively modulates the pooled Transformer\nrepresentation, and (3) train with label-smoothed cross-entropy and a cosine-\ndecay schedule to fight over-fitting. The resulting \u201cGated-Hybrid Transformer\u201d\nkeeps parameter count modest while giving the network global statistical cues\n(unigrams, bigrams, length) and an explicit mechanism to let symbolic evidence\nemphasise or suppress token-level patterns. We retain the previous lightweight\ncharacter-level encoder, regularise with dropout and weight-decay, and report\nMacro-F1 each epoch. The script below is self-contained: it loads SPR_BENCH if\npresent (otherwise synthesises toy data), trains for a few epochs on GPU/CPU,\nprints validation loss and Macro-F1, evaluates on the test split, and stores all\nmetrics/predictions in ./working/experiment_data.npy for later analysis.", "Ablation name: Remove_Gating_Mechanism. The ablation simply deletes the learned\nsigmoid gate that adaptively scales the transformer representation before\nfusion. Instead, we directly concatenate the pooled transformer output with the\nprojected symbolic features, allowing us to isolate whether performance gains\narise from the extra features themselves or from the gating mechanism.\nEverything else (data pipeline, training loop, metric logging and persistence)\nremains identical to the baseline so comparison is fair.", "Ablation name: Remove_Symbolic_Feature_Pathway. The ablation is realised by\ndeleting every layer that processes the symbolic feature vector.  The new model\n\u2013 CharOnlyTransformer \u2013 in-gests only (padded) character IDs, applies positional\nencodings, a shallow Transformer encoder, average-pools the hidden states and\nfeeds them through a classifier head.  The rest of the training / evaluation\nloop, the synthetic-fallback data loader, metrics logging and saving remain\nunchanged, so we can re-use almost all baseline code while simply ignoring the\nsym_feat argument.", "Ablation name: Remove_Positional_Encoding. The ablation simply replaces the\nsinusoidal positional\u2010encoding module with nn.Identity so the transformer\nreceives raw character embeddings. Everything else (dataset processing, hybrid\ngating, training loop, logging and saving) stays unchanged, enabling a direct\ncomparison with the original model. Results are stored under the key\n'Remove_Positional_Encoding' in experiment_data and saved as\nexperiment_data.npy.", "Ablation name: Remove_Bigram_Hashed_Features. We rerun the full training\npipeline but force the symbolic feature vector\u2019s bigram portion to zero, while\nkeeping dimensions unchanged. This isolates the impact of the hashed-bigram\nsignal because the rest of the architecture (and hyper-parameters) is untouched.\nResults, losses and predictions are stored under the ablation key\n\u201cRemove_Bigram_Hashed_Features\u201d in the required experiment_data structure and\nsaved to experiment_data.npy.", "Ablation name: Remove_Unigram_Count_Features. We rerun the full training loop\nbut modify the symbolic feature pathway so that the unigram-frequency vector is\nreplaced by an all-zero tensor of identical dimensionality; the hashed bigram\nstatistics and the normalized length feature are left untouched.  The model\narchitecture, optimization recipe, reporting boiler-plate and file-saving logic\nremain unchanged.  All results are stored under the ablation key\n\u201cRemove_Unigram_Count_Features\u201d and written to `experiment_data.npy` so they can\nbe compared directly with earlier runs.", "Ablation name: Remove_Label_Smoothing_Loss. The ablation simply swaps out the\ncustom LabelSmoothingCE for the vanilla torch.nn.CrossEntropyLoss while keeping\nevery other component untouched. This lets us isolate the contribution of label-\nsmoothing to generalisation on SPR_BENCH. The script below reproduces the full\ntraining / evaluation pipeline, logs per-epoch macro-F1 and losses, and stores\nevery plottable artefact in the required experiment_data.npy file under the\nablation key \"Remove_Label_Smoothing_Loss\".", "Ablation name: Remove_Length_Feature. The ablation is realised by dropping the\nexplicit length scalar from the symbolic feature vector and read-justing any\ndimensionalities that depended on it. Concretely, SPRTorchDataset no longer\nappends length_feat, and the model\u2019s sym_proj input dimension is reduced by one.\nEverything else (data loading, training loop, metrics collection, saving\nexperiment_data.npy) is kept identical so that results are directly comparable\nwith the baseline. The script below is completely self-contained and executes\nthe full training / evaluation pipeline for the Remove_Length_Feature ablation.", "Ablation name: Remove_Transformer_Encoder. Below is the modified script: it\nkeeps every pipeline component intact except that the entire TransformerEncoder\n(and the positional encoding feeding it) is removed.  The character embeddings\nare simply averaged (adaptive\u2010average pooling) before passing through the\noriginal gating branch and classifier, letting us measure how much performance\ndrops when contextual self-attention is absent.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Remove-Gating-Mechanism ablation \u2013 self-contained script\nimport os, pathlib, random, math, time, numpy as np, torch\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# --------------------------------------------------------------------- #\n# experiment logging dict\nexperiment_data = {\n    \"Remove_Gating_Mechanism\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_key = \"Remove_Gating_Mechanism\"\n\n# --------------------------------------------------------------------- #\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# --------------------------------------------------------------------- #\n# load SPR-BENCH (or fallback synthetic)\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # helper\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 using synthetic toy data.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Number of labels:\", num_labels)\n\n# --------------------------------------------------------------------- #\n# vocabulary over characters\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nprint(\"Vocab size:\", vocab_size, \"| MAX_LEN:\", MAX_LEN)\n\n# --------------------------------------------------------------------- #\n# hashed bigram util\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# --------------------------------------------------------------------- #\n# torch dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq, self.lab = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1  # exclude PAD\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            pad = [PAD_ID] * (MAX_LEN - len(ids))\n            ids.extend(pad)\n            attn.extend([0] * len(pad))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_loader = DataLoader(SPRTorchDataset(spr[\"train\"]), 128, True, collate_fn=collate)\ndev_loader = DataLoader(SPRTorchDataset(spr[\"dev\"]), 256, False, collate_fn=collate)\ntest_loader = DataLoader(SPRTorchDataset(spr[\"test\"]), 256, False, collate_fn=collate)\n\n\n# --------------------------------------------------------------------- #\n# model \u2013 same backbone but WITHOUT gating\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass ConcatHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = (vocab_size - 1) + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)  # [B,L,dm]\n        x = self.pos(x)\n        x = self.encoder(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)  # [B,dm]\n        s = self.sym_proj(sym_feat)  # [B,64]\n        h = torch.cat([x, s], dim=-1)  # concat, no gating\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = ConcatHybridTransformer().to(device)\n\n\n# --------------------------------------------------------------------- #\n# criterion, optimiser, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, targets):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true = torch.zeros_like(log_probs)\n            true.fill_(self.smoothing / (n_class - 1))\n            true.scatter_(1, targets.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n\n# --------------------------------------------------------------------- #\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    f1 = f1_score(gts, preds, average=\"macro\")\n    return tot_loss / len(loader.dataset), f1, preds, gts\n\n\n# --------------------------------------------------------------------- #\nEPOCHS = 10\nstart = time.time()\n\nfor ep in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n\n    d = experiment_data[exp_key][\"SPR_BENCH\"]\n    d[\"losses\"][\"train\"].append(tr_loss)\n    d[\"losses\"][\"val\"].append(val_loss)\n    d[\"metrics\"][\"train\"].append(tr_f1)\n    d[\"metrics\"][\"val\"].append(val_f1)\n    d[\"epochs\"].append(ep)\n\n    print(f\"Epoch {ep}: val_loss={val_loss:.4f}, MacroF1={val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss={test_loss:.4f}, MacroF1={test_f1:.4f}\")\n\nd = experiment_data[exp_key][\"SPR_BENCH\"]\nd[\"predictions\"], d[\"ground_truth\"] = test_preds, test_gts\nd[\"test_loss\"], d[\"test_macroF1\"] = test_loss, test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved all metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# ------------------------------------------------------------\n# Ablation: Remove_Symbolic_Feature_Pathway\n# ------------------------------------------------------------\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# experiment bookkeeping dict\nexperiment_data = {\n    \"Remove_Symbolic_Feature_Pathway\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# SPR-BENCH or synthetic fallback\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 using synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Labels:\", num_labels)\n\n# ---------------------------------------------------------------------\n# vocabulary\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nprint(\"Vocab size:\", vocab_size, \"| MAX_LEN:\", MAX_LEN)\n\n# ---------------------------------------------------------------------\n# hashed bigram helper (still computed but unused)\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids / mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # ----- symbolic features (computed but ignored later)\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        # ----------------------------------------------------\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,  # kept for API compatibility\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# Positional encoding\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\n# ---------------------------------------------------------------------\n# Char-only Transformer (ablation model)\nclass CharOnlyTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat=None):\n        h = self.embed(input_ids)\n        h = self.pos(h)\n        h = self.encoder(h, src_key_padding_mask=(attention_mask == 0))\n        h = self.pool(h.transpose(1, 2)).squeeze(-1)  # [B, d_model]\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = CharOnlyTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# criterion, optimizer, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true = torch.zeros_like(log_probs)\n            true.fill_(self.smoothing / (n_class - 1))\n            true.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        total_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    scheduler.step()\n    ed = experiment_data[\"Remove_Symbolic_Feature_Pathway\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train\"].append(tr_f1)\n    ed[\"metrics\"][\"val\"].append(val_f1)\n    ed[\"epochs\"].append(epoch)\n    print(\n        f\"Epoch {epoch:02d} | train_loss {tr_loss:.4f} | val_loss {val_loss:.4f} | val_macroF1 {val_f1:.4f}\"\n    )\n\n# ---------------------------------------------------------------------\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test | loss {test_loss:.4f} | macroF1 {test_f1:.4f}\")\n\ned = experiment_data[\"Remove_Symbolic_Feature_Pathway\"][\"SPR_BENCH\"]\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\ned[\"test_loss\"] = test_loss\ned[\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "import os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# experiment bookkeeping\nexperiment_data = {\n    \"Remove_Positional_Encoding\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model (positional encoding removed)\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = nn.Identity()  # <-- removed sinusoidal encoding\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    ed = experiment_data[\"Remove_Positional_Encoding\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train\"].append(tr_f1)\n    ed[\"metrics\"][\"val\"].append(val_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f} | MacroF1={val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss={test_loss:.4f} | MacroF1={test_f1:.4f}\")\ned = experiment_data[\"Remove_Positional_Encoding\"][\"SPR_BENCH\"]\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\ned[\"test_loss\"] = test_loss\ned[\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "import os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# experiment dict skeleton\nexperiment_data = {\n    \"Remove_Bigram_Hashed_Features\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create toy synthetic dataset\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nBIGRAM_DIM = 64\n\n\n# ---------------------------------------------------------------------\n# hashed bigram helper (kept for completeness)\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset with REMOVED bigram hashed features\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            pad_len = MAX_LEN - len(ids)\n            ids += [PAD_ID] * pad_len\n            attn += [0] * pad_len\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # BIGRAM FEATURES ARE ZEROED FOR ABLATION\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)  # all zeros\n        # normalized length scalar\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = (\n            vocab_size - 1 + BIGRAM_DIM + 1\n        )  # unchanged, bigram slice will be zeros\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    exp = experiment_data[\"Remove_Bigram_Hashed_Features\"][\"SPR_BENCH\"]\n    exp[\"losses\"][\"train\"].append(tr_loss)\n    exp[\"losses\"][\"val\"].append(val_loss)\n    exp[\"metrics\"][\"train\"].append(tr_f1)\n    exp[\"metrics\"][\"val\"].append(val_f1)\n    exp[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\n\nexp[\"predictions\"] = test_preds\nexp[\"ground_truth\"] = test_gts\nexp[\"test_loss\"] = test_loss\nexp[\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Remove_Unigram_Count_Features Ablation \u2013 self-contained script\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ------------------ runtime folders / device --------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ------------------ dataset helpers -----------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {split: _load(f\"{split}.csv\") for split in [\"train\", \"dev\", \"test\"]}\n    )\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Labels:\", num_labels)\n\n# ------------------ vocab & constants ---------------------------------\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ------------------ torch Dataset (unigram counts zeroed) -------------\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1  # exclude PAD\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # token ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        pad_len = MAX_LEN - len(ids)\n        if pad_len:\n            ids += [PAD_ID] * pad_len\n            attn += [0] * pad_len\n        # -------- Ablation: remove unigram counts ---------------------\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)  # zeroed vector\n        # -------- keep bigram hashed freq -----------------------------\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------ model ---------------------------------------------\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.tf = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = (vocab_size - 1) + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.tf(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ------------------ training utilities --------------------------------\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n\n    def forward(self, logits, target):\n        n = logits.size(1)\n        log_p = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true = torch.zeros_like(log_p).fill_(self.s / (n - 1))\n            true.scatter_(1, target.unsqueeze(1), 1 - self.s)\n        return torch.mean(torch.sum(-true * log_p, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ------------------ experiment data dict ------------------------------\nexperiment_data = {\n    \"Remove_Unigram_Count_Features\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ------------------ train / eval loops --------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\nEPOCHS = 10\nstart = time.time()\nfor ep in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    ed = experiment_data[\"Remove_Unigram_Count_Features\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train\"].append(tr_f1)\n    ed[\"metrics\"][\"val\"].append(val_f1)\n    ed[\"epochs\"].append(ep)\n    print(f\"Epoch {ep}: val_loss={val_loss:.4f} | MacroF1={val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"TEST: loss={test_loss:.4f} | MacroF1={test_f1:.4f}\")\ned = experiment_data[\"Remove_Unigram_Count_Features\"][\"SPR_BENCH\"]\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\ned[\"test_loss\"] = test_loss\ned[\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# -----------------------------------------------------------\n# Remove_Label_Smoothing_Loss \u2013 full, self-contained ablation\n# -----------------------------------------------------------\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# -----------------------------------------------------------\n# experiment dict\nexperiment_data = {\n    \"Remove_Label_Smoothing_Loss\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_rec = experiment_data[\"Remove_Label_Smoothing_Loss\"][\"SPR_BENCH\"]\n\n# -----------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# -----------------------------------------------------------\n# load SPR_BENCH or synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 using synthetic data.\")\n    spr = DatasetDict(\n        {\n            \"train\": synth_ds(1200),\n            \"dev\": synth_ds(300),\n            \"test\": synth_ds(300),\n        }\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Number of labels:\", num_labels)\n\n# -----------------------------------------------------------\n# vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# -----------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq, self.lab = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        pad_len = MAX_LEN - len(ids)\n        if pad_len:\n            ids += [PAD_ID] * pad_len\n            attn += [0] * pad_len\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi:\n                cnt[vi - 1] = n\n        cnt /= len(s)\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym = torch.cat([cnt, big, length_feat], 0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds, dev_ds, test_ds = map(\n    SPRTorchDataset, (spr[\"train\"], spr[\"dev\"], spr[\"test\"])\n)\ntrain_loader = DataLoader(train_ds, 128, True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, 256, False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, 256, False, collate_fn=collate)\n\n\n# -----------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d, 2) * (-math.log(10000.0) / d))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, ids, attn, sym_feat):\n        x = self.embed(ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attn == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        x = x * torch.sigmoid(self.gate_fc(s))\n        h = self.dropout(torch.cat([x, s], -1))\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n# -----------------------------------------------------------\n# loss / optimiser / sched (no label smoothing)\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=10)\n\n\n# -----------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tots, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n            tots += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return tots / len(loader.dataset), f1_score(gts, preds, average=\"macro\"), preds, gts\n\n\n# -----------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor ep in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    for k, v in [(\"train\", tr_loss), (\"val\", val_loss)]:\n        exp_rec[\"losses\"][k].append(v)\n    for k, v in [(\"train\", tr_f1), (\"val\", val_f1)]:\n        exp_rec[\"metrics\"][k].append(v)\n    exp_rec[\"epochs\"].append(ep)\n    print(f\"Epoch {ep}: val_loss={val_loss:.4f}, MacroF1={val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nexp_rec[\"predictions\"] = test_preds\nexp_rec[\"ground_truth\"] = test_gts\nexp_rec[\"test_loss\"] = test_loss\nexp_rec[\"test_macroF1\"] = test_f1\nprint(f\"Test: loss={test_loss:.4f}, MacroF1={test_f1:.4f}\")\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved experiment_data.npy to\", working_dir)\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "import os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ------------------------------------------------------------\n# experiment bookkeeping\nexperiment_data = {\n    \"Remove_Length_Feature\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\nexp_key = (\"Remove_Length_Feature\", \"SPR_BENCH\")\n\n# ------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ------------------------------------------------------------\n# load SPR-BENCH or synthetic\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        d[split] = _load(f\"{split}.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Labels:\", num_labels)\n\n# ------------------------------------------------------------\n# vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ------------------------------------------------------------\n# torch Dataset WITHOUT length scalar\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids / mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            pad = MAX_LEN - len(ids)\n            ids += [PAD_ID] * pad\n            attn += [0] * pad\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # hashed bigram counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        sym_feat = torch.cat([cnt, big], dim=0)  # length feature removed\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM  # length scalar removed\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ------------------------------------------------------------\n# loss / optimizer\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n\n# ------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        total_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    ed = experiment_data[exp_key[0]][exp_key[1]]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train\"].append(tr_f1)\n    ed[\"metrics\"][\"val\"].append(val_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f} | MacroF1={val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\ned = experiment_data[exp_key[0]][exp_key[1]]\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\ned[\"test_loss\"] = test_loss\ned[\"test_macroF1\"] = test_f1\nprint(f\"Test: loss={test_loss:.4f} | MacroF1={test_f1:.4f}\")\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# ------------------------------------------------------------\n# Remove_Transformer_Encoder ablation study \u2013 single-file script\n# ------------------------------------------------------------\nimport os, pathlib, random, math, time, numpy as np, torch\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ---------------------------------------------------------------------\n# load SPR-BENCH or create synthetic toy data\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(\"Labels:\", num_labels)\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            pad = MAX_LEN - len(ids)\n            ids += [PAD_ID] * pad\n            attn += [0] * pad\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], 0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(b):\n    return {k: torch.stack([d[k] for d in b]) for k in b[0]}\n\n\ntrain_loader = DataLoader(\n    SPRTorchDataset(spr[\"train\"]), batch_size=128, shuffle=True, collate_fn=collate\n)\ndev_loader = DataLoader(\n    SPRTorchDataset(spr[\"dev\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\ntest_loader = DataLoader(\n    SPRTorchDataset(spr[\"test\"]), batch_size=256, shuffle=False, collate_fn=collate\n)\n\n\n# ---------------------------------------------------------------------\n# Ablated model \u2013 NO TransformerEncoder\nclass GatedBagEmbeddings(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pool = nn.AdaptiveAvgPool1d(1)  # average over tokens\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)  # B,L,D\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)  # B,D\n        s = self.sym_proj(sym_feat)  # B,64\n        gate = torch.sigmoid(self.gate_fc(s))  # B,D\n        x = x * gate  # gated bag-embeddings\n        h = torch.cat([x, s], -1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedBagEmbeddings().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n\n    def forward(self, logits, target):\n        n = logits.size(1)\n        logp = torch.log_softmax(logits, 1)\n        with torch.no_grad():\n            true = torch.zeros_like(logp).fill_(self.s / (n - 1))\n            true.scatter_(1, target.unsqueeze(1), 1 - self.s)\n        return torch.mean(torch.sum(-true * logp, 1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data dict\nexperiment_data = {\n    \"Remove_Transformer_Encoder\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    total, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        total / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    ed = experiment_data[\"Remove_Transformer_Encoder\"][\"SPR_BENCH\"]\n    ed[\"losses\"][\"train\"].append(tr_loss)\n    ed[\"losses\"][\"val\"].append(val_loss)\n    ed[\"metrics\"][\"train\"].append(tr_f1)\n    ed[\"metrics\"][\"val\"].append(val_f1)\n    ed[\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: val_loss={val_loss:.4f} | MacroF1={val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss={test_loss:.4f} | MacroF1={test_f1:.4f}\")\ned = experiment_data[\"Remove_Transformer_Encoder\"][\"SPR_BENCH\"]\ned[\"predictions\"] = test_preds\ned[\"ground_truth\"] = test_gts\ned[\"test_loss\"] = test_loss\ned[\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to:\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, random, math, numpy as np, torch, time\nfrom collections import Counter\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------------------------------------------------\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ---------------------------------------------------------------------\n# utility to load SPR_BENCH or create tiny synthetic set\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\ndef synth_ds(n_rows=300, max_len=18, n_labels=6):\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWX\")\n    data = {\"id\": [], \"sequence\": [], \"label\": []}\n    for i in range(n_rows):\n        seq = \"\".join(random.choices(alphabet, k=random.randint(5, max_len)))\n        data[\"id\"].append(str(i))\n        data[\"sequence\"].append(seq)\n        data[\"label\"].append(random.randint(0, n_labels - 1))\n    return load_dataset(\"json\", data_files={\"train\": [data]}, split=\"train\")\n\n\ndata_root = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\nif data_root.exists():\n    spr = load_spr_bench(data_root)\nelse:\n    print(\"SPR_BENCH not found \u2013 generating synthetic toy dataset.\")\n    spr = DatasetDict(\n        {\"train\": synth_ds(1200), \"dev\": synth_ds(300), \"test\": synth_ds(300)}\n    )\n\nnum_labels = len(set(spr[\"train\"][\"label\"]))\nprint(f\"Labels: {num_labels}\")\n\n# ---------------------------------------------------------------------\n# build vocab\nPAD_ID = 0\n\n\ndef build_vocab(ds):\n    chars = set()\n    for s in ds[\"sequence\"]:\n        chars.update(s)\n    vocab = {c: i + 1 for i, c in enumerate(sorted(chars))}\n    vocab[\"<PAD>\"] = PAD_ID\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nvocab_size = len(vocab)\nid2char = {i: c for c, i in vocab.items()}\nprint(\"Vocab size:\", vocab_size)\n\nMAX_LEN = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n# ---------------------------------------------------------------------\n# hashed bigram helper\nBIGRAM_DIM = 64\n\n\ndef bigram_hash(a, b):\n    return (hash(a + \":\" + b) & 0xFFFFFFFF) % BIGRAM_DIM\n\n\n# ---------------------------------------------------------------------\n# torch Dataset\nclass SPRTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_ds):\n        self.seq = hf_ds[\"sequence\"]\n        self.lab = hf_ds[\"label\"]\n        self.count_dim = vocab_size - 1\n\n    def __len__(self):\n        return len(self.seq)\n\n    def __getitem__(self, idx):\n        s = self.seq[idx]\n        # ids & mask\n        ids = [vocab.get(c, PAD_ID) for c in s[:MAX_LEN]]\n        attn = [1] * len(ids)\n        if len(ids) < MAX_LEN:\n            ids += [PAD_ID] * (MAX_LEN - len(ids))\n            attn += [0] * (MAX_LEN - len(attn))\n        # unigram counts\n        cnt = torch.zeros(self.count_dim, dtype=torch.float32)\n        for ch, n in Counter(s).items():\n            vi = vocab.get(ch, 0)\n            if vi > 0:\n                cnt[vi - 1] = float(n)\n        cnt /= len(s)\n        # bigram hashed counts\n        big = torch.zeros(BIGRAM_DIM, dtype=torch.float32)\n        for a, b in zip(s, s[1:]):\n            big[bigram_hash(a, b)] += 1.0\n        if len(s) > 1:\n            big /= len(s) - 1\n        # length feature\n        length_feat = torch.tensor([len(s) / MAX_LEN], dtype=torch.float32)\n        sym_feat = torch.cat([cnt, big, length_feat], dim=0)\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"attention_mask\": torch.tensor(attn),\n            \"sym_feat\": sym_feat,\n            \"labels\": torch.tensor(self.lab[idx]),\n        }\n\n\ndef collate(batch):\n    return {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n\n\ntrain_ds = SPRTorchDataset(spr[\"train\"])\ndev_ds = SPRTorchDataset(spr[\"dev\"])\ntest_ds = SPRTorchDataset(spr[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\ndev_loader = DataLoader(dev_ds, batch_size=256, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate)\n\n\n# ---------------------------------------------------------------------\n# model\nclass PosEnc(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass GatedHybridTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_model = 128\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n        self.pos = PosEnc(d_model, MAX_LEN)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, 4, 256, 0.1, batch_first=True), 2\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        sym_in = vocab_size - 1 + BIGRAM_DIM + 1\n        self.sym_proj = nn.Sequential(\n            nn.Linear(sym_in, 64), nn.ReLU(), nn.LayerNorm(64)\n        )\n        self.gate_fc = nn.Linear(64, d_model)\n        self.dropout = nn.Dropout(0.2)\n        self.cls = nn.Linear(d_model + 64, num_labels)\n\n    def forward(self, input_ids, attention_mask, sym_feat):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))\n        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n        s = self.sym_proj(sym_feat)\n        gate = torch.sigmoid(self.gate_fc(s))\n        x = x * gate  # gated pooling\n        h = torch.cat([x, s], dim=-1)\n        h = self.dropout(h)\n        return self.cls(h)\n\n\nmodel = GatedHybridTransformer().to(device)\n\n\n# ---------------------------------------------------------------------\n# loss, opt, scheduler\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n_class = logits.size(1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(self.smoothing / (n_class - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n\n\ncriterion = LabelSmoothingCE(0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# ---------------------------------------------------------------------\n# experiment data\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# ---------------------------------------------------------------------\ndef run_epoch(loader, train=False):\n    model.train() if train else model.eval()\n    tot_loss, preds, gts = 0.0, [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(\n                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"sym_feat\"]\n            )\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            gts.extend(batch[\"labels\"].cpu().tolist())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(gts, preds, average=\"macro\"),\n        preds,\n        gts,\n    )\n\n\n# ---------------------------------------------------------------------\nEPOCHS = 10\nstart = time.time()\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_f1, _, _ = run_epoch(train_loader, True)\n    val_loss, val_f1, _, _ = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_f1)\n    experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | MacroF1 = {val_f1:.4f}\")\n\ntest_loss, test_f1, test_preds, test_gts = run_epoch(test_loader, False)\nprint(f\"Test: loss = {test_loss:.4f} | MacroF1 = {test_f1:.4f}\")\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = test_preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = test_gts\nexperiment_data[\"SPR_BENCH\"][\"test_loss\"] = test_loss\nexperiment_data[\"SPR_BENCH\"][\"test_macroF1\"] = test_f1\n\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(\"Saved metrics to\", os.path.join(working_dir, \"experiment_data.npy\"))\nprint(\"Total runtime: %.1fs\" % (time.time() - start))\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.7373\n| MacroF1 = 0.6007', '\\n', 'Epoch 2: validation_loss = 0.8751 | MacroF1 =\n0.6899', '\\n', 'Epoch 3: validation_loss = 0.8708 | MacroF1 = 0.6980', '\\n',\n'Epoch 4: validation_loss = 0.8634 | MacroF1 = 0.6899', '\\n', 'Epoch 5:\nvalidation_loss = 0.8477 | MacroF1 = 0.6960', '\\n', 'Epoch 6: validation_loss =\n0.8493 | MacroF1 = 0.6960', '\\n', 'Epoch 7: validation_loss = 0.8340 | MacroF1 =\n0.6980', '\\n', 'Epoch 8: validation_loss = 0.8866 | MacroF1 = 0.6980', '\\n',\n'Epoch 9: validation_loss = 0.8674 | MacroF1 = 0.7000', '\\n', 'Epoch 10:\nvalidation_loss = 0.8694 | MacroF1 = 0.7000', '\\n', 'Test: loss = 0.8654 |\nMacroF1 = 0.6999', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Total runtime: 18.3s', '\\n', 'Execution\ntime: 21 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', '\\rGenerating train split: 0 examples\n[00:00, ? examples/s]', '', '\\rGenerating train split: 2000 examples [00:00,\n160978.85 examples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 133534.03\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 213244.39\nexamples/s]', '\\n', 'Number of labels:', ' ', '2', '\\n', 'Vocab size:', ' ',\n'10', ' ', '| MAX_LEN:', ' ', '95', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.7046,\nMacroF1=0.6836', '\\n', 'Epoch 2: val_loss=0.9384, MacroF1=0.6818', '\\n', 'Epoch\n3: val_loss=0.8937, MacroF1=0.6818', '\\n', 'Epoch 4: val_loss=0.8971,\nMacroF1=0.6818', '\\n', 'Epoch 5: val_loss=0.9205, MacroF1=0.6899', '\\n', 'Epoch\n6: val_loss=0.9045, MacroF1=0.6980', '\\n', 'Epoch 7: val_loss=0.8815,\nMacroF1=0.6980', '\\n', 'Epoch 8: val_loss=0.8679, MacroF1=0.6980', '\\n', 'Epoch\n9: val_loss=0.8864, MacroF1=0.6980', '\\n', 'Epoch 10: val_loss=0.8826,\nMacroF1=0.6980', '\\n', 'Test: loss=0.8773, MacroF1=0.6999', '\\n', 'Saved all\nmetrics to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n16/working/experiment_data.npy', '\\n', 'Total runtime: 21.9s', '\\n', 'Execution\ntime: 25 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 110810.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 106568.02\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 154469.27\nexamples/s]', '\\n', 'Labels:', ' ', '2', '\\n', 'Vocab size:', ' ', '10', ' ', '|\nMAX_LEN:', ' ', '95', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 01 | train_loss 0.8071 |\nval_loss 0.7941 | val_macroF1 0.6960', '\\n', 'Epoch 02 | train_loss 0.4313 |\nval_loss 0.8523 | val_macroF1 0.6817', '\\n', 'Epoch 03 | train_loss 0.3594 |\nval_loss 0.8504 | val_macroF1 0.6960', '\\n', 'Epoch 04 | train_loss 0.3454 |\nval_loss 0.8641 | val_macroF1 0.6960', '\\n', 'Epoch 05 | train_loss 0.3377 |\nval_loss 0.8338 | val_macroF1 0.6980', '\\n', 'Epoch 06 | train_loss 0.3385 |\nval_loss 0.8764 | val_macroF1 0.6940', '\\n', 'Epoch 07 | train_loss 0.3361 |\nval_loss 0.8774 | val_macroF1 0.6960', '\\n', 'Epoch 08 | train_loss 0.3349 |\nval_loss 0.8735 | val_macroF1 0.6940', '\\n', 'Epoch 09 | train_loss 0.3344 |\nval_loss 0.8711 | val_macroF1 0.6960', '\\n', 'Epoch 10 | train_loss 0.3337 |\nval_loss 0.8720 | val_macroF1 0.6960', '\\n', 'Test | loss 0.8666 | macroF1\n0.6969', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n17/working/experiment_data.npy', '\\n', 'Total runtime: 18.9s', '\\n', 'Execution\ntime: 21 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 94970.03\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 98504.09\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 159285.43\nexamples/s]', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.0631 |\nMacroF1=0.6859', '\\n', 'Epoch 2: val_loss=0.9483 | MacroF1=0.6879', '\\n', 'Epoch\n3: val_loss=0.9100 | MacroF1=0.6859', '\\n', 'Epoch 4: val_loss=0.8914 |\nMacroF1=0.6960', '\\n', 'Epoch 5: val_loss=0.8344 | MacroF1=0.6960', '\\n', 'Epoch\n6: val_loss=0.8519 | MacroF1=0.6960', '\\n', 'Epoch 7: val_loss=0.8579 |\nMacroF1=0.6960', '\\n', 'Epoch 8: val_loss=0.8791 | MacroF1=0.6960', '\\n', 'Epoch\n9: val_loss=0.8684 | MacroF1=0.6960', '\\n', 'Epoch 10: val_loss=0.8727 |\nMacroF1=0.6960', '\\n', 'Test: loss=0.8729 | MacroF1=0.6949', '\\n', 'Saved\nmetrics to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n18/working/experiment_data.npy', '\\n', 'Total runtime: 18.9s', '\\n', 'Execution\ntime: 21 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 101446.46\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 108178.69\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 146890.24\nexamples/s]', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss = 0.9116 |\nMacroF1 = 0.6342', '\\n', 'Epoch 2: val_loss = 0.8849 | MacroF1 = 0.6818', '\\n',\n'Epoch 3: val_loss = 0.8397 | MacroF1 = 0.6960', '\\n', 'Epoch 4: val_loss =\n0.8717 | MacroF1 = 0.6899', '\\n', 'Epoch 5: val_loss = 0.8888 | MacroF1 =\n0.6859', '\\n', 'Epoch 6: val_loss = 0.8541 | MacroF1 = 0.6960', '\\n', 'Epoch 7:\nval_loss = 0.8542 | MacroF1 = 0.6960', '\\n', 'Epoch 8: val_loss = 0.8561 |\nMacroF1 = 0.6980', '\\n', 'Epoch 9: val_loss = 0.8805 | MacroF1 = 0.6960', '\\n',\n'Epoch 10: val_loss = 0.8744 | MacroF1 = 0.6980', '\\n', 'Test: loss = 0.8707 |\nMacroF1 = 0.6969', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n19/working/experiment_data.npy', '\\n', 'Total runtime: 6.4s', '\\n', 'Execution\ntime: 9 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Labels:', ' ', '2', '\\n', 'Vocab size:', '\n', '10', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6861 |\nMacroF1=0.6818', '\\n', 'Epoch 2: val_loss=0.8502 | MacroF1=0.6920', '\\n', 'Epoch\n3: val_loss=0.8743 | MacroF1=0.6960', '\\n', 'Epoch 4: val_loss=0.9319 |\nMacroF1=0.6858', '\\n', 'Epoch 5: val_loss=0.9083 | MacroF1=0.6839', '\\n', 'Epoch\n6: val_loss=0.8762 | MacroF1=0.6960', '\\n', 'Epoch 7: val_loss=0.8478 |\nMacroF1=0.6918', '\\n', 'Epoch 8: val_loss=0.8507 | MacroF1=0.6900', '\\n', 'Epoch\n9: val_loss=0.8807 | MacroF1=0.6960', '\\n', 'Epoch 10: val_loss=0.8794 |\nMacroF1=0.6960', '\\n', 'TEST: loss=0.8732 | MacroF1=0.6959', '\\n', 'Saved\nmetrics to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n18/working/experiment_data.npy', '\\n', 'Total runtime: 21.0s', '\\n', 'Execution\ntime: 23 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Number of labels:', ' ', '2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=1.2478,\nMacroF1=0.6879', '\\n', 'Epoch 2: val_loss=1.9914, MacroF1=0.6859', '\\n', 'Epoch\n3: val_loss=2.2237, MacroF1=0.6878', '\\n', 'Epoch 4: val_loss=2.0006,\nMacroF1=0.6960', '\\n', 'Epoch 5: val_loss=1.6430, MacroF1=0.6960', '\\n', 'Epoch\n6: val_loss=1.8793, MacroF1=0.6920', '\\n', 'Epoch 7: val_loss=1.9348,\nMacroF1=0.6980', '\\n', 'Epoch 8: val_loss=1.9154, MacroF1=0.6940', '\\n', 'Epoch\n9: val_loss=2.0168, MacroF1=0.6960', '\\n', 'Epoch 10: val_loss=1.9959,\nMacroF1=0.6960', '\\n', 'Test: loss=1.9799, MacroF1=0.6969', '\\n', 'Saved\nexperiment_data.npy to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-\n08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/0-\nrun/process_ForkProcess-16/working', '\\n', 'Total runtime: 18.8s', '\\n',\n'Execution time: 21 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels:', ' ', '2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: val_loss=0.6677 |\nMacroF1=0.6676', '\\n', 'Epoch 2: val_loss=0.8511 | MacroF1=0.6920', '\\n', 'Epoch\n3: val_loss=0.8434 | MacroF1=0.6880', '\\n', 'Epoch 4: val_loss=0.9075 |\nMacroF1=0.6899', '\\n', 'Epoch 5: val_loss=0.8561 | MacroF1=0.6960', '\\n', 'Epoch\n6: val_loss=0.9004 | MacroF1=0.6859', '\\n', 'Epoch 7: val_loss=0.8833 |\nMacroF1=0.6899', '\\n', 'Epoch 8: val_loss=0.8847 | MacroF1=0.6879', '\\n', 'Epoch\n9: val_loss=0.8741 | MacroF1=0.6960', '\\n', 'Epoch 10: val_loss=0.8763 |\nMacroF1=0.6960', '\\n', 'Test: loss=0.8700 | MacroF1=0.6959', '\\n', 'Saved\nmetrics to', ' ', '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n17/working/experiment_data.npy', '\\n', 'Total runtime: 18.7s', '\\n', 'Execution\ntime: 21 seconds seconds (time limit is 30 minutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Labels:', ' ', '2', '\\n', 'Vocab size:', '\n', '10', '\\n', 'Epoch 1: val_loss=0.6655 | MacroF1=0.6712', '\\n', 'Epoch 2:\nval_loss=0.9737 | MacroF1=0.6818', '\\n', 'Epoch 3: val_loss=0.8481 |\nMacroF1=0.6818', '\\n', 'Epoch 4: val_loss=0.8837 | MacroF1=0.6879', '\\n', 'Epoch\n5: val_loss=0.8250 | MacroF1=0.6920', '\\n', 'Epoch 6: val_loss=0.8944 |\nMacroF1=0.6859', '\\n', 'Epoch 7: val_loss=0.8655 | MacroF1=0.6899', '\\n', 'Epoch\n8: val_loss=0.8766 | MacroF1=0.6899', '\\n', 'Epoch 9: val_loss=0.8805 |\nMacroF1=0.6899', '\\n', 'Epoch 10: val_loss=0.8736 | MacroF1=0.6899', '\\n',\n'Test: loss=0.8618 | MacroF1=0.6958', '\\n', 'Saved metrics to:', ' ',\n'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n19/working/experiment_data.npy', '\\n', 'Total runtime: 19.4s', '\\n', 'Execution\ntime: 22 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.7388\n| MacroF1 = 0.6920', '\\n', 'Epoch 2: validation_loss = 0.8271 | MacroF1 =\n0.6920', '\\n', 'Epoch 3: validation_loss = 0.8779 | MacroF1 = 0.6940', '\\n',\n'Epoch 4: validation_loss = 0.8392 | MacroF1 = 0.6980', '\\n', 'Epoch 5:\nvalidation_loss = 0.8989 | MacroF1 = 0.6839', '\\n', 'Epoch 6: validation_loss =\n0.8657 | MacroF1 = 0.6960', '\\n', 'Epoch 7: validation_loss = 0.8626 | MacroF1 =\n0.6940', '\\n', 'Epoch 8: validation_loss = 0.8660 | MacroF1 = 0.6960', '\\n',\n'Epoch 9: validation_loss = 0.8798 | MacroF1 = 0.6960', '\\n', 'Epoch 10:\nvalidation_loss = 0.8754 | MacroF1 = 0.6960', '\\n', 'Test: loss = 0.8716 |\nMacroF1 = 0.6969', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n17/working/experiment_data.npy', '\\n', 'Total runtime: 21.8s', '\\n', 'Execution\ntime: 25 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6610\n| MacroF1 = 0.6879', '\\n', 'Epoch 2: validation_loss = 0.8787 | MacroF1 =\n0.6778', '\\n', 'Epoch 3: validation_loss = 0.9000 | MacroF1 = 0.6879', '\\n',\n'Epoch 4: validation_loss = 0.8902 | MacroF1 = 0.6920', '\\n', 'Epoch 5:\nvalidation_loss = 0.8694 | MacroF1 = 0.6960', '\\n', 'Epoch 6: validation_loss =\n0.8852 | MacroF1 = 0.6899', '\\n', 'Epoch 7: validation_loss = 0.8671 | MacroF1 =\n0.6960', '\\n', 'Epoch 8: validation_loss = 0.8755 | MacroF1 = 0.6960', '\\n',\n'Epoch 9: validation_loss = 0.8731 | MacroF1 = 0.6960', '\\n', 'Epoch 10:\nvalidation_loss = 0.8716 | MacroF1 = 0.6960', '\\n', 'Test: loss = 0.8684 |\nMacroF1 = 0.6959', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n18/working/experiment_data.npy', '\\n', 'Total runtime: 18.6s', '\\n', 'Execution\ntime: 22 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Labels: 2', '\\n', 'Vocab size:', ' ', '10', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.6595\n| MacroF1 = 0.6745', '\\n', 'Epoch 2: validation_loss = 0.9036 | MacroF1 =\n0.6960', '\\n', 'Epoch 3: validation_loss = 0.8843 | MacroF1 = 0.6960', '\\n',\n'Epoch 4: validation_loss = 0.8762 | MacroF1 = 0.6858', '\\n', 'Epoch 5:\nvalidation_loss = 0.8436 | MacroF1 = 0.6980', '\\n', 'Epoch 6: validation_loss =\n0.8716 | MacroF1 = 0.6980', '\\n', 'Epoch 7: validation_loss = 0.8884 | MacroF1 =\n0.6940', '\\n', 'Epoch 8: validation_loss = 0.8822 | MacroF1 = 0.6940', '\\n',\n'Epoch 9: validation_loss = 0.8662 | MacroF1 = 0.6980', '\\n', 'Epoch 10:\nvalidation_loss = 0.8781 | MacroF1 = 0.6980', '\\n', 'Test: loss = 0.8766 |\nMacroF1 = 0.6939', '\\n', 'Saved metrics to', ' ', '/home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_23-44-\n27_conceptual_generalization_poly_rule_attempt_0/0-run/process_ForkProcess-\n16/working/experiment_data.npy', '\\n', 'Total runtime: 18.8s', '\\n', 'Execution\ntime: 22 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution ran successfully without any errors or bugs. The\nGatedHybridTransformer model was trained and evaluated on the SPR_BENCH dataset.\nThe model achieved a test MacroF1 score of 0.6999, which is very close to the\nstate-of-the-art benchmark of 70.0%. The experiment data was saved successfully,\nand the total runtime was within the expected limits.", "The execution of the script was successful without any bugs. It loaded the\nSPR_BENCH dataset (or synthetic data as a fallback), processed the data, and\ntrained the model for 10 epochs. The validation loss and Macro F1 scores were\nlogged for each epoch, and the test set performance was reported as Macro F1 =\n0.6999. The results and metrics were saved successfully. The total runtime was\n21.9 seconds, well within the 30-minute time limit.", "", "", "", "", "", "The training script exhibits a potential issue with overfitting. Despite a\nsteady decrease in validation loss during the first few epochs, it starts to\nincrease from epoch 3 onwards, while the MacroF1 score remains inconsistent.\nThis suggests that the model may be overfitting to the training data.\nAdditionally, the final test performance (MacroF1 = 0.6959) is relatively low,\nindicating suboptimal generalization. To address this, consider implementing the\nfollowing fixes:   1. Add regularization techniques, such as dropout or weight\ndecay, to reduce overfitting. 2. Use early stopping based on validation loss or\nMacroF1 score to prevent over-training. 3. Experiment with reducing the model\ncomplexity or using fewer training epochs. 4. Investigate the training and\nvalidation data splits to ensure they are representative and not introducing\nbias.", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7, "best_value": 0.7}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3282, "best_value": 0.3282}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8694, "best_value": 0.7373}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during testing", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8654, "best_value": 0.8654}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value calculated during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3278, "best_value": 0.3278}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9995, "best_value": 0.9995}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7046, "best_value": 0.7046}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value calculated during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8773, "best_value": 0.8773}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score during testing.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6999, "best_value": 0.6999}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3337, "best_value": 0.3337}]}, {"metric_name": "training F1 score", "lower_is_better": false, "description": "The F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9955, "best_value": 0.9955}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7941, "best_value": 0.7941}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "The F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8666, "best_value": 0.8666}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "The F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.6969}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Measures the loss during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.336, "best_value": 0.336}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the loss during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8727, "best_value": 0.8727}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Measures the loss during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8729, "best_value": 0.8729}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during the test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6949, "best_value": 0.6949}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the weighted average of precision and recall across all classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.698}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Represents the error in the model's predictions. Lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8707, "best_value": 0.3328}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Measures the harmonic mean of precision and recall for classification tasks.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6959, "best_value": 0.696}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or difference between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8732, "best_value": 0.6861}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score, which considers the F1 score for each class and averages them equally.", "data": [{"dataset_name": "training", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "validation", "final_value": 0.696, "best_value": 0.696}, {"dataset_name": "test", "final_value": 0.6969, "best_value": 0.6969}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss value, which measures the error of the model. Lower values indicate better performance.", "data": [{"dataset_name": "training", "final_value": 0.0234, "best_value": 0.0234}, {"dataset_name": "validation", "final_value": 1.9959, "best_value": 1.9959}, {"dataset_name": "test", "final_value": 1.9799, "best_value": 1.9799}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss value during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3339, "best_value": 0.3339}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6677, "best_value": 0.6677}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value during test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.87, "best_value": 0.87}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score during test phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6959, "best_value": 0.6959}]}]}, {"metric_names": [{"metric_name": "macro F1 score", "lower_is_better": false, "description": "A metric used to evaluate the balance between precision and recall in multi-class classification.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6958, "best_value": 0.6958}]}, {"metric_name": "loss", "lower_is_better": true, "description": "A measure of error or deviation from the expected outcome.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8618, "best_value": 0.6655}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.9945, "best_value": 0.9945}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.698}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3365, "best_value": 0.3365}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8754, "best_value": 0.7388}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6969, "best_value": 0.6969}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8716, "best_value": 0.8716}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.997, "best_value": 0.997}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.696, "best_value": 0.696}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.333, "best_value": 0.333}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8716, "best_value": 0.661}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6959, "best_value": 0.6959}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss during testing phase.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8684, "best_value": 0.8684}]}]}, {"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.698, "best_value": 0.698}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.3346, "best_value": 0.3346}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8781, "best_value": 0.6595}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro F1 score for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8766, "best_value": 0.8766}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_class_count_comparison.png"], ["../../logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_label_distribution.png"], [], ["../../logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_macroF1_curve.png", "../../logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_conf_matrix.png", "../../logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_label_distribution.png", "../../logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/cross_dataset_val_macroF1.png"], ["../../logs/0-run/experiment_results/seed_aggregation_5543300662e34d1b87fa6eed80e9b1a3/SPR_BENCH_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_5543300662e34d1b87fa6eed80e9b1a3/SPR_BENCH_aggregated_macroF1_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_5543300662e34d1b87fa6eed80e9b1a3/cross_dataset_final_val_macroF1.png"]], "plot_paths": [["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_macroF1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_label_distribution.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_class_count_comparison.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_f1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_confusion_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_label_distribution.png"], [], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_macroF1_curves.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_conf_matrix.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_label_distribution.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/cross_dataset_val_macroF1.png"], ["experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_5543300662e34d1b87fa6eed80e9b1a3/SPR_BENCH_aggregated_loss_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_5543300662e34d1b87fa6eed80e9b1a3/SPR_BENCH_aggregated_macroF1_curve.png", "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/seed_aggregation_5543300662e34d1b87fa6eed80e9b1a3/cross_dataset_final_val_macroF1.png"]], "plot_analyses": [[{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is learning effectively during training. However, the validation loss remains relatively high and does not decrease significantly after an initial drop, suggesting potential overfitting or insufficient generalization to the validation set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score quickly converges to 1.0, indicating excellent performance on the training data. However, the validation Macro-F1 score plateaus around 0.7, which, while better than random guessing, shows a gap in generalization performance between the training and validation datasets.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix indicates that the model performs well on both classes, as the diagonal elements dominate. However, there is still some level of misclassification, as indicated by the off-diagonal elements.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_conf_matrix.png"}, {"analysis": "The ground-truth label distribution is balanced, with an equal number of samples for each class. This balance ensures that the model is not biased due to class imbalance, and the performance metrics are reflective of the model's actual ability to generalize.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/SPR_BENCH_label_distribution.png"}, {"analysis": "The final validation Macro-F1 score for SPR_BENCH is approximately 0.7, which aligns with the results observed in the training vs. validation Macro-F1 plot. This score indicates that while the model performs reasonably well, it has room for improvement to achieve state-of-the-art performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_65321ac8ff4547189fa5ca65071deef2_proc_3469031/cross_dataset_val_macroF1.png"}], [{"analysis": "The loss curves indicate that the training loss decreases steadily over time, suggesting that the model is learning effectively during training. However, the validation loss remains significantly higher than the training loss and shows minimal improvement after the initial epochs. This discrepancy suggests potential overfitting, where the model performs well on the training data but struggles to generalize to unseen validation data. The removal of the gating mechanism appears to impact the model's ability to generalize effectively.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show a similar trend to the loss curves. While the training F1 score quickly reaches near-perfect levels, the validation F1 score remains relatively low and flat, with only a slight improvement over the epochs. This further supports the observation of overfitting. The model's performance on the validation set does not improve significantly despite the high training performance, indicating that the removed gating mechanism might be critical for learning generalizable features.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The confusion matrix reveals a significant imbalance in the model's predictions. While one class is predicted with high accuracy, the other class has a much lower prediction rate. This imbalance might be a consequence of the removal of the gating mechanism, which could have been essential for balancing the decision-making process across different classes. The model seems to favor one class over the other, leading to suboptimal performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_50a2be79119b457db7bf4002c4564740_proc_3475347/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "This plot shows the training and validation loss over 10 epochs. The training loss decreases steadily, indicating that the model is learning effectively on the training data. However, the validation loss remains relatively flat after an initial increase, suggesting that the model is not generalizing well to unseen data. This could be a sign of overfitting, where the model performs well on the training set but struggles to generalize to the validation set. The gap between training and validation loss also supports this observation.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot illustrates the macro-F1 scores for both training and validation sets over 10 epochs. The training macro-F1 score quickly reaches near-perfect levels, while the validation macro-F1 score remains stagnant around 0.7. This stark contrast further indicates overfitting, as the model is performing exceptionally well on the training data but fails to improve on the validation set. This could be due to the model memorizing the training data rather than learning generalizable patterns.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix for the test set reveals the model's performance in distinguishing between the two classes. While the model correctly classifies a majority of the instances (339 true negatives and 358 true positives), there are a notable number of misclassifications (147 false positives and 156 false negatives). This indicates that the model has room for improvement in both precision and recall. The relatively high number of false positives and false negatives suggests that the model struggles with certain patterns or edge cases in the data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_06b6e30139f24245a681af92f496ac7f_proc_3475348/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that the training loss decreases steadily and stabilizes at a low value, suggesting that the model is learning effectively on the training data. However, the validation loss remains significantly higher and plateaus early, indicating a possible overfitting issue. This suggests that removing positional encoding negatively impacts the model's generalization ability on validation data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that the training Macro-F1 score quickly reaches near-perfect levels, while the validation Macro-F1 score remains stagnant at around 0.7. This further supports the observation of overfitting, as the model performs exceedingly well on the training set but fails to generalize to the validation set. Removing positional encoding appears to compromise the model's ability to capture generalizable patterns.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix highlights a significant imbalance in the model's predictions. While certain classes are predicted correctly with high accuracy, others are misclassified more frequently. This imbalance could be due to the removal of positional encoding, which might disrupt the model's ability to differentiate between certain symbolic patterns, leading to biased predictions.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_810c7a54116f4d18ad9535cd3824c8b1_proc_3475349/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that the training loss rapidly decreases and stabilizes at a low value, suggesting effective learning by the model. However, the validation loss remains consistently higher than the training loss and does not show significant improvement after an initial drop. This indicates potential overfitting, where the model performs well on the training data but struggles to generalize to unseen validation data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that the training Macro-F1 score quickly approaches 1.0, indicating excellent performance on the training set. The validation Macro-F1 score, however, remains significantly lower and plateaus around 0.7, further supporting the observation of overfitting. This suggests that the model's ability to generalize across different data splits is limited.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_macroF1_curves.png"}, {"analysis": "The confusion matrix reveals a relatively balanced performance across the predicted classes, with the majority of instances being correctly classified. However, there are some misclassifications, which may be contributing to the lower validation performance observed in the loss and Macro-F1 curves. This could be indicative of the model's difficulty in learning some specific patterns or rules in the dataset.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_confusion_matrix.png"}, {"analysis": "The label distribution plot shows that the predicted labels closely match the ground truth labels in terms of distribution. This indicates that the model is not biased towards any particular class and is capable of maintaining a balanced prediction across different categories. However, the overall performance still suggests room for improvement in accurately learning the underlying rules of the dataset.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f8f2780ee057416d97a0615af96cfaef_proc_3475350/Remove_Bigram_Hashed_Features_SPR_BENCH_label_distribution.png"}], [{"analysis": "The loss curves indicate that the model achieves a low training loss quickly, suggesting effective optimization during training. However, the validation loss remains significantly higher and does not improve over epochs, indicating potential overfitting. This suggests that the model may be memorizing training data patterns instead of generalizing well to unseen data. Removing unigram count features appears to have negatively impacted the model's ability to generalize.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show a similar trend to the loss curves. The training Macro-F1 score reaches near-perfect levels rapidly, while the validation Macro-F1 score stagnates at a much lower value, around 0.7. This further supports the observation of overfitting, as the model performs well on training data but struggles on validation data. The removal of unigram count features may have reduced the model's ability to capture essential patterns in the data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs reasonably well in classifying both classes, with a slight imbalance in accuracy. However, the overall performance is not optimal, as seen from the validation metrics. This suggests that while the model can distinguish between classes to some extent, it struggles with more nuanced patterns in the data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The bar charts comparing ground truth and generated samples show a balanced distribution for both classes, indicating that the dataset is not biased. However, this balance does not translate into high validation performance, suggesting that the issue lies in the model's architecture or feature representation rather than the dataset.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_f00da480006e45aba494926217801d47_proc_3475349/SPR_BENCH_class_count_comparison.png"}], [{"analysis": "The loss curves indicate that the model achieves a very low training loss, stabilizing quickly after a few epochs. However, the validation loss remains significantly higher and fluctuates without a clear downward trend. This suggests overfitting, as the model learns the training data well but struggles to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves show a stark contrast between training and validation performance. The training Macro-F1 score quickly reaches nearly 1.0, indicating almost perfect classification on the training data. Meanwhile, the validation Macro-F1 score remains consistently low, around 0.7, further supporting the evidence of overfitting and poor generalization.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs well for one class but struggles significantly with the other. The imbalance in predictions suggests that the model may not be capturing the underlying rules equally for both classes, potentially due to biases in the data or the model's architecture.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_confusion_matrix.png"}, {"analysis": "The label distribution plot shows that the predicted label distribution closely matches the ground truth distribution. This indicates that the model is not suffering from a severe class imbalance issue in its predictions. However, this does not necessarily translate to high performance, as seen in the confusion matrix and validation metrics.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_b3902c60a0e14c5ab38d1bf75831e7c4_proc_3475347/SPR_BENCH_label_distribution.png"}], [], [{"analysis": "The loss curves show a clear divergence between the training and validation losses as the epochs progress. The training loss steadily decreases, indicating the model is learning effectively on the training data. However, the validation loss stops improving after the initial epochs and even shows a slight upward trend, suggesting the model may be overfitting to the training data and struggling to generalize to unseen data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_loss_curves.png"}, {"analysis": "The Macro-F1 curves reveal that the training Macro-F1 score increases significantly and stabilizes near 1.0, which aligns with the observed decrease in training loss. However, the validation Macro-F1 score remains relatively flat and does not show significant improvement, staying around 0.7. This further supports the observation of potential overfitting, as the model performs well on training data but fails to generalize effectively to validation data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_macroF1_curves.png"}, {"analysis": "The confusion matrix provides a detailed view of the model's classification performance. While the model correctly predicts a substantial number of both class 0 and class 1 examples (336 and 360, respectively), it also misclassifies a significant number of examples (150 for class 0 and 154 for class 1). This indicates that the model struggles with achieving a high level of accuracy and balance in its predictions, which may be tied to the generalization issues observed in the loss and Macro-F1 curves.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d431c8105f1e4c47bbf288d937ec82d1_proc_3475350/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The training loss decreases steadily over epochs, indicating that the model is learning effectively from the training data. However, the validation loss remains consistently higher than the training loss and does not show significant improvement after the initial epochs. This suggests potential overfitting, where the model performs well on the training data but struggles to generalize to unseen validation data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score quickly reaches near-perfect levels, demonstrating that the model is able to classify the training data with high accuracy. In contrast, the validation Macro-F1 score remains relatively stagnant and does not exhibit significant improvement, staying around 0.7 throughout the training process. This further supports the observation of overfitting, as the model's performance on the validation set is not improving.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix shows that the model has a strong tendency to predict certain classes correctly while struggling with others. This imbalance could indicate that the model is biased towards specific patterns in the data or that some classes are inherently more challenging to predict. Further error analysis is needed to understand the root cause of these misclassifications.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_conf_matrix.png"}, {"analysis": "The ground-truth label distribution appears balanced, with an equal frequency of labels in the dataset. This balance suggests that the model's performance issues are unlikely to be caused by class imbalance and are more likely related to the model architecture or training process.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/SPR_BENCH_label_distribution.png"}, {"analysis": "The comparison of final validation Macro-F1 scores across datasets shows that the performance on SPR_BENCH remains limited, with a final score of around 0.7. This indicates that the proposed model struggles to generalize effectively to the validation set, and further architectural or training modifications may be necessary to improve performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/cross_dataset_val_macroF1.png"}], [{"analysis": "This plot illustrates the training and validation loss over 10 epochs. The training loss decreases steadily, indicating effective learning by the model. However, the validation loss remains relatively high after an initial increase, suggesting potential overfitting or difficulty in generalizing to the validation set.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_loss_curve.png"}, {"analysis": "This plot shows the training and validation Macro-F1 scores over 10 epochs. The training Macro-F1 rapidly increases and plateaus near 1.0, while the validation Macro-F1 remains constant around 0.7, indicating a significant gap between training and validation performance. This may be due to overfitting or insufficient model generalization.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The confusion matrix highlights the normalized predictions of the model. The diagonal dominance indicates that the model is correctly predicting the majority of the samples, but the off-diagonal values suggest some misclassifications, which need further investigation.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_conf_matrix.png"}, {"analysis": "This plot represents the label distribution in the dataset. The labels are evenly distributed, which ensures that the model is not biased toward any specific class during training and evaluation.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/SPR_BENCH_label_distribution.png"}, {"analysis": "This plot compares the final validation Macro-F1 score across datasets. The SPR_BENCH dataset achieves a final validation Macro-F1 score of approximately 0.7, which is consistent with the earlier analysis of the validation performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/cross_dataset_val_macroF1.png"}], [{"analysis": "The training loss decreases steadily over the epochs, indicating that the model is effectively learning from the training data. However, the validation loss remains relatively high and does not decrease significantly after the initial epochs, suggesting potential overfitting or challenges in generalizing to the validation set. Further investigation into regularization techniques or data augmentation might be needed.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_loss_curve.png"}, {"analysis": "The training Macro-F1 score improves rapidly and reaches nearly perfect performance, but the validation Macro-F1 score plateaus at a lower value and shows minimal improvement over epochs. This discrepancy highlights a potential overfitting issue, where the model performs well on the training data but struggles to generalize to unseen validation data. Exploring techniques such as dropout, early stopping, or more diverse training data could be beneficial.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_macroF1_curve.png"}, {"analysis": "The normalized confusion matrix shows a clear diagonal pattern, indicating that the model is correctly predicting the majority of the samples. However, the imbalance in intensity across the diagonal suggests that the model may perform better on certain classes than others. This could be addressed by analyzing class-wise performance and potentially rebalancing the training data.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_conf_matrix.png"}, {"analysis": "The ground-truth label distribution is balanced, with an equal frequency of the two labels. This ensures that the model is not biased toward a particular class due to data imbalance and validates that the dataset is well-prepared for training and evaluation.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/SPR_BENCH_label_distribution.png"}, {"analysis": "The comparison of the final validation Macro-F1 score across datasets shows that the SPR_BENCH dataset achieves a moderate Macro-F1 score. While this result is promising, it indicates room for improvement in the model's generalization capabilities. Further optimization or architectural modifications may be required to achieve state-of-the-art performance.", "plot_path": "experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/cross_dataset_val_macroF1.png"}], []], "vlm_feedback_summary": ["The analysis highlights a general trend of effective training but limited\ngeneralization, as evidenced by the gap between training and validation\nperformance metrics. The balanced dataset ensures unbiased evaluation, but the\nmodel's validation performance suggests a need for further optimization or\narchitectural improvements to achieve state-of-the-art results.", "The plots suggest that the removal of the gating mechanism negatively impacts\nthe model's generalization ability and leads to overfitting. The training\nperformance remains high, but the validation performance is significantly lower.\nAdditionally, the confusion matrix highlights a class imbalance in predictions,\nwhich further indicates that the gating mechanism might play a crucial role in\nmaintaining balanced and generalizable learning.", "The experimental results indicate significant overfitting, as evidenced by the\ndivergence between training and validation performance in both loss and macro-F1\nscore plots. The confusion matrix highlights areas where the model's predictive\ncapabilities can be improved, particularly in reducing false positives and false\nnegatives. These findings suggest the need for strategies to enhance\ngeneralization, such as regularization, data augmentation, or adjustments to the\nmodel architecture.", "The experimental results suggest that removing positional encoding results in\noverfitting and reduced generalization performance. The training metrics are\nexcellent, but the validation metrics are suboptimal, indicating the model\nstruggles to generalize. Additionally, the confusion matrix reveals prediction\nimbalances, further highlighting the limitations introduced by removing\npositional encoding.", "The experimental results highlight the model's strong performance on the\ntraining data but limited generalization to the validation data. The loss and\nMacro-F1 curves both suggest overfitting, where the model struggles to\ngeneralize the learned symbolic reasoning capabilities. The confusion matrix and\nlabel distribution indicate balanced predictions but also reveal some\nmisclassifications that need to be addressed. Further tuning or architectural\nadjustments may be required to improve generalization.", "The experimental results indicate that removing unigram count features has led\nto overfitting and reduced generalization performance. While the model optimizes\nwell on training data, its validation performance suffers, as shown by the loss\nand Macro-F1 curves. The confusion matrix and bar charts confirm that the\ndataset is balanced, but the model fails to leverage this balance effectively.\nFurther investigations into feature engineering and regularization techniques\nare recommended to improve validation performance.", "The results suggest that the model is overfitting to the training data and\nstruggles to generalize to the validation set. While the label distribution\nappears balanced, the confusion matrix and validation metrics indicate poor\ngeneralization and potential biases in learning complex rules. Further\ninvestigation and architectural adjustments are needed to improve generalization\non the SPR_BENCH benchmark.", "[]", "The plots collectively indicate that the model is learning well on the training\ndata but faces challenges in generalizing to validation data, as evidenced by\nthe divergence in loss curves, the flat validation Macro-F1 score, and the\nconfusion matrix showing significant misclassifications. This suggests potential\noverfitting and highlights areas for improvement in model design or training\nstrategy.", "The results suggest that the model is overfitting to the training data and\nstruggling to generalize to the validation set. While the training metrics\nindicate effective learning on the training data, the validation metrics remain\nstagnant, highlighting the need for architectural improvements or regularization\ntechniques to address overfitting. Additional error analysis is recommended to\nidentify specific areas for improvement.", "The plots reveal a consistent pattern of overfitting, as evidenced by the gap\nbetween training and validation performance. The model performs well on the\ntraining set but struggles to generalize to the validation set. The balanced\nlabel distribution is a positive aspect, ensuring fairness in training. Further\nablation studies and regularization techniques are recommended to address the\ngeneralization issues.", "The plots reveal that while the model is effectively learning from the training\ndata, it struggles to generalize to the validation set, as evidenced by the\ndiscrepancy between training and validation metrics. The balanced dataset\nensures fairness in evaluation, but further efforts are needed to address\noverfitting and improve generalization. Exploring advanced techniques like\nregularization, data augmentation, or architectural enhancements could enhance\nthe performance.", "[]"], "exec_time": [21.212788820266724, 25.078840255737305, 21.56001091003418, 21.688451290130615, 9.093899011611938, 23.874391794204712, 21.545110940933228, 21.51461935043335, 22.16283941268921, 25.561496019363403, 22.159366846084595, 22.638415575027466, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["[]"], ["[]"], ["[]"], ["[]"], ["[]"], ["[]"], [], ["[]"], ["[]"], ["[]"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------ #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    try:\n        d = experiment_data[\"Remove_Gating_Mechanism\"][\"SPR_BENCH\"]\n        epochs = np.array(d[\"epochs\"])\n        tr_loss = np.array(d[\"losses\"][\"train\"])\n        val_loss = np.array(d[\"losses\"][\"val\"])\n        tr_f1 = np.array(d[\"metrics\"][\"train\"])\n        val_f1 = np.array(d[\"metrics\"][\"val\"])\n        test_preds = np.array(d[\"predictions\"])\n        test_gts = np.array(d[\"ground_truth\"])\n        test_loss = d.get(\"test_loss\", None)\n        test_f1 = d.get(\"test_macroF1\", None)\n        print(f\"Test loss: {test_loss:.4f}, Test macro-F1: {test_f1:.4f}\")\n    except Exception as e:\n        print(f\"Error extracting sub-dictionary: {e}\")\n        experiment_data = None\n\n# ------------------------------------------------------------------ #\nif experiment_data is not None:\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Loss Curves - Remove_Gating_Mechanism (SPR_BENCH)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # 2. Macro-F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"Macro-F1 Curves - Remove_Gating_Mechanism (SPR_BENCH)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        cm = confusion_matrix(test_gts, test_preds)\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\"Confusion Matrix - Remove_Gating_Mechanism (SPR_BENCH)\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    ed = experiment_data[\"Remove_Symbolic_Feature_Pathway\"][\"SPR_BENCH\"]\n    epochs = ed.get(\"epochs\", [])\n    train_loss = ed.get(\"losses\", {}).get(\"train\", [])\n    val_loss = ed.get(\"losses\", {}).get(\"val\", [])\n    train_f1 = ed.get(\"metrics\", {}).get(\"train\", [])\n    val_f1 = ed.get(\"metrics\", {}).get(\"val\", [])\n    preds = ed.get(\"predictions\", [])\n    gts = ed.get(\"ground_truth\", [])\n    test_loss = ed.get(\"test_loss\", None)\n    test_macroF1 = ed.get(\"test_macroF1\", None)\n\n    # --------------------------------------------------------\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 2) Macro-F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro F1\")\n        plt.title(\"SPR_BENCH \u2013 Training vs Validation Macro-F1\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # 3) Confusion matrix on test set\n    try:\n        if preds and gts:\n            cm = confusion_matrix(gts, preds, labels=sorted(set(gts)))\n            disp = ConfusionMatrixDisplay(cm)\n            disp.plot(cmap=\"Blues\", colorbar=False)\n            plt.title(\"SPR_BENCH \u2013 Test Confusion Matrix\")\n            fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n            plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n\n    # --------------------------------------------------------\n    # print evaluation summary\n    print(f\"Test Loss: {test_loss:.4f} | Test Macro-F1: {test_macroF1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ------------------------------------------------------------------\nexp_key = \"Remove_Positional_Encoding\"\nds_key = \"SPR_BENCH\"\ned = experiment_data.get(exp_key, {}).get(ds_key, {})\n\n# Basic safety checks\nepochs = ed.get(\"epochs\", [])\ntrain_loss = ed.get(\"losses\", {}).get(\"train\", [])\nval_loss = ed.get(\"losses\", {}).get(\"val\", [])\ntrain_f1 = ed.get(\"metrics\", {}).get(\"train\", [])\nval_f1 = ed.get(\"metrics\", {}).get(\"val\", [])\ny_pred = ed.get(\"predictions\", [])\ny_true = ed.get(\"ground_truth\", [])\ntest_loss = ed.get(\"test_loss\", None)\ntest_macroF1 = ed.get(\"test_macroF1\", None)\n\n# ------------------------------------------------------------------\n# 1) Loss curve\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curves\\nModel: Remove_Positional_Encoding\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 2) F1 curve\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH \u2013 Macro-F1 Curves\\nModel: Remove_Positional_Encoding\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n    plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 3) Confusion matrix on test set\nif len(y_true) and len(y_pred):\n    try:\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure(figsize=(6, 5))\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted label\")\n        plt.ylabel(\"True label\")\n        plt.title(\"SPR_BENCH \u2013 Confusion Matrix\\nModel: Remove_Positional_Encoding\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname, dpi=120, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n# ------------------------------------------------------------------\n# Print stored evaluation numbers\nprint(f\"Test loss   : {test_loss}\")\nprint(f\"Test macroF1: {test_macroF1}\")\nprint(\"Plots saved to:\", working_dir)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor exp_name, ds_dict in experiment_data.items():\n    for ds_name, rec in ds_dict.items():\n        epochs = np.array(rec.get(\"epochs\", []))\n        tr_loss = np.array(rec.get(\"losses\", {}).get(\"train\", []))\n        val_loss = np.array(rec.get(\"losses\", {}).get(\"val\", []))\n        tr_f1 = np.array(rec.get(\"metrics\", {}).get(\"train\", []))\n        val_f1 = np.array(rec.get(\"metrics\", {}).get(\"val\", []))\n        preds = np.array(rec.get(\"predictions\", []))\n        gts = np.array(rec.get(\"ground_truth\", []))\n        test_f1 = rec.get(\"test_macroF1\", None)\n        if test_f1 is not None:\n            print(f\"{exp_name}-{ds_name}  Test Macro-F1: {test_f1:.4f}\")\n\n        # 1) loss curves\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{exp_name} \u2013 {ds_name} Loss Curves\")\n            plt.legend()\n            fn = f\"{exp_name}_{ds_name}_loss_curves.png\".replace(\" \", \"_\")\n            plt.savefig(os.path.join(working_dir, fn))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot: {e}\")\n            plt.close()\n\n        # 2) macro-F1 curves\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_f1, label=\"Train\")\n            plt.plot(epochs, val_f1, label=\"Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.title(f\"{exp_name} \u2013 {ds_name} Macro-F1 Curves\")\n            plt.legend()\n            fn = f\"{exp_name}_{ds_name}_macroF1_curves.png\".replace(\" \", \"_\")\n            plt.savefig(os.path.join(working_dir, fn))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating F1 plot: {e}\")\n            plt.close()\n\n        # 3) confusion matrix\n        try:\n            if preds.size and gts.size:\n                num_labels = int(max(gts.max(), preds.max()) + 1)\n                cm = np.zeros((num_labels, num_labels), dtype=int)\n                for t, p in zip(gts, preds):\n                    cm[t, p] += 1\n                plt.figure(figsize=(5, 5))\n                im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n                plt.colorbar(im)\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                plt.title(f\"{exp_name} \u2013 {ds_name} Confusion Matrix\")\n                plt.savefig(\n                    os.path.join(\n                        working_dir,\n                        f\"{exp_name}_{ds_name}_confusion_matrix.png\".replace(\" \", \"_\"),\n                    )\n                )\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating confusion matrix: {e}\")\n            plt.close()\n\n        # 4) class distribution comparison\n        try:\n            if preds.size and gts.size:\n                num_labels = int(max(gts.max(), preds.max()) + 1)\n                ind = np.arange(num_labels)\n                width = 0.35\n                gt_counts = np.bincount(gts, minlength=num_labels)\n                pr_counts = np.bincount(preds, minlength=num_labels)\n                plt.figure()\n                plt.bar(ind - width / 2, gt_counts, width, label=\"Ground Truth\")\n                plt.bar(ind + width / 2, pr_counts, width, label=\"Predictions\")\n                plt.xlabel(\"Class\")\n                plt.ylabel(\"Count\")\n                plt.title(f\"{exp_name} \u2013 {ds_name} Label Distribution\")\n                plt.legend()\n                fn = f\"{exp_name}_{ds_name}_label_distribution.png\".replace(\" \", \"_\")\n                plt.savefig(os.path.join(working_dir, fn))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating distribution plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------------------------------------------------------------------\nexp_name = \"Remove_Unigram_Count_Features\"\nds_name = \"SPR_BENCH\"\ned = experiment_data.get(exp_name, {}).get(ds_name, None)\nif ed is None:\n    print(\"No data found for the specified experiment/dataset.\")\n    exit()\n\nepochs = ed.get(\"epochs\", [])\ntr_loss = ed[\"losses\"][\"train\"]\nval_loss = ed[\"losses\"][\"val\"]\ntr_f1 = ed[\"metrics\"][\"train\"]\nval_f1 = ed[\"metrics\"][\"val\"]\npreds = np.array(ed.get(\"predictions\", []))\ngts = np.array(ed.get(\"ground_truth\", []))\ntest_loss = ed.get(\"test_loss\", None)\ntest_f1 = ed.get(\"test_macroF1\", None)\n\n# ---------------- Plot 1: Loss curves --------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, tr_loss, label=\"Train Loss\")\n    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{exp_name} \u2013 {ds_name} Loss Curves\")\n    plt.legend()\n    fpath = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ---------------- Plot 2: F1 curves ----------------------------------\ntry:\n    plt.figure()\n    plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n    plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(f\"{exp_name} \u2013 {ds_name} Macro-F1 Curves\")\n    plt.legend()\n    fpath = os.path.join(working_dir, f\"{ds_name}_f1_curves.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve plot: {e}\")\n    plt.close()\n\n# ---------------- Plot 3: Confusion matrix ---------------------------\ntry:\n    from sklearn.metrics import confusion_matrix\n\n    cm = confusion_matrix(gts, preds)\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"{exp_name} \u2013 {ds_name} Confusion Matrix\")\n    fpath = os.path.join(working_dir, f\"{ds_name}_confusion_matrix.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# ---------------- Plot 4: Prediction vs GT histogram -----------------\ntry:\n    plt.figure(figsize=(8, 4))\n    uniq = np.arange(max(gts.max(), preds.max()) + 1)\n    plt.subplot(1, 2, 1)\n    gt_counts = [(gts == u).sum() for u in uniq]\n    plt.bar(uniq, gt_counts)\n    plt.title(\"Ground Truth Counts\")\n    plt.subplot(1, 2, 2)\n    pred_counts = [(preds == u).sum() for u in uniq]\n    plt.bar(uniq, pred_counts, color=\"orange\")\n    plt.title(\"Prediction Counts\")\n    plt.suptitle(\n        f\"{exp_name} \u2013 {ds_name}\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    fpath = os.path.join(working_dir, f\"{ds_name}_class_count_comparison.png\")\n    plt.savefig(fpath)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating class count plot: {e}\")\n    plt.close()\n\n# ---------------- Print final metrics --------------------------------\nprint(f\"Test Loss: {test_loss:.4f} | Test Macro-F1: {test_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    exp_name, ds_name = \"Remove_Label_Smoothing_Loss\", \"SPR_BENCH\"\n    rec = experiment_data[exp_name][ds_name]\n    epochs = rec[\"epochs\"]\n    # -------------------------------------------------------\n    # 1) Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, rec[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, rec[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH \u2013 Loss Curves (Train vs Validation)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n    # -------------------------------------------------------\n    # 2) Macro-F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, rec[\"metrics\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, rec[\"metrics\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH \u2013 Macro-F1 Curves (Train vs Validation)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n    # -------------------------------------------------------\n    # 3) Confusion matrix\n    try:\n        y_true = np.array(rec[\"ground_truth\"])\n        y_pred = np.array(rec[\"predictions\"])\n        n_labels = int(max(y_true.max(), y_pred.max())) + 1\n        cm = np.zeros((n_labels, n_labels), dtype=int)\n        for t, p in zip(y_true, y_pred):\n            cm[t, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(\n            \"SPR_BENCH \u2013 Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.xticks(range(n_labels))\n        plt.yticks(range(n_labels))\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n    # -------------------------------------------------------\n    # 4) Label distribution comparison\n    try:\n        gt_counts = np.bincount(y_true, minlength=n_labels)\n        pred_counts = np.bincount(y_pred, minlength=n_labels)\n        x = np.arange(n_labels)\n        width = 0.35\n        plt.figure()\n        plt.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n        plt.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n        plt.xlabel(\"Label\")\n        plt.ylabel(\"Count\")\n        plt.title(\"SPR_BENCH \u2013 Label Distribution (GT vs Predictions)\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_label_distribution.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating label distribution plot: {e}\")\n        plt.close()\n    # -------------------------------------------------------\n    # print final metrics\n    print(f\"Test loss : {rec.get('test_loss'):.4f}\")\n    print(f\"Test Macro-F1 : {rec.get('test_macroF1'):.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper to safely fetch nested keys\ndef _get(d, *keys, default=None):\n    for k in keys:\n        d = d.get(k, {})\n    return d if d else default\n\n\ned = _get(experiment_data, \"Remove_Transformer_Encoder\", \"SPR_BENCH\", default={})\n\nif not ed:\n    print(\"No data found for Remove_Transformer_Encoder / SPR_BENCH\")\n    exit()\n\nepochs = ed.get(\"epochs\", [])\nloss_tr, loss_val = ed[\"losses\"][\"train\"], ed[\"losses\"][\"val\"]\nf1_tr, f1_val = ed[\"metrics\"][\"train\"], ed[\"metrics\"][\"val\"]\ntest_preds, test_gts = ed.get(\"predictions\", []), ed.get(\"ground_truth\", [])\n\n# ------------------------------------------------------------\n# 1) Loss curves\ntry:\n    plt.figure()\n    plt.plot(epochs, loss_tr, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH \u2013 Loss Curves\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Loss Curves plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# 2) Macro-F1 curves\ntry:\n    plt.figure()\n    plt.plot(epochs, f1_tr, label=\"Train\")\n    plt.plot(epochs, f1_val, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH \u2013 Macro-F1 Curves\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macroF1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Macro-F1 Curves plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# 3) Confusion matrix on test set\ntry:\n    if test_preds and test_gts:\n        cm = confusion_matrix(test_gts, test_preds)\n        plt.figure()\n        plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n        plt.title(\n            \"SPR_BENCH \u2013 Confusion Matrix\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        plt.colorbar()\n        tick_marks = np.arange(cm.shape[0])\n        plt.xticks(tick_marks)\n        plt.yticks(tick_marks)\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                format(cm[i, j], \"d\"),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n            )\n        plt.ylabel(\"True label\")\n        plt.xlabel(\"Predicted label\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating Confusion Matrix plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------\n# Print test metrics\nprint(f\"Test loss  : {ed.get('test_loss', 'N/A')}\")\nprint(f\"Test MacroF1: {ed.get('test_macroF1', 'N/A')}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- Load experiment data -----------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfinal_val_f1 = {}  # collect for cross-dataset comparison\n\nfor ds_name, ds_dict in experiment_data.items():\n    epochs = ds_dict.get(\"epochs\", [])\n    tr_loss = ds_dict[\"losses\"][\"train\"] if \"losses\" in ds_dict else []\n    val_loss = ds_dict[\"losses\"][\"val\"] if \"losses\" in ds_dict else []\n    tr_f1 = ds_dict[\"metrics\"][\"train\"] if \"metrics\" in ds_dict else []\n    val_f1 = ds_dict[\"metrics\"][\"val\"] if \"metrics\" in ds_dict else []\n    preds = ds_dict.get(\"predictions\", [])\n    gts = ds_dict.get(\"ground_truth\", [])\n    test_f1 = ds_dict.get(\"test_macroF1\", None)\n    if test_f1 is not None:\n        print(f\"{ds_name} \u2013 Test Macro-F1: {test_f1:.4f}\")\n\n    # save last val f1 for comparison\n    if val_f1:\n        final_val_f1[ds_name] = val_f1[-1]\n\n    # ---------------- Loss curve ---------------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Training vs Validation Loss\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting loss for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Macro-F1 curve -----------------------------------\n    try:\n        plt.figure()\n        plt.plot(epochs, tr_f1, label=\"Train Macro-F1\")\n        plt.plot(epochs, val_f1, label=\"Validation Macro-F1\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Training vs Validation Macro-F1\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_macroF1_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Confusion matrix ---------------------------------\n    try:\n        if preds and gts:\n            from sklearn.metrics import confusion_matrix\n\n            cm = confusion_matrix(gts, preds, normalize=\"true\")\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im, fraction=0.046, pad=0.04)\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\n                f\"{ds_name}: Normalised Confusion Matrix\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_conf_matrix.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting confusion matrix for {ds_name}: {e}\")\n        plt.close()\n\n    # ---------------- Label distribution --------------------------------\n    try:\n        if gts:\n            labels, counts = np.unique(gts, return_counts=True)\n            plt.figure()\n            plt.bar(labels, counts)\n            plt.xlabel(\"Label\")\n            plt.ylabel(\"Frequency\")\n            plt.title(f\"{ds_name}: Ground-Truth Label Distribution\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error plotting label distribution for {ds_name}: {e}\")\n        plt.close()\n\n# ---------------- Cross-dataset comparison plot -------------------------\ntry:\n    if final_val_f1:\n        plt.figure()\n        names = list(final_val_f1.keys())\n        scores = [final_val_f1[n] for n in names]\n        plt.barh(names, scores)\n        plt.xlabel(\"Final Validation Macro-F1\")\n        plt.title(\"Comparison of Final Validation Macro-F1 Across Datasets\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_val_macroF1.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------------------\n# 1. Load all experiment_data.npy files that were provided\n# -----------------------------------------------------------------------\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_bf27f61eddc94237a911bdc8d58d4dfa_proc_3475348/experiment_data.npy\",\n        \"experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_561bc6e689bd4956adfbd96c7ab0fe6c_proc_3475349/experiment_data.npy\",\n        \"experiments/2025-08-17_23-44-27_conceptual_generalization_poly_rule_attempt_0/logs/0-run/experiment_results/experiment_d9625dc2dea04ba2bf2978443e5a13a2_proc_3475347/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for experiment_data_path in experiment_data_path_list:\n        # We rely on the environment variable if it exists, otherwise use relative path\n        root_path = os.getenv(\"AI_SCIENTIST_ROOT\", \"\")\n        full_path = os.path.join(root_path, experiment_data_path)\n        exp_dict = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dict)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# -----------------------------------------------------------------------\n# 2. Aggregate metrics per dataset across different runs\n# -----------------------------------------------------------------------\nagg = {}  # {dataset: {'epochs': [[..],[..]], 'train_loss': [[..]], ...}}\nfor run_dict in all_experiment_data:\n    for ds_name, ds in run_dict.items():\n        ds_agg = agg.setdefault(\n            ds_name,\n            {\n                \"epochs\": [],\n                \"train_loss\": [],\n                \"val_loss\": [],\n                \"train_f1\": [],\n                \"val_f1\": [],\n            },\n        )\n        ds_agg[\"epochs\"].append(np.asarray(ds.get(\"epochs\", []), dtype=float))\n        ds_agg[\"train_loss\"].append(\n            np.asarray(ds.get(\"losses\", {}).get(\"train\", []), dtype=float)\n        )\n        ds_agg[\"val_loss\"].append(\n            np.asarray(ds.get(\"losses\", {}).get(\"val\", []), dtype=float)\n        )\n        ds_agg[\"train_f1\"].append(\n            np.asarray(ds.get(\"metrics\", {}).get(\"train\", []), dtype=float)\n        )\n        ds_agg[\"val_f1\"].append(\n            np.asarray(ds.get(\"metrics\", {}).get(\"val\", []), dtype=float)\n        )\n\n# store final val scores for summary plot\nfinal_val_summary = {}  # {ds: (mean, sem)}\n\n# -----------------------------------------------------------------------\n# 3. Plot aggregated curves for each dataset\n# -----------------------------------------------------------------------\nfor ds_name, ds_agg in agg.items():\n    # Skip if we do not have at least one valid run\n    if not ds_agg[\"epochs\"]:\n        continue\n\n    # Align sequence lengths (use min length across runs)\n    min_len = min(len(e) for e in ds_agg[\"epochs\"] if len(e) > 0)\n    if min_len == 0:\n        continue  # nothing to plot\n\n    # Create matrices shape (n_runs, min_len)\n    def stack_and_truncate(list_of_arr):\n        return np.vstack([arr[:min_len] for arr in list_of_arr])\n\n    epochs = ds_agg[\"epochs\"][0][:min_len]  # assume identical epoch values\n\n    train_loss_mtx = stack_and_truncate(ds_agg[\"train_loss\"])\n    val_loss_mtx = stack_and_truncate(ds_agg[\"val_loss\"])\n    train_f1_mtx = stack_and_truncate(ds_agg[\"train_f1\"])\n    val_f1_mtx = stack_and_truncate(ds_agg[\"val_f1\"])\n\n    n_runs = train_loss_mtx.shape[0]\n    sem = lambda x: (\n        np.std(x, axis=0, ddof=1) / np.sqrt(n_runs)\n        if n_runs > 1\n        else np.zeros_like(np.mean(x, axis=0))\n    )\n\n    # -------------------- Aggregated Loss curve -------------------------\n    try:\n        plt.figure()\n        tr_mean, tr_sem = np.mean(train_loss_mtx, axis=0), sem(train_loss_mtx)\n        val_mean, val_sem = np.mean(val_loss_mtx, axis=0), sem(val_loss_mtx)\n\n        plt.plot(epochs, tr_mean, label=\"Train Loss (mean)\")\n        plt.fill_between(\n            epochs,\n            tr_mean - tr_sem,\n            tr_mean + tr_sem,\n            alpha=0.3,\n            label=\"Train Loss \u00b1 SEM\",\n        )\n        plt.plot(epochs, val_mean, label=\"Val Loss (mean)\")\n        plt.fill_between(\n            epochs,\n            val_mean - val_sem,\n            val_mean + val_sem,\n            alpha=0.3,\n            label=\"Val Loss \u00b1 SEM\",\n        )\n\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name}: Aggregated Training vs Validation Loss (Mean \u00b1 SEM)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_aggregated_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated loss for {ds_name}: {e}\")\n        plt.close()\n\n    # -------------------- Aggregated Macro-F1 curve --------------------\n    try:\n        plt.figure()\n        tr_mean, tr_sem = np.mean(train_f1_mtx, axis=0), sem(train_f1_mtx)\n        val_mean, val_sem = np.mean(val_f1_mtx, axis=0), sem(val_f1_mtx)\n\n        plt.plot(epochs, tr_mean, label=\"Train Macro-F1 (mean)\")\n        plt.fill_between(\n            epochs,\n            tr_mean - tr_sem,\n            tr_mean + tr_sem,\n            alpha=0.3,\n            label=\"Train Macro-F1 \u00b1 SEM\",\n        )\n        plt.plot(epochs, val_mean, label=\"Val Macro-F1 (mean)\")\n        plt.fill_between(\n            epochs,\n            val_mean - val_sem,\n            val_mean + val_sem,\n            alpha=0.3,\n            label=\"Val Macro-F1 \u00b1 SEM\",\n        )\n\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(f\"{ds_name}: Aggregated Training vs Validation Macro-F1 (Mean \u00b1 SEM)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\n            os.path.join(working_dir, f\"{ds_name}_aggregated_macroF1_curve.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error plotting aggregated f1 for {ds_name}: {e}\")\n        plt.close()\n\n    # store final val f1\n    final_val_summary[ds_name] = (val_mean[-1], val_sem[-1])\n\n# -----------------------------------------------------------------------\n# 4. Cross-dataset comparison bar chart of Final Validation Macro-F1\n# -----------------------------------------------------------------------\ntry:\n    if final_val_summary:\n        plt.figure()\n        names = list(final_val_summary.keys())\n        means = [final_val_summary[n][0] for n in names]\n        sems = [final_val_summary[n][1] for n in names]\n\n        y_pos = np.arange(len(names))\n        plt.barh(y_pos, means, xerr=sems, align=\"center\", alpha=0.7)\n        plt.yticks(y_pos, names)\n        plt.xlabel(\"Final Validation Macro-F1 (mean \u00b1 SEM)\")\n        plt.title(\"Cross-Dataset Comparison of Final Validation Macro-F1\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"cross_dataset_final_val_macroF1.png\"))\n        plt.close()\n\n        # print aggregated numbers\n        print(\"Final Validation Macro-F1 (mean \u00b1 SEM):\")\n        for n, m, s in zip(names, means, sems):\n            print(f\"  {n}: {m:.4f} \u00b1 {s:.4f}\")\nexcept Exception as e:\n    print(f\"Error plotting cross-dataset aggregated comparison: {e}\")\n    plt.close()\n"], "plot_plan": ["We load experiment_data.npy, gracefully handle missing files, and iterate over every dataset key present (currently only SPR_BENCH but the loop generalises). For each dataset we extract epochs, training/validation loss and macro-F1, predictions and ground-truth. We then create: (1) loss curves, (2) macro-F1 curves, (3) normalised confusion matrices if predictions are available, and (4) a simple class-distribution bar chart. Each plot is wrapped in its own try-except to ensure the script never crashes and all figures are closed. After the per-dataset plots, we build an across-dataset comparison bar chart of the final validation macro-F1 so that future runs with multiple datasets are visualised consistently. All figures are saved to working_dir with descriptive names. Finally, we print the test macro-F1 for every dataset for quick inspection.", "We will load the experiment_data.npy file, extract the Remove Gating Mechanism / SPR_BENCH sub-dictionary, and pull out epoch indices, per-epoch losses, per-epoch macro-F1 scores, as well as the saved test predictions and ground-truth labels.  \nThree standard visualizations will be produced: (1) training vs validation loss over epochs, (2) training vs validation macro-F1 over epochs, and (3) a confusion-matrix heat-map for the test set.  \nEach plot is wrapped in its own try-except block to prevent a single failure from aborting the whole script; figures are always closed after saving.  \nAll plots are titled clearly and saved inside working_dir with informative filenames that include the dataset name and plot type.  \nIf the npy file cannot be loaded an error is reported gracefully.  \nThe script also prints out the stored test loss and test macro-F1 so users can quickly verify final performance without opening the plots.  \nOnly data present in experiment_data.npy are used\u2014no values are synthesized.  \nThe code follows the required import order and directory creation, uses only basic matplotlib, and limits output to the three essential figures (\u22645 as requested).", "We will first load the serialized experiment_data.npy, recover the sub-dictionary for the \u201cRemove_Symbolic_Feature_Pathway / SPR_BENCH\u201d run, and pull out the per-epoch training/validation losses and macro-F1 scores.  Using matplotlib we will draw two time-series plots\u2014(1) loss curves and (2) F1 curves\u2014each with epoch on the x-axis and the relevant metric on the y-axis; these will help visualise convergence and any overfitting.  Next, using the saved test-set predictions and ground-truth labels, we will compute a confusion matrix and render it as a heat-map for a quick qualitative check of class-specific performance.  Each figure is wrapped in its own try-except so that one failure does not mask the others, and we always close figures.  All images are saved into the working directory with descriptive filenames that include the dataset name and plot type.  Finally, we print out the stored test macro-F1 and test loss to give a numeric summary alongside the visuals.  Only data present in experiment_data.npy are accessed; no synthetic values are created.  No more than three figures are produced, staying well under the 5-plot limit.", "We will load the stored NumPy dictionary, pick the sole experiment (\u201cRemove _Positional _Encoding\u201d on the SPR_BENCH dataset), and visualise its contents. Three independent plots suffice: (1) train/validation loss across epochs, (2) train/validation macro-F1 across epochs, and (3) a confusion-matrix heat-map for the test predictions vs. ground truth. Each plot will be wrapped in a try-except guard, saved to the working directory with descriptive names, and the figure will be closed afterwards. We respect the \u201c\u2264 5 figures\u201d rule, add clear titles/sub-titles indicating dataset/model, and ensure we do not invent any data. Finally, we print the stored test loss and macro-F1 so that the code outputs the main evaluation numbers alongside producing the plots.", "We will load experiment_data.npy, iterate through each experiment/dataset entry, and extract epochs, train/val losses, train/val macro-F1, test predictions, and ground-truth labels. Using basic matplotlib, we will plot: (1) training vs validation loss curves, (2) training vs validation macro-F1 curves, (3) a confusion matrix of test predictions vs ground truth, and (4) a side-by-side bar chart comparing class distributions of ground truth and predictions. Each plot lives in a separate try-except block, is titled descriptively, saved to working_dir with dataset and plot type in the filename, and the figure is always closed. We cap the total number of figures to four, satisfying the \u201c\u22645\u201d rule. Finally, the script prints the test macro-F1 that is stored in the experiment data so the user sees the key evaluation metric on stdout.", "We first load the serialized experiment_data.npy and pull out the single run (Remove_Unigram_Count_Features \u2013 SPR_BENCH).  From it we extract epoch indices, train/val losses, train/val macro-F1 scores, and the stored test predictions/ground-truth plus test loss/F1.  We create up to four standard plots: (1) train vs. validation loss per epoch, (2) train vs. validation macro-F1 per epoch, (3) a test-set confusion matrix heat-map, and (4) bar plots comparing class counts of test predictions versus ground truth.  Each plot is wrapped in its own try-except so that one failure won\u2019t halt the others, and every figure is saved to working_dir with descriptive names and then closed.  All plots include titles that mention both the dataset (SPR_BENCH) and the plot type; multi-panel figures show clear subtitles like \u201cLeft: Ground Truth, Right: Generated Samples\u201d when applicable.  After plotting we also print the recorded test macro-F1 and test loss so the script gives immediate feedback in the console.  Only data present in experiment_data.npy are used\u2014no fabricated values are introduced.  Finally, we respect the 5-figure limit (we only create four) and keep the matplotlib usage minimal and standard.  The code below implements this workflow concisely.", "The solution loads the saved `experiment_data.npy`, extracts the record for the `SPR_BENCH` dataset, and generates at most four standard plots: (1) train/validation loss curves, (2) train/validation macro-F1 curves, (3) a confusion-matrix heat-map, and (4) a per-label count comparison between ground-truth and predictions. Each plot is produced inside its own `try-except` block, closed after saving, titled clearly, and stored in the `working` directory with descriptive file names. Epoch indices are used directly, so no need for sub-sampling (only 10 epochs). The code relies solely on `matplotlib`, `numpy`, and `os`, respecting the requirement to avoid made-up data. Finally, it prints the recorded test loss and macro-F1 to give an immediate textual summary alongside the plots.", null, "We will load the saved experiment_data.npy, pull out epochs, train/val losses, train/val macro-F1 values, and the held-out test predictions/labels. Three separate plots will be produced: (1) training vs. validation loss, (2) training vs. validation macro-F1, and (3) a confusion-matrix heat-map on the test set\u2014all for the SPR_BENCH dataset in the Remove_Transformer_Encoder ablation study. Each figure is wrapped in its own try-except block, saved into working_dir with descriptive filenames, titled clearly, and the figures are always closed. Finally, the script prints the test loss and macro-F1 that were stored in experiment_data so users can confirm the numbers used to generate the plots.", null, null, null, null], "ablation_name": [null, "Remove_Gating_Mechanism", "Remove_Symbolic_Feature_Pathway", "Remove_Positional_Encoding", "Remove_Bigram_Hashed_Features", "Remove_Unigram_Count_Features", "Remove_Label_Smoothing_Loss", "Remove_Length_Feature", "Remove_Transformer_Encoder", null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", "The script below loads the saved experiment_data.npy file from the working\ndirectory, extracts the metric arrays, computes either the final (for training)\nor best (for validation) values, and prints them together with the test results.\nEach metric is clearly labeled so the output is self-explanatory. All code\nexecutes immediately upon running the file.", "The script will load the saved numpy file from the \u201cworking\u201d directory, turn it\nback into the original Python dict, and iterate over every experiment and every\ncontained dataset.   For each dataset it will compute the final-epoch training\nloss and F1 score, the best (minimum) validation loss and best (maximum)\nvalidation F1 score, and simply read the stored test loss and test macro-F1.\nIt then prints the dataset name followed by clearly labelled metric values.\nAll code is in the global scope so the file executes immediately when run.", "The script will load the saved NumPy file from the \u201cworking\u201d directory, walk\nthrough the nested dictionary (experiment \u2192 dataset), and then print the key\nresults.  For each dataset it will display (1) the last-epoch training loss, (2)\nthe best training macro F1 score, (3) the last-epoch validation loss, (4) the\nbest validation macro F1 score, and (5) the single test loss and macro F1 score\nrecorded after training.  Metrics are clearly labeled (e.g., \u201ctraining macro F1\nscore\u201d) and printed immediately after the dataset name.  No plots are generated\nand the code runs at top level as required.", "The script will locate the \u201fworking\u201d directory, load the saved NumPy dictionary,\nand iterate through every experiment and dataset it contains.   For each dataset\nit will extract the lists of training/validation F1 scores and losses\naccumulated over the epochs, select the best value (maximum F1, minimum loss),\nand read the final test metrics stored separately.   It will then print a small,\nclearly-labeled report: dataset name first, followed by the best training\nmacro-F1 score, best validation macro-F1 score, lowest training loss, lowest\nvalidation loss, final test macro-F1 score, and final test loss\u2014each with an\nexplicit metric name.", "The script below immediately loads the saved experiment_data.npy file from the\nworking directory, navigates the nested dictionary structure, and prints\nclearly-labeled final (or best) metrics for each dataset. It follows the\nrequired formatting rules, avoids an `if __name__ == \"__main__\":` guard, and\nperforms no plotting.", "The script loads the saved numpy dictionary from the working directory, drills\ndown through the nested experiment \u2192 dataset \u2192 metrics structure, and prints the\nfinal (last\u2010epoch) values for every tracked metric. It explicitly labels each\nmetric (e.g., \u201cfinal training loss\u201d, \u201ctest macro F1 score\u201d) and always prints\nthe dataset name first, satisfying the formatting constraints. No code is\nwrapped in an entry-point guard, so it runs immediately on execution.", "The script loads the experiment_data.npy file from the working directory,\nnavigates the nested dictionary structure, and extracts the relevant loss and\nF1-score lists for each dataset. It then computes the final training values\n(last epoch), the best validation values (lowest loss and highest F1), and the\nsingle stored test values. These metrics are printed with fully-qualified names\nso their purpose is unambiguous, and each dataset\u2019s block is clearly separated\nby its name. No plots are generated and the code runs immediately at the top\nlevel.", "We load the saved NumPy dictionary from the \u201cworking\u201d directory, traverse the\nnested structure (experiment \u2192 dataset), and pull out the recorded metric and\nloss lists. The script then computes the relevant summary statistics: the final\ntraining values, the best (minimum) validation loss, the best (maximum)\nvalidation macro F1, and the single stored test figures. Each dataset name is\nprinted first, followed by clearly-labelled metric lines. The code executes\nimmediately at import without requiring a special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", "The script will load the saved NumPy dictionary from the working directory,\niterate through each stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute the best-\nperforming or final values for every recorded metric. For F1 scores the \u201cbest\u2019\u2019\nvalue is the maximum, while for losses it is the minimum (lower is better). It\nthen prints the dataset name followed by clearly-labelled metrics such as \u201cbest\ntraining macro F1 score,\u201d \u201clowest validation loss,\u201d \u201ctest macro F1 score,\u201d etc.\nThe code executes immediately without needing any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------\n# Helper to print a metric only if it exists\ndef _print_metric(label: str, value):\n    if value is not None:\n        print(f\"  {label}: {value:.4f}\")\n\n\n# ------------------------------------------------------------\n# Iterate through ablation \u2192 dataset \u2192 metrics\nfor ablation_name, dataset_dict in experiment_data.items():\n    for dataset_name, stats in dataset_dict.items():\n        print(dataset_name + \":\")  # dataset heading\n\n        train_losses = stats[\"losses\"][\"train\"]\n        val_losses = stats[\"losses\"][\"val\"]\n        train_f1s = stats[\"metrics\"][\"train\"]\n        val_f1s = stats[\"metrics\"][\"val\"]\n\n        # Final training values\n        final_train_loss = train_losses[-1] if train_losses else None\n        final_train_f1 = train_f1s[-1] if train_f1s else None\n\n        # Best validation values\n        best_val_loss = min(val_losses) if val_losses else None\n        best_val_f1 = max(val_f1s) if val_f1s else None\n\n        # Test set values (added after training loop)\n        test_loss = stats.get(\"test_loss\")\n        test_f1 = stats.get(\"test_macroF1\")\n\n        # Print metrics with explicit labels\n        _print_metric(\"training loss\", final_train_loss)\n        _print_metric(\"training macro F1 score\", final_train_f1)\n        _print_metric(\"best validation loss\", best_val_loss)\n        _print_metric(\"best validation macro F1 score\", best_val_f1)\n        _print_metric(\"test loss\", test_loss)\n        _print_metric(\"test macro F1 score\", test_f1)\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# ----------------------------------------------------------------------\n# 0. Locate and load the numpy file that stores the bookkeeping dict\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n# ----------------------------------------------------------------------\n# 1\u20135. Traverse the experiments \u2192 datasets and print the requested stats\nfor exp_name, datasets in experiment_data.items():\n    for ds_name, ds_dict in datasets.items():\n        print(f\"\\nDataset: {ds_name}\")\n\n        # Helper aliases\n        train_losses = ds_dict[\"losses\"][\"train\"]\n        val_losses = ds_dict[\"losses\"][\"val\"]\n        train_f1s = ds_dict[\"metrics\"][\"train\"]\n        val_f1s = ds_dict[\"metrics\"][\"val\"]\n\n        # Final-epoch results\n        final_train_loss = train_losses[-1] if train_losses else None\n        final_train_f1 = train_f1s[-1] if train_f1s else None\n\n        # Best validation results\n        best_val_loss = min(val_losses) if val_losses else None\n        best_val_f1 = max(val_f1s) if val_f1s else None\n\n        # Test results saved separately\n        test_loss = ds_dict.get(\"test_loss\", None)\n        test_f1 = ds_dict.get(\"test_macroF1\", None)\n\n        # ------------------------------------------------------------------\n        # 3. Print metrics with explicit labels\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n        if final_train_f1 is not None:\n            print(f\"final training F1 score: {final_train_f1:.4f}\")\n\n        if best_val_loss is not None:\n            print(f\"best validation loss: {best_val_loss:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"best validation F1 score: {best_val_f1:.4f}\")\n\n        if test_loss is not None:\n            print(f\"test loss: {test_loss:.4f}\")\n        if test_f1 is not None:\n            print(f\"test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the saved experiment dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper for nice printing\ndef print_metric(label: str, value):\n    if isinstance(value, float):\n        print(f\"  {label}: {value:.4f}\")\n    else:\n        print(f\"  {label}: {value}\")\n\n\n# ------------------------------------------------------------------\n# Traverse experiments and datasets\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}  (experiment: {exp_name})\")\n\n        # Training metrics\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        train_f1s = data[\"metrics\"][\"train\"]\n        val_f1s = data[\"metrics\"][\"val\"]\n\n        final_train_loss = train_losses[-1] if train_losses else None\n        best_train_f1 = max(train_f1s) if train_f1s else None\n        final_validation_loss = val_losses[-1] if val_losses else None\n        best_validation_f1 = max(val_f1s) if val_f1s else None\n\n        print_metric(\"final training loss\", final_train_loss)\n        print_metric(\"best training macro F1 score\", best_train_f1)\n        print_metric(\"final validation loss\", final_validation_loss)\n        print_metric(\"best validation macro F1 score\", best_validation_f1)\n\n        # Test metrics\n        test_loss = data.get(\"test_loss\")\n        test_f1 = data.get(\"test_macroF1\")\n\n        if test_loss is not None:\n            print_metric(\"test loss\", test_loss)\n        if test_f1 is not None:\n            print_metric(\"test macro F1 score\", test_f1)\n\n        # Spacing between datasets\n        print()\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# 0. Locate the working directory and load the numpy file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\ndef print_metrics():\n    for exp_name, exp_content in experiment_data.items():\n        for dataset_name, dataset_dict in exp_content.items():\n            # Fetch metric/loss history\n            train_f1_history = dataset_dict[\"metrics\"][\"train\"]\n            val_f1_history = dataset_dict[\"metrics\"][\"val\"]\n            train_loss_hist = dataset_dict[\"losses\"][\"train\"]\n            val_loss_hist = dataset_dict[\"losses\"][\"val\"]\n\n            # Choose best values: max F1, min loss\n            best_train_f1 = max(train_f1_history) if train_f1_history else None\n            best_val_f1 = max(val_f1_history) if val_f1_history else None\n            best_train_loss = min(train_loss_hist) if train_loss_hist else None\n            best_val_loss = min(val_loss_hist) if val_loss_hist else None\n\n            # Test metrics (stored once, not per-epoch)\n            test_f1 = dataset_dict.get(\"test_macroF1\", None)\n            test_loss = dataset_dict.get(\"test_loss\", None)\n\n            # -----------------------------------------------------------------\n            # 3. Print results with clear, explicit metric names\n            print(f\"Dataset: {dataset_name}\")\n            if best_train_f1 is not None:\n                print(f\"Best training macro F1 score: {best_train_f1:.4f}\")\n            if best_val_f1 is not None:\n                print(f\"Best validation macro F1 score: {best_val_f1:.4f}\")\n            if best_train_loss is not None:\n                print(f\"Lowest training loss: {best_train_loss:.4f}\")\n            if best_val_loss is not None:\n                print(f\"Lowest validation loss: {best_val_loss:.4f}\")\n            if test_f1 is not None:\n                print(f\"Test macro F1 score: {test_f1:.4f}\")\n            if test_loss is not None:\n                print(f\"Test loss: {test_loss:.4f}\")\n            print()  # blank line between datasets\n\n\nprint_metrics()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# Locate and load the stored experiment data\n# ------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\n# Helper to obtain final / best metrics from a dataset entry\n# ------------------------------------------------------------------\ndef summarize_dataset(ds_dict):\n    \"\"\"\n    Given the dictionary for a single dataset, return a dict with\n    final training metrics, best validation metrics, and test metrics.\n    \"\"\"\n    summary = {}\n\n    # Final epoch values\n    summary[\"final training macro F1 score\"] = ds_dict[\"metrics\"][\"train\"][-1]\n    summary[\"final training loss\"] = ds_dict[\"losses\"][\"train\"][-1]\n\n    # Best validation values\n    summary[\"best validation macro F1 score\"] = max(ds_dict[\"metrics\"][\"val\"])\n    summary[\"best validation loss\"] = min(ds_dict[\"losses\"][\"val\"])\n\n    # Test set values (single numbers saved after training)\n    summary[\"test macro F1 score\"] = ds_dict.get(\"test_macroF1\")\n    summary[\"test loss\"] = ds_dict.get(\"test_loss\")\n\n    return summary\n\n\n# ------------------------------------------------------------------\n# Iterate over experiments and datasets, printing requested metrics\n# ------------------------------------------------------------------\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, dataset_dict in datasets.items():\n        print(f\"Dataset: {dataset_name}\")  # dataset name first, per requirements\n\n        metrics_to_print = summarize_dataset(dataset_dict)\n\n        for metric_name, value in metrics_to_print.items():\n            if value is not None:\n                print(f\"{metric_name}: {value:.4f}\")\n\n        print()  # blank line for readability between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the results file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        # Print dataset name first\n        print(dataset_name)\n\n        # Retrieve stored arrays / scalars\n        metrics = data.get(\"metrics\", {})\n        losses = data.get(\"losses\", {})\n        test_f1 = data.get(\"test_macroF1\")\n        test_loss = data.get(\"test_loss\")\n\n        # Final / last epoch metrics\n        if \"train\" in metrics and metrics[\"train\"]:\n            print(f\"Final training macro F1 score: {metrics['train'][-1]:.4f}\")\n        if \"val\" in metrics and metrics[\"val\"]:\n            print(f\"Final validation macro F1 score: {metrics['val'][-1]:.4f}\")\n\n        if test_f1 is not None:\n            print(f\"Test macro F1 score: {test_f1:.4f}\")\n\n        if \"train\" in losses and losses[\"train\"]:\n            print(f\"Final training loss: {losses['train'][-1]:.4f}\")\n        if \"val\" in losses and losses[\"val\"]:\n            print(f\"Final validation loss: {losses['val'][-1]:.4f}\")\n\n        if test_loss is not None:\n            print(f\"Test loss: {test_loss:.4f}\")\n\n        # Add a blank line between datasets for readability\n        print()\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------\n# locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Cannot locate {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------\n# iterate over experiments / datasets and report metrics\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, ds_data in datasets.items():\n        print(f\"\\nDataset: {dataset_name}  (experiment: {experiment_name})\")\n\n        # training values (final epoch)\n        train_losses = ds_data[\"losses\"][\"train\"]\n        train_f1s = ds_data[\"metrics\"][\"train\"]\n        final_train_loss = train_losses[-1] if train_losses else None\n        final_train_f1 = train_f1s[-1] if train_f1s else None\n\n        # validation values (best across epochs)\n        val_losses = ds_data[\"losses\"][\"val\"]\n        val_f1s = ds_data[\"metrics\"][\"val\"]\n        best_val_loss = min(val_losses) if val_losses else None\n        best_val_f1 = max(val_f1s) if val_f1s else None\n\n        # test values (single run stored after training)\n        test_loss = ds_data.get(\"test_loss\")\n        test_f1 = ds_data.get(\"test_macroF1\")\n\n        # print metrics with explicit names\n        if final_train_loss is not None:\n            print(f\"final training loss: {final_train_loss:.4f}\")\n        if final_train_f1 is not None:\n            print(f\"final training macro F1 score: {final_train_f1:.4f}\")\n        if best_val_loss is not None:\n            print(f\"best validation loss: {best_val_loss:.4f}\")\n        if best_val_f1 is not None:\n            print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n        if test_loss is not None:\n            print(f\"test loss: {test_loss:.4f}\")\n        if test_f1 is not None:\n            print(f\"test macro F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# --------------------------------------------------------------------\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# --------------------------------------------------------------------\ndef summarize_dataset(data_dict):\n    \"\"\"Print formatted summary statistics for one dataset dictionary.\"\"\"\n    # Training metrics\n    final_train_f1 = data_dict[\"metrics\"][\"train\"][-1]\n    final_train_loss = data_dict[\"losses\"][\"train\"][-1]\n\n    # Validation metrics (best across epochs)\n    best_val_f1 = max(data_dict[\"metrics\"][\"val\"])\n    best_val_loss = min(data_dict[\"losses\"][\"val\"])\n\n    # Test metrics (single numbers)\n    test_f1 = data_dict.get(\"test_macroF1\", None)\n    test_loss = data_dict.get(\"test_loss\", None)\n\n    print(f\"  training macro F1 score: {final_train_f1:.4f}\")\n    print(f\"  best validation macro F1 score: {best_val_f1:.4f}\")\n    print(f\"  test macro F1 score: {test_f1:.4f}\" if test_f1 is not None else \"\")\n    print(f\"  final training loss: {final_train_loss:.4f}\")\n    print(f\"  best validation loss: {best_val_loss:.4f}\")\n    print(f\"  test loss: {test_loss:.4f}\" if test_loss is not None else \"\")\n\n\n# --------------------------------------------------------------------\n# Iterate over experiments and datasets\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, dataset_metrics in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        summarize_dataset(dataset_metrics)\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", "import os\nimport numpy as np\n\n# ---------------------------------------------------------------------\n# Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# ---------------------------------------------------------------------\n# Helper to format float values\ndef fmt(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, int, np.floating)) else str(x)\n\n\n# ---------------------------------------------------------------------\n# Iterate over datasets and print metrics\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # Metric arrays\n    train_f1s = ds_dict.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = ds_dict.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds_dict.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds_dict.get(\"losses\", {}).get(\"val\", [])\n\n    # Best / final computations\n    if train_f1s:\n        print(\"  best training macro F1 score:\", fmt(max(train_f1s)))\n        print(\"  final training macro F1 score:\", fmt(train_f1s[-1]))\n    if val_f1s:\n        print(\"  best validation macro F1 score:\", fmt(max(val_f1s)))\n        print(\"  final validation macro F1 score:\", fmt(val_f1s[-1]))\n    if train_losses:\n        print(\"  lowest training loss:\", fmt(min(train_losses)))\n        print(\"  final training loss:\", fmt(train_losses[-1]))\n    if val_losses:\n        print(\"  lowest validation loss:\", fmt(min(val_losses)))\n        print(\"  final validation loss:\", fmt(val_losses[-1]))\n\n    # Test-set metrics\n    if \"test_macroF1\" in ds_dict:\n        print(\"  test macro F1 score:\", fmt(ds_dict[\"test_macroF1\"]))\n    if \"test_loss\" in ds_dict:\n        print(\"  test loss:\", fmt(ds_dict[\"test_loss\"]))\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '1.0000',\n'\\n', '  final training macro F1 score:', ' ', '1.0000', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.7000', '\\n', '  final validation macro F1\nscore:', ' ', '0.7000', '\\n', '  lowest training loss:', ' ', '0.3282', '\\n', '\nfinal training loss:', ' ', '0.3282', '\\n', '  lowest validation loss:', ' ',\n'0.7373', '\\n', '  final validation loss:', ' ', '0.8694', '\\n', '  test macro\nF1 score:', ' ', '0.6999', '\\n', '  test loss:', ' ', '0.8654', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH:', '\\n', '  training loss: 0.3278', '\\n', '  training macro F1\nscore: 0.9995', '\\n', '  best validation loss: 0.7046', '\\n', '  best validation\nmacro F1 score: 0.6980', '\\n', '  test loss: 0.8773', '\\n', '  test macro F1\nscore: 0.6999', '\\n', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'final training loss: 0.3337', '\\n', 'final\ntraining F1 score: 0.9955', '\\n', 'best validation loss: 0.7941', '\\n', 'best\nvalidation F1 score: 0.6980', '\\n', 'test loss: 0.8666', '\\n', 'test F1 score:\n0.6969', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH  (experiment: Remove_Positional_Encoding)', '\\n', '  final\ntraining loss: 0.3360', '\\n', '  best training macro F1 score: 0.9950', '\\n', '\nfinal validation loss: 0.8727', '\\n', '  best validation macro F1 score:\n0.6960', '\\n', '  test loss: 0.8729', '\\n', '  test macro F1 score: 0.6949',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'Best training macro F1 score: 0.9960', '\\n', 'Best\nvalidation macro F1 score: 0.6980', '\\n', 'Lowest training loss: 0.3328', '\\n',\n'Lowest validation loss: 0.8397', '\\n', 'Test macro F1 score: 0.6969', '\\n',\n'Test loss: 0.8707', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', 'final training macro F1 score: 0.9940', '\\n',\n'final training loss: 0.3359', '\\n', 'best validation macro F1 score: 0.6960',\n'\\n', 'best validation loss: 0.6861', '\\n', 'test macro F1 score: 0.6959', '\\n',\n'test loss: 0.8732', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['SPR_BENCH', '\\n', 'Final training macro F1 score: 0.9950', '\\n', 'Final\nvalidation macro F1 score: 0.6960', '\\n', 'Test macro F1 score: 0.6969', '\\n',\n'Final training loss: 0.0234', '\\n', 'Final validation loss: 1.9959', '\\n',\n'Test loss: 1.9799', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis 30 minutes).']", "['\\nDataset: SPR_BENCH  (experiment: Remove_Length_Feature)', '\\n', 'final\ntraining loss: 0.3339', '\\n', 'final training macro F1 score: 0.9960', '\\n',\n'best validation loss: 0.6677', '\\n', 'best validation macro F1 score: 0.6960',\n'\\n', 'test loss: 0.8700', '\\n', 'test macro F1 score: 0.6959', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  training macro F1 score: 0.9870', '\\n', '  best\nvalidation macro F1 score: 0.6920', '\\n', '  test macro F1 score: 0.6958', '\\n',\n'  final training loss: 0.3562', '\\n', '  best validation loss: 0.6655', '\\n', '\ntest loss: 0.8618', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '0.9945',\n'\\n', '  final training macro F1 score:', ' ', '0.9945', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.6980', '\\n', '  final validation macro F1\nscore:', ' ', '0.6960', '\\n', '  lowest training loss:', ' ', '0.3365', '\\n', '\nfinal training loss:', ' ', '0.3365', '\\n', '  lowest validation loss:', ' ',\n'0.7388', '\\n', '  final validation loss:', ' ', '0.8754', '\\n', '  test macro\nF1 score:', ' ', '0.6969', '\\n', '  test loss:', ' ', '0.8716', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '0.9970',\n'\\n', '  final training macro F1 score:', ' ', '0.9970', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.6960', '\\n', '  final validation macro F1\nscore:', ' ', '0.6960', '\\n', '  lowest training loss:', ' ', '0.3330', '\\n', '\nfinal training loss:', ' ', '0.3330', '\\n', '  lowest validation loss:', ' ',\n'0.6610', '\\n', '  final validation loss:', ' ', '0.8716', '\\n', '  test macro\nF1 score:', ' ', '0.6959', '\\n', '  test loss:', ' ', '0.8684', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  best training macro F1 score:', ' ', '0.9960',\n'\\n', '  final training macro F1 score:', ' ', '0.9960', '\\n', '  best\nvalidation macro F1 score:', ' ', '0.6980', '\\n', '  final validation macro F1\nscore:', ' ', '0.6980', '\\n', '  lowest training loss:', ' ', '0.3346', '\\n', '\nfinal training loss:', ' ', '0.3346', '\\n', '  lowest validation loss:', ' ',\n'0.6595', '\\n', '  final validation loss:', ' ', '0.8781', '\\n', '  test macro\nF1 score:', ' ', '0.6939', '\\n', '  test loss:', ' ', '0.8766', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}