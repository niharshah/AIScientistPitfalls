
% This paper introduces the transformer model, a foundational concept for modern deep learning architectures, particularly in sequence transduction tasks. It is highly relevant to this research as it forms the backbone of the proposed transformer-based model for symbolic reasoning tasks. It should be cited in the related work and methodology sections to provide a foundation for the use of transformer architectures and attention mechanisms.
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}

% This paper introduces a declarative framework for generating synthetic logical reasoning datasets using context-sensitive rules. It is highly relevant as it likely describes SPR_BENCH, the dataset used in the experiments to evaluate symbolic rule reasoning tasks. This citation should be included in the methodology section to acknowledge the dataset's source and provide context for its use in the experiments.
@article{sileo2024scalingsl,
 author = {Damien Sileo},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 journal = {ArXiv},
 title = {Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars},
 volume = {abs/2406.11035},
 year = {2024}
}

% This paper discusses the role of contextual embeddings in NLP tasks, highlighting their effectiveness in improving performance metrics and achieving state-of-the-art results with models like GPT and RoBERTa. It is relevant for providing theoretical support for leveraging contextual embeddings in symbolic reasoning and should be cited in the related work section to justify the hypothesis of adapting these embeddings for Synthetic PolyRule Reasoning tasks.
@conference{iskandarova2024advancingnl,
 author = {S. Iskandarova and Umidjon Kuziyev and Dilmurod Ashurov and Dilafruzkhon Rakhmatullayeva},
 booktitle = {2024 International Conference on IoT, Communication and Automation Technology (ICICAT)},
 journal = {2024 International Conference on IoT, Communication and Automation Technology (ICICAT)},
 pages = {792-796},
 title = {Advancing Natural Language Processing: Beyond Embeddings},
 year = {2024}
}

% This paper discusses the integration of auxiliary tasks in a multitask learning framework to enhance performance in deep learning models, specifically in encoder-decoder based speech recognition. It is relevant for supporting the methodology used in the proposed approach for Synthetic PolyRule Reasoning, where auxiliary tasks are employed to improve model generalization and sequence-level statistics encoding. This citation should be included in the methodology section.
@article{toshniwal2017multitasklw,
 author = {Shubham Toshniwal and Hao Tang and Liang Lu and Karen Livescu},
 booktitle = {Interspeech},
 pages = {3532-3536},
 title = {Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition},
 year = {2017}
}

% This paper analyzes the internal mechanisms of transformers trained on symbolic reasoning tasks, identifying interpretable mechanisms and their limitations. It highlights challenges in adapting transformers to symbolic reasoning, such as their reliance on depth-bounded recurrent mechanisms to handle multi-step reasoning. This citation will be included in the related work section to emphasize the inherent challenges and adaptations required for transformers in symbolic reasoning tasks.
@article{brinkmann2024ama,
 author = {Jannik Brinkmann and A. Sheshadri and Victor Levoso and Paul Swoboda and Christian Bartelt},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 pages = {4082-4102},
 title = {A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task},
 year = {2024}
}

% This paper discusses the limitations of high-dimensional concept embeddings in symbolic reasoning tasks and introduces interpretable approaches to address these gaps. It is relevant for highlighting research gaps in the application of contextual embeddings to symbolic reasoning tasks and should be cited in the related work section to emphasize the novelty of the proposed study.
@article{barbiero2023interpretablenc,
 author = {Pietro Barbiero and Gabriele Ciravegna and Francesco Giannini and Mateo Espinosa Zarlenga and Lucie Charlotte Magister and A. Tonda and Pietro Lio' and F. Precioso and M. Jamnik and G. Marra},
 booktitle = {International Workshop on Neural-Symbolic Learning and Reasoning},
 journal = {ArXiv},
 title = {Interpretable Neural-Symbolic Concept Reasoning},
 volume = {abs/2304.14068},
 year = {2023}
}

% This paper introduces a novel method called late chunking, which enhances contextual embeddings by preserving information across chunks in long-context embedding models. It is relevant for supporting the hypothesis that contextual embeddings can be adapted to capture dependencies and patterns beyond traditional NLP tasks. This citation should be included in the related work section to substantiate the versatility and adaptability of contextual embeddings for tasks like Synthetic PolyRule Reasoning.
@article{gunther2024latecc,
 author = {Michael Gunther and Isabelle Mohr and Bo Wang and Han Xiao},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models},
 volume = {abs/2409.04701},
 year = {2024}
}

% This paper presents 'Magnushammer,' a transformer-based approach to premise selection in automated theorem proving, and evaluates it on benchmarks such as PISA and miniF2F. It highlights the effectiveness of transformers in symbolic reasoning tasks and introduces a new dataset for premise selection. This is relevant for strengthening the methodology and related work sections, particularly in discussing benchmarks and transformer-based methods for symbolic reasoning.
@article{mikua2023magnushammerat,
 author = {Maciej Mikuła and Szymon Antoniak and Szymon Tworkowski and Albert Qiaochu Jiang and Jinyi Zhou and Christian Szegedy and Lukasz Kuci'nski and Piotr Milo's and Yuhuai Wu},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Magnushammer: A Transformer-based Approach to Premise Selection},
 volume = {abs/2303.04488},
 year = {2023}
}

% The paper 'AttentionDrop: A Novel Regularization Method for Transformer Models' introduces stochastic regularization techniques to mitigate overfitting in transformer models, which aligns well with the identified risk factors in the proposed study. It should be cited in the methodology section to strengthen the discussion on techniques employed to prevent overfitting in transformer-based symbolic reasoning models.
@article{baig2025attentiondropan,
 author = {Mirza Samad Ahmed Baig and Syeda Anshrah Gillani and Abdul Akbar Khan and Shahid Munir Shah},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {AttentionDrop: A Novel Regularization Method for Transformer Models},
 volume = {abs/2504.12088},
 year = {2025}
}

% The paper 'AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs' discusses how transformer models can be adapted for symbolic reasoning tasks, demonstrating strong generalization capabilities by surpassing their synthetic data generator in accuracy and efficiency. This citation should be included in the related work section to support the claim about the potential of transformers to generalize effectively in symbolic reasoning contexts when properly adapted.
@article{unsal2024alphaintegratorta,
 author = {Mert Ünsal and T. Gehr and Martin T. Vechev},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs},
 volume = {abs/2410.02666},
 year = {2024}
}
