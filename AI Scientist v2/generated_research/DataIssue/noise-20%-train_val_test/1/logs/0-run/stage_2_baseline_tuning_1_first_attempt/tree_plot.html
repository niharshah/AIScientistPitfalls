<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 7], [0, 8], [0, 5], [0, 4], [0, 6], [0, 3], [0, 2], [0, 1], [4, 12], [4, 9], [4, 11], [4, 10]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.14285714285714285, 0.5], [0.2857142857142857, 0.5], [0.42857142857142855, 0.5], [0.5714285714285714, 0.5], [0.7142857142857143, 0.5], [0.8571428571428571, 0.5], [1.0, 0.5], [0.21428571428571427, 1.0], [0.35714285714285715, 1.0], [0.5, 1.0], [0.6428571428571429, 1.0]], "plan": ["A robust fix is to make the script automatically discover the SPR_BENCH folder\ninstead of assuming it lies inside the experiment working directory.   We try,\nin order, (i) explicit environment variables, (ii) common relative paths, and\n(iii) the absolute path shown in the prompt; the first candidate that contains\nthe three CSV files is taken.   Because positional-embedding length was hard-\ncoded to 500, the model may crash on longer sequences, so we compute the maximum\ntraining length and size the positional matrix accordingly.   Finally, we ensure\nthat tensors are placed on the correct device inside the training / evaluation\nloops (the collate-fn now leaves them on CPU).", "Hyperparam tuning name: epochs. Below we explore extended training time as a\nsingle hyper-parameter (\u201cepochs\u201d).   For each budget in [10, 20, 30, 50] we\ntrain a fresh transformer, apply early-stopping (patience = 3), keep the best\ncheckpoint and log all metrics / predictions into experiment_data, finally saved\nas experiment_data.npy.", "Hyperparam tuning name: learning_rate. We will perform a small grid-search over\nfive candidate learning-rates.   For every lr we (1) build a fresh\nmodel/optimizer, (2) train for 10 epochs, (3) track epoch-wise\ntraining/validation losses & accuracies, and (4) evaluate on the held-out test\nsplit, saving predictions and ground-truth.   Results of every run are appended\ninto experiment_data['learning_rate']['SPR_BENCH']['runs']; the whole structure\nis finally saved to working/experiment_data.npy so it can be plotted later.", "Hyperparam tuning name: batch_size. The solution sweeps three training mini-\nbatch sizes (32, 128 and 256).   For each value it rebuilds a fresh model,\ntrains for the same number of epochs, evaluates on the dev/test splits, and\nstores per-epoch losses/accuracies plus final test predictions in a nested\nexperiment_data dictionary under the key \"batch_size\".  All results are saved to\n`experiment_data.npy` for later plotting or analysis.", "Hyperparam tuning name: dropout. The solution runs a separate training-\nevaluation cycle for each candidate dropout value (0.0, 0.1, 0.2, 0.3).   For\nevery setting it rebuilds the model, trains for a fixed number of epochs,\nrecords train/validation losses and accuracies, evaluates on test, and stores\npredictions plus ground-truth in a structured experiment_data dictionary.   All\ndata are finally saved to working/experiment_data.npy.   The code below is\ncompletely self-contained and executable.", "Hyperparam tuning name: weight_decay. We sweep weight _decay over {0, 1e-5,\n1e-4, 1e-3}.   For every value a fresh classifier is trained for 10 epochs, and\nper-epoch training/validation loss and accuracy are recorded.   After training,\nthe model is evaluated on the test split and its predictions are stored together\nwith the ground-truth labels.   All results are organised in an experiment_data\ndictionary under the key \u201cweight_decay\u201d and finally persisted to\nexperiment_data.npy.", "Hyperparam tuning name: d_model (Transformer hidden/embedding size). We reuse\nthe existing data-loading and training utilities, but wrap the whole training\nroutine inside a loop that sweeps through four values of d_model (32, 64, 128,\n256).   For every setting we re-instantiate the model, train for the same 10\nepochs, evaluate on the dev and test splits, log epoch-wise metrics/losses as\nwell as final test predictions/ground-truth, and then free GPU memory before\nmoving to the next size.   All results are stored in a nested NumPy-serialisable\ndictionary experiment_data['d_model_tuning']['SPR_BENCH'][f'd_model_{size}'] =\n{...} and saved to working/experiment_data.npy at the end.", "Hyperparam tuning name: num_layers. We keep the original data-loading, model,\nand training logic, but wrap the whole training/evaluation cycle in a loop that\ninstantiates a new SimpleTransformerClassifier for each desired depth.  The list\ndepths = [1, 2, 3, 4, 6] is explored; for every depth we record per-epoch\ntrain/validation losses and accuracies, plus final test predictions and labels.\nResults are stored in the mandated experiment_data structure under the top-level\nkey \"num_layers\", then persisted to experiment_data.npy.  No other hyper-\nparameters are modified, so the influence of encoder depth can be isolated.  The\nscript is entirely self-contained and executable.", "Hyperparam tuning name: nhead. The script below loops over nhead \u2208 {2, 4, 8},\ntrains a fresh Transformer for 10 epochs each, evaluates on train/dev/test,\nkeeps per-configuration losses, accuracies, predictions and ground-truth, and\nfinally stores everything in the required experiment_data.npy file. The best-dev\nmodel is also identified. All logic from the baseline is preserved; only the\nhyper-parameter sweep and result bookkeeping were added.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, time, numpy as np, torch, math\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# directories / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH  -----------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    \"\"\"Return a Path that contains train.csv/dev.csv/test.csv for SPR_BENCH.\"\"\"\n    # candidate roots (ordered)\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\n            \"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"\n        ).resolve(),  # prompt default\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset loading helper (from prompt) ----------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\n# -----------------------------------------------------------------------------#\n# PyTorch dataset -------------------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = set()\n    for ex in train_split:\n        chars.update(ex[\"sequence\"].replace(\" \", \"\"))\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        pos = self.pos_embed[:seq_len, :].unsqueeze(0)  # 1,S,E\n        x = self.embed(input_ids) + pos  # B,S,E\n        x = x.transpose(0, 1)  # S,B,E\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)  # B,S,E\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            # move to device\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# experiment data structure ---------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": []},\n        \"losses\": {\"train_loss\": [], \"val_loss\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# main pipeline (auto-exec) ---------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\n\n# determine max sequence length\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\n# datasets / loaders\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\nmodel = SimpleTransformerClassifier(len(vocab), num_labels, max_len=max_len).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# training loop\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n    v_loss, v_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n    print(\n        f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n    )\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train_loss\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val_loss\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n\n# final test evaluation\ntest_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\nprint(f\"Test accuracy: {test_acc*100:.2f}%\")\n\n# store predictions / ground truth\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds = logits.argmax(1).cpu().numpy()\n        experiment_data[\"SPR_BENCH\"][\"predictions\"].extend(preds)\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"].extend(\n            batch[\"labels\"].cpu().numpy()\n        )\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, time, numpy as np, torch, math, copy, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# reproducibility (optional, comment out if not wanted)\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic, torch.backends.cudnn.benchmark = True, False\n\n# -----------------------------------------------------------------------------#\n# experiment log container ----------------------------------------------------#\nexperiment_data = {\"epochs\": {}}  # hyperparameter type we are tuning\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\"Could not locate SPR_BENCH. Set SPR_DATA/SPR_DATASET_PATH\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = set()\n    for ex in train_split:\n        chars.update(ex[\"sequence\"].replace(\" \", \"\"))\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        pos = self.pos_embed[:seq_len].unsqueeze(0)  # 1,S,E\n        x = self.embed(input_ids) + pos  # B,S,E\n        x = x.transpose(0, 1)  # S,B,E\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)  # B,S,E\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# data preparation (only once) ------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nvocab = build_vocab(spr[\"train\"])\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\n# -----------------------------------------------------------------------------#\n# hyper-parameter sweep: epochs -----------------------------------------------#\nepoch_budgets = [10, 20, 30, 50]\npatience = 3  # early stopping patience\n\nfor budget in epoch_budgets:\n    tag = f\"epochs_{budget}\"\n    print(f\"\\n=== Training with budget {budget} epochs ===\")\n    experiment_data[\"epochs\"][tag] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # fresh model / optimizer\n    model = SimpleTransformerClassifier(len(vocab), num_labels, max_len=max_len).to(\n        device\n    )\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    best_state, best_val_loss, wait = None, float(\"inf\"), 0\n\n    for ep in range(1, budget + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n\n        experiment_data[\"epochs\"][tag][\"metrics\"][\"train\"].append(t_acc)\n        experiment_data[\"epochs\"][tag][\"metrics\"][\"val\"].append(v_acc)\n        experiment_data[\"epochs\"][tag][\"losses\"][\"train\"].append(t_loss)\n        experiment_data[\"epochs\"][tag][\"losses\"][\"val\"].append(v_loss)\n\n        print(\n            f\"[{tag}] Epoch {ep:02d}/{budget}  \"\n            f\"train_loss={t_loss:.4f} val_loss={v_loss:.4f} \"\n            f\"val_acc={v_acc*100:.2f}%\"\n        )\n\n        # early stopping bookkeeping\n        if v_loss < best_val_loss - 1e-4:  # small delta\n            best_val_loss = v_loss\n            best_state = copy.deepcopy(model.state_dict())\n            wait = 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(f\"[{tag}] Early stopping triggered at epoch {ep}\")\n                break\n\n    # restore best checkpoint\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # final test evaluation & predictions\n    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n    print(f\"[{tag}] Test accuracy: {test_acc*100:.2f}% (loss {test_loss:.4f})\")\n\n    # store predictions / ground truth\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds = logits.argmax(1).cpu().numpy()\n            experiment_data[\"epochs\"][tag][\"predictions\"].extend(preds)\n            experiment_data[\"epochs\"][tag][\"ground_truth\"].extend(\n                batch[\"labels\"].cpu().numpy()\n            )\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nExperiment data saved to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os, pathlib, time, random, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# reproducibility -------------------------------------------------------------#\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# -----------------------------------------------------------------------------#\n# directories / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH \u2013 set env var SPR_DATA or SPR_DATASET_PATH.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset loading helper ------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\n# -----------------------------------------------------------------------------#\n# PyTorch dataset -------------------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _enc(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._enc(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = set()\n    for ex in train_split:\n        chars.update(ex[\"sequence\"].replace(\" \", \"\"))\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attn_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len, :].unsqueeze(0)\n        x = x.transpose(0, 1)  # S,B,E\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)  # B,S,E\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    ep_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(out, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            ep_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = out.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return ep_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# hyperparameter sweep setup --------------------------------------------------#\nlr_grid = [1e-3, 5e-4, 2e-4, 1e-4, 5e-5]\nepochs = 10\nbatch_train = 64\nbatch_eval = 128\n\nexperiment_data = {\"learning_rate\": {\"SPR_BENCH\": {\"runs\": []}}}\n\n# -----------------------------------------------------------------------------#\n# data ------------------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=batch_train,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=batch_eval,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=batch_eval,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\ncriterion = nn.CrossEntropyLoss()\n\n# -----------------------------------------------------------------------------#\n# sweep -----------------------------------------------------------------------#\nfor lr in lr_grid:\n    print(f\"\\n===== Training with learning_rate = {lr:.1e} =====\")\n    model = SimpleTransformerClassifier(len(vocab), num_labels, max_len=max_len).to(\n        device\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    run_data = {\n        \"lr\": lr,\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"test_acc\": None,\n    }\n\n    best_val_acc = 0.0\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n\n        run_data[\"metrics\"][\"train\"].append(t_acc)\n        run_data[\"metrics\"][\"val\"].append(v_acc)\n        run_data[\"losses\"][\"train\"].append(t_loss)\n        run_data[\"losses\"][\"val\"].append(v_loss)\n\n        if v_acc > best_val_acc:\n            best_val_acc = v_acc\n        print(\n            f\"Epoch {epoch:02d}: train_loss={t_loss:.4f}  val_loss={v_loss:.4f}  val_acc={v_acc*100:.2f}%\"\n        )\n\n    # final test evaluation ----------------------------------------------------#\n    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n    run_data[\"test_acc\"] = test_acc\n    print(f\"Test accuracy (lr {lr:.1e}): {test_acc*100:.2f}%\")\n\n    # store predictions --------------------------------------------------------#\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds = logits.argmax(1).cpu().numpy()\n            run_data[\"predictions\"].extend(preds)\n            run_data[\"ground_truth\"].extend(batch[\"labels\"].cpu().numpy())\n\n    # add run to experiment_data ----------------------------------------------#\n    experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"runs\"].append(run_data)\n\n# -----------------------------------------------------------------------------#\n# save experiment data --------------------------------------------------------#\nnp.save(\n    os.path.join(working_dir, \"experiment_data.npy\"), experiment_data, allow_pickle=True\n)\nprint(f\"Results saved to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os, pathlib, time, random, math, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# -----------------------------------------------------------------------------#\n# directories / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH.\"\n    )\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data, self.vocab, self.pad_id = hf_dataset, vocab, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _enc(self, seq):\n        return torch.tensor(\n            [self.vocab[ch] for ch in seq.replace(\" \", \"\")], dtype=torch.long\n        )\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._enc(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, ids, attn_mask):\n        pos = self.pos_embed[: ids.size(1)].unsqueeze(0)\n        x = self.embed(ids) + pos  # B,S,E\n        x = x.transpose(0, 1)  # S,B,E\n        x = self.encoder(x, src_key_padding_mask=~attn_mask.bool())\n        x = x.transpose(0, 1)  # B,S,E\n        pooled = (x * attn_mask.unsqueeze(-1)).sum(1) / attn_mask.sum(1, keepdim=True)\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return total_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# load data -------------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded splits:\", list(spr.keys()))\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\n\n# -----------------------------------------------------------------------------#\n# hyper-parameter tuning: batch_size ------------------------------------------#\nbatch_sizes = [32, 128, 256]\nepochs = 10\nexperiment_data = {\"batch_size\": {}}\n\nfor bs in batch_sizes:\n    print(f\"\\n===== Training with batch size {bs} =====\")\n    exp_key = f\"bs_{bs}\"\n    experiment_data[\"batch_size\"][exp_key] = {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": []},\n        \"losses\": {\"train_loss\": [], \"val_loss\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=bs,\n        shuffle=True,\n        collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n    )\n    model = SimpleTransformerClassifier(\n        len(vocab), len(set(int(x[\"label\"]) for x in spr[\"train\"])), max_len=max_len\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for ep in range(1, epochs + 1):\n        tr_loss, tr_acc = run_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_acc = run_epoch(model, dev_loader, criterion)\n        print(\n            f\"Epoch {ep:2d}/{epochs} | bs {bs:3d} | \"\n            f\"train_loss {tr_loss:.4f} train_acc {tr_acc:.3f} | \"\n            f\"val_loss {val_loss:.4f} val_acc {val_acc:.3f}\"\n        )\n        ed = experiment_data[\"batch_size\"][exp_key]\n        ed[\"losses\"][\"train_loss\"].append(tr_loss)\n        ed[\"losses\"][\"val_loss\"].append(val_loss)\n        ed[\"metrics\"][\"train_acc\"].append(tr_acc)\n        ed[\"metrics\"][\"val_acc\"].append(val_acc)\n\n    # final test evaluation + predictions\n    tst_loss, tst_acc = run_epoch(model, test_loader, criterion)\n    print(f\"Batch size {bs} | Test accuracy: {tst_acc*100:.2f}%\")\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds = logits.argmax(1).cpu().numpy()\n            experiment_data[\"batch_size\"][exp_key][\"predictions\"].extend(preds)\n            experiment_data[\"batch_size\"][exp_key][\"ground_truth\"].extend(\n                batch[\"labels\"].cpu().numpy()\n            )\n\n# -----------------------------------------------------------------------------#\n# save results ----------------------------------------------------------------#\nsave_path = os.path.join(working_dir, \"experiment_data.npy\")\nnp.save(save_path, experiment_data)\nprint(f\"\\nAll experiment data saved to {save_path}\")\n", "import os, pathlib, numpy as np, torch, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"dropout_tuning\": {\n        # each dataset will be inserted here\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = x.transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ndropout_rates = [0.0, 0.1, 0.2, 0.3]\nepochs = 10\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data[\"dropout_tuning\"][\"SPR_BENCH\"] = {}\n\nfor dp in dropout_rates:\n    print(f\"\\n=== Training with dropout={dp} ===\")\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, dropout=dp\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion)\n        metrics[\"train_acc\"].append(t_acc)\n        metrics[\"val_acc\"].append(v_acc)\n        losses[\"train_loss\"].append(t_loss)\n        losses[\"val_loss\"].append(v_loss)\n        print(\n            f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n    test_loss, test_acc = run_epoch(model, test_loader, criterion)\n    print(f\"Dropout {dp}: Test accuracy {test_acc*100:.2f}%\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_all.extend(logits.argmax(1).cpu().numpy())\n            gts_all.extend(batch[\"labels\"].cpu().numpy())\n\n    # store results\n    experiment_data[\"dropout_tuning\"][\"SPR_BENCH\"][str(dp)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n        \"test_acc\": test_acc,\n        \"test_loss\": test_loss,\n    }\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, time, numpy as np, torch, math\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# directories / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset loading helper\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\n# -----------------------------------------------------------------------------#\n# PyTorch dataset\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data, self.vocab, self.pad_id = hf_dataset, vocab, vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        return torch.tensor(\n            [self.vocab[ch] for ch in seq.replace(\" \", \"\")], dtype=torch.long\n        )\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attn_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        pos = self.pos_embed[: input_ids.size(1)].unsqueeze(0)\n        x = (self.embed(input_ids) + pos).transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool()).transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(logits, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# experiment data container\nexperiment_data = {\n    \"weight_decay\": {\n        \"SPR_BENCH\": {\n            \"settings\": [],  # weight_decay values\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"test_acc\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# pipeline\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\n# datasets / loaders\ntrain_ds, dev_ds, test_ds = (\n    SPRCharDataset(spr[s], vocab) for s in (\"train\", \"dev\", \"test\")\n)\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\ncriterion = nn.CrossEntropyLoss()\nweight_decays = [0.0, 1e-5, 1e-4, 1e-3]\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\nepochs = 10\n\n# collect ground truth once\nfor batch in test_loader:\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"ground_truth\"].extend(\n        batch[\"labels\"].numpy()\n    )\n# -----------------------------------------------------------------------------#\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay={wd} ===\")\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"settings\"].append(wd)\n    model = SimpleTransformerClassifier(len(vocab), num_labels, max_len=max_len).to(\n        device\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion)\n        train_losses.append(t_loss)\n        val_losses.append(v_loss)\n        train_accs.append(t_acc)\n        val_accs.append(v_acc)\n        print(\n            f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, \"\n            f\"val_acc={v_acc*100:.2f}%\"\n        )\n\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(train_losses)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_losses)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"metrics\"][\"train\"].append(train_accs)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"metrics\"][\"val\"].append(val_accs)\n\n    # final test evaluation\n    test_loss, test_acc = run_epoch(model, test_loader, criterion)\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"test_acc\"].append(test_acc)\n    print(f\"Test accuracy (weight_decay={wd}): {test_acc*100:.2f}%\")\n\n    # store predictions\n    model.eval()\n    preds_list = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_list.extend(logits.argmax(1).cpu().numpy())\n    experiment_data[\"weight_decay\"][\"SPR_BENCH\"][\"predictions\"].append(preds_list)\n\n# -----------------------------------------------------------------------------#\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, time, numpy as np, torch, math, gc\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"d_model_tuning\": {\n        \"SPR_BENCH\": {\n            # will be filled with one sub-dict per d_model setting\n        }\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    req = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and req.issubset({p.name for p in c.iterdir()}):\n            print(\"Found SPR_BENCH at:\", c)\n            return c\n    raise FileNotFoundError(\"SPR_BENCH dataset not found. Set env variable.\")\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # always use split=\"train\" to load the whole file\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\n# -----------------------------------------------------------------------------#\n# PyTorch dataset -------------------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data, self.vocab = hf_dataset, vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _enc(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._enc(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len, :].unsqueeze(0)\n        x = x.transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# training / eval loops -------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(out, batch[\"labels\"])\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item() * batch[\"labels\"].size(0)\n            correct += (out.argmax(1) == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return total_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# prepare data loaders (done once) --------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded splits:\", list(spr.keys()))\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(\"Max seq len:\", max_len)\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\n\n# -----------------------------------------------------------------------------#\n# d_model hyper-parameter sweep -----------------------------------------------#\nd_model_values = [32, 64, 128, 256]\nepochs = 10\nfor dm in d_model_values:\n    print(f\"\\n=== Training with d_model={dm} ===\")\n    tag = f\"d_model_{dm}\"\n    experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    model = SimpleTransformerClassifier(len(vocab), num_labels, max_len, d_model=dm).to(\n        device\n    )\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n        print(\n            f\"Ep{epoch:02d} - train_loss:{t_loss:.4f} val_loss:{v_loss:.4f} val_acc:{v_acc*100:.2f}%\"\n        )\n        experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag][\"losses\"][\"train\"].append(\n            t_loss\n        )\n        experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag][\"losses\"][\"val\"].append(\n            v_loss\n        )\n        experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag][\"metrics\"][\"train\"].append(\n            t_acc\n        )\n        experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag][\"metrics\"][\"val\"].append(\n            v_acc\n        )\n\n    # final test\n    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n    print(f\"Test acc for d_model={dm}: {test_acc*100:.2f}%\")\n\n    # store predictions / ground truth\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds = logits.argmax(1).cpu().numpy()\n            experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag][\"predictions\"].extend(\n                preds\n            )\n            experiment_data[\"d_model_tuning\"][\"SPR_BENCH\"][tag][\"ground_truth\"].extend(\n                batch[\"labels\"].cpu().numpy()\n            )\n\n    # free memory before next run\n    del model, optimizer, criterion, logits, preds\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# -----------------------------------------------------------------------------#\n# save results ----------------------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f\"All experiment data saved to {os.path.join(working_dir, 'experiment_data.npy')}\"\n)\n", "import os, pathlib, time, numpy as np, torch, math, gc\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------------------------------------------------#\n# experiment data skeleton\nexperiment_data = {\"num_layers\": {}}  # will fill with layer-specific results\n\n# --------------------------------------------------------------#\n# directories / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# --------------------------------------------------------------#\n# locate SPR_BENCH\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# --------------------------------------------------------------#\n# dataset helper\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\n# --------------------------------------------------------------#\n# torch dataset\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = set()\n    for ex in train_split:\n        chars.update(ex[\"sequence\"].replace(\" \", \"\"))\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# --------------------------------------------------------------#\n# model\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        pos = self.pos_embed[:seq_len, :].unsqueeze(0)  # 1,S,E\n        x = self.embed(input_ids) + pos  # B,S,E\n        x = x.transpose(0, 1)  # S,B,E\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)  # B,S,E\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# --------------------------------------------------------------#\n# train / eval functions\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# --------------------------------------------------------------#\n# data loading (shared across all runs)\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\n# --------------------------------------------------------------#\n# hyperparameter tuning: number of encoder layers\ndepth_values = [1, 2, 3, 4, 6]\nepochs = 10\ncriterion = nn.CrossEntropyLoss()\n\nfor depth in depth_values:\n    print(f\"\\n\\n=== Training with num_layers = {depth} ===\")\n    layer_key = f\"layers_{depth}\"\n    # allocate experiment storage\n    experiment_data[\"num_layers\"].setdefault(\"SPR_BENCH\", {})\n    experiment_data[\"num_layers\"][\"SPR_BENCH\"][layer_key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # build model\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, num_layers=depth\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n        print(\n            f\"[L{depth}] Epoch {epoch}: train_loss={t_loss:.4f}, \"\n            f\"val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n        ed = experiment_data[\"num_layers\"][\"SPR_BENCH\"][layer_key]\n        ed[\"losses\"][\"train\"].append(t_loss)\n        ed[\"losses\"][\"val\"].append(v_loss)\n        ed[\"metrics\"][\"train\"].append(t_acc)\n        ed[\"metrics\"][\"val\"].append(v_acc)\n\n    # test evaluation\n    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n    print(f\"[L{depth}] Test accuracy: {test_acc*100:.2f}%\")\n\n    # store predictions / ground truth\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds = logits.argmax(1).cpu().numpy()\n            experiment_data[\"num_layers\"][\"SPR_BENCH\"][layer_key][\"predictions\"].extend(\n                preds\n            )\n            experiment_data[\"num_layers\"][\"SPR_BENCH\"][layer_key][\n                \"ground_truth\"\n            ].extend(batch[\"labels\"].cpu().numpy())\n\n    # free gpu memory before next depth\n    del model, optimizer, logits\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# --------------------------------------------------------------#\n# Save all collected data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\n    f'\\nAll experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}'\n)\n", "import os, pathlib, time, numpy as np, torch, math, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# directories / device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH  -----------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH. Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset loading helper ------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in (\"train\", \"dev\", \"test\"):\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\n# -----------------------------------------------------------------------------#\n# PyTorch dataset -------------------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = set()\n    for ex in train_split:\n        chars.update(ex[\"sequence\"].replace(\" \", \"\"))\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        pos = self.pos_embed[:seq_len, :].unsqueeze(0)  # 1,S,E\n        x = self.embed(input_ids) + pos  # B,S,E\n        x = x.transpose(0, 1)  # S,B,E\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)  # B,S,E\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\n# fix random seeds for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\n# -----------------------------------------------------------------------------#\n# hyperparameter tuning: nhead -------------------------------------------------#\nnhead_values = [2, 4, 8]\nepochs = 10\nexperiment_data = {\n    \"nhead_tuning\": {\n        \"SPR_BENCH\": {\n            \"configs\": [],\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"test_acc\": []},\n            \"losses\": {\"train_loss\": [], \"val_loss\": [], \"test_loss\": []},\n            \"predictions\": [],  # list of np arrays per config\n            \"ground_truth\": [],  # filled once\n        }\n    }\n}\n\nbest_val_acc, best_idx = -1, -1\n\nfor idx, nh in enumerate(nhead_values):\n    print(f\"\\n### Starting run {idx+1}/{len(nhead_values)} with nhead={nh} ###\")\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, nhead=nh\n    ).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n        print(\n            f\"[nhead={nh}] Epoch {epoch:02d}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n    # after training evaluate on all splits\n    tr_loss, tr_acc = run_epoch(model, train_loader, criterion, optimizer=None)\n    val_loss, val_acc = run_epoch(model, dev_loader, criterion, optimizer=None)\n    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n\n    # store metrics\n    ed = experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"]\n    ed[\"configs\"].append({\"nhead\": nh})\n    ed[\"metrics\"][\"train_acc\"].append(tr_acc)\n    ed[\"metrics\"][\"val_acc\"].append(val_acc)\n    ed[\"metrics\"][\"test_acc\"].append(test_acc)\n    ed[\"losses\"][\"train_loss\"].append(tr_loss)\n    ed[\"losses\"][\"val_loss\"].append(val_loss)\n    ed[\"losses\"][\"test_loss\"].append(test_loss)\n\n    # store predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_all.extend(logits.argmax(1).cpu().numpy())\n            gts_all.extend(batch[\"labels\"].cpu().numpy())\n    ed[\"predictions\"].append(np.array(preds_all))\n    if len(ed[\"ground_truth\"]) == 0:\n        ed[\"ground_truth\"] = np.array(gts_all)\n\n    if val_acc > best_val_acc:\n        best_val_acc, best_idx = val_acc, idx\n\nprint(\n    \"\\nValidation accuracies per nhead:\",\n    experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][\"metrics\"][\"val_acc\"],\n)\nprint(f\"Best nhead={nhead_values[best_idx]} with val_acc={best_val_acc*100:.2f}%\")\nprint(\n    f\"Corresponding test accuracy: {experiment_data['nhead_tuning']['SPR_BENCH']['metrics']['test_acc'][best_idx]*100:.2f}%\"\n)\n\n# -----------------------------------------------------------------------------#\n# save results ----------------------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Experiment data saved to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"dropout_tuning\": {\n        # each dataset will be inserted here\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = x.transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ndropout_rates = [0.0, 0.1, 0.2, 0.3]\nepochs = 10\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data[\"dropout_tuning\"][\"SPR_BENCH\"] = {}\n\nfor dp in dropout_rates:\n    print(f\"\\n=== Training with dropout={dp} ===\")\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, dropout=dp\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion)\n        metrics[\"train_acc\"].append(t_acc)\n        metrics[\"val_acc\"].append(v_acc)\n        losses[\"train_loss\"].append(t_loss)\n        losses[\"val_loss\"].append(v_loss)\n        print(\n            f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n    test_loss, test_acc = run_epoch(model, test_loader, criterion)\n    print(f\"Dropout {dp}: Test accuracy {test_acc*100:.2f}%\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_all.extend(logits.argmax(1).cpu().numpy())\n            gts_all.extend(batch[\"labels\"].cpu().numpy())\n\n    # store results\n    experiment_data[\"dropout_tuning\"][\"SPR_BENCH\"][str(dp)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n        \"test_acc\": test_acc,\n        \"test_loss\": test_loss,\n    }\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"dropout_tuning\": {\n        # each dataset will be inserted here\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = x.transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ndropout_rates = [0.0, 0.1, 0.2, 0.3]\nepochs = 10\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data[\"dropout_tuning\"][\"SPR_BENCH\"] = {}\n\nfor dp in dropout_rates:\n    print(f\"\\n=== Training with dropout={dp} ===\")\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, dropout=dp\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion)\n        metrics[\"train_acc\"].append(t_acc)\n        metrics[\"val_acc\"].append(v_acc)\n        losses[\"train_loss\"].append(t_loss)\n        losses[\"val_loss\"].append(v_loss)\n        print(\n            f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n    test_loss, test_acc = run_epoch(model, test_loader, criterion)\n    print(f\"Dropout {dp}: Test accuracy {test_acc*100:.2f}%\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_all.extend(logits.argmax(1).cpu().numpy())\n            gts_all.extend(batch[\"labels\"].cpu().numpy())\n\n    # store results\n    experiment_data[\"dropout_tuning\"][\"SPR_BENCH\"][str(dp)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n        \"test_acc\": test_acc,\n        \"test_loss\": test_loss,\n    }\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"dropout_tuning\": {\n        # each dataset will be inserted here\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = x.transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ndropout_rates = [0.0, 0.1, 0.2, 0.3]\nepochs = 10\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data[\"dropout_tuning\"][\"SPR_BENCH\"] = {}\n\nfor dp in dropout_rates:\n    print(f\"\\n=== Training with dropout={dp} ===\")\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, dropout=dp\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion)\n        metrics[\"train_acc\"].append(t_acc)\n        metrics[\"val_acc\"].append(v_acc)\n        losses[\"train_loss\"].append(t_loss)\n        losses[\"val_loss\"].append(v_loss)\n        print(\n            f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n    test_loss, test_acc = run_epoch(model, test_loader, criterion)\n    print(f\"Dropout {dp}: Test accuracy {test_acc*100:.2f}%\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_all.extend(logits.argmax(1).cpu().numpy())\n            gts_all.extend(batch[\"labels\"].cpu().numpy())\n\n    # store results\n    experiment_data[\"dropout_tuning\"][\"SPR_BENCH\"][str(dp)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n        \"test_acc\": test_acc,\n        \"test_loss\": test_loss,\n    }\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 62825.19\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 104009.92\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 190719.53\nexamples/s]', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training set: 64', '\\n',\n'Epoch 1: train_loss=0.6265, val_loss=0.5521, val_acc=76.60%', '\\n', 'Epoch 2:\ntrain_loss=0.5297, val_loss=0.5319, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5281, val_loss=0.5277, val_acc=78.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5136, val_loss=0.5144, val_acc=79.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5126, val_loss=0.5237, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5086, val_loss=0.5349, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5150, val_loss=0.5230, val_acc=79.20%', '\\n', 'Epoch 8:\ntrain_loss=0.5123, val_loss=0.5179, val_acc=79.00%', '\\n', 'Epoch 9:\ntrain_loss=0.5075, val_loss=0.5365, val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5048, val_loss=0.5156, val_acc=79.60%', '\\n', 'Test accuracy:\n79.50%', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-1/working/experiment_data.npy', '\\n', 'Execution time: 5\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 69979.13\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 108447.20\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 179021.90\nexamples/s]', '\\n', '\\n=== Training with budget 10 epochs ===', '\\n',\n'[epochs_10] Epoch 01/10  train_loss=0.5994 val_loss=0.5946 val_acc=74.60%',\n'\\n', '[epochs_10] Epoch 02/10  train_loss=0.5439 val_loss=0.5421\nval_acc=78.00%', '\\n', '[epochs_10] Epoch 03/10  train_loss=0.5311\nval_loss=0.5334 val_acc=78.60%', '\\n', '[epochs_10] Epoch 04/10\ntrain_loss=0.5231 val_loss=0.5301 val_acc=78.60%', '\\n', '[epochs_10] Epoch\n05/10  train_loss=0.5184 val_loss=0.5331 val_acc=78.60%', '\\n', '[epochs_10]\nEpoch 06/10  train_loss=0.5197 val_loss=0.5311 val_acc=79.60%', '\\n',\n'[epochs_10] Epoch 07/10  train_loss=0.5182 val_loss=0.5246 val_acc=79.20%',\n'\\n', '[epochs_10] Epoch 08/10  train_loss=0.5064 val_loss=0.5323\nval_acc=78.60%', '\\n', '[epochs_10] Epoch 09/10  train_loss=0.5037\nval_loss=0.5202 val_acc=79.00%', '\\n', '[epochs_10] Epoch 10/10\ntrain_loss=0.5069 val_loss=0.5884 val_acc=66.20%', '\\n', '[epochs_10] Test\naccuracy: 79.30% (loss 0.5123)', '\\n', '\\n=== Training with budget 20 epochs\n===', '\\n', '[epochs_20] Epoch 01/20  train_loss=0.5925 val_loss=0.5845\nval_acc=74.40%', '\\n', '[epochs_20] Epoch 02/20  train_loss=0.5286\nval_loss=0.5303 val_acc=78.60%', '\\n', '[epochs_20] Epoch 03/20\ntrain_loss=0.5167 val_loss=0.5258 val_acc=78.60%', '\\n', '[epochs_20] Epoch\n04/20  train_loss=0.5245 val_loss=0.5318 val_acc=78.20%', '\\n', '[epochs_20]\nEpoch 05/20  train_loss=0.5138 val_loss=0.5146 val_acc=79.60%', '\\n',\n'[epochs_20] Epoch 06/20  train_loss=0.5097 val_loss=0.5183 val_acc=79.60%',\n'\\n', '[epochs_20] Epoch 07/20  train_loss=0.5101 val_loss=0.5204\nval_acc=78.60%', '\\n', '[epochs_20] Epoch 08/20  train_loss=0.5114\nval_loss=0.5339 val_acc=78.60%', '\\n', '[epochs_20] Early stopping triggered at\nepoch 8', '\\n', '[epochs_20] Test accuracy: 79.50% (loss 0.5112)', '\\n', '\\n===\nTraining with budget 30 epochs ===', '\\n', '[epochs_30] Epoch 01/30\ntrain_loss=0.5915 val_loss=0.5808 val_acc=73.20%', '\\n', '[epochs_30] Epoch\n02/30  train_loss=0.5401 val_loss=0.5484 val_acc=77.00%', '\\n', '[epochs_30]\nEpoch 03/30  train_loss=0.5240 val_loss=0.5297 val_acc=78.60%', '\\n',\n'[epochs_30] Epoch 04/30  train_loss=0.5172 val_loss=0.5179 val_acc=79.60%',\n'\\n', '[epochs_30] Epoch 05/30  train_loss=0.5272 val_loss=0.5416\nval_acc=78.40%', '\\n', '[epochs_30] Epoch 06/30  train_loss=0.5271\nval_loss=0.5345 val_acc=79.60%', '\\n', '[epochs_30] Epoch 07/30\ntrain_loss=0.5134 val_loss=0.5142 val_acc=79.60%', '\\n', '[epochs_30] Epoch\n08/30  train_loss=0.5065 val_loss=0.5186 val_acc=79.60%', '\\n', '[epochs_30]\nEpoch 09/30  train_loss=0.5079 val_loss=0.5785 val_acc=69.60%', '\\n',\n'[epochs_30] Epoch 10/30  train_loss=0.5122 val_loss=0.5124 val_acc=79.60%',\n'\\n', '[epochs_30] Epoch 11/30  train_loss=0.5085 val_loss=0.5141\nval_acc=79.20%', '\\n', '[epochs_30] Epoch 12/30  train_loss=0.5090\nval_loss=0.5183 val_acc=79.60%', '\\n', '[epochs_30] Epoch 13/30\ntrain_loss=0.5117 val_loss=0.5138 val_acc=79.60%', '\\n', '[epochs_30] Early\nstopping triggered at epoch 13', '\\n', '[epochs_30] Test accuracy: 79.40% (loss\n0.5130)', '\\n', '\\n=== Training with budget 50 epochs ===', '\\n', '[epochs_50]\nEpoch 01/50  train_loss=0.5808 val_loss=0.5511 val_acc=76.00%', '\\n',\n'[epochs_50] Epoch 02/50  train_loss=0.5273 val_loss=0.5612 val_acc=76.00%',\n'\\n', '[epochs_50] Epoch 03/50  train_loss=0.5365 val_loss=0.5353\nval_acc=79.60%', '\\n', '[epochs_50] Epoch 04/50  train_loss=0.5208\nval_loss=0.5239 val_acc=78.60%', '\\n', '[epochs_50] Epoch 05/50\ntrain_loss=0.5149 val_loss=0.5204 val_acc=79.60%', '\\n', '[epochs_50] Epoch\n06/50  train_loss=0.5121 val_loss=0.5319 val_acc=78.60%', '\\n', '[epochs_50]\nEpoch 07/50  train_loss=0.5108 val_loss=0.5298 val_acc=78.60%', '\\n',\n'[epochs_50] Epoch 08/50  train_loss=0.5087 val_loss=0.5249 val_acc=79.60%',\n'\\n', '[epochs_50] Early stopping triggered at epoch 8', '\\n', '[epochs_50] Test\naccuracy: 79.50% (loss 0.5170)', '\\n', '\\nExperiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 55241.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 108733.97\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 141203.34\nexamples/s]', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size: 9', '\\n', 'Max sequence length: 64', '\\n', '\\n===== Training\nwith learning_rate = 1.0e-03 =====', '\\n', 'Epoch 01: train_loss=0.5994\nval_loss=0.5946  val_acc=74.60%', '\\n', 'Epoch 02: train_loss=0.5439\nval_loss=0.5421  val_acc=78.00%', '\\n', 'Epoch 03: train_loss=0.5311\nval_loss=0.5334  val_acc=78.60%', '\\n', 'Epoch 04: train_loss=0.5231\nval_loss=0.5301  val_acc=78.60%', '\\n', 'Epoch 05: train_loss=0.5184\nval_loss=0.5331  val_acc=78.60%', '\\n', 'Epoch 06: train_loss=0.5197\nval_loss=0.5311  val_acc=79.60%', '\\n', 'Epoch 07: train_loss=0.5182\nval_loss=0.5246  val_acc=79.20%', '\\n', 'Epoch 08: train_loss=0.5064\nval_loss=0.5323  val_acc=78.60%', '\\n', 'Epoch 09: train_loss=0.5037\nval_loss=0.5202  val_acc=79.00%', '\\n', 'Epoch 10: train_loss=0.5069\nval_loss=0.5884  val_acc=66.20%', '\\n', 'Test accuracy (lr 1.0e-03): 68.30%',\n'\\n', '\\n===== Training with learning_rate = 5.0e-04 =====', '\\n', 'Epoch 01:\ntrain_loss=0.6178  val_loss=0.5627  val_acc=74.80%', '\\n', 'Epoch 02:\ntrain_loss=0.5323  val_loss=0.5427  val_acc=76.60%', '\\n', 'Epoch 03:\ntrain_loss=0.5230  val_loss=0.5446  val_acc=76.80%', '\\n', 'Epoch 04:\ntrain_loss=0.5279  val_loss=0.5521  val_acc=76.20%', '\\n', 'Epoch 05:\ntrain_loss=0.5187  val_loss=0.5243  val_acc=78.60%', '\\n', 'Epoch 06:\ntrain_loss=0.5143  val_loss=0.5226  val_acc=79.60%', '\\n', 'Epoch 07:\ntrain_loss=0.5156  val_loss=0.5261  val_acc=78.60%', '\\n', 'Epoch 08:\ntrain_loss=0.5142  val_loss=0.5340  val_acc=78.60%', '\\n', 'Epoch 09:\ntrain_loss=0.5226  val_loss=0.5186  val_acc=79.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5100  val_loss=0.5141  val_acc=79.60%', '\\n', 'Test accuracy (lr\n5.0e-04): 79.50%', '\\n', '\\n===== Training with learning_rate = 2.0e-04 =====',\n'\\n', 'Epoch 01: train_loss=0.6306  val_loss=0.5833  val_acc=75.80%', '\\n',\n'Epoch 02: train_loss=0.5504  val_loss=0.5495  val_acc=78.60%', '\\n', 'Epoch 03:\ntrain_loss=0.5328  val_loss=0.5462  val_acc=77.00%', '\\n', 'Epoch 04:\ntrain_loss=0.5332  val_loss=0.5393  val_acc=78.60%', '\\n', 'Epoch 05:\ntrain_loss=0.5244  val_loss=0.5372  val_acc=78.60%', '\\n', 'Epoch 06:\ntrain_loss=0.5228  val_loss=0.5343  val_acc=78.60%', '\\n', 'Epoch 07:\ntrain_loss=0.5218  val_loss=0.5318  val_acc=78.60%', '\\n', 'Epoch 08:\ntrain_loss=0.5212  val_loss=0.5321  val_acc=79.00%', '\\n', 'Epoch 09:\ntrain_loss=0.5171  val_loss=0.5294  val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5151  val_loss=0.5268  val_acc=78.60%', '\\n', 'Test accuracy (lr\n2.0e-04): 78.90%', '\\n', '\\n===== Training with learning_rate = 1.0e-04 =====',\n'\\n', 'Epoch 01: train_loss=0.6691  val_loss=0.6457  val_acc=69.80%', '\\n',\n'Epoch 02: train_loss=0.6284  val_loss=0.6040  val_acc=77.00%', '\\n', 'Epoch 03:\ntrain_loss=0.5808  val_loss=0.5651  val_acc=75.60%', '\\n', 'Epoch 04:\ntrain_loss=0.5420  val_loss=0.5433  val_acc=77.80%', '\\n', 'Epoch 05:\ntrain_loss=0.5312  val_loss=0.5416  val_acc=78.40%', '\\n', 'Epoch 06:\ntrain_loss=0.5283  val_loss=0.5398  val_acc=78.60%', '\\n', 'Epoch 07:\ntrain_loss=0.5298  val_loss=0.5389  val_acc=78.60%', '\\n', 'Epoch 08:\ntrain_loss=0.5263  val_loss=0.5401  val_acc=78.20%', '\\n', 'Epoch 09:\ntrain_loss=0.5235  val_loss=0.5389  val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5233  val_loss=0.5401  val_acc=78.20%', '\\n', 'Test accuracy (lr\n1.0e-04): 78.60%', '\\n', '\\n===== Training with learning_rate = 5.0e-05 =====',\n'\\n', 'Epoch 01: train_loss=0.7163  val_loss=0.6938  val_acc=49.40%', '\\n',\n'Epoch 02: train_loss=0.6825  val_loss=0.6669  val_acc=69.20%', '\\n', 'Epoch 03:\ntrain_loss=0.6535  val_loss=0.6379  val_acc=74.60%', '\\n', 'Epoch 04:\ntrain_loss=0.6177  val_loss=0.5999  val_acc=76.20%', '\\n', 'Epoch 05:\ntrain_loss=0.5780  val_loss=0.5664  val_acc=76.80%', '\\n', 'Epoch 06:\ntrain_loss=0.5453  val_loss=0.5486  val_acc=77.20%', '\\n', 'Epoch 07:\ntrain_loss=0.5326  val_loss=0.5468  val_acc=76.60%', '\\n', 'Epoch 08:\ntrain_loss=0.5270  val_loss=0.5424  val_acc=78.00%', '\\n', 'Epoch 09:\ntrain_loss=0.5255  val_loss=0.5442  val_acc=77.00%', '\\n', 'Epoch 10:\ntrain_loss=0.5239  val_loss=0.5408  val_acc=78.60%', '\\n', 'Test accuracy (lr\n5.0e-05): 78.90%', '\\n', 'Results saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time:\n22 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 108427.58\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 105236.45\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 165815.54\nexamples/s]', '\\n', 'Loaded splits:', ' ', \"['train', 'dev', 'test']\", '\\n',\n'Vocab size:', ' ', '9', '\\n', '\\n===== Training with batch size 32 =====',\n'\\n', 'Epoch  1/10 | bs  32 | train_loss 0.5809 train_acc 0.731 | val_loss\n0.5732 val_acc 0.744', '\\n', 'Epoch  2/10 | bs  32 | train_loss 0.5374 train_acc\n0.783 | val_loss 0.5435 val_acc 0.768', '\\n', 'Epoch  3/10 | bs  32 | train_loss\n0.5243 train_acc 0.790 | val_loss 0.5386 val_acc 0.786', '\\n', 'Epoch  4/10 | bs\n32 | train_loss 0.5130 train_acc 0.796 | val_loss 0.5221 val_acc 0.786', '\\n',\n'Epoch  5/10 | bs  32 | train_loss 0.5099 train_acc 0.794 | val_loss 0.5660\nval_acc 0.778', '\\n', 'Epoch  6/10 | bs  32 | train_loss 0.5217 train_acc 0.789\n| val_loss 0.5474 val_acc 0.778', '\\n', 'Epoch  7/10 | bs  32 | train_loss\n0.5143 train_acc 0.794 | val_loss 0.5311 val_acc 0.796', '\\n', 'Epoch  8/10 | bs\n32 | train_loss 0.5028 train_acc 0.798 | val_loss 0.5416 val_acc 0.780', '\\n',\n'Epoch  9/10 | bs  32 | train_loss 0.5047 train_acc 0.794 | val_loss 0.5134\nval_acc 0.794', '\\n', 'Epoch 10/10 | bs  32 | train_loss 0.5082 train_acc 0.795\n| val_loss 0.5213 val_acc 0.792', '\\n', 'Batch size 32 | Test accuracy: 79.30%',\n'\\n', '\\n===== Training with batch size 128 =====', '\\n', 'Epoch  1/10 | bs 128\n| train_loss 0.6232 train_acc 0.661 | val_loss 0.5562 val_acc 0.786', '\\n',\n'Epoch  2/10 | bs 128 | train_loss 0.5539 train_acc 0.777 | val_loss 0.5900\nval_acc 0.740', '\\n', 'Epoch  3/10 | bs 128 | train_loss 0.5399 train_acc 0.777\n| val_loss 0.5427 val_acc 0.768', '\\n', 'Epoch  4/10 | bs 128 | train_loss\n0.5262 train_acc 0.782 | val_loss 0.5419 val_acc 0.772', '\\n', 'Epoch  5/10 | bs\n128 | train_loss 0.5182 train_acc 0.785 | val_loss 0.5249 val_acc 0.786', '\\n',\n'Epoch  6/10 | bs 128 | train_loss 0.5130 train_acc 0.795 | val_loss 0.5182\nval_acc 0.796', '\\n', 'Epoch  7/10 | bs 128 | train_loss 0.5141 train_acc 0.795\n| val_loss 0.5266 val_acc 0.786', '\\n', 'Epoch  8/10 | bs 128 | train_loss\n0.5129 train_acc 0.796 | val_loss 0.5171 val_acc 0.796', '\\n', 'Epoch  9/10 | bs\n128 | train_loss 0.5069 train_acc 0.797 | val_loss 0.5195 val_acc 0.796', '\\n',\n'Epoch 10/10 | bs 128 | train_loss 0.5087 train_acc 0.795 | val_loss 0.5200\nval_acc 0.796', '\\n', 'Batch size 128 | Test accuracy: 79.50%', '\\n', '\\n=====\nTraining with batch size 256 =====', '\\n', 'Epoch  1/10 | bs 256 | train_loss\n0.6146 train_acc 0.735 | val_loss 0.5703 val_acc 0.740', '\\n', 'Epoch  2/10 | bs\n256 | train_loss 0.5493 train_acc 0.774 | val_loss 0.5633 val_acc 0.766', '\\n',\n'Epoch  3/10 | bs 256 | train_loss 0.5341 train_acc 0.786 | val_loss 0.5623\nval_acc 0.758', '\\n', 'Epoch  4/10 | bs 256 | train_loss 0.5309 train_acc 0.782\n| val_loss 0.5408 val_acc 0.786', '\\n', 'Epoch  5/10 | bs 256 | train_loss\n0.5232 train_acc 0.784 | val_loss 0.5409 val_acc 0.786', '\\n', 'Epoch  6/10 | bs\n256 | train_loss 0.5273 train_acc 0.790 | val_loss 0.5363 val_acc 0.786', '\\n',\n'Epoch  7/10 | bs 256 | train_loss 0.5263 train_acc 0.789 | val_loss 0.5386\nval_acc 0.786', '\\n', 'Epoch  8/10 | bs 256 | train_loss 0.5200 train_acc 0.793\n| val_loss 0.5330 val_acc 0.786', '\\n', 'Epoch  9/10 | bs 256 | train_loss\n0.5140 train_acc 0.793 | val_loss 0.5301 val_acc 0.786', '\\n', 'Epoch 10/10 | bs\n256 | train_loss 0.5117 train_acc 0.795 | val_loss 0.5299 val_acc 0.786', '\\n',\n'Batch size 256 | Test accuracy: 78.90%', '\\n', '\\nAll experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 11 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 109114.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 86997.10\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 137324.56\nexamples/s]', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training set: 64', '\\n',\n'\\n=== Training with dropout=0.0 ===', '\\n', 'Epoch 1: train_loss=0.6262,\nval_loss=0.5524, val_acc=76.80%', '\\n', 'Epoch 2: train_loss=0.5296,\nval_loss=0.5317, val_acc=78.60%', '\\n', 'Epoch 3: train_loss=0.5282,\nval_loss=0.5290, val_acc=78.60%', '\\n', 'Epoch 4: train_loss=0.5126,\nval_loss=0.5147, val_acc=79.60%', '\\n', 'Epoch 5: train_loss=0.5137,\nval_loss=0.5243, val_acc=78.60%', '\\n', 'Epoch 6: train_loss=0.5083,\nval_loss=0.5389, val_acc=78.60%', '\\n', 'Epoch 7: train_loss=0.5151,\nval_loss=0.5245, val_acc=79.20%', '\\n', 'Epoch 8: train_loss=0.5114,\nval_loss=0.5192, val_acc=79.20%', '\\n', 'Epoch 9: train_loss=0.5049,\nval_loss=0.5230, val_acc=79.00%', '\\n', 'Epoch 10: train_loss=0.5043,\nval_loss=0.5182, val_acc=79.60%', '\\n', 'Dropout 0.0: Test accuracy 79.50%',\n'\\n', '\\n=== Training with dropout=0.1 ===', '\\n', 'Epoch 1: train_loss=0.5835,\nval_loss=0.5594, val_acc=76.20%', '\\n', 'Epoch 2: train_loss=0.5351,\nval_loss=0.5448, val_acc=77.00%', '\\n', 'Epoch 3: train_loss=0.5203,\nval_loss=0.5490, val_acc=79.40%', '\\n', 'Epoch 4: train_loss=0.5195,\nval_loss=0.5258, val_acc=78.60%', '\\n', 'Epoch 5: train_loss=0.5193,\nval_loss=0.5268, val_acc=79.60%', '\\n', 'Epoch 6: train_loss=0.5221,\nval_loss=0.5269, val_acc=78.60%', '\\n', 'Epoch 7: train_loss=0.5212,\nval_loss=0.5645, val_acc=77.00%', '\\n', 'Epoch 8: train_loss=0.5182,\nval_loss=0.5258, val_acc=79.00%', '\\n', 'Epoch 9: train_loss=0.5121,\nval_loss=0.5283, val_acc=79.60%', '\\n', 'Epoch 10: train_loss=0.5106,\nval_loss=0.5193, val_acc=78.60%', '\\n', 'Dropout 0.1: Test accuracy 79.00%',\n'\\n', '\\n=== Training with dropout=0.2 ===', '\\n', 'Epoch 1: train_loss=0.5906,\nval_loss=0.5621, val_acc=76.20%', '\\n', 'Epoch 2: train_loss=0.5270,\nval_loss=0.5439, val_acc=78.60%', '\\n', 'Epoch 3: train_loss=0.5207,\nval_loss=0.5380, val_acc=79.60%', '\\n', 'Epoch 4: train_loss=0.5277,\nval_loss=0.5199, val_acc=79.40%', '\\n', 'Epoch 5: train_loss=0.5095,\nval_loss=0.5465, val_acc=78.60%', '\\n', 'Epoch 6: train_loss=0.5189,\nval_loss=0.5161, val_acc=79.60%', '\\n', 'Epoch 7: train_loss=0.5083,\nval_loss=0.5153, val_acc=79.40%', '\\n', 'Epoch 8: train_loss=0.5067,\nval_loss=0.5187, val_acc=79.60%', '\\n', 'Epoch 9: train_loss=0.5081,\nval_loss=0.5654, val_acc=76.40%', '\\n', 'Epoch 10: train_loss=0.5159,\nval_loss=0.5360, val_acc=78.60%', '\\n', 'Dropout 0.2: Test accuracy 78.80%',\n'\\n', '\\n=== Training with dropout=0.3 ===', '\\n', 'Epoch 1: train_loss=0.6157,\nval_loss=0.5461, val_acc=77.80%', '\\n', 'Epoch 2: train_loss=0.5279,\nval_loss=0.5773, val_acc=75.60%', '\\n', 'Epoch 3: train_loss=0.5225,\nval_loss=0.5373, val_acc=78.60%', '\\n', 'Epoch 4: train_loss=0.5149,\nval_loss=0.5228, val_acc=78.60%', '\\n', 'Epoch 5: train_loss=0.5100,\nval_loss=0.5152, val_acc=79.60%', '\\n', 'Epoch 6: train_loss=0.5126,\nval_loss=0.5130, val_acc=79.60%', '\\n', 'Epoch 7: train_loss=0.5141,\nval_loss=0.5453, val_acc=77.40%', '\\n', 'Epoch 8: train_loss=0.5134,\nval_loss=0.5236, val_acc=79.20%', '\\n', 'Epoch 9: train_loss=0.5110,\nval_loss=0.5123, val_acc=79.20%', '\\n', 'Epoch 10: train_loss=0.5086,\nval_loss=0.5204, val_acc=79.40%', '\\n', 'Dropout 0.3: Test accuracy 79.50%',\n'\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time:\n15 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train',\n'dev', 'test']\", '\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training\nset: 64', '\\n', '\\n=== Training with weight_decay=0.0 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5998, val_loss=0.5461, val_acc=78.60%', '\\n', 'Epoch 2:\ntrain_loss=0.5275, val_loss=0.5366, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5371, val_loss=0.5417, val_acc=78.40%', '\\n', 'Epoch 4:\ntrain_loss=0.5200, val_loss=0.5212, val_acc=79.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5161, val_loss=0.5232, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5127, val_loss=0.5194, val_acc=79.00%', '\\n', 'Epoch 7:\ntrain_loss=0.5077, val_loss=0.5155, val_acc=79.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5151, val_loss=0.5190, val_acc=78.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5132, val_loss=0.5248, val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5035, val_loss=0.5125, val_acc=79.60%', '\\n', 'Test accuracy\n(weight_decay=0.0): 79.40%', '\\n', '\\n=== Training with weight_decay=1e-05 ===',\n'\\n', 'Epoch 1: train_loss=0.5828, val_loss=0.6042, val_acc=71.60%', '\\n',\n'Epoch 2: train_loss=0.5403, val_loss=0.5487, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5394, val_loss=0.5482, val_acc=79.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5270, val_loss=0.5285, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5187, val_loss=0.5234, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5101, val_loss=0.5211, val_acc=79.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5146, val_loss=0.5261, val_acc=78.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5196, val_loss=0.5333, val_acc=78.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5152, val_loss=0.5473, val_acc=77.80%', '\\n', 'Epoch 10:\ntrain_loss=0.5113, val_loss=0.5191, val_acc=79.40%', '\\n', 'Test accuracy\n(weight_decay=1e-05): 79.20%', '\\n', '\\n=== Training with weight_decay=0.0001\n===', '\\n', 'Epoch 1: train_loss=0.6211, val_loss=0.5445, val_acc=78.40%', '\\n',\n'Epoch 2: train_loss=0.5280, val_loss=0.5439, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5210, val_loss=0.5549, val_acc=78.00%', '\\n', 'Epoch 4:\ntrain_loss=0.5247, val_loss=0.5313, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5171, val_loss=0.5235, val_acc=79.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5171, val_loss=0.5251, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5097, val_loss=0.5209, val_acc=79.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5144, val_loss=0.5185, val_acc=79.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5142, val_loss=0.5336, val_acc=77.00%', '\\n', 'Epoch 10:\ntrain_loss=0.5123, val_loss=0.5293, val_acc=79.00%', '\\n', 'Test accuracy\n(weight_decay=0.0001): 79.20%', '\\n', '\\n=== Training with weight_decay=0.001\n===', '\\n', 'Epoch 1: train_loss=0.5935, val_loss=0.5467, val_acc=78.60%', '\\n',\n'Epoch 2: train_loss=0.5408, val_loss=0.5421, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5276, val_loss=0.5323, val_acc=78.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5202, val_loss=0.5255, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5185, val_loss=0.5435, val_acc=78.40%', '\\n', 'Epoch 6:\ntrain_loss=0.5179, val_loss=0.5229, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5145, val_loss=0.5220, val_acc=78.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5254, val_loss=0.5452, val_acc=76.20%', '\\n', 'Epoch 9:\ntrain_loss=0.5103, val_loss=0.5201, val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5058, val_loss=0.5265, val_acc=78.60%', '\\n', 'Test accuracy\n(weight_decay=0.001): 79.00%', '\\n', 'Experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n8/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at:', ' ', '/home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded splits:', ' ', \"['train', 'dev',\n'test']\", '\\n', 'Vocab size:', ' ', '9', '\\n', 'Max seq len:', ' ', '64', '\\n',\n'\\n=== Training with d_model=32 ===', '\\n', 'Ep01 - train_loss:0.6360\nval_loss:0.5667 val_acc:73.80%', '\\n', 'Ep02 - train_loss:0.5311 val_loss:0.5312\nval_acc:78.60%', '\\n', 'Ep03 - train_loss:0.5330 val_loss:0.5343\nval_acc:77.80%', '\\n', 'Ep04 - train_loss:0.5188 val_loss:0.5216\nval_acc:78.60%', '\\n', 'Ep05 - train_loss:0.5153 val_loss:0.5159\nval_acc:79.40%', '\\n', 'Ep06 - train_loss:0.5107 val_loss:0.5241\nval_acc:78.80%', '\\n', 'Ep07 - train_loss:0.5153 val_loss:0.5149\nval_acc:78.80%', '\\n', 'Ep08 - train_loss:0.5135 val_loss:0.5167\nval_acc:79.60%', '\\n', 'Ep09 - train_loss:0.5122 val_loss:0.5128\nval_acc:79.60%', '\\n', 'Ep10 - train_loss:0.5082 val_loss:0.5109\nval_acc:79.60%', '\\n', 'Test acc for d_model=32: 79.50%', '\\n', '\\n=== Training\nwith d_model=64 ===', '\\n', 'Ep01 - train_loss:0.6157 val_loss:0.6045\nval_acc:72.00%', '\\n', 'Ep02 - train_loss:0.5515 val_loss:0.5404\nval_acc:78.80%', '\\n', 'Ep03 - train_loss:0.5240 val_loss:0.5249\nval_acc:78.60%', '\\n', 'Ep04 - train_loss:0.5181 val_loss:0.5252\nval_acc:78.80%', '\\n', 'Ep05 - train_loss:0.5141 val_loss:0.5232\nval_acc:78.60%', '\\n', 'Ep06 - train_loss:0.5129 val_loss:0.5242\nval_acc:78.60%', '\\n', 'Ep07 - train_loss:0.5106 val_loss:0.5216\nval_acc:79.00%', '\\n', 'Ep08 - train_loss:0.5143 val_loss:0.5239\nval_acc:78.80%', '\\n', 'Ep09 - train_loss:0.5097 val_loss:0.6208\nval_acc:60.20%', '\\n', 'Ep10 - train_loss:0.5225 val_loss:0.5348\nval_acc:78.60%', '\\n', 'Test acc for d_model=64: 78.90%', '\\n', '\\n=== Training\nwith d_model=128 ===', '\\n', 'Ep01 - train_loss:0.5957 val_loss:0.5692\nval_acc:78.60%', '\\n', 'Ep02 - train_loss:0.5574 val_loss:0.6080\nval_acc:69.40%', '\\n', 'Ep03 - train_loss:0.5332 val_loss:0.5325\nval_acc:78.60%', '\\n', 'Ep04 - train_loss:0.5271 val_loss:0.5280\nval_acc:79.40%', '\\n', 'Ep05 - train_loss:0.5172 val_loss:0.5325\nval_acc:78.40%', '\\n', 'Ep06 - train_loss:0.5198 val_loss:0.5174\nval_acc:79.60%', '\\n', 'Ep07 - train_loss:0.5116 val_loss:0.5459\nval_acc:78.40%', '\\n', 'Ep08 - train_loss:0.5291 val_loss:0.5222\nval_acc:78.60%', '\\n', 'Ep09 - train_loss:0.5133 val_loss:0.5370\nval_acc:78.00%', '\\n', 'Ep10 - train_loss:0.5166 val_loss:0.5253\nval_acc:78.60%', '\\n', 'Test acc for d_model=128: 78.80%', '\\n', '\\n=== Training\nwith d_model=256 ===', '\\n', 'Ep01 - train_loss:0.6368 val_loss:0.5651\nval_acc:74.40%', '\\n', 'Ep02 - train_loss:0.5485 val_loss:0.5449\nval_acc:78.60%', '\\n', 'Ep03 - train_loss:0.5264 val_loss:0.5312\nval_acc:78.60%', '\\n', 'Ep04 - train_loss:0.5293 val_loss:0.5711\nval_acc:77.00%', '\\n', 'Ep05 - train_loss:0.5338 val_loss:0.5755\nval_acc:75.80%', '\\n', 'Ep06 - train_loss:0.5355 val_loss:0.5238\nval_acc:78.60%', '\\n', 'Ep07 - train_loss:0.5193 val_loss:0.5441\nval_acc:76.00%', '\\n', 'Ep08 - train_loss:0.5301 val_loss:0.5886\nval_acc:74.20%', '\\n', 'Ep09 - train_loss:0.5296 val_loss:0.6408\nval_acc:55.00%', '\\n', 'Ep10 - train_loss:0.5335 val_loss:0.5845\nval_acc:75.20%', '\\n', 'Test acc for d_model=256: 77.10%', '\\n', 'All experiment\ndata saved to /home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 16 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train',\n'dev', 'test']\", '\\n', 'Vocab size: 9', '\\n', 'Max sequence length: 64', '\\n',\n'\\n\\n=== Training with num_layers = 1 ===', '\\n', '[L1] Epoch 1:\ntrain_loss=0.6423, val_loss=0.5758, val_acc=73.80%', '\\n', '[L1] Epoch 2:\ntrain_loss=0.5346, val_loss=0.5439, val_acc=78.80%', '\\n', '[L1] Epoch 3:\ntrain_loss=0.5253, val_loss=0.5335, val_acc=78.60%', '\\n', '[L1] Epoch 4:\ntrain_loss=0.5168, val_loss=0.5243, val_acc=78.80%', '\\n', '[L1] Epoch 5:\ntrain_loss=0.5189, val_loss=0.5336, val_acc=78.60%', '\\n', '[L1] Epoch 6:\ntrain_loss=0.5097, val_loss=0.5275, val_acc=78.80%', '\\n', '[L1] Epoch 7:\ntrain_loss=0.5174, val_loss=0.5392, val_acc=78.60%', '\\n', '[L1] Epoch 8:\ntrain_loss=0.5129, val_loss=0.5198, val_acc=79.60%', '\\n', '[L1] Epoch 9:\ntrain_loss=0.5100, val_loss=0.5319, val_acc=78.60%', '\\n', '[L1] Epoch 10:\ntrain_loss=0.5034, val_loss=0.5242, val_acc=79.60%', '\\n', '[L1] Test accuracy:\n79.50%', '\\n', '\\n\\n=== Training with num_layers = 2 ===', '\\n', '[L2] Epoch 1:\ntrain_loss=0.5836, val_loss=0.5589, val_acc=76.20%', '\\n', '[L2] Epoch 2:\ntrain_loss=0.5343, val_loss=0.5446, val_acc=77.00%', '\\n', '[L2] Epoch 3:\ntrain_loss=0.5202, val_loss=0.5480, val_acc=79.40%', '\\n', '[L2] Epoch 4:\ntrain_loss=0.5189, val_loss=0.5247, val_acc=78.60%', '\\n', '[L2] Epoch 5:\ntrain_loss=0.5186, val_loss=0.5273, val_acc=79.40%', '\\n', '[L2] Epoch 6:\ntrain_loss=0.5229, val_loss=0.5248, val_acc=78.60%', '\\n', '[L2] Epoch 7:\ntrain_loss=0.5193, val_loss=0.5629, val_acc=75.60%', '\\n', '[L2] Epoch 8:\ntrain_loss=0.5167, val_loss=0.5258, val_acc=79.20%', '\\n', '[L2] Epoch 9:\ntrain_loss=0.5152, val_loss=0.5197, val_acc=79.60%', '\\n', '[L2] Epoch 10:\ntrain_loss=0.5113, val_loss=0.5174, val_acc=79.20%', '\\n', '[L2] Test accuracy:\n79.20%', '\\n', '\\n\\n=== Training with num_layers = 3 ===', '\\n', '[L3] Epoch 1:\ntrain_loss=0.5871, val_loss=0.5423, val_acc=78.40%', '\\n', '[L3] Epoch 2:\ntrain_loss=0.5247, val_loss=0.5292, val_acc=78.60%', '\\n', '[L3] Epoch 3:\ntrain_loss=0.5134, val_loss=0.5929, val_acc=68.60%', '\\n', '[L3] Epoch 4:\ntrain_loss=0.5243, val_loss=0.5292, val_acc=78.60%', '\\n', '[L3] Epoch 5:\ntrain_loss=0.5129, val_loss=0.5376, val_acc=78.60%', '\\n', '[L3] Epoch 6:\ntrain_loss=0.5191, val_loss=0.5147, val_acc=79.60%', '\\n', '[L3] Epoch 7:\ntrain_loss=0.5081, val_loss=0.5192, val_acc=78.60%', '\\n', '[L3] Epoch 8:\ntrain_loss=0.5092, val_loss=0.5106, val_acc=79.60%', '\\n', '[L3] Epoch 9:\ntrain_loss=0.5115, val_loss=0.5728, val_acc=74.40%', '\\n', '[L3] Epoch 10:\ntrain_loss=0.5226, val_loss=0.5209, val_acc=79.40%', '\\n', '[L3] Test accuracy:\n79.50%', '\\n', '\\n\\n=== Training with num_layers = 4 ===', '\\n', '[L4] Epoch 1:\ntrain_loss=0.6008, val_loss=0.5387, val_acc=78.00%', '\\n', '[L4] Epoch 2:\ntrain_loss=0.5205, val_loss=0.5447, val_acc=77.60%', '\\n', '[L4] Epoch 3:\ntrain_loss=0.5128, val_loss=0.5129, val_acc=79.40%', '\\n', '[L4] Epoch 4:\ntrain_loss=0.5164, val_loss=0.5100, val_acc=79.60%', '\\n', '[L4] Epoch 5:\ntrain_loss=0.5096, val_loss=0.5244, val_acc=78.60%', '\\n', '[L4] Epoch 6:\ntrain_loss=0.5073, val_loss=0.5089, val_acc=79.60%', '\\n', '[L4] Epoch 7:\ntrain_loss=0.5104, val_loss=0.5185, val_acc=78.60%', '\\n', '[L4] Epoch 8:\ntrain_loss=0.5085, val_loss=0.5254, val_acc=78.60%', '\\n', '[L4] Epoch 9:\ntrain_loss=0.5097, val_loss=0.5101, val_acc=78.80%', '\\n', '[L4] Epoch 10:\ntrain_loss=0.5096, val_loss=0.5060, val_acc=79.60%', '\\n', '[L4] Test accuracy:\n79.50%', '\\n', '\\n\\n=== Training with num_layers = 6 ===', '\\n', '[L6] Epoch 1:\ntrain_loss=0.6834, val_loss=0.6162, val_acc=62.20%', '\\n', '[L6] Epoch 2:\ntrain_loss=0.5375, val_loss=0.5640, val_acc=75.60%', '\\n', '[L6] Epoch 3:\ntrain_loss=0.5213, val_loss=0.5345, val_acc=79.00%', '\\n', '[L6] Epoch 4:\ntrain_loss=0.5236, val_loss=0.5100, val_acc=79.40%', '\\n', '[L6] Epoch 5:\ntrain_loss=0.5095, val_loss=0.5103, val_acc=79.40%', '\\n', '[L6] Epoch 6:\ntrain_loss=0.5072, val_loss=0.5338, val_acc=77.20%', '\\n', '[L6] Epoch 7:\ntrain_loss=0.5108, val_loss=0.5287, val_acc=78.60%', '\\n', '[L6] Epoch 8:\ntrain_loss=0.5101, val_loss=0.5214, val_acc=78.40%', '\\n', '[L6] Epoch 9:\ntrain_loss=0.5106, val_loss=0.5331, val_acc=78.60%', '\\n', '[L6] Epoch 10:\ntrain_loss=0.5111, val_loss=0.5095, val_acc=79.60%', '\\n', '[L6] Test accuracy:\n79.50%', '\\n', '\\nAll experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time:\n25 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train',\n'dev', 'test']\", '\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training\nset: 64', '\\n', '\\n### Starting run 1/3 with nhead=2 ###', '\\n', '[nhead=2]\nEpoch 01: train_loss=0.5933, val_loss=0.5694, val_acc=76.20%', '\\n', '[nhead=2]\nEpoch 02: train_loss=0.5387, val_loss=0.5432, val_acc=77.60%', '\\n', '[nhead=2]\nEpoch 03: train_loss=0.5387, val_loss=0.5363, val_acc=78.20%', '\\n', '[nhead=2]\nEpoch 04: train_loss=0.5246, val_loss=0.5333, val_acc=78.60%', '\\n', '[nhead=2]\nEpoch 05: train_loss=0.5204, val_loss=0.5313, val_acc=78.60%', '\\n', '[nhead=2]\nEpoch 06: train_loss=0.5220, val_loss=0.5274, val_acc=79.60%', '\\n', '[nhead=2]\nEpoch 07: train_loss=0.5212, val_loss=0.5336, val_acc=78.80%', '\\n', '[nhead=2]\nEpoch 08: train_loss=0.5105, val_loss=0.5274, val_acc=78.80%', '\\n', '[nhead=2]\nEpoch 09: train_loss=0.5086, val_loss=0.5214, val_acc=79.60%', '\\n', '[nhead=2]\nEpoch 10: train_loss=0.5097, val_loss=0.5700, val_acc=71.80%', '\\n', '\\n###\nStarting run 2/3 with nhead=4 ###', '\\n', '[nhead=4] Epoch 01:\ntrain_loss=0.5960, val_loss=0.5549, val_acc=76.20%', '\\n', '[nhead=4] Epoch 02:\ntrain_loss=0.5295, val_loss=0.5402, val_acc=78.20%', '\\n', '[nhead=4] Epoch 03:\ntrain_loss=0.5319, val_loss=0.5294, val_acc=79.00%', '\\n', '[nhead=4] Epoch 04:\ntrain_loss=0.5186, val_loss=0.5160, val_acc=79.40%', '\\n', '[nhead=4] Epoch 05:\ntrain_loss=0.5136, val_loss=0.5284, val_acc=79.60%', '\\n', '[nhead=4] Epoch 06:\ntrain_loss=0.5105, val_loss=0.5287, val_acc=78.60%', '\\n', '[nhead=4] Epoch 07:\ntrain_loss=0.5165, val_loss=0.5315, val_acc=78.60%', '\\n', '[nhead=4] Epoch 08:\ntrain_loss=0.5160, val_loss=0.5311, val_acc=78.60%', '\\n', '[nhead=4] Epoch 09:\ntrain_loss=0.5222, val_loss=0.5148, val_acc=79.60%', '\\n', '[nhead=4] Epoch 10:\ntrain_loss=0.5051, val_loss=0.5176, val_acc=79.60%', '\\n', '\\n### Starting run\n3/3 with nhead=8 ###', '\\n', '[nhead=8] Epoch 01: train_loss=0.5819,\nval_loss=0.5750, val_acc=74.00%', '\\n', '[nhead=8] Epoch 02: train_loss=0.5368,\nval_loss=0.5558, val_acc=76.20%', '\\n', '[nhead=8] Epoch 03: train_loss=0.5337,\nval_loss=0.5423, val_acc=78.60%', '\\n', '[nhead=8] Epoch 04: train_loss=0.5281,\nval_loss=0.5555, val_acc=76.00%', '\\n', '[nhead=8] Epoch 05: train_loss=0.5269,\nval_loss=0.5320, val_acc=79.60%', '\\n', '[nhead=8] Epoch 06: train_loss=0.5255,\nval_loss=0.5256, val_acc=79.60%', '\\n', '[nhead=8] Epoch 07: train_loss=0.5150,\nval_loss=0.5189, val_acc=79.60%', '\\n', '[nhead=8] Epoch 08: train_loss=0.5126,\nval_loss=0.5184, val_acc=79.60%', '\\n', '[nhead=8] Epoch 09: train_loss=0.5113,\nval_loss=0.5195, val_acc=79.60%', '\\n', '[nhead=8] Epoch 10: train_loss=0.5110,\nval_loss=0.5292, val_acc=78.60%', '\\n', '\\nValidation accuracies per nhead:', '\n', '[0.718, 0.796, 0.786]', '\\n', 'Best nhead=4 with val_acc=79.60%', '\\n',\n'Corresponding test accuracy: 79.50%', '\\n', 'Experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n7/working/experiment_data.npy', '\\n', 'Execution time: 13 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train',\n'dev', 'test']\", '\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training\nset: 64', '\\n', '\\n=== Training with dropout=0.0 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5832, val_loss=0.5451, val_acc=78.60%', '\\n', 'Epoch 2:\ntrain_loss=0.5330, val_loss=0.5341, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5259, val_loss=0.5269, val_acc=79.40%', '\\n', 'Epoch 4:\ntrain_loss=0.5207, val_loss=0.5366, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5243, val_loss=0.5256, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5115, val_loss=0.5643, val_acc=76.00%', '\\n', 'Epoch 7:\ntrain_loss=0.5168, val_loss=0.5209, val_acc=78.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5135, val_loss=0.5232, val_acc=78.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5107, val_loss=0.5121, val_acc=79.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5092, val_loss=0.5271, val_acc=78.40%', '\\n', 'Dropout 0.0: Test\naccuracy 78.70%', '\\n', '\\n=== Training with dropout=0.1 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6049, val_loss=0.5508, val_acc=77.40%', '\\n', 'Epoch 2:\ntrain_loss=0.5335, val_loss=0.5801, val_acc=79.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5467, val_loss=0.5388, val_acc=78.00%', '\\n', 'Epoch 4:\ntrain_loss=0.5207, val_loss=0.5564, val_acc=76.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5179, val_loss=0.5331, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5144, val_loss=0.5180, val_acc=78.80%', '\\n', 'Epoch 7:\ntrain_loss=0.5160, val_loss=0.5183, val_acc=79.40%', '\\n', 'Epoch 8:\ntrain_loss=0.5067, val_loss=0.5168, val_acc=79.40%', '\\n', 'Epoch 9:\ntrain_loss=0.5071, val_loss=0.5152, val_acc=79.40%', '\\n', 'Epoch 10:\ntrain_loss=0.5081, val_loss=0.5119, val_acc=79.60%', '\\n', 'Dropout 0.1: Test\naccuracy 79.50%', '\\n', '\\n=== Training with dropout=0.2 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5921, val_loss=0.5683, val_acc=79.60%', '\\n', 'Epoch 2:\ntrain_loss=0.5308, val_loss=0.5306, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5251, val_loss=0.5397, val_acc=78.00%', '\\n', 'Epoch 4:\ntrain_loss=0.5192, val_loss=0.5325, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5207, val_loss=0.5451, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5218, val_loss=0.5182, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5128, val_loss=0.5169, val_acc=79.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5207, val_loss=0.5199, val_acc=79.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5079, val_loss=0.5205, val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5050, val_loss=0.5374, val_acc=78.80%', '\\n', 'Dropout 0.2: Test\naccuracy 79.30%', '\\n', '\\n=== Training with dropout=0.3 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6370, val_loss=0.5520, val_acc=78.80%', '\\n', 'Epoch 2:\ntrain_loss=0.5353, val_loss=0.5652, val_acc=76.00%', '\\n', 'Epoch 3:\ntrain_loss=0.5354, val_loss=0.5285, val_acc=78.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5257, val_loss=0.5262, val_acc=79.00%', '\\n', 'Epoch 5:\ntrain_loss=0.5192, val_loss=0.5421, val_acc=77.80%', '\\n', 'Epoch 6:\ntrain_loss=0.5141, val_loss=0.5161, val_acc=79.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5121, val_loss=0.5206, val_acc=78.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5097, val_loss=0.5167, val_acc=79.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5116, val_loss=0.5237, val_acc=79.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5166, val_loss=0.5225, val_acc=79.40%', '\\n', 'Dropout 0.3: Test\naccuracy 79.40%', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-8/working/experiment_data.npy', '\\n', 'Execution time:\n17 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train',\n'dev', 'test']\", '\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training\nset: 64', '\\n', '\\n=== Training with dropout=0.0 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5887, val_loss=0.5542, val_acc=78.60%', '\\n', 'Epoch 2:\ntrain_loss=0.5357, val_loss=0.5526, val_acc=75.80%', '\\n', 'Epoch 3:\ntrain_loss=0.5249, val_loss=0.5553, val_acc=76.20%', '\\n', 'Epoch 4:\ntrain_loss=0.5210, val_loss=0.5407, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5144, val_loss=0.5222, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5119, val_loss=0.5279, val_acc=79.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5122, val_loss=0.5285, val_acc=78.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5128, val_loss=0.5139, val_acc=79.20%', '\\n', 'Epoch 9:\ntrain_loss=0.5117, val_loss=0.5338, val_acc=79.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5173, val_loss=0.5454, val_acc=76.80%', '\\n', 'Dropout 0.0: Test\naccuracy 77.20%', '\\n', '\\n=== Training with dropout=0.1 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5960, val_loss=0.5585, val_acc=75.80%', '\\n', 'Epoch 2:\ntrain_loss=0.5361, val_loss=0.5443, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5307, val_loss=0.5365, val_acc=78.40%', '\\n', 'Epoch 4:\ntrain_loss=0.5202, val_loss=0.5366, val_acc=78.80%', '\\n', 'Epoch 5:\ntrain_loss=0.5156, val_loss=0.5328, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5122, val_loss=0.5209, val_acc=79.20%', '\\n', 'Epoch 7:\ntrain_loss=0.5093, val_loss=0.5290, val_acc=78.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5310, val_loss=0.5278, val_acc=79.00%', '\\n', 'Epoch 9:\ntrain_loss=0.5157, val_loss=0.5216, val_acc=79.20%', '\\n', 'Epoch 10:\ntrain_loss=0.5132, val_loss=0.5302, val_acc=78.00%', '\\n', 'Dropout 0.1: Test\naccuracy 78.80%', '\\n', '\\n=== Training with dropout=0.2 ===', '\\n', 'Epoch 1:\ntrain_loss=0.6026, val_loss=0.5631, val_acc=79.00%', '\\n', 'Epoch 2:\ntrain_loss=0.5448, val_loss=0.5391, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5284, val_loss=0.5315, val_acc=78.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5182, val_loss=0.5352, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5162, val_loss=0.5269, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5148, val_loss=0.5220, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5112, val_loss=0.5380, val_acc=78.40%', '\\n', 'Epoch 8:\ntrain_loss=0.5198, val_loss=0.5247, val_acc=78.40%', '\\n', 'Epoch 9:\ntrain_loss=0.5069, val_loss=0.5396, val_acc=79.00%', '\\n', 'Epoch 10:\ntrain_loss=0.5112, val_loss=0.5243, val_acc=79.60%', '\\n', 'Dropout 0.2: Test\naccuracy 79.40%', '\\n', '\\n=== Training with dropout=0.3 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5989, val_loss=0.5502, val_acc=78.20%', '\\n', 'Epoch 2:\ntrain_loss=0.5420, val_loss=0.5361, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5274, val_loss=0.5256, val_acc=79.20%', '\\n', 'Epoch 4:\ntrain_loss=0.5156, val_loss=0.5162, val_acc=79.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5190, val_loss=0.5117, val_acc=79.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5180, val_loss=0.5195, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5127, val_loss=0.5131, val_acc=79.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5102, val_loss=0.5103, val_acc=79.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5080, val_loss=0.5243, val_acc=79.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5201, val_loss=0.5284, val_acc=79.60%', '\\n', 'Dropout 0.3: Test\naccuracy 79.50%', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-6/working/experiment_data.npy', '\\n', 'Execution time:\n17 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train',\n'dev', 'test']\", '\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training\nset: 64', '\\n', '\\n=== Training with dropout=0.0 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5748, val_loss=0.6557, val_acc=64.40%', '\\n', 'Epoch 2:\ntrain_loss=0.5342, val_loss=0.5376, val_acc=79.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5439, val_loss=0.5286, val_acc=78.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5240, val_loss=0.5198, val_acc=79.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5179, val_loss=0.5237, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5173, val_loss=0.5198, val_acc=78.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5104, val_loss=0.5194, val_acc=79.60%', '\\n', 'Epoch 8:\ntrain_loss=0.5141, val_loss=0.5224, val_acc=78.80%', '\\n', 'Epoch 9:\ntrain_loss=0.5193, val_loss=0.5147, val_acc=79.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5064, val_loss=0.5185, val_acc=79.00%', '\\n', 'Dropout 0.0: Test\naccuracy 79.40%', '\\n', '\\n=== Training with dropout=0.1 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5636, val_loss=0.6135, val_acc=71.40%', '\\n', 'Epoch 2:\ntrain_loss=0.5370, val_loss=0.5278, val_acc=78.60%', '\\n', 'Epoch 3:\ntrain_loss=0.5229, val_loss=0.5294, val_acc=79.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5167, val_loss=0.5207, val_acc=79.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5177, val_loss=0.5214, val_acc=79.40%', '\\n', 'Epoch 6:\ntrain_loss=0.5168, val_loss=0.5338, val_acc=78.40%', '\\n', 'Epoch 7:\ntrain_loss=0.5113, val_loss=0.5174, val_acc=79.40%', '\\n', 'Epoch 8:\ntrain_loss=0.5138, val_loss=0.5226, val_acc=79.60%', '\\n', 'Epoch 9:\ntrain_loss=0.5099, val_loss=0.5311, val_acc=78.80%', '\\n', 'Epoch 10:\ntrain_loss=0.5055, val_loss=0.5287, val_acc=78.40%', '\\n', 'Dropout 0.1: Test\naccuracy 78.90%', '\\n', '\\n=== Training with dropout=0.2 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5828, val_loss=0.5804, val_acc=79.60%', '\\n', 'Epoch 2:\ntrain_loss=0.5299, val_loss=0.5476, val_acc=76.80%', '\\n', 'Epoch 3:\ntrain_loss=0.5222, val_loss=0.5248, val_acc=79.60%', '\\n', 'Epoch 4:\ntrain_loss=0.5177, val_loss=0.5384, val_acc=78.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5159, val_loss=0.5292, val_acc=78.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5113, val_loss=0.5177, val_acc=79.60%', '\\n', 'Epoch 7:\ntrain_loss=0.5091, val_loss=0.5181, val_acc=79.00%', '\\n', 'Epoch 8:\ntrain_loss=0.5139, val_loss=0.5371, val_acc=76.80%', '\\n', 'Epoch 9:\ntrain_loss=0.5142, val_loss=0.5504, val_acc=78.60%', '\\n', 'Epoch 10:\ntrain_loss=0.5138, val_loss=0.5265, val_acc=78.60%', '\\n', 'Dropout 0.2: Test\naccuracy 78.90%', '\\n', '\\n=== Training with dropout=0.3 ===', '\\n', 'Epoch 1:\ntrain_loss=0.5984, val_loss=0.5566, val_acc=78.40%', '\\n', 'Epoch 2:\ntrain_loss=0.5382, val_loss=0.5738, val_acc=76.00%', '\\n', 'Epoch 3:\ntrain_loss=0.5367, val_loss=0.5472, val_acc=79.40%', '\\n', 'Epoch 4:\ntrain_loss=0.5265, val_loss=0.5331, val_acc=79.60%', '\\n', 'Epoch 5:\ntrain_loss=0.5192, val_loss=0.5346, val_acc=79.60%', '\\n', 'Epoch 6:\ntrain_loss=0.5106, val_loss=0.5225, val_acc=79.40%', '\\n', 'Epoch 7:\ntrain_loss=0.5138, val_loss=0.5282, val_acc=79.00%', '\\n', 'Epoch 8:\ntrain_loss=0.5146, val_loss=0.5248, val_acc=79.20%', '\\n', 'Epoch 9:\ntrain_loss=0.5126, val_loss=0.5427, val_acc=78.20%', '\\n', 'Epoch 10:\ntrain_loss=0.5141, val_loss=0.5279, val_acc=79.20%', '\\n', 'Dropout 0.3: Test\naccuracy 79.30%', '\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time:\n18 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["The execution of the training script was successful without any bugs. The\ndataset was loaded correctly, and the training process proceeded as expected.\nThe model achieved a validation accuracy of 79.60% and a test accuracy of\n79.50%, which is close to the state-of-the-art benchmark of 80.0%. The\nexperiment data was successfully saved for further analysis.", "The execution of the training script was successful without any bugs. The script\nimplemented a hyperparameter sweep over different epoch budgets (10, 20, 30, 50)\nand utilized early stopping based on validation loss. The model achieved a test\naccuracy of approximately 79.3%-79.5% across all configurations, which is close\nto the current state-of-the-art (80.0%) on the SPR_BENCH dataset. The experiment\ndata was saved successfully for further analysis.", "The execution of the training script was successful, and there were no errors or\nbugs observed. The training process iterated through different learning rates,\nand the model's performance was evaluated on the validation and test datasets.\nThe best test accuracy achieved was 79.5%, which is close to the SOTA benchmark\nof 80.0%. The results were saved successfully, and the execution time was well\nwithin the limit.", "", "", "", "The training script executed successfully without any bugs. The model was\ntrained with different values of d_model (32, 64, 128, 256) and achieved a\nmaximum test accuracy of 79.50% with d_model=32. The results were saved to a\nfile for further analysis. No issues were encountered during execution.", "", "The execution of the training script was successful. The script implemented a\nhyperparameter tuning experiment for the 'nhead' parameter of a transformer-\nbased classifier on the SPR_BENCH dataset. Three configurations (nhead=2, 4, 8)\nwere tested, and the best validation accuracy was achieved with nhead=4\n(79.60%), corresponding to a test accuracy of 79.50%. The results were saved\nsuccessfully, and there were no errors or bugs in the execution.", "", "", "The execution output shows no errors or bugs. The training process was\nsuccessful, iterating over multiple dropout rates (0.0, 0.1, 0.2, 0.3). The\nmodel was trained and evaluated on the provided dataset, and test accuracies\nwere reported for each configuration. The results were saved successfully, and\nthe execution completed within the time limit. No issues were detected.", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7985, "best_value": 0.7985}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5048, "best_value": 0.5048}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5144, "best_value": 0.5144}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "epochs_10", "final_value": 0.7955, "best_value": 0.7955}, {"dataset_name": "epochs_20", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "epochs_30", "final_value": 0.7915, "best_value": 0.7915}, {"dataset_name": "epochs_50", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "epochs_10", "final_value": 0.662, "best_value": 0.662}, {"dataset_name": "epochs_20", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "epochs_30", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "epochs_50", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "epochs_10", "final_value": 0.5069, "best_value": 0.5069}, {"dataset_name": "epochs_20", "final_value": 0.5114, "best_value": 0.5114}, {"dataset_name": "epochs_30", "final_value": 0.5117, "best_value": 0.5117}, {"dataset_name": "epochs_50", "final_value": 0.5087, "best_value": 0.5087}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "epochs_10", "final_value": 0.5884, "best_value": 0.5884}, {"dataset_name": "epochs_20", "final_value": 0.5339, "best_value": 0.5339}, {"dataset_name": "epochs_30", "final_value": 0.5138, "best_value": 0.5138}, {"dataset_name": "epochs_50", "final_value": 0.5249, "best_value": 0.5249}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "epochs_10", "final_value": 0.793, "best_value": 0.793}, {"dataset_name": "epochs_20", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "epochs_30", "final_value": 0.794, "best_value": 0.794}, {"dataset_name": "epochs_50", "final_value": 0.795, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.781, "best_value": 0.7955}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.786, "best_value": 0.796}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5069, "best_value": 0.5069}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5141, "best_value": 0.5141}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.789, "best_value": 0.795}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.7955, "best_value": 0.7955}, {"dataset_name": "bs_128", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "bs_256", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.5082, "best_value": 0.5082}, {"dataset_name": "bs_128", "final_value": 0.5087, "best_value": 0.5087}, {"dataset_name": "bs_256", "final_value": 0.5117, "best_value": 0.5117}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.792, "best_value": 0.792}, {"dataset_name": "bs_128", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "bs_256", "final_value": 0.786, "best_value": 0.786}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.5213, "best_value": 0.5213}, {"dataset_name": "bs_128", "final_value": 0.52, "best_value": 0.52}, {"dataset_name": "bs_256", "final_value": 0.5299, "best_value": 0.5299}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "bs_32", "final_value": 0.793, "best_value": 0.793}, {"dataset_name": "bs_128", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "bs_256", "final_value": 0.789, "best_value": 0.789}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5086, "best_value": 0.5086}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5123, "best_value": 0.5123}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5113, "best_value": 0.5113}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.65, "best_value": 79.65}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.6, "best_value": 79.6}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5125, "best_value": 0.5125}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.4, "best_value": 79.4}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy achieved on the training dataset.", "data": [{"dataset_name": "SPR_BENCH | Setting: d_model_32", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH | Setting: d_model_64", "final_value": 0.778, "best_value": 0.778}, {"dataset_name": "SPR_BENCH | Setting: d_model_128", "final_value": 0.7895, "best_value": 0.7895}, {"dataset_name": "SPR_BENCH | Setting: d_model_256", "final_value": 0.7725, "best_value": 0.7725}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH | Setting: d_model_32", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "SPR_BENCH | Setting: d_model_64", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "SPR_BENCH | Setting: d_model_128", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "SPR_BENCH | Setting: d_model_256", "final_value": 0.752, "best_value": 0.752}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss value achieved on the training dataset.", "data": [{"dataset_name": "SPR_BENCH | Setting: d_model_32", "final_value": 0.5082, "best_value": 0.5082}, {"dataset_name": "SPR_BENCH | Setting: d_model_64", "final_value": 0.5225, "best_value": 0.5225}, {"dataset_name": "SPR_BENCH | Setting: d_model_128", "final_value": 0.5166, "best_value": 0.5166}, {"dataset_name": "SPR_BENCH | Setting: d_model_256", "final_value": 0.5335, "best_value": 0.5335}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH | Setting: d_model_32", "final_value": 0.5109, "best_value": 0.5109}, {"dataset_name": "SPR_BENCH | Setting: d_model_64", "final_value": 0.5348, "best_value": 0.5348}, {"dataset_name": "SPR_BENCH | Setting: d_model_128", "final_value": 0.5253, "best_value": 0.5253}, {"dataset_name": "SPR_BENCH | Setting: d_model_256", "final_value": 0.5845, "best_value": 0.5845}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH | Setting: d_model_32", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "SPR_BENCH | Setting: d_model_64", "final_value": 0.789, "best_value": 0.789}, {"dataset_name": "SPR_BENCH | Setting: d_model_128", "final_value": 0.788, "best_value": 0.788}, {"dataset_name": "SPR_BENCH | Setting: d_model_256", "final_value": 0.771, "best_value": 0.771}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.7, "best_value": 79.7}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.6, "best_value": 79.6}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5034, "best_value": 0.5034}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5198, "best_value": 0.5198}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.5, "best_value": 79.5}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Accuracy measures the proportion of correct predictions out of total predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 79.5, "best_value": 79.6}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Loss quantifies the error between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5169, "best_value": 0.5039}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.798, "best_value": 0.798}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.505, "best_value": 0.505}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5119, "best_value": 0.5119}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5112, "best_value": 0.5112}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5173, "best_value": 0.5173}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5139, "best_value": 0.5139}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss of the model on the test data.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5158, "best_value": 0.5158}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy achieved on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy achieved on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5064, "best_value": 0.5064}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5147, "best_value": 0.5147}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "Loss calculated on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5135, "best_value": 0.5135}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_confusion_matrix_epochs_30.png"], ["../../logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_confusion_bs_128.png"], ["../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"], ["../../logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_loss_curves_weight_decay.png", "../../logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_accuracy_curves_weight_decay.png", "../../logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_test_accuracy_weight_decay.png", "../../logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_confusion_matrix_best_weight_decay.png"], ["../../logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_loss_curves_d_model_sweep.png", "../../logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_accuracy_curves_d_model_sweep.png", "../../logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_final_val_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_confusion_matrix_d_model_32.png"], ["../../logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_val_accuracy_bar.png"], ["../../logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_accuracy_vs_nhead.png", "../../logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_loss_vs_nhead.png", "../../logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_val_vs_test_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_confusion_matrix_best_dropout_0.1.png"], ["../../logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_confusion_matrix_best_dropout_0.3.png"], ["../../logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"], ["../../logs/0-run/experiment_results/seed_aggregation_d1d83ae62ab440918405879c1c909405/SPR_BENCH_mean_accuracy_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_d1d83ae62ab440918405879c1c909405/SPR_BENCH_mean_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_d1d83ae62ab440918405879c1c909405/SPR_BENCH_mean_test_accuracy_bar.png"]], "plot_paths": [["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_confusion_matrix_epochs_30.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_confusion_bs_128.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_loss_curves_weight_decay.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_accuracy_curves_weight_decay.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_test_accuracy_weight_decay.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_confusion_matrix_best_weight_decay.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_loss_curves_d_model_sweep.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_accuracy_curves_d_model_sweep.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_final_val_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_confusion_matrix_d_model_32.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_val_accuracy_bar.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_accuracy_vs_nhead.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_loss_vs_nhead.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_val_vs_test_accuracy.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_confusion_matrix_best_dropout_0.1.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_confusion_matrix_best_dropout_0.3.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_d1d83ae62ab440918405879c1c909405/SPR_BENCH_mean_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_d1d83ae62ab440918405879c1c909405/SPR_BENCH_mean_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_d1d83ae62ab440918405879c1c909405/SPR_BENCH_mean_test_accuracy_bar.png"]], "plot_analyses": [[{"analysis": "The loss curves indicate that the model is converging well. Both the training and validation loss decrease significantly in the initial epochs, with the validation loss stabilizing around epoch 4. The gap between training and validation loss is minimal, suggesting that the model is not overfitting. However, the slight fluctuations in validation loss after epoch 4 could indicate some sensitivity to the dataset or hyperparameter settings.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show consistent improvement over the epochs. Both training and validation accuracies reach a plateau near 80%, which aligns with the state-of-the-art (SOTA) benchmark accuracy mentioned in the hypothesis. The close tracking of validation accuracy with training accuracy is a positive sign, indicating good generalization. Further fine-tuning or architectural adjustments may be needed to surpass the SOTA.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_accuracy_curves.png"}, {"analysis": "The confusion matrix for the test set reveals a strong performance, with a high number of correctly classified samples for both classes. The balance in the confusion matrix suggests that the model is not biased toward any particular class. However, the exact number of misclassifications could be analyzed further to identify specific patterns or sequences that the model struggles with.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_89abcd2ab84b4fb89db44a3c173d28b4_proc_3154414/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curves depict training and validation loss trends across varying epoch budgets (10, 20, 30, 50). Notably, the training loss consistently decreases with increasing epochs, indicating effective learning. However, the validation loss shows fluctuations, especially for lower epoch budgets, which might suggest overfitting or instability in generalization. For the 10-epoch budget, the validation loss sharply increases after epoch 8, highlighting overfitting. The 30-epoch budget appears to strike a balance between training and validation loss, with relatively stable validation performance.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_loss_curves.png"}, {"analysis": "The accuracy curves demonstrate the model's performance on training and validation datasets across epoch budgets. While training accuracy generally improves with epochs, the validation accuracy exhibits a plateau after initial improvements. This indicates that the model quickly learns the patterns in the data but struggles to improve further due to potential overfitting or limitations in the model's capacity. The sharp drop in validation accuracy for the 10-epoch budget at later epochs further supports the overfitting observation. The 30-epoch budget achieves the best balance, with validation accuracy stabilizing around 80%.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The confusion matrix for the best-performing model (30 epochs) reveals the classification performance. The true positives (394 and 400) and true negatives (102 and 104) indicate a reasonable balance in predictions. However, there is still room for improvement, as misclassifications are significant. This suggests that while the model performs well, it might benefit from further optimization, such as enhanced feature representation or regularization techniques.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ec0e1eba37134161aa12fd1ff834b162_proc_3159061/SPR_BENCH_confusion_matrix_epochs_30.png"}], [{"analysis": "The accuracy curves for both training and validation sets show varying trends based on the learning rate. For the learning rate of 1e-03, there is a sharp increase in accuracy initially, but the validation accuracy drops significantly after epoch 8, indicating potential overfitting. For lower learning rates (5e-04, 2e-04, and 1e-04), the training and validation accuracies converge more smoothly, with the learning rate of 5e-04 showing the best and most stable performance. The learning rate of 5e-05, while stable, converges more slowly and achieves slightly lower overall accuracy.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The loss curves complement the accuracy analysis. For a learning rate of 1e-03, the training loss decreases rapidly but the validation loss starts increasing after epoch 8, confirming overfitting. Lower learning rates (5e-04, 2e-04, and 1e-04) show a steady decrease in both training and validation losses, with 5e-04 achieving the smoothest and lowest loss values. The learning rate of 5e-05 shows a slower decline in loss, which aligns with its slower convergence in accuracy.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart illustrates that the test accuracy is highest for the learning rate of 5e-04, slightly outperforming 2e-04 and 1e-04. The learning rate of 1e-03 achieves the lowest test accuracy, likely due to overfitting. The learning rate of 5e-05 achieves reasonable accuracy but is not competitive with the mid-range learning rates.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "The confusion matrix for the best learning rate (5e-04) shows a strong performance with 394 true positives, 104 false positives, 401 true negatives, and 101 false negatives. This indicates a balanced model performance with relatively low misclassification rates for both classes. The false positive and false negative rates are comparable, suggesting no significant bias toward either class.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_eb512937c20e4bc4942bf72e49122e81_proc_3159062/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The first plot shows the training and validation loss for different batch sizes (32, 128, 256) across epochs. The training loss decreases consistently for all batch sizes, indicating proper learning. However, the validation loss fluctuates more, particularly for smaller batch sizes (e.g., 32). This suggests potential overfitting for smaller batch sizes as the model might be memorizing the training data instead of generalizing well. Batch size 128 appears to strike a better balance, as its validation loss is relatively stable and lower compared to other batch sizes.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_loss_curves.png"}, {"analysis": "The second plot illustrates the training and validation accuracy for different batch sizes across epochs. All batch sizes show an improvement in accuracy over epochs, with batch size 128 achieving the most consistent and high performance in both training and validation. Smaller batch sizes (e.g., 32) exhibit larger fluctuations in validation accuracy, which could be attributed to overfitting or instability during training. Batch size 256 shows slightly less fluctuation but does not outperform batch size 128 in overall accuracy.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_accuracy_curves.png"}, {"analysis": "The confusion matrix for the best-performing batch size (128) reveals the model's classification performance. It shows 394 true negatives, 401 true positives, 104 false positives, and 101 false negatives. This indicates a balanced performance with a slight tendency to misclassify some instances. The high true positive and true negative counts suggest that the model is generally effective at distinguishing between the two classes, but further tuning might reduce the false positive and false negative rates.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_51182828a6654822ac342cd2c60bfac8_proc_3159063/spr_bench_confusion_bs_128.png"}], [{"analysis": "The first plot shows the training and validation accuracy across epochs for different dropout rates. All dropout rates converge to around 80% accuracy, with minimal variance across different dropout settings. The training accuracy is slightly higher than the validation accuracy, indicating a small degree of overfitting, particularly for lower dropout rates. The dropout rate of 0.0 (no dropout) achieves the highest and most consistent accuracy, suggesting that dropout may not significantly benefit this task.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The second plot displays the training and validation loss over epochs for various dropout rates. The loss decreases steadily across epochs, with minimal differences between dropout rates. The training loss is consistently lower than the validation loss, further confirming slight overfitting. The dropout rate of 0.0 demonstrates the lowest loss, aligning with the accuracy findings. Higher dropout rates introduce more fluctuation in validation loss, potentially indicating instability in learning.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_loss_curves.png"}, {"analysis": "The third plot summarizes the test accuracy for different dropout rates. All dropout rates achieve nearly identical test accuracy, around 80%, with negligible differences. This result suggests that dropout has minimal impact on the model's generalization performance for this task.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "The fourth plot is a confusion matrix for the best-performing dropout rate (0.0). The model shows balanced performance across both classes, with comparable true positives and true negatives. However, there is a small number of misclassifications in both directions (false positives and false negatives). The confusion matrix indicates robust overall performance, with room for improvement in reducing misclassification rates.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"}], [{"analysis": "The plot shows the training and validation loss over epochs for different weight decay values. All weight decay configurations exhibit a general decrease in loss as training progresses, indicating effective learning. However, the validation loss fluctuates more than the training loss, suggesting potential overfitting, especially at higher epochs. Among the configurations, weight decay of 0.0 and 1e-05 appear to perform slightly better in terms of maintaining lower validation loss, indicating that they may help balance the trade-off between underfitting and overfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_loss_curves_weight_decay.png"}, {"analysis": "This plot illustrates the training and validation accuracy over epochs for different weight decay values. All configurations achieve a rapid improvement in accuracy within the first few epochs, plateauing around 80%. The validation accuracy shows slight fluctuations, likely due to noise or overfitting. Weight decay values of 0.0 and 1e-05 seem to provide marginally better results in terms of stability and final accuracy, aligning with observations from the loss plot.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_accuracy_curves_weight_decay.png"}, {"analysis": "This bar chart compares test accuracy across different weight decay values. The results indicate that test accuracy is relatively insensitive to the choice of weight decay, with all configurations achieving similar performance around 80%. This suggests that the model is robust to variations in weight decay within the tested range.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_test_accuracy_weight_decay.png"}, {"analysis": "The confusion matrix provides insights into the classification performance for the best-performing weight decay value (0.0). The model demonstrates a good balance between true positives and true negatives, indicating that it does not favor one class over the other. However, there is still room for improvement in reducing misclassifications, as evidenced by the off-diagonal values.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9bb49440184e4b41870a98cdd912af0b_proc_3159063/SPR_BENCH_confusion_matrix_best_weight_decay.png"}], [{"analysis": "This plot shows the train and validation cross-entropy loss for different values of d_model (32, 64, 128, 256) over 10 epochs. The training loss consistently decreases across all configurations, indicating effective learning. However, the validation loss fluctuates significantly, especially for d_model=64 and d_model=256, suggesting overfitting or instability in these configurations. The d_model=32 configuration demonstrates the most stable and consistent validation loss, indicating better generalization.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_loss_curves_d_model_sweep.png"}, {"analysis": "This plot illustrates train and validation accuracy across different d_model values over 10 epochs. While all configurations achieve high accuracy early on, d_model=64 and d_model=256 experience significant drops in validation accuracy in later epochs, indicating overfitting or instability. The d_model=32 and d_model=128 configurations maintain stable validation accuracy, with d_model=32 showing the most consistent performance.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_accuracy_curves_d_model_sweep.png"}, {"analysis": "This bar chart compares the final validation accuracy at the last epoch for different d_model values. All configurations achieve similar accuracy, with d_model=32 and d_model=128 slightly outperforming others. This suggests that increasing d_model beyond a certain point does not improve performance and may lead to diminishing returns or instability.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_final_val_accuracy_bar.png"}, {"analysis": "The confusion matrix for d_model=32 provides a detailed breakdown of predictions versus ground truth. The model performs well, with high true positive (401) and true negative (394) counts. However, there are some false positives (104) and false negatives (101), indicating room for improvement in precision and recall. Overall, d_model=32 appears to balance accuracy and stability effectively.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f560bbaebd774b8585949d55f62ae958_proc_3159061/SPR_BENCH_confusion_matrix_d_model_32.png"}], [{"analysis": "This plot shows the cross-entropy loss for training and validation across epochs for different transformer depths. The training loss decreases consistently across all depths, which indicates that the model is learning effectively. However, the validation loss fluctuates more significantly, especially for smaller depths, suggesting potential overfitting or instability in generalization. Larger depths (e.g., 6 layers) seem to stabilize the validation loss better, though the improvement is marginal. The convergence of training and validation losses around epoch 5 suggests that further training may not yield substantial gains.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_loss_curves.png"}, {"analysis": "This plot displays the training and validation accuracy across epochs for different transformer depths. Training accuracy improves steadily for all depths, reaching a plateau around 5 epochs. Validation accuracy also improves but exhibits more noise, particularly for smaller depths. Despite these fluctuations, all depths achieve similar final validation accuracies, suggesting that depth does not significantly impact the model's ability to generalize in this setup.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This bar chart summarizes the final validation accuracy for different transformer depths. All depths achieve nearly identical accuracies, indicating that increasing the number of layers beyond a certain point does not yield noticeable improvements in performance. This suggests that the SPR_BENCH task may not heavily rely on deeper architectures, or the dataset does not sufficiently exploit the additional capacity of deeper models.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_df0324026c334c2a9664e9b84a9b9ba8_proc_3159064/SPR_BENCH_val_accuracy_bar.png"}], [{"analysis": "The first plot shows how accuracy changes across training, validation, and testing datasets as the number of attention heads (nhead) increases. Accuracy improves significantly when nhead increases from 2 to 4, reaching a peak at nhead=4. Beyond this point, accuracy slightly drops for all datasets, suggesting that 4 attention heads provide the optimal balance for capturing sequence dependencies in the SPR task without overfitting or underfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_accuracy_vs_nhead.png"}, {"analysis": "The second plot illustrates the cross-entropy loss for training, validation, and testing datasets as a function of nhead. Loss decreases sharply from nhead=2 to nhead=4, indicating better model performance. However, as nhead increases beyond 4, the loss starts to rise, particularly for the validation dataset, which may indicate overfitting or diminishing returns in performance improvement with additional attention heads.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_loss_vs_nhead.png"}, {"analysis": "The third plot compares validation and test accuracies across different nhead values. The red point highlights the best-performing model based on validation accuracy, which occurs at nhead=4. This point also corresponds to the highest test accuracy, confirming the consistency between validation and test performance. The plot demonstrates a strong correlation between validation and test accuracies, with nhead=4 being the most effective configuration.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6452470054614d678460c4aa4b4a7ce0_proc_3159062/SPR_BENCH_val_vs_test_accuracy.png"}], [{"analysis": "The plot shows the training and validation accuracy over 10 epochs for different dropout rates. All configurations exhibit rapid convergence in the first few epochs, with accuracy stabilizing afterward. Dropout rates between 0.1 and 0.2 seem to provide slightly better validation accuracy compared to other rates, indicating that these dropout values may help the model generalize better. However, the differences in validation accuracy across dropout rates are relatively minor, suggesting that dropout may not be a critical factor in this experiment.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The plot illustrates the training and validation loss trends over 10 epochs for various dropout rates. Loss decreases steadily across all configurations, with minimal differences between dropout rates after the initial epochs. Dropout rates of 0.1 and 0.2 appear to achieve slightly lower validation loss on average, aligning with the accuracy observations. However, the overall impact of dropout on loss reduction is limited, indicating that the model's performance is not highly sensitive to this parameter.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart compares test accuracy for different dropout rates. The test accuracy is consistently around 80% across all dropout rates, with negligible variation. This result suggests that dropout rate tuning has minimal impact on the final test performance, and other hyperparameters or architectural changes may need to be explored to achieve further improvements.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "The confusion matrix for the best dropout rate (0.1) shows a balanced performance across the two classes. The model correctly predicts 394 and 401 instances for class 0 and class 1, respectively, with 104 and 101 misclassifications. This indicates that the model performs similarly well on both classes, with no significant bias. The overall performance aligns with the observed accuracy of approximately 80%, and further fine-tuning may help reduce the misclassification rates.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/SPR_BENCH_confusion_matrix_best_dropout_0.1.png"}], [{"analysis": "This plot illustrates the training and validation accuracy trends for different dropout rates over 10 epochs. All configurations demonstrate an improvement in accuracy in the early epochs, with the validation accuracy stabilizing or slightly fluctuating after epoch 4. The dropout rate of 0.3 appears to provide a balanced performance, as it maintains relatively high validation accuracy while avoiding overfitting. Lower dropout rates, such as 0.0, show a slight tendency towards overfitting, evidenced by the divergence between training and validation accuracy in later epochs.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot shows the training and validation loss trends for various dropout rates. Loss decreases steadily across the epochs for all configurations, with minimal divergence between training and validation loss for dropout rates of 0.2 and 0.3. This indicates that these dropout rates effectively prevent overfitting. Conversely, a dropout rate of 0.0 shows a larger gap between training and validation loss, suggesting overfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_loss_curves.png"}, {"analysis": "This bar chart compares test accuracy across different dropout rates. All dropout rates achieve similar accuracy, around 80%, with a slight edge for dropout rates of 0.2 and 0.3. This suggests that dropout has a limited but consistent impact on generalization performance, with higher dropout rates slightly improving robustness.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "This confusion matrix evaluates the performance of the model with a dropout rate of 0.3. The model achieves a balanced performance across both classes, with 394 true negatives and 401 true positives. The false positives (104) and false negatives (101) are relatively low, indicating good overall classification performance. The results suggest that a dropout rate of 0.3 effectively balances precision and recall.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/SPR_BENCH_confusion_matrix_best_dropout_0.3.png"}], [{"analysis": "The plot shows training and validation accuracy trends for different dropout rates across epochs. All dropout configurations converge to similar accuracy levels by epoch 10, with minor fluctuations in validation accuracy. Dropout 0.0 achieves slightly higher and more stable accuracy compared to other dropout rates, suggesting that dropout regularization may not significantly improve generalization in this task.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_accuracy_curves.png"}, {"analysis": "This plot tracks the training and validation loss trends for various dropout rates. Loss decreases steadily across epochs for all configurations, with dropout 0.0 achieving the lowest validation loss. The remaining dropout rates exhibit slightly higher and more fluctuating validation losses, indicating that dropout may introduce some instability in this context.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_loss_curves.png"}, {"analysis": "The bar chart illustrates test accuracy for different dropout rates, all clustered around 80%, with minimal variation. This indicates that dropout rate has little to no impact on the final test accuracy of the model, reinforcing the observation that regularization through dropout does not substantially affect performance in this case.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "The confusion matrix for the best dropout configuration (0.0) shows a balanced performance across both classes. The model correctly predicts 395 instances of class 0 and 399 instances of class 1, with 103 misclassifications for each class. This indicates that the model is well-calibrated and performs consistently across the two classes.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"}], []], "vlm_feedback_summary": ["The plots demonstrate that the model is performing well, with good convergence\nand generalization. The accuracy is approaching the SOTA benchmark, and the\nconfusion matrix suggests balanced performance across classes. Further analysis\nand fine-tuning could help in achieving or exceeding the SOTA performance.", "The provided plots effectively illustrate the model's performance across\ndifferent epoch budgets. Key insights include the observation of overfitting at\nlower epoch budgets, stabilization of validation accuracy around 80% for the\n30-epoch budget, and reasonable but improvable classification performance as\nindicated by the confusion matrix. Further tuning or architectural enhancements\ncould help address these issues.", "The experimental results suggest that a learning rate of 5e-04 achieves the best\nbalance between training and validation performance, as seen in both accuracy\nand loss curves. This is further supported by the test accuracy and confusion\nmatrix, which highlight the model's strong generalization ability and balanced\nclassification performance. Lower and higher learning rates either converge too\nslowly or lead to overfitting, respectively.", "The analysis highlights that batch size 128 provides the best balance between\ntraining and validation performance, as evidenced by lower loss, higher\naccuracy, and a balanced confusion matrix. Smaller batch sizes suffer from\ninstability and potential overfitting, while larger batch sizes, though stable,\ndo not outperform batch size 128. The model demonstrates strong classification\ncapabilities with room for improvement in reducing misclassification errors.", "The plots indicate that dropout has a minimal impact on the model's performance,\nwith the best results achieved at a dropout rate of 0.0. The model achieves\nconsistent accuracy and loss trends across training and validation, with slight\noverfitting observed. The confusion matrix highlights balanced performance\nacross classes but reveals some misclassification errors. Overall, the results\nare promising but suggest the need for further fine-tuning to enhance\ngeneralization and reduce errors.", "The plots provide valuable insights into the impact of weight decay on model\nperformance. While all configurations achieve similar final accuracy, weight\ndecay values of 0.0 and 1e-05 seem to offer marginal advantages. The confusion\nmatrix highlights balanced classification performance but also points to areas\nfor reducing errors. Overall, the results suggest that the proposed approach is\neffective but could benefit from further fine-tuning.", "The plots indicate that d_model=32 provides the most consistent and stable\nperformance across metrics, with minimal overfitting and balanced accuracy.\nIncreasing d_model beyond this value leads to instability and diminishing\nreturns, as seen in the fluctuating validation loss and accuracy for d_model=64\nand d_model=256. The confusion matrix further highlights the effectiveness of\nd_model=32, with a good balance of true positives and true negatives. Future\ntuning should focus on refining hyperparameters around this configuration to\nfurther improve generalization and reduce false predictions.", "The plots indicate that while deeper transformer architectures stabilize\ntraining and validation metrics, they do not significantly improve final\nvalidation accuracy. This suggests that the SPR_BENCH task may not require\ndeeper architectures for optimal performance. Further experimentation with other\nhyperparameters or architectural adjustments may be needed to achieve meaningful\ngains.", "The plots provide clear insights into the relationship between the number of\nattention heads and model performance on SPR tasks. Accuracy and loss metrics\nconfirm that nhead=4 is the optimal configuration, achieving the highest\naccuracy and lowest loss while avoiding overfitting. The results validate the\nhypothesis that contextual embeddings can enhance SPR task performance, with\noptimal hyperparameter tuning playing a critical role.", "The plots indicate that dropout rate has a limited impact on model performance,\nwith test accuracy consistently around 80% across all configurations. Dropout\nrates of 0.1 and 0.2 yield slightly better validation accuracy and loss. The\nconfusion matrix confirms balanced performance across classes, but further\noptimization is needed for significant improvements.", "The plots demonstrate that dropout tuning has a noticeable impact on the model's\nability to generalize. A dropout rate of 0.3 provides the best trade-off between\npreventing overfitting and achieving high accuracy. Both accuracy and loss\ntrends confirm the model's robustness at this dropout rate. The confusion matrix\nfurther validates the effectiveness of this configuration, showing balanced\nclassification performance across classes.", "The analysis indicates that dropout regularization has limited impact on\nperformance for this task. Dropout 0.0 achieves slightly better and more stable\naccuracy and loss metrics, while all configurations converge to similar test\naccuracy. The confusion matrix suggests balanced performance across classes,\nwith no significant bias.", "[]"], "exec_time": [5.69148588180542, 15.09608268737793, 22.41311526298523, 11.847506284713745, 15.551154375076294, 15.813275337219238, 16.977099657058716, 25.33176827430725, 13.049391269683838, 17.03395414352417, 17.13613271713257, 18.133572101593018, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['128']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# proceed only if data are present\nif \"SPR_BENCH\" in experiment_data:\n    data = experiment_data[\"SPR_BENCH\"]\n    train_loss = np.array(data[\"losses\"][\"train_loss\"])\n    val_loss = np.array(data[\"losses\"][\"val_loss\"])\n    train_acc = np.array(data[\"metrics\"][\"train_acc\"])\n    val_acc = np.array(data[\"metrics\"][\"val_acc\"])\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # ----------------------------------------------------------------- #\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 2. Accuracy curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_acc, label=\"Train Accuracy\")\n        plt.plot(epochs, val_acc, label=\"Validation Accuracy\")\n        plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 3. Confusion matrix on test set\n    try:\n        preds = np.array(data[\"predictions\"])\n        gts = np.array(data[\"ground_truth\"])\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n        plt.close()\n\n        test_acc = (preds == gts).mean()\n        print(f\"Test accuracy from saved predictions: {test_acc*100:.2f}%\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\nelse:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# helper: gather per-budget info\nfinal_test_acc = {}\nbest_tag, best_val_loss = None, float(\"inf\")\n\nfor tag, rec in experiment_data.get(\"epochs\", {}).items():\n    val_losses = rec[\"losses\"][\"val\"]\n    if val_losses and min(val_losses) < best_val_loss:\n        best_val_loss = min(val_losses)\n        best_tag = tag\n    # final test accuracy is last element of metrics['val']? we stored separately in print, but gather here from val\n    # safer: compute from predictions vs ground_truth\n    preds, gts = np.array(rec[\"predictions\"]), np.array(rec[\"ground_truth\"])\n    if len(preds) == len(gts) and len(preds):\n        final_test_acc[tag] = (preds == gts).mean()\n\nprint(\"Final test accuracies:\")\nfor t, acc in final_test_acc.items():\n    print(f\"  {t}: {acc*100:.2f}%\")\nprint(f\"Best tag by val loss: {best_tag} (min val loss {best_val_loss:.4f})\")\n\n# ------------------------------------------------------------------#\n# 1) Loss-vs-Epoch plot\ntry:\n    plt.figure()\n    for tag, rec in experiment_data.get(\"epochs\", {}).items():\n        plt.plot(rec[\"losses\"][\"train\"], label=f\"{tag}-train\")\n        plt.plot(rec[\"losses\"][\"val\"], linestyle=\"--\", label=f\"{tag}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves across Epoch Budgets\\nDataset: SPR_BENCH\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------#\n# 2) Accuracy-vs-Epoch plot\ntry:\n    plt.figure()\n    for tag, rec in experiment_data.get(\"epochs\", {}).items():\n        plt.plot(rec[\"metrics\"][\"train\"], label=f\"{tag}-train\")\n        plt.plot(rec[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"{tag}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy Curves across Epoch Budgets\\nDataset: SPR_BENCH\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------#\n# 3) Confusion matrix for best model\ntry:\n    if best_tag and best_tag in experiment_data[\"epochs\"]:\n        rec = experiment_data[\"epochs\"][best_tag]\n        preds, gts = np.array(rec[\"predictions\"]), np.array(rec[\"ground_truth\"])\n        num_labels = int(max(gts.max(), preds.max())) + 1 if len(gts) else 0\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for p, g in zip(preds, gts):\n            cm[g, p] += 1\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        for i in range(num_labels):\n            for j in range(num_labels):\n                plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"Confusion Matrix (Best: {best_tag})\\nDataset: SPR_BENCH\")\n        fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_{best_tag}.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------------------------------#\n# load experiment data                                                     #\n# -------------------------------------------------------------------------#\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    runs = experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"runs\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    runs = []\n\n# helper to ensure we have something to plot\nif not runs:\n    print(\"No runs found \u2013 exiting plotting script.\")\nelse:\n    # ---------------------------------------------------------------------#\n    # 1. accuracy curves                                                   #\n    # ---------------------------------------------------------------------#\n    try:\n        plt.figure()\n        for run in runs:\n            epochs = np.arange(1, len(run[\"metrics\"][\"train\"]) + 1)\n            plt.plot(\n                epochs,\n                run[\"metrics\"][\"train\"],\n                label=f\"train lr={run['lr']:.0e}\",\n            )\n            plt.plot(\n                epochs,\n                run[\"metrics\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"val lr={run['lr']:.0e}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Train vs Validation Accuracy Curves\")\n        plt.legend(fontsize=7)\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curves: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------------------------#\n    # 2. loss curves                                                       #\n    # ---------------------------------------------------------------------#\n    try:\n        plt.figure()\n        for run in runs:\n            epochs = np.arange(1, len(run[\"losses\"][\"train\"]) + 1)\n            plt.plot(\n                epochs,\n                run[\"losses\"][\"train\"],\n                label=f\"train lr={run['lr']:.0e}\",\n            )\n            plt.plot(\n                epochs,\n                run[\"losses\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"val lr={run['lr']:.0e}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"SPR_BENCH: Train vs Validation Loss Curves\")\n        plt.legend(fontsize=7)\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------------------------#\n    # 3. test accuracy bar plot                                            #\n    # ---------------------------------------------------------------------#\n    try:\n        lrs = [run[\"lr\"] for run in runs]\n        test_accs = [run[\"test_acc\"] for run in runs]\n        plt.figure()\n        plt.bar([f\"{lr:.0e}\" for lr in lrs], test_accs, color=\"skyblue\")\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(\"SPR_BENCH: Test Accuracy vs Learning Rate\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test accuracy bar plot: {e}\")\n        plt.close()\n\n    # ---------------------------------------------------------------------#\n    # 4. confusion matrix for best LR                                      #\n    # ---------------------------------------------------------------------#\n    try:\n        # pick run with highest test accuracy\n        best_run = max(runs, key=lambda r: r[\"test_acc\"])\n        preds = np.asarray(best_run[\"predictions\"])\n        gts = np.asarray(best_run[\"ground_truth\"])\n        classes = np.unique(np.concatenate([preds, gts]))\n        num_classes = len(classes)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[t, p] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xlabel(\"Predicted label\")\n        plt.ylabel(\"True label\")\n        plt.title(\n            f\"SPR_BENCH: Confusion Matrix (Best LR={best_run['lr']:.0e})\\nLeft: Ground Truth, Right: Predictions\"\n        )\n        tick_marks = np.arange(num_classes)\n        plt.xticks(tick_marks, tick_marks)\n        plt.yticks(tick_marks, tick_marks)\n        # annotate cells\n        for i in range(num_classes):\n            for j in range(num_classes):\n                plt.text(\n                    j,\n                    i,\n                    cm[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n                    fontsize=6,\n                )\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------- load data -------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# ---------------------- gather summary --------------------------------------\nsummary = {}\nfor exp_key, exp_dict in experiment_data.get(\"batch_size\", {}).items():\n    test_preds = np.array(exp_dict[\"predictions\"])\n    test_gt = np.array(exp_dict[\"ground_truth\"])\n    acc = (test_preds == test_gt).mean()\n    summary[exp_key] = acc\n# Print results\nprint(\"Test accuracy summary (SPR_BENCH):\")\nfor k, v in summary.items():\n    print(f\"  {k}: {v:.4f}\")\nbest_exp = max(summary, key=summary.get) if summary else None\n\n# ---------------------- figure 1: Loss curves -------------------------------\ntry:\n    plt.figure()\n    for exp_key, exp_dict in experiment_data.get(\"batch_size\", {}).items():\n        plt.plot(exp_dict[\"losses\"][\"train_loss\"], label=f\"{exp_key} train\")\n        plt.plot(exp_dict[\"losses\"][\"val_loss\"], label=f\"{exp_key} val\", ls=\"--\")\n    plt.title(\"SPR_BENCH Training & Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ---------------------- figure 2: Accuracy curves ---------------------------\ntry:\n    plt.figure()\n    for exp_key, exp_dict in experiment_data.get(\"batch_size\", {}).items():\n        plt.plot(exp_dict[\"metrics\"][\"train_acc\"], label=f\"{exp_key} train\")\n        plt.plot(exp_dict[\"metrics\"][\"val_acc\"], label=f\"{exp_key} val\", ls=\"--\")\n    plt.title(\"SPR_BENCH Training & Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ---------------------- figure 3: Confusion matrix for best model ----------\ntry:\n    if best_exp:\n        preds = experiment_data[\"batch_size\"][best_exp][\"predictions\"]\n        gts = experiment_data[\"batch_size\"][best_exp][\"ground_truth\"]\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(f\"SPR_BENCH Confusion Matrix (Best {best_exp})\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, f\"spr_bench_confusion_{best_exp}.png\")\n        plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory for plots\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\n# extract SPR_BENCH results\nbench = experiment_data.get(\"dropout_tuning\", {}).get(\"SPR_BENCH\", {})\nif not bench:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\ndropouts = sorted(bench.keys(), key=float)\nepochs = len(bench[dropouts[0]][\"metrics\"][\"train_acc\"])\n\n# gather stats\ntest_accs = {dp: bench[dp][\"test_acc\"] for dp in dropouts}\nbest_dp = max(test_accs, key=test_accs.get)\nbest_acc = test_accs[best_dp]\n\nprint(\"=== Test Accuracies ===\")\nfor dp, acc in test_accs.items():\n    print(f\"Dropout {dp}: {acc*100:.2f}%\")\nprint(f\"\\nBest dropout: {best_dp} (test acc {best_acc*100:.2f}%)\")\n\n# --------------------------------------------------------------------- #\n# 1) accuracy curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"metrics\"][\"train_acc\"]\n        val = bench[dp][\"metrics\"][\"val_acc\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Training and Validation Accuracy vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 2) loss curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"losses\"][\"train_loss\"]\n        val = bench[dp][\"losses\"][\"val_loss\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training and Validation Loss vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 3) bar chart of test accuracies\ntry:\n    plt.figure()\n    x = np.arange(len(dropouts))\n    y = [test_accs[dp] * 100 for dp in dropouts]\n    plt.bar(x, y, color=\"skyblue\")\n    plt.xticks(x, dropouts)\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\"SPR_BENCH: Test Accuracy by Dropout Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 4) confusion matrix for best dropout\ntry:\n    preds = np.array(bench[best_dp][\"predictions\"])\n    gts = np.array(bench[best_dp][\"ground_truth\"])\n    labels = np.unique(gts)\n    cm = np.zeros((len(labels), len(labels)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR_BENCH: Confusion Matrix (Best dropout={best_dp})\")\n    plt.xticks(labels)\n    plt.yticks(labels)\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_best_dropout_{best_dp}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -----------------------------------------------------------------------------#\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data is not None:\n    exp = experiment_data[\"weight_decay\"][\"SPR_BENCH\"]\n    wds = exp[\"settings\"]\n    train_losses = exp[\"losses\"][\"train\"]  # list[len(wds)][epochs]\n    val_losses = exp[\"losses\"][\"val\"]\n    train_accs = exp[\"metrics\"][\"train\"]\n    val_accs = exp[\"metrics\"][\"val\"]\n    test_accs = exp[\"test_acc\"]\n    gts = np.array(exp[\"ground_truth\"])\n\n    # -------------------------------------------------------------------------#\n    # 1) Loss curves\n    try:\n        plt.figure()\n        for i, wd in enumerate(wds):\n            epochs = np.arange(1, len(train_losses[i]) + 1)\n            plt.plot(epochs, train_losses[i], label=f\"train (wd={wd})\", linestyle=\"-\")\n            plt.plot(epochs, val_losses[i], label=f\"val   (wd={wd})\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Training/Validation Loss vs Epoch\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_weight_decay.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # -------------------------------------------------------------------------#\n    # 2) Accuracy curves\n    try:\n        plt.figure()\n        for i, wd in enumerate(wds):\n            epochs = np.arange(1, len(train_accs[i]) + 1)\n            plt.plot(epochs, train_accs[i], label=f\"train (wd={wd})\", linestyle=\"-\")\n            plt.plot(epochs, val_accs[i], label=f\"val   (wd={wd})\", linestyle=\"--\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Training/Validation Accuracy vs Epoch\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves_weight_decay.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # -------------------------------------------------------------------------#\n    # 3) Final test accuracy bar chart\n    try:\n        plt.figure()\n        plt.bar([str(wd) for wd in wds], test_accs, color=\"skyblue\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Test Accuracy\")\n        plt.xlabel(\"Weight Decay\")\n        plt.title(\"SPR_BENCH: Test Accuracy vs Weight Decay\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_weight_decay.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test-accuracy plot: {e}\")\n        plt.close()\n\n    # -------------------------------------------------------------------------#\n    # 4) Confusion matrix for best model\n    try:\n        best_idx = int(np.argmax(test_accs))\n        preds = np.array(exp[\"predictions\"][best_idx])\n        num_labels = preds.max() + 1\n        cm = np.zeros((num_labels, num_labels), dtype=int)\n        for t, p in zip(gts, preds):\n            cm[int(t), int(p)] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        plt.title(f\"SPR_BENCH Confusion Matrix (Best wd={wds[best_idx]})\")\n        plt.xticks(range(num_labels))\n        plt.yticks(range(num_labels))\n        fname = os.path.join(\n            working_dir, \"SPR_BENCH_confusion_matrix_best_weight_decay.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion-matrix plot: {e}\")\n        plt.close()\n\n    # -------------------------------------------------------------------------#\n    # Print numeric summary\n    print(\"Weight Decay  |  Test Accuracy\")\n    for wd, acc in zip(wds, test_accs):\n        print(f\"{wd:<12}  {acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------- load data ---------------- #\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nspr_data = experiment_data.get(\"d_model_tuning\", {}).get(\"SPR_BENCH\", {})\nif not spr_data:\n    print(\"No SPR_BENCH data available.\")\n    exit()\n\ntags = sorted(spr_data.keys(), key=lambda x: int(x.split(\"_\")[-1]))  # e.g. d_model_32\nepochs = len(next(iter(spr_data.values()))[\"losses\"][\"train\"])\n\n# ---------------- figure 1: loss curves ---------------- #\ntry:\n    plt.figure(figsize=(6, 4))\n    for tag in tags:\n        ep = np.arange(1, epochs + 1)\n        plt.plot(\n            ep, spr_data[tag][\"losses\"][\"train\"], label=f\"{tag}-train\", linestyle=\"-\"\n        )\n        plt.plot(ep, spr_data[tag][\"losses\"][\"val\"], label=f\"{tag}-val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH: Train vs Val Loss (d_model sweep)\")\n    plt.legend(fontsize=7)\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves_d_model_sweep.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# ---------------- figure 2: accuracy curves ---------------- #\ntry:\n    plt.figure(figsize=(6, 4))\n    for tag in tags:\n        ep = np.arange(1, epochs + 1)\n        plt.plot(\n            ep, spr_data[tag][\"metrics\"][\"train\"], label=f\"{tag}-train\", linestyle=\"-\"\n        )\n        plt.plot(\n            ep, spr_data[tag][\"metrics\"][\"val\"], label=f\"{tag}-val\", linestyle=\"--\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Train vs Val Accuracy (d_model sweep)\")\n    plt.legend(fontsize=7)\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves_d_model_sweep.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# ---------------- figure 3: final test accuracy bar chart ---------------- #\ntry:\n    test_accs = [spr_data[tag][\"metrics\"][\"val\"][-1] for tag in tags]  # val final epoch\n    plt.figure(figsize=(5, 3.5))\n    plt.bar(tags, test_accs, color=\"skyblue\")\n    plt.ylabel(\"Validation Accuracy at Last Epoch\")\n    plt.title(\"SPR_BENCH: Final Validation Accuracy by d_model\")\n    plt.xticks(rotation=45, ha=\"right\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_final_val_accuracy_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ---------------- confusion matrix for best model ---------------- #\ntry:\n    # Determine best d_model by best validation accuracy last epoch\n    best_idx = int(np.argmax(test_accs))\n    best_tag = tags[best_idx]\n    preds = np.array(spr_data[best_tag][\"predictions\"])\n    gts = np.array(spr_data[best_tag][\"ground_truth\"])\n    num_labels = len(np.unique(gts))\n    cm = np.zeros((num_labels, num_labels), dtype=int)\n    for gt, pr in zip(gts, preds):\n        cm[gt, pr] += 1\n\n    plt.figure(figsize=(4, 4))\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(f\"SPR_BENCH Confusion Matrix (best {best_tag})\")\n    for i in range(num_labels):\n        for j in range(num_labels):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    fname = os.path.join(working_dir, f\"SPR_BENCH_confusion_matrix_{best_tag}.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n\n# ---------------- print evaluation metric ---------------- #\nbest_val_acc = test_accs[best_idx]\nprint(\n    f\"Best d_model: {best_tag}  |  Final validation accuracy: {best_val_acc*100:.2f}%\"\n)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------#\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Retrieve SPR_BENCH results\nspr_data = experiment_data.get(\"num_layers\", {}).get(\"SPR_BENCH\", {})\ndepth_keys = sorted(\n    spr_data.keys(), key=lambda k: int(k.split(\"_\")[1])\n)  # e.g. 'layers_2' -> 2\n\n# Aggregate metrics for printing\nresults_table = []\n\n# ------------------------------------------------------------------#\n# Plot 1: Loss curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for dk in depth_keys:\n        epochs = range(1, len(spr_data[dk][\"losses\"][\"train\"]) + 1)\n        plt.plot(\n            epochs, spr_data[dk][\"losses\"][\"train\"], label=f\"{dk}-train\", linestyle=\"-\"\n        )\n        plt.plot(\n            epochs, spr_data[dk][\"losses\"][\"val\"], label=f\"{dk}-val\", linestyle=\"--\"\n        )\n    plt.title(\"SPR_BENCH: Training vs Validation Loss\\n(Ablating Transformer Depth)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend(fontsize=\"small\", ncol=2)\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------#\n# Plot 2: Accuracy curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for dk in depth_keys:\n        epochs = range(1, len(spr_data[dk][\"metrics\"][\"train\"]) + 1)\n        plt.plot(\n            epochs, spr_data[dk][\"metrics\"][\"train\"], label=f\"{dk}-train\", linestyle=\"-\"\n        )\n        plt.plot(\n            epochs, spr_data[dk][\"metrics\"][\"val\"], label=f\"{dk}-val\", linestyle=\"--\"\n        )\n    plt.title(\n        \"SPR_BENCH: Training vs Validation Accuracy\\n(Ablating Transformer Depth)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(fontsize=\"small\", ncol=2)\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------#\n# Prepare bar-plot data & compute test accuracy\nval_final, test_final = [], []\nfor dk in depth_keys:\n    val_acc = (\n        spr_data[dk][\"metrics\"][\"val\"][-1] if spr_data[dk][\"metrics\"][\"val\"] else 0\n    )\n    preds = np.array(spr_data[dk][\"predictions\"])\n    gts = np.array(spr_data[dk][\"ground_truth\"])\n    test_acc = (preds == gts).mean() if len(gts) else 0\n    val_final.append(val_acc)\n    test_final.append(test_acc)\n    results_table.append((dk, val_acc, test_acc))\n\n# ------------------------------------------------------------------#\n# Plot 3: Final validation accuracy bar chart\ntry:\n    plt.figure(figsize=(6, 4))\n    x = np.arange(len(depth_keys))\n    plt.bar(x, val_final, color=\"steelblue\")\n    plt.xticks(x, [k.replace(\"layers_\", \"\") for k in depth_keys])\n    plt.title(\"SPR_BENCH: Final Validation Accuracy by Depth\")\n    plt.xlabel(\"Num Transformer Layers\")\n    plt.ylabel(\"Validation Accuracy\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_val_accuracy_bar.png\")\n    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating bar chart: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------#\n# Print summary metrics\nprint(\"\\nDepth | Final Val Acc | Test Acc\")\nfor dk, v_acc, t_acc in results_table:\n    n_layers = dk.replace(\"layers_\", \"\")\n    print(f\"{n_layers:>5} | {v_acc:.4f}       | {t_acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------------------------------------------------------------- #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# -------------------------------------------------------------------------- #\ndef get_spr_info(edict):\n    nh_vals = [cfg[\"nhead\"] for cfg in edict[\"configs\"]]\n    t_acc, v_acc, s_acc = (\n        edict[\"metrics\"][k] for k in (\"train_acc\", \"val_acc\", \"test_acc\")\n    )\n    t_loss, v_loss, s_loss = (\n        edict[\"losses\"][k] for k in (\"train_loss\", \"val_loss\", \"test_loss\")\n    )\n    return nh_vals, t_acc, v_acc, s_acc, t_loss, v_loss, s_loss\n\n\nif experiment_data:\n    ed = experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"]\n    nheads, tr_acc, val_acc, te_acc, tr_loss, val_loss, te_loss = get_spr_info(ed)\n\n    # print quick table\n    print(\"nhead | train_acc | val_acc | test_acc\")\n    for nh, ta, va, tsa in zip(nheads, tr_acc, val_acc, te_acc):\n        print(f\"{nh:5d} | {ta:.3f}     | {va:.3f}   | {tsa:.3f}\")\n    best_idx = int(np.argmax(val_acc))\n    print(\n        f\"\\nBest nhead={nheads[best_idx]} | val_acc={val_acc[best_idx]:.3f} | test_acc={te_acc[best_idx]:.3f}\"\n    )\n\n    # ------------------ Plot 1: accuracy vs nhead -------------------------- #\n    try:\n        plt.figure()\n        plt.plot(nheads, tr_acc, \"o-\", label=\"Train\")\n        plt.plot(nheads, val_acc, \"s-\", label=\"Validation\")\n        plt.plot(nheads, te_acc, \"d-\", label=\"Test\")\n        plt.xlabel(\"nhead\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"SPR_BENCH: Accuracy vs nhead\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_vs_nhead.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot: {e}\")\n        plt.close()\n\n    # ------------------ Plot 2: loss vs nhead ------------------------------ #\n    try:\n        plt.figure()\n        plt.plot(nheads, tr_loss, \"o-\", label=\"Train\")\n        plt.plot(nheads, val_loss, \"s-\", label=\"Validation\")\n        plt.plot(nheads, te_loss, \"d-\", label=\"Test\")\n        plt.xlabel(\"nhead\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH: Loss vs nhead\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_vs_nhead.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # ------------------ Plot 3: val vs test scatter ------------------------ #\n    try:\n        plt.figure()\n        plt.scatter(val_acc, te_acc, c=\"blue\")\n        plt.scatter(val_acc[best_idx], te_acc[best_idx], c=\"red\", label=\"Best val\")\n        for i, nh in enumerate(nheads):\n            plt.annotate(str(nh), (val_acc[i], te_acc[i]))\n        plt.xlabel(\"Validation Accuracy\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(\"SPR_BENCH: Validation vs Test Accuracy per nhead\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_val_vs_test_accuracy.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating scatter plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory for plots\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\n# extract SPR_BENCH results\nbench = experiment_data.get(\"dropout_tuning\", {}).get(\"SPR_BENCH\", {})\nif not bench:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\ndropouts = sorted(bench.keys(), key=float)\nepochs = len(bench[dropouts[0]][\"metrics\"][\"train_acc\"])\n\n# gather stats\ntest_accs = {dp: bench[dp][\"test_acc\"] for dp in dropouts}\nbest_dp = max(test_accs, key=test_accs.get)\nbest_acc = test_accs[best_dp]\n\nprint(\"=== Test Accuracies ===\")\nfor dp, acc in test_accs.items():\n    print(f\"Dropout {dp}: {acc*100:.2f}%\")\nprint(f\"\\nBest dropout: {best_dp} (test acc {best_acc*100:.2f}%)\")\n\n# --------------------------------------------------------------------- #\n# 1) accuracy curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"metrics\"][\"train_acc\"]\n        val = bench[dp][\"metrics\"][\"val_acc\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Training and Validation Accuracy vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 2) loss curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"losses\"][\"train_loss\"]\n        val = bench[dp][\"losses\"][\"val_loss\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training and Validation Loss vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 3) bar chart of test accuracies\ntry:\n    plt.figure()\n    x = np.arange(len(dropouts))\n    y = [test_accs[dp] * 100 for dp in dropouts]\n    plt.bar(x, y, color=\"skyblue\")\n    plt.xticks(x, dropouts)\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\"SPR_BENCH: Test Accuracy by Dropout Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 4) confusion matrix for best dropout\ntry:\n    preds = np.array(bench[best_dp][\"predictions\"])\n    gts = np.array(bench[best_dp][\"ground_truth\"])\n    labels = np.unique(gts)\n    cm = np.zeros((len(labels), len(labels)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR_BENCH: Confusion Matrix (Best dropout={best_dp})\")\n    plt.xticks(labels)\n    plt.yticks(labels)\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_best_dropout_{best_dp}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory for plots\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\n# extract SPR_BENCH results\nbench = experiment_data.get(\"dropout_tuning\", {}).get(\"SPR_BENCH\", {})\nif not bench:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\ndropouts = sorted(bench.keys(), key=float)\nepochs = len(bench[dropouts[0]][\"metrics\"][\"train_acc\"])\n\n# gather stats\ntest_accs = {dp: bench[dp][\"test_acc\"] for dp in dropouts}\nbest_dp = max(test_accs, key=test_accs.get)\nbest_acc = test_accs[best_dp]\n\nprint(\"=== Test Accuracies ===\")\nfor dp, acc in test_accs.items():\n    print(f\"Dropout {dp}: {acc*100:.2f}%\")\nprint(f\"\\nBest dropout: {best_dp} (test acc {best_acc*100:.2f}%)\")\n\n# --------------------------------------------------------------------- #\n# 1) accuracy curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"metrics\"][\"train_acc\"]\n        val = bench[dp][\"metrics\"][\"val_acc\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Training and Validation Accuracy vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 2) loss curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"losses\"][\"train_loss\"]\n        val = bench[dp][\"losses\"][\"val_loss\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training and Validation Loss vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 3) bar chart of test accuracies\ntry:\n    plt.figure()\n    x = np.arange(len(dropouts))\n    y = [test_accs[dp] * 100 for dp in dropouts]\n    plt.bar(x, y, color=\"skyblue\")\n    plt.xticks(x, dropouts)\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\"SPR_BENCH: Test Accuracy by Dropout Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 4) confusion matrix for best dropout\ntry:\n    preds = np.array(bench[best_dp][\"predictions\"])\n    gts = np.array(bench[best_dp][\"ground_truth\"])\n    labels = np.unique(gts)\n    cm = np.zeros((len(labels), len(labels)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR_BENCH: Confusion Matrix (Best dropout={best_dp})\")\n    plt.xticks(labels)\n    plt.yticks(labels)\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_best_dropout_{best_dp}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory for plots\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\n# extract SPR_BENCH results\nbench = experiment_data.get(\"dropout_tuning\", {}).get(\"SPR_BENCH\", {})\nif not bench:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\ndropouts = sorted(bench.keys(), key=float)\nepochs = len(bench[dropouts[0]][\"metrics\"][\"train_acc\"])\n\n# gather stats\ntest_accs = {dp: bench[dp][\"test_acc\"] for dp in dropouts}\nbest_dp = max(test_accs, key=test_accs.get)\nbest_acc = test_accs[best_dp]\n\nprint(\"=== Test Accuracies ===\")\nfor dp, acc in test_accs.items():\n    print(f\"Dropout {dp}: {acc*100:.2f}%\")\nprint(f\"\\nBest dropout: {best_dp} (test acc {best_acc*100:.2f}%)\")\n\n# --------------------------------------------------------------------- #\n# 1) accuracy curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"metrics\"][\"train_acc\"]\n        val = bench[dp][\"metrics\"][\"val_acc\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Training and Validation Accuracy vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 2) loss curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"losses\"][\"train_loss\"]\n        val = bench[dp][\"losses\"][\"val_loss\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training and Validation Loss vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 3) bar chart of test accuracies\ntry:\n    plt.figure()\n    x = np.arange(len(dropouts))\n    y = [test_accs[dp] * 100 for dp in dropouts]\n    plt.bar(x, y, color=\"skyblue\")\n    plt.xticks(x, dropouts)\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\"SPR_BENCH: Test Accuracy by Dropout Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 4) confusion matrix for best dropout\ntry:\n    preds = np.array(bench[best_dp][\"predictions\"])\n    gts = np.array(bench[best_dp][\"ground_truth\"])\n    labels = np.unique(gts)\n    cm = np.zeros((len(labels), len(labels)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR_BENCH: Confusion Matrix (Best dropout={best_dp})\")\n    plt.xticks(labels)\n    plt.yticks(labels)\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_best_dropout_{best_dp}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------------------------------------------------------- #\n# basic setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# paths to results (provided by the system)\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_be549cedf21f4fd08a55c4b587c4973e_proc_3159063/experiment_data.npy\",\n    \"experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_ef5da56a675846ef9a5ca7aae9a29cfd_proc_3159061/experiment_data.npy\",\n    \"experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9da198f02965469ca21acf01847e8b4a_proc_3159062/experiment_data.npy\",\n]\n\n# detect root folder (default to cwd if env var not set)\nroot = os.getenv(\"AI_SCIENTIST_ROOT\", os.getcwd())\n\nall_experiment_data = []\ntry:\n    for p in experiment_data_path_list:\n        full_path = os.path.join(root, p)\n        if not os.path.isfile(full_path):\n            print(f\"Missing file: {full_path}\")\n            continue\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nif len(all_experiment_data) == 0:\n    print(\"No experiment data loaded \u2014 exiting.\")\n    exit()\n\n# --------------------------------------------------------------------- #\n# collect SPR_BENCH data across runs\nruns_bench = []\nfor data in all_experiment_data:\n    bench = data.get(\"dropout_tuning\", {}).get(\"SPR_BENCH\", {})\n    if bench:\n        runs_bench.append(bench)\n\nif len(runs_bench) == 0:\n    print(\"No SPR_BENCH section found in any run \u2014 exiting.\")\n    exit()\n\n# assume all runs share identical dropout settings\ndropouts = sorted(list(runs_bench[0].keys()), key=float)\n\n# sanity check: epoch length consistency\ntry:\n    EPOCHS = len(runs_bench[0][dropouts[0]][\"metrics\"][\"train_acc\"])\n    for run in runs_bench:\n        for dp in dropouts:\n            assert len(run[dp][\"metrics\"][\"train_acc\"]) == EPOCHS\nexcept AssertionError:\n    print(\"Mismatch in epoch counts across runs \u2013 unable to aggregate.\")\n    exit()\n\nnum_runs = len(runs_bench)\n\n# containers\nmean_curves = {\n    dp: {\"train\": [], \"val\": [], \"train_loss\": [], \"val_loss\": []} for dp in dropouts\n}\nse_curves = {\n    dp: {\"train\": [], \"val\": [], \"train_loss\": [], \"val_loss\": []} for dp in dropouts\n}\ntest_acc_stats = {dp: {\"mean\": None, \"se\": None} for dp in dropouts}\n\n# --------------------------------------------------------------------- #\n# compute statistics\nfor dp in dropouts:\n    # gather epoch-wise arrays of shape (num_runs, epochs)\n    tr_arr = np.array([run[dp][\"metrics\"][\"train_acc\"] for run in runs_bench])\n    val_arr = np.array([run[dp][\"metrics\"][\"val_acc\"] for run in runs_bench])\n    tr_loss_arr = np.array([run[dp][\"losses\"][\"train_loss\"] for run in runs_bench])\n    val_loss_arr = np.array([run[dp][\"losses\"][\"val_loss\"] for run in runs_bench])\n\n    # mean & standard error\n    mean_curves[dp][\"train\"] = tr_arr.mean(axis=0)\n    mean_curves[dp][\"val\"] = val_arr.mean(axis=0)\n    mean_curves[dp][\"train_loss\"] = tr_loss_arr.mean(axis=0)\n    mean_curves[dp][\"val_loss\"] = val_loss_arr.mean(axis=0)\n\n    se_curves[dp][\"train\"] = tr_arr.std(axis=0, ddof=1) / np.sqrt(num_runs)\n    se_curves[dp][\"val\"] = val_arr.std(axis=0, ddof=1) / np.sqrt(num_runs)\n    se_curves[dp][\"train_loss\"] = tr_loss_arr.std(axis=0, ddof=1) / np.sqrt(num_runs)\n    se_curves[dp][\"val_loss\"] = val_loss_arr.std(axis=0, ddof=1) / np.sqrt(num_runs)\n\n    # test accuracy statistics\n    test_vals = np.array([run[dp][\"test_acc\"] for run in runs_bench])\n    test_acc_stats[dp][\"mean\"] = test_vals.mean()\n    test_acc_stats[dp][\"se\"] = test_vals.std(ddof=1) / np.sqrt(num_runs)\n\n# print summary table\nprint(\"\\n=== Aggregated Test Accuracies (mean \u00b1 SE) ===\")\nfor dp in dropouts:\n    m = test_acc_stats[dp][\"mean\"] * 100\n    s = test_acc_stats[dp][\"se\"] * 100\n    print(f\"Dropout {dp}: {m:.2f}% \u00b1 {s:.2f}%\")\n\n# --------------------------------------------------------------------- #\n# Plot 1: accuracy curves with SE envelope\ntry:\n    plt.figure()\n    epochs_axis = np.arange(1, EPOCHS + 1)\n    for dp in dropouts:\n        m_tr = mean_curves[dp][\"train\"]\n        se_tr = se_curves[dp][\"train\"]\n        m_val = mean_curves[dp][\"val\"]\n        se_val = se_curves[dp][\"val\"]\n\n        # plot mean lines\n        plt.plot(epochs_axis, m_tr, label=f\"{dp} train\", linewidth=1.5)\n        plt.plot(epochs_axis, m_val, label=f\"{dp} val\", linestyle=\"--\", linewidth=1.5)\n\n        # shaded SE\n        plt.fill_between(epochs_axis, m_tr - se_tr, m_tr + se_tr, alpha=0.2)\n        plt.fill_between(epochs_axis, m_val - se_val, m_val + se_val, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\n        \"SPR_BENCH: Mean Training/Validation Accuracy \u00b1 SE\\n(Aggregated over runs)\"\n    )\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_mean_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated accuracy plot: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# Plot 2: loss curves with SE envelope\ntry:\n    plt.figure()\n    for dp in dropouts:\n        m_tr = mean_curves[dp][\"train_loss\"]\n        se_tr = se_curves[dp][\"train_loss\"]\n        m_val = mean_curves[dp][\"val_loss\"]\n        se_val = se_curves[dp][\"val_loss\"]\n\n        plt.plot(epochs_axis, m_tr, label=f\"{dp} train\", linewidth=1.5)\n        plt.plot(epochs_axis, m_val, label=f\"{dp} val\", linestyle=\"--\", linewidth=1.5)\n\n        plt.fill_between(epochs_axis, m_tr - se_tr, m_tr + se_tr, alpha=0.2)\n        plt.fill_between(epochs_axis, m_val - se_val, m_val + se_val, alpha=0.2)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Mean Training/Validation Loss \u00b1 SE\\n(Aggregated over runs)\")\n    plt.legend(fontsize=8)\n    fname = os.path.join(working_dir, \"SPR_BENCH_mean_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# Plot 3: bar chart of test accuracy with SE error bars\ntry:\n    plt.figure()\n    x = np.arange(len(dropouts))\n    y_mean = [test_acc_stats[dp][\"mean\"] * 100 for dp in dropouts]\n    y_se = [test_acc_stats[dp][\"se\"] * 100 for dp in dropouts]\n    plt.bar(x, y_mean, yerr=y_se, capsize=5, color=\"skyblue\", alpha=0.8)\n    plt.xticks(x, dropouts)\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\n        \"SPR_BENCH: Mean Test Accuracy \u00b1 SE by Dropout Rate\\n(Aggregated over runs)\"\n    )\n    fname = os.path.join(working_dir, \"SPR_BENCH_mean_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated test accuracy bar chart: {e}\")\n    plt.close()\n"], "plot_plan": ["The script first loads the saved experiment_data.npy, extracts the SPR_BENCH training/validation losses and accuracies, and derives the epoch indices. It then plots (1) the loss curves and (2) the accuracy curves, each in its own try-except block, saving the figures with descriptive names in working_dir. Next, it computes a confusion matrix from the stored test\u2010set predictions and ground truths, plots it with class labels, and prints the overall test accuracy. All figures are closed after saving. No synthetic data are created; every plot strictly reflects the contents of experiment_data.npy.", "We first load experiment_data.npy and determine which epoch-budget run achieved the lowest validation loss; this \u201cbest\u201d tag will be used for any detailed diagnostic plots such as a confusion-matrix.  \nThen, for every budget we extract arrays of training / validation loss and accuracy that were stored during training.  \nWe create two overview plots: (1) \u201cLoss vs Epoch\u201d and (2) \u201cAccuracy vs Epoch\u201d, each containing all budgets with separate line styles for train and validation so that trends are comparable in one figure apiece.  \nNext, for the single best budget we build a confusion-matrix from stored predictions vs. ground-truth on the test set and visualize it as an image plot with integer counts annotated.  \nEach figure is wrapped in its own try-except block, saved to working_dir with descriptive names, titled appropriately, closed afterward, and only data that actually exists in the numpy file are used.  \nFinally, the script prints final test accuracies for every budget and also prints which budget was selected as the best by validation loss.  \nOnly matplotlib, numpy, and os are used, and figures are limited to three, well within the five-figure maximum.  \nAll steps follow the provided plotting guidelines, including explicit figure titles and subtitles where relevant, and respecting the working_dir path.", "We will load experiment_data.npy, grab the list of runs under experiment_data[\"learning_rate\"][\"SPR_BENCH\"][\"runs\"], and extract per-epoch train/val accuracy & loss, learning rates, and final test accuracies.  \nFour concise visualizations will be produced (each inside its own try-except block and saved/closed immediately):  \n1) Train and validation accuracy curves (all learning rates overlaid).  \n2) Train and validation loss curves (all learning rates overlaid).  \n3) Bar chart of final test accuracy versus learning rate.  \n4) Confusion matrix on the test split for the best-performing learning rate run.  \nAll figures will be titled with the dataset name and plot type, saved in working_dir using descriptive filenames, and no more than these four figures are generated (\u22645 limit).  \nOnly data already stored in experiment_data.npy are used; no synthetic values are created.  \nThe code follows the required import header and plotting guidelines, always closing figures even on error.", "We will load experiment_data.npy and iterate through the three stored \u201cbs_*\u201d experiments. For each we will extract per-epoch train/val losses and accuracies as well as the final test accuracy that was already printed during training. We will first compute and print a compact summary table of test accuracies so the user can identify the best batch size. Then, within three independent try-except blocks, we will create:  \n(1) a single figure overlaying train & validation loss curves for all batch sizes,  \n(2) a single figure overlaying train & validation accuracy curves for all batch sizes, and  \n(3) a confusion-matrix style bar plot (ground-truth vs. predictions) for the batch size with the highest test accuracy.  \nEvery plot will be given a clear title that specifies it uses SPR_BENCH and saved to working_dir with descriptive filenames. We ensure figures are closed even on error, obey the \u22645 figure guideline, and rely solely on the real data in experiment_data.npy.", "The script loads the saved experiment data, gathers per-epoch accuracy/loss and final test accuracy for each dropout setting, and determines the best model by test accuracy. It then creates four independent figures\u2014accuracy curves, loss curves, a bar chart of test accuracies, and a confusion-matrix heat-map for the best dropout\u2014each wrapped in a try-except block to satisfy safety requirements. Titles, subtitles, and descriptive filenames include the dataset name and plot type, and every figure is closed after saving. All plots rely solely on the contents of experiment_data.npy; nothing is synthesized. After plotting, the script prints the test accuracy for every dropout and highlights the best-performing rate so users can quickly assess results. This keeps the number of figures \u22645, uses plain matplotlib, and adheres to the directory and naming conventions specified.", "We will load the saved experiment_data.npy, extract the lists of weight-decay settings, per-epoch losses/accuracies, final test accuracies, ground-truth labels and predictions.  \nFor visualization we create up to four figures: (1) Train/validation loss curves across epochs for every weight-decay value; (2) Train/validation accuracy curves across epochs; (3) A bar chart of final test accuracies versus weight-decay; (4) A confusion-matrix heat-map for the model achieving the highest test accuracy, contrasting ground truth with its predictions.  \nEach figure is wrapped in its own try-except block to guarantee robustness and proper figure closure.  \nAll plots are titled clearly, indicating the dataset (SPR_BENCH) and the nature of the plot, and each is saved to working_dir with descriptive filenames.  \nWhen plotting multiple runs on the same axes we use distinct colors/linestyles and add legends for clarity.  \nEpoch indices are inferred directly from the length of the stored curves; we make no assumption about missing or extra epochs.  \nAfter generating the plots, the script prints the test-accuracy table so users can easily inspect the numerical results alongside the figures.", "We will load the serialized experiment_data.npy, extract the results for the SPR_BENCH d_model sweep, and then create up to four standard figures: (1) combined train/validation loss curves for each d_model, (2) combined train/validation accuracy curves, (3) a bar chart comparing final test accuracies, and (4) a confusion matrix for the best-performing model. Each figure is created inside its own try-except block, saved to working_dir with descriptive names, and the figure is always closed. After plotting, we compute and print the best d_model together with its test accuracy so the script also outputs an evaluation metric. All plotting strictly relies on the stored experiment_data; no synthetic data are generated.", "We will load the saved experiment_data.npy, restrict ourselves to the SPR_BENCH entry, and iterate over every depth configuration (layers_1, layers_2, \u2026).  For every depth we extract the full history of training / validation loss and accuracy, as well as the test-set predictions and labels in order to recompute the test accuracy.  We then assemble three visualizations: (1) one multi-curve plot of train/val loss over epochs for all depths, (2) an analogous multi-curve plot for train/val accuracy, and (3) a simple bar chart comparing the final validation accuracy for every depth.  Each plot is wrapped in its own try-except guard, saved to the working directory with descriptive names, and the figure is closed regardless of success.  After plotting, we print a concise table that lists every depth together with the final validation accuracy and the recomputed test accuracy so that the user can see the key performance metrics at a glance.  The code respects all provided constraints: only data actually present in the .npy file are used, matplotlib is the sole plotting backend, at most three figures are generated, and all figures include clear titles/subtitles indicating dataset and content.  Finally, we ensure the working directory exists and that any plotting errors do not halt execution.", "We load the stored experiment_data.npy, extract the portion related to the \u201cnhead_tuning\u201d experiment on SPR_BENCH, and pull out the list of tried n-head values together with their train/val/test accuracies and losses.  \nThree simple visualizations are produced: (1) line plots of accuracy vs n-head for train/val/test, (2) line plots of loss vs n-head, and (3) a scatter showing the relationship between validation and test accuracy for each configuration, with the best\u2010validation point highlighted.  \nEach plot is wrapped in its own try-except block to avoid breaking the script if something is missing; figures are always closed after saving.  \nThe plots are titled clearly, mention the dataset, and are saved to the working directory with descriptive filenames.  \nWe also print a concise table and identify the best configuration by validation accuracy so the user can read the numbers in the console without opening the plots.  \nNo made-up data are introduced\u2014everything is read directly from the .npy file.  \nThe code stays minimal, uses only numpy and matplotlib, and produces at most three figures, satisfying the \u22645 plot requirement.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "epochs", "learning_rate", "batch_size", "dropout", "weight_decay", "d_model (Transformer hidden/embedding size)", "num_layers", "nhead", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script loads the serialized experiment dictionary from the working\ndirectory, iterates over every dataset it contains, and prints a concise report.\nFor each dataset it walks through the stored accuracy and loss histories,\nautomatically selecting the \u201cbest\u201d value (maximum for accuracies, minimum for\nlosses). Metric names are converted into human-readable forms such as \u201ctrain\naccuracy\u201d or \u201cvalidation loss\u201d before printing. Finally, if test\u2010set predictions\nare present, the script computes and prints the resulting test accuracy. The\ncode runs immediately on execution and follows all structural requirements.", "Below is a concise outline followed by a complete Python script that immediately\nloads the saved NumPy file, finds the best / final values for every metric\ninside each epoch-budget run, computes test accuracy from the stored\npredictions, and prints everything in a clear, labelled fashion.", "The script will locate the working directory, load experiment_data.npy into a\nPython dict, and iterate through every dataset and run stored inside.   For each\ndataset, it prints the dataset name once and then, for every learning-rate run,\nprints clearly labelled summary statistics: final training accuracy, best\nvalidation accuracy, final training loss, final validation loss, and test\naccuracy.   All code is placed at the top level so it executes immediately when\nthe file is run, and no plots are generated.", "The script loads the saved NumPy dictionary, iterates over each batch-size\nexperiment, extracts the final epoch\u2019s training/validation metrics, computes the\noverall test accuracy from the stored predictions, and prints everything in a\nclear, labeled manner. All logic is placed at the top level so the code runs\nimmediately upon execution, fulfilling the required structure.", "Below is a small utility that immediately loads the saved experiment file from\nthe working directory, walks through every dataset and every hyper-parameter\nconfiguration, and prints (1) the final train accuracy/loss, (2) the best\nvalidation accuracy/loss achieved during training, and (3) the test\naccuracy/loss recorded after training. All metric names are spelled out\nexplicitly so there is no ambiguity.", "Below is a compact script that immediately loads the stored NumPy dictionary,\ntraverses its structure, and prints out the final-epoch and best values for\nevery recorded metric per weight-decay setting. It first announces the dataset\nname, then the weight-decay setting, and finally each metric with an explicit\nname. No plotting or special entry-point logic is used.", "Below is a compact script that (1) loads the saved numpy dictionary from the\nworking directory, (2) iterates over every d_model setting recorded for the\nSPR_BENCH dataset, (3) extracts the final-epoch train/validation accuracy &\nloss, computes the test accuracy from the stored predictions vs. ground-truth\nlabels, and (4) prints each metric with a clear, explicit label.", "The script will load the saved numpy dictionary, iterate through the stored\nresults for the single dataset (\u201cSPR_BENCH\u201d) and each depth setting, and keep\ntrack of the best metrics it encounters.   For accuracies it keeps the maximum\nvalue (best epoch) and for losses the minimum value.   Whichever depth achieves\nthe highest validation accuracy is considered the overall \u201cbest\u201d run; its\ncorresponding best-epoch train/validation accuracies and losses, together with\nthe test accuracy computed from its stored predictions, are then printed in a\nclear, labelled format.", "The code will locate the pre-computed NumPy file inside the \u201cworking\u201d folder,\nload it into a Python dictionary, and then iterate through the stored\nexperiments.   For every dataset found (e.g., \u201cSPR_BENCH\u201d) it will identify the\nconfiguration that achieved the highest validation accuracy and print only that\nconfiguration\u2019s final metrics, clearly labelling each value (e.g., \u201ctrain\naccuracy,\u201d \u201cvalidation loss,\u201d \u201ctest accuracy\u201d).   All logic is executed at the\ntop level so that the script runs immediately when executed, without requiring\nan explicit main entry point.", "Below is a small utility that immediately loads the saved experiment file from\nthe working directory, walks through every dataset and every hyper-parameter\nconfiguration, and prints (1) the final train accuracy/loss, (2) the best\nvalidation accuracy/loss achieved during training, and (3) the test\naccuracy/loss recorded after training. All metric names are spelled out\nexplicitly so there is no ambiguity.", "Below is a small utility that immediately loads the saved experiment file from\nthe working directory, walks through every dataset and every hyper-parameter\nconfiguration, and prints (1) the final train accuracy/loss, (2) the best\nvalidation accuracy/loss achieved during training, and (3) the test\naccuracy/loss recorded after training. All metric names are spelled out\nexplicitly so there is no ambiguity.", "Below is a small utility that immediately loads the saved experiment file from\nthe working directory, walks through every dataset and every hyper-parameter\nconfiguration, and prints (1) the final train accuracy/loss, (2) the best\nvalidation accuracy/loss achieved during training, and (3) the test\naccuracy/loss recorded after training. All metric names are spelled out\nexplicitly so there is no ambiguity.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helpers ---------------------------------------------------------------------#\ndef _pretty_metric_name(raw_name: str) -> str:\n    \"\"\"Convert keys like 'train_acc' into 'train accuracy'.\"\"\"\n    name = raw_name\n    name = name.replace(\"train_\", \"train \")\n    name = name.replace(\"val_\", \"validation \")\n    name = name.replace(\"test_\", \"test \")\n    name = name.replace(\"_acc\", \" accuracy\")\n    name = name.replace(\"_loss\", \" loss\")\n    name = name.replace(\"_\", \" \")\n    return name.strip()\n\n\ndef _best_metric_value(key: str, values):\n    \"\"\"Select best value: max for accuracy, min for loss, else last element.\"\"\"\n    if not isinstance(values, (list, np.ndarray)):\n        return values\n    if \"acc\" in key:\n        return float(max(values))\n    if \"loss\" in key:\n        return float(min(values))\n    return float(values[-1])\n\n\n# -----------------------------------------------------------------------------#\n# load experiment data --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# print report ----------------------------------------------------------------#\nfor dataset_name, content in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # metrics and losses (history lists)\n    for group_name in (\"metrics\", \"losses\"):\n        group = content.get(group_name, {})\n        for metric_key, history in group.items():\n            metric_print_name = _pretty_metric_name(metric_key)\n            best_val = _best_metric_value(metric_key, history)\n            print(f\"{metric_print_name}: {best_val:.4f}\")\n\n    # optional test accuracy from stored predictions / ground truth\n    preds = np.asarray(content.get(\"predictions\", []))\n    gts = np.asarray(content.get(\"ground_truth\", []))\n    if preds.size and gts.size and preds.shape == gts.shape:\n        test_acc = float((preds == gts).mean())\n        print(f\"test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------------- #\n# Locate and load the experiment data                                       #\n# ------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------------- #\n# Helper: pretty printing of one run                                        #\n# ------------------------------------------------------------------------- #\ndef summarize_run(run_name: str, run_dict: dict):\n    \"\"\"\n    Print the best/final metrics for a single hyper-parameter run.\n    \"\"\"\n    # Arrays of metrics collected during training\n    train_acc = run_dict[\"metrics\"][\"train\"]\n    val_acc = run_dict[\"metrics\"][\"val\"]\n    train_los = run_dict[\"losses\"][\"train\"]\n    val_los = run_dict[\"losses\"][\"val\"]\n\n    # Use last recorded item as the 'final' value\n    final_train_acc = train_acc[-1]\n    final_val_acc = val_acc[-1]\n    final_train_loss = train_los[-1]\n    final_val_loss = val_los[-1]\n\n    # Compute test accuracy from stored predictions / ground-truth\n    preds = np.array(run_dict[\"predictions\"])\n    gts = np.array(run_dict[\"ground_truth\"])\n    test_accuracy = (preds == gts).mean() if preds.size else float(\"nan\")\n\n    # ------------------------------------------------------------------ #\n    # Printing                                                            #\n    # ------------------------------------------------------------------ #\n    print(f\"Dataset: {run_name}\")\n    print(f\"  train accuracy: {final_train_acc:.4f}\")\n    print(f\"  validation accuracy: {final_val_acc:.4f}\")\n    print(f\"  train loss: {final_train_loss:.4f}\")\n    print(f\"  validation loss: {final_val_loss:.4f}\")\n    print(f\"  test accuracy: {test_accuracy:.4f}\\n\")\n\n\n# ------------------------------------------------------------------------- #\n# Iterate over every epoch-budget dataset                                   #\n# ------------------------------------------------------------------------- #\nfor tag, run_content in experiment_data.get(\"epochs\", {}).items():\n    summarize_run(tag, run_content)\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------------- #\n# Load experiment data                                                      #\n# ------------------------------------------------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment_data.npy at {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------------------------------------------------------------------- #\n# Parse and print metrics                                                   #\n# ------------------------------------------------------------------------- #\nlr_section = experiment_data.get(\"learning_rate\", {})\n\nfor dataset_name, dataset_blob in lr_section.items():\n    print(f\"Dataset: {dataset_name}\")\n    runs = dataset_blob.get(\"runs\", [])\n    for run in runs:\n        lr = run.get(\"lr\")\n        metrics = run.get(\"metrics\", {})\n        losses = run.get(\"losses\", {})\n\n        train_acc_list = metrics.get(\"train\", [])\n        val_acc_list = metrics.get(\"val\", [])\n        train_loss_list = losses.get(\"train\", [])\n        val_loss_list = losses.get(\"val\", [])\n\n        # Safely extract required values\n        final_train_acc = train_acc_list[-1] if train_acc_list else float(\"nan\")\n        final_val_acc = val_acc_list[-1] if val_acc_list else float(\"nan\")\n        best_val_acc = max(val_acc_list) if val_acc_list else float(\"nan\")\n        final_train_loss = train_loss_list[-1] if train_loss_list else float(\"nan\")\n        final_val_loss = val_loss_list[-1] if val_loss_list else float(\"nan\")\n        test_acc = run.get(\"test_acc\", float(\"nan\"))\n\n        # Print metrics with explicit labels\n        print(f\"  Learning rate: {lr}\")\n        print(f\"    Final training accuracy: {final_train_acc:.4f}\")\n        print(f\"    Best validation accuracy: {best_val_acc:.4f}\")\n        print(f\"    Final training loss: {final_train_loss:.4f}\")\n        print(f\"    Final validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_acc:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# locate and load experiment data                                              #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# helper for printing                                                           #\n# -----------------------------------------------------------------------------#\ndef print_metric(name: str, value: float):\n    \"\"\"Print metric with consistent formatting (4 decimal places).\"\"\"\n    print(f\"  {name}: {value:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# extract and display metrics                                                   #\n# -----------------------------------------------------------------------------#\nbatch_results = experiment_data.get(\"batch_size\", {})\nfor dataset_name, results in batch_results.items():\n    metrics = results.get(\"metrics\", {})\n    losses = results.get(\"losses\", {})\n    preds = np.array(results.get(\"predictions\", []))\n    gts = np.array(results.get(\"ground_truth\", []))\n    test_acc = (preds == gts).mean() if preds.size else float(\"nan\")\n\n    # Fetch final values from training history\n    train_acc_final = metrics.get(\"train_acc\", [float(\"nan\")])[-1]\n    val_acc_final = metrics.get(\"val_acc\", [float(\"nan\")])[-1]\n    train_loss_final = losses.get(\"train_loss\", [float(\"nan\")])[-1]\n    val_loss_final = losses.get(\"val_loss\", [float(\"nan\")])[-1]\n\n    # Print dataset header\n    print(f\"Dataset: {dataset_name}\")\n    # Print metrics\n    print_metric(\"train accuracy\", train_acc_final)\n    print_metric(\"training loss\", train_loss_final)\n    print_metric(\"validation accuracy\", val_acc_final)\n    print_metric(\"validation loss\", val_loss_final)\n    print_metric(\"test accuracy\", test_acc)\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper functions -------------------------------------------------------------#\ndef _best_val_accuracy(val_acc_list):\n    \"\"\"Return the maximum validation accuracy.\"\"\"\n    return max(val_acc_list) if val_acc_list else float(\"nan\")\n\n\ndef _best_val_loss(val_loss_list):\n    \"\"\"Return the minimum validation loss.\"\"\"\n    return min(val_loss_list) if val_loss_list else float(\"nan\")\n\n\ndef _print_metrics(config_name, res_dict):\n    \"\"\"Pretty-print metrics for one hyper-parameter configuration.\"\"\"\n    train_acc_final = res_dict[\"metrics\"][\"train_acc\"][-1]\n    val_acc_best = _best_val_accuracy(res_dict[\"metrics\"][\"val_acc\"])\n    train_loss_final = res_dict[\"losses\"][\"train_loss\"][-1]\n    val_loss_best = _best_val_loss(res_dict[\"losses\"][\"val_loss\"])\n    test_acc = res_dict[\"test_acc\"]\n    test_loss = res_dict[\"test_loss\"]\n\n    print(f\"  Configuration: dropout = {config_name}\")\n    print(f\"    final train accuracy: {train_acc_final:.4f}\")\n    print(f\"    best validation accuracy: {val_acc_best:.4f}\")\n    print(f\"    final train loss: {train_loss_final:.4f}\")\n    print(f\"    best validation loss: {val_loss_best:.4f}\")\n    print(f\"    test accuracy: {test_acc:.4f}\")\n    print(f\"    test loss: {test_loss:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# main script -----------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# walk through experiments \u2192 datasets \u2192 hyper-parameter configurations\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, configs in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        for config_name, res_dict in configs.items():\n            _print_metrics(config_name, res_dict)\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# helper to format a float as percentage when appropriate\ndef pct(x):\n    return f\"{x*100:.2f}%\" if isinstance(x, float) else x\n\n\n# -----------------------------------------------------------------------------#\n# traverse the structure and print the required summaries\nfor dataset_name, ds_blob in experiment_data[\"weight_decay\"].items():\n    print(dataset_name)  # dataset headline\n    settings = ds_blob[\"settings\"]\n    train_metrics = ds_blob[\"metrics\"][\"train\"]\n    val_metrics = ds_blob[\"metrics\"][\"val\"]\n    train_losses = ds_blob[\"losses\"][\"train\"]\n    val_losses = ds_blob[\"losses\"][\"val\"]\n    test_accs = ds_blob[\"test_acc\"]\n\n    for idx, wd in enumerate(settings):\n        print(f\"  weight decay: {wd}\")\n        tr_accs = train_metrics[idx]\n        va_accs = val_metrics[idx]\n        tr_los = train_losses[idx]\n        va_los = val_losses[idx]\n\n        # final values\n        final_train_acc = tr_accs[-1]\n        final_validation_acc = va_accs[-1]\n        final_train_loss = tr_los[-1]\n        final_validation_loss = va_los[-1]\n        test_accuracy = test_accs[idx]\n\n        # best values\n        best_validation_acc = max(va_accs)\n        lowest_validation_loss = min(va_los)\n\n        # print metrics\n        print(f\"    final train accuracy: {pct(final_train_acc)}\")\n        print(f\"    final validation accuracy: {pct(final_validation_acc)}\")\n        print(f\"    best validation accuracy: {pct(best_validation_acc)}\")\n        print(f\"    final train loss: {final_train_loss:.4f}\")\n        print(f\"    final validation loss: {final_validation_loss:.4f}\")\n        print(f\"    lowest validation loss: {lowest_validation_loss:.4f}\")\n        print(f\"    test accuracy: {pct(test_accuracy)}\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# locate and load experiment results                                           #\n# -----------------------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# iterate through datasets / hyper-parameter settings                          #\n# -----------------------------------------------------------------------------#\nfor dataset_name, dataset_dict in experiment_data[\"d_model_tuning\"].items():\n    # currently only \"SPR_BENCH\", but the loop is generic\n    for tag, run_data in dataset_dict.items():\n        losses = run_data[\"losses\"]\n        metrics = run_data[\"metrics\"]\n        preds = np.asarray(run_data[\"predictions\"])\n        gts = np.asarray(run_data[\"ground_truth\"])\n\n        # final values are simply the last entry of the lists\n        final_train_acc = metrics[\"train\"][-1]\n        final_val_acc = metrics[\"val\"][-1]\n        final_train_loss = losses[\"train\"][-1]\n        final_val_loss = losses[\"val\"][-1]\n\n        # test accuracy from stored predictions\n        test_accuracy = (preds == gts).mean() if preds.size else float(\"nan\")\n\n        # ---------------------------------------------------------------------#\n        # printing                                                             #\n        # ---------------------------------------------------------------------#\n        print(f\"\\nDataset: {dataset_name}  |  Setting: {tag}\")\n        print(f\"train accuracy:      {final_train_acc:.4f}\")\n        print(f\"validation accuracy: {final_val_acc:.4f}\")\n        print(f\"train loss:          {final_train_loss:.4f}\")\n        print(f\"validation loss:     {final_val_loss:.4f}\")\n        print(f\"test accuracy:       {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# ------------------------------------------------------------------\n# 0. Locate and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# ------------------------------------------------------------------\ndef compute_accuracy(preds, gts):\n    \"\"\"Simple accuracy computation.\"\"\"\n    preds = np.asarray(preds)\n    gts = np.asarray(gts)\n    return (preds == gts).mean()\n\n\n# ------------------------------------------------------------------\n# 1. Iterate over datasets present in the file\nfor dataset_name, layers_dict in experiment_data.get(\"num_layers\", {}).items():\n    # Containers for the best overall metrics for this dataset\n    best_metrics = {\n        \"train_accuracy\": -1.0,\n        \"validation_accuracy\": -1.0,\n        \"train_loss\": float(\"inf\"),\n        \"validation_loss\": float(\"inf\"),\n        \"test_accuracy\": -1.0,\n    }\n    best_layer_key = None\n\n    # 2. Find the layer configuration that gives the highest validation accuracy\n    for layer_key, run_data in layers_dict.items():\n        train_acc_list = run_data[\"metrics\"][\"train\"]\n        val_acc_list = run_data[\"metrics\"][\"val\"]\n        train_loss_list = run_data[\"losses\"][\"train\"]\n        val_loss_list = run_data[\"losses\"][\"val\"]\n\n        best_train_acc = max(train_acc_list)\n        best_val_acc = max(val_acc_list)\n        best_train_loss = min(train_loss_list)\n        best_val_loss = min(val_loss_list)\n\n        # Determine if this run is the new best (highest val accuracy wins)\n        if best_val_acc > best_metrics[\"validation_accuracy\"]:\n            best_metrics.update(\n                {\n                    \"train_accuracy\": best_train_acc,\n                    \"validation_accuracy\": best_val_acc,\n                    \"train_loss\": best_train_loss,\n                    \"validation_loss\": best_val_loss,\n                    # test accuracy computed from stored predictions\n                    \"test_accuracy\": compute_accuracy(\n                        run_data[\"predictions\"], run_data[\"ground_truth\"]\n                    ),\n                }\n            )\n            best_layer_key = layer_key\n\n    # ------------------------------------------------------------------\n    # 3. Print the results for this dataset\n    print(f\"\\nDataset: {dataset_name}  (best run: {best_layer_key})\")\n    print(f\"train accuracy: {best_metrics['train_accuracy']*100:.2f}%\")\n    print(f\"validation accuracy: {best_metrics['validation_accuracy']*100:.2f}%\")\n    print(f\"train loss: {best_metrics['train_loss']:.4f}\")\n    print(f\"validation loss: {best_metrics['validation_loss']:.4f}\")\n    print(f\"test accuracy: {best_metrics['test_accuracy']*100:.2f}%\")\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# locate results file ----------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(file_path):\n    raise FileNotFoundError(f\"Cannot find experiment_data.npy at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# helper to pretty-print one dataset -------------------------------------------#\ndef print_best_metrics(dset_dict):\n    \"\"\"\n    dset_dict has keys:\n      configs, metrics{train_acc, val_acc, test_acc}, losses{train_loss, ...}\n    \"\"\"\n    val_accs = dset_dict[\"metrics\"][\"val_acc\"]\n    best_idx = int(np.argmax(val_accs))  # best validation accuracy\n\n    # Gather best values\n    best_metrics = {\n        \"train accuracy\": dset_dict[\"metrics\"][\"train_acc\"][best_idx],\n        \"validation accuracy\": dset_dict[\"metrics\"][\"val_acc\"][best_idx],\n        \"test accuracy\": dset_dict[\"metrics\"][\"test_acc\"][best_idx],\n        \"train loss\": dset_dict[\"losses\"][\"train_loss\"][best_idx],\n        \"validation loss\": dset_dict[\"losses\"][\"val_loss\"][best_idx],\n        \"test loss\": dset_dict[\"losses\"][\"test_loss\"][best_idx],\n    }\n\n    # Print metrics\n    for metric_name, value in best_metrics.items():\n        # accuracy as percentage, loss as raw float\n        if \"accuracy\" in metric_name:\n            print(f\"{metric_name}: {value * 100:.2f}%\")\n        else:\n            print(f\"{metric_name}: {value:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# iterate over experiments / datasets -----------------------------------------#\nfor exp_name, exp_content in experiment_data.items():\n    # exp_content is a dict whose keys are dataset names\n    for dataset_name, dset_dict in exp_content.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        print_best_metrics(dset_dict)\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper functions -------------------------------------------------------------#\ndef _best_val_accuracy(val_acc_list):\n    \"\"\"Return the maximum validation accuracy.\"\"\"\n    return max(val_acc_list) if val_acc_list else float(\"nan\")\n\n\ndef _best_val_loss(val_loss_list):\n    \"\"\"Return the minimum validation loss.\"\"\"\n    return min(val_loss_list) if val_loss_list else float(\"nan\")\n\n\ndef _print_metrics(config_name, res_dict):\n    \"\"\"Pretty-print metrics for one hyper-parameter configuration.\"\"\"\n    train_acc_final = res_dict[\"metrics\"][\"train_acc\"][-1]\n    val_acc_best = _best_val_accuracy(res_dict[\"metrics\"][\"val_acc\"])\n    train_loss_final = res_dict[\"losses\"][\"train_loss\"][-1]\n    val_loss_best = _best_val_loss(res_dict[\"losses\"][\"val_loss\"])\n    test_acc = res_dict[\"test_acc\"]\n    test_loss = res_dict[\"test_loss\"]\n\n    print(f\"  Configuration: dropout = {config_name}\")\n    print(f\"    final train accuracy: {train_acc_final:.4f}\")\n    print(f\"    best validation accuracy: {val_acc_best:.4f}\")\n    print(f\"    final train loss: {train_loss_final:.4f}\")\n    print(f\"    best validation loss: {val_loss_best:.4f}\")\n    print(f\"    test accuracy: {test_acc:.4f}\")\n    print(f\"    test loss: {test_loss:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# main script -----------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# walk through experiments \u2192 datasets \u2192 hyper-parameter configurations\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, configs in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        for config_name, res_dict in configs.items():\n            _print_metrics(config_name, res_dict)\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper functions -------------------------------------------------------------#\ndef _best_val_accuracy(val_acc_list):\n    \"\"\"Return the maximum validation accuracy.\"\"\"\n    return max(val_acc_list) if val_acc_list else float(\"nan\")\n\n\ndef _best_val_loss(val_loss_list):\n    \"\"\"Return the minimum validation loss.\"\"\"\n    return min(val_loss_list) if val_loss_list else float(\"nan\")\n\n\ndef _print_metrics(config_name, res_dict):\n    \"\"\"Pretty-print metrics for one hyper-parameter configuration.\"\"\"\n    train_acc_final = res_dict[\"metrics\"][\"train_acc\"][-1]\n    val_acc_best = _best_val_accuracy(res_dict[\"metrics\"][\"val_acc\"])\n    train_loss_final = res_dict[\"losses\"][\"train_loss\"][-1]\n    val_loss_best = _best_val_loss(res_dict[\"losses\"][\"val_loss\"])\n    test_acc = res_dict[\"test_acc\"]\n    test_loss = res_dict[\"test_loss\"]\n\n    print(f\"  Configuration: dropout = {config_name}\")\n    print(f\"    final train accuracy: {train_acc_final:.4f}\")\n    print(f\"    best validation accuracy: {val_acc_best:.4f}\")\n    print(f\"    final train loss: {train_loss_final:.4f}\")\n    print(f\"    best validation loss: {val_loss_best:.4f}\")\n    print(f\"    test accuracy: {test_acc:.4f}\")\n    print(f\"    test loss: {test_loss:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# main script -----------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# walk through experiments \u2192 datasets \u2192 hyper-parameter configurations\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, configs in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        for config_name, res_dict in configs.items():\n            _print_metrics(config_name, res_dict)\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper functions -------------------------------------------------------------#\ndef _best_val_accuracy(val_acc_list):\n    \"\"\"Return the maximum validation accuracy.\"\"\"\n    return max(val_acc_list) if val_acc_list else float(\"nan\")\n\n\ndef _best_val_loss(val_loss_list):\n    \"\"\"Return the minimum validation loss.\"\"\"\n    return min(val_loss_list) if val_loss_list else float(\"nan\")\n\n\ndef _print_metrics(config_name, res_dict):\n    \"\"\"Pretty-print metrics for one hyper-parameter configuration.\"\"\"\n    train_acc_final = res_dict[\"metrics\"][\"train_acc\"][-1]\n    val_acc_best = _best_val_accuracy(res_dict[\"metrics\"][\"val_acc\"])\n    train_loss_final = res_dict[\"losses\"][\"train_loss\"][-1]\n    val_loss_best = _best_val_loss(res_dict[\"losses\"][\"val_loss\"])\n    test_acc = res_dict[\"test_acc\"]\n    test_loss = res_dict[\"test_loss\"]\n\n    print(f\"  Configuration: dropout = {config_name}\")\n    print(f\"    final train accuracy: {train_acc_final:.4f}\")\n    print(f\"    best validation accuracy: {val_acc_best:.4f}\")\n    print(f\"    final train loss: {train_loss_final:.4f}\")\n    print(f\"    best validation loss: {val_loss_best:.4f}\")\n    print(f\"    test accuracy: {test_acc:.4f}\")\n    print(f\"    test loss: {test_loss:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# main script -----------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# walk through experiments \u2192 datasets \u2192 hyper-parameter configurations\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, configs in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        for config_name, res_dict in configs.items():\n            _print_metrics(config_name, res_dict)\n", ""], "parse_term_out": ["['Dataset: SPR_BENCH', '\\n', 'train acc: 0.7985', '\\n', 'validation acc:\n0.7960', '\\n', 'train loss: 0.5048', '\\n', 'validation loss: 0.5144', '\\n',\n'test accuracy: 0.7950', '\\n', 'Execution time: a moment seconds (time limit is\n30 minutes).']", "['Dataset: epochs_10', '\\n', '  train accuracy: 0.7955', '\\n', '  validation\naccuracy: 0.6620', '\\n', '  train loss: 0.5069', '\\n', '  validation loss:\n0.5884', '\\n', '  test accuracy: 0.7930\\n', '\\n', 'Dataset: epochs_20', '\\n', '\ntrain accuracy: 0.7965', '\\n', '  validation accuracy: 0.7860', '\\n', '  train\nloss: 0.5114', '\\n', '  validation loss: 0.5339', '\\n', '  test accuracy:\n0.7950\\n', '\\n', 'Dataset: epochs_30', '\\n', '  train accuracy: 0.7915', '\\n', '\nvalidation accuracy: 0.7960', '\\n', '  train loss: 0.5117', '\\n', '  validation\nloss: 0.5138', '\\n', '  test accuracy: 0.7940\\n', '\\n', 'Dataset: epochs_50',\n'\\n', '  train accuracy: 0.7955', '\\n', '  validation accuracy: 0.7960', '\\n', '\ntrain loss: 0.5087', '\\n', '  validation loss: 0.5249', '\\n', '  test accuracy:\n0.7950\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: SPR_BENCH', '\\n', '  Learning rate: 0.001', '\\n', '    Final training\naccuracy: 0.7955', '\\n', '    Best validation accuracy: 0.7960', '\\n', '\nFinal training loss: 0.5069', '\\n', '    Final validation loss: 0.5884', '\\n', '\nTest accuracy: 0.6830', '\\n', '  Learning rate: 0.0005', '\\n', '    Final\ntraining accuracy: 0.7940', '\\n', '    Best validation accuracy: 0.7960', '\\n',\n'    Final training loss: 0.5100', '\\n', '    Final validation loss: 0.5141',\n'\\n', '    Test accuracy: 0.7950', '\\n', '  Learning rate: 0.0002', '\\n', '\nFinal training accuracy: 0.7935', '\\n', '    Best validation accuracy: 0.7900',\n'\\n', '    Final training loss: 0.5151', '\\n', '    Final validation loss:\n0.5268', '\\n', '    Test accuracy: 0.7890', '\\n', '  Learning rate: 0.0001',\n'\\n', '    Final training accuracy: 0.7885', '\\n', '    Best validation\naccuracy: 0.7860', '\\n', '    Final training loss: 0.5233', '\\n', '    Final\nvalidation loss: 0.5401', '\\n', '    Test accuracy: 0.7860', '\\n', '  Learning\nrate: 5e-05', '\\n', '    Final training accuracy: 0.7810', '\\n', '    Best\nvalidation accuracy: 0.7860', '\\n', '    Final training loss: 0.5239', '\\n', '\nFinal validation loss: 0.5408', '\\n', '    Test accuracy: 0.7890', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: bs_32', '\\n', '  train accuracy: 0.7955', '\\n', '  training loss:\n0.5082', '\\n', '  validation accuracy: 0.7920', '\\n', '  validation loss:\n0.5213', '\\n', '  test accuracy: 0.7930', '\\n', 'Dataset: bs_128', '\\n', '\ntrain accuracy: 0.7950', '\\n', '  training loss: 0.5087', '\\n', '  validation\naccuracy: 0.7960', '\\n', '  validation loss: 0.5200', '\\n', '  test accuracy:\n0.7950', '\\n', 'Dataset: bs_256', '\\n', '  train accuracy: 0.7950', '\\n', '\ntraining loss: 0.5117', '\\n', '  validation accuracy: 0.7860', '\\n', '\nvalidation loss: 0.5299', '\\n', '  test accuracy: 0.7890', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: dropout = 0.0', '\\n', '\nfinal train accuracy: 0.7940', '\\n', '    best validation accuracy: 0.7960',\n'\\n', '    final train loss: 0.5043', '\\n', '    best validation loss: 0.5147',\n'\\n', '    test accuracy: 0.7950', '\\n', '    test loss: 0.5113', '\\n', '\nConfiguration: dropout = 0.1', '\\n', '    final train accuracy: 0.7945', '\\n', '\nbest validation accuracy: 0.7960', '\\n', '    final train loss: 0.5106', '\\n', '\nbest validation loss: 0.5193', '\\n', '    test accuracy: 0.7900', '\\n', '\ntest loss: 0.5113', '\\n', '  Configuration: dropout = 0.2', '\\n', '    final\ntrain accuracy: 0.7870', '\\n', '    best validation accuracy: 0.7960', '\\n', '\nfinal train loss: 0.5159', '\\n', '    best validation loss: 0.5153', '\\n', '\ntest accuracy: 0.7880', '\\n', '    test loss: 0.5179', '\\n', '  Configuration:\ndropout = 0.3', '\\n', '    final train accuracy: 0.7950', '\\n', '    best\nvalidation accuracy: 0.7960', '\\n', '    final train loss: 0.5086', '\\n', '\nbest validation loss: 0.5123', '\\n', '    test accuracy: 0.7950', '\\n', '\ntest loss: 0.5128', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', '  weight decay: 0.0', '\\n', '    final train accuracy:\n79.65%', '\\n', '    final validation accuracy: 79.60%', '\\n', '    best\nvalidation accuracy: 79.60%', '\\n', '    final train loss: 0.5035', '\\n', '\nfinal validation loss: 0.5125', '\\n', '    lowest validation loss: 0.5125',\n'\\n', '    test accuracy: 79.40%', '\\n', '  weight decay: 1e-05', '\\n', '\nfinal train accuracy: 79.30%', '\\n', '    final validation accuracy: 79.40%',\n'\\n', '    best validation accuracy: 79.60%', '\\n', '    final train loss:\n0.5113', '\\n', '    final validation loss: 0.5191', '\\n', '    lowest validation\nloss: 0.5191', '\\n', '    test accuracy: 79.20%', '\\n', '  weight decay:\n0.0001', '\\n', '    final train accuracy: 79.25%', '\\n', '    final validation\naccuracy: 79.00%', '\\n', '    best validation accuracy: 79.60%', '\\n', '\nfinal train loss: 0.5123', '\\n', '    final validation loss: 0.5293', '\\n', '\nlowest validation loss: 0.5185', '\\n', '    test accuracy: 79.20%', '\\n', '\nweight decay: 0.001', '\\n', '    final train accuracy: 79.60%', '\\n', '    final\nvalidation accuracy: 78.60%', '\\n', '    best validation accuracy: 78.60%',\n'\\n', '    final train loss: 0.5058', '\\n', '    final validation loss: 0.5265',\n'\\n', '    lowest validation loss: 0.5201', '\\n', '    test accuracy: 79.00%',\n'\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  |  Setting: d_model_32', '\\n', 'train accuracy:\n0.7960', '\\n', 'validation accuracy: 0.7960', '\\n', 'train loss:\n0.5082', '\\n', 'validation loss:     0.5109', '\\n', 'test accuracy:\n0.7950', '\\n', '\\nDataset: SPR_BENCH  |  Setting: d_model_64', '\\n', 'train\naccuracy:      0.7780', '\\n', 'validation accuracy: 0.7860', '\\n', 'train loss:\n0.5225', '\\n', 'validation loss:     0.5348', '\\n', 'test accuracy:\n0.7890', '\\n', '\\nDataset: SPR_BENCH  |  Setting: d_model_128', '\\n', 'train\naccuracy:      0.7895', '\\n', 'validation accuracy: 0.7860', '\\n', 'train loss:\n0.5166', '\\n', 'validation loss:     0.5253', '\\n', 'test accuracy:\n0.7880', '\\n', '\\nDataset: SPR_BENCH  |  Setting: d_model_256', '\\n', 'train\naccuracy:      0.7725', '\\n', 'validation accuracy: 0.7520', '\\n', 'train loss:\n0.5335', '\\n', 'validation loss:     0.5845', '\\n', 'test accuracy:\n0.7710', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH  (best run: layers_1)', '\\n', 'train accuracy: 79.70%',\n'\\n', 'validation accuracy: 79.60%', '\\n', 'train loss: 0.5034', '\\n',\n'validation loss: 0.5198', '\\n', 'test accuracy: 79.50%', '\\n', 'Execution time:\na moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'train accuracy: 79.60%', '\\n', 'validation\naccuracy: 79.60%', '\\n', 'test accuracy: 79.50%', '\\n', 'train loss: 0.5039',\n'\\n', 'validation loss: 0.5176', '\\n', 'test loss: 0.5169', '\\n', 'Execution\ntime: a moment seconds (time limit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: dropout = 0.0', '\\n', '\nfinal train accuracy: 0.7940', '\\n', '    best validation accuracy: 0.7960',\n'\\n', '    final train loss: 0.5092', '\\n', '    best validation loss: 0.5121',\n'\\n', '    test accuracy: 0.7870', '\\n', '    test loss: 0.5152', '\\n', '\nConfiguration: dropout = 0.1', '\\n', '    final train accuracy: 0.7950', '\\n', '\nbest validation accuracy: 0.7960', '\\n', '    final train loss: 0.5081', '\\n', '\nbest validation loss: 0.5119', '\\n', '    test accuracy: 0.7950', '\\n', '\ntest loss: 0.5112', '\\n', '  Configuration: dropout = 0.2', '\\n', '    final\ntrain accuracy: 0.7980', '\\n', '    best validation accuracy: 0.7960', '\\n', '\nfinal train loss: 0.5050', '\\n', '    best validation loss: 0.5169', '\\n', '\ntest accuracy: 0.7930', '\\n', '    test loss: 0.5298', '\\n', '  Configuration:\ndropout = 0.3', '\\n', '    final train accuracy: 0.7915', '\\n', '    best\nvalidation accuracy: 0.7960', '\\n', '    final train loss: 0.5166', '\\n', '\nbest validation loss: 0.5161', '\\n', '    test accuracy: 0.7940', '\\n', '\ntest loss: 0.5247', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: dropout = 0.0', '\\n', '\nfinal train accuracy: 0.7955', '\\n', '    best validation accuracy: 0.7960',\n'\\n', '    final train loss: 0.5173', '\\n', '    best validation loss: 0.5139',\n'\\n', '    test accuracy: 0.7720', '\\n', '    test loss: 0.5489', '\\n', '\nConfiguration: dropout = 0.1', '\\n', '    final train accuracy: 0.7925', '\\n', '\nbest validation accuracy: 0.7920', '\\n', '    final train loss: 0.5132', '\\n', '\nbest validation loss: 0.5209', '\\n', '    test accuracy: 0.7880', '\\n', '\ntest loss: 0.5158', '\\n', '  Configuration: dropout = 0.2', '\\n', '    final\ntrain accuracy: 0.7945', '\\n', '    best validation accuracy: 0.7960', '\\n', '\nfinal train loss: 0.5112', '\\n', '    best validation loss: 0.5220', '\\n', '\ntest accuracy: 0.7940', '\\n', '    test loss: 0.5200', '\\n', '  Configuration:\ndropout = 0.3', '\\n', '    final train accuracy: 0.7885', '\\n', '    best\nvalidation accuracy: 0.7960', '\\n', '    final train loss: 0.5201', '\\n', '\nbest validation loss: 0.5103', '\\n', '    test accuracy: 0.7950', '\\n', '\ntest loss: 0.5286', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', '  Configuration: dropout = 0.0', '\\n', '\nfinal train accuracy: 0.7955', '\\n', '    best validation accuracy: 0.7960',\n'\\n', '    final train loss: 0.5064', '\\n', '    best validation loss: 0.5147',\n'\\n', '    test accuracy: 0.7940', '\\n', '    test loss: 0.5135', '\\n', '\nConfiguration: dropout = 0.1', '\\n', '    final train accuracy: 0.7960', '\\n', '\nbest validation accuracy: 0.7960', '\\n', '    final train loss: 0.5055', '\\n', '\nbest validation loss: 0.5174', '\\n', '    test accuracy: 0.7890', '\\n', '\ntest loss: 0.5218', '\\n', '  Configuration: dropout = 0.2', '\\n', '    final\ntrain accuracy: 0.7955', '\\n', '    best validation accuracy: 0.7960', '\\n', '\nfinal train loss: 0.5138', '\\n', '    best validation loss: 0.5177', '\\n', '\ntest accuracy: 0.7890', '\\n', '    test loss: 0.5161', '\\n', '  Configuration:\ndropout = 0.3', '\\n', '    final train accuracy: 0.7940', '\\n', '    best\nvalidation accuracy: 0.7960', '\\n', '    final train loss: 0.5141', '\\n', '\nbest validation loss: 0.5225', '\\n', '    test accuracy: 0.7930', '\\n', '\ntest loss: 0.5227', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>
