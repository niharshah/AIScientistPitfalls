{"edges": [[0, 3], [0, 2], [0, 5], [0, 4], [0, 1], [3, 8], [3, 6], [3, 7], [7, 11], [7, 12], [7, 9], [7, 10]], "layout": [[0.5, 0.0], [0.0, 0.33333333333333337], [0.25, 0.33333333333333337], [0.5, 0.33333333333333337], [0.75, 0.33333333333333337], [1.0, 0.33333333333333337], [0.25, 0.6666666666666667], [0.5, 0.6666666666666667], [0.75, 0.6666666666666667], [0.125, 1.0], [0.375, 1.0], [0.625, 1.0], [0.875, 1.0]], "plan": ["Hyperparam tuning name: dropout. The solution runs a separate training-\nevaluation cycle for each candidate dropout value (0.0, 0.1, 0.2, 0.3).   For\nevery setting it rebuilds the model, trains for a fixed number of epochs,\nrecords train/validation losses and accuracies, evaluates on test, and stores\npredictions plus ground-truth in a structured experiment_data dictionary.   All\ndata are finally saved to working/experiment_data.npy.   The code below is\ncompletely self-contained and executable.", "We extend the earlier character-level transformer by (i) inserting an explicit\n[CLS] token whose contextual representation is pooled for classification, (ii)\nincreasing model capacity (d_model = 128, 4 encoder layers, 8 heads) and (iii)\nreplacing plain cross-entropy with label-smoothing loss to reduce over-\nconfidence and improve generalisation.  We train for a small number of epochs\n(configurable) while monitoring both accuracy and the required Macro-F1 on the\ndev set, keeping the best checkpoint to evaluate on the hidden test split.  All\nmetrics, losses, predictions and ground-truth labels are stored in the\nexperiment_data dict and saved to ./working/experiment_data.npy for later\ninspection.  The code is self-contained, moves all tensors to GPU when\navailable, and prints validation loss plus Macro-F1 every epoch.  This stronger\ncontextual model is expected to capture long-range symbolic dependencies more\nfaithfully, pushing performance beyond the previous baseline.  (Replace\nSPR_BENCH path via $SPR_DATA or folder detection if necessary.)", "Leveraging auxiliary self-supervision can push contextual models to encode\nrule\u2010critical statistics better. I propose a multi-task character-level\ntransformer: the encoder is identical to the previous version, but the pooled\nsequence embedding feeds three heads \u2013 (1) the true SPR label (main task), (2)\nsequence-length parity (even/odd), and (3) bucketised \u201cnumber of distinct\nsymbols\u201d (1-10). These auxiliary targets force the representation to internalise\nparity, counting and composition features that are often decisive for PolyRule\nreasoning. We train with a weighted sum of cross-entropy losses, monitor\nMacro-F1 on the development set each epoch, and choose the best checkpoint for\nfinal test evaluation. This quick first-attempt keeps training affordable (\u22645\nepochs) while illustrating the research direction; subsequent work can extend\nthe auxiliary set or add relative position encoding. All metrics, losses and\npredictions are stored for later analysis.", "We propose to enhance the baseline by (i) switching from character\u2010level to\nspace\u2013delimited token embeddings that better respect the atomic \u201csymbol\u201d units\nin SPR, (ii) adding an explicit [CLS] token whose contextual embedding is used\nfor classification, (iii) employing sinusoidal positional encodings so the model\ncan generalise to longer sequences, and (iv) reporting both accuracy and the\nrequired macro-F1 each epoch.  The architecture is still a lightweight\nTransformerEncoder to keep runtime low, but these changes give the model a\nricher contextual view of each symbolic unit and a cleaner pooling strategy.\nThe script below trains for a few epochs (adjustable) and prints validation\nloss, accuracy, and macro-F1, then evaluates on the test split and stores all\nmetrics / predictions in the prescribed experiment_data structure for later\nanalysis.  It follows all GPU-handling, logging, and saving requirements.", "We switch from pure character-level modeling to a whitespace token-level\napproach with an explicit [CLS] embedding, deeper transformer and a Macro-F1\ndriven early-stopping.  The new tokenizer captures higher-order symbolic units;\nthe model uses a learnable CLS pooled representation and label-smoothing cross-\nentropy plus small MLM pre-training inside each batch to improve\ncontextualisation.  We train for up to 20 epochs, stopping after 4 epochs\nwithout Macro-F1 gain.  All train/val losses and Macro-F1 scores are saved in\nexperiment_data and printed, together with final test accuracy and Macro-F1.", "We enhance the baseline by (1) inserting an explicit [CLS] token whose\ncontextual embedding is used for SPR classification, (2) adding an auxiliary\n\u201csequence-length parity\u201d head that forces the encoder to track counting-style\ninformation, and (3) applying label-smoothing to reduce over-confidence. The\nmodel is a lightweight Transformer encoder with learned positional embeddings.\nTraining optimizes a weighted sum of main and auxiliary cross-entropy losses,\nand we monitor accuracy and Macro-F1 after every epoch to pick the best\ncheckpoint. All data, metrics and predictions are stored under ./working for\nlater analysis. The whole script is self-contained, uses GPU when available, and\ncompletes a short 8-epoch run suitable for quick insight into the contribution\nof the auxiliary task.", "To push beyond the current baseline we fuse the successful transformer backbone\nwith two lightweight auxiliary objectives that inject inductive biases about\nrule-critical statistics: (1) sequence-length parity (even/odd) and (2) the\nnumber of distinct symbols (bucketed 0-10+).  The model now has three parallel\nclassification heads that share a single encoder; training uses label-smoothing\non the main SPR rule label plus standard cross-entropy on the two auxiliaries,\nwith a combined loss (1.0 + 0.2 + 0.2).  These extra signals encourage the\nencoder to represent global counting information that is hard to learn from the\nmain task alone.  We keep the successful token-level vocabulary, CLS token,\nsinusoidal positions, and macro-F1 early feedback, but add deeper capacity (6\nlayers) and a little more dropout for regularisation.  All metrics and losses\nare logged per epoch and saved for later analysis.  The code below loads\nSPR_BENCH automatically, trains for a few epochs well within the runtime budget,\nprints validation loss/F1 each epoch, and finally reports test accuracy and\nmacro-F1 while persisting every tracked tensor to ./working.", "We enlarge the transformer, replace plain cross-entropy with label-smoothing,\nand add an auxiliary \u201clength-parity\u201d task that compels the network to encode\nsequence-level statistics known to be useful for SPR rules. The model now has\ntwo heads: one for the true SPR class and one for parity (even/odd). Training\nminimizes the sum of the main loss and a down-weighted auxiliary loss; at\nevaluation we report the usual Macro-F1 on the main task. This simple multitask\n+ better regularisation setup regularly yields several extra F1 points over the\nbaseline while remaining lightweight enough for a 30-minute run. Code below\nloads the data, builds a larger 6-layer transformer with 256-dimensional\nembeddings, trains for a handful of epochs, prints per-epoch metrics, and saves\neverything to working/experiment_data.npy.", "We add two lightweight auxiliary self-supervision signals\u2014sequence-length parity\nand (clipped) count of distinct symbols\u2014so the transformer is nudged to\ninternalise rule-critical statistics.  A single shared encoder feeds three\nclassification heads, and the joint loss is the label-smoothed main loss plus\nsmaller-weighted auxiliary losses.  This encourages richer representations with\nminimal overhead.  We also widen the model, add dropout, apply cosine learning-\nrate decay with warm-up, and use early stopping on dev Macro-F1.  The script\ntokenises sequences at symbol level, builds a vocabulary, trains with\nDataLoaders on GPU if available, logs per-epoch metrics, and finally evaluates\non the test split while saving everything in the required numpy container.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os, pathlib, numpy as np, torch, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"dropout_tuning\": {\n        # each dataset will be inserted here\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=64,\n        nhead=4,\n        num_layers=2,\n        dim_ff=128,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = x.transpose(0, 1)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        x = x.transpose(0, 1)\n        pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(\n            1, keepdim=True\n        )\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    return epoch_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nprint(f\"Max sequence length in training set: {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[\"<pad>\"]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ndropout_rates = [0.0, 0.1, 0.2, 0.3]\nepochs = 10\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data[\"dropout_tuning\"][\"SPR_BENCH\"] = {}\n\nfor dp in dropout_rates:\n    print(f\"\\n=== Training with dropout={dp} ===\")\n    model = SimpleTransformerClassifier(\n        len(vocab), num_labels, max_len=max_len, dropout=dp\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    metrics = {\"train_acc\": [], \"val_acc\": []}\n    losses = {\"train_loss\": [], \"val_loss\": []}\n\n    for epoch in range(1, epochs + 1):\n        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)\n        v_loss, v_acc = run_epoch(model, dev_loader, criterion)\n        metrics[\"train_acc\"].append(t_acc)\n        metrics[\"val_acc\"].append(v_acc)\n        losses[\"train_loss\"].append(t_loss)\n        losses[\"val_loss\"].append(v_loss)\n        print(\n            f\"Epoch {epoch}: train_loss={t_loss:.4f}, val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%\"\n        )\n\n    test_loss, test_acc = run_epoch(model, test_loader, criterion)\n    print(f\"Dropout {dp}: Test accuracy {test_acc*100:.2f}%\")\n\n    # predictions / ground truth\n    preds_all, gts_all = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds_all.extend(logits.argmax(1).cpu().numpy())\n            gts_all.extend(batch[\"labels\"].cpu().numpy())\n\n    # store results\n    experiment_data[\"dropout_tuning\"][\"SPR_BENCH\"][str(dp)] = {\n        \"metrics\": metrics,\n        \"losses\": losses,\n        \"predictions\": preds_all,\n        \"ground_truth\": gts_all,\n        \"test_acc\": test_acc,\n        \"test_loss\": test_loss,\n    }\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, numpy as np, torch, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"contextual_big_transformer\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"test_acc\": None,\n            \"test_f1\": None,\n            \"test_loss\": None,\n        }\n    }\n}\n\n# -----------------------------------------------------------------------------#\n# directories / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH ------------------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH.  Set SPR_DATA or SPR_DATASET_PATH env var.\"\n    )\n\n\n# -----------------------------------------------------------------------------#\n# dataset helpers -------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\nCLS_TOKEN = \"<cls>\"\nPAD_TOKEN = \"<pad>\"\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[PAD_TOKEN]\n        self.cls_id = vocab[CLS_TOKEN]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        ids = [self.cls_id] + [self.vocab[ch] for ch in seq]\n        return torch.tensor(ids, dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": self._encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {PAD_TOKEN: 0, CLS_TOKEN: 1}\n    for ch in sorted(chars):\n        if ch not in vocab:\n            vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id=0):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass SimpleTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=128,\n        nhead=8,\n        num_layers=4,\n        dim_ff=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        cls_rep = x[:, 0]  # representation of CLS token\n        return self.classifier(cls_rep)\n\n\n# -----------------------------------------------------------------------------#\n# loss ------------------------------------------------------------------------#\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, eps=0.1):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, logits, target):\n        n = logits.size(-1)\n        log_probs = torch.log_softmax(logits, dim=-1)\n        loss = -log_probs.gather(1, target.view(-1, 1)).squeeze(1)\n        loss = (1 - self.eps) * loss - self.eps * log_probs.mean(dim=-1)\n        return loss.mean()\n\n\n# -----------------------------------------------------------------------------#\n# metrics ---------------------------------------------------------------------#\ndef macro_f1(preds: torch.Tensor, targets: torch.Tensor, num_labels: int):\n    f1s = []\n    for lbl in range(num_labels):\n        tp = ((preds == lbl) & (targets == lbl)).sum().item()\n        fp = ((preds == lbl) & (targets != lbl)).sum().item()\n        fn = ((preds != lbl) & (targets == lbl)).sum().item()\n        prec = tp / (tp + fp + 1e-9)\n        rec = tp / (tp + fn + 1e-9)\n        if prec + rec == 0:\n            f1s.append(0.0)\n        else:\n            f1s.append(2 * prec * rec / (prec + rec))\n    return sum(f1s) / num_labels\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, criterion, num_labels, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    epoch_loss, correct, total = 0.0, 0, 0\n    preds_all, gts_all = [], []\n    with torch.set_grad_enabled(train_mode):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = criterion(outputs, batch[\"labels\"])\n            if train_mode:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            epoch_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = outputs.argmax(1)\n            preds_all.append(preds.cpu())\n            gts_all.append(batch[\"labels\"].cpu())\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    preds_all = torch.cat(preds_all)\n    gts_all = torch.cat(gts_all)\n    acc = correct / total\n    f1 = macro_f1(preds_all, gts_all, num_labels)\n    return epoch_loss / total, acc, f1, preds_all, gts_all\n\n\n# -----------------------------------------------------------------------------#\n# main pipeline ---------------------------------------------------------------#\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nprint(\"Loaded SPR_BENCH splits:\", list(spr.keys()))\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocab size: {len(vocab)}\")\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"]) + 1  # +CLS\nprint(f\"Max sequence length (with CLS): {max_len}\")\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[PAD_TOKEN]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[PAD_TOKEN]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, pad_id=vocab[PAD_TOKEN]),\n)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\nepochs = 8\ncriterion = LabelSmoothingCE(eps=0.1)\n\nmodel = SimpleTransformerClassifier(\n    len(vocab), num_labels, max_len=max_len, dropout=0.1\n).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n\nbest_val_f1, best_state = 0.0, None\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1, _, _ = run_epoch(\n        model, train_loader, criterion, num_labels, optimizer\n    )\n    v_loss, v_acc, v_f1, _, _ = run_epoch(\n        model, dev_loader, criterion, num_labels, optimizer=None\n    )\n    experiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train_acc\"\n    ].append(t_acc)\n    experiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val_acc\"\n    ].append(v_acc)\n    experiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train_f1\"\n    ].append(t_f1)\n    experiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"metrics\"][\n        \"val_f1\"\n    ].append(v_f1)\n    experiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"losses\"][\n        \"train\"\n    ].append(t_loss)\n    experiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        v_loss\n    )\n    print(\n        f\"Epoch {epoch}: val_loss={v_loss:.4f}, val_acc={v_acc*100:.2f}%, val_macroF1={v_f1:.4f}\"\n    )\n    if v_f1 > best_val_f1:\n        best_val_f1 = v_f1\n        best_state = {\n            k: v.clone().detach().cpu() for k, v in model.state_dict().items()\n        }\n\n# load best model\nmodel.load_state_dict(best_state)\ntest_loss, test_acc, test_f1, preds_all, gts_all = run_epoch(\n    model, test_loader, criterion, num_labels, optimizer=None\n)\nexperiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\n    \"predictions\"\n] = preds_all.numpy()\nexperiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\n    \"ground_truth\"\n] = gts_all.numpy()\nexperiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"test_acc\"] = test_acc\nexperiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"test_f1\"] = test_f1\nexperiment_data[\"contextual_big_transformer\"][\"SPR_BENCH\"][\"test_loss\"] = test_loss\n\nprint(\n    f\"\\nBEST MODEL -- Test accuracy: {test_acc*100:.2f}%, Test macro-F1: {test_f1:.4f}\"\n)\n\n# -----------------------------------------------------------------------------#\n# save all experiment data ----------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f'Experiment data saved to {os.path.join(working_dir, \"experiment_data.npy\")}')\n", "import os, pathlib, torch, numpy as np, time, math, random\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# -----------------------------------------------------------------------------#\n# working directory & device                                                   #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n# experiment data container                                                    #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -----------------------------------------------------------------------------#\n# locate SPR_BENCH                                                             #\ndef _find_spr_bench() -> pathlib.Path:\n    for p in [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]:\n        if p and p.exists():\n            needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n            if needed.issubset({f.name for f in p.iterdir()}):\n                print(f\"Found SPR_BENCH at {p}\")\n                return p\n    raise FileNotFoundError(\"SPR_BENCH not found; set SPR_DATA env var\")\n\n\n# -----------------------------------------------------------------------------#\n# dataset                                                                      #\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name):  # hugface CSV loader\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.data = hf_ds\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq):\n        seq = seq.replace(\" \", \"\")\n        return torch.tensor([self.vocab[ch] for ch in seq], dtype=torch.long)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        seq_tensor = self._encode(row[\"sequence\"])\n        lbl = int(row[\"label\"])\n        # auxiliary targets\n        parity = len(seq_tensor) % 2  # 0 even, 1 odd\n        uniq_bucket = min(len(set(seq_tensor.tolist())), 10) - 1  # 0-9\n        return {\n            \"input_ids\": seq_tensor,\n            \"labels\": torch.tensor(lbl, dtype=torch.long),\n            \"parity\": torch.tensor(parity, dtype=torch.long),\n            \"uniq\": torch.tensor(uniq_bucket, dtype=torch.long),\n        }\n\n\ndef collate_fn(batch, pad_id):\n    keys = [\"input_ids\", \"labels\", \"parity\", \"uniq\"]\n    seqs = [b[\"input_ids\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attention = (padded != pad_id).long()\n    collated = {\n        \"input_ids\": padded,\n        \"attention_mask\": attention,\n    }\n    for k in keys[1:]:\n        collated[k] = torch.stack([b[k] for b in batch])\n    return collated\n\n\n# -----------------------------------------------------------------------------#\n# model                                                                        #\nclass MultiTaskTransformer(nn.Module):\n    def __init__(\n        self, vocab, d_model, nhead, nlayers, num_labels, max_len=512, dropout=0.1\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(len(vocab), d_model, padding_idx=0)\n        self.pos_embed = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, d_model * 4, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.pool = lambda x, m: (x * m.unsqueeze(-1)).sum(1) / m.sum(1, keepdim=True)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n        self.uniq_head = nn.Linear(d_model, 10)\n\n    def forward(self, input_ids, attention_mask):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        pooled = self.pool(x, attention_mask)\n        return {\n            \"main\": self.main_head(pooled),\n            \"parity\": self.parity_head(pooled),\n            \"uniq\": self.uniq_head(pooled),\n        }\n\n\n# -----------------------------------------------------------------------------#\n# training / evaluation                                                        #\ndef run_epoch(model, loader, crit_main, crit_aux, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    epoch_loss, y_true, y_pred = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        with torch.set_grad_enabled(train):\n            out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss_main = crit_main(out[\"main\"], batch[\"labels\"])\n            loss_parity = crit_aux(out[\"parity\"], batch[\"parity\"])\n            loss_uniq = crit_aux(out[\"uniq\"], batch[\"uniq\"])\n            loss = loss_main + 0.2 * (loss_parity + loss_uniq)\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        epoch_loss += loss.item() * batch[\"labels\"].size(0)\n        y_true.extend(batch[\"labels\"].cpu().tolist())\n        y_pred.extend(out[\"main\"].argmax(1).cpu().tolist())\n    avg_loss = epoch_loss / len(loader.dataset)\n    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n    return avg_loss, macro_f1, y_true, y_pred\n\n\n# -----------------------------------------------------------------------------#\n# pipeline                                                                     #\ndata_dir = _find_spr_bench()\nspr = load_spr_bench(data_dir)\nvocab = build_vocab(spr[\"train\"])\nmax_len = max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in spr[\"train\"])\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\n\ntrain_ds = SPRCharDataset(spr[\"train\"], vocab)\ndev_ds = SPRCharDataset(spr[\"dev\"], vocab)\ntest_ds = SPRCharDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\n\nmodel = MultiTaskTransformer(\n    vocab,\n    d_model=96,\n    nhead=4,\n    nlayers=3,\n    num_labels=num_labels,\n    max_len=max_len,\n    dropout=0.1,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncrit_main = nn.CrossEntropyLoss()\ncrit_aux = nn.CrossEntropyLoss()\n\nbest_f1, best_state = 0.0, None\nepochs = 5\nfor epoch in range(1, epochs + 1):\n    t_loss, t_f1, _, _ = run_epoch(model, train_loader, crit_main, crit_aux, optimizer)\n    v_loss, v_f1, _, _ = run_epoch(model, dev_loader, crit_main, crit_aux)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(v_f1)\n    print(f\"Epoch {epoch}: validation_loss = {v_loss:.4f}, Macro-F1 = {v_f1:.4f}\")\n    if v_f1 > best_f1:\n        best_f1, best_state = v_f1, {\n            k: v.clone().detach().cpu() for k, v in model.state_dict().items()\n        }\n\n# restore best\nmodel.load_state_dict(best_state)\n\n# final test\ntest_loss, test_f1, gts, preds = run_epoch(model, test_loader, crit_main, crit_aux)\nprint(f\"Test Macro-F1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Experiment data saved.\")\n", "import os, pathlib, math, numpy as np, torch, time\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# obligatory working dir & device                                              #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n# experiment data container                                                    #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -----------------------------------------------------------------------------#\n# locate dataset                                                               #\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\ndata_root = _find_spr_bench()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(root / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(data_root)\n\n\n# -----------------------------------------------------------------------------#\n# dataset / vocab                                                              #\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n        self.cls_id = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def encode(self, seq: str):\n        toks = seq.strip().split()\n        return [self.cls_id] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        return {\"input_ids\": ids, \"labels\": label}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocabulary size: {len(vocab)}\")\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])  # +1 for CLS\nprint(\"Max token length:\", max_len)\n\n\n# -----------------------------------------------------------------------------#\n# model                                                                        #\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SymbolicTransformer(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        d_model=128,\n        nhead=8,\n        nlayers=4,\n        ff=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, dropout, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(encoder_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.classifier = nn.Linear(d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls_vec = x[:, 0]  # [CLS] position\n        logits = self.classifier(self.norm(cls_vec))\n        return logits\n\n\nmodel = SymbolicTransformer(len(vocab), num_labels).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\ncriterion = nn.CrossEntropyLoss()\n\n\n# -----------------------------------------------------------------------------#\n# metrics                                                                      #\ndef macro_f1(preds, labels, num_cls):\n    preds = preds.cpu().numpy()\n    labels = labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n        if tp == fp == fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp) if tp + fp > 0 else 0\n        rec = tp / (tp + fn) if tp + fn > 0 else 0\n        f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops                                                           #\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss, tot_correct, tot_cnt = 0.0, 0, 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss = criterion(logits, batch[\"labels\"])\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        with torch.no_grad():\n            preds = logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# -----------------------------------------------------------------------------#\n# training                                                                     #\nepochs = 8\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, train=True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, train=False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: val_loss = {v_loss:.4f}, val_acc = {v_acc*100:.2f}%, val_macroF1 = {v_f1:.4f}\"\n    )\n\n# -----------------------------------------------------------------------------#\n# test evaluation                                                              #\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}%, Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\n\n# -----------------------------------------------------------------------------#\n# save                                                                         #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "import os, pathlib, numpy as np, torch, random, math, time\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# working dir / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"token_level_transformer\": {\n        \"SPR_BENCH\": {\n            \"metrics\": {\"train_f1\": [], \"val_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# -----------------------------------------------------------------------------#\n# helper to locate dataset ----------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    files = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and files.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at: {c}\")\n            return c\n    raise FileNotFoundError(\"SPR_BENCH not found \u2013 set SPR_DATA or SPR_DATASET_PATH\")\n\n\n# -----------------------------------------------------------------------------#\n# load dataset ----------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict({s: _load(f\"{s}.csv\") for s in (\"train\", \"dev\", \"test\")})\n\n\ndata_root = _find_spr_bench()\nspr = load_spr_bench(data_root)\n\n# -----------------------------------------------------------------------------#\n# tokenisation ----------------------------------------------------------------#\nCLS_TOKEN = \"<cls>\"\nPAD_TOKEN = \"<pad>\"\n\n\ndef build_token_vocab(train_split):\n    tokens = set()\n    for ex in train_split:\n        tokens.update(ex[\"sequence\"].strip().split())\n    vocab = {PAD_TOKEN: 0, CLS_TOKEN: 1}\n    for tok in sorted(tokens):\n        if tok not in vocab:\n            vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_token_vocab(spr[\"train\"])\nvocab_size = len(vocab)\npad_id = vocab[PAD_TOKEN]\ncls_id = vocab[CLS_TOKEN]\nprint(f\"Vocab size (token level): {vocab_size}\")\n\n\ndef encode(seq: str):\n    ids = [cls_id] + [vocab[tok] for tok in seq.strip().split()]\n    return torch.tensor(ids, dtype=torch.long)\n\n\nmax_len = max(\n    len(ex[\"sequence\"].strip().split()) + 1 for ex in spr[\"train\"]\n)  # +1 for CLS\nprint(f\"Max token length (with CLS): {max_len}\")\n\n\n# -----------------------------------------------------------------------------#\n# Dataset / collate -----------------------------------------------------------#\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        return {\n            \"input_ids\": encode(row[\"sequence\"]),\n            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n        }\n\n\ndef collate_fn(batch):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn_mask = (padded != pad_id).long()\n    return {\"input_ids\": padded, \"attention_mask\": attn_mask, \"labels\": labels}\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"])\ndev_ds = SPRTokenDataset(spr[\"dev\"])\ntest_ds = SPRTokenDataset(spr[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)\n\nnum_labels = len(set(int(ex[\"label\"]) for ex in spr[\"train\"]))\nprint(f\"Number of classes: {num_labels}\")\n\n\n# -----------------------------------------------------------------------------#\n# Macro-F1 utility ------------------------------------------------------------#\ndef macro_f1_score(y_true, y_pred, num_classes):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((y_true == c) & (y_pred == c))\n        fp = np.sum((y_true != c) & (y_pred == c))\n        fn = np.sum((y_true == c) & (y_pred != c))\n        if tp == 0 and (fp == 0 or fn == 0):\n            f1 = 0.0\n        else:\n            prec = tp / (tp + fp + 1e-9)\n            rec = tp / (tp + fn + 1e-9)\n            f1 = 2 * prec * rec / (prec + rec + 1e-9)\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# Model -----------------------------------------------------------------------#\nclass TokenTransformerClassifier(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=128,\n        nhead=8,\n        num_layers=4,\n        dim_ff=256,\n        dropout=0.1,\n        mlm_prob=0.15,\n    ):\n        super().__init__()\n        self.mlm_prob = mlm_prob\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos_embed = nn.Parameter(torch.zeros(max_len, d_model))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.cls_head = nn.Linear(d_model, num_labels)\n        # MLM head shares weights with embedding\n        self.mlm_head = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_ids, attention_mask, compute_mlm=False):\n        seq_len = input_ids.size(1)\n        x = self.embed(input_ids) + self.pos_embed[:seq_len].unsqueeze(0)\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        cls_rep = x[:, 0]  # CLS position\n        logits_cls = self.cls_head(self.dropout(cls_rep))\n        if compute_mlm:\n            mlm_logits = self.mlm_head(self.dropout(x))\n            return logits_cls, mlm_logits\n        else:\n            return logits_cls\n\n\n# -----------------------------------------------------------------------------#\n# Training utilities ----------------------------------------------------------#\ndef mask_tokens(inputs):\n    \"\"\"Randomly mask tokens for MLM objective\"\"\"\n    inputs = inputs.clone()\n    labels = inputs.clone()\n    probability_matrix = torch.full(labels.shape, model.mlm_prob, device=inputs.device)\n    special_mask = (inputs == pad_id) | (inputs == cls_id)\n    probability_matrix.masked_fill_(special_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = -100  # ignore index\n    inputs[masked_indices] = vocab[PAD_TOKEN]  # simple mask as PAD\n    return inputs, labels\n\n\ndef run_epoch(model, loader, cls_criterion, mlm_criterion=None, optimizer=None):\n    train_mode = optimizer is not None\n    model.train() if train_mode else model.eval()\n    total_loss, cls_losses, mlm_losses = 0.0, 0.0, 0.0\n    preds_all, gts_all = [], []\n    for batch in loader:\n        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n        if train_mode and mlm_criterion is not None:\n            inputs_masked, mlm_labels = mask_tokens(batch[\"input_ids\"])\n            logits_cls, logits_mlm = model(\n                inputs_masked, batch[\"attention_mask\"], compute_mlm=True\n            )\n            loss_cls = cls_criterion(logits_cls, batch[\"labels\"])\n            loss_mlm = mlm_criterion(\n                logits_mlm.view(-1, vocab_size), mlm_labels.view(-1)\n            )\n            loss = loss_cls + 0.5 * loss_mlm\n        else:\n            logits_cls = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss = cls_criterion(logits_cls, batch[\"labels\"])\n            loss_cls, loss_mlm = loss, torch.tensor(0.0, device=device)\n        if train_mode:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        cls_losses += loss_cls.item() * batch[\"labels\"].size(0)\n        mlm_losses += loss_mlm.item() * batch[\"labels\"].size(0)\n        preds_all.extend(logits_cls.argmax(1).detach().cpu().numpy())\n        gts_all.extend(batch[\"labels\"].cpu().numpy())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = macro_f1_score(gts_all, preds_all, num_labels)\n    return avg_loss, macro_f1\n\n\n# -----------------------------------------------------------------------------#\n# Instantiate model / losses / optimiser --------------------------------------#\nmodel = TokenTransformerClassifier(vocab_size, num_labels, max_len).to(device)\ncls_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\nmlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n\n# -----------------------------------------------------------------------------#\n# Training loop with early stopping ------------------------------------------#\nbest_val_f1, patience, epochs, waited = 0.0, 4, 20, 0\nfor epoch in range(1, epochs + 1):\n    t_start = time.time()\n    tr_loss, tr_f1 = run_epoch(model, train_loader, cls_loss_fn, mlm_loss_fn, optimizer)\n    val_loss, val_f1 = run_epoch(model, dev_loader, cls_loss_fn)\n    experiment_data[\"token_level_transformer\"][\"SPR_BENCH\"][\"metrics\"][\n        \"train_f1\"\n    ].append(tr_f1)\n    experiment_data[\"token_level_transformer\"][\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(\n        val_f1\n    )\n    experiment_data[\"token_level_transformer\"][\"SPR_BENCH\"][\"losses\"][\"train\"].append(\n        tr_loss\n    )\n    experiment_data[\"token_level_transformer\"][\"SPR_BENCH\"][\"losses\"][\"val\"].append(\n        val_loss\n    )\n    print(\n        f\"Epoch {epoch}: val_loss = {val_loss:.4f} | Macro-F1 = {val_f1*100:.2f}% | time {time.time()-t_start:.1f}s\"\n    )\n    if val_f1 > best_val_f1 + 1e-4:\n        best_val_f1 = val_f1\n        waited = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        waited += 1\n        if waited >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# -----------------------------------------------------------------------------#\n# Evaluation on test ----------------------------------------------------------#\nmodel.load_state_dict(\n    torch.load(os.path.join(working_dir, \"best_model.pt\"), map_location=device)\n)\ntest_loss, test_f1 = run_epoch(model, test_loader, cls_loss_fn)\nprint(f\"TEST Macro-F1: {test_f1*100:.2f}%\")\n\n# gather predictions for saving\nmodel.eval()\npreds_all, gts_all = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n        logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.extend(logits.argmax(1).cpu().numpy())\n        gts_all.extend(batch[\"labels\"].cpu().numpy())\nexperiment_data[\"token_level_transformer\"][\"SPR_BENCH\"][\"predictions\"] = preds_all\nexperiment_data[\"token_level_transformer\"][\"SPR_BENCH\"][\"ground_truth\"] = gts_all\n\n# -----------------------------------------------------------------------------#\n# save experiment data --------------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {os.path.join(working_dir,'experiment_data.npy')}\")\n", "import os, pathlib, numpy as np, torch, math, time, random\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import f1_score\n\n# -----------------------------------------------------------------------------#\n# working dir / device --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n# experiment data container ---------------------------------------------------#\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -----------------------------------------------------------------------------#\n# utility: locate dataset -----------------------------------------------------#\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(os.getenv(\"SPR_DATASET_PATH\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    req = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and req.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\"SPR_BENCH folder not found; set env vars if necessary.\")\n\n\n# -----------------------------------------------------------------------------#\n# load dataset ----------------------------------------------------------------#\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / f\"{name}.csv\"),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\"train\": _load(\"train\"), \"dev\": _load(\"dev\"), \"test\": _load(\"test\")}\n    )\n\n\n# -----------------------------------------------------------------------------#\n# vocab / dataset classes -----------------------------------------------------#\nclass SPRCharDataset(Dataset):\n    def __init__(self, hf_split, vocab, max_len):\n        self.data = hf_split\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n        self.cls_id = vocab[\"<cls>\"]\n        self.max_len = max_len\n\n    def _encode(self, seq: str):\n        seq = seq.replace(\" \", \"\")\n        ids = [self.cls_id] + [self.vocab[ch] for ch in seq]\n        return torch.tensor(ids, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        ids = self._encode(row[\"sequence\"])\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor(\n            len(row[\"sequence\"].replace(\" \", \"\")) % 2, dtype=torch.long\n        )\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    chars = {ch for ex in train_split for ch in ex[\"sequence\"].replace(\" \", \"\")}\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    return vocab\n\n\ndef collate_fn(batch, pad_id):\n    keys = [\"input_ids\", \"labels\", \"parity\"]\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    mask = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": mask,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\n# -----------------------------------------------------------------------------#\n# model -----------------------------------------------------------------------#\nclass TransformerWithAux(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        num_labels,\n        max_len,\n        d_model=128,\n        nhead=4,\n        nlayers=3,\n        d_ff=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.zeros(max_len + 1, d_model))\n        nn.init.trunc_normal_(self.pos, std=0.02)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, d_ff, dropout, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n        self.cls_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        B, L = input_ids.size()\n        x = self.embed(input_ids) + self.pos[:L]\n        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n        cls_tok = x[:, 0, :]  # representation of [CLS]\n        logits_main = self.cls_head(cls_tok)\n        logits_parity = self.parity_head(cls_tok)\n        return logits_main, logits_parity\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loops -----------------------------------------------------------#\ndef run_epoch(model, loader, crit_main, crit_aux, aux_lambda, optimizer=None):\n    train = optimizer is not None\n    model.train() if train else model.eval()\n    tot_loss = tot_acc = tot_size = 0.0\n    all_preds, all_gts = [], []\n    with torch.set_grad_enabled(train):\n        for batch in loader:\n            batch = {\n                k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()\n            }\n            logits_main, logits_aux = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n            loss_main = crit_main(logits_main, batch[\"labels\"])\n            loss_aux = crit_aux(logits_aux, batch[\"parity\"])\n            loss = loss_main + aux_lambda * loss_aux\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            tot_loss += loss.item() * batch[\"labels\"].size(0)\n            preds = logits_main.argmax(1)\n            tot_acc += (preds == batch[\"labels\"]).sum().item()\n            tot_size += batch[\"labels\"].size(0)\n            all_preds.extend(preds.cpu().numpy())\n            all_gts.extend(batch[\"labels\"].cpu().numpy())\n    acc = tot_acc / tot_size\n    f1 = f1_score(all_gts, all_preds, average=\"macro\")\n    return tot_loss / tot_size, acc, f1\n\n\n# -----------------------------------------------------------------------------#\n# pipeline --------------------------------------------------------------------#\ndata_dir = _find_spr_bench()\ndset = load_spr_bench(data_dir)\nnum_labels = len(set(int(ex[\"label\"]) for ex in dset[\"train\"]))\nvocab = build_vocab(dset[\"train\"])\nmax_len = (\n    max(len(ex[\"sequence\"].replace(\" \", \"\")) for ex in dset[\"train\"]) + 1\n)  # +1 for CLS\nprint(f\"Vocab size {len(vocab)}, max_len {max_len}, num_labels {num_labels}\")\n\ntrain_ds = SPRCharDataset(dset[\"train\"], vocab, max_len)\ndev_ds = SPRCharDataset(dset[\"dev\"], vocab, max_len)\ntest_ds = SPRCharDataset(dset[\"test\"], vocab, max_len)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\n\nmodel = TransformerWithAux(len(vocab), num_labels, max_len, dropout=0.1).to(device)\ntry:\n    crit_main = nn.CrossEntropyLoss(label_smoothing=0.1)\nexcept TypeError:  # fallback if older torch\n    crit_main = nn.CrossEntropyLoss()\ncrit_aux = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\naux_lambda = 0.3\nepochs = 8\nbest_val_f1 = -1.0\nbest_state = None\n\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(\n        model, train_loader, crit_main, crit_aux, aux_lambda, optimizer\n    )\n    v_loss, v_acc, v_f1 = run_epoch(model, dev_loader, crit_main, crit_aux, aux_lambda)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append({\"acc\": t_acc, \"f1\": t_f1})\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append({\"acc\": v_acc, \"f1\": v_f1})\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | Val Acc {v_acc*100:.2f}% | Val Macro-F1 {v_f1:.4f}\"\n    )\n    if v_f1 > best_val_f1:\n        best_val_f1 = v_f1\n        best_state = model.state_dict()\n\n# -----------------------------------------------------------------------------#\n# evaluate on test ------------------------------------------------------------#\nmodel.load_state_dict(best_state)\ntest_loss, test_acc, test_f1 = run_epoch(\n    model, test_loader, crit_main, crit_aux, aux_lambda\n)\nprint(\n    f\"TEST  ->  loss {test_loss:.4f} | Acc {test_acc*100:.2f}% | Macro-F1 {test_f1:.4f}\"\n)\n\n# collect predictions / ground truth for test set\nmodel.eval()\npreds_all, gts_all = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.extend(logits.argmax(1).cpu().numpy())\n        gts_all.extend(batch[\"labels\"].cpu().numpy())\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts_all\n\n# save everything -------------------------------------------------------------#\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# required working dir and device                                              #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n# experiment data container                                                    #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -----------------------------------------------------------------------------#\n# locate and load dataset                                                      #\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\ndata_root = _find_spr_bench()\n\n\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_name):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(data_root)\n\n\n# -----------------------------------------------------------------------------#\n# vocabulary                                                                   #\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].strip().split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(f\"Vocabulary size: {len(vocab)}\")\n\n\n# -----------------------------------------------------------------------------#\n# dataset object                                                               #\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_dataset, vocab):\n        self.data = hf_dataset\n        self.vocab = vocab\n        self.cls_id = vocab[\"<cls>\"]\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _encode(self, seq: str):\n        toks = seq.strip().split()\n        return [self.cls_id] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        ids = torch.tensor(self._encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        length = len(row[\"sequence\"].split())\n        parity = torch.tensor(length % 2, dtype=torch.long)\n        distinct_ct = min(len(set(row[\"sequence\"].split())), 10)  # bucket 0-10+\n        distinct = torch.tensor(distinct_ct, dtype=torch.long)\n        return {\n            \"input_ids\": ids,\n            \"labels\": label,\n            \"parity\": parity,\n            \"distinct\": distinct,\n        }\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    distinct = torch.stack([b[\"distinct\"] for b in batch])\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n        \"distinct\": distinct,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_rule_labels = len({int(r[\"label\"]) for r in spr[\"train\"]})\nnum_parity_labels = 2\nnum_distinct_labels = 11\nmax_len = max(len(r[\"sequence\"].split()) + 1 for r in spr[\"train\"])\nprint(f\"Max token length: {max_len}\")\n\n\n# -----------------------------------------------------------------------------#\n# model                                                                        #\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):  # x: [B,T,D]\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRMultiHeadTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, d_model=160, nhead=8, nlayers=6, ff=320, dropout=0.2\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, dropout, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_rule_labels)\n        self.parity_head = nn.Linear(d_model, num_parity_labels)\n        self.distinct_head = nn.Linear(d_model, num_distinct_labels)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return (self.main_head(cls), self.parity_head(cls), self.distinct_head(cls))\n\n\nmodel = SPRMultiHeadTransformer(len(vocab)).to(device)\n\n\n# -----------------------------------------------------------------------------#\n# loss functions & optimizer                                                   #\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        n = logits.size(1)\n        log_prob = nn.functional.log_softmax(logits, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_prob)\n            true_dist.fill_(self.smoothing / (n - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_prob, dim=1))\n\n\nmain_criterion = LabelSmoothingCE(0.1)\naux_criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\n\n# -----------------------------------------------------------------------------#\n# metrics                                                                      #\ndef macro_f1(preds, labels, num_cls):\n    preds = preds.cpu().numpy()\n    labels = labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n        if tp == fp == fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp) if tp + fp > 0 else 0\n        rec = tp / (tp + fn) if tp + fn > 0 else 0\n        f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# training & evaluation loops                                                  #\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0.0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits, distinct_logits = model(\n            batch[\"input_ids\"], batch[\"attention_mask\"]\n        )\n        loss_main = main_criterion(main_logits, batch[\"labels\"])\n        loss_par = aux_criterion(parity_logits, batch[\"parity\"])\n        loss_dist = aux_criterion(distinct_logits, batch[\"distinct\"])\n        loss = loss_main + 0.2 * loss_par + 0.2 * loss_dist\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_rule_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# -----------------------------------------------------------------------------#\n# main training loop                                                           #\nepochs = 6\nbest_f1 = 0.0\nfor ep in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, train=True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, train=False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(v_f1)\n    print(f\"Epoch {ep}: validation_loss = {v_loss:.4f}, val_macroF1 = {v_f1:.4f}\")\n    # simple checkpoint\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n\n# -----------------------------------------------------------------------------#\n# test evaluation with best checkpoint                                         #\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_rule_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}%, Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\n\n# -----------------------------------------------------------------------------#\n# save experiment data                                                         #\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "import os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- house-keeping & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment data container ------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor(\n            (len(ids) - 1) % 2, dtype=torch.long\n        )  # even/odd length (exclude CLS)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | val_acc = {v_acc*100:.2f}% | val_macroF1 = {v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "import os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# -----------------------------------------------------------------------------#\n# working dir & device                                                         #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------#\n# experiment data                                                              #\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macroF1\": [], \"val_macroF1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# -----------------------------------------------------------------------------#\n# locate dataset                                                               #\ndef find_spr_bench():\n    opts = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\"),\n        pathlib.Path(\"../SPR_BENCH\"),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\"),\n    ]\n    for p in opts:\n        if p and p.exists() and (p / \"train.csv\").exists():\n            return p.resolve()\n    raise FileNotFoundError(\"SPR_BENCH folder not found\")\n\n\ndata_root = find_spr_bench()\n\n\ndef load_spr(root):\n    def _ld(split):\n        return load_dataset(\n            \"csv\", data_files=str(root / split), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\"train\": _ld(\"train.csv\"), \"dev\": _ld(\"dev.csv\"), \"test\": _ld(\"test.csv\")}\n    )\n\n\nspr = load_spr(data_root)\n\n\n# -----------------------------------------------------------------------------#\n# vocab                                                                         #\ndef build_vocab(ds):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in ds:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\n# -----------------------------------------------------------------------------#\n# dataset                                                                       #\nclass SPRDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.data = hf_ds\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n        self.cls_id = vocab[\"<cls>\"]\n\n    def encode(self, seq):\n        return [self.cls_id] + [self.vocab[t] for t in seq.split()]\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        # auxiliary labels\n        length_parity = torch.tensor(len(ids) % 2, dtype=torch.long)\n        uniq = len(set(row[\"sequence\"].split()))\n        uniq_clip = min(uniq, 10)  # 0-10 bucket\n        uniq_label = torch.tensor(uniq_clip, dtype=torch.long)\n        return {\n            \"input_ids\": ids,\n            \"labels\": label,\n            \"parity\": length_parity,\n            \"uniq\": uniq_label,\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n\ndef collate_fn(batch, pad_id):\n    keys = [\"input_ids\", \"labels\", \"parity\", \"uniq\"]\n    col = {k: [b[k] for b in batch] for k in keys}\n    seqs = nn.utils.rnn.pad_sequence(\n        col[\"input_ids\"], batch_first=True, padding_value=pad_id\n    )\n    attn = (seqs != pad_id).long()\n    return {\n        \"input_ids\": seqs,\n        \"attention_mask\": attn,\n        \"labels\": torch.stack(col[\"labels\"]),\n        \"parity\": torch.stack(col[\"parity\"]),\n        \"uniq\": torch.stack(col[\"uniq\"]),\n    }\n\n\ntrain_ds, dev_ds, test_ds = (\n    SPRDataset(spr[s], vocab) for s in [\"train\", \"dev\", \"test\"]\n)\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(x[\"label\"]) for x in spr[\"train\"]})\nmax_len = max(len(x[\"sequence\"].split()) + 1 for x in spr[\"train\"])\nprint(\"Num classes:\", num_labels, \"| max len:\", max_len)\n\n\n# -----------------------------------------------------------------------------#\n# model                                                                         #\nclass PosEnc(nn.Module):\n    def __init__(self, d, mlen):\n        super().__init__()\n        pe = torch.zeros(mlen, d)\n        pos = torch.arange(0, mlen).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d, 2) * -(math.log(10000.0) / d))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab, classes, aux_par=2, aux_uniq=11, d=192, heads=8, layers=4, drop=0.1\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab, d, padding_idx=0)\n        self.pos = PosEnc(d, max_len + 5)\n        enc_layer = nn.TransformerEncoderLayer(d, heads, 4 * d, drop, batch_first=True)\n        self.enc = nn.TransformerEncoder(enc_layer, layers)\n        self.norm = nn.LayerNorm(d)\n        self.main_head = nn.Linear(d, classes)\n        self.par_head = nn.Linear(d, aux_par)\n        self.uniq_head = nn.Linear(d, aux_uniq)\n\n    def forward(self, ids, mask):\n        x = self.embed(ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.par_head(cls), self.uniq_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# -----------------------------------------------------------------------------#\n# optimizer / scheduler                                                         #\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_aux = nn.CrossEntropyLoss()\n\n\n# -----------------------------------------------------------------------------#\n# metrics                                                                       #\ndef macro_f1(preds, labels, ncls):\n    p, l = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(ncls):\n        tp = ((p == c) & (l == c)).sum()\n        fp = ((p == c) & (l != c)).sum()\n        fn = ((p != c) & (l == c)).sum()\n        prec = tp / (tp + fp) if tp + fp else 0\n        rec = tp / (tp + fn) if tp + fn else 0\n        f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# train / eval loop                                                             #\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss, preds_all, labels_all = 0, [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, par_logits, uniq_logits = model(\n            batch[\"input_ids\"], batch[\"attention_mask\"]\n        )\n        loss = (\n            criterion_main(main_logits, batch[\"labels\"])\n            + 0.3 * criterion_aux(par_logits, batch[\"parity\"])\n            + 0.3 * criterion_aux(uniq_logits, batch[\"uniq\"])\n        )\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        with torch.no_grad():\n            preds_all.append(main_logits.argmax(1))\n            labels_all.append(batch[\"labels\"])\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\n    macro = macro_f1(preds_all, labels_all, num_labels)\n    return tot_loss / len(loader.dataset), macro\n\n\n# -----------------------------------------------------------------------------#\n# training loop with early stopping                                            #\nbest_macro, patience, max_pat = 0, 0, 3\nepochs = 15\nfor epoch in range(1, epochs + 1):\n    tr_loss, tr_macro = run_epoch(train_loader, True)\n    val_loss, val_macro = run_epoch(dev_loader, False)\n    sched.step()\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macroF1\"].append(tr_macro)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macroF1\"].append(val_macro)\n    print(\n        f\"Epoch {epoch}: validation_loss = {val_loss:.4f} | val_macroF1 = {val_macro:.4f}\"\n    )\n    if val_macro > best_macro:\n        best_macro = val_macro\n        patience = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best.pt\"))\n    else:\n        patience += 1\n        if patience > max_pat:\n            print(\"Early stopping\")\n            break\n\n# -----------------------------------------------------------------------------#\n# load best & test                                                             #\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best.pt\")))\nmodel.eval()\npreds_all, labels_all = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main, _, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(main.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\npreds_all = torch.cat(preds_all)\nlabels_all = torch.cat(labels_all)\ntest_macro = macro_f1(preds_all, labels_all, num_labels)\ntest_acc = (preds_all == labels_all).float().mean().item()\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_macro:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- house-keeping & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment data container ------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor(\n            (len(ids) - 1) % 2, dtype=torch.long\n        )  # even/odd length (exclude CLS)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | val_acc = {v_acc*100:.2f}% | val_macroF1 = {v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- house-keeping & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment data container ------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor(\n            (len(ids) - 1) % 2, dtype=torch.long\n        )  # even/odd length (exclude CLS)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | val_acc = {v_acc*100:.2f}% | val_macroF1 = {v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, math, time, numpy as np, torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------- house-keeping & device ---------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --------------------- experiment data container ------------------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# --------------------- locate & load dataset ----------------------------------\ndef _find_spr_bench() -> pathlib.Path:\n    candidates = [\n        pathlib.Path(os.getenv(\"SPR_DATA\", \"\")),\n        pathlib.Path(\"./SPR_BENCH\").resolve(),\n        pathlib.Path(\"../SPR_BENCH\").resolve(),\n        pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH\").resolve(),\n    ]\n    needed = {\"train.csv\", \"dev.csv\", \"test.csv\"}\n    for c in candidates:\n        if c and c.exists() and needed.issubset({p.name for p in c.iterdir()}):\n            print(f\"Found SPR_BENCH at {c}\")\n            return c\n    raise FileNotFoundError(\n        \"SPR_BENCH not found; set SPR_DATA env var or place folder.\"\n    )\n\n\nroot = _find_spr_bench()\n\n\ndef load_spr_bench(path: pathlib.Path) -> DatasetDict:\n    def _load(name):\n        return load_dataset(\n            \"csv\", data_files=str(path / name), split=\"train\", cache_dir=\".cache_dsets\"\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nspr = load_spr_bench(root)\n\n\n# --------------------- vocabulary & dataset -----------------------------------\nclass SPRTokenDataset(Dataset):\n    def __init__(self, hf_ds, vocab):\n        self.hf = hf_ds\n        self.vocab = vocab\n        self.pad = vocab[\"<pad>\"]\n        self.cls = vocab[\"<cls>\"]\n\n    def __len__(self):\n        return len(self.hf)\n\n    def encode(self, s: str):\n        toks = s.strip().split()\n        return [self.cls] + [self.vocab[t] for t in toks]\n\n    def __getitem__(self, idx):\n        row = self.hf[idx]\n        ids = torch.tensor(self.encode(row[\"sequence\"]), dtype=torch.long)\n        label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n        parity = torch.tensor(\n            (len(ids) - 1) % 2, dtype=torch.long\n        )  # even/odd length (exclude CLS)\n        return {\"input_ids\": ids, \"labels\": label, \"parity\": parity}\n\n\ndef build_vocab(train_split):\n    vocab = {\"<pad>\": 0, \"<cls>\": 1}\n    for ex in train_split:\n        for tok in ex[\"sequence\"].split():\n            if tok not in vocab:\n                vocab[tok] = len(vocab)\n    return vocab\n\n\nvocab = build_vocab(spr[\"train\"])\nprint(\"Vocab size:\", len(vocab))\n\n\ndef collate(batch, pad_id):\n    seqs = [b[\"input_ids\"] for b in batch]\n    labels = torch.stack([b[\"labels\"] for b in batch])\n    parity = torch.stack([b[\"parity\"] for b in batch])\n    padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n    attn = (padded != pad_id).long()\n    return {\n        \"input_ids\": padded,\n        \"attention_mask\": attn,\n        \"labels\": labels,\n        \"parity\": parity,\n    }\n\n\ntrain_ds = SPRTokenDataset(spr[\"train\"], vocab)\ndev_ds = SPRTokenDataset(spr[\"dev\"], vocab)\ntest_ds = SPRTokenDataset(spr[\"test\"], vocab)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=128,\n    shuffle=True,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ndev_loader = DataLoader(\n    dev_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=256,\n    shuffle=False,\n    collate_fn=lambda b: collate(b, vocab[\"<pad>\"]),\n)\n\nnum_labels = len({int(ex[\"label\"]) for ex in spr[\"train\"]})\nmax_len = max(len(ex[\"sequence\"].split()) + 1 for ex in spr[\"train\"])\nprint(\"Max token length:\", max_len)\n\n\n# --------------------- model ---------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass SPRTransformer(nn.Module):\n    def __init__(\n        self, vocab_size, num_labels, d_model=256, nhead=8, nlayers=6, ff=512, drop=0.15\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model, nhead, ff, drop, batch_first=True\n        )\n        self.enc = nn.TransformerEncoder(enc_layer, nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.main_head = nn.Linear(d_model, num_labels)\n        self.parity_head = nn.Linear(d_model, 2)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embed(input_ids)\n        x = self.pos(x)\n        x = self.enc(x, src_key_padding_mask=(attention_mask == 0))\n        cls = self.norm(x[:, 0])\n        return self.main_head(cls), self.parity_head(cls)\n\n\nmodel = SPRTransformer(len(vocab), num_labels).to(device)\n\n# --------------------- optimiser & loss ---------------------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\ncriterion_main = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_parity = nn.CrossEntropyLoss()\naux_weight = 0.2\n\n\n# --------------------- metrics -------------------------------------------------\ndef macro_f1(preds, labels, num_cls):\n    preds, labels = preds.cpu().numpy(), labels.cpu().numpy()\n    f1s = []\n    for c in range(num_cls):\n        tp = ((preds == c) & (labels == c)).sum()\n        fp = ((preds == c) & (labels != c)).sum()\n        fn = ((preds != c) & (labels == c)).sum()\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        f1s.append(0 if prec + rec == 0 else 2 * prec * rec / (prec + rec))\n    return float(np.mean(f1s))\n\n\n# --------------------- train/val loops ----------------------------------------\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n    tot_loss = tot_correct = tot_cnt = 0\n    all_preds, all_lbls = [], []\n    for batch in loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        main_logits, parity_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        loss_main = criterion_main(main_logits, batch[\"labels\"])\n        loss_parity = criterion_parity(parity_logits, batch[\"parity\"])\n        loss = loss_main + aux_weight * loss_parity\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        with torch.no_grad():\n            preds = main_logits.argmax(1)\n        tot_loss += loss.item() * len(preds)\n        tot_correct += (preds == batch[\"labels\"]).sum().item()\n        tot_cnt += len(preds)\n        all_preds.append(preds)\n        all_lbls.append(batch[\"labels\"])\n    all_preds = torch.cat(all_preds)\n    all_lbls = torch.cat(all_lbls)\n    acc = tot_correct / tot_cnt\n    f1 = macro_f1(all_preds, all_lbls, num_labels)\n    return tot_loss / tot_cnt, acc, f1\n\n\n# --------------------- training ------------------------------------------------\nbest_f1 = 0\npatience = 2\nwait = 0\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    t_loss, t_acc, t_f1 = run_epoch(train_loader, True)\n    v_loss, v_acc, v_f1 = run_epoch(dev_loader, False)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(t_loss)\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(v_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_acc\"].append(t_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_acc\"].append(v_acc)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_f1\"].append(t_f1)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_f1\"].append(v_f1)\n    print(\n        f\"Epoch {epoch}: validation_loss = {v_loss:.4f} | val_acc = {v_acc*100:.2f}% | val_macroF1 = {v_f1:.4f}\"\n    )\n    if v_f1 > best_f1:\n        best_f1 = v_f1\n        wait = 0\n        torch.save(model.state_dict(), os.path.join(working_dir, \"best_model.pt\"))\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping.\")\n            break\n\n# --------------------- test evaluation ----------------------------------------\nmodel.load_state_dict(torch.load(os.path.join(working_dir, \"best_model.pt\")))\nmodel.eval()\nwith torch.no_grad():\n    preds_all, labels_all = [], []\n    for batch in test_loader:\n        batch = {\n            k: (v.to(device) if torch.is_tensor(v) else v) for k, v in batch.items()\n        }\n        logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n        preds_all.append(logits.argmax(1).cpu())\n        labels_all.append(batch[\"labels\"].cpu())\n    preds_all = torch.cat(preds_all)\n    labels_all = torch.cat(labels_all)\ntest_acc = (preds_all == labels_all).float().mean().item()\ntest_f1 = macro_f1(preds_all, labels_all, num_labels)\nprint(f\"Test accuracy: {test_acc*100:.2f}% | Test macroF1: {test_f1:.4f}\")\n\nexperiment_data[\"SPR_BENCH\"][\"predictions\"] = preds_all.numpy()\nexperiment_data[\"SPR_BENCH\"][\"ground_truth\"] = labels_all.numpy()\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data to working/experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 109114.43\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 86997.10\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 137324.56\nexamples/s]', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size: 9', '\\n', 'Max sequence length in training set: 64', '\\n',\n'\\n=== Training with dropout=0.0 ===', '\\n', 'Epoch 1: train_loss=0.6262,\nval_loss=0.5524, val_acc=76.80%', '\\n', 'Epoch 2: train_loss=0.5296,\nval_loss=0.5317, val_acc=78.60%', '\\n', 'Epoch 3: train_loss=0.5282,\nval_loss=0.5290, val_acc=78.60%', '\\n', 'Epoch 4: train_loss=0.5126,\nval_loss=0.5147, val_acc=79.60%', '\\n', 'Epoch 5: train_loss=0.5137,\nval_loss=0.5243, val_acc=78.60%', '\\n', 'Epoch 6: train_loss=0.5083,\nval_loss=0.5389, val_acc=78.60%', '\\n', 'Epoch 7: train_loss=0.5151,\nval_loss=0.5245, val_acc=79.20%', '\\n', 'Epoch 8: train_loss=0.5114,\nval_loss=0.5192, val_acc=79.20%', '\\n', 'Epoch 9: train_loss=0.5049,\nval_loss=0.5230, val_acc=79.00%', '\\n', 'Epoch 10: train_loss=0.5043,\nval_loss=0.5182, val_acc=79.60%', '\\n', 'Dropout 0.0: Test accuracy 79.50%',\n'\\n', '\\n=== Training with dropout=0.1 ===', '\\n', 'Epoch 1: train_loss=0.5835,\nval_loss=0.5594, val_acc=76.20%', '\\n', 'Epoch 2: train_loss=0.5351,\nval_loss=0.5448, val_acc=77.00%', '\\n', 'Epoch 3: train_loss=0.5203,\nval_loss=0.5490, val_acc=79.40%', '\\n', 'Epoch 4: train_loss=0.5195,\nval_loss=0.5258, val_acc=78.60%', '\\n', 'Epoch 5: train_loss=0.5193,\nval_loss=0.5268, val_acc=79.60%', '\\n', 'Epoch 6: train_loss=0.5221,\nval_loss=0.5269, val_acc=78.60%', '\\n', 'Epoch 7: train_loss=0.5212,\nval_loss=0.5645, val_acc=77.00%', '\\n', 'Epoch 8: train_loss=0.5182,\nval_loss=0.5258, val_acc=79.00%', '\\n', 'Epoch 9: train_loss=0.5121,\nval_loss=0.5283, val_acc=79.60%', '\\n', 'Epoch 10: train_loss=0.5106,\nval_loss=0.5193, val_acc=78.60%', '\\n', 'Dropout 0.1: Test accuracy 79.00%',\n'\\n', '\\n=== Training with dropout=0.2 ===', '\\n', 'Epoch 1: train_loss=0.5906,\nval_loss=0.5621, val_acc=76.20%', '\\n', 'Epoch 2: train_loss=0.5270,\nval_loss=0.5439, val_acc=78.60%', '\\n', 'Epoch 3: train_loss=0.5207,\nval_loss=0.5380, val_acc=79.60%', '\\n', 'Epoch 4: train_loss=0.5277,\nval_loss=0.5199, val_acc=79.40%', '\\n', 'Epoch 5: train_loss=0.5095,\nval_loss=0.5465, val_acc=78.60%', '\\n', 'Epoch 6: train_loss=0.5189,\nval_loss=0.5161, val_acc=79.60%', '\\n', 'Epoch 7: train_loss=0.5083,\nval_loss=0.5153, val_acc=79.40%', '\\n', 'Epoch 8: train_loss=0.5067,\nval_loss=0.5187, val_acc=79.60%', '\\n', 'Epoch 9: train_loss=0.5081,\nval_loss=0.5654, val_acc=76.40%', '\\n', 'Epoch 10: train_loss=0.5159,\nval_loss=0.5360, val_acc=78.60%', '\\n', 'Dropout 0.2: Test accuracy 78.80%',\n'\\n', '\\n=== Training with dropout=0.3 ===', '\\n', 'Epoch 1: train_loss=0.6157,\nval_loss=0.5461, val_acc=77.80%', '\\n', 'Epoch 2: train_loss=0.5279,\nval_loss=0.5773, val_acc=75.60%', '\\n', 'Epoch 3: train_loss=0.5225,\nval_loss=0.5373, val_acc=78.60%', '\\n', 'Epoch 4: train_loss=0.5149,\nval_loss=0.5228, val_acc=78.60%', '\\n', 'Epoch 5: train_loss=0.5100,\nval_loss=0.5152, val_acc=79.60%', '\\n', 'Epoch 6: train_loss=0.5126,\nval_loss=0.5130, val_acc=79.60%', '\\n', 'Epoch 7: train_loss=0.5141,\nval_loss=0.5453, val_acc=77.40%', '\\n', 'Epoch 8: train_loss=0.5134,\nval_loss=0.5236, val_acc=79.20%', '\\n', 'Epoch 9: train_loss=0.5110,\nval_loss=0.5123, val_acc=79.20%', '\\n', 'Epoch 10: train_loss=0.5086,\nval_loss=0.5204, val_acc=79.40%', '\\n', 'Dropout 0.3: Test accuracy 79.50%',\n'\\n', 'Experiment data saved to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-9/working/experiment_data.npy', '\\n', 'Execution time:\n15 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 108528.58\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 76999.27\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 141771.30\nexamples/s]', '\\n', 'Loaded SPR_BENCH splits:', ' ', \"['train', 'dev', 'test']\",\n'\\n', 'Vocab size: 10', '\\n', 'Max sequence length (with CLS): 65', '\\n', 'Epoch\n1: val_loss=0.5882, val_acc=74.80%, val_macroF1=0.7476', '\\n', 'Epoch 2:\nval_loss=0.5796, val_acc=78.60%, val_macroF1=0.7860', '\\n', 'Epoch 3:\nval_loss=0.5805, val_acc=79.00%, val_macroF1=0.7900', '\\n', 'Epoch 4:\nval_loss=0.5703, val_acc=78.60%, val_macroF1=0.7860', '\\n', 'Epoch 5:\nval_loss=0.5706, val_acc=78.40%, val_macroF1=0.7840', '\\n', 'Epoch 6:\nval_loss=0.5674, val_acc=78.60%, val_macroF1=0.7860', '\\n', 'Epoch 7:\nval_loss=0.5642, val_acc=78.60%, val_macroF1=0.7860', '\\n', 'Epoch 8:\nval_loss=0.5602, val_acc=78.60%, val_macroF1=0.7860', '\\n', '\\nBEST MODEL --\nTest accuracy: 78.90%, Test macro-F1: 0.7890', '\\n', 'Experiment data saved to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n11/working/experiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 107547.64\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 83628.50\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 136466.70\nexamples/s]', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.5831,\nMacro-F1 = 0.7959', '\\n', 'Epoch 2: validation_loss = 0.5457, Macro-F1 =\n0.7840', '\\n', 'Epoch 3: validation_loss = 0.5408, Macro-F1 = 0.7860', '\\n',\n'Epoch 4: validation_loss = 0.5252, Macro-F1 = 0.7959', '\\n', 'Epoch 5:\nvalidation_loss = 0.5380, Macro-F1 = 0.7959', '\\n', 'Test Macro-F1: 0.7950',\n'\\n', 'Experiment data saved.', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 38841.54\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 102036.30\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 120401.42\nexamples/s]', '\\n', 'Vocabulary size: 18', '\\n', 'Max token length:', ' ', '33',\n'\\n', 'Epoch 1: val_loss = 0.5772, val_acc = 78.00%, val_macroF1 = 0.7796',\n'\\n', 'Epoch 2: val_loss = 0.5745, val_acc = 74.80%, val_macroF1 = 0.7477',\n'\\n', 'Epoch 3: val_loss = 0.5429, val_acc = 78.80%, val_macroF1 = 0.7880',\n'\\n', 'Epoch 4: val_loss = 0.5464, val_acc = 77.20%, val_macroF1 = 0.7720',\n'\\n', 'Epoch 5: val_loss = 0.5508, val_acc = 77.60%, val_macroF1 = 0.7760',\n'\\n', 'Epoch 6: val_loss = 0.5255, val_acc = 78.80%, val_macroF1 = 0.7880',\n'\\n', 'Epoch 7: val_loss = 0.5214, val_acc = 78.80%, val_macroF1 = 0.7880',\n'\\n', 'Epoch 8: val_loss = 0.5203, val_acc = 78.80%, val_macroF1 = 0.7880',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.20%, Test\nmacroF1: 0.7920', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at: /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 57090.60\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 63523.11\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 163227.90\nexamples/s]', '\\n', 'Vocab size (token level): 18', '\\n', 'Max token length\n(with CLS): 33', '\\n', 'Number of classes: 2', '\\n', 'Epoch 1: val_loss = 0.5675\n| Macro-F1 = 77.20% | time 0.8s', '\\n', 'Epoch 2: val_loss = 0.5639 | Macro-F1 =\n78.80% | time 0.4s', '\\n', 'Epoch 3: val_loss = 0.5563 | Macro-F1 = 77.60% |\ntime 0.5s', '\\n', 'Epoch 4: val_loss = 0.5555 | Macro-F1 = 76.80% | time 0.5s',\n'\\n', 'Epoch 5: val_loss = 0.5434 | Macro-F1 = 78.80% | time 0.5s', '\\n', 'Epoch\n6: val_loss = 0.5710 | Macro-F1 = 75.99% | time 0.5s', '\\n', 'Early stopping\ntriggered.', '\\n', 'TEST Macro-F1: 79.10%', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Saved experiment data to\n/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-43-\n58_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n14/working/experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size 10, max_len 65, num_labels 2', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Epoch 1: validation_loss = 0.5714\n| Val Acc 78.80% | Val Macro-F1 0.7880', '\\n', 'Epoch 2: validation_loss =\n0.5740 | Val Acc 76.20% | Val Macro-F1 0.7619', '\\n', 'Epoch 3: validation_loss\n= 0.5642 | Val Acc 79.60% | Val Macro-F1 0.7959', '\\n', 'Epoch 4:\nvalidation_loss = 0.5698 | Val Acc 78.60% | Val Macro-F1 0.7860', '\\n', 'Epoch\n5: validation_loss = 0.5579 | Val Acc 78.60% | Val Macro-F1 0.7860', '\\n',\n'Epoch 6: validation_loss = 0.5590 | Val Acc 78.60% | Val Macro-F1 0.7860',\n'\\n', 'Epoch 7: validation_loss = 0.5553 | Val Acc 79.00% | Val Macro-F1\n0.7900', '\\n', 'Epoch 8: validation_loss = 0.5553 | Val Acc 79.60% | Val\nMacro-F1 0.7959', '\\n', 'TEST  ->  loss 0.5494 | Acc 79.50% | Macro-F1 0.7950',\n'\\n', 'Saved experiment data to /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-12/working/experiment_data.npy', '\\n', 'Execution time:\n7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocabulary size: 18', '\\n', 'Max token length:\n33', '\\n', 'Epoch 1: validation_loss = 0.6886, val_macroF1 = 0.4388', '\\n',\n'Epoch 2: validation_loss = 0.6288, val_macroF1 = 0.7859', '\\n', 'Epoch 3:\nvalidation_loss = 0.6025, val_macroF1 = 0.7880', '\\n', 'Epoch 4: validation_loss\n= 0.6267, val_macroF1 = 0.7456', '\\n', 'Epoch 5: validation_loss = 0.5970,\nval_macroF1 = 0.7959', '\\n', 'Epoch 6: validation_loss = 0.5957, val_macroF1 =\n0.7939', '\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.70%, Test\nmacroF1: 0.7970', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 5 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Max token\nlength:', ' ', '33', '\\n', 'Epoch 1: validation_loss = 0.6104 | val_acc = 76.00%\n| val_macroF1 = 0.7597', '\\n', 'Epoch 2: validation_loss = 0.5865 | val_acc =\n80.40% | val_macroF1 = 0.8038', '\\n', 'Epoch 3: validation_loss = 0.5700 |\nval_acc = 77.60% | val_macroF1 = 0.7760', '\\n', 'Epoch 4: validation_loss =\n0.5653 | val_acc = 78.60% | val_macroF1 = 0.7860', '\\n', 'Early stopping.',\n'\\n', '/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.00% | Test\nmacroF1: 0.7900', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Num classes:', '\n', '2', ' ', '| max len:', ' ', '33', '\\n', 'Epoch 1: validation_loss = 0.6364 |\nval_macroF1 = 0.7800', '\\n', 'Epoch 2: validation_loss = 0.5853 | val_macroF1 =\n0.7780', '\\n', 'Epoch 3: validation_loss = 0.5757 | val_macroF1 = 0.7780', '\\n',\n'Epoch 4: validation_loss = 0.5894 | val_macroF1 = 0.7919', '\\n', 'Epoch 5:\nvalidation_loss = 0.5800 | val_macroF1 = 0.7940', '\\n', 'Epoch 6:\nvalidation_loss = 0.5660 | val_macroF1 = 0.7880', '\\n', 'Epoch 7:\nvalidation_loss = 0.5642 | val_macroF1 = 0.7860', '\\n', 'Epoch 8:\nvalidation_loss = 0.5654 | val_macroF1 = 0.7940', '\\n', 'Epoch 9:\nvalidation_loss = 0.5613 | val_macroF1 = 0.7920', '\\n', 'Early stopping', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.40% | Test\nmacroF1: 0.7940', '\\n', 'Saved experiment data', '\\n', 'Execution time: 6\nseconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Max token\nlength:', ' ', '33', '\\n', 'Epoch 1: validation_loss = 0.5974 | val_acc = 77.20%\n| val_macroF1 = 0.7720', '\\n', 'Epoch 2: validation_loss = 0.5749 | val_acc =\n78.40% | val_macroF1 = 0.7839', '\\n', 'Epoch 3: validation_loss = 0.5610 |\nval_acc = 79.20% | val_macroF1 = 0.7919', '\\n', 'Epoch 4: validation_loss =\n0.5900 | val_acc = 76.60% | val_macroF1 = 0.7660', '\\n', 'Epoch 5:\nvalidation_loss = 0.5579 | val_acc = 79.60% | val_macroF1 = 0.7959', '\\n',\n'Epoch 6: validation_loss = 0.5579 | val_acc = 79.00% | val_macroF1 = 0.7900',\n'\\n', 'Epoch 7: validation_loss = 0.5934 | val_acc = 78.40% | val_macroF1 =\n0.7840', '\\n', 'Early stopping.', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.30% | Test\nmacroF1: 0.7930', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Max token\nlength:', ' ', '33', '\\n', 'Epoch 1: validation_loss = 0.6297 | val_acc = 67.40%\n| val_macroF1 = 0.6607', '\\n', 'Epoch 2: validation_loss = 0.5862 | val_acc =\n77.20% | val_macroF1 = 0.7720', '\\n', 'Epoch 3: validation_loss = 0.5624 |\nval_acc = 78.80% | val_macroF1 = 0.7880', '\\n', 'Epoch 4: validation_loss =\n0.5699 | val_acc = 78.60% | val_macroF1 = 0.7856', '\\n', 'Epoch 5:\nvalidation_loss = 0.5737 | val_acc = 78.20% | val_macroF1 = 0.7820', '\\n',\n'Early stopping.', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.00% | Test\nmacroF1: 0.7900', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 6 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Found SPR_BENCH at /home/zxl240011/AI-\nScientist-v2/SPR_BENCH', '\\n', 'Vocab size:', ' ', '18', '\\n', 'Max token\nlength:', ' ', '33', '\\n', 'Epoch 1: validation_loss = 0.6083 | val_acc = 72.60%\n| val_macroF1 = 0.7241', '\\n', 'Epoch 2: validation_loss = 0.6178 | val_acc =\n69.60% | val_macroF1 = 0.6924', '\\n', 'Epoch 3: validation_loss = 0.5983 |\nval_acc = 74.80% | val_macroF1 = 0.7476', '\\n', 'Epoch 4: validation_loss =\n0.5826 | val_acc = 78.40% | val_macroF1 = 0.7837', '\\n', 'Epoch 5:\nvalidation_loss = 0.5572 | val_acc = 79.00% | val_macroF1 = 0.7900', '\\n',\n'Epoch 6: validation_loss = 0.5516 | val_acc = 78.80% | val_macroF1 = 0.7879',\n'\\n', 'Epoch 7: validation_loss = 0.5744 | val_acc = 75.60% | val_macroF1 =\n0.7539', '\\n', 'Early stopping.', '\\n',\n'/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of\nnested tensors is in prototype stage and will change in the near future.\n(Triggered internally at /opt/conda/conda-\nbld/pytorch_1729647429097/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\\n\noutput = torch._nested_tensor_from_mask(\\n', 'Test accuracy: 79.20% | Test\nmacroF1: 0.7920', '\\n', 'Saved experiment data to working/experiment_data.npy',\n'\\n', 'Execution time: 7 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "", "The execution was successful, and the training script ran without errors. The\nmodel achieved a test accuracy of 79.20% and a test macro F1 score of 0.7920.\nWhile the performance is slightly below the SOTA benchmark of 80%, there are no\nbugs in the implementation. Further optimization or experimentation might be\nneeded to surpass the SOTA.", "", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "The loss of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5086, "best_value": 0.5086}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5123, "best_value": 0.5123}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5113, "best_value": 0.5113}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified instances.", "data": [{"dataset_name": "train", "final_value": 0.787, "best_value": 0.787}, {"dataset_name": "validation", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "test", "final_value": 0.789, "best_value": 0.789}]}, {"metric_name": "F1 score", "lower_is_better": false, "description": "Harmonic mean of precision and recall, used for imbalanced datasets.", "data": [{"dataset_name": "train", "final_value": 0.7868, "best_value": 0.7868}, {"dataset_name": "validation", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "test", "final_value": 0.789, "best_value": 0.789}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error of the model; lower values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.5653, "best_value": 0.5653}, {"dataset_name": "validation", "final_value": 0.5602, "best_value": 0.5602}, {"dataset_name": "test", "final_value": 0.5745, "best_value": 0.5745}]}]}, {"metric_names": [{"metric_name": "F1 score", "lower_is_better": false, "description": "Measures the harmonic mean of precision and recall.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.795, "best_value": 0.7959}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error; lower values indicate better performance.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5252, "best_value": 0.5194}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Proportion of correctly predicted samples out of the total samples.", "data": [{"dataset_name": "train", "final_value": 0.7915, "best_value": 0.7915}, {"dataset_name": "validation", "final_value": 0.788, "best_value": 0.788}, {"dataset_name": "test", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "F1 score", "lower_is_better": false, "description": "Harmonic mean of precision and recall, measuring the balance between them.", "data": [{"dataset_name": "train", "final_value": 0.79147701034039, "best_value": 0.79147701034039}, {"dataset_name": "validation", "final_value": 0.7879864311315924, "best_value": 0.7879864311315924}, {"dataset_name": "test", "final_value": 0.7919700436862909, "best_value": 0.7919700436862909}]}, {"metric_name": "loss", "lower_is_better": true, "description": "A measure of error, lower values indicate better performance.", "data": [{"dataset_name": "train", "final_value": 0.5103659129142761, "best_value": 0.5103659129142761}, {"dataset_name": "validation", "final_value": 0.5202790570259094, "best_value": 0.5202790570259094}]}]}, {"metric_names": [{"metric_name": "training F1 score", "lower_is_better": false, "description": "F1 score during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7788, "best_value": 0.7788}]}, {"metric_name": "validation F1 score", "lower_is_better": false, "description": "F1 score during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.788, "best_value": 0.788}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss during training phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 1.9251, "best_value": 1.9251}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss during validation phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.571, "best_value": 0.571}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score during test phase", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.791, "best_value": 0.791}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified samples.", "data": [{"dataset_name": "train", "final_value": 0.793, "best_value": 0.793}, {"dataset_name": "validation", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "test", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "F1 score", "lower_is_better": false, "description": "Harmonic mean of precision and recall.", "data": [{"dataset_name": "train", "final_value": 0.793, "best_value": 0.793}, {"dataset_name": "validation", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "test", "final_value": 0.795, "best_value": 0.795}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or deviation from the true values.", "data": [{"dataset_name": "train", "final_value": 0.5513, "best_value": 0.5513}, {"dataset_name": "validation", "final_value": 0.5553, "best_value": 0.5553}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified instances.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.797, "best_value": 0.797}]}, {"metric_name": "F1 score", "lower_is_better": false, "description": "Harmonic mean of precision and recall, used for imbalanced datasets.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.797, "best_value": 0.797}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Measures the error or difference between predicted and true values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.595679, "best_value": 0.593557}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "The best accuracy achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7865, "best_value": 0.7865}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The best accuracy achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.804, "best_value": 0.804}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The best macro F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7864, "best_value": 0.7864}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The best macro F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.8038, "best_value": 0.8038}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The lowest loss achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.559914, "best_value": 0.559914}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The lowest loss achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.565252, "best_value": 0.565252}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79, "best_value": 0.79}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score achieved on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79, "best_value": 0.79}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "The loss value calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.553591, "best_value": 0.553591}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.561289, "best_value": 0.561289}]}, {"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.791443, "best_value": 0.791443}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79396, "best_value": 0.79396}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy calculated on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.793979, "best_value": 0.793979}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "The accuracy of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7935, "best_value": 0.7935}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "The accuracy of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7935, "best_value": 0.7935}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value of the model on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.548907, "best_value": 0.548907}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the model on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.557889, "best_value": 0.557889}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "The accuracy of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.793, "best_value": 0.793}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score of the model on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.793, "best_value": 0.793}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "The proportion of correct predictions out of all predictions.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79, "best_value": 0.79}]}, {"metric_name": "macro F1 score", "lower_is_better": false, "description": "The harmonic mean of precision and recall, averaged across classes.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79, "best_value": 0.79}]}, {"metric_name": "loss", "lower_is_better": true, "description": "The measure of error or difference between predicted and actual values.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.562414, "best_value": 0.550702}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified instances during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified instances during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79, "best_value": 0.79}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.79, "best_value": 0.79}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Measures the error during training. Lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.548828, "best_value": 0.548828}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Measures the error during validation. Lower is better.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.551552, "best_value": 0.551552}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Measures the proportion of correctly classified instances on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "Macro-averaged F1 score for the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"], ["../../logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_f1_curves.png", "../../logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_per_class_f1.png"], ["../../logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_loss_curves.png", "../../logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_f1_curves.png", "../../logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_acc_f1_curves.png", "../../logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_macroF1_curves.png", "../../logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_f1_curves.png", "../../logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_confusion_matrix.png"], ["../../logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_mean_loss_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_mean_accuracy_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_mean_f1_curves.png", "../../logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_aggregated_confusion_matrix.png"]], "plot_paths": [["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_test_accuracy_bar.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_confusion_matrix.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_per_class_f1.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_acc_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_macroF1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_confusion_matrix.png"], ["experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_mean_loss_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_mean_accuracy_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_mean_f1_curves.png", "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_74d1876b72cf459c80e5f8b1785eb9c8/spr_bench_aggregated_confusion_matrix.png"]], "plot_analyses": [[{"analysis": "The first plot shows the training and validation accuracy across epochs for different dropout rates. All dropout rates converge to around 80% accuracy, with minimal variance across different dropout settings. The training accuracy is slightly higher than the validation accuracy, indicating a small degree of overfitting, particularly for lower dropout rates. The dropout rate of 0.0 (no dropout) achieves the highest and most consistent accuracy, suggesting that dropout may not significantly benefit this task.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_accuracy_curves.png"}, {"analysis": "The second plot displays the training and validation loss over epochs for various dropout rates. The loss decreases steadily across epochs, with minimal differences between dropout rates. The training loss is consistently lower than the validation loss, further confirming slight overfitting. The dropout rate of 0.0 demonstrates the lowest loss, aligning with the accuracy findings. Higher dropout rates introduce more fluctuation in validation loss, potentially indicating instability in learning.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_loss_curves.png"}, {"analysis": "The third plot summarizes the test accuracy for different dropout rates. All dropout rates achieve nearly identical test accuracy, around 80%, with negligible differences. This result suggests that dropout has minimal impact on the model's generalization performance for this task.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_test_accuracy_bar.png"}, {"analysis": "The fourth plot is a confusion matrix for the best-performing dropout rate (0.0). The model shows balanced performance across both classes, with comparable true positives and true negatives. However, there is a small number of misclassifications in both directions (false positives and false negatives). The confusion matrix indicates robust overall performance, with room for improvement in reducing misclassification rates.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_8cfd6dac2fc94b1b94009a67887899f8_proc_3159064/SPR_BENCH_confusion_matrix_best_dropout_0.0.png"}], [{"analysis": "The loss curves indicate a consistent decrease in both training and validation loss over epochs, suggesting that the model is learning effectively without overfitting. The gap between training and validation loss is minimal, which is a positive sign of generalization.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_loss_curves.png"}, {"analysis": "The accuracy curves show a rapid increase in both training and validation accuracy during the initial epochs, followed by stabilization. The validation accuracy remains slightly higher than the training accuracy, indicating good generalization and no signs of overfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_accuracy_curves.png"}, {"analysis": "The macro-F1 score curves align with the accuracy trends, showing rapid improvement in the initial epochs and stabilization thereafter. The high and stable F1 scores for both training and validation sets highlight the model's balanced performance across classes.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_f1_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs well on both classes, with a high number of correct predictions for each. The balance in the confusion matrix suggests that the model is not biased towards any particular class.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_292d3f9b3f8d4a729f416eb1f39b01af_proc_3165648/contextual_big_transformer_SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The cross-entropy loss curves for both training and validation sets show a steady decrease over the epochs, indicating that the model is learning effectively. The convergence of both curves at lower loss values suggests that the model is not overfitting and is generalizing well to the validation set. However, the slight increase in validation loss after epoch 4 might indicate the beginning of overfitting, warranting regularization or early stopping.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_loss_curves.png"}, {"analysis": "The macro-F1 score curves for training and validation demonstrate a rapid improvement in the initial epochs, with both curves stabilizing and closely aligning from epoch 2 onwards. This alignment reflects effective learning and generalization, as the model performs consistently across both sets. The peak macro-F1 score on the validation set suggests that the model is competitive with the SOTA performance.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix on the test set reveals a strong performance, with most predictions correctly classified into their respective labels. The balance between true positives and true negatives indicates that the model is not biased towards any particular class. This reinforces the model's robustness and ability to handle the symbolic reasoning tasks effectively.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_9cd49c3109e84c64b8704e694550f576_proc_3165649/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curves indicate a steady and consistent decrease in cross-entropy loss for both the training and validation sets over the epochs. The validation loss closely follows the training loss, suggesting that the model is not overfitting and is generalizing well to unseen data. However, the slight flattening of the curve towards the end suggests that further training may yield diminishing returns.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves for both training and validation datasets show a rapid improvement during the initial epochs, followed by a plateau. The training and validation accuracies are closely aligned, which is a strong indicator of good generalization. The final accuracy appears to be nearing the SOTA benchmark of 80.0%, indicating promising results.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_accuracy_curves.png"}, {"analysis": "The macro-F1 curves mirror the trends observed in the accuracy curves, with rapid improvements in the initial epochs and stabilization thereafter. The alignment of the training and validation macro-F1 scores further supports the conclusion that the model generalizes well and maintains balanced performance across classes.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix for the test set shows a strong diagonal dominance, indicating that the model is correctly classifying a majority of the examples in both classes. However, there are some misclassifications, which could be attributed to edge cases or inherent ambiguities in the dataset.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_confusion_matrix.png"}, {"analysis": "The per-class F1 scores are nearly identical, suggesting that the model performs equally well across both classes. This balanced performance is crucial for tasks with imbalanced or equally important classes and indicates that the model is not biased towards any particular class.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_608cbaa9ee3d4a99b30475063aa2017a_proc_3165650/spr_bench_per_class_f1.png"}], [{"analysis": "The loss curves indicate that the training loss decreases steadily over the epochs, which suggests that the model is learning effectively. However, the validation loss remains relatively flat after an initial decrease, with a slight increase towards the end. This could imply potential overfitting or that the model is struggling to generalize to the validation set. Further regularization techniques or hyperparameter tuning might help mitigate this issue.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_loss_curves.png"}, {"analysis": "The Macro-F1 curves show that both training and validation performance improve over the epochs, with the validation F1 starting higher than the training F1. The training F1 steadily increases, while the validation F1 fluctuates slightly, peaking around epoch 5 before declining. This suggests that the model achieves good performance but may require further fine-tuning to stabilize validation performance and maximize generalization.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_f1_curves.png"}, {"analysis": "The confusion matrix shows that the model performs well in distinguishing between the two classes, with a high number of correct predictions for both classes. However, there is still room for improvement as there are some misclassifications. Analyzing the misclassified samples and identifying patterns could provide insights for targeted improvements in the model.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_641429e3b8664c1dac5c52b7c04b0a6f_proc_3165651/spr_bench_token_level_transformer_confusion_matrix.png"}], [{"analysis": "The loss curves show a steady decline in both the training and validation loss over the epochs, indicating that the model is learning effectively. The validation loss closely follows the training loss, suggesting that the model generalizes well without significant overfitting. The relatively minor fluctuations in the validation loss might be due to noise in the data or small variations in gradient updates.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_loss_curves.png"}, {"analysis": "The accuracy and macro-F1 curves demonstrate consistent improvement over the epochs. The validation accuracy and F1 scores are closely aligned with the training scores, further supporting the model's generalization ability. By the final epochs, the scores stabilize, indicating convergence. The model achieves a strong performance, nearing or potentially surpassing the SOTA benchmark of 80% accuracy, as hypothesized.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_acc_f1_curves.png"}, {"analysis": "The confusion matrix for the test set indicates a balanced performance across the two classes. The diagonal dominance suggests that the model is correctly classifying most samples, with relatively few misclassifications. This result supports the model's effectiveness in handling the SPR_BENCH classification task.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_fc9d6882546a4c18ae0eeb5a664d6a76_proc_3165649/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curves indicate that the model is learning effectively. Both training and validation loss decrease rapidly in the first few epochs and plateau towards the later epochs. There is no significant overfitting as the validation loss closely follows the training loss. However, the slight fluctuation in validation loss around epochs 4 and 5 might suggest a need for further hyperparameter tuning or regularization to stabilize performance.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show a consistent improvement in both training and validation accuracy over the epochs, reaching approximately 80%. The validation accuracy surpasses the training accuracy in the early epochs, which may indicate that the model generalizes well initially but could also suggest that the training data might be slightly less challenging than the validation data. The final accuracy suggests that the model is competitive with the SOTA benchmark (80%).", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_accuracy_curves.png"}, {"analysis": "The macro-F1 curves demonstrate a similar trend to the accuracy curves, indicating that the model performs well across all classes. Both training and validation macro-F1 scores converge towards 80%, which aligns with the accuracy results and suggests balanced performance across different classes. Slight fluctuations in the validation macro-F1 around epochs 4 and 5 mirror those seen in the accuracy and loss curves, reinforcing the need for further stabilization.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix for the test set reveals a strong performance, with high true positive and true negative counts. Misclassifications are relatively low, indicating that the model handles both classes effectively. This supports the claim that the model is competitive with the SOTA benchmark and suggests that the contextual embedding-based approach is effective for SPR tasks.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_36c9f19623fd45a9a6b15038bead5645_proc_3165648/spr_bench_confusion_matrix.png"}], [{"analysis": "This plot shows the cross-entropy loss for both the training and validation datasets over four epochs. The training loss decreases sharply in the first two epochs and stabilizes afterward, indicating that the model is learning effectively during this period. The validation loss follows a similar trend, decreasing steadily and showing no signs of overfitting, as the validation loss does not increase relative to the training loss. This suggests good generalization performance of the model.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_loss_curves.png"}, {"analysis": "This plot illustrates the accuracy for both the training and validation datasets over four epochs. The training accuracy increases rapidly in the first two epochs and stabilizes afterward, indicating effective learning. The validation accuracy exhibits a similar trend, with a slight plateau after the second epoch. The convergence of training and validation accuracies toward the end suggests that the model is well-tuned and does not suffer from overfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_accuracy_curves.png"}, {"analysis": "This plot represents the macro-F1 score for both the training and validation datasets over four epochs. The macro-F1 score, which considers both precision and recall, shows a rapid increase in the first two epochs for both datasets, followed by a stabilization phase. The convergence of the training and validation macro-F1 scores indicates that the model performs consistently across different classes and does not exhibit overfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix for the test set indicates the distribution of true positive, true negative, false positive, and false negative predictions. The diagonal dominance in the matrix suggests that the model performs well in correctly classifying both classes. However, there is still room for improvement in reducing the misclassification rates for each class. The model seems to be fairly balanced in its predictions, as there is no significant bias toward any particular class.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e1175791daf84894abd198fece9b2a3b_proc_3165651/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curves indicate a steady decline in both training and validation loss over the epochs, suggesting effective learning. The gap between the two curves is minimal, which is a sign that the model generalizes well without overfitting. However, the validation loss plateauing towards the end implies that further improvements in performance might require model or hyperparameter adjustments.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_loss_curves.png"}, {"analysis": "The Macro-F1 scores demonstrate a rapid improvement during the initial epochs, with both training and validation scores converging at around 0.8 by the end of training. This indicates that the model achieves a good balance in performance across all classes. The close alignment of the training and validation curves suggests that the model is not overfitting and maintains consistent performance on unseen data.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_macroF1_curves.png"}, {"analysis": "The confusion matrix for the test set reveals a balanced performance, with a high number of correct predictions for both classes. The diagonal dominance in the matrix confirms that the model is effective at distinguishing between the classes, with minimal misclassifications. This supports the conclusion that the model has successfully learned the underlying patterns in the dataset.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_7555392f22a040719bbe02b4daceb1bc_proc_3165650/spr_bench_confusion_matrix.png"}], [{"analysis": "The loss curves indicate a steady decrease in training loss, which is a sign of the model learning effectively. However, the validation loss shows fluctuations after epoch 4, which could be a sign of overfitting or instability in the model's generalization capability. The validation loss is consistently lower than the training loss, which could indicate that the model is regularized or that the validation set is less challenging than the training set.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show that the model achieves a rapid increase in training accuracy within the first two epochs, followed by a plateau. Validation accuracy follows a similar trend, but it fluctuates slightly after epoch 4, which aligns with the loss curve observations. The model appears to be performing well, achieving nearly 80% accuracy on both training and validation data, which is close to the stated SOTA benchmark.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_accuracy_curves.png"}, {"analysis": "The macro-F1 curves closely resemble the accuracy trends. This indicates that the model performs well across classes, with balanced precision and recall. The slight fluctuations in validation macro-F1 after epoch 4 suggest some instability, but the overall performance is robust and near the SOTA benchmark.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix shows that the model performs well on both classes, with high true positive and true negative counts. There is a relatively small number of misclassifications, indicating that the model has learned to distinguish between the two classes effectively. However, further analysis would be needed to identify whether these errors are systematic or random.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/spr_bench_confusion_matrix.png"}], [{"analysis": "The cross-entropy loss curves indicate that the model is effectively learning from the training data. The training loss decreases steadily over the epochs, while the validation loss shows a similar trend, stabilizing around epoch 3. However, the slight increase in validation loss after epoch 4 suggests potential overfitting or that the model has reached its optimal performance.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_loss_curves.png"}, {"analysis": "The accuracy curves show a significant improvement in both training and validation accuracy over the initial epochs. Both curves converge and stabilize around 80% accuracy by epoch 3, indicating that the model is performing well and achieving the benchmark SOTA performance of 80%.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_accuracy_curves.png"}, {"analysis": "The macro-F1 score curves follow a similar trend to the accuracy curves, with both training and validation scores improving rapidly and stabilizing around epoch 3. This indicates that the model maintains a balanced performance across classes and aligns well with the accuracy trends.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix reveals that the model performs well on both classes, with a high number of correct predictions for both labels. However, there is still a noticeable number of misclassifications, which could be addressed through further model refinement or hyperparameter tuning.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/spr_bench_confusion_matrix.png"}], [{"analysis": "This plot shows the cross-entropy loss for both the training and validation sets over 7 epochs. The training loss decreases steadily, indicating that the model is learning effectively. The validation loss initially decreases but starts to increase slightly after epoch 5, suggesting potential overfitting. The model may benefit from regularization techniques such as dropout or early stopping to prevent performance degradation on unseen data.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_loss_curves.png"}, {"analysis": "This plot illustrates the accuracy trends for both training and validation sets across 7 epochs. The training accuracy improves consistently, reaching a plateau around epoch 5. Validation accuracy follows a similar trend, peaking at epoch 5 and then slightly declining. This behavior further supports the observation of potential overfitting after epoch 5.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_accuracy_curves.png"}, {"analysis": "The macro-F1 score trends for training and validation sets are displayed here. Both scores improve over time, with the validation macro-F1 score peaking at epoch 5 and declining slightly afterward. This metric corroborates the accuracy and loss trends, indicating that the model's performance on balanced class predictions is optimal around epoch 5 but deteriorates slightly due to overfitting.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_f1_curves.png"}, {"analysis": "The confusion matrix for the test set reveals the model's classification performance. The majority of predictions align with the ground truth, as indicated by the high values along the diagonal. However, there are some misclassifications, particularly in one of the classes. This suggests that while the model performs well overall, it might struggle with certain patterns or rules within the SPR_BENCH dataset. Analyzing these misclassified instances could provide insights for further improvements.", "plot_path": "experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/spr_bench_confusion_matrix.png"}], []], "vlm_feedback_summary": ["The plots indicate that dropout has a minimal impact on the model's performance,\nwith the best results achieved at a dropout rate of 0.0. The model achieves\nconsistent accuracy and loss trends across training and validation, with slight\noverfitting observed. The confusion matrix highlights balanced performance\nacross classes but reveals some misclassification errors. Overall, the results\nare promising but suggest the need for further fine-tuning to enhance\ngeneralization and reduce errors.", "The model demonstrates strong learning capabilities, with consistent improvement\nin loss, accuracy, and F1 scores across training and validation sets. The\nconfusion matrix confirms balanced performance across classes, indicating\neffective handling of symbolic reasoning tasks.", "The plots demonstrate effective learning and generalization of the model, with\ncompetitive performance against the SOTA benchmark. The loss and macro-F1 score\ncurves indicate consistent improvement and alignment between training and\nvalidation sets, while the confusion matrix highlights robustness in handling\nsymbolic reasoning tasks.", "The experimental results indicate strong performance with good generalization,\nbalanced class performance, and alignment with the SOTA benchmark. The model's\nability to avoid overfitting and maintain consistent improvements across metrics\nis particularly noteworthy.", "The provided plots illustrate the model's learning process, performance metrics,\nand classification outcomes. While the model shows promise with steady\nimprovements in training loss and F1 scores, the validation metrics highlight\npotential overfitting and instability. The confusion matrix confirms good\noverall classification but suggests areas for refinement in handling challenging\nsamples.", "The experimental results indicate that the proposed model performs effectively\non the SPR_BENCH task. The loss curves suggest good learning and generalization,\nwhile the accuracy and F1 scores highlight competitive performance that may\nsurpass the SOTA. The confusion matrix confirms balanced and accurate\npredictions on the test set, validating the model's capability in symbolic\nreasoning tasks.", "The experimental results indicate that the model is learning effectively and\nachieving performance comparable to the SOTA benchmark. The loss, accuracy, and\nmacro-F1 curves show consistent improvement and convergence, while the confusion\nmatrix demonstrates balanced and accurate classification. Minor fluctuations in\nvalidation metrics suggest potential areas for refinement, such as\nhyperparameter tuning or regularization.", "The plots indicate a well-performing model with no signs of overfitting. The\nloss, accuracy, and macro-F1 curves show consistent improvements and convergence\nbetween training and validation datasets. The confusion matrix suggests that the\nmodel is effective but has some room for improvement in reducing\nmisclassifications.", "The plots collectively show that the model achieves strong performance, with\neffective learning and generalization. The loss curves and Macro-F1 scores\nindicate steady improvement and convergence, while the confusion matrix confirms\nbalanced and accurate predictions across classes. The results suggest that the\nproposed approach of using contextual embeddings for SPR tasks is promising and\ncompetitive with the SOTA.", "The plots demonstrate that the model is learning effectively and achieving\nperformance near the SOTA benchmark. While the loss, accuracy, and macro-F1\ncurves suggest robust performance, the fluctuations in validation metrics after\nepoch 4 indicate potential overfitting or instability. The confusion matrix\nconfirms strong classification performance with minimal misclassifications.\nOverall, the results are promising but warrant further investigation into the\nobserved fluctuations and potential overfitting.", "The model demonstrates strong learning and generalization capabilities,\nachieving the benchmark SOTA performance of 80% accuracy. The loss, accuracy,\nand macro-F1 score curves suggest that the model stabilizes its performance\naround epoch 3. The confusion matrix indicates good classification performance,\nwith room for improvement in reducing misclassifications.", "The experimental results reveal that the model learns effectively during the\ninitial epochs, with optimal performance achieved around epoch 5. However, signs\nof overfitting emerge in later epochs, as seen in the validation loss, accuracy,\nand macro-F1 score trends. The confusion matrix highlights overall good\nclassification performance but also points to specific areas requiring further\nanalysis to address misclassifications. Regularization techniques and a deeper\ninvestigation into challenging instances could enhance the model's robustness\nand generalization capabilities.", "[]"], "exec_time": [15.551154375076294, 7.602583169937134, 4.542991638183594, 5.945902585983276, 5.984825849533081, 7.083611488342285, 5.536861181259155, 4.798099994659424, 6.037048578262329, 7.353945016860962, 6.452288866043091, 7.097304105758667, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['SPR_BENCH']"], ["\"\""], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[]"], ["['SPR_BENCH']"], ["['SPR_BENCH']"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], ["['SPR_BENCH']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory for plots\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\n# extract SPR_BENCH results\nbench = experiment_data.get(\"dropout_tuning\", {}).get(\"SPR_BENCH\", {})\nif not bench:\n    print(\"No SPR_BENCH data found.\")\n    exit()\n\ndropouts = sorted(bench.keys(), key=float)\nepochs = len(bench[dropouts[0]][\"metrics\"][\"train_acc\"])\n\n# gather stats\ntest_accs = {dp: bench[dp][\"test_acc\"] for dp in dropouts}\nbest_dp = max(test_accs, key=test_accs.get)\nbest_acc = test_accs[best_dp]\n\nprint(\"=== Test Accuracies ===\")\nfor dp, acc in test_accs.items():\n    print(f\"Dropout {dp}: {acc*100:.2f}%\")\nprint(f\"\\nBest dropout: {best_dp} (test acc {best_acc*100:.2f}%)\")\n\n# --------------------------------------------------------------------- #\n# 1) accuracy curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"metrics\"][\"train_acc\"]\n        val = bench[dp][\"metrics\"][\"val_acc\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"SPR_BENCH: Training and Validation Accuracy vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_accuracy_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 2) loss curves\ntry:\n    plt.figure()\n    for dp in dropouts:\n        tr = bench[dp][\"losses\"][\"train_loss\"]\n        val = bench[dp][\"losses\"][\"val_loss\"]\n        plt.plot(range(1, epochs + 1), tr, label=f\"dropout {dp} train\", linestyle=\"-\")\n        plt.plot(range(1, epochs + 1), val, label=f\"dropout {dp} val\", linestyle=\"--\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"SPR_BENCH: Training and Validation Loss vs Epoch\\n(Dropout tuning)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 3) bar chart of test accuracies\ntry:\n    plt.figure()\n    x = np.arange(len(dropouts))\n    y = [test_accs[dp] * 100 for dp in dropouts]\n    plt.bar(x, y, color=\"skyblue\")\n    plt.xticks(x, dropouts)\n    plt.ylabel(\"Test Accuracy (%)\")\n    plt.title(\"SPR_BENCH: Test Accuracy by Dropout Rate\")\n    fname = os.path.join(working_dir, \"SPR_BENCH_test_accuracy_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n\n# --------------------------------------------------------------------- #\n# 4) confusion matrix for best dropout\ntry:\n    preds = np.array(bench[best_dp][\"predictions\"])\n    gts = np.array(bench[best_dp][\"ground_truth\"])\n    labels = np.unique(gts)\n    cm = np.zeros((len(labels), len(labels)), dtype=int)\n    for t, p in zip(gts, preds):\n        cm[t, p] += 1\n\n    plt.figure()\n    im = plt.imshow(cm, cmap=\"Blues\")\n    plt.colorbar(im)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"SPR_BENCH: Confusion Matrix (Best dropout={best_dp})\")\n    plt.xticks(labels)\n    plt.yticks(labels)\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            plt.text(\n                j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n            )\n    fname = os.path.join(\n        working_dir, f\"SPR_BENCH_confusion_matrix_best_dropout_{best_dp}.png\"\n    )\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# iterate through all stored model/dataset results\nfor model_name, datasets in experiment_data.items():\n    for dset_name, d in datasets.items():\n        # ------------------------- curves ----------------------------- #\n        metrics, losses = d.get(\"metrics\", {}), d.get(\"losses\", {})\n        train_loss, val_loss = map(\n            np.array, (losses.get(\"train\", []), losses.get(\"val\", []))\n        )\n        train_acc, val_acc = map(\n            np.array, (metrics.get(\"train_acc\", []), metrics.get(\"val_acc\", []))\n        )\n        train_f1, val_f1 = map(\n            np.array, (metrics.get(\"train_f1\", []), metrics.get(\"val_f1\", []))\n        )\n        epochs = np.arange(1, max(len(train_loss), len(train_acc), len(train_f1)) + 1)\n\n        # 1. loss curves\n        if train_loss.size and val_loss.size:\n            try:\n                plt.figure()\n                plt.plot(epochs[: len(train_loss)], train_loss, label=\"Train Loss\")\n                plt.plot(epochs[: len(val_loss)], val_loss, label=\"Val Loss\")\n                plt.title(f\"{dset_name} Loss Curves (Model: {model_name})\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.legend()\n                plt.savefig(\n                    os.path.join(\n                        working_dir, f\"{model_name}_{dset_name}_loss_curves.png\"\n                    )\n                )\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating loss curve for {dset_name}: {e}\")\n                plt.close()\n\n        # 2. accuracy curves\n        if train_acc.size and val_acc.size:\n            try:\n                plt.figure()\n                plt.plot(epochs[: len(train_acc)], train_acc, label=\"Train Acc\")\n                plt.plot(epochs[: len(val_acc)], val_acc, label=\"Val Acc\")\n                plt.title(f\"{dset_name} Accuracy Curves (Model: {model_name})\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Accuracy\")\n                plt.legend()\n                plt.savefig(\n                    os.path.join(\n                        working_dir, f\"{model_name}_{dset_name}_accuracy_curves.png\"\n                    )\n                )\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating accuracy curve for {dset_name}: {e}\")\n                plt.close()\n\n        # 3. macro-F1 curves\n        if train_f1.size and val_f1.size:\n            try:\n                plt.figure()\n                plt.plot(epochs[: len(train_f1)], train_f1, label=\"Train F1\")\n                plt.plot(epochs[: len(val_f1)], val_f1, label=\"Val F1\")\n                plt.title(f\"{dset_name} Macro-F1 Curves (Model: {model_name})\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Macro-F1\")\n                plt.legend()\n                plt.savefig(\n                    os.path.join(working_dir, f\"{model_name}_{dset_name}_f1_curves.png\")\n                )\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating F1 curve for {dset_name}: {e}\")\n                plt.close()\n\n        # 4. confusion matrix (test set)\n        preds, gts = map(\n            np.asarray, (d.get(\"predictions\", []), d.get(\"ground_truth\", []))\n        )\n        if preds.size and gts.size:\n            try:\n                n_cls = int(max(preds.max(), gts.max()) + 1)\n                cm = np.zeros((n_cls, n_cls), dtype=int)\n                for gt, pr in zip(gts, preds):\n                    cm[gt, pr] += 1\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im)\n                plt.title(f\"{dset_name} Confusion Matrix (Model: {model_name})\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                ticks = np.arange(n_cls)\n                plt.xticks(ticks)\n                plt.yticks(ticks)\n                plt.savefig(\n                    os.path.join(\n                        working_dir, f\"{model_name}_{dset_name}_confusion_matrix.png\"\n                    )\n                )\n                plt.close()\n                # print evaluation metrics for quick reference\n                test_acc = (preds == gts).mean()\n                test_f1 = d.get(\"test_f1\", None)\n                print(\n                    f\"{model_name}/{dset_name} -- Test Acc: {test_acc*100:.2f}% | Test Macro-F1: {test_f1:.4f}\"\n                    if test_f1 is not None\n                    else f\"{model_name}/{dset_name} -- Test Acc: {test_acc*100:.2f}%\"\n                )\n            except Exception as e:\n                print(f\"Error creating confusion matrix for {dset_name}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\nif \"SPR_BENCH\" in experiment_data:\n    data = experiment_data[\"SPR_BENCH\"]\n    # safely get arrays\n    train_loss = np.array(data[\"losses\"].get(\"train\", []))\n    val_loss = np.array(data[\"losses\"].get(\"val\", []))\n    train_f1 = np.array(data[\"metrics\"].get(\"train\", []))\n    val_f1 = np.array(data[\"metrics\"].get(\"val\", []))\n    preds = np.array(data.get(\"predictions\", []))\n    gts = np.array(data.get(\"ground_truth\", []))\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # 1. Loss curves\n    try:\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # 2. Macro-F1 curves\n    try:\n        if train_f1.size and val_f1.size:\n            plt.figure()\n            plt.plot(epochs, train_f1, label=\"Train\")\n            plt.plot(epochs, val_f1, label=\"Validation\")\n            plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, \"spr_bench_f1_curves.png\"))\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # 3. Confusion matrix\n    try:\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for gt, pr in zip(gts, preds):\n                cm[gt, pr] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n            plt.xlabel(\"Predicted Label\")\n            plt.ylabel(\"True Label\")\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks)\n            plt.yticks(ticks)\n            plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n            plt.close()\n            print(f\"Test accuracy: {(preds == gts).mean()*100:.2f}%\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\nelse:\n    print(\"SPR_BENCH results not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ----------------------------- setup & load --------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Proceed only if SPR_BENCH exists\nif \"SPR_BENCH\" in experiment_data:\n    d = experiment_data[\"SPR_BENCH\"]\n    train_loss = np.asarray(d[\"losses\"][\"train\"])\n    val_loss = np.asarray(d[\"losses\"][\"val\"])\n    train_acc = np.asarray(d[\"metrics\"][\"train_acc\"])\n    val_acc = np.asarray(d[\"metrics\"][\"val_acc\"])\n    train_f1 = np.asarray(d[\"metrics\"][\"train_f1\"])\n    val_f1 = np.asarray(d[\"metrics\"][\"val_f1\"])\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # --------------------------- 1. Loss curves ----------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train\")\n        plt.plot(epochs, val_loss, label=\"Validation\")\n        plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    # ------------------------- 2. Accuracy curves --------------------------- #\n    try:\n        plt.figure()\n        plt.plot(epochs, train_acc, label=\"Train\")\n        plt.plot(epochs, val_acc, label=\"Validation\")\n        plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curves: {e}\")\n        plt.close()\n\n    # --------------------------- 3. F1 curves ------------------------------ #\n    try:\n        plt.figure()\n        plt.plot(epochs, train_f1, label=\"Train\")\n        plt.plot(epochs, val_f1, label=\"Validation\")\n        plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curves: {e}\")\n        plt.close()\n\n    # ------------------- 4. Confusion matrix (test set) --------------------- #\n    try:\n        preds = np.asarray(d[\"predictions\"])\n        gts = np.asarray(d[\"ground_truth\"])\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Ground Truth\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ---------------------- 5. Per-class F1 bar plot ------------------------ #\n    try:\n        per_class_f1 = []\n        for c in range(num_classes):\n            tp = np.sum((preds == c) & (gts == c))\n            fp = np.sum((preds == c) & (gts != c))\n            fn = np.sum((preds != c) & (gts == c))\n            prec = tp / (tp + fp) if tp + fp else 0\n            rec = tp / (tp + fn) if tp + fn else 0\n            f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n            per_class_f1.append(f1)\n\n        plt.figure()\n        plt.bar(np.arange(num_classes), per_class_f1)\n        plt.title(\"SPR_BENCH Per-Class F1 (Test Set)\")\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"F1 Score\")\n        plt.savefig(os.path.join(working_dir, \"spr_bench_per_class_f1.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating per-class F1 plot: {e}\")\n        plt.close()\n\n    # --------------------------- Print metrics ----------------------------- #\n    test_acc = (preds == gts).mean()\n    macro_f1 = per_class_f1 and float(np.mean(per_class_f1))\n    print(f\"Test accuracy: {test_acc*100:.2f}%  |  Test macro-F1: {macro_f1:.4f}\")\nelse:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# --------------------------------------------------------------------- #\n# iterate over model / dataset pairs\nfor model_name, model_blob in experiment_data.items():\n    for dataset_name, data in model_blob.items():\n\n        # fetch arrays safely\n        train_loss = np.asarray(data[\"losses\"].get(\"train\", []), dtype=float)\n        val_loss = np.asarray(data[\"losses\"].get(\"val\", []), dtype=float)\n        train_f1 = np.asarray(data[\"metrics\"].get(\"train_f1\", []), dtype=float)\n        val_f1 = np.asarray(data[\"metrics\"].get(\"val_f1\", []), dtype=float)\n        preds = np.asarray(data.get(\"predictions\", []), dtype=int)\n        gts = np.asarray(data.get(\"ground_truth\", []), dtype=int)\n        epochs = np.arange(1, len(train_loss) + 1)\n\n        tag = f\"{dataset_name.lower()}_{model_name.lower()}\"\n\n        # ------------------------------------------------------------- #\n        # 1. Loss curves\n        try:\n            if train_loss.size and val_loss.size:\n                plt.figure()\n                plt.plot(epochs, train_loss, label=\"Train\")\n                plt.plot(epochs, val_loss, label=\"Validation\")\n                plt.title(f\"{dataset_name} ({model_name}) Loss Curves\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Cross-Entropy Loss\")\n                plt.legend()\n                plt.savefig(os.path.join(working_dir, f\"{tag}_loss_curves.png\"))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curve for {tag}: {e}\")\n            plt.close()\n\n        # ------------------------------------------------------------- #\n        # 2. F1 curves\n        try:\n            if train_f1.size and val_f1.size:\n                plt.figure()\n                plt.plot(epochs, train_f1, label=\"Train\")\n                plt.plot(epochs, val_f1, label=\"Validation\")\n                plt.title(f\"{dataset_name} ({model_name}) Macro-F1 Curves\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Macro-F1\")\n                plt.legend()\n                plt.savefig(os.path.join(working_dir, f\"{tag}_f1_curves.png\"))\n                plt.close()\n        except Exception as e:\n            print(f\"Error creating F1 curve for {tag}: {e}\")\n            plt.close()\n\n        # ------------------------------------------------------------- #\n        # 3. Confusion matrix on test set\n        try:\n            if preds.size and gts.size:\n                num_classes = int(max(preds.max(), gts.max()) + 1)\n                cm = np.zeros((num_classes, num_classes), dtype=int)\n                for y_true, y_pred in zip(gts, preds):\n                    cm[y_true, y_pred] += 1\n                plt.figure()\n                im = plt.imshow(cm, cmap=\"Blues\")\n                plt.colorbar(im)\n                plt.title(f\"{dataset_name} ({model_name}) Confusion Matrix\")\n                plt.xlabel(\"Predicted\")\n                plt.ylabel(\"Ground Truth\")\n                ticks = np.arange(num_classes)\n                plt.xticks(ticks)\n                plt.yticks(ticks)\n                plt.savefig(os.path.join(working_dir, f\"{tag}_confusion_matrix.png\"))\n                plt.close()\n\n                test_acc = (preds == gts).mean()\n                # simple Macro-F1 for confirmation\n                f1_per_class = []\n                for c in range(num_classes):\n                    tp = np.sum((gts == c) & (preds == c))\n                    fp = np.sum((gts != c) & (preds == c))\n                    fn = np.sum((gts == c) & (preds != c))\n                    prec = tp / (tp + fp + 1e-9) if tp + fp else 0.0\n                    rec = tp / (tp + fn + 1e-9) if tp + fn else 0.0\n                    f1 = 2 * prec * rec / (prec + rec + 1e-9) if prec + rec else 0.0\n                    f1_per_class.append(f1)\n                print(\n                    f\"{tag}: Test Acc={test_acc*100:.2f}%  Macro-F1={np.mean(f1_per_class)*100:.2f}%\"\n                )\n        except Exception as e:\n            print(f\"Error creating confusion matrix for {tag}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# proceed only if data are present\nif \"SPR_BENCH\" in experiment_data:\n    data = experiment_data[\"SPR_BENCH\"]\n\n    # extract arrays safely\n    train_loss = np.asarray(data[\"losses\"].get(\"train\", []), dtype=float)\n    val_loss = np.asarray(data[\"losses\"].get(\"val\", []), dtype=float)\n    train_acc = np.asarray(\n        [m[\"acc\"] for m in data[\"metrics\"].get(\"train\", [])], dtype=float\n    )\n    val_acc = np.asarray(\n        [m[\"acc\"] for m in data[\"metrics\"].get(\"val\", [])], dtype=float\n    )\n    train_f1 = np.asarray(\n        [m[\"f1\"] for m in data[\"metrics\"].get(\"train\", [])], dtype=float\n    )\n    val_f1 = np.asarray([m[\"f1\"] for m in data[\"metrics\"].get(\"val\", [])], dtype=float)\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # ----------------------------------------------------------------- #\n    # 1. Loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 2. Accuracy & F1 curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_acc, label=\"Train Acc\")\n        plt.plot(epochs, val_acc, label=\"Val Acc\")\n        plt.plot(epochs, train_f1, \"--\", label=\"Train F1\")\n        plt.plot(epochs, val_f1, \"--\", label=\"Val F1\")\n        plt.title(\"SPR_BENCH Accuracy & Macro-F1 (Sequence Classification)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Score\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"spr_bench_acc_f1_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating acc/F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 3. Confusion matrix on test set\n    try:\n        preds = np.asarray(data.get(\"predictions\", []), dtype=int)\n        gts = np.asarray(data.get(\"ground_truth\", []), dtype=int)\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for gt, pr in zip(gts, preds):\n                cm[gt, pr] += 1\n\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks)\n            plt.yticks(ticks)\n            plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n            plt.close()\n\n            test_acc = (preds == gts).mean()\n            # macro-F1\n            f1_scores = []\n            for c in range(num_classes):\n                tp = cm[c, c]\n                fp = cm[:, c].sum() - tp\n                fn = cm[c, :].sum() - tp\n                precision = tp / (tp + fp) if tp + fp else 0\n                recall = tp / (tp + fn) if tp + fn else 0\n                f1_scores.append(\n                    2 * precision * recall / (precision + recall)\n                    if precision + recall\n                    else 0\n                )\n            test_f1 = np.mean(f1_scores)\n            print(f\"Test Acc: {test_acc*100:.2f}% | Test Macro-F1: {test_f1:.4f}\")\n        else:\n            print(\"Predictions / ground truth not found for confusion matrix.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\nelse:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------------------------------------------------------- #\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# proceed only if data are present\nif \"SPR_BENCH\" in experiment_data:\n    data = experiment_data[\"SPR_BENCH\"]\n\n    # helper to safely fetch lists -> numpy arrays\n    def _get(arr):\n        try:\n            return np.asarray(arr, dtype=float)\n        except Exception:\n            return np.array([])\n\n    train_loss = _get(data[\"losses\"].get(\"train\", []))\n    val_loss = _get(data[\"losses\"].get(\"val\", []))\n    train_acc = _get(data[\"metrics\"].get(\"train_acc\", []))\n    val_acc = _get(data[\"metrics\"].get(\"val_acc\", []))\n    train_f1 = _get(data[\"metrics\"].get(\"train_f1\", []))\n    val_f1 = _get(data[\"metrics\"].get(\"val_f1\", []))\n    epochs = np.arange(1, len(train_loss) + 1)\n\n    # ----------------------------------------------------------------- #\n    # 1. Loss curves\n    try:\n        if train_loss.size and val_loss.size:\n            plt.figure()\n            plt.plot(epochs, train_loss, label=\"Train\")\n            plt.plot(epochs, val_loss, label=\"Validation\")\n            plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 2. Accuracy curves\n    try:\n        if train_acc.size and val_acc.size:\n            plt.figure()\n            plt.plot(epochs, train_acc, label=\"Train\")\n            plt.plot(epochs, val_acc, label=\"Validation\")\n            plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend()\n            fname = os.path.join(working_dir, \"spr_bench_accuracy_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 3. Macro-F1 curves\n    try:\n        if train_f1.size and val_f1.size:\n            plt.figure()\n            plt.plot(epochs, train_f1, label=\"Train\")\n            plt.plot(epochs, val_f1, label=\"Validation\")\n            plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.legend()\n            fname = os.path.join(working_dir, \"spr_bench_f1_curves.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ----------------------------------------------------------------- #\n    # 4. Confusion matrix on test set\n    try:\n        preds = np.asarray(data.get(\"predictions\", []), dtype=int)\n        gts = np.asarray(data.get(\"ground_truth\", []), dtype=int)\n        if preds.size and gts.size:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for gt, pr in zip(gts, preds):\n                cm[gt, pr] += 1\n\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"Ground Truth\")\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks)\n            plt.yticks(ticks)\n            fname = os.path.join(working_dir, \"spr_bench_confusion_matrix.png\")\n            plt.savefig(fname)\n            print(f\"Saved {fname}\")\n            print(f\"Test accuracy: {(preds == gts).mean()*100:.2f}%\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\nelse:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------ set up and load ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"No SPR_BENCH data found; nothing to plot.\")\n    exit()\n\nspr = experiment_data[\"SPR_BENCH\"]\n\n\n# Helper to safely fetch arrays\ndef _np(arr_name, d, fallback=[]):\n    return np.asarray(d.get(arr_name, fallback), dtype=float)\n\n\ntrain_loss = _np(\"train\", spr[\"losses\"])\nval_loss = _np(\"val\", spr[\"losses\"])\ntrain_acc = _np(\"train_acc\", spr[\"metrics\"])\nval_acc = _np(\"val_acc\", spr[\"metrics\"])\ntrain_f1 = _np(\"train_f1\", spr[\"metrics\"])\nval_f1 = _np(\"val_f1\", spr[\"metrics\"])\nepochs = np.arange(1, len(train_loss) + 1)\n\n\n# ------------------------ plotting helpers ---------------------------------- #\ndef _safe_save(fig_name):\n    return os.path.join(working_dir, f\"spr_bench_{fig_name}.png\")\n\n\n# 1. Loss Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(_safe_save(\"loss_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. Accuracy Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_acc, label=\"Train\")\n    plt.plot(epochs, val_acc, label=\"Validation\")\n    plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(_safe_save(\"accuracy_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# 3. Macro-F1 Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.savefig(_safe_save(\"f1_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 4. Confusion Matrix on Test Set\ntry:\n    preds = np.asarray(spr.get(\"predictions\", []), dtype=int)\n    gts = np.asarray(spr.get(\"ground_truth\", []), dtype=int)\n    if preds.size and gts.size:\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"Ground Truth Label\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(_safe_save(\"confusion_matrix\"))\n        plt.close()\n\n        test_acc = (preds == gts).mean()\n        # Avoid div/0 if single class missing\n        f1_per_class = []\n        for c in range(num_classes):\n            tp = ((preds == c) & (gts == c)).sum()\n            fp = ((preds == c) & (gts != c)).sum()\n            fn = ((preds != c) & (gts == c)).sum()\n            prec = tp / (tp + fp) if tp + fp else 0\n            rec = tp / (tp + fn) if tp + fn else 0\n            f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n            f1_per_class.append(f1)\n        macro_f1 = float(np.mean(f1_per_class))\n        print(f\"Test accuracy: {test_acc*100:.2f}% | Test macro-F1: {macro_f1:.4f}\")\n    else:\n        print(\"Predictions or ground-truth not found; skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# --------------------------------------------------------------------- #\n# load experiment results\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# proceed only if data are present\nif \"SPR_BENCH\" in experiment_data:\n    data = experiment_data[\"SPR_BENCH\"]\n    # guard against missing keys\n    tr_loss = np.array(data[\"losses\"][\"train\"]) if data[\"losses\"][\"train\"] else None\n    val_loss = np.array(data[\"losses\"][\"val\"]) if data[\"losses\"][\"val\"] else None\n    tr_macro = (\n        np.array(data[\"metrics\"][\"train_macroF1\"])\n        if data[\"metrics\"][\"train_macroF1\"]\n        else None\n    )\n    val_macro = (\n        np.array(data[\"metrics\"][\"val_macroF1\"])\n        if data[\"metrics\"][\"val_macroF1\"]\n        else None\n    )\n    preds = np.array(data[\"predictions\"]) if len(data[\"predictions\"]) else None\n    gts = np.array(data[\"ground_truth\"]) if len(data[\"ground_truth\"]) else None\n    epochs = np.arange(1, len(tr_loss) + 1) if tr_loss is not None else None\n\n    # 1. Loss curves\n    try:\n        if tr_loss is not None and val_loss is not None:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Validation Loss\")\n            plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Cross-Entropy Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, \"spr_bench_loss_curves.png\"))\n        else:\n            print(\"Loss data unavailable, skipping loss curve plot.\")\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n    finally:\n        plt.close()\n\n    # 2. Macro-F1 curves\n    try:\n        if tr_macro is not None and val_macro is not None:\n            plt.figure()\n            plt.plot(epochs, tr_macro, label=\"Train Macro-F1\")\n            plt.plot(epochs, val_macro, label=\"Validation Macro-F1\")\n            plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Macro-F1\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, \"spr_bench_macroF1_curves.png\"))\n        else:\n            print(\"Macro-F1 data unavailable, skipping F1 curve plot.\")\n    except Exception as e:\n        print(f\"Error creating Macro-F1 curve: {e}\")\n    finally:\n        plt.close()\n\n    # 3. Confusion matrix on test set\n    try:\n        if preds is not None and gts is not None:\n            num_classes = int(max(preds.max(), gts.max()) + 1)\n            cm = np.zeros((num_classes, num_classes), dtype=int)\n            for gt, pr in zip(gts, preds):\n                cm[gt, pr] += 1\n            plt.figure()\n            im = plt.imshow(cm, cmap=\"Blues\")\n            plt.colorbar(im)\n            plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n            plt.xlabel(\"Predicted Class\")\n            plt.ylabel(\"Ground Truth Class\")\n            ticks = np.arange(num_classes)\n            plt.xticks(ticks)\n            plt.yticks(ticks)\n            plt.savefig(os.path.join(working_dir, \"spr_bench_confusion_matrix.png\"))\n            # print test accuracy\n            test_acc = (preds == gts).mean()\n            print(f\"Test accuracy from saved predictions: {test_acc*100:.2f}%\")\n        else:\n            print(\"Prediction data unavailable, skipping confusion matrix plot.\")\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n    finally:\n        plt.close()\nelse:\n    print(\"SPR_BENCH data not found in experiment_data.npy\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------ set up and load ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"No SPR_BENCH data found; nothing to plot.\")\n    exit()\n\nspr = experiment_data[\"SPR_BENCH\"]\n\n\n# Helper to safely fetch arrays\ndef _np(arr_name, d, fallback=[]):\n    return np.asarray(d.get(arr_name, fallback), dtype=float)\n\n\ntrain_loss = _np(\"train\", spr[\"losses\"])\nval_loss = _np(\"val\", spr[\"losses\"])\ntrain_acc = _np(\"train_acc\", spr[\"metrics\"])\nval_acc = _np(\"val_acc\", spr[\"metrics\"])\ntrain_f1 = _np(\"train_f1\", spr[\"metrics\"])\nval_f1 = _np(\"val_f1\", spr[\"metrics\"])\nepochs = np.arange(1, len(train_loss) + 1)\n\n\n# ------------------------ plotting helpers ---------------------------------- #\ndef _safe_save(fig_name):\n    return os.path.join(working_dir, f\"spr_bench_{fig_name}.png\")\n\n\n# 1. Loss Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(_safe_save(\"loss_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. Accuracy Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_acc, label=\"Train\")\n    plt.plot(epochs, val_acc, label=\"Validation\")\n    plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(_safe_save(\"accuracy_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# 3. Macro-F1 Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.savefig(_safe_save(\"f1_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 4. Confusion Matrix on Test Set\ntry:\n    preds = np.asarray(spr.get(\"predictions\", []), dtype=int)\n    gts = np.asarray(spr.get(\"ground_truth\", []), dtype=int)\n    if preds.size and gts.size:\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"Ground Truth Label\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(_safe_save(\"confusion_matrix\"))\n        plt.close()\n\n        test_acc = (preds == gts).mean()\n        # Avoid div/0 if single class missing\n        f1_per_class = []\n        for c in range(num_classes):\n            tp = ((preds == c) & (gts == c)).sum()\n            fp = ((preds == c) & (gts != c)).sum()\n            fn = ((preds != c) & (gts == c)).sum()\n            prec = tp / (tp + fp) if tp + fp else 0\n            rec = tp / (tp + fn) if tp + fn else 0\n            f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n            f1_per_class.append(f1)\n        macro_f1 = float(np.mean(f1_per_class))\n        print(f\"Test accuracy: {test_acc*100:.2f}% | Test macro-F1: {macro_f1:.4f}\")\n    else:\n        print(\"Predictions or ground-truth not found; skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------ set up and load ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"No SPR_BENCH data found; nothing to plot.\")\n    exit()\n\nspr = experiment_data[\"SPR_BENCH\"]\n\n\n# Helper to safely fetch arrays\ndef _np(arr_name, d, fallback=[]):\n    return np.asarray(d.get(arr_name, fallback), dtype=float)\n\n\ntrain_loss = _np(\"train\", spr[\"losses\"])\nval_loss = _np(\"val\", spr[\"losses\"])\ntrain_acc = _np(\"train_acc\", spr[\"metrics\"])\nval_acc = _np(\"val_acc\", spr[\"metrics\"])\ntrain_f1 = _np(\"train_f1\", spr[\"metrics\"])\nval_f1 = _np(\"val_f1\", spr[\"metrics\"])\nepochs = np.arange(1, len(train_loss) + 1)\n\n\n# ------------------------ plotting helpers ---------------------------------- #\ndef _safe_save(fig_name):\n    return os.path.join(working_dir, f\"spr_bench_{fig_name}.png\")\n\n\n# 1. Loss Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(_safe_save(\"loss_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. Accuracy Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_acc, label=\"Train\")\n    plt.plot(epochs, val_acc, label=\"Validation\")\n    plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(_safe_save(\"accuracy_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# 3. Macro-F1 Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.savefig(_safe_save(\"f1_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 4. Confusion Matrix on Test Set\ntry:\n    preds = np.asarray(spr.get(\"predictions\", []), dtype=int)\n    gts = np.asarray(spr.get(\"ground_truth\", []), dtype=int)\n    if preds.size and gts.size:\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"Ground Truth Label\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(_safe_save(\"confusion_matrix\"))\n        plt.close()\n\n        test_acc = (preds == gts).mean()\n        # Avoid div/0 if single class missing\n        f1_per_class = []\n        for c in range(num_classes):\n            tp = ((preds == c) & (gts == c)).sum()\n            fp = ((preds == c) & (gts != c)).sum()\n            fn = ((preds != c) & (gts == c)).sum()\n            prec = tp / (tp + fp) if tp + fp else 0\n            rec = tp / (tp + fn) if tp + fn else 0\n            f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n            f1_per_class.append(f1)\n        macro_f1 = float(np.mean(f1_per_class))\n        print(f\"Test accuracy: {test_acc*100:.2f}% | Test macro-F1: {macro_f1:.4f}\")\n    else:\n        print(\"Predictions or ground-truth not found; skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------ set up and load ---------------------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nif \"SPR_BENCH\" not in experiment_data:\n    print(\"No SPR_BENCH data found; nothing to plot.\")\n    exit()\n\nspr = experiment_data[\"SPR_BENCH\"]\n\n\n# Helper to safely fetch arrays\ndef _np(arr_name, d, fallback=[]):\n    return np.asarray(d.get(arr_name, fallback), dtype=float)\n\n\ntrain_loss = _np(\"train\", spr[\"losses\"])\nval_loss = _np(\"val\", spr[\"losses\"])\ntrain_acc = _np(\"train_acc\", spr[\"metrics\"])\nval_acc = _np(\"val_acc\", spr[\"metrics\"])\ntrain_f1 = _np(\"train_f1\", spr[\"metrics\"])\nval_f1 = _np(\"val_f1\", spr[\"metrics\"])\nepochs = np.arange(1, len(train_loss) + 1)\n\n\n# ------------------------ plotting helpers ---------------------------------- #\ndef _safe_save(fig_name):\n    return os.path.join(working_dir, f\"spr_bench_{fig_name}.png\")\n\n\n# 1. Loss Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.title(\"SPR_BENCH Loss Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(_safe_save(\"loss_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 2. Accuracy Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_acc, label=\"Train\")\n    plt.plot(epochs, val_acc, label=\"Validation\")\n    plt.title(\"SPR_BENCH Accuracy Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(_safe_save(\"accuracy_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# 3. Macro-F1 Curves\ntry:\n    plt.figure()\n    plt.plot(epochs, train_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.savefig(_safe_save(\"f1_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating F1 curve: {e}\")\n    plt.close()\n\n# 4. Confusion Matrix on Test Set\ntry:\n    preds = np.asarray(spr.get(\"predictions\", []), dtype=int)\n    gts = np.asarray(spr.get(\"ground_truth\", []), dtype=int)\n    if preds.size and gts.size:\n        num_classes = int(max(preds.max(), gts.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Test Set)\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"Ground Truth Label\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(_safe_save(\"confusion_matrix\"))\n        plt.close()\n\n        test_acc = (preds == gts).mean()\n        # Avoid div/0 if single class missing\n        f1_per_class = []\n        for c in range(num_classes):\n            tp = ((preds == c) & (gts == c)).sum()\n            fp = ((preds == c) & (gts != c)).sum()\n            fn = ((preds != c) & (gts == c)).sum()\n            prec = tp / (tp + fp) if tp + fp else 0\n            rec = tp / (tp + fn) if tp + fn else 0\n            f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n            f1_per_class.append(f1)\n        macro_f1 = float(np.mean(f1_per_class))\n        print(f\"Test accuracy: {test_acc*100:.2f}% | Test macro-F1: {macro_f1:.4f}\")\n    else:\n        print(\"Predictions or ground-truth not found; skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# --------------------- set-up and load multiple runs ----------------------- #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Paths supplied in the instructions\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_6aa66816593f4e348669a73aefe3accf_proc_3165650/experiment_data.npy\",\n    \"experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_f98776920b86441fbe3539396a25ac23_proc_3165651/experiment_data.npy\",\n    \"experiments/2025-08-17_00-43-58_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_50e9a1032a074217aeb86db5fd47fd2e_proc_3165649/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_p = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp = np.load(full_p, allow_pickle=True).item()\n        if \"SPR_BENCH\" in exp:\n            all_experiment_data.append(exp[\"SPR_BENCH\"])\n        else:\n            print(f\"Warning: 'SPR_BENCH' not found in {p}, skipping this run.\")\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif not all_experiment_data:\n    print(\"No valid SPR_BENCH runs found; nothing to aggregate.\")\n    exit()\n\n\n# --------------------- helper to extract arrays safely -------------------- #\ndef _np(arr_name, d, parent_key, fallback=[]):\n    return np.asarray(d.get(parent_key, {}).get(arr_name, fallback), dtype=float)\n\n\n# Gather per-run arrays\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\ntrain_f1s, val_f1s = [], []\ntest_preds, test_gts = [], []\nfor run in all_experiment_data:\n    train_losses.append(_np(\"train\", run, \"losses\"))\n    val_losses.append(_np(\"val\", run, \"losses\"))\n    train_accs.append(_np(\"train_acc\", run, \"metrics\"))\n    val_accs.append(_np(\"val_acc\", run, \"metrics\"))\n    train_f1s.append(_np(\"train_f1\", run, \"metrics\"))\n    val_f1s.append(_np(\"val_f1\", run, \"metrics\"))\n    # predictions/gt could be absent\n    test_preds.append(np.asarray(run.get(\"predictions\", []), dtype=int))\n    test_gts.append(np.asarray(run.get(\"ground_truth\", []), dtype=int))\n\n# Trim to common epoch length\nmin_epochs = min(map(len, train_losses))\n\n\ndef _stack_and_trim(lst):\n    return np.stack([x[:min_epochs] for x in lst], axis=0)  # shape (runs, epochs)\n\n\ntrain_losses = _stack_and_trim(train_losses)\nval_losses = _stack_and_trim(val_losses)\ntrain_accs = _stack_and_trim(train_accs)\nval_accs = _stack_and_trim(val_accs)\ntrain_f1s = _stack_and_trim(train_f1s)\nval_f1s = _stack_and_trim(val_f1s)\nepochs = np.arange(1, min_epochs + 1)\n\n\n# Mean and SE\ndef _mean_se(arr):\n    mean = arr.mean(axis=0)\n    se = arr.std(axis=0, ddof=1) / np.sqrt(arr.shape[0])  # standard error\n    return mean, se\n\n\nmean_train_loss, se_train_loss = _mean_se(train_losses)\nmean_val_loss, se_val_loss = _mean_se(val_losses)\nmean_train_acc, se_train_acc = _mean_se(train_accs)\nmean_val_acc, se_val_acc = _mean_se(val_accs)\nmean_train_f1, se_train_f1 = _mean_se(train_f1s)\nmean_val_f1, se_val_f1 = _mean_se(val_f1s)\n\n\ndef _safe_save(fig_name):\n    return os.path.join(working_dir, f\"spr_bench_{fig_name}.png\")\n\n\n# ----------------------------- 1. Loss Curves ------------------------------ #\ntry:\n    plt.figure()\n    plt.plot(epochs, mean_train_loss, label=\"Train (mean)\")\n    plt.fill_between(\n        epochs,\n        mean_train_loss - se_train_loss,\n        mean_train_loss + se_train_loss,\n        alpha=0.3,\n        label=\"Train (\u00b1SE)\",\n    )\n    plt.plot(epochs, mean_val_loss, label=\"Validation (mean)\")\n    plt.fill_between(\n        epochs,\n        mean_val_loss - se_val_loss,\n        mean_val_loss + se_val_loss,\n        alpha=0.3,\n        label=\"Validation (\u00b1SE)\",\n    )\n    plt.title(\"SPR_BENCH Loss Curves with Mean \u00b1 SE (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(_safe_save(\"mean_loss_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curve: {e}\")\n    plt.close()\n\n# --------------------------- 2. Accuracy Curves ---------------------------- #\ntry:\n    plt.figure()\n    plt.plot(epochs, mean_train_acc, label=\"Train (mean)\")\n    plt.fill_between(\n        epochs,\n        mean_train_acc - se_train_acc,\n        mean_train_acc + se_train_acc,\n        alpha=0.3,\n        label=\"Train (\u00b1SE)\",\n    )\n    plt.plot(epochs, mean_val_acc, label=\"Validation (mean)\")\n    plt.fill_between(\n        epochs,\n        mean_val_acc - se_val_acc,\n        mean_val_acc + se_val_acc,\n        alpha=0.3,\n        label=\"Validation (\u00b1SE)\",\n    )\n    plt.title(\"SPR_BENCH Accuracy Curves with Mean \u00b1 SE (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(_safe_save(\"mean_accuracy_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated accuracy curve: {e}\")\n    plt.close()\n\n# --------------------------- 3. Macro-F1 Curves ---------------------------- #\ntry:\n    plt.figure()\n    plt.plot(epochs, mean_train_f1, label=\"Train (mean)\")\n    plt.fill_between(\n        epochs,\n        mean_train_f1 - se_train_f1,\n        mean_train_f1 + se_train_f1,\n        alpha=0.3,\n        label=\"Train (\u00b1SE)\",\n    )\n    plt.plot(epochs, mean_val_f1, label=\"Validation (mean)\")\n    plt.fill_between(\n        epochs,\n        mean_val_f1 - se_val_f1,\n        mean_val_f1 + se_val_f1,\n        alpha=0.3,\n        label=\"Validation (\u00b1SE)\",\n    )\n    plt.title(\"SPR_BENCH Macro-F1 Curves with Mean \u00b1 SE (Sequence Classification)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.savefig(_safe_save(\"mean_f1_curves\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated F1 curve: {e}\")\n    plt.close()\n\n# ---------------------- 4. Aggregated Confusion Matrix --------------------- #\ntry:\n    # Concatenate predictions / gts that are present\n    preds_concat = np.concatenate([p for p in test_preds if p.size])\n    gts_concat = np.concatenate([g for g in test_gts if g.size])\n    if preds_concat.size and gts_concat.size:\n        num_classes = int(max(preds_concat.max(), gts_concat.max()) + 1)\n        cm = np.zeros((num_classes, num_classes), dtype=int)\n        for gt, pr in zip(gts_concat, preds_concat):\n            cm[gt, pr] += 1\n\n        plt.figure()\n        im = plt.imshow(cm, cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.title(\"SPR_BENCH Confusion Matrix (Aggregated Test Set)\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"Ground Truth Label\")\n        ticks = np.arange(num_classes)\n        plt.xticks(ticks)\n        plt.yticks(ticks)\n        plt.savefig(_safe_save(\"aggregated_confusion_matrix\"))\n        plt.close()\n\n        # Compute per-run accuracy and macro-F1 to report mean\u00b1std\n        accs, f1s = [], []\n        for p, g in zip(test_preds, test_gts):\n            if p.size and g.size:\n                accs.append((p == g).mean())\n                # macro-F1\n                nc = int(max(p.max(), g.max()) + 1)\n                f1_per_cls = []\n                for c in range(nc):\n                    tp = ((p == c) & (g == c)).sum()\n                    fp = ((p == c) & (g != c)).sum()\n                    fn = ((p != c) & (g == c)).sum()\n                    prec = tp / (tp + fp) if tp + fp else 0\n                    rec = tp / (tp + fn) if tp + fn else 0\n                    f1 = 2 * prec * rec / (prec + rec) if prec + rec else 0\n                    f1_per_cls.append(f1)\n                f1s.append(np.mean(f1_per_cls))\n        if accs:\n            accs = np.asarray(accs)\n            f1s = np.asarray(f1s)\n            print(\n                f\"Test accuracy: {accs.mean()*100:.2f}% \u00b1 {accs.std()*100:.2f}% | \"\n                f\"Test macro-F1: {f1s.mean():.4f} \u00b1 {f1s.std():.4f}\"\n            )\n    else:\n        print(\n            \"Predictions or ground-truth not found in any run; skipping confusion matrix.\"\n        )\nexcept Exception as e:\n    print(f\"Error creating aggregated confusion matrix: {e}\")\n    plt.close()\n"], "plot_plan": ["The script loads the saved experiment data, gathers per-epoch accuracy/loss and final test accuracy for each dropout setting, and determines the best model by test accuracy. It then creates four independent figures\u2014accuracy curves, loss curves, a bar chart of test accuracies, and a confusion-matrix heat-map for the best dropout\u2014each wrapped in a try-except block to satisfy safety requirements. Titles, subtitles, and descriptive filenames include the dataset name and plot type, and every figure is closed after saving. All plots rely solely on the contents of experiment_data.npy; nothing is synthesized. After plotting, the script prints the test accuracy for every dropout and highlights the best-performing rate so users can quickly assess results. This keeps the number of figures \u22645, uses plain matplotlib, and adheres to the directory and naming conventions specified.", "We will load experiment_data.npy, iterate through all recorded model\u2013dataset pairs, and extract the lists already stored during training (train/val losses, accuracies, macro-F1).  For every dataset we will create up to four standard visualizations\u2014loss curves, accuracy curves, macro-F1 curves and a confusion matrix on the test set\u2014each wrapped in a separate try/except to guarantee safe execution and proper figure closing.  All plots will be saved to the working directory with descriptive filenames that include both the dataset and the plot type, and each figure will contain a clear title specifying the dataset and the content (e.g. \u201cSPR_BENCH Loss Curves (Sequence Classification)\u201d).  We avoid fabricating data by only plotting keys that actually exist in the stored dict, skipping any missing metrics.  Confusion matrices are generated only when predictions and ground-truth arrays are present; the accompanying test accuracy and macro-F1 are printed to the console for quick inspection.  To respect the \u201cmax 5 figures\u201d rule, we limit ourselves to the four plots mentioned per dataset and never loop over epochs for sample visualisations.  Every figure is explicitly closed with plt.close() to free memory.  The concise code below follows the mandatory imports and directory handling required by the guidelines and can be run as-is inside the provided environment.", "The solution loads the stored experiment results, extracts available loss, macro-F1 and prediction arrays, and then produces three standard figures: (1) training/validation loss curves, (2) training/validation macro-F1 curves, and (3) a confusion matrix for the test set. Each figure is generated inside its own try-except block, saved to the working directory with descriptive names, and the figure is always closed afterward. The code follows the required import order, recreates working_dir, never fabricates data, and skips plots gracefully if any expected array is missing. All titles explicitly mention \u201cSPR_BENCH\u201d and the task type, while saved filenames include the dataset name plus the plot type for clarity. Printing a simple test accuracy from predictions gives an extra quick metric check. The whole script is concise, self-contained and respects the five-figure upper limit.", "We will load the saved experiment_data.npy, extract the SPR_BENCH split, and convert the recorded lists into NumPy arrays for easy plotting. Five independent try-except blocks will create: (1) loss curves, (2) accuracy curves, (3) macro-F1 curves, (4) a test-set confusion matrix, and (5) a per-class F1 bar chart. Each figure is titled clearly and saved in working_dir with descriptive names that include the dataset and plot type. After plotting, we compute test accuracy and macro-F1 directly from the stored predictions/ground_truth and print them. All figures are closed after saving to avoid memory leaks, and any plotting error is caught and reported without stopping subsequent plots. Only data present in experiment_data.npy are used\u2014no synthetic values are introduced. At most five figures are produced, satisfying the interval/quantity constraint. The code begins with the required imports and always ensures the working directory exists before saving plots.", "We first load experiment_data.npy and iterate over each model/dataset present. For every (model,dataset) pair we extract training/validation loss and F1 arrays along with test\u2010set predictions and ground truth. Three separate try-except blocks then 1) draw loss curves, 2) draw F1 curves, and 3) build a confusion matrix, saving each figure to working_dir with descriptive filenames that include both model and dataset names. Each figure is closed after saving to avoid memory leaks. We also compute and print the final test accuracy/Macro-F1 so users can confirm consistency with stored metrics. All matplotlib code follows basic style, and no synthetic data are generated\u2014everything comes directly from the .npy file. The whole script is concise, self-contained, and respects the 5-figure maximum (only 3 plots here). Finally, figures include explicit titles like \u201cSPR_BENCH (Token-level Transformer) Loss Curves\u201d with clear axis labels and legends.", "We need to load experiment_data.npy, extract the lists stored during training (losses, accuracies, F1 scores, predictions/ground-truth), and generate three separate plots: loss curves, accuracy/F1 curves, and a confusion matrix for the test set. Each plot must live in its own try-except block, be titled clearly, saved in working_dir with descriptive names, and the figure must be closed afterwards. We will also compute and print test accuracy and macro-F1 from the saved predictions. The code begins with the mandated imports, ensures working_dir exists, safely loads the data, gracefully handles missing keys, limits itself to real data, and prints the evaluation metrics at the end.", "We first load the saved numpy dictionary, check that the \u201cSPR_BENCH\u201d key is present, and unpack loss, accuracy, and f1 arrays.  Next we create up-to-date epoch indices and, inside individual try-except blocks, draw and save (1) loss curves, (2) accuracy curves, (3) macro-F1 curves, and (4) a confusion matrix for the test set.  All figures are titled, labelled, saved to working_dir with descriptive filenames, and closed in every path (success or failure).  Before plotting the confusion matrix we reconstruct it with numpy only, and we also compute and print the test accuracy for confirmation.  We cap \u201cepoch-wise\u201d plots to the single set of curves so we never exceed five similar figures.  Every file name contains both the dataset (\u201cspr_bench\u201d) and the plot type for clarity.  The whole script uses only matplotlib + numpy, no custom styles, and nothing is plotted unless the data arrays actually exist.  Finally, the code prints the saved file names to reassure the user everything was generated successfully.", "The script first checks that working/experiment_data.npy exists and loads it, aborting cleanly if the file or the SPR_BENCH entry is missing. It then pulls out the stored train/val loss, accuracy and macro-F1 arrays, converts them to NumPy and builds an epoch index. For each of the three learning curves (loss, accuracy, F1) it opens a new figure inside its own try-except, plots train vs. validation, adds an informative title, labels, legend, saves to working_dir with a descriptive name and finally closes the figure. If predictions and ground-truth labels are present, it builds a confusion matrix, visualises it with imshow, titles it clearly, saves and closes; it also prints the test accuracy and macro-F1 derived from those arrays so the user sees numerical results in the log. All figures are guaranteed to close even on error, and no more than four plots are produced, keeping within the five-figure limit. The code relies only on data already stored in experiment_data.npy and uses plain matplotlib without extra styling. Paths and file names are consistent and self-descriptive so downstream scripts can find the outputs easily.", "We will first load experiment_data.npy, verify that the SPR_BENCH entry exists, and safely extract the lists we actually stored during training (losses[\"train\"], losses[\"val\"], metrics[\"train_macroF1\"], metrics[\"val_macroF1\"], predictions, ground_truth).  Using numpy we convert these lists into arrays and build an epochs index.  Three independent visualisations are created in separate try-except blocks: (1) train/validation loss curves, (2) train/validation macro-F1 curves, and (3) a confusion matrix built from the saved test predictions.  For robustness we always close figures in finally/except paths and skip a plot if the required arrays are missing.  Every file is saved to working_dir with descriptive names that include the dataset and plot type (e.g. spr_bench_loss_curves.png).  Titles explicitly mention the dataset and a clear subtitle when relevant; axis labels and legends are added for clarity.  We keep the total number of figures to three (\u22645 as required) and rely solely on the data present in experiment_data.npy\u2014no fabricated values.  The script begins with the mandated imports and directory setup so it can be dropped into the runtime as-is.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["dropout", null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["Below is a small utility that immediately loads the saved experiment file from\nthe working directory, walks through every dataset and every hyper-parameter\nconfiguration, and prints (1) the final train accuracy/loss, (2) the best\nvalidation accuracy/loss achieved during training, and (3) the test\naccuracy/loss recorded after training. All metric names are spelled out\nexplicitly so there is no ambiguity.", "Below is the plan and code.   The script first locates the working directory,\nloads the NumPy file into a dictionary, and then iterates through all stored\nmodel/dataset combinations.   For every dataset it prints the dataset name once,\nthen prints the final (last-epoch) value for every training/validation metric\nand the recorded test metrics, each preceded by an explicit, human-readable\nlabel.   No figures are generated and the script runs immediately on execution.", "We will load the numpy file from the working directory, pull out the dictionary\nthat contains metrics, and for each dataset compute (1) the best macro-F1 on the\ntraining split, (2) the best macro-F1 on the validation split, (3) the minimum\ntraining loss, (4) the minimum validation loss, and (5) the final test macro-F1\nusing the stored predictions and ground-truth labels.  Each value is printed\nwith an explicit, descriptive name so the output is self-explanatory.  The\nscript runs immediately when executed.", "We load the NumPy file from the working directory, turn it back into a Python\ndictionary, and iterate over each stored dataset. For every dataset we pick the\nlast (i.e., final) value of each recorded metric series and print it with an\nexplicit, human-readable label. If test-set predictions and ground-truth labels\nare present, we additionally compute and print final test accuracy and macro-F1.\nThe script contains no entry-point guard, so it executes immediately when run.", "Below is a small utility that immediately loads the saved NumPy file, extracts\nthe recorded metrics, computes the test-set macro-F1 from the stored\npredictions, and prints the requested best/final statistics for every dataset it\nfinds. It respects the original dictionary structure and follows the printing\nrules you specified.", "The script will load experiment_data.npy from the working directory, iterate\nover every stored dataset (e.g., \u201cSPR_BENCH\u201d), and compute/collect the final-\nepoch training and validation metrics as well as test-set accuracy and macro-F1\nderived from the saved predictions and ground-truth labels. It prints each\ndataset name followed by clearly labelled metric lines such as \u201ctrain accuracy\u201d,\n\u201cvalidation loss\u201d, or \u201ctest F1 score\u201d, always showing the last recorded (or, for\nthe test set, freshly computed) value. No plots are generated and the code\nexecutes immediately at the global scope.", "The script will locate the saved NumPy file inside the working directory,\ndeserialize it into a Python dictionary, and iterate through each dataset stored\nin it (e.g., \u201cSPR_BENCH\u201d).   For every dataset it extracts the lists of per-\nepoch metrics, selects the best value for each metric (maximum for accuracy/F1,\nminimum for loss), and recomputes final test accuracy and macro-F1 from the\nstored predictions/ground-truth.   The results are printed in a clear, labeled\nmanner: the dataset name first, followed by explicit metric names such as \u201cbest\ntraining accuracy,\u201d \u201cbest validation loss,\u201d and \u201ctest F1 score.\u201d", "Below is a tiny script that 1. loads working/experiment_data.npy, 2. extracts\nall metric lists for every dataset present, 3. selects the \u201cbest\u201d value (max for\naccuracy/F1 lists, min for loss lists, fresh recomputation for test metrics), 4.\nprints each dataset name first and then every metric with a clear, explicit\nlabel.", "Below is a small utility that immediately loads the saved NumPy file from the\nworking directory, extracts the stored arrays/lists, computes the required\n\u201cfinal\u201d or \u201cbest\u201d values, and prints them with fully-qualified metric names for\neach dataset (here, \u201cSPR_BENCH\u201d).  \u201cFinal\u201d means the last element in the\ntraining/validation loss or training macro-F1 list, while \u201cbest\u201d refers to the\nmaximum validation macro-F1 achieved during training.  The script additionally\nrecomputes test accuracy and test macro-F1 from the saved predictions and\nground-truth labels so every core metric is reported in a single run.", "Below is a tiny script that 1. loads working/experiment_data.npy, 2. extracts\nall metric lists for every dataset present, 3. selects the \u201cbest\u201d value (max for\naccuracy/F1 lists, min for loss lists, fresh recomputation for test metrics), 4.\nprints each dataset name first and then every metric with a clear, explicit\nlabel.", "Below is a tiny script that 1. loads working/experiment_data.npy, 2. extracts\nall metric lists for every dataset present, 3. selects the \u201cbest\u201d value (max for\naccuracy/F1 lists, min for loss lists, fresh recomputation for test metrics), 4.\nprints each dataset name first and then every metric with a clear, explicit\nlabel.", "Below is a tiny script that 1. loads working/experiment_data.npy, 2. extracts\nall metric lists for every dataset present, 3. selects the \u201cbest\u201d value (max for\naccuracy/F1 lists, min for loss lists, fresh recomputation for test metrics), 4.\nprints each dataset name first and then every metric with a clear, explicit\nlabel.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper functions -------------------------------------------------------------#\ndef _best_val_accuracy(val_acc_list):\n    \"\"\"Return the maximum validation accuracy.\"\"\"\n    return max(val_acc_list) if val_acc_list else float(\"nan\")\n\n\ndef _best_val_loss(val_loss_list):\n    \"\"\"Return the minimum validation loss.\"\"\"\n    return min(val_loss_list) if val_loss_list else float(\"nan\")\n\n\ndef _print_metrics(config_name, res_dict):\n    \"\"\"Pretty-print metrics for one hyper-parameter configuration.\"\"\"\n    train_acc_final = res_dict[\"metrics\"][\"train_acc\"][-1]\n    val_acc_best = _best_val_accuracy(res_dict[\"metrics\"][\"val_acc\"])\n    train_loss_final = res_dict[\"losses\"][\"train_loss\"][-1]\n    val_loss_best = _best_val_loss(res_dict[\"losses\"][\"val_loss\"])\n    test_acc = res_dict[\"test_acc\"]\n    test_loss = res_dict[\"test_loss\"]\n\n    print(f\"  Configuration: dropout = {config_name}\")\n    print(f\"    final train accuracy: {train_acc_final:.4f}\")\n    print(f\"    best validation accuracy: {val_acc_best:.4f}\")\n    print(f\"    final train loss: {train_loss_final:.4f}\")\n    print(f\"    best validation loss: {val_loss_best:.4f}\")\n    print(f\"    test accuracy: {test_acc:.4f}\")\n    print(f\"    test loss: {test_loss:.4f}\")\n\n\n# -----------------------------------------------------------------------------#\n# main script -----------------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(exp_path):\n    raise FileNotFoundError(f\"Could not find experiment data at: {exp_path}\")\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# walk through experiments \u2192 datasets \u2192 hyper-parameter configurations\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, configs in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")\n        for config_name, res_dict in configs.items():\n            _print_metrics(config_name, res_dict)\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# locate saved experiment file -------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nnpy_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(npy_path):\n    raise FileNotFoundError(f\"Could not find experiment data at {npy_path}\")\n\nexperiment_data = np.load(npy_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# helper to print a metric with consistent formatting -------------------------#\ndef _p(label: str, value):\n    if isinstance(value, float):\n        print(f\"{label}: {value:.4f}\")\n    else:\n        print(f\"{label}: {value}\")\n\n\n# -----------------------------------------------------------------------------#\n# iterate through all results --------------------------------------------------#\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, details in datasets.items():\n        print(dataset_name)  # dataset header\n        # --- training / validation metrics ------------------------------------\n        metrics = details.get(\"metrics\", {})\n        losses = details.get(\"losses\", {})\n        if \"train_acc\" in metrics and metrics[\"train_acc\"]:\n            _p(\"train accuracy\", metrics[\"train_acc\"][-1])\n        if \"val_acc\" in metrics and metrics[\"val_acc\"]:\n            _p(\"validation accuracy\", metrics[\"val_acc\"][-1])\n        if \"train_f1\" in metrics and metrics[\"train_f1\"]:\n            _p(\"train F1 score\", metrics[\"train_f1\"][-1])\n        if \"val_f1\" in metrics and metrics[\"val_f1\"]:\n            _p(\"validation F1 score\", metrics[\"val_f1\"][-1])\n\n        if \"train\" in losses and losses[\"train\"]:\n            _p(\"train loss\", losses[\"train\"][-1])\n        if \"val\" in losses and losses[\"val\"]:\n            _p(\"validation loss\", losses[\"val\"][-1])\n\n        # --- test metrics ------------------------------------------------------\n        test_acc = details.get(\"test_acc\")\n        test_f1 = details.get(\"test_f1\")\n        test_loss = details.get(\"test_loss\")\n        if test_acc is not None:\n            _p(\"test accuracy\", test_acc)\n        if test_f1 is not None:\n            _p(\"test F1 score\", test_f1)\n        if test_loss is not None:\n            _p(\"test loss\", test_loss)\n\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# -----------------------------------------------------------------------------#\n# locate and load experiment data                                              #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# iterate over datasets and report metrics                                     #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # dataset header\n\n    # Extract stored histories\n    train_f1_history = data[\"metrics\"][\"train\"]\n    val_f1_history = data[\"metrics\"][\"val\"]\n    train_loss_history = data[\"losses\"][\"train\"]\n    val_loss_history = data[\"losses\"][\"val\"]\n\n    # Best / final values\n    best_train_f1 = max(train_f1_history) if train_f1_history else float(\"nan\")\n    best_val_f1 = max(val_f1_history) if val_f1_history else float(\"nan\")\n    min_train_loss = min(train_loss_history) if train_loss_history else float(\"nan\")\n    min_val_loss = min(val_loss_history) if val_loss_history else float(\"nan\")\n\n    # Print results\n    print(f\"best train F1 score: {best_train_f1:.4f}\")\n    print(f\"best validation F1 score: {best_val_f1:.4f}\")\n    print(f\"minimum train loss: {min_train_loss:.4f}\")\n    print(f\"minimum validation loss: {min_val_loss:.4f}\")\n\n    # Compute test F1 from saved predictions / ground truth if available\n    preds = data.get(\"predictions\", [])\n    gts = data.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        print(f\"test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helpers                                                                      #\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, num_cls: int) -> float:\n    \"\"\"Compute unweighted (macro) F1 score.\"\"\"\n    f1s = []\n    for c in range(num_cls):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n        if tp == fp == fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n        rec = tp / (tp + fn) if tp + fn > 0 else 0.0\n        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# load experiment data                                                         #\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# iterate and report                                                           #\nfor ds_name, ds_info in experiment_data.items():\n    print(ds_name)  # dataset header\n\n    # ----- training / validation metrics -------------------------------------#\n    metrics = ds_info.get(\"metrics\", {})\n    losses = ds_info.get(\"losses\", {})\n\n    if metrics:\n        if metrics.get(\"train_acc\"):\n            print(\"train accuracy:\", metrics[\"train_acc\"][-1])\n        if metrics.get(\"validation_acc\") or metrics.get(\"val_acc\"):\n            # handle either key\n            val_key = \"validation_acc\" if \"validation_acc\" in metrics else \"val_acc\"\n            print(\"validation accuracy:\", metrics[val_key][-1])\n\n        if metrics.get(\"train_f1\"):\n            print(\"train F1 score:\", metrics[\"train_f1\"][-1])\n        if metrics.get(\"validation_f1\") or metrics.get(\"val_f1\"):\n            val_f1_key = \"validation_f1\" if \"validation_f1\" in metrics else \"val_f1\"\n            print(\"validation F1 score:\", metrics[val_f1_key][-1])\n\n    if losses:\n        if losses.get(\"train\"):\n            print(\"train loss:\", losses[\"train\"][-1])\n        if losses.get(\"val\") or losses.get(\"validation\"):\n            val_loss_key = \"validation\" if \"validation\" in losses else \"val\"\n            print(\"validation loss:\", losses[val_loss_key][-1])\n\n    # ----- test metrics -------------------------------------------------------#\n    preds = ds_info.get(\"predictions\")\n    gts = ds_info.get(\"ground_truth\")\n\n    if isinstance(preds, np.ndarray) and isinstance(gts, np.ndarray) and preds.size:\n        test_acc = float(np.mean(preds == gts))\n        num_labels = len(np.unique(gts))\n        test_f1 = macro_f1(preds, gts, num_labels)\n        print(\"test accuracy:\", test_acc)\n        print(\"test F1 score:\", test_f1)\n", "import os\nimport numpy as np\n\n# -----------------------------------------------------------------------------#\n# Locate and load experiment data ---------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# Macro-F1 utility (identical to training script) -----------------------------#\ndef macro_f1_score(y_true, y_pred, num_classes):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((y_true == c) & (y_pred == c))\n        fp = np.sum((y_true != c) & (y_pred == c))\n        fn = np.sum((y_true == c) & (y_pred != c))\n        if tp == 0 and (fp == 0 or fn == 0):\n            f1 = 0.0\n        else:\n            prec = tp / (tp + fp + 1e-9)\n            rec = tp / (tp + fn + 1e-9)\n            f1 = 2 * prec * rec / (prec + rec + 1e-9)\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# Traverse experiments and print metrics --------------------------------------#\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"\\nDataset: {dataset_name}\")  # requirement 3\n\n        # --- F1 scores --------------------------------------------------------#\n        train_f1_list = data.get(\"metrics\", {}).get(\"train_f1\", [])\n        val_f1_list = data.get(\"metrics\", {}).get(\"val_f1\", [])\n        if train_f1_list:\n            best_train_f1 = max(train_f1_list)\n            print(f\"Best training F1 score: {best_train_f1:.4f}\")\n        if val_f1_list:\n            best_val_f1 = max(val_f1_list)\n            print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n\n        # --- Losses -----------------------------------------------------------#\n        train_loss_list = data.get(\"losses\", {}).get(\"train\", [])\n        val_loss_list = data.get(\"losses\", {}).get(\"val\", [])\n        if train_loss_list:\n            final_train_loss = train_loss_list[-1]\n            print(f\"Final training loss: {final_train_loss:.4f}\")\n        if val_loss_list:\n            final_val_loss = val_loss_list[-1]\n            print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n        # --- Test F1 (compute from stored predictions) ------------------------#\n        preds = data.get(\"predictions\", [])\n        gts = data.get(\"ground_truth\", [])\n        if preds and gts and len(preds) == len(gts):\n            num_classes = len(set(gts))\n            test_f1 = macro_f1_score(gts, preds, num_classes)\n            print(f\"Test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# -----------------------------------------------------------------------------#\n# locate and load experiment data ---------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n\n# -----------------------------------------------------------------------------#\n# helper to safely fetch last element -----------------------------------------#\ndef _last(lst, default=None):\n    return lst[-1] if lst else default\n\n\n# -----------------------------------------------------------------------------#\n# iterate over datasets --------------------------------------------------------#\nfor dset_name, dset_dict in experiment_data.items():\n    print(f\"{dset_name}\")  # dataset header\n\n    # training / validation metrics -------------------------------------------#\n    train_metrics = _last(dset_dict.get(\"metrics\", {}).get(\"train\", []), {})\n    val_metrics = _last(dset_dict.get(\"metrics\", {}).get(\"val\", []), {})\n    train_loss = _last(dset_dict.get(\"losses\", {}).get(\"train\", []))\n    val_loss = _last(dset_dict.get(\"losses\", {}).get(\"val\", []))\n\n    if train_metrics:\n        print(f\"train accuracy: {train_metrics.get('acc', np.nan):.4f}\")\n        print(f\"train F1 score: {train_metrics.get('f1',  np.nan):.4f}\")\n    if val_metrics:\n        print(f\"validation accuracy: {val_metrics.get('acc', np.nan):.4f}\")\n        print(f\"validation F1 score: {val_metrics.get('f1',  np.nan):.4f}\")\n    if train_loss is not None:\n        print(f\"train loss: {train_loss:.4f}\")\n    if val_loss is not None:\n        print(f\"validation loss: {val_loss:.4f}\")\n\n    # test metrics from saved predictions -------------------------------------#\n    preds = dset_dict.get(\"predictions\", [])\n    gts = dset_dict.get(\"ground_truth\", [])\n    if preds and gts:\n        test_acc = accuracy_score(gts, preds)\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n        print(f\"test accuracy: {test_acc:.4f}\")\n        print(f\"test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper functions                                                              #\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int) -> float:\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n        if tp == fp == fn == 0:\n            f1s.append(0.0)\n            continue\n        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\ndef best_or_final(values, mode=\"max\"):\n    \"\"\"Return best value according to mode ('max' or 'min'); fallback to last.\"\"\"\n    if not values:\n        return None\n    return max(values) if mode == \"max\" else min(values)\n\n\n# -----------------------------------------------------------------------------#\n# load experiment data                                                          #\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# iterate through datasets and print metrics                                    #\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)  # Dataset heading\n\n    # Training / validation metrics\n    train_acc_best = best_or_final(data[\"metrics\"][\"train_acc\"], mode=\"max\")\n    val_acc_best = best_or_final(data[\"metrics\"][\"val_acc\"], mode=\"max\")\n    train_f1_best = best_or_final(data[\"metrics\"][\"train_f1\"], mode=\"max\")\n    val_f1_best = best_or_final(data[\"metrics\"][\"val_f1\"], mode=\"max\")\n    train_loss_best = best_or_final(data[\"losses\"][\"train\"], mode=\"min\")\n    val_loss_best = best_or_final(data[\"losses\"][\"val\"], mode=\"min\")\n\n    if train_acc_best is not None:\n        print(f\"best training accuracy: {train_acc_best:.4f}\")\n    if val_acc_best is not None:\n        print(f\"best validation accuracy: {val_acc_best:.4f}\")\n    if train_f1_best is not None:\n        print(f\"best training F1 score: {train_f1_best:.4f}\")\n    if val_f1_best is not None:\n        print(f\"best validation F1 score: {val_f1_best:.4f}\")\n    if train_loss_best is not None:\n        print(f\"best training loss: {train_loss_best:.6f}\")\n    if val_loss_best is not None:\n        print(f\"best validation loss: {val_loss_best:.6f}\")\n\n    # Test set metrics (recompute from stored predictions & labels)\n    preds = np.asarray(data.get(\"predictions\", []))\n    gts = np.asarray(data.get(\"ground_truth\", []))\n    if preds.size > 0 and gts.size > 0:\n        num_classes = len(np.unique(np.concatenate([preds, gts])))\n        test_acc = (preds == gts).mean()\n        test_f1 = macro_f1(preds, gts, num_classes)\n        print(f\"test accuracy: {test_acc:.4f}\")\n        print(f\"test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n\n# ------------- helper ---------------------------------------------------------\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int) -> float:\n    \"\"\"\n    Compute unweighted (macro) F1 score for integer\u2010labelled predictions.\n    \"\"\"\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = (\n            0.0\n            if precision + recall == 0\n            else 2 * precision * recall / (precision + recall)\n        )\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# ------------- load -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------- iterate & report ----------------------------------------------\nfor ds_name, ds_data in experiment_data.items():\n    print(ds_name)\n\n    # ---- metric lists --------------------------------------------------------\n    train_acc_list = ds_data[\"metrics\"].get(\"train_acc\", [])\n    val_acc_list = ds_data[\"metrics\"].get(\"val_acc\", [])\n    train_f1_list = ds_data[\"metrics\"].get(\"train_f1\", [])\n    val_f1_list = ds_data[\"metrics\"].get(\"val_f1\", [])\n    train_loss_lst = ds_data[\"losses\"].get(\"train\", [])\n    val_loss_lst = ds_data[\"losses\"].get(\"val\", [])\n\n    if train_acc_list:\n        print(f\"best training accuracy: {max(train_acc_list):.4f}\")\n    if val_acc_list:\n        print(f\"best validation accuracy: {max(val_acc_list):.4f}\")\n    if train_f1_list:\n        print(f\"best training macro F1 score: {max(train_f1_list):.4f}\")\n    if val_f1_list:\n        print(f\"best validation macro F1 score: {max(val_f1_list):.4f}\")\n    if train_loss_lst:\n        print(f\"best training loss: {min(train_loss_lst):.6f}\")\n    if val_loss_lst:\n        print(f\"best validation loss: {min(val_loss_lst):.6f}\")\n\n    # ---- test metrics --------------------------------------------------------\n    preds = np.asarray(ds_data.get(\"predictions\", []))\n    gts = np.asarray(ds_data.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_acc = float(np.mean(preds == gts))\n        num_classes = len(np.unique(np.concatenate([preds, gts])))\n        test_f1 = macro_f1(preds, gts, num_classes)\n        print(f\"test accuracy: {test_acc:.4f}\")\n        print(f\"test macro F1 score: {test_f1:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n\n# -----------------------------------------------------------------------------#\n# helper ----------------------------------------------------------------------#\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, n_classes: int) -> float:\n    f1s = []\n    for c in range(n_classes):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n        prec = tp / (tp + fp) if (tp + fp) else 0.0\n        rec = tp / (tp + fn) if (tp + fn) else 0.0\n        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# -----------------------------------------------------------------------------#\n# load experiment data --------------------------------------------------------#\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# -----------------------------------------------------------------------------#\n# iterate over datasets -------------------------------------------------------#\nfor ds_name, ds_dict in experiment_data.items():\n    print(f\"\\nDataset: {ds_name}\")\n\n    # metrics lists\n    train_macroF1_list = ds_dict[\"metrics\"][\"train_macroF1\"]\n    val_macroF1_list = ds_dict[\"metrics\"][\"val_macroF1\"]\n    train_loss_list = ds_dict[\"losses\"][\"train\"]\n    val_loss_list = ds_dict[\"losses\"][\"val\"]\n\n    # final values\n    final_train_macroF1 = train_macroF1_list[-1] if train_macroF1_list else None\n    best_val_macroF1 = max(val_macroF1_list) if val_macroF1_list else None\n    final_train_loss = train_loss_list[-1] if train_loss_list else None\n    final_val_loss = val_loss_list[-1] if val_loss_list else None\n\n    # predictions / ground truth for test metrics\n    preds = np.array(ds_dict.get(\"predictions\", []))\n    ground_truth = np.array(ds_dict.get(\"ground_truth\", []))\n    if preds.size and ground_truth.size:\n        num_classes = len(np.unique(ground_truth))\n        test_accuracy = float(np.mean(preds == ground_truth))\n        test_macroF1 = macro_f1(preds, ground_truth, num_classes)\n    else:\n        test_accuracy = None\n        test_macroF1 = None\n\n    # print metrics -----------------------------------------------------------#\n    if final_train_loss is not None:\n        print(f\"train loss: {final_train_loss:.6f}\")\n    if final_val_loss is not None:\n        print(f\"validation loss: {final_val_loss:.6f}\")\n    if final_train_macroF1 is not None:\n        print(f\"train macro F1 score: {final_train_macroF1:.6f}\")\n    if best_val_macroF1 is not None:\n        print(f\"best validation macro F1 score: {best_val_macroF1:.6f}\")\n    if test_accuracy is not None:\n        print(f\"test accuracy: {test_accuracy:.6f}\")\n    if test_macroF1 is not None:\n        print(f\"test macro F1 score: {test_macroF1:.6f}\")\n", "import os\nimport numpy as np\n\n\n# ------------- helper ---------------------------------------------------------\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int) -> float:\n    \"\"\"\n    Compute unweighted (macro) F1 score for integer\u2010labelled predictions.\n    \"\"\"\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = (\n            0.0\n            if precision + recall == 0\n            else 2 * precision * recall / (precision + recall)\n        )\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# ------------- load -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------- iterate & report ----------------------------------------------\nfor ds_name, ds_data in experiment_data.items():\n    print(ds_name)\n\n    # ---- metric lists --------------------------------------------------------\n    train_acc_list = ds_data[\"metrics\"].get(\"train_acc\", [])\n    val_acc_list = ds_data[\"metrics\"].get(\"val_acc\", [])\n    train_f1_list = ds_data[\"metrics\"].get(\"train_f1\", [])\n    val_f1_list = ds_data[\"metrics\"].get(\"val_f1\", [])\n    train_loss_lst = ds_data[\"losses\"].get(\"train\", [])\n    val_loss_lst = ds_data[\"losses\"].get(\"val\", [])\n\n    if train_acc_list:\n        print(f\"best training accuracy: {max(train_acc_list):.4f}\")\n    if val_acc_list:\n        print(f\"best validation accuracy: {max(val_acc_list):.4f}\")\n    if train_f1_list:\n        print(f\"best training macro F1 score: {max(train_f1_list):.4f}\")\n    if val_f1_list:\n        print(f\"best validation macro F1 score: {max(val_f1_list):.4f}\")\n    if train_loss_lst:\n        print(f\"best training loss: {min(train_loss_lst):.6f}\")\n    if val_loss_lst:\n        print(f\"best validation loss: {min(val_loss_lst):.6f}\")\n\n    # ---- test metrics --------------------------------------------------------\n    preds = np.asarray(ds_data.get(\"predictions\", []))\n    gts = np.asarray(ds_data.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_acc = float(np.mean(preds == gts))\n        num_classes = len(np.unique(np.concatenate([preds, gts])))\n        test_f1 = macro_f1(preds, gts, num_classes)\n        print(f\"test accuracy: {test_acc:.4f}\")\n        print(f\"test macro F1 score: {test_f1:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n\n# ------------- helper ---------------------------------------------------------\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int) -> float:\n    \"\"\"\n    Compute unweighted (macro) F1 score for integer\u2010labelled predictions.\n    \"\"\"\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = (\n            0.0\n            if precision + recall == 0\n            else 2 * precision * recall / (precision + recall)\n        )\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# ------------- load -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------- iterate & report ----------------------------------------------\nfor ds_name, ds_data in experiment_data.items():\n    print(ds_name)\n\n    # ---- metric lists --------------------------------------------------------\n    train_acc_list = ds_data[\"metrics\"].get(\"train_acc\", [])\n    val_acc_list = ds_data[\"metrics\"].get(\"val_acc\", [])\n    train_f1_list = ds_data[\"metrics\"].get(\"train_f1\", [])\n    val_f1_list = ds_data[\"metrics\"].get(\"val_f1\", [])\n    train_loss_lst = ds_data[\"losses\"].get(\"train\", [])\n    val_loss_lst = ds_data[\"losses\"].get(\"val\", [])\n\n    if train_acc_list:\n        print(f\"best training accuracy: {max(train_acc_list):.4f}\")\n    if val_acc_list:\n        print(f\"best validation accuracy: {max(val_acc_list):.4f}\")\n    if train_f1_list:\n        print(f\"best training macro F1 score: {max(train_f1_list):.4f}\")\n    if val_f1_list:\n        print(f\"best validation macro F1 score: {max(val_f1_list):.4f}\")\n    if train_loss_lst:\n        print(f\"best training loss: {min(train_loss_lst):.6f}\")\n    if val_loss_lst:\n        print(f\"best validation loss: {min(val_loss_lst):.6f}\")\n\n    # ---- test metrics --------------------------------------------------------\n    preds = np.asarray(ds_data.get(\"predictions\", []))\n    gts = np.asarray(ds_data.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_acc = float(np.mean(preds == gts))\n        num_classes = len(np.unique(np.concatenate([preds, gts])))\n        test_f1 = macro_f1(preds, gts, num_classes)\n        print(f\"test accuracy: {test_acc:.4f}\")\n        print(f\"test macro F1 score: {test_f1:.4f}\")\n\n    print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n\n# ------------- helper ---------------------------------------------------------\ndef macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int) -> float:\n    \"\"\"\n    Compute unweighted (macro) F1 score for integer\u2010labelled predictions.\n    \"\"\"\n    f1s = []\n    for c in range(num_classes):\n        tp = np.sum((preds == c) & (labels == c))\n        fp = np.sum((preds == c) & (labels != c))\n        fn = np.sum((preds != c) & (labels == c))\n\n        if tp + fp == 0 or tp + fn == 0:\n            f1s.append(0.0)\n            continue\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = (\n            0.0\n            if precision + recall == 0\n            else 2 * precision * recall / (precision + recall)\n        )\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n\n# ------------- load -----------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# ------------- iterate & report ----------------------------------------------\nfor ds_name, ds_data in experiment_data.items():\n    print(ds_name)\n\n    # ---- metric lists --------------------------------------------------------\n    train_acc_list = ds_data[\"metrics\"].get(\"train_acc\", [])\n    val_acc_list = ds_data[\"metrics\"].get(\"val_acc\", [])\n    train_f1_list = ds_data[\"metrics\"].get(\"train_f1\", [])\n    val_f1_list = ds_data[\"metrics\"].get(\"val_f1\", [])\n    train_loss_lst = ds_data[\"losses\"].get(\"train\", [])\n    val_loss_lst = ds_data[\"losses\"].get(\"val\", [])\n\n    if train_acc_list:\n        print(f\"best training accuracy: {max(train_acc_list):.4f}\")\n    if val_acc_list:\n        print(f\"best validation accuracy: {max(val_acc_list):.4f}\")\n    if train_f1_list:\n        print(f\"best training macro F1 score: {max(train_f1_list):.4f}\")\n    if val_f1_list:\n        print(f\"best validation macro F1 score: {max(val_f1_list):.4f}\")\n    if train_loss_lst:\n        print(f\"best training loss: {min(train_loss_lst):.6f}\")\n    if val_loss_lst:\n        print(f\"best validation loss: {min(val_loss_lst):.6f}\")\n\n    # ---- test metrics --------------------------------------------------------\n    preds = np.asarray(ds_data.get(\"predictions\", []))\n    gts = np.asarray(ds_data.get(\"ground_truth\", []))\n    if preds.size and gts.size:\n        test_acc = float(np.mean(preds == gts))\n        num_classes = len(np.unique(np.concatenate([preds, gts])))\n        test_f1 = macro_f1(preds, gts, num_classes)\n        print(f\"test accuracy: {test_acc:.4f}\")\n        print(f\"test macro F1 score: {test_f1:.4f}\")\n\n    print()  # blank line between datasets\n", ""], "parse_term_out": ["['\\nDataset: SPR_BENCH', '\\n', '  Configuration: dropout = 0.0', '\\n', '\nfinal train accuracy: 0.7940', '\\n', '    best validation accuracy: 0.7960',\n'\\n', '    final train loss: 0.5043', '\\n', '    best validation loss: 0.5147',\n'\\n', '    test accuracy: 0.7950', '\\n', '    test loss: 0.5113', '\\n', '\nConfiguration: dropout = 0.1', '\\n', '    final train accuracy: 0.7945', '\\n', '\nbest validation accuracy: 0.7960', '\\n', '    final train loss: 0.5106', '\\n', '\nbest validation loss: 0.5193', '\\n', '    test accuracy: 0.7900', '\\n', '\ntest loss: 0.5113', '\\n', '  Configuration: dropout = 0.2', '\\n', '    final\ntrain accuracy: 0.7870', '\\n', '    best validation accuracy: 0.7960', '\\n', '\nfinal train loss: 0.5159', '\\n', '    best validation loss: 0.5153', '\\n', '\ntest accuracy: 0.7880', '\\n', '    test loss: 0.5179', '\\n', '  Configuration:\ndropout = 0.3', '\\n', '    final train accuracy: 0.7950', '\\n', '    best\nvalidation accuracy: 0.7960', '\\n', '    final train loss: 0.5086', '\\n', '\nbest validation loss: 0.5123', '\\n', '    test accuracy: 0.7950', '\\n', '\ntest loss: 0.5128', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.7870', '\\n', 'validation accuracy:\n0.7860', '\\n', 'train F1 score: 0.7868', '\\n', 'validation F1 score: 0.7860',\n'\\n', 'train loss: 0.5653', '\\n', 'validation loss: 0.5602', '\\n', 'test\naccuracy: 0.7890', '\\n', 'test F1 score: 0.7890', '\\n', 'test loss: 0.5745',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best train F1 score: 0.7950', '\\n', 'best validation F1\nscore: 0.7959', '\\n', 'minimum train loss: 0.5194', '\\n', 'minimum validation\nloss: 0.5252', '\\n', 'test F1 score: 0.7950', '\\n', 'Execution time: a moment\nseconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy:', ' ', '0.7915', '\\n', 'validation\naccuracy:', ' ', '0.788', '\\n', 'train F1 score:', ' ', '0.79147701034039',\n'\\n', 'validation F1 score:', ' ', '0.7879864311315924', '\\n', 'train loss:', '\n', '0.5103659129142761', '\\n', 'validation loss:', ' ', '0.5202790570259094',\n'\\n', 'test accuracy:', ' ', '0.792', '\\n', 'test F1 score:', ' ',\n'0.7919700436862909', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'Best training F1 score: 0.7788', '\\n', 'Best\nvalidation F1 score: 0.7880', '\\n', 'Final training loss: 1.9251', '\\n', 'Final\nvalidation loss: 0.5710', '\\n', 'Test F1 score: 0.7910', '\\n', 'Execution time:\na moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'train accuracy: 0.7930', '\\n', 'train F1 score: 0.7930',\n'\\n', 'validation accuracy: 0.7960', '\\n', 'validation F1 score: 0.7959', '\\n',\n'train loss: 0.5513', '\\n', 'validation loss: 0.5553', '\\n', 'test accuracy:\n0.7950', '\\n', 'test F1 score: 0.7950', '\\n', 'Execution time: a moment seconds\n(time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best training accuracy: 0.7880', '\\n', 'best validation\naccuracy: 0.7960', '\\n', 'best training F1 score: 0.7880', '\\n', 'best\nvalidation F1 score: 0.7959', '\\n', 'best training loss: 0.593557', '\\n', 'best\nvalidation loss: 0.595679', '\\n', 'test accuracy: 0.7970', '\\n', 'test F1 score:\n0.7970', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best training accuracy: 0.7865', '\\n', 'best validation\naccuracy: 0.8040', '\\n', 'best training macro F1 score: 0.7864', '\\n', 'best\nvalidation macro F1 score: 0.8038', '\\n', 'best training loss: 0.559914', '\\n',\n'best validation loss: 0.565252', '\\n', 'test accuracy: 0.7900', '\\n', 'test\nmacro F1 score: 0.7900', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['\\nDataset: SPR_BENCH', '\\n', 'train loss: 0.553591', '\\n', 'validation loss:\n0.561289', '\\n', 'train macro F1 score: 0.791443', '\\n', 'best validation macro\nF1 score: 0.793960', '\\n', 'test accuracy: 0.794000', '\\n', 'test macro F1\nscore: 0.793979', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['SPR_BENCH', '\\n', 'best training accuracy: 0.7935', '\\n', 'best validation\naccuracy: 0.7960', '\\n', 'best training macro F1 score: 0.7935', '\\n', 'best\nvalidation macro F1 score: 0.7959', '\\n', 'best training loss: 0.548907', '\\n',\n'best validation loss: 0.557889', '\\n', 'test accuracy: 0.7930', '\\n', 'test\nmacro F1 score: 0.7930', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best training accuracy: 0.7880', '\\n', 'best validation\naccuracy: 0.7880', '\\n', 'best training macro F1 score: 0.7880', '\\n', 'best\nvalidation macro F1 score: 0.7880', '\\n', 'best training loss: 0.550702', '\\n',\n'best validation loss: 0.562414', '\\n', 'test accuracy: 0.7900', '\\n', 'test\nmacro F1 score: 0.7900', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['SPR_BENCH', '\\n', 'best training accuracy: 0.7940', '\\n', 'best validation\naccuracy: 0.7900', '\\n', 'best training macro F1 score: 0.7940', '\\n', 'best\nvalidation macro F1 score: 0.7900', '\\n', 'best training loss: 0.548828', '\\n',\n'best validation loss: 0.551552', '\\n', 'test accuracy: 0.7920', '\\n', 'test\nmacro F1 score: 0.7920', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}