{"edges": [[0, 7], [0, 2], [0, 5], [0, 1], [0, 4], [0, 3], [1, 11], [1, 10], [1, 9], [1, 12], [2, 8], [4, 6]], "layout": [[0.65625, 0.0], [0.1875, 0.5], [0.5, 0.5], [0.625, 0.5], [0.75, 0.5], [0.875, 0.5], [0.75, 1.0], [1.0, 0.5], [0.5, 1.0], [0.0, 1.0], [0.125, 1.0], [0.25, 1.0], [0.375, 1.0]], "plan": ["We convert each symbolic sequence to character-level tokens, build a vocabulary\nfrom the training split, and pad/clip all sequences to a fixed length.  A small\ntransformer encoder with learnable positional embeddings yields contextual\nrepresentations whose mean is passed to a linear classifier.  We train the model\nwith cross-entropy on the SPR_BENCH train set and monitor validation loss and\nmacro-F1 on the dev set after every epoch, printing the results and storing them\nin a structured experiment_data dictionary.  All tensors and the model are moved\nto GPU when available, following the required device handling guidelines.  The\nscript automatically loads the benchmark using the provided utility, constructs\nPyTorch datasets/dataloaders, trains for a few epochs (default 5), evaluates on\nthe held-out test split once training finishes, and saves metrics, predictions,\nand ground truth to \u201c./working/experiment_data.npy\u201d.  This serves as a clean,\nreproducible baseline that can be extended with larger models or richer\ntokenization strategies in later experiments.", "Hyperparam tuning name: num_epochs. We grid-search the single hyper-parameter\nnum_epochs (5, 10, 20, 30).   For every setting a fresh model is trained; after\neach epoch the model is evaluated on the dev set and the scores/losses are\nstored.   An early\u2013stopping mechanism with patience = 5 is added so runs can\nfinish earlier if the validation macro-F1 stops improving.   All results for\nevery epoch configuration are collected in the dictionary experiment_data \u2192\n'num_epochs' \u2192 '<epochs>' and finally saved to experiment_data.npy.", "Hyperparam tuning name: learning_rate. The solution trains a fresh Transformer\nmodel for each learning-rate in the grid {5e-4, 7e-4, 1e-3, 2e-3}.   For every\nrate it logs epoch-wise losses and macro-F1 for train/validation splits,\nperforms a final test evaluation, stores predictions/ground-truth, and finally\nsaves everything in a single numpy file under the \u201clearning_rate\u201d\nhyperparameter-tuning group.", "Hyperparam tuning name: batch_size. We loop over the candidate batch sizes {32,\n64, 128, 256}.   For every batch size we (re)build dataloaders, model,\noptimizer, and train for a fixed number of epochs while collecting loss/F1.\nValidation macro-F1 is monitored; the best-performing checkpoint across all\nbatch sizes is retained and finally evaluated on the test set.   All metrics,\nlosses, predictions, etc. are stored in experiment_data under the key\n\u201cbatch_size\u201d and saved to experiment_data.npy for later analysis.", "Hyperparam tuning name: d_model (Transformer hidden/embedding dimension). We\nwill grid-search the Transformer embedding dimension (`d_model` \u2208 {64, 128, 256,\n384}).   For each value we build a fresh model, train for five epochs with the\nsame optimizer/loader settings, evaluate on dev and test splits, and store\nlosses, macro-F1 scores, predictions and ground-truth in\n`experiment_data['d_model']['SPR_BENCH'][str(d_model)]`.   After trying all four\nconfigurations the populated `experiment_data` dict is saved to\n`experiment_data.npy`.", "Hyperparam tuning name: weight_decay. We loop over a list of candidate weight-\ndecay values, re-instantiating the model and Adam optimiser each time with the\ncurrent decay. For every setting we train for a small number of epochs, track\nper-epoch train/validation loss and macro-F1, then evaluate on the test set. All\nresults are collected in the experiment_data dictionary under the key\n\"weight_decay\" and saved as experiment_data.npy so they can be plotted later.", "We remove the deprecated __main__ guard so the script runs immediately, filter\nthe \u201cbatch\u201d dictionary so only tensors are moved to GPU (fixing an occasional\ntype-error), and add a learning-rate scheduler plus early-stopping to satisfy\nthe hyper-parameter-tuning stage. All metrics/losses are stored and saved to\nworking_dir as required.", "Hyperparam tuning name: nhead. We keep d_model fixed at 128 and sweep nhead \u2208\n{2, 4, 8}.   For each head count we build a fresh SPRModel, train for five\nepochs, record per-epoch losses and macro-F1 on both train/dev splits, then\nevaluate on the test split.   All results are stored in a nested experiment_data\nstructure under the key 'nhead_tuning'\u2192'SPR_BENCH'\u2192'head_{n}' and finally saved\nto working/experiment_data.npy.   Aside from the loop over nhead values the\noriginal data-loading, training, and evaluation logic is unchanged.", "The primary issue is that the previous script encloses the entire execution\nlogic inside `if __name__ == \"__main__\":`, which violates the framework\u2019s\nrequirement that code must execute immediately when the file is run.  We move\nthe training logic to the global scope so it starts automatically, keep all\ntensors/models on the correct device, and retain the original architecture while\nstill allowing learning-rate sweeps.  The script now also attempts to locate the\nbenchmark folder flexibly (environment variable, relative path, or fallback\nerror) and follows the mandatory experiment-data saving conventions.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport pathlib\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\n\n# -------------------- dataset loader utility (from prompt) --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    dset[\"train\"] = _load(\"train.csv\")\n    dset[\"dev\"] = _load(\"dev.csv\")\n    dset[\"test\"] = _load(\"test.csv\")\n    return dset\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        label = self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        length = len(ids)\n        if length < self.max_len:\n            ids += [self.pad_id] * (self.max_len - length)\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=256\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1), :]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)  # (batch, seq, d_model)\n        x = x.mean(dim=1)\n        return self.cls(x)\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    total_loss, all_pred, all_true = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        optimizer.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = out.argmax(dim=1).detach().cpu().numpy()\n        all_pred.extend(preds)\n        all_true.extend(batch[\"labels\"].cpu().numpy())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_true, all_pred, average=\"macro\")\n    return avg_loss, macro_f1\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    total_loss, all_pred, all_true = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        total_loss += loss.item() * batch[\"labels\"].size(0)\n        preds = out.argmax(dim=1).cpu().numpy()\n        all_pred.extend(preds)\n        all_true.extend(batch[\"labels\"].cpu().numpy())\n    avg_loss = total_loss / len(loader.dataset)\n    macro_f1 = f1_score(all_true, all_pred, average=\"macro\")\n    return avg_loss, macro_f1, all_pred, all_true\n\n\n# -------------------- main routine --------------------\ndef main():\n    # Path to SPR_BENCH folder (adjust if needed)\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"Dataset path {DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # Build vocab from training sequences\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256)\n    test_loader = DataLoader(test_ds, batch_size=256)\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    epochs = 5\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_macro_f1 = {val_f1:.4f}\"\n        )\n\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(epoch)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n    # Final test evaluation\n    test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n    print(f\"Test macro_f1 = {test_f1:.4f}\")\n    experiment_data[\"SPR_BENCH\"][\"predictions\"] = preds\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"] = gts\n\n    # Save experiment data\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# Execute immediately\nmain()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"num_epochs\": {}}  # hyper-parameter tuning type\n\n\n# -------------------- dataset loader utility --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        dset[s if s != \"dev\" else \"dev\"] = _load(f\"{s}.csv\")\n    return dset\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq, label = self.seqs[idx], self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed, self.pos = nn.Embedding(vocab_size, d_model), nn.Parameter(\n            torch.randn(1, max_len, d_model)\n        )\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256)\n        self.transformer, self.cls = nn.TransformerEncoder(\n            enc_layer, num_layers\n        ), nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1).mean(1)\n        return self.cls(x)\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(true, pred, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        tot_loss += criterion(out, batch[\"labels\"]).item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(true, pred, average=\"macro\"),\n        pred,\n        true,\n    )\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab & datasets ---------------------------------------------------------\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({ch: i + 2 for i, ch in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    train_loader = DataLoader(\n        SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    epoch_grid = [5, 10, 20, 30]  # hyper-parameter values\n    patience = 5\n\n    for max_epochs in epoch_grid:\n        key = f\"epochs_{max_epochs}\"\n        experiment_data[\"num_epochs\"][key] = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n\n        model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        best_val, wait, best_state = 0.0, 0, None\n        for epoch in range(1, max_epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(\n                f\"[{key}] Epoch {epoch}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\"\n            )\n\n            # store\n            ed = experiment_data[\"num_epochs\"][key]\n            ed[\"epochs\"].append(epoch)\n            ed[\"losses\"][\"train\"].append(tr_loss)\n            ed[\"losses\"][\"val\"].append(val_loss)\n            ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n            # early stopping\n            if val_f1 > best_val:\n                best_val, wait, best_state = val_f1, 0, model.state_dict()\n            else:\n                wait += 1\n            if wait >= patience:\n                print(\n                    f\"Early stopping at epoch {epoch} (best val_macro_f1={best_val:.4f})\"\n                )\n                break\n\n        # load best model and evaluate on test set\n        if best_state is not None:\n            model.load_state_dict(best_state)\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"[{key}] Test macro_f1={test_f1:.4f}\")\n        ed = experiment_data[\"num_epochs\"][key]\n        ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n        ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # -------------------- save all experiment data --------------------\n    working_dir = os.path.join(os.getcwd(), \"working\")\n    os.makedirs(working_dir, exist_ok=True)\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# Execute immediately\nmain()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- reproducibility & device --------------------\ntorch.manual_seed(42)\nnp.random.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\n    \"learning_rate\": {\"SPR_BENCH\": {}}  # we will add one entry per LR value\n}\n\n\n# -------------------- dataset utilities ---------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq, label = self.seqs[idx], self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        if len(ids) < self.max_len:\n            ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model ---------------------------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=256)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n        return self.cls(x.mean(dim=1))\n\n\n# -------------------- train / eval utils --------------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tot_loss, yp, yt = 0.0, [], []\n    for b in loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        optimizer.zero_grad()\n        out = model(b[\"input_ids\"])\n        loss = criterion(out, b[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * b[\"labels\"].size(0)\n        yp.extend(out.argmax(1).detach().cpu().numpy())\n        yt.extend(b[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(yt, yp, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, yp, yt = 0.0, [], []\n    for b in loader:\n        b = {k: v.to(device) for k, v in b.items()}\n        out = model(b[\"input_ids\"])\n        loss = criterion(out, b[\"labels\"])\n        tot_loss += loss.item() * b[\"labels\"].size(0)\n        yp.extend(out.argmax(1).cpu().numpy())\n        yt.extend(b[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(yt, yp, average=\"macro\"), yp, yt\n\n\n# -------------------- main routine --------------------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab & lengths\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\n        \"<pad>\": 0,\n        \"<unk>\": 1,\n        **{ch: i + 2 for i, ch in enumerate(sorted(chars))},\n    }\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    # datasets\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n    num_cls = len(set(spr[\"train\"][\"label\"]))\n\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256)\n    test_loader = DataLoader(test_ds, batch_size=256)\n\n    lrs = [5e-4, 7e-4, 1e-3, 2e-3]\n    epochs = 5\n    criterion = nn.CrossEntropyLoss()\n\n    for lr in lrs:\n        print(f\"\\n=== Training with learning_rate = {lr} ===\")\n        model = SPRModel(len(vocab), num_cls, max_len=max_len).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n        lr_key = f\"lr_{lr}\"\n        experiment_data[\"learning_rate\"][\"SPR_BENCH\"][lr_key] = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n        rec = experiment_data[\"learning_rate\"][\"SPR_BENCH\"][lr_key]\n\n        for ep in range(1, epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(f\"Epoch {ep}: val_loss={val_loss:.4f}, val_macro_f1={val_f1:.4f}\")\n\n            rec[\"epochs\"].append(ep)\n            rec[\"losses\"][\"train\"].append(tr_loss)\n            rec[\"losses\"][\"val\"].append(val_loss)\n            rec[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            rec[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        # final test evaluation\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"Test macro_f1={test_f1:.4f}\")\n        rec[\"test_macro_f1\"] = test_f1\n        rec[\"predictions\"] = preds\n        rec[\"ground_truth\"] = gts\n\n    # -------------- save experiment data ---------------------------\n    working_dir = os.path.join(os.getcwd(), \"working\")\n    os.makedirs(working_dir, exist_ok=True)\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# run\nif __name__ == \"__main__\":\n    main()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- dirs & experiment container --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nexperiment_data = {\"batch_size\": {\"SPR_BENCH\": {}}}\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# -------------------- dataset utils --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        ids = [\n            self.vocab.get(ch, self.vocab[\"<unk>\"])\n            for ch in self.seqs[idx][: self.max_len]\n        ]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.encoder(x.transpose(0, 1)).transpose(0, 1).mean(1)\n        return self.cls(x)\n\n\n# -------------------- train / eval --------------------\ndef train_epoch(model, loader, crit, opt):\n    model.train()\n    tot_loss, preds, trues = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        trues.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(trues, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, crit):\n    model.eval()\n    tot_loss, preds, trues = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        trues.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(trues, preds, average=\"macro\"),\n        preds,\n        trues,\n    )\n\n\n# -------------------- main --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(DATA_PATH)\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab & datasets\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\n        \"<pad>\": 0,\n        \"<unk>\": 1,\n        **{ch: i + 2 for i, ch in enumerate(sorted(chars))},\n    }\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    crit = nn.CrossEntropyLoss()\n\n    candidate_bsz = [32, 64, 128, 256]\n    best_val, best_state, best_bs = -1.0, None, None\n\n    for bs in candidate_bsz:\n        print(f\"\\n=== Training with batch_size={bs} ===\")\n        exp_entry = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n        experiment_data[\"batch_size\"][\"SPR_BENCH\"][bs] = exp_entry\n\n        train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=256)\n        model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        epochs = 5\n        best_val_bs = -1.0\n        best_state_bs = None\n        for ep in range(1, epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, crit, opt)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, crit)\n            print(f\"  Epoch {ep}: val_loss={val_loss:.4f}, val_f1={val_f1:.4f}\")\n            exp_entry[\"epochs\"].append(ep)\n            exp_entry[\"losses\"][\"train\"].append(tr_loss)\n            exp_entry[\"losses\"][\"val\"].append(val_loss)\n            exp_entry[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            exp_entry[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n            if val_f1 > best_val_bs:\n                best_val_bs, best_state_bs = val_f1, model.state_dict()\n\n        # keep best per batch size\n        if best_val_bs > best_val:\n            best_val, best_state, best_bs = best_val_bs, best_state_bs, bs\n\n    # -------------------- final test with best batch size --------------------\n    print(f\"\\nBest batch_size = {best_bs} with val_macro_f1 = {best_val:.4f}\")\n    model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n    model.load_state_dict(best_state)\n    test_loader = DataLoader(test_ds, batch_size=256)\n    test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, crit)\n    print(f\"Test macro_f1 = {test_f1:.4f}\")\n\n    # store predictions for the best model\n    exp_best = experiment_data[\"batch_size\"][\"SPR_BENCH\"][best_bs]\n    exp_best[\"predictions\"] = preds\n    exp_best[\"ground_truth\"] = gts\n    exp_best[\"test_macro_f1\"] = test_f1\n\n    # save\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# --------------------  config / misc --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# experiment data container\nexperiment_data = {\n    \"d_model\": {  # hyper-parameter tuned\n        \"SPR_BENCH\": {}  # will hold per-d_model results\n    }\n}\n\n\n# -------------------- dataset loader --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dset[split if split != \"dev\" else \"dev\"] = _load(f\"{split}.csv\")\n    return dset\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids),\n            \"labels\": torch.tensor(self.labels[idx]),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=256\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n        return self.cls(x.mean(1))\n\n\n# -------------------- train / eval --------------------\ndef train_epoch(model, loader, criterion, optim):\n    model.train()\n    tl, ap, at = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optim.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optim.step()\n        tl += loss.item() * batch[\"labels\"].size(0)\n        ap.extend(out.argmax(1).cpu().numpy())\n        at.extend(batch[\"labels\"].cpu().numpy())\n    return tl / len(loader.dataset), f1_score(at, ap, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tl, ap, at = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        tl += loss.item() * batch[\"labels\"].size(0)\n        ap.extend(out.argmax(1).cpu().numpy())\n        at.extend(batch[\"labels\"].cpu().numpy())\n    return tl / len(loader.dataset), f1_score(at, ap, average=\"macro\"), ap, at\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"Dataset path {DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab / datasets (shared across runs)\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\n        \"<pad>\": 0,\n        \"<unk>\": 1,\n        **{ch: i + 2 for i, ch in enumerate(sorted(chars))},\n    }\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256)\n    test_loader = DataLoader(test_ds, batch_size=256)\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    for d_model in [64, 128, 256, 384]:\n        print(f\"\\n=== Training with d_model = {d_model} ===\")\n        model = SPRModel(len(vocab), num_classes, d_model=d_model, max_len=max_len).to(\n            device\n        )\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        # containers for this run\n        run_dict = {\n            \"epochs\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for epoch in range(1, 6):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            vl_loss, vl_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(f\"Epoch {epoch}: val_loss={vl_loss:.4f}, val_macro_f1={vl_f1:.4f}\")\n            run_dict[\"epochs\"].append(epoch)\n            run_dict[\"metrics\"][\"train\"].append(tr_f1)\n            run_dict[\"metrics\"][\"val\"].append(vl_f1)\n            run_dict[\"losses\"][\"train\"].append(tr_loss)\n            run_dict[\"losses\"][\"val\"].append(vl_loss)\n\n        # final test evaluation\n        te_loss, te_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"Test macro_f1 (d_model={d_model}) = {te_f1:.4f}\")\n        run_dict[\"predictions\"] = preds\n        run_dict[\"ground_truth\"] = gts\n        run_dict[\"test_macro_f1\"] = te_f1\n        experiment_data[\"d_model\"][\"SPR_BENCH\"][str(d_model)] = run_dict\n\n    # save all experiments\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"weight_decay\": {}}  # will hold results for each wd value\n\n\n# -------------------- dataset loader utility --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        {\n            \"train\": _load(\"train.csv\"),\n            \"dev\": _load(\"dev.csv\"),\n            \"test\": _load(\"test.csv\"),\n        }\n    )\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels, self.vocab, self.max_len = (\n            hf_ds[\"sequence\"],\n            hf_ds[\"label\"],\n            vocab,\n            max_len,\n        )\n        self.pad_id = vocab[\"<pad>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        if len(ids) < self.max_len:\n            ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(self, vocab_sz, n_cls, d_model=128, nhead=4, n_layers=2, max_len=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n        self.cls = nn.Linear(d_model, n_cls)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n        return self.cls(x.mean(1))\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, crit, opt):\n    model.train()\n    tot_loss, preds, trues = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).detach().cpu().numpy())\n        trues.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(trues, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, crit):\n    model.eval()\n    tot_loss, preds, trues = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        trues.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(trues, preds, average=\"macro\"),\n        preds,\n        trues,\n    )\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"Dataset path {DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # Build vocab\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for ch in sorted(set(\"\".join(spr[\"train\"][\"sequence\"]))):\n        vocab[ch] = len(vocab)\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    # Datasets / loaders (shared across experiments)\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256)\n    test_loader = DataLoader(test_ds, batch_size=256)\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    crit = nn.CrossEntropyLoss()\n\n    weight_decays = [0.0, 1e-5, 1e-4, 5e-4, 1e-3]\n    epochs = 5\n\n    for wd in weight_decays:\n        print(f\"\\n=== Training with weight_decay={wd} ===\")\n        model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n        run_data = {\n            \"epochs\": [],\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for ep in range(1, epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, crit, opt)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, crit)\n            print(f\"  Ep {ep}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\")\n\n            run_data[\"epochs\"].append(ep)\n            run_data[\"losses\"][\"train\"].append(tr_loss)\n            run_data[\"losses\"][\"val\"].append(val_loss)\n            run_data[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            run_data[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        # final test evaluation\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, crit)\n        print(f\"  Test macro_F1={test_f1:.4f}\")\n        run_data[\"predictions\"] = preds\n        run_data[\"ground_truth\"] = gts\n\n        experiment_data[\"weight_decay\"][str(wd)] = run_data\n\n    # save all\n    np.save(\"experiment_data.npy\", experiment_data, allow_pickle=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 housekeeping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# container that will be saved to disk\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n        \"hyperparams\": {},\n    }\n}\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dataset utils \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for split in [\"train\", \"dev\", \"test\"]:\n        dset[split] = _load(f\"{split}.csv\")\n    return dset\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=4 * d_model, dropout=0.1\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.encoder(x.transpose(0, 1)).transpose(0, 1)\n        return self.cls(x.mean(1))\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 train / eval helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef move_batch(batch):\n    return {\n        k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()\n    }\n\n\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tl, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_batch(batch)\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tl += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(logits.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tl / len(loader.dataset), f1_score(gts, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tl, preds, gts = 0.0, [], []\n    for batch in loader:\n        batch = move_batch(batch)\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        tl += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(logits.argmax(1).cpu().numpy())\n        gts.extend(batch[\"labels\"].cpu().numpy())\n    return tl / len(loader.dataset), f1_score(gts, preds, average=\"macro\"), preds, gts\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 experiment runner \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef run_experiment():\n    # path to SPR_BENCH (adjust if necessary)\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"Dataset path {DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # build vocabulary\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\n        \"<pad>\": 0,\n        \"<unk>\": 1,\n        **{ch: i + 2 for i, ch in enumerate(sorted(chars))},\n    }\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    # torch datasets / loaders (batch size tuned later)\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    hyper_grid = [\n        {\"d_model\": 128, \"lr\": 1e-3, \"batch\": 128},\n        {\"d_model\": 256, \"lr\": 5e-4, \"batch\": 128},\n    ]\n\n    for hp in hyper_grid:\n        print(f\"\\n\u2500\u2500\u2500\u2500 Hyper-params: {hp} \u2500\u2500\u2500\u2500\")\n        experiment_data[\"SPR_BENCH\"][\"hyperparams\"][str(hp)] = {}\n\n        # loaders with chosen batch size\n        train_loader = DataLoader(train_ds, batch_size=hp[\"batch\"], shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=hp[\"batch\"] * 2)\n        test_loader = DataLoader(test_ds, batch_size=hp[\"batch\"] * 2)\n\n        model = SPRModel(\n            len(vocab), num_classes, d_model=hp[\"d_model\"], max_len=max_len\n        ).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=hp[\"lr\"])\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode=\"max\", factor=0.5, patience=2, verbose=False\n        )\n\n        best_val, patience, max_patience = 0.0, 0, 3\n        for epoch in range(1, 11):  # up to 10 epochs\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            vl_loss, vl_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            scheduler.step(vl_f1)\n\n            # record\n            experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n            experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(vl_loss)\n            experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train\"].append(tr_f1)\n            experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val\"].append(vl_f1)\n            experiment_data[\"SPR_BENCH\"][\"epochs\"].append(\n                {\"epoch\": epoch, \"time\": time.time(), \"hp\": hp}\n            )\n\n            print(\n                f\"Epoch {epoch:02d} | val_loss={vl_loss:.4f} | val_macro_f1={vl_f1:.4f}\"\n            )\n            if vl_f1 > best_val:\n                best_val, patience = vl_f1, 0\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(working_dir, f'best_model_{hp[\"d_model\"]}.pt'),\n                )\n            else:\n                patience += 1\n            if patience >= max_patience:\n                print(\"Early stopping triggered.\")\n                break\n\n        # load the best weights before final test\n        model.load_state_dict(\n            torch.load(os.path.join(working_dir, f'best_model_{hp[\"d_model\"]}.pt'))\n        )\n        te_loss, te_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"Test Macro-F1 (d_model={hp['d_model']}) = {te_f1:.4f}\")\n\n        # store test data\n        experiment_data[\"SPR_BENCH\"][\"predictions\"].append(preds)\n        experiment_data[\"SPR_BENCH\"][\"ground_truth\"].append(gts)\n        experiment_data[\"SPR_BENCH\"][\"hyperparams\"][str(hp)][\"test_macro_f1\"] = te_f1\n\n    # persist everything\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n    print(\"All experiment data saved.\")\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrun_experiment()\n", "import os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- paths / dirs --------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment container --------------------\nexperiment_data = {\"nhead_tuning\": {\"SPR_BENCH\": {}}}\n\n\n# -------------------- dataset loader --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    d = DatasetDict()\n    d[\"train\"] = _load(\"train.csv\")\n    d[\"dev\"] = _load(\"dev.csv\")\n    d[\"test\"] = _load(\"test.csv\")\n    return d\n\n\n# -------------------- torch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs = hf_ds[\"sequence\"]\n        self.labels = hf_ds[\"label\"]\n        self.vocab = vocab\n        self.pad_id = vocab[\"<pad>\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq, label = self.seqs[idx], self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        if len(ids) < self.max_len:\n            ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_sz, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_sz, d_model)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=256\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1), :]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n        return self.cls(x.mean(dim=1))\n\n\n# -------------------- train / eval utils --------------------\ndef train_epoch(model, loader, crit, opt):\n    model.train()\n    tot_loss, preds, trues = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        opt.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        loss.backward()\n        opt.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).detach().cpu().numpy())\n        trues.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(trues, preds, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, crit):\n    model.eval()\n    tot_loss, preds, trues = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        loss = crit(out, batch[\"labels\"])\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        preds.extend(out.argmax(1).cpu().numpy())\n        trues.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(trues, preds, average=\"macro\"),\n        preds,\n        trues,\n    )\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"Dataset path {DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    for ch in sorted(chars):\n        vocab[ch] = len(vocab)\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    # datasets / loaders\n    train_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\n    val_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\n    test_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256)\n    test_loader = DataLoader(test_ds, batch_size=256)\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    for nhead in [2, 4, 8]:\n        print(f\"\\n===== Training with nhead={nhead} =====\")\n        log = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n        model = SPRModel(\n            len(vocab), num_classes, d_model=128, nhead=nhead, max_len=max_len\n        ).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        epochs = 5\n        for ep in range(1, epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(f\"Epoch {ep}: val_loss={val_loss:.4f}, val_macro_f1={val_f1:.4f}\")\n            log[\"epochs\"].append(ep)\n            log[\"losses\"][\"train\"].append(tr_loss)\n            log[\"losses\"][\"val\"].append(val_loss)\n            log[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            log[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n        # final test\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"nhead={nhead} Test macro_F1={test_f1:.4f}\")\n        log[\"predictions\"] = preds\n        log[\"ground_truth\"] = gts\n        log[\"test_macro_f1\"] = test_f1\n        experiment_data[\"nhead_tuning\"][\"SPR_BENCH\"][f\"head_{nhead}\"] = log\n\n    # save\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os, pathlib, time, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# ---------------------- mandatory working dir ----------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ---------------------- device & reproducibility -------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# ---------------------- experiment container -----------------------\nexperiment_data = {\n    \"SPR_BENCH\": {\n        \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": [], \"test_macro_f1\": []},\n        \"losses\": {\"train\": [], \"val\": [], \"test\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"hyperparams\": {},\n    }\n}\n\n\n# ---------------------- data helpers -------------------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    \"\"\"Load the three SPR splits into a HuggingFace DatasetDict.\"\"\"\n\n    def _load(csv_name: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / csv_name),\n            split=\"train\",  # treat one CSV as one split\n            cache_dir=\".cache_dsets\",\n        )\n\n    return DatasetDict(\n        train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\n    )\n\n\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab: dict, max_len: int):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab = vocab\n        self.max_len = max_len\n        self.pad_id, self.unk_id = vocab[\"<pad>\"], vocab[\"<unk>\"]\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq = self.seqs[idx]\n        label = self.labels[idx]\n        ids = [self.vocab.get(ch, self.unk_id) for ch in seq[: self.max_len]]\n        if len(ids) < self.max_len:\n            ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# ---------------------- model --------------------------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        num_classes: int,\n        d_model: int = 128,\n        nhead: int = 4,\n        num_layers: int = 2,\n        max_len: int = 128,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = nn.Parameter(torch.randn(1, max_len, d_model))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=256,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        # x : (batch, seq_len)\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        h = self.encoder(x)  # (batch, seq_len, d_model)\n        pooled = h.mean(dim=1)  # simple mean-pool\n        return self.cls(pooled)\n\n\n# ---------------------- train / eval loops -------------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tot_loss, yp, yt = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        optimizer.zero_grad()\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        yp.extend(logits.argmax(dim=1).detach().cpu().numpy())\n        yt.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(yt, yp, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, yp, yt = 0.0, [], []\n    for batch in loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(logits, batch[\"labels\"])\n\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        yp.extend(logits.argmax(dim=1).cpu().numpy())\n        yt.extend(batch[\"labels\"].cpu().numpy())\n    return (tot_loss / len(loader.dataset), f1_score(yt, yp, average=\"macro\"), yp, yt)\n\n\n# ---------------------- locate dataset -----------------------------\npossible_roots = [\n    pathlib.Path(os.environ.get(\"SPR_DATA\", \"\")),\n    pathlib.Path(\"./SPR_BENCH\"),\n    pathlib.Path(\"../SPR_BENCH\"),\n]\nDATA_PATH = None\nfor p in possible_roots:\n    if p and p.exists():\n        DATA_PATH = p.resolve()\n        break\nif DATA_PATH is None:\n    raise FileNotFoundError(\n        \"Could not locate SPR_BENCH dataset. \"\n        \"Set SPR_DATA env variable or place folder in working dir.\"\n    )\nprint(f\"Dataset found at: {DATA_PATH}\")\n\n# ---------------------- preparation --------------------------------\nspr = load_spr_bench(DATA_PATH)\nall_chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\nvocab = {\n    \"<pad>\": 0,\n    \"<unk>\": 1,\n    **{ch: i + 2 for i, ch in enumerate(sorted(all_chars))},\n}\nmax_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\nnum_classes = len(set(spr[\"train\"][\"label\"]))\n\ntrain_ds = SPRTorchDataset(spr[\"train\"], vocab, max_len)\nval_ds = SPRTorchDataset(spr[\"dev\"], vocab, max_len)\ntest_ds = SPRTorchDataset(spr[\"test\"], vocab, max_len)\n\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, drop_last=False)\nval_loader = DataLoader(val_ds, batch_size=256)\ntest_loader = DataLoader(test_ds, batch_size=256)\n\n# ---------------------- hyper-parameter sweep ----------------------\nlearning_rates = [5e-4, 1e-3]\nepochs = 5\ncriterion = nn.CrossEntropyLoss()\n\nfor lr in learning_rates:\n    print(f\"\\n=== Learning-rate {lr} ===\")\n    model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    for ep in range(1, epochs + 1):\n        start = time.time()\n        tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n\n        # bookkeeping\n        experiment_data[\"SPR_BENCH\"][\"epochs\"].append(ep)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"SPR_BENCH\"][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n        experiment_data[\"SPR_BENCH\"][\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n        print(\n            f\"Epoch {ep:02d} \u2011 \"\n            f\"train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} | \"\n            f\"val_macro_f1={val_f1:.4f} | time={time.time()-start:.1f}s\"\n        )\n\n    # final test evaluation\n    test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n    print(f\"Final Test \u2011 loss={test_loss:.4f} | macro_f1={test_f1:.4f}\")\n\n    experiment_data[\"SPR_BENCH\"][\"losses\"][\"test\"].append(test_loss)\n    experiment_data[\"SPR_BENCH\"][\"metrics\"][\"test_macro_f1\"].append(test_f1)\n    experiment_data[\"SPR_BENCH\"][\"predictions\"].extend(preds)\n    experiment_data[\"SPR_BENCH\"][\"ground_truth\"].extend(gts)\n    experiment_data[\"SPR_BENCH\"][\"hyperparams\"][f\"lr_{lr}\"] = {\n        \"epochs\": epochs,\n        \"d_model\": 128,\n        \"nhead\": 4,\n        \"num_layers\": 2,\n        \"batch_size\": 128,\n    }\n\n# ---------------------- persist results ----------------------------\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nAll metrics saved to {working_dir}/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"num_epochs\": {}}  # hyper-parameter tuning type\n\n\n# -------------------- dataset loader utility --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        dset[s if s != \"dev\" else \"dev\"] = _load(f\"{s}.csv\")\n    return dset\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq, label = self.seqs[idx], self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed, self.pos = nn.Embedding(vocab_size, d_model), nn.Parameter(\n            torch.randn(1, max_len, d_model)\n        )\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256)\n        self.transformer, self.cls = nn.TransformerEncoder(\n            enc_layer, num_layers\n        ), nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1).mean(1)\n        return self.cls(x)\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(true, pred, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        tot_loss += criterion(out, batch[\"labels\"]).item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(true, pred, average=\"macro\"),\n        pred,\n        true,\n    )\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab & datasets ---------------------------------------------------------\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({ch: i + 2 for i, ch in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    train_loader = DataLoader(\n        SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    epoch_grid = [5, 10, 20, 30]  # hyper-parameter values\n    patience = 5\n\n    for max_epochs in epoch_grid:\n        key = f\"epochs_{max_epochs}\"\n        experiment_data[\"num_epochs\"][key] = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n\n        model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        best_val, wait, best_state = 0.0, 0, None\n        for epoch in range(1, max_epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(\n                f\"[{key}] Epoch {epoch}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\"\n            )\n\n            # store\n            ed = experiment_data[\"num_epochs\"][key]\n            ed[\"epochs\"].append(epoch)\n            ed[\"losses\"][\"train\"].append(tr_loss)\n            ed[\"losses\"][\"val\"].append(val_loss)\n            ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n            # early stopping\n            if val_f1 > best_val:\n                best_val, wait, best_state = val_f1, 0, model.state_dict()\n            else:\n                wait += 1\n            if wait >= patience:\n                print(\n                    f\"Early stopping at epoch {epoch} (best val_macro_f1={best_val:.4f})\"\n                )\n                break\n\n        # load best model and evaluate on test set\n        if best_state is not None:\n            model.load_state_dict(best_state)\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"[{key}] Test macro_f1={test_f1:.4f}\")\n        ed = experiment_data[\"num_epochs\"][key]\n        ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n        ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # -------------------- save all experiment data --------------------\n    working_dir = os.path.join(os.getcwd(), \"working\")\n    os.makedirs(working_dir, exist_ok=True)\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# Execute immediately\nmain()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"num_epochs\": {}}  # hyper-parameter tuning type\n\n\n# -------------------- dataset loader utility --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        dset[s if s != \"dev\" else \"dev\"] = _load(f\"{s}.csv\")\n    return dset\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq, label = self.seqs[idx], self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed, self.pos = nn.Embedding(vocab_size, d_model), nn.Parameter(\n            torch.randn(1, max_len, d_model)\n        )\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256)\n        self.transformer, self.cls = nn.TransformerEncoder(\n            enc_layer, num_layers\n        ), nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1).mean(1)\n        return self.cls(x)\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(true, pred, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        tot_loss += criterion(out, batch[\"labels\"]).item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(true, pred, average=\"macro\"),\n        pred,\n        true,\n    )\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab & datasets ---------------------------------------------------------\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({ch: i + 2 for i, ch in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    train_loader = DataLoader(\n        SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    epoch_grid = [5, 10, 20, 30]  # hyper-parameter values\n    patience = 5\n\n    for max_epochs in epoch_grid:\n        key = f\"epochs_{max_epochs}\"\n        experiment_data[\"num_epochs\"][key] = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n\n        model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        best_val, wait, best_state = 0.0, 0, None\n        for epoch in range(1, max_epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(\n                f\"[{key}] Epoch {epoch}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\"\n            )\n\n            # store\n            ed = experiment_data[\"num_epochs\"][key]\n            ed[\"epochs\"].append(epoch)\n            ed[\"losses\"][\"train\"].append(tr_loss)\n            ed[\"losses\"][\"val\"].append(val_loss)\n            ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n            # early stopping\n            if val_f1 > best_val:\n                best_val, wait, best_state = val_f1, 0, model.state_dict()\n            else:\n                wait += 1\n            if wait >= patience:\n                print(\n                    f\"Early stopping at epoch {epoch} (best val_macro_f1={best_val:.4f})\"\n                )\n                break\n\n        # load best model and evaluate on test set\n        if best_state is not None:\n            model.load_state_dict(best_state)\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"[{key}] Test macro_f1={test_f1:.4f}\")\n        ed = experiment_data[\"num_epochs\"][key]\n        ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n        ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # -------------------- save all experiment data --------------------\n    working_dir = os.path.join(os.getcwd(), \"working\")\n    os.makedirs(working_dir, exist_ok=True)\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# Execute immediately\nmain()\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, pathlib, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom datasets import load_dataset, DatasetDict\n\n# -------------------- device --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# -------------------- experiment data container --------------------\nexperiment_data = {\"num_epochs\": {}}  # hyper-parameter tuning type\n\n\n# -------------------- dataset loader utility --------------------\ndef load_spr_bench(root: pathlib.Path) -> DatasetDict:\n    def _load(split_csv: str):\n        return load_dataset(\n            \"csv\",\n            data_files=str(root / split_csv),\n            split=\"train\",\n            cache_dir=\".cache_dsets\",\n        )\n\n    dset = DatasetDict()\n    for s in [\"train\", \"dev\", \"test\"]:\n        dset[s if s != \"dev\" else \"dev\"] = _load(f\"{s}.csv\")\n    return dset\n\n\n# -------------------- PyTorch dataset --------------------\nclass SPRTorchDataset(Dataset):\n    def __init__(self, hf_ds, vocab, max_len):\n        self.seqs, self.labels = hf_ds[\"sequence\"], hf_ds[\"label\"]\n        self.vocab, self.pad_id, self.max_len = vocab, vocab[\"<pad>\"], max_len\n\n    def __len__(self):\n        return len(self.seqs)\n\n    def __getitem__(self, idx):\n        seq, label = self.seqs[idx], self.labels[idx]\n        ids = [self.vocab.get(ch, self.vocab[\"<unk>\"]) for ch in seq[: self.max_len]]\n        ids += [self.pad_id] * (self.max_len - len(ids))\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# -------------------- model --------------------\nclass SPRModel(nn.Module):\n    def __init__(\n        self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2, max_len=128\n    ):\n        super().__init__()\n        self.embed, self.pos = nn.Embedding(vocab_size, d_model), nn.Parameter(\n            torch.randn(1, max_len, d_model)\n        )\n        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256)\n        self.transformer, self.cls = nn.TransformerEncoder(\n            enc_layer, num_layers\n        ), nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x) + self.pos[:, : x.size(1)]\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1).mean(1)\n        return self.cls(x)\n\n\n# -------------------- training utils --------------------\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        out = model(batch[\"input_ids\"])\n        loss = criterion(out, batch[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        tot_loss += loss.item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return tot_loss / len(loader.dataset), f1_score(true, pred, average=\"macro\")\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion):\n    model.eval()\n    tot_loss, pred, true = 0.0, [], []\n    for batch in loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = model(batch[\"input_ids\"])\n        tot_loss += criterion(out, batch[\"labels\"]).item() * batch[\"labels\"].size(0)\n        pred.extend(out.argmax(1).cpu().numpy())\n        true.extend(batch[\"labels\"].cpu().numpy())\n    return (\n        tot_loss / len(loader.dataset),\n        f1_score(true, pred, average=\"macro\"),\n        pred,\n        true,\n    )\n\n\n# -------------------- main routine --------------------\ndef main():\n    DATA_PATH = pathlib.Path(\"/home/zxl240011/AI-Scientist-v2/SPR_BENCH/\")\n    if not DATA_PATH.exists():\n        raise FileNotFoundError(f\"{DATA_PATH} not found.\")\n    spr = load_spr_bench(DATA_PATH)\n\n    # vocab & datasets ---------------------------------------------------------\n    chars = set(\"\".join(spr[\"train\"][\"sequence\"]))\n    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n    vocab.update({ch: i + 2 for i, ch in enumerate(sorted(chars))})\n    max_len = min(128, max(len(s) for s in spr[\"train\"][\"sequence\"]))\n\n    train_loader = DataLoader(\n        SPRTorchDataset(spr[\"train\"], vocab, max_len), batch_size=128, shuffle=True\n    )\n    val_loader = DataLoader(SPRTorchDataset(spr[\"dev\"], vocab, max_len), batch_size=256)\n    test_loader = DataLoader(\n        SPRTorchDataset(spr[\"test\"], vocab, max_len), batch_size=256\n    )\n\n    num_classes = len(set(spr[\"train\"][\"label\"]))\n    criterion = nn.CrossEntropyLoss()\n\n    epoch_grid = [5, 10, 20, 30]  # hyper-parameter values\n    patience = 5\n\n    for max_epochs in epoch_grid:\n        key = f\"epochs_{max_epochs}\"\n        experiment_data[\"num_epochs\"][key] = {\n            \"metrics\": {\"train_macro_f1\": [], \"val_macro_f1\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": [],\n        }\n\n        model = SPRModel(len(vocab), num_classes, max_len=max_len).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        best_val, wait, best_state = 0.0, 0, None\n        for epoch in range(1, max_epochs + 1):\n            tr_loss, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n            val_loss, val_f1, _, _ = eval_epoch(model, val_loader, criterion)\n            print(\n                f\"[{key}] Epoch {epoch}: val_loss={val_loss:.4f}  val_macro_f1={val_f1:.4f}\"\n            )\n\n            # store\n            ed = experiment_data[\"num_epochs\"][key]\n            ed[\"epochs\"].append(epoch)\n            ed[\"losses\"][\"train\"].append(tr_loss)\n            ed[\"losses\"][\"val\"].append(val_loss)\n            ed[\"metrics\"][\"train_macro_f1\"].append(tr_f1)\n            ed[\"metrics\"][\"val_macro_f1\"].append(val_f1)\n\n            # early stopping\n            if val_f1 > best_val:\n                best_val, wait, best_state = val_f1, 0, model.state_dict()\n            else:\n                wait += 1\n            if wait >= patience:\n                print(\n                    f\"Early stopping at epoch {epoch} (best val_macro_f1={best_val:.4f})\"\n                )\n                break\n\n        # load best model and evaluate on test set\n        if best_state is not None:\n            model.load_state_dict(best_state)\n        test_loss, test_f1, preds, gts = eval_epoch(model, test_loader, criterion)\n        print(f\"[{key}] Test macro_f1={test_f1:.4f}\")\n        ed = experiment_data[\"num_epochs\"][key]\n        ed[\"predictions\"], ed[\"ground_truth\"] = preds, gts\n        ed[\"test_macro_f1\"], ed[\"test_loss\"] = test_f1, test_loss\n\n    # -------------------- save all experiment data --------------------\n    working_dir = os.path.join(os.getcwd(), \"working\")\n    os.makedirs(working_dir, exist_ok=True)\n    np.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n\n\n# Execute immediately\nmain()\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 51003.26\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 90660.21\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 178162.60\nexamples/s]', '\\n', 'Epoch 1: validation_loss = 0.5978, val_macro_f1 = 0.7161',\n'\\n', 'Epoch 2: validation_loss = 0.5617, val_macro_f1 = 0.7599', '\\n', 'Epoch\n3: validation_loss = 0.5421, val_macro_f1 = 0.7840', '\\n', 'Epoch 4:\nvalidation_loss = 0.5328, val_macro_f1 = 0.7920', '\\n', 'Epoch 5:\nvalidation_loss = 0.5362, val_macro_f1 = 0.7860', '\\n', 'Test macro_f1 =\n0.7949', '\\n', 'Execution time: 3 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 151156.98\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 139819.45\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 173547.83\nexamples/s]', '\\n', '[epochs_5] Epoch 1: val_loss=0.5975  val_macro_f1=0.7161',\n'\\n', '[epochs_5] Epoch 2: val_loss=0.5617  val_macro_f1=0.7599', '\\n',\n'[epochs_5] Epoch 3: val_loss=0.5430  val_macro_f1=0.7840', '\\n', '[epochs_5]\nEpoch 4: val_loss=0.5357  val_macro_f1=0.7940', '\\n', '[epochs_5] Epoch 5:\nval_loss=0.5381  val_macro_f1=0.7900', '\\n', '[epochs_5] Test macro_f1=0.7950',\n'\\n', '[epochs_10] Epoch 1: val_loss=0.5712  val_macro_f1=0.7393', '\\n',\n'[epochs_10] Epoch 2: val_loss=0.5364  val_macro_f1=0.7880', '\\n', '[epochs_10]\nEpoch 3: val_loss=0.5423  val_macro_f1=0.7840', '\\n', '[epochs_10] Epoch 4:\nval_loss=0.5297  val_macro_f1=0.7900', '\\n', '[epochs_10] Epoch 5:\nval_loss=0.5344  val_macro_f1=0.7820', '\\n', '[epochs_10] Epoch 6:\nval_loss=0.5417  val_macro_f1=0.7959', '\\n', '[epochs_10] Epoch 7:\nval_loss=0.5261  val_macro_f1=0.7959', '\\n', '[epochs_10] Epoch 8:\nval_loss=0.5536  val_macro_f1=0.7740', '\\n', '[epochs_10] Epoch 9:\nval_loss=0.5416  val_macro_f1=0.7900', '\\n', '[epochs_10] Epoch 10:\nval_loss=0.5447  val_macro_f1=0.7795', '\\n', '[epochs_10] Test macro_f1=0.7860',\n'\\n', '[epochs_20] Epoch 1: val_loss=0.6247  val_macro_f1=0.7157', '\\n',\n'[epochs_20] Epoch 2: val_loss=0.5415  val_macro_f1=0.7740', '\\n', '[epochs_20]\nEpoch 3: val_loss=0.5560  val_macro_f1=0.7680', '\\n', '[epochs_20] Epoch 4:\nval_loss=0.5615  val_macro_f1=0.7679', '\\n', '[epochs_20] Epoch 5:\nval_loss=0.5390  val_macro_f1=0.7860', '\\n', '[epochs_20] Epoch 6:\nval_loss=0.5692  val_macro_f1=0.7740', '\\n', '[epochs_20] Epoch 7:\nval_loss=0.5487  val_macro_f1=0.7719', '\\n', '[epochs_20] Epoch 8:\nval_loss=0.5505  val_macro_f1=0.7779', '\\n', '[epochs_20] Epoch 9:\nval_loss=0.5619  val_macro_f1=0.7780', '\\n', '[epochs_20] Epoch 10:\nval_loss=0.5881  val_macro_f1=0.7780', '\\n', 'Early stopping at epoch 10 (best\nval_macro_f1=0.7860)', '\\n', '[epochs_20] Test macro_f1=0.7828', '\\n',\n'[epochs_30] Epoch 1: val_loss=0.5923  val_macro_f1=0.7308', '\\n', '[epochs_30]\nEpoch 2: val_loss=0.5452  val_macro_f1=0.7780', '\\n', '[epochs_30] Epoch 3:\nval_loss=0.5773  val_macro_f1=0.7476', '\\n', '[epochs_30] Epoch 4:\nval_loss=0.5428  val_macro_f1=0.7840', '\\n', '[epochs_30] Epoch 5:\nval_loss=0.5350  val_macro_f1=0.7858', '\\n', '[epochs_30] Epoch 6:\nval_loss=0.5374  val_macro_f1=0.7774', '\\n', '[epochs_30] Epoch 7:\nval_loss=0.5324  val_macro_f1=0.7940', '\\n', '[epochs_30] Epoch 8:\nval_loss=0.5338  val_macro_f1=0.7959', '\\n', '[epochs_30] Epoch 9:\nval_loss=0.5446  val_macro_f1=0.7716', '\\n', '[epochs_30] Epoch 10:\nval_loss=0.5477  val_macro_f1=0.7920', '\\n', '[epochs_30] Epoch 11:\nval_loss=0.5506  val_macro_f1=0.7920', '\\n', '[epochs_30] Epoch 12:\nval_loss=0.5478  val_macro_f1=0.7674', '\\n', '[epochs_30] Epoch 13:\nval_loss=0.5933  val_macro_f1=0.7799', '\\n', 'Early stopping at epoch 13 (best\nval_macro_f1=0.7959)', '\\n', '[epochs_30] Test macro_f1=0.8000', '\\n',\n'Execution time: 8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device:', ' ', 'cuda', '\\n', 'Execution time: a second seconds (time\nlimit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 116977.98\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 91646.72\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 143067.30\nexamples/s]', '\\n', \"\\n\u2500\u2500\u2500\u2500 Hyper-params: {'d_model': 128, 'lr': 0.001, 'batch':\n128} \u2500\u2500\u2500\u2500\", '\\n', 'Epoch 01 | val_loss=0.5995 | val_macro_f1=0.7558', '\\n',\n'Epoch 02 | val_loss=0.5379 | val_macro_f1=0.7860', '\\n', 'Epoch 03 |\nval_loss=0.5537 | val_macro_f1=0.7820', '\\n', 'Epoch 04 | val_loss=0.5574 |\nval_macro_f1=0.7720', '\\n', 'Epoch 05 | val_loss=0.5793 | val_macro_f1=0.7457',\n'\\n', 'Early stopping triggered.', '\\n', 'Test Macro-F1 (d_model=128) = 0.7909',\n'\\n', \"\\n\u2500\u2500\u2500\u2500 Hyper-params: {'d_model': 256, 'lr': 0.0005, 'batch': 128} \u2500\u2500\u2500\u2500\",\n'\\n', 'Epoch 01 | val_loss=0.6615 | val_macro_f1=0.3421', '\\n', 'Epoch 02 |\nval_loss=0.5432 | val_macro_f1=0.7800', '\\n', 'Epoch 03 | val_loss=0.5429 |\nval_macro_f1=0.7700', '\\n', 'Epoch 04 | val_loss=0.5384 | val_macro_f1=0.7900',\n'\\n', 'Epoch 05 | val_loss=0.5430 | val_macro_f1=0.7800', '\\n', 'Epoch 06 |\nval_loss=0.5490 | val_macro_f1=0.7919', '\\n', 'Epoch 07 | val_loss=0.5671 |\nval_macro_f1=0.7800', '\\n', 'Epoch 08 | val_loss=0.5332 | val_macro_f1=0.7899',\n'\\n', 'Epoch 09 | val_loss=0.5437 | val_macro_f1=0.7959', '\\n', 'Epoch 10 |\nval_loss=0.5484 | val_macro_f1=0.7859', '\\n', 'Test Macro-F1 (d_model=256) =\n0.7940', '\\n', 'All experiment data saved.', '\\n', 'Execution time: 8 seconds\nseconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', 'Execution time: a moment seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', 'Dataset found at: /home/zxl240011/AI-Scientist-\nv2/experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/0-\nrun/process_ForkProcess-6', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 155, in <module>\\n    spr = load_spr_bench(DATA_PATH)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 42, in load_spr_bench\\n\ntrain=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")\\n\n^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 34, in _load\\n    return\nload_dataset(\\n           ^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1497, in dataset_module_factory\\n\n).get_module()\\n      ^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 913, in get_module\\n    data_files =\nDataFilesDict.from_patterns(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 690, in from_patterns\\n    else\nDataFilesList.from_patterns(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 583, in from_patterns\\n\nresolve_pattern(\\n  File\n\"/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/data_files.py\", line 384, in resolve_pattern\\n    raise\nFileNotFoundError(error_msg)\\nFileNotFoundError: Unable to find\n\\'/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-\n6/train.csv\\'\\n', 'Execution time: a second seconds (time limit is 30\nminutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 178804.39\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 76999.27\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 179251.42\nexamples/s]', '\\n', '[epochs_5] Epoch 1: val_loss=0.6371  val_macro_f1=0.5974',\n'\\n', '[epochs_5] Epoch 2: val_loss=0.5416  val_macro_f1=0.7800', '\\n',\n'[epochs_5] Epoch 3: val_loss=0.5513  val_macro_f1=0.7660', '\\n', '[epochs_5]\nEpoch 4: val_loss=0.5305  val_macro_f1=0.7940', '\\n', '[epochs_5] Epoch 5:\nval_loss=0.5400  val_macro_f1=0.7820', '\\n', '[epochs_5] Test macro_f1=0.7970',\n'\\n', '[epochs_10] Epoch 1: val_loss=0.5803  val_macro_f1=0.7435', '\\n',\n'[epochs_10] Epoch 2: val_loss=0.5445  val_macro_f1=0.7919', '\\n', '[epochs_10]\nEpoch 3: val_loss=0.5382  val_macro_f1=0.7800', '\\n', '[epochs_10] Epoch 4:\nval_loss=0.5846  val_macro_f1=0.7411', '\\n', '[epochs_10] Epoch 5:\nval_loss=0.5331  val_macro_f1=0.7838', '\\n', '[epochs_10] Epoch 6:\nval_loss=0.5492  val_macro_f1=0.7720', '\\n', '[epochs_10] Epoch 7:\nval_loss=0.5274  val_macro_f1=0.7880', '\\n', 'Early stopping at epoch 7 (best\nval_macro_f1=0.7919)', '\\n', '[epochs_10] Test macro_f1=0.8000', '\\n',\n'[epochs_20] Epoch 1: val_loss=0.6226  val_macro_f1=0.6450', '\\n', '[epochs_20]\nEpoch 2: val_loss=0.5357  val_macro_f1=0.7819', '\\n', '[epochs_20] Epoch 3:\nval_loss=0.5371  val_macro_f1=0.7840', '\\n', '[epochs_20] Epoch 4:\nval_loss=0.5535  val_macro_f1=0.7639', '\\n', '[epochs_20] Epoch 5:\nval_loss=0.5343  val_macro_f1=0.7800', '\\n', '[epochs_20] Epoch 6:\nval_loss=0.5342  val_macro_f1=0.7820', '\\n', '[epochs_20] Epoch 7:\nval_loss=0.5409  val_macro_f1=0.7820', '\\n', '[epochs_20] Epoch 8:\nval_loss=0.5320  val_macro_f1=0.7899', '\\n', '[epochs_20] Epoch 9:\nval_loss=0.5255  val_macro_f1=0.7899', '\\n', '[epochs_20] Epoch 10:\nval_loss=0.5313  val_macro_f1=0.7837', '\\n', '[epochs_20] Epoch 11:\nval_loss=0.5364  val_macro_f1=0.7899', '\\n', '[epochs_20] Epoch 12:\nval_loss=0.5575  val_macro_f1=0.7839', '\\n', '[epochs_20] Epoch 13:\nval_loss=0.5546  val_macro_f1=0.7839', '\\n', 'Early stopping at epoch 13 (best\nval_macro_f1=0.7899)', '\\n', '[epochs_20] Test macro_f1=0.7950', '\\n',\n'[epochs_30] Epoch 1: val_loss=0.6466  val_macro_f1=0.5513', '\\n', '[epochs_30]\nEpoch 2: val_loss=0.5659  val_macro_f1=0.7714', '\\n', '[epochs_30] Epoch 3:\nval_loss=0.5459  val_macro_f1=0.7820', '\\n', '[epochs_30] Epoch 4:\nval_loss=0.5575  val_macro_f1=0.7760', '\\n', '[epochs_30] Epoch 5:\nval_loss=0.5431  val_macro_f1=0.7840', '\\n', '[epochs_30] Epoch 6:\nval_loss=0.5458  val_macro_f1=0.7839', '\\n', '[epochs_30] Epoch 7:\nval_loss=0.5440  val_macro_f1=0.7820', '\\n', '[epochs_30] Epoch 8:\nval_loss=0.5433  val_macro_f1=0.7820', '\\n', '[epochs_30] Epoch 9:\nval_loss=0.5371  val_macro_f1=0.7714', '\\n', '[epochs_30] Epoch 10:\nval_loss=0.5498  val_macro_f1=0.7753', '\\n', 'Early stopping at epoch 10 (best\nval_macro_f1=0.7840)', '\\n', '[epochs_30] Test macro_f1=0.7920', '\\n',\n'Execution time: 8 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 2000 examples [00:00, 171398.96\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 500 examples [00:00, 139336.39\nexamples/s]', '\\n', '\\rGenerating train split: 0 examples [00:00, ?\nexamples/s]', '', '\\rGenerating train split: 1000 examples [00:00, 223374.55\nexamples/s]', '\\n', '[epochs_5] Epoch 1: val_loss=0.5546  val_macro_f1=0.7700',\n'\\n', '[epochs_5] Epoch 2: val_loss=0.5622  val_macro_f1=0.7517', '\\n',\n'[epochs_5] Epoch 3: val_loss=0.5431  val_macro_f1=0.7959', '\\n', '[epochs_5]\nEpoch 4: val_loss=0.5431  val_macro_f1=0.7760', '\\n', '[epochs_5] Epoch 5:\nval_loss=0.5383  val_macro_f1=0.7820', '\\n', '[epochs_5] Test macro_f1=0.7939',\n'\\n', '[epochs_10] Epoch 1: val_loss=0.6054  val_macro_f1=0.7700', '\\n',\n'[epochs_10] Epoch 2: val_loss=0.5838  val_macro_f1=0.7476', '\\n', '[epochs_10]\nEpoch 3: val_loss=0.5418  val_macro_f1=0.7820', '\\n', '[epochs_10] Epoch 4:\nval_loss=0.5493  val_macro_f1=0.7860', '\\n', '[epochs_10] Epoch 5:\nval_loss=0.5461  val_macro_f1=0.7780', '\\n', '[epochs_10] Epoch 6:\nval_loss=0.5448  val_macro_f1=0.7840', '\\n', '[epochs_10] Epoch 7:\nval_loss=0.5514  val_macro_f1=0.7819', '\\n', '[epochs_10] Epoch 8:\nval_loss=0.5763  val_macro_f1=0.7488', '\\n', '[epochs_10] Epoch 9:\nval_loss=0.5925  val_macro_f1=0.7840', '\\n', 'Early stopping at epoch 9 (best\nval_macro_f1=0.7860)', '\\n', '[epochs_10] Test macro_f1=0.7859', '\\n',\n'[epochs_20] Epoch 1: val_loss=0.5930  val_macro_f1=0.7497', '\\n', '[epochs_20]\nEpoch 2: val_loss=0.5505  val_macro_f1=0.7599', '\\n', '[epochs_20] Epoch 3:\nval_loss=0.5563  val_macro_f1=0.7660', '\\n', '[epochs_20] Epoch 4:\nval_loss=0.5375  val_macro_f1=0.7920', '\\n', '[epochs_20] Epoch 5:\nval_loss=0.5563  val_macro_f1=0.7700', '\\n', '[epochs_20] Epoch 6:\nval_loss=0.5440  val_macro_f1=0.7752', '\\n', '[epochs_20] Epoch 7:\nval_loss=0.5367  val_macro_f1=0.7919', '\\n', '[epochs_20] Epoch 8:\nval_loss=0.5419  val_macro_f1=0.7858', '\\n', '[epochs_20] Epoch 9:\nval_loss=0.5625  val_macro_f1=0.7800', '\\n', 'Early stopping at epoch 9 (best\nval_macro_f1=0.7920)', '\\n', '[epochs_20] Test macro_f1=0.7919', '\\n',\n'[epochs_30] Epoch 1: val_loss=0.5530  val_macro_f1=0.7840', '\\n', '[epochs_30]\nEpoch 2: val_loss=0.5472  val_macro_f1=0.7859', '\\n', '[epochs_30] Epoch 3:\nval_loss=0.5698  val_macro_f1=0.7700', '\\n', '[epochs_30] Epoch 4:\nval_loss=0.5677  val_macro_f1=0.7680', '\\n', '[epochs_30] Epoch 5:\nval_loss=0.5424  val_macro_f1=0.7840', '\\n', '[epochs_30] Epoch 6:\nval_loss=0.5564  val_macro_f1=0.7659', '\\n', '[epochs_30] Epoch 7:\nval_loss=0.5652  val_macro_f1=0.7338', '\\n', 'Early stopping at epoch 7 (best\nval_macro_f1=0.7859)', '\\n', '[epochs_30] Test macro_f1=0.7554', '\\n',\n'Execution time: 7 seconds seconds (time limit is 30 minutes).']", "['Using device: cuda', '\\n', '[epochs_5] Epoch 1: val_loss=0.5598\nval_macro_f1=0.7820', '\\n', '[epochs_5] Epoch 2: val_loss=0.5496\nval_macro_f1=0.7639', '\\n', '[epochs_5] Epoch 3: val_loss=0.5451\nval_macro_f1=0.7740', '\\n', '[epochs_5] Epoch 4: val_loss=0.5329\nval_macro_f1=0.7879', '\\n', '[epochs_5] Epoch 5: val_loss=0.5704\nval_macro_f1=0.7780', '\\n', '[epochs_5] Test macro_f1=0.7929', '\\n',\n'[epochs_10] Epoch 1: val_loss=0.5756  val_macro_f1=0.7920', '\\n', '[epochs_10]\nEpoch 2: val_loss=0.5296  val_macro_f1=0.7920', '\\n', '[epochs_10] Epoch 3:\nval_loss=0.5438  val_macro_f1=0.7760', '\\n', '[epochs_10] Epoch 4:\nval_loss=0.5271  val_macro_f1=0.7880', '\\n', '[epochs_10] Epoch 5:\nval_loss=0.5296  val_macro_f1=0.7940', '\\n', '[epochs_10] Epoch 6:\nval_loss=0.5328  val_macro_f1=0.7959', '\\n', '[epochs_10] Epoch 7:\nval_loss=0.5582  val_macro_f1=0.7719', '\\n', '[epochs_10] Epoch 8:\nval_loss=0.5388  val_macro_f1=0.7979', '\\n', '[epochs_10] Epoch 9:\nval_loss=0.5384  val_macro_f1=0.7898', '\\n', '[epochs_10] Epoch 10:\nval_loss=0.5415  val_macro_f1=0.7920', '\\n', '[epochs_10] Test macro_f1=0.8000',\n'\\n', '[epochs_20] Epoch 1: val_loss=0.6225  val_macro_f1=0.7136', '\\n',\n'[epochs_20] Epoch 2: val_loss=0.5571  val_macro_f1=0.7760', '\\n', '[epochs_20]\nEpoch 3: val_loss=0.5493  val_macro_f1=0.7800', '\\n', '[epochs_20] Epoch 4:\nval_loss=0.5520  val_macro_f1=0.7760', '\\n', '[epochs_20] Epoch 5:\nval_loss=0.5592  val_macro_f1=0.7614', '\\n', '[epochs_20] Epoch 6:\nval_loss=0.5590  val_macro_f1=0.7740', '\\n', '[epochs_20] Epoch 7:\nval_loss=0.5534  val_macro_f1=0.7693', '\\n', '[epochs_20] Epoch 8:\nval_loss=0.5633  val_macro_f1=0.7780', '\\n', 'Early stopping at epoch 8 (best\nval_macro_f1=0.7800)', '\\n', '[epochs_20] Test macro_f1=0.7909', '\\n',\n'[epochs_30] Epoch 1: val_loss=0.6298  val_macro_f1=0.6034', '\\n', '[epochs_30]\nEpoch 2: val_loss=0.5586  val_macro_f1=0.7858', '\\n', '[epochs_30] Epoch 3:\nval_loss=0.5349  val_macro_f1=0.7840', '\\n', '[epochs_30] Epoch 4:\nval_loss=0.5387  val_macro_f1=0.7860', '\\n', '[epochs_30] Epoch 5:\nval_loss=0.5384  val_macro_f1=0.7778', '\\n', '[epochs_30] Epoch 6:\nval_loss=0.5438  val_macro_f1=0.7720', '\\n', '[epochs_30] Epoch 7:\nval_loss=0.5290  val_macro_f1=0.7918', '\\n', '[epochs_30] Epoch 8:\nval_loss=0.5251  val_macro_f1=0.7899', '\\n', '[epochs_30] Epoch 9:\nval_loss=0.5346  val_macro_f1=0.7838', '\\n', '[epochs_30] Epoch 10:\nval_loss=0.5324  val_macro_f1=0.7858', '\\n', '[epochs_30] Epoch 11:\nval_loss=0.5527  val_macro_f1=0.7736', '\\n', '[epochs_30] Epoch 12:\nval_loss=0.5394  val_macro_f1=0.7878', '\\n', 'Early stopping at epoch 12 (best\nval_macro_f1=0.7918)', '\\n', '[epochs_30] Test macro_f1=0.7990', '\\n',\n'Execution time: 9 seconds seconds (time limit is 30 minutes).']", ""], "analysis": ["", "", "The execution output does not provide any meaningful information about the\ntraining process. The only message shown is 'Using device: cuda', which\nindicates the device being used, but there is no information about the training\nprogress, validation results, or final test performance. This could indicate\nthat the script terminated prematurely or failed to output logs correctly.\nProposed Fix: 1. Ensure that print statements within the training loop and\nevaluation sections are correctly executed and flushed to the output. 2. Verify\nif there are any hidden exceptions causing the script to terminate without\ncompleting the training process. 3. Check if the dataset path\n'/home/zxl240011/AI-Scientist-v2/SPR_BENCH/' is accessible and contains the\nrequired files. 4. Add more detailed logging to capture intermediate states and\npotential issues during execution.", "The output log does not indicate the completion of the training or evaluation\nprocess. It only shows the device being used and execution time, but no further\nlogs about training progress, validation, or test results. This suggests that\nthe script may not have executed properly or there is an issue with logging.\nProposed Fix: Ensure that the main function and training loop are properly\nexecuted. Add more logging statements to confirm the progress of training,\nvalidation, and testing. Additionally, verify that the dataset path exists and\nis correctly loaded, as this could halt execution silently if it fails.", "The output log does not provide any meaningful information about the execution\nof the training script. It only mentions the device being used (CUDA) and the\nexecution time, but no details about training progress, validation losses, F1\nscores, or test results are displayed. This makes it impossible to assess\nwhether the code ran successfully or achieved the desired goals. To fix this,\nensure that the script outputs key metrics and progress logs during execution,\nsuch as epoch-wise training and validation losses, F1 scores, and final test\nperformance. This will allow proper evaluation of the model's performance and\ndebugging if needed.", "The execution output lacks any meaningful logs or results from the training\nprocess. It only indicates the device being used (CUDA) and an execution time\nplaceholder. This suggests that the training script either did not run properly\nor there was an issue in capturing the output logs. To address this, ensure that\nthe training loop and evaluation results are properly logged and printed during\nexecution, and verify that the script runs without errors in the training and\nevaluation phases.", "The execution output indicates that the training script ran successfully without\nany bugs. The model was trained and evaluated with two different sets of\nhyperparameters. Both configurations achieved reasonable results, with the best\ntest Macro-F1 score being 0.7940. Early stopping was appropriately triggered in\none configuration to prevent overfitting. The experiment data was saved\ncorrectly, and the execution time was well within the limit. There are no issues\nrequiring fixes.", "The execution output lacks detailed logs, such as training progress, validation\nloss, and F1 scores per epoch. This indicates that the script might not have\nbeen executed properly or the logs were not captured. To fix this, ensure that\nthe script is correctly executed and that the output logs are captured and\ndisplayed. If the script was executed correctly, verify that the logging\nstatements (e.g., print statements) are functioning as expected. Additionally,\nconfirm that the dataset path is correct and accessible.", "The execution failed due to a FileNotFoundError. The script attempted to locate\nthe dataset at '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-\n27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-6/train.csv',\nbut the file was not found. To resolve this issue, ensure that the dataset files\n(train.csv, dev.csv, test.csv) are correctly placed in the specified directory\nor update the DATA_PATH variable to point to the correct location of the dataset\nfiles.", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, "FileNotFoundError", null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, {"args": ["Unable to find '/home/zxl240011/AI-Scientist-v2/experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/0-run/process_ForkProcess-6/train.csv'"]}, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, [["/home/zxl240011/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 155, "<module>", "spr = load_spr_bench(DATA_PATH)"], ["runfile.py", 42, "load_spr_bench", "train=_load(\"train.csv\"), dev=_load(\"dev.csv\"), test=_load(\"test.csv\")"], ["runfile.py", 34, "_load", "return load_dataset("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1497, "dataset_module_factory", ").get_module()"], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 913, "get_module", "data_files = DataFilesDict.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 690, "from_patterns", "else DataFilesList.from_patterns("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 583, "from_patterns", "resolve_pattern("], ["/home/zxl240011/anaconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/data_files.py", 384, "resolve_pattern", "raise FileNotFoundError(error_msg)"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7955, "best_value": 0.7955}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro-averaged F1 score achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value achieved during training.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5092, "best_value": 0.5092}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value achieved during validation.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5328, "best_value": 0.5328}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.796, "best_value": 0.796}, {"dataset_name": "epochs_10", "final_value": 0.7995, "best_value": 0.7995}, {"dataset_name": "epochs_20", "final_value": 0.8, "best_value": 0.8}, {"dataset_name": "epochs_30", "final_value": 0.8054, "best_value": 0.8054}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.794, "best_value": 0.794}, {"dataset_name": "epochs_10", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "epochs_20", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "epochs_30", "final_value": 0.7959, "best_value": 0.7959}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "epochs_10", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "epochs_20", "final_value": 0.7828, "best_value": 0.7828}, {"dataset_name": "epochs_30", "final_value": 0.8, "best_value": 0.8}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value for the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5079, "best_value": 0.5079}, {"dataset_name": "epochs_10", "final_value": 0.4901, "best_value": 0.4901}, {"dataset_name": "epochs_20", "final_value": 0.4865, "best_value": 0.4865}, {"dataset_name": "epochs_30", "final_value": 0.4716, "best_value": 0.4716}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5381, "best_value": 0.5381}, {"dataset_name": "epochs_10", "final_value": 0.5447, "best_value": 0.5447}, {"dataset_name": "epochs_20", "final_value": 0.5881, "best_value": 0.5881}, {"dataset_name": "epochs_30", "final_value": 0.5933, "best_value": 0.5933}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value for the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5106, "best_value": 0.5106}, {"dataset_name": "epochs_10", "final_value": 0.532, "best_value": 0.532}, {"dataset_name": "epochs_20", "final_value": 0.5521, "best_value": 0.5521}, {"dataset_name": "epochs_30", "final_value": 0.5388, "best_value": 0.5388}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "The loss calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.4982, "best_value": 0.4982}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.5484, "best_value": 0.5484}]}, {"metric_name": "training macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the training dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.796, "best_value": 0.796}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the validation dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.7859, "best_value": 0.7859}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score calculated on the test dataset.", "data": [{"dataset_name": "SPR_BENCH", "final_value": 0.794, "best_value": 0.794}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.7945, "best_value": 0.7945}, {"dataset_name": "epochs_10", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "epochs_20", "final_value": 0.8045, "best_value": 0.8045}, {"dataset_name": "epochs_30", "final_value": 0.802, "best_value": 0.802}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.794, "best_value": 0.794}, {"dataset_name": "epochs_10", "final_value": 0.7919, "best_value": 0.7919}, {"dataset_name": "epochs_20", "final_value": 0.7899, "best_value": 0.7899}, {"dataset_name": "epochs_30", "final_value": 0.784, "best_value": 0.784}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.797, "best_value": 0.797}, {"dataset_name": "epochs_10", "final_value": 0.8, "best_value": 0.8}, {"dataset_name": "epochs_20", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "epochs_30", "final_value": 0.792, "best_value": 0.792}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value for the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.4993, "best_value": 0.4993}, {"dataset_name": "epochs_10", "final_value": 0.5002, "best_value": 0.5002}, {"dataset_name": "epochs_20", "final_value": 0.4675, "best_value": 0.4675}, {"dataset_name": "epochs_30", "final_value": 0.4803, "best_value": 0.4803}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.54, "best_value": 0.54}, {"dataset_name": "epochs_10", "final_value": 0.5274, "best_value": 0.5274}, {"dataset_name": "epochs_20", "final_value": 0.5546, "best_value": 0.5546}, {"dataset_name": "epochs_30", "final_value": 0.5498, "best_value": 0.5498}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value for the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5127, "best_value": 0.5127}, {"dataset_name": "epochs_10", "final_value": 0.5069, "best_value": 0.5069}, {"dataset_name": "epochs_20", "final_value": 0.5328, "best_value": 0.5328}, {"dataset_name": "epochs_30", "final_value": 0.5289, "best_value": 0.5289}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The F1 score of the training dataset, which measures the harmonic mean of precision and recall.", "data": [{"dataset_name": "epochs_5", "final_value": 0.794, "best_value": 0.794}, {"dataset_name": "epochs_10", "final_value": 0.7955, "best_value": 0.7955}, {"dataset_name": "epochs_20", "final_value": 0.797, "best_value": 0.797}, {"dataset_name": "epochs_30", "final_value": 0.7924, "best_value": 0.7924}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The F1 score of the validation dataset, which measures the harmonic mean of precision and recall.", "data": [{"dataset_name": "epochs_5", "final_value": 0.7959, "best_value": 0.7959}, {"dataset_name": "epochs_10", "final_value": 0.786, "best_value": 0.786}, {"dataset_name": "epochs_20", "final_value": 0.792, "best_value": 0.792}, {"dataset_name": "epochs_30", "final_value": 0.7859, "best_value": 0.7859}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The F1 score of the test dataset, which measures the harmonic mean of precision and recall.", "data": [{"dataset_name": "epochs_5", "final_value": 0.7939, "best_value": 0.7939}, {"dataset_name": "epochs_10", "final_value": 0.7859, "best_value": 0.7859}, {"dataset_name": "epochs_20", "final_value": 0.7919, "best_value": 0.7919}, {"dataset_name": "epochs_30", "final_value": 0.7554, "best_value": 0.7554}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value of the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5067, "best_value": 0.5067}, {"dataset_name": "epochs_10", "final_value": 0.4997, "best_value": 0.4997}, {"dataset_name": "epochs_20", "final_value": 0.5096, "best_value": 0.5096}, {"dataset_name": "epochs_30", "final_value": 0.5204, "best_value": 0.5204}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value of the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5383, "best_value": 0.5383}, {"dataset_name": "epochs_10", "final_value": 0.5925, "best_value": 0.5925}, {"dataset_name": "epochs_20", "final_value": 0.5625, "best_value": 0.5625}, {"dataset_name": "epochs_30", "final_value": 0.5652, "best_value": 0.5652}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value of the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5147, "best_value": 0.5147}, {"dataset_name": "epochs_10", "final_value": 0.5618, "best_value": 0.5618}, {"dataset_name": "epochs_20", "final_value": 0.5325, "best_value": 0.5325}, {"dataset_name": "epochs_30", "final_value": 0.5591, "best_value": 0.5591}]}]}, {"metric_names": [{"metric_name": "train macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.7965, "best_value": 0.7965}, {"dataset_name": "epochs_10", "final_value": 0.7985, "best_value": 0.7985}, {"dataset_name": "epochs_20", "final_value": 0.792, "best_value": 0.792}, {"dataset_name": "epochs_30", "final_value": 0.805, "best_value": 0.805}]}, {"metric_name": "validation macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.7879, "best_value": 0.7879}, {"dataset_name": "epochs_10", "final_value": 0.7979, "best_value": 0.7979}, {"dataset_name": "epochs_20", "final_value": 0.78, "best_value": 0.78}, {"dataset_name": "epochs_30", "final_value": 0.7918, "best_value": 0.7918}]}, {"metric_name": "test macro F1 score", "lower_is_better": false, "description": "The macro F1 score for the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.7929, "best_value": 0.7929}, {"dataset_name": "epochs_10", "final_value": 0.8, "best_value": 0.8}, {"dataset_name": "epochs_20", "final_value": 0.7909, "best_value": 0.7909}, {"dataset_name": "epochs_30", "final_value": 0.799, "best_value": 0.799}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "The loss value for the training dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5162, "best_value": 0.5162}, {"dataset_name": "epochs_10", "final_value": 0.4947, "best_value": 0.4947}, {"dataset_name": "epochs_20", "final_value": 0.5087, "best_value": 0.5087}, {"dataset_name": "epochs_30", "final_value": 0.4799, "best_value": 0.4799}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "The loss value for the validation dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5704, "best_value": 0.5704}, {"dataset_name": "epochs_10", "final_value": 0.5415, "best_value": 0.5415}, {"dataset_name": "epochs_20", "final_value": 0.5633, "best_value": 0.5633}, {"dataset_name": "epochs_30", "final_value": 0.5394, "best_value": 0.5394}]}, {"metric_name": "test loss", "lower_is_better": true, "description": "The loss value for the test dataset.", "data": [{"dataset_name": "epochs_5", "final_value": 0.5286, "best_value": 0.5286}, {"dataset_name": "epochs_10", "final_value": 0.5134, "best_value": 0.5134}, {"dataset_name": "epochs_20", "final_value": 0.5343, "best_value": 0.5343}, {"dataset_name": "epochs_30", "final_value": 0.5139, "best_value": 0.5139}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_confusion_matrix.png"], ["../../logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_test_macro_f1_bar.png"], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_loss_curve.png", "../../logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_f1_curve.png", "../../logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_test_f1_bar.png", "../../logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_confusion_matrix.png"], [], [], ["../../logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_test_macro_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_test_macro_f1_bar.png"], ["../../logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_macro_f1_curves.png", "../../logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_test_macro_f1_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_09f49e5001b84cc1abb77f119aaf3a70/spr_bench_test_macro_f1_bar_agg.png"]], "plot_paths": [["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_confusion_matrix.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_test_macro_f1_bar.png"], [], [], [], [], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_loss_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_f1_curve.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_test_f1_bar.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_confusion_matrix.png"], [], [], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_test_macro_f1_bar.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_test_macro_f1_bar.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_macro_f1_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_loss_curves.png", "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_test_macro_f1_bar.png"], ["experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/seed_aggregation_09f49e5001b84cc1abb77f119aaf3a70/spr_bench_test_macro_f1_bar_agg.png"]], "plot_analyses": [[{"analysis": "The cross-entropy loss curves for both training and validation sets demonstrate a steady decline over the epochs. The training loss decreases more sharply compared to the validation loss, which indicates effective learning on the training data. However, the validation loss plateaus after a few epochs, suggesting that the model's generalization capability might be reaching its limit. There is no indication of overfitting as the validation loss does not increase after the initial epochs.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_loss_curves.png"}, {"analysis": "The macro-F1 curves for both training and validation sets show consistent improvement over the epochs. The validation macro-F1 score closely follows the training macro-F1 score, indicating that the model is generalizing well to unseen data. The scores plateau after about four epochs, suggesting that the model has reached its peak performance on this metric.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_macro_f1_curves.png"}, {"analysis": "The confusion matrix for the test set shows a strong diagonal dominance, indicating that the model performs well in correctly classifying the majority of the test samples. However, there is some misclassification evident in the off-diagonal cells, which could be further analyzed to identify specific patterns or classes that are more prone to errors.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_c25ffad6a8124c778dd9407aea4546cc_proc_3154576/SPR_BENCH_confusion_matrix.png"}], [{"analysis": "The Macro-F1 curves show the performance of the model across different epochs for both training and validation sets. The training curves (dashed lines) exhibit a steady increase in Macro-F1 score as the number of epochs increases, indicating that the model is learning effectively from the training data. However, the validation curves (solid lines) plateau or fluctuate after a certain point, particularly for higher epoch settings (e.g., 20 and 30 epochs). This suggests that the model may be overfitting to the training data as the number of epochs increases, leading to diminished generalization performance on the validation set. The optimal number of epochs appears to be around 10, where the validation performance stabilizes without significant degradation.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_macro_f1_curves.png"}, {"analysis": "The loss curves provide insights into the optimization process and potential overfitting. For the training set (dashed lines), the loss decreases consistently as the number of epochs increases, which is expected during effective training. However, for the validation set (solid lines), the loss initially decreases but starts to increase or fluctuate for higher epoch settings (e.g., 20 and 30 epochs). This divergence between training and validation loss indicates overfitting as the model starts to memorize the training data rather than generalizing. The epoch setting of 10 seems to strike a balance between reducing loss and avoiding overfitting, as the validation loss is relatively stable at this point.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_loss_curves.png"}, {"analysis": "The bar chart summarizes the test set performance in terms of Macro-F1 for different epoch settings. The scores are relatively close across all settings, with a slight advantage for the 5-epoch setting. This indicates that while increasing the number of epochs improves training performance, it does not significantly enhance test performance. This further supports the observation that excessive training leads to overfitting, and a lower epoch setting (e.g., 5 or 10) may be preferable for achieving optimal generalization on unseen data.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_e323339a7c5841b299bfd57947090433_proc_3158726/spr_bench_test_macro_f1_bar.png"}], [], [], [], [], [{"analysis": "The loss curve indicates a significant spike in the training loss around epoch 6, which is mirrored by a similar spike in the validation loss. This suggests potential instability in the model training, possibly due to a learning rate that is too high or an issue with gradient updates. After epoch 6, both training and validation losses stabilize and decrease steadily, which is a positive sign of convergence. However, the validation loss remains consistently higher than the training loss, pointing to a potential overfitting issue.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_loss_curve.png"}, {"analysis": "The macro-F1 scores show a sharp drop for both training and validation sets around epoch 6, coinciding with the loss spikes observed earlier. This further highlights training instability at this point. Post epoch 6, the macro-F1 scores recover and stabilize around 0.79 to 0.8, indicating the model's ability to generalize well after overcoming the instability. The validation macro-F1 closely tracks the training macro-F1, suggesting a good balance between training and validation performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_f1_curve.png"}, {"analysis": "The bar chart shows that the macro-F1 scores for two different hyperparameter configurations are very close, with values of 0.791 and 0.794. This indicates that both configurations perform similarly, with a marginal advantage for the second configuration. The small difference suggests that the model's performance is not significantly sensitive to these particular hyperparameter changes.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_test_f1_bar.png"}, {"analysis": "The confusion matrix for the best hyperparameter configuration indicates a balanced performance across the two classes, with high values along the diagonal representing correct predictions. The off-diagonal values are relatively low, suggesting minimal misclassification. This confirms that the model is effectively distinguishing between the two classes under the chosen hyperparameters.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_20d6a86bfc20420684e54304952d5030_proc_3158729/SPR_BENCH_confusion_matrix.png"}], [], [], [{"analysis": "The Macro-F1 curves show the performance trend for both training and validation sets across different epoch configurations. Initially, all configurations exhibit a steep increase in Macro-F1, indicating rapid learning. Beyond 5 epochs, the performance stabilizes for most configurations, with diminishing returns as epochs increase. Training curves (dashed) tend to slightly overfit after 10 epochs, as seen by their divergence from validation curves (solid). The 20-epoch configuration achieves a balance between training and validation performance, suggesting it might be the optimal choice for this setup.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_macro_f1_curves.png"}, {"analysis": "The loss curves reveal the convergence behavior of the model for various epoch settings. All configurations show a rapid decrease in loss during the initial epochs, with a slower decline afterward. The 20-epoch configuration exhibits the most consistent reduction in loss for both training and validation sets, while the 30-epoch setup shows potential overfitting, as validation loss increases slightly after 10 epochs. These observations align with the Macro-F1 trends, reinforcing the 20-epoch configuration as a balanced choice.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_loss_curves.png"}, {"analysis": "The bar chart summarizes the test set Macro-F1 scores for different epoch configurations. All configurations achieve comparable performance, with scores around 0.80. This suggests that while increasing epochs improves training stability, the test performance plateaus after 5 epochs. The 10-epoch and 20-epoch settings achieve slightly better test scores, indicating that they might offer a good trade-off between training time and performance.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/spr_bench_test_macro_f1_bar.png"}], [{"analysis": "The Macro-F1 curves indicate that the model generally converges to a stable performance level across all hyperparameter settings. For training curves, the performance stabilizes quickly, typically within the first few epochs. Validation performance is slightly more volatile, with some settings (e.g., epochs_30) showing a decline in Macro-F1 after an initial peak. This suggests potential overfitting when training for too many epochs.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_macro_f1_curves.png"}, {"analysis": "The loss curves reveal that the training loss decreases steadily across all settings, indicating that the model is learning effectively. However, the validation loss exhibits some fluctuation, particularly for higher epoch settings like epochs_30, where it starts to increase after a certain point. This further supports the observation of overfitting in prolonged training scenarios.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_loss_curves.png"}, {"analysis": "The bar chart summarizing test Macro-F1 scores across hyperparameter settings shows that the performance is relatively consistent, with epochs_20 achieving the highest score. This suggests that training for 20 epochs strikes a good balance between learning and avoiding overfitting, making it a potentially optimal setting for this task.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/spr_bench_test_macro_f1_bar.png"}], [{"analysis": "This plot illustrates the Macro-F1 scores for both training and validation datasets across different epoch settings (5, 10, 20, and 30 epochs). The dashed lines represent the training performance, while the solid lines represent validation performance. The results indicate that the Macro-F1 score improves significantly within the first few epochs and stabilizes thereafter. The validation performance closely tracks the training performance, suggesting that the model generalizes well and does not overfit. However, the choice of epochs (beyond 10) does not seem to have a significant impact on improving performance, as all curves converge to similar Macro-F1 scores around 0.80.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_macro_f1_curves.png"}, {"analysis": "This plot shows the cross-entropy loss for both training and validation datasets across different epoch settings. The dashed lines represent training loss, while the solid lines represent validation loss. The loss decreases rapidly in the first few epochs and stabilizes afterward. For longer epoch settings (20 and 30 epochs), the training loss continues to decrease slightly, but the validation loss does not show significant improvement, and in some cases, it even increases slightly, indicating potential overfitting. This suggests that training for more than 10 epochs does not yield substantial benefits and may lead to diminishing returns.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_loss_curves.png"}, {"analysis": "This bar chart represents the Macro-F1 scores on the test set for different epoch settings. The scores are approximately equal across all settings, with no significant variation. This indicates that extending the number of epochs beyond 10 does not provide additional performance gains on the test set, and the model achieves similar results regardless of the training duration.", "plot_path": "experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/spr_bench_test_macro_f1_bar.png"}], []], "vlm_feedback_summary": ["The results indicate that the model is learning effectively and generalizing\nwell, as evidenced by the consistent decline in loss and improvement in macro-F1\nscores. The confusion matrix highlights strong classification performance with\nsome room for improvement in reducing misclassifications.", "The plots collectively indicate that while the model learns effectively from the\ntraining data, increasing the number of epochs leads to overfitting, as\nevidenced by the divergence between training and validation performance. The\noptimal epoch setting appears to be around 10, where the model achieves a good\nbalance between training and validation performance without significant\noverfitting. Further tuning of hyperparameters or regularization techniques may\nbe necessary to enhance generalization.", "[]", "[]", "[]", "[]", "The results indicate that the model demonstrates good performance after initial\ntraining instability, achieving macro-F1 scores close to the SOTA benchmark. The\nconfusion matrix shows balanced classification, and hyperparameter tuning has a\nmarginal impact on performance. Further investigation into the training\ninstability around epoch 6 is recommended to improve consistency.", "[]", "[]", "The plots provide detailed insights into the model's performance across\ndifferent epoch configurations. The Macro-F1 and loss curves highlight the\nlearning dynamics, while the bar chart summarizes the final test performance.\nThe 20-epoch configuration appears optimal, offering a balance between training\nand validation performance without overfitting. Test performance plateaus after\n5 epochs, suggesting limited gains from extended training.", "The plots indicate that training for 20 epochs achieves the best balance between\nperformance and overfitting. Prolonged training beyond this point leads to\ndiminishing returns or overfitting, as evidenced by the validation loss and\nMacro-F1 trends. The results suggest that careful tuning of training duration is\ncritical for optimal performance in SPR tasks.", "The plots indicate that the model's performance stabilizes after 10 epochs, and\nextending training beyond this point does not yield significant improvements.\nBoth Macro-F1 and cross-entropy loss metrics suggest diminishing returns with\nlonger training durations, and the model generalizes well without overfitting.\nThe test set results confirm that the choice of epochs has minimal impact on the\nfinal performance.", "[]"], "exec_time": [3.4055440425872803, 8.526899814605713, 0.9891421794891357, 1.005033016204834, 1.120741605758667, 1.2770178318023682, 8.167437553405762, 0.893826961517334, 1.1599855422973633, 8.394721508026123, 7.180891752243042, 9.401138067245483, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"SPR_BENCH\"]"], ["[\"SPR_BENCH\"]"], [], [], [], [], ["['SPR_BENCH']"], [], [], ["[\"SPR_BENCH\"]"], ["['epochs_20']"], ["<keys>"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------------------- load data --------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nds_key = \"SPR_BENCH\"\nif ds_key not in experiment_data:\n    print(f\"Dataset {ds_key} not found in experiment data.\")\n    exit()\n\nds = experiment_data[ds_key]\n\n# -------------------- 1. Loss curves --------------------\ntry:\n    epochs = ds[\"epochs\"]\n    tr_loss = ds[\"losses\"][\"train\"]\n    val_loss = ds[\"losses\"][\"val\"]\n    plt.figure()\n    plt.plot(epochs, tr_loss, label=\"Train\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# -------------------- 2. Macro-F1 curves --------------------\ntry:\n    tr_f1 = ds[\"metrics\"][\"train_macro_f1\"]\n    val_f1 = ds[\"metrics\"][\"val_macro_f1\"]\n    plt.figure()\n    plt.plot(epochs, tr_f1, label=\"Train\")\n    plt.plot(epochs, val_f1, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves\\nLeft: Train, Right: Validation\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"SPR_BENCH_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# -------------------- 3. Confusion matrix --------------------\ntry:\n    preds = np.array(ds.get(\"predictions\", []))\n    gts = np.array(ds.get(\"ground_truth\", []))\n    if preds.size > 0 and gts.size > 0:\n        cm = confusion_matrix(gts, preds)\n        plt.figure()\n        im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n        plt.colorbar(im)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"SPR_BENCH Confusion Matrix\\nDataset: Test Set\")\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    else:\n        print(\"Predictions / ground-truth empty; skipping confusion matrix.\")\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n\n# -------------------- print evaluation metric --------------------\nif preds.size > 0 and gts.size > 0:\n    test_macro_f1 = f1_score(gts, preds, average=\"macro\")\n    print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper: gather keys and colours --------------------------------------------------------\nkeys = list(experiment_data.get(\"num_epochs\", {}).keys())\ncolors = plt.cm.tab10.colors if keys else []\n\n# 1) Train / Val Macro-F1 curves ----------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"train_macro_f1\"]\n        val_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"val_macro_f1\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_f1, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_f1, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# 2) Train / Val Loss curves --------------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"train\"]\n        val_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"val\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_loss, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_loss, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 3) Test Macro-F1 bar chart --------------------------------------------------------------\ntest_scores = {}\ntry:\n    for k in keys:\n        test_scores[k] = experiment_data[\"num_epochs\"][k].get(\"test_macro_f1\", np.nan)\n\n    plt.figure()\n    plt.bar(\n        range(len(test_scores)),\n        list(test_scores.values()),\n        tick_label=list(test_scores.keys()),\n    )\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Test Macro-F1 per Hyper-param Setting\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_macro_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test score bar plot: {e}\")\n    plt.close()\n\n# Print numeric summary -------------------------------------------------------------------\nprint(\"Test Macro-F1 scores:\", test_scores)\n", null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# -------- load experiment data ----------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    spr = experiment_data[\"SPR_BENCH\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    spr = None\n\nif spr is not None:\n\n    # ---------------- plot 1: loss curves ----------------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(spr[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, spr[\"losses\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, spr[\"losses\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Cross-Entropy Loss\")\n        plt.title(\"SPR_BENCH \u2013 Loss vs Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_loss_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # ---------------- plot 2: macro-F1 curves -------------\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(spr[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, spr[\"metrics\"][\"train\"], label=\"Train\")\n        plt.plot(epochs, spr[\"metrics\"][\"val\"], label=\"Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH \u2013 Macro-F1 vs Epochs\")\n        plt.legend()\n        fname = os.path.join(working_dir, \"SPR_BENCH_f1_curve.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating F1 curve: {e}\")\n        plt.close()\n\n    # ---------------- plot 3: test macro-F1 summary -------\n    try:\n        plt.figure()\n        hp_scores = {k: v[\"test_macro_f1\"] for k, v in spr[\"hyperparams\"].items()}\n        keys, vals = list(hp_scores.keys()), list(hp_scores.values())\n        plt.bar(range(len(vals)), vals, tick_label=[str(i) for i in range(len(vals))])\n        plt.ylabel(\"Test Macro-F1\")\n        plt.xlabel(\"Hyper-parameter Index\")\n        plt.title(\"SPR_BENCH \u2013 Test Macro-F1 per Hyper-param\")\n        for idx, v in enumerate(vals):\n            plt.text(idx, v + 0.005, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n        fname = os.path.join(working_dir, \"SPR_BENCH_test_f1_bar.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating bar chart: {e}\")\n        plt.close()\n\n    # ---------------- plot 4: confusion matrix ------------\n    try:\n        best_hp = max(hp_scores, key=hp_scores.get)\n        best_idx = list(spr[\"hyperparams\"].keys()).index(best_hp)\n        preds = np.array(spr[\"predictions\"][best_idx])\n        gts = np.array(spr[\"ground_truth\"][best_idx])\n        labels = np.unique(gts)\n        cm = np.zeros((len(labels), len(labels)), dtype=int)\n        for gt, pr in zip(gts, preds):\n            cm[np.where(labels == gt)[0][0], np.where(labels == pr)[0][0]] += 1\n        cm_norm = cm / cm.sum(axis=1, keepdims=True)\n        plt.figure()\n        im = plt.imshow(cm_norm, cmap=\"Blues\")\n        plt.colorbar(im, fraction=0.046, pad=0.04)\n        plt.xticks(range(len(labels)), labels)\n        plt.yticks(range(len(labels)), labels)\n        plt.ylabel(\"True Label\")\n        plt.xlabel(\"Predicted Label\")\n        plt.title(f\"SPR_BENCH \u2013 Confusion Matrix (Best HP {best_hp})\")\n        fname = os.path.join(working_dir, \"SPR_BENCH_confusion_matrix.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating confusion matrix: {e}\")\n        plt.close()\n\n    # ------------- print evaluation metrics ---------------\n    print(\"Test Macro-F1 per hyper-parameter:\")\n    print(hp_scores)\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper: gather keys and colours --------------------------------------------------------\nkeys = list(experiment_data.get(\"num_epochs\", {}).keys())\ncolors = plt.cm.tab10.colors if keys else []\n\n# 1) Train / Val Macro-F1 curves ----------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"train_macro_f1\"]\n        val_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"val_macro_f1\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_f1, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_f1, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# 2) Train / Val Loss curves --------------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"train\"]\n        val_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"val\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_loss, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_loss, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 3) Test Macro-F1 bar chart --------------------------------------------------------------\ntest_scores = {}\ntry:\n    for k in keys:\n        test_scores[k] = experiment_data[\"num_epochs\"][k].get(\"test_macro_f1\", np.nan)\n\n    plt.figure()\n    plt.bar(\n        range(len(test_scores)),\n        list(test_scores.values()),\n        tick_label=list(test_scores.keys()),\n    )\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Test Macro-F1 per Hyper-param Setting\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_macro_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test score bar plot: {e}\")\n    plt.close()\n\n# Print numeric summary -------------------------------------------------------------------\nprint(\"Test Macro-F1 scores:\", test_scores)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper: gather keys and colours --------------------------------------------------------\nkeys = list(experiment_data.get(\"num_epochs\", {}).keys())\ncolors = plt.cm.tab10.colors if keys else []\n\n# 1) Train / Val Macro-F1 curves ----------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"train_macro_f1\"]\n        val_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"val_macro_f1\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_f1, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_f1, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# 2) Train / Val Loss curves --------------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"train\"]\n        val_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"val\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_loss, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_loss, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 3) Test Macro-F1 bar chart --------------------------------------------------------------\ntest_scores = {}\ntry:\n    for k in keys:\n        test_scores[k] = experiment_data[\"num_epochs\"][k].get(\"test_macro_f1\", np.nan)\n\n    plt.figure()\n    plt.bar(\n        range(len(test_scores)),\n        list(test_scores.values()),\n        tick_label=list(test_scores.keys()),\n    )\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Test Macro-F1 per Hyper-param Setting\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_macro_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test score bar plot: {e}\")\n    plt.close()\n\n# Print numeric summary -------------------------------------------------------------------\nprint(\"Test Macro-F1 scores:\", test_scores)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data ------------------------------------------------------------------\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Helper: gather keys and colours --------------------------------------------------------\nkeys = list(experiment_data.get(\"num_epochs\", {}).keys())\ncolors = plt.cm.tab10.colors if keys else []\n\n# 1) Train / Val Macro-F1 curves ----------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"train_macro_f1\"]\n        val_f1 = experiment_data[\"num_epochs\"][k][\"metrics\"][\"val_macro_f1\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_f1, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_f1, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Macro-F1 Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_macro_f1_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating macro-F1 plot: {e}\")\n    plt.close()\n\n# 2) Train / Val Loss curves --------------------------------------------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        epochs = experiment_data[\"num_epochs\"][k][\"epochs\"]\n        tr_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"train\"]\n        val_loss = experiment_data[\"num_epochs\"][k][\"losses\"][\"val\"]\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_loss, linestyle=\"--\", color=c, label=f\"{k}-train\")\n        plt.plot(epochs, val_loss, linestyle=\"-\", color=c, label=f\"{k}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"SPR_BENCH Loss Curves (Left: Train dashed, Right: Validation solid)\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# 3) Test Macro-F1 bar chart --------------------------------------------------------------\ntest_scores = {}\ntry:\n    for k in keys:\n        test_scores[k] = experiment_data[\"num_epochs\"][k].get(\"test_macro_f1\", np.nan)\n\n    plt.figure()\n    plt.bar(\n        range(len(test_scores)),\n        list(test_scores.values()),\n        tick_label=list(test_scores.keys()),\n    )\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"SPR_BENCH Test Macro-F1 per Hyper-param Setting\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    fname = os.path.join(working_dir, \"spr_bench_test_macro_f1_bar.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test score bar plot: {e}\")\n    plt.close()\n\n# Print numeric summary -------------------------------------------------------------------\nprint(\"Test Macro-F1 scores:\", test_scores)\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# ------------------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ------------------------------------------------------------------\n# 1. Load all experiment_data dicts ---------------------------------\nexperiment_data_path_list = [\n    \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_573920eca39f485bab1a49bfd7816f2f_proc_3158727/experiment_data.npy\",\n    \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_3d431d4d26dc4023824f7680e9e21620_proc_3158728/experiment_data.npy\",\n    \"experiments/2025-08-17_00-44-27_contextual_embedding_spr_attempt_0/logs/0-run/experiment_results/experiment_d7a04eeff3f6467493663d873a6d0e39_proc_3158726/experiment_data.npy\",\n]\n\nall_experiment_data = []\nfor p in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), p)\n        exp_dict = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_dict)\n    except Exception as e:\n        print(f\"Error loading {p}: {e}\")\n\nif not all_experiment_data:\n    print(\"No experiment results could be loaded; aborting plotting.\")\n    exit()\n\n# ------------------------------------------------------------------\n# 2. Aggregate metrics ---------------------------------------------\n# Structure: agg_data[key] = list_of_run_dicts_for_that_key\nagg_data = {}\nfor run in all_experiment_data:\n    for k, v in run.get(\"num_epochs\", {}).items():\n        agg_data.setdefault(k, []).append(v)\n\n\ndef _stack_and_stats(list_of_arrays):\n    \"\"\"Returns (mean, se) along axis 0.\"\"\"\n    arr = np.stack(list_of_arrays, axis=0)  # shape [n_runs, time]\n    mean = np.mean(arr, axis=0)\n    se = (\n        np.std(arr, axis=0, ddof=1) / np.sqrt(arr.shape[0])\n        if arr.shape[0] > 1\n        else np.zeros_like(mean)\n    )\n    return mean, se\n\n\n# Colours for distinct hyper-param keys\nkeys = list(agg_data.keys())\ncolors = plt.cm.tab10.colors if keys else []\n\n# ------------------------------------------------------------------\n# 3. Train/Val Macro-F1 curves with SE shading ----------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        runs_for_k = agg_data[k]\n        # assume all epochs arrays are identical \u2013 take from first run\n        epochs = runs_for_k[0][\"epochs\"]\n        tr_arrays = [\n            r[\"metrics\"][\"train_macro_f1\"]\n            for r in runs_for_k\n            if \"metrics\" in r and \"train_macro_f1\" in r[\"metrics\"]\n        ]\n        val_arrays = [\n            r[\"metrics\"][\"val_macro_f1\"]\n            for r in runs_for_k\n            if \"metrics\" in r and \"val_macro_f1\" in r[\"metrics\"]\n        ]\n        if not tr_arrays or not val_arrays:\n            continue\n        tr_mean, tr_se = _stack_and_stats(tr_arrays)\n        val_mean, val_se = _stack_and_stats(val_arrays)\n        c = colors[idx % len(colors)]\n        # Train\n        plt.plot(epochs, tr_mean, linestyle=\"--\", color=c, label=f\"{k}-train mean\")\n        plt.fill_between(epochs, tr_mean - tr_se, tr_mean + tr_se, color=c, alpha=0.2)\n        # Val\n        plt.plot(epochs, val_mean, linestyle=\"-\", color=c, label=f\"{k}-val mean\")\n        plt.fill_between(\n            epochs, val_mean - val_se, val_mean + val_se, color=c, alpha=0.2\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\n        \"SPR_BENCH Aggregate Macro-F1 (shaded = \u00b1SE)\\nLeft dashed: Train, Right solid: Validation\"\n    )\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_macro_f1_curves_agg.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated macro-F1 plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 4. Train/Val Loss curves with SE shading --------------------------\ntry:\n    plt.figure()\n    for idx, k in enumerate(keys):\n        runs_for_k = agg_data[k]\n        epochs = runs_for_k[0][\"epochs\"]\n        tr_arrays = [\n            r[\"losses\"][\"train\"]\n            for r in runs_for_k\n            if \"losses\" in r and \"train\" in r[\"losses\"]\n        ]\n        val_arrays = [\n            r[\"losses\"][\"val\"]\n            for r in runs_for_k\n            if \"losses\" in r and \"val\" in r[\"losses\"]\n        ]\n        if not tr_arrays or not val_arrays:\n            continue\n        tr_mean, tr_se = _stack_and_stats(tr_arrays)\n        val_mean, val_se = _stack_and_stats(val_arrays)\n        c = colors[idx % len(colors)]\n        plt.plot(epochs, tr_mean, linestyle=\"--\", color=c, label=f\"{k}-train mean\")\n        plt.fill_between(epochs, tr_mean - tr_se, tr_mean + tr_se, color=c, alpha=0.2)\n        plt.plot(epochs, val_mean, linestyle=\"-\", color=c, label=f\"{k}-val mean\")\n        plt.fill_between(\n            epochs, val_mean - val_se, val_mean + val_se, color=c, alpha=0.2\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\n        \"SPR_BENCH Aggregate Loss (shaded = \u00b1SE)\\nLeft dashed: Train, Right solid: Validation\"\n    )\n    plt.legend()\n    fname = os.path.join(working_dir, \"spr_bench_loss_curves_agg.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 5. Test Macro-F1 bar chart (mean \u00b1 SE) ----------------------------\naggregated_test_scores = {}\ntry:\n    # collect arrays of final test Macro-F1 per key\n    for k in keys:\n        values = []\n        for run_dict in agg_data[k]:\n            if \"test_macro_f1\" in run_dict:\n                values.append(run_dict[\"test_macro_f1\"])\n        if values:\n            values = np.asarray(values)\n            aggregated_test_scores[k] = (\n                values.mean(),\n                values.std(ddof=1) / np.sqrt(len(values)) if len(values) > 1 else 0.0,\n            )\n\n    if aggregated_test_scores:\n        plt.figure()\n        means = [v[0] for v in aggregated_test_scores.values()]\n        ses = [v[1] for v in aggregated_test_scores.values()]\n        plt.bar(\n            range(len(means)),\n            means,\n            yerr=ses,\n            capsize=5,\n            tick_label=list(aggregated_test_scores.keys()),\n        )\n        plt.ylabel(\"Macro-F1\")\n        plt.title(\"SPR_BENCH Aggregate Test Macro-F1 (bars = mean, error = \u00b1SE)\")\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        fname = os.path.join(working_dir, \"spr_bench_test_macro_f1_bar_agg.png\")\n        plt.savefig(fname)\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated test score bar plot: {e}\")\n    plt.close()\n\n# ------------------------------------------------------------------\n# 6. Numeric summary -----------------------------------------------\nprint(\"Aggregated Test Macro-F1 (mean, \u00b1SE):\")\nfor k, (m, se) in aggregated_test_scores.items():\n    print(f\"  {k}: {m:.4f} \u00b1 {se:.4f}\")\n"], "plot_plan": ["We will first load experiment_data.npy from the working directory and verify that the expected keys exist. Then, for the single dataset (SPR_BENCH), we extract epoch indices, training/validation losses, and training/validation macro-F1 scores. Three figures are created in independent try\u2013except blocks: (1) a line chart of training vs. validation loss, (2) a line chart of training vs. validation macro-F1, and (3) a confusion-matrix heat-map built from stored test predictions and ground-truth labels (plotted only if those arrays are non-empty). Each plot is titled clearly, saved with descriptive file names in working_dir, and the figure is always closed afterward. Finally, we compute and print the overall test macro-F1 so the user sees a quantitative summary alongside the visualizations.", "We will load the saved experiment_data.npy, iterate over the four hyper-parameter settings (epochs_5, epochs_10, epochs_20, epochs_30) and extract per-epoch losses, macro-F1s, and the single test macro-F1 value. Three concise visualizations will be generated: (1) a combined line plot of train and validation macro-F1 curves for every setting, (2) a combined line plot of train and validation loss curves, and (3) a bar chart comparing the final test macro-F1 achieved by each setting. Each figure is created in its own try-except block, titled clearly, saved in working_dir with descriptive filenames, and closed afterwards. Only data present in experiment_data.npy are used; if a key is missing or plotting fails it is caught and reported. After the plots are produced, the script prints the dictionary of test macro-F1 scores so the user can quickly inspect numeric results. This approach yields a maximum of three figures (well under the five-figure cap) and follows all specified coding and plotting conventions.", null, null, null, null, "We will load the saved numpy dictionary, extract the loss and macro-F1 histories, and plot train/validation curves across all recorded epochs.  \nNext we build a bar chart summarising the test macro-F1 obtained for every explored hyper-parameter setting to spot the best configuration.  \nUsing that best hyper-parameter key we fetch the corresponding predictions/ground-truth arrays (their order in the list matches insertion order of the dict) and compute a normalised confusion matrix, visualising it with imshow.  \nAll plots are wrapped in separate try\u2013except blocks, saved inside the working directory with descriptive filenames and always closed afterwards.  \nWe limit ourselves to four figures, well below the maximum of five, and only use data stored in experiment_data.npy.  \nTitles clearly state the dataset and what is shown; subtitles further clarify content when helpful.  \nFinally, the script prints the per-hyper-param test macro-F1 dictionary so users can inspect numerical values alongside the graphics.", null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "num_epochs", "learning_rate", "batch_size", "d_model (Transformer hidden/embedding dimension)", "weight_decay", null, "nhead", null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["We will load the saved NumPy dictionary, iterate over every dataset entry, and\nfor each one inspect the \u201cmetrics\u201d and \u201closses\u201d sub-dictionaries.   For\nF1-scores we report the best (maximum) value; for losses we report the best\n(minimum) value.   Each line is printed with an explicit, self-descriptive label\nsuch as \u201cbest training macro F1 score\u201d or \u201cbest validation loss\u201d so that the\noutput is unambiguous.   The script contains no `if __name__ == \"__main__\":`\nguard; everything executes immediately when the file is run.", "The script loads the aggregated experiment results, iterates over each hyper-\nparameter setting (treated here as separate datasets), and prints the key\nperformance indicators. For each setting it reports the best training/validation\nmacro-F1, the final training/validation loss, and the stored test macro-F1 and\ntest loss, clearly labeling every value.", "", "", "", "", "The script will load the saved NumPy file from the \u201cworking\u201d directory, unpack\nthe nested dictionary, and then report the final training/validation loss and F1\nscores (taken from the last recorded epoch) as well as the best test-set\nmacro-F1 score (best across all hyper-parameter configurations). All output is\nclearly labelled and prefixed by the dataset name.", "", "", "The script loads the aggregated experiment results, iterates over each hyper-\nparameter setting (treated here as separate datasets), and prints the key\nperformance indicators. For each setting it reports the best training/validation\nmacro-F1, the final training/validation loss, and the stored test macro-F1 and\ntest loss, clearly labeling every value.", "The script loads the aggregated experiment results, iterates over each hyper-\nparameter setting (treated here as separate datasets), and prints the key\nperformance indicators. For each setting it reports the best training/validation\nmacro-F1, the final training/validation loss, and the stored test macro-F1 and\ntest loss, clearly labeling every value.", "The script loads the aggregated experiment results, iterates over each hyper-\nparameter setting (treated here as separate datasets), and prints the key\nperformance indicators. For each setting it reports the best training/validation\nmacro-F1, the final training/validation loss, and the stored test macro-F1 and\ntest loss, clearly labeling every value.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# -------------------------------------------------\n# locate and load the experiment data\n# -------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_path):\n    raise FileNotFoundError(f\"Could not find file at {data_path}\")\n\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n\n# -------------------------------------------------\n# helper to decide whether to maximise or minimise\n# -------------------------------------------------\ndef best_value(values, higher_is_better=True):\n    if not values:  # empty list safety-check\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\n# -------------------------------------------------\n# iterate through every stored dataset\n# -------------------------------------------------\nfor dataset_name, content in experiment_data.items():\n    print(f\"\\n{dataset_name}\")  # dataset heading\n\n    # ----- F1 metrics -----\n    metrics = content.get(\"metrics\", {})\n    if \"train_macro_f1\" in metrics:\n        best_train_f1 = best_value(metrics[\"train_macro_f1\"], higher_is_better=True)\n        print(f\"best training macro F1 score: {best_train_f1:.4f}\")\n    if \"val_macro_f1\" in metrics:\n        best_val_f1 = best_value(metrics[\"val_macro_f1\"], higher_is_better=True)\n        print(f\"best validation macro F1 score: {best_val_f1:.4f}\")\n\n    # ----- Loss values -----\n    losses = content.get(\"losses\", {})\n    if \"train\" in losses:\n        best_train_loss = best_value(losses[\"train\"], higher_is_better=False)\n        print(f\"best training loss: {best_train_loss:.4f}\")\n    if \"val\" in losses:\n        best_val_loss = best_value(losses[\"val\"], higher_is_better=False)\n        print(f\"best validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n\n# ------------------------------------------------------------------\n# utility -----------------------------------------------------------------\ndef safe_fmt(value, fmt=\".4f\"):\n    \"\"\"Return a formatted numeric value or 'N/A' if missing.\"\"\"\n    return f\"{value:{fmt}}\" if isinstance(value, (float, int, np.number)) else \"N/A\"\n\n\ndef display_metrics(exp_dict):\n    \"\"\"\n    exp_dict is experiment_data['num_epochs'][<key>]\n    Prints a concise summary with clear metric names.\n    \"\"\"\n    train_f1s = exp_dict[\"metrics\"][\"train_macro_f1\"]\n    val_f1s = exp_dict[\"metrics\"][\"val_macro_f1\"]\n    train_loss = exp_dict[\"losses\"][\"train\"]\n    val_loss = exp_dict[\"losses\"][\"val\"]\n\n    # best or final values -------------------------------------------------\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n    final_train_loss = train_loss[-1] if train_loss else None\n    final_val_loss = val_loss[-1] if val_loss else None\n    test_f1 = exp_dict.get(\"test_macro_f1\")\n    test_loss = exp_dict.get(\"test_loss\")\n\n    # print ----------------------------------------------------------------\n    print(f\"  train macro F1 score: {safe_fmt(best_train_f1)}\")\n    print(f\"  validation macro F1 score: {safe_fmt(best_val_f1)}\")\n    print(f\"  test macro F1 score: {safe_fmt(test_f1)}\")\n    print(f\"  training loss: {safe_fmt(final_train_loss)}\")\n    print(f\"  validation loss: {safe_fmt(final_val_loss)}\")\n    print(f\"  test loss: {safe_fmt(test_loss)}\\n\")\n\n\n# ------------------------------------------------------------------\n# main execution (runs immediately) --------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"{data_file} does not exist.\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\nnum_epochs_section = experiment_data.get(\"num_epochs\", {})\nif not num_epochs_section:\n    print(\"No experiment results found under 'num_epochs'.\")\nelse:\n    for ds_name, ds_content in num_epochs_section.items():\n        print(f\"Dataset: {ds_name}\")\n        display_metrics(ds_content)\n", "", "", "", "", "import os\nimport numpy as np\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locate and load data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"Cannot find experiment data at {file_path}\")\n\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 helper to get safe last element \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef _last_or_none(lst):\n    return lst[-1] if lst else None\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 iterate over datasets \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfor dset_name, dset_info in experiment_data.items():\n    print(f\"\\nDataset: {dset_name}\")\n\n    # losses and metrics recorded epoch-wise\n    train_losses = dset_info.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dset_info.get(\"losses\", {}).get(\"val\", [])\n    train_f1s = dset_info.get(\"metrics\", {}).get(\"train\", [])\n    val_f1s = dset_info.get(\"metrics\", {}).get(\"val\", [])\n\n    final_train_loss = _last_or_none(train_losses)\n    final_val_loss = _last_or_none(val_losses)\n    final_train_f1 = _last_or_none(train_f1s)\n    final_val_f1 = _last_or_none(val_f1s)\n\n    # best test metric across all hyper-parameter settings\n    hp_dict = dset_info.get(\"hyperparams\", {})\n    test_f1_scores = [\n        v.get(\"test_macro_f1\") for v in hp_dict.values() if \"test_macro_f1\" in v\n    ]\n    best_test_f1 = max(test_f1_scores) if test_f1_scores else None\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 pretty printing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n    if final_train_f1 is not None:\n        print(f\"Final training macro F1 score: {final_train_f1:.4f}\")\n    if final_val_f1 is not None:\n        print(f\"Final validation macro F1 score: {final_val_f1:.4f}\")\n    if best_test_f1 is not None:\n        print(f\"Best test macro F1 score: {best_test_f1:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n\n# ------------------------------------------------------------------\n# utility -----------------------------------------------------------------\ndef safe_fmt(value, fmt=\".4f\"):\n    \"\"\"Return a formatted numeric value or 'N/A' if missing.\"\"\"\n    return f\"{value:{fmt}}\" if isinstance(value, (float, int, np.number)) else \"N/A\"\n\n\ndef display_metrics(exp_dict):\n    \"\"\"\n    exp_dict is experiment_data['num_epochs'][<key>]\n    Prints a concise summary with clear metric names.\n    \"\"\"\n    train_f1s = exp_dict[\"metrics\"][\"train_macro_f1\"]\n    val_f1s = exp_dict[\"metrics\"][\"val_macro_f1\"]\n    train_loss = exp_dict[\"losses\"][\"train\"]\n    val_loss = exp_dict[\"losses\"][\"val\"]\n\n    # best or final values -------------------------------------------------\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n    final_train_loss = train_loss[-1] if train_loss else None\n    final_val_loss = val_loss[-1] if val_loss else None\n    test_f1 = exp_dict.get(\"test_macro_f1\")\n    test_loss = exp_dict.get(\"test_loss\")\n\n    # print ----------------------------------------------------------------\n    print(f\"  train macro F1 score: {safe_fmt(best_train_f1)}\")\n    print(f\"  validation macro F1 score: {safe_fmt(best_val_f1)}\")\n    print(f\"  test macro F1 score: {safe_fmt(test_f1)}\")\n    print(f\"  training loss: {safe_fmt(final_train_loss)}\")\n    print(f\"  validation loss: {safe_fmt(final_val_loss)}\")\n    print(f\"  test loss: {safe_fmt(test_loss)}\\n\")\n\n\n# ------------------------------------------------------------------\n# main execution (runs immediately) --------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"{data_file} does not exist.\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\nnum_epochs_section = experiment_data.get(\"num_epochs\", {})\nif not num_epochs_section:\n    print(\"No experiment results found under 'num_epochs'.\")\nelse:\n    for ds_name, ds_content in num_epochs_section.items():\n        print(f\"Dataset: {ds_name}\")\n        display_metrics(ds_content)\n", "import os\nimport numpy as np\n\n\n# ------------------------------------------------------------------\n# utility -----------------------------------------------------------------\ndef safe_fmt(value, fmt=\".4f\"):\n    \"\"\"Return a formatted numeric value or 'N/A' if missing.\"\"\"\n    return f\"{value:{fmt}}\" if isinstance(value, (float, int, np.number)) else \"N/A\"\n\n\ndef display_metrics(exp_dict):\n    \"\"\"\n    exp_dict is experiment_data['num_epochs'][<key>]\n    Prints a concise summary with clear metric names.\n    \"\"\"\n    train_f1s = exp_dict[\"metrics\"][\"train_macro_f1\"]\n    val_f1s = exp_dict[\"metrics\"][\"val_macro_f1\"]\n    train_loss = exp_dict[\"losses\"][\"train\"]\n    val_loss = exp_dict[\"losses\"][\"val\"]\n\n    # best or final values -------------------------------------------------\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n    final_train_loss = train_loss[-1] if train_loss else None\n    final_val_loss = val_loss[-1] if val_loss else None\n    test_f1 = exp_dict.get(\"test_macro_f1\")\n    test_loss = exp_dict.get(\"test_loss\")\n\n    # print ----------------------------------------------------------------\n    print(f\"  train macro F1 score: {safe_fmt(best_train_f1)}\")\n    print(f\"  validation macro F1 score: {safe_fmt(best_val_f1)}\")\n    print(f\"  test macro F1 score: {safe_fmt(test_f1)}\")\n    print(f\"  training loss: {safe_fmt(final_train_loss)}\")\n    print(f\"  validation loss: {safe_fmt(final_val_loss)}\")\n    print(f\"  test loss: {safe_fmt(test_loss)}\\n\")\n\n\n# ------------------------------------------------------------------\n# main execution (runs immediately) --------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"{data_file} does not exist.\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\nnum_epochs_section = experiment_data.get(\"num_epochs\", {})\nif not num_epochs_section:\n    print(\"No experiment results found under 'num_epochs'.\")\nelse:\n    for ds_name, ds_content in num_epochs_section.items():\n        print(f\"Dataset: {ds_name}\")\n        display_metrics(ds_content)\n", "import os\nimport numpy as np\n\n\n# ------------------------------------------------------------------\n# utility -----------------------------------------------------------------\ndef safe_fmt(value, fmt=\".4f\"):\n    \"\"\"Return a formatted numeric value or 'N/A' if missing.\"\"\"\n    return f\"{value:{fmt}}\" if isinstance(value, (float, int, np.number)) else \"N/A\"\n\n\ndef display_metrics(exp_dict):\n    \"\"\"\n    exp_dict is experiment_data['num_epochs'][<key>]\n    Prints a concise summary with clear metric names.\n    \"\"\"\n    train_f1s = exp_dict[\"metrics\"][\"train_macro_f1\"]\n    val_f1s = exp_dict[\"metrics\"][\"val_macro_f1\"]\n    train_loss = exp_dict[\"losses\"][\"train\"]\n    val_loss = exp_dict[\"losses\"][\"val\"]\n\n    # best or final values -------------------------------------------------\n    best_train_f1 = max(train_f1s) if train_f1s else None\n    best_val_f1 = max(val_f1s) if val_f1s else None\n    final_train_loss = train_loss[-1] if train_loss else None\n    final_val_loss = val_loss[-1] if val_loss else None\n    test_f1 = exp_dict.get(\"test_macro_f1\")\n    test_loss = exp_dict.get(\"test_loss\")\n\n    # print ----------------------------------------------------------------\n    print(f\"  train macro F1 score: {safe_fmt(best_train_f1)}\")\n    print(f\"  validation macro F1 score: {safe_fmt(best_val_f1)}\")\n    print(f\"  test macro F1 score: {safe_fmt(test_f1)}\")\n    print(f\"  training loss: {safe_fmt(final_train_loss)}\")\n    print(f\"  validation loss: {safe_fmt(final_val_loss)}\")\n    print(f\"  test loss: {safe_fmt(test_loss)}\\n\")\n\n\n# ------------------------------------------------------------------\n# main execution (runs immediately) --------------------------------\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\n\nif not os.path.isfile(data_file):\n    raise FileNotFoundError(f\"{data_file} does not exist.\")\n\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\nnum_epochs_section = experiment_data.get(\"num_epochs\", {})\nif not num_epochs_section:\n    print(\"No experiment results found under 'num_epochs'.\")\nelse:\n    for ds_name, ds_content in num_epochs_section.items():\n        print(f\"Dataset: {ds_name}\")\n        display_metrics(ds_content)\n", ""], "parse_term_out": ["['\\nSPR_BENCH', '\\n', 'best training macro F1 score: 0.7955', '\\n', 'best\nvalidation macro F1 score: 0.7920', '\\n', 'best training loss: 0.5092', '\\n',\n'best validation loss: 0.5328', '\\n', 'Execution time: a moment seconds (time\nlimit is 30 minutes).']", "['Dataset: epochs_5', '\\n', '  train macro F1 score: 0.7960', '\\n', '\nvalidation macro F1 score: 0.7940', '\\n', '  test macro F1 score: 0.7950', '\\n',\n'  training loss: 0.5079', '\\n', '  validation loss: 0.5381', '\\n', '  test\nloss: 0.5106\\n', '\\n', 'Dataset: epochs_10', '\\n', '  train macro F1 score:\n0.7995', '\\n', '  validation macro F1 score: 0.7959', '\\n', '  test macro F1\nscore: 0.7860', '\\n', '  training loss: 0.4901', '\\n', '  validation loss:\n0.5447', '\\n', '  test loss: 0.5320\\n', '\\n', 'Dataset: epochs_20', '\\n', '\ntrain macro F1 score: 0.8000', '\\n', '  validation macro F1 score: 0.7860',\n'\\n', '  test macro F1 score: 0.7828', '\\n', '  training loss: 0.4865', '\\n', '\nvalidation loss: 0.5881', '\\n', '  test loss: 0.5521\\n', '\\n', 'Dataset:\nepochs_30', '\\n', '  train macro F1 score: 0.8054', '\\n', '  validation macro F1\nscore: 0.7959', '\\n', '  test macro F1 score: 0.8000', '\\n', '  training loss:\n0.4716', '\\n', '  validation loss: 0.5933', '\\n', '  test loss: 0.5388\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "", "", "", "", "['\\nDataset: SPR_BENCH', '\\n', 'Final training loss: 0.4982', '\\n', 'Final\nvalidation loss: 0.5484', '\\n', 'Final training macro F1 score: 0.7960', '\\n',\n'Final validation macro F1 score: 0.7859', '\\n', 'Best test macro F1 score:\n0.7940', '\\n', 'Execution time: a moment seconds (time limit is 30 minutes).']", "", "", "['Dataset: epochs_5', '\\n', '  train macro F1 score: 0.7945', '\\n', '\nvalidation macro F1 score: 0.7940', '\\n', '  test macro F1 score: 0.7970', '\\n',\n'  training loss: 0.4993', '\\n', '  validation loss: 0.5400', '\\n', '  test\nloss: 0.5127\\n', '\\n', 'Dataset: epochs_10', '\\n', '  train macro F1 score:\n0.7965', '\\n', '  validation macro F1 score: 0.7919', '\\n', '  test macro F1\nscore: 0.8000', '\\n', '  training loss: 0.5002', '\\n', '  validation loss:\n0.5274', '\\n', '  test loss: 0.5069\\n', '\\n', 'Dataset: epochs_20', '\\n', '\ntrain macro F1 score: 0.8045', '\\n', '  validation macro F1 score: 0.7899',\n'\\n', '  test macro F1 score: 0.7950', '\\n', '  training loss: 0.4675', '\\n', '\nvalidation loss: 0.5546', '\\n', '  test loss: 0.5328\\n', '\\n', 'Dataset:\nepochs_30', '\\n', '  train macro F1 score: 0.8020', '\\n', '  validation macro F1\nscore: 0.7840', '\\n', '  test macro F1 score: 0.7920', '\\n', '  training loss:\n0.4803', '\\n', '  validation loss: 0.5498', '\\n', '  test loss: 0.5289\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: epochs_5', '\\n', '  train macro F1 score: 0.7940', '\\n', '\nvalidation macro F1 score: 0.7959', '\\n', '  test macro F1 score: 0.7939', '\\n',\n'  training loss: 0.5067', '\\n', '  validation loss: 0.5383', '\\n', '  test\nloss: 0.5147\\n', '\\n', 'Dataset: epochs_10', '\\n', '  train macro F1 score:\n0.7955', '\\n', '  validation macro F1 score: 0.7860', '\\n', '  test macro F1\nscore: 0.7859', '\\n', '  training loss: 0.4997', '\\n', '  validation loss:\n0.5925', '\\n', '  test loss: 0.5618\\n', '\\n', 'Dataset: epochs_20', '\\n', '\ntrain macro F1 score: 0.7970', '\\n', '  validation macro F1 score: 0.7920',\n'\\n', '  test macro F1 score: 0.7919', '\\n', '  training loss: 0.5096', '\\n', '\nvalidation loss: 0.5625', '\\n', '  test loss: 0.5325\\n', '\\n', 'Dataset:\nepochs_30', '\\n', '  train macro F1 score: 0.7924', '\\n', '  validation macro F1\nscore: 0.7859', '\\n', '  test macro F1 score: 0.7554', '\\n', '  training loss:\n0.5204', '\\n', '  validation loss: 0.5652', '\\n', '  test loss: 0.5591\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", "['Dataset: epochs_5', '\\n', '  train macro F1 score: 0.7965', '\\n', '\nvalidation macro F1 score: 0.7879', '\\n', '  test macro F1 score: 0.7929', '\\n',\n'  training loss: 0.5162', '\\n', '  validation loss: 0.5704', '\\n', '  test\nloss: 0.5286\\n', '\\n', 'Dataset: epochs_10', '\\n', '  train macro F1 score:\n0.7985', '\\n', '  validation macro F1 score: 0.7979', '\\n', '  test macro F1\nscore: 0.8000', '\\n', '  training loss: 0.4947', '\\n', '  validation loss:\n0.5415', '\\n', '  test loss: 0.5134\\n', '\\n', 'Dataset: epochs_20', '\\n', '\ntrain macro F1 score: 0.7920', '\\n', '  validation macro F1 score: 0.7800',\n'\\n', '  test macro F1 score: 0.7909', '\\n', '  training loss: 0.5087', '\\n', '\nvalidation loss: 0.5633', '\\n', '  test loss: 0.5343\\n', '\\n', 'Dataset:\nepochs_30', '\\n', '  train macro F1 score: 0.8050', '\\n', '  validation macro F1\nscore: 0.7918', '\\n', '  test macro F1 score: 0.7990', '\\n', '  training loss:\n0.4799', '\\n', '  validation loss: 0.5394', '\\n', '  test loss: 0.5139\\n', '\\n',\n'Execution time: a moment seconds (time limit is 30 minutes).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}